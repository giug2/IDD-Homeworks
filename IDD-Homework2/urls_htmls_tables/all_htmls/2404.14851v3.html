<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>From Matching to Generation: A Survey on Generative Information Retrieval</title>
<!--Generated on Thu May 16 03:31:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Generative Information Retrieval; Generative Document Retrieval; Reliable Response Generation
" lang="en" name="keywords"/>
<base href="/html/2404.14851v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S1" title="In From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S2" title="In From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_smallcaps">Background and Preliminaries</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S2.SS1" title="In 2 Background and Preliminaries ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span><span class="ltx_text ltx_font_italic">Traditional Information Retrieval</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S2.SS2" title="In 2 Background and Preliminaries ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span><span class="ltx_text ltx_font_italic">Generative Retrieval</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S2.SS3" title="In 2 Background and Preliminaries ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span><span class="ltx_text ltx_font_italic">Large Language Models</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3" title="In From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS1" title="In 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span><span class="ltx_text ltx_font_italic">Model Training and Structure</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS1.SSS1" title="In 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Model Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS1.SSS2" title="In 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Model Structure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS2" title="In 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span><span class="ltx_text ltx_font_italic">Design of Document Identifiers</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS2.SSS1" title="In 3.2 Design of Document Identifiers ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Numeric-based Identifiers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS2.SSS2" title="In 3.2 Design of Document Identifiers ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Text-based Identifiers</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS3" title="In 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span><span class="ltx_text ltx_font_italic">Incremental Learning on Dynamic Corpora</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS4" title="In 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span><span class="ltx_text ltx_font_italic">Downstream Task Adaption</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS4.SSS1" title="In 3.4 Downstream Task Adaption ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Separate Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS4.SSS2" title="In 3.4 Downstream Task Adaption ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Joint Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS4.SSS3" title="In 3.4 Downstream Task Adaption ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Multi-modal Generative Retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS5" title="In 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span><span class="ltx_text ltx_font_italic">Generative Recommender Systems</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4" title="In From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_text ltx_font_smallcaps">Reliable Response Generation: Direct Information Accessing with Generative Language Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS1" title="In 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span><span class="ltx_text ltx_font_italic">Internal Knowledge Memorization</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS1.SSS1" title="In 4.1 Internal Knowledge Memorization ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Model Structure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS1.SSS2" title="In 4.1 Internal Knowledge Memorization ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Training and Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS1.SSS3" title="In 4.1 Internal Knowledge Memorization ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Knowledge Updating</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS2" title="In 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span><span class="ltx_text ltx_font_italic">External Knowledge Augmentation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS2.SSS1" title="In 4.2 External Knowledge Augmentation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Retrieval Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS2.SSS2" title="In 4.2 External Knowledge Augmentation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Tool Augmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS3" title="In 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span><span class="ltx_text ltx_font_italic">Generating Response with Citation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS3.SSS1" title="In 4.3 Generating Response with Citation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Direct Generating Response with Citation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS3.SSS2" title="In 4.3 Generating Response with Citation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Retrieval-based Response with Citation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS4" title="In 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span><span class="ltx_text ltx_font_italic">Personal Information Assistant</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS4.SSS1" title="In 4.4 Personal Information Assistant ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Personalized Dialogue System</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS4.SSS2" title="In 4.4 Personal Information Assistant ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Domain-specific Personalization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5" title="In From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS1" title="In 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span><span class="ltx_text ltx_font_italic">Evaluation for Generative Document Retrieval</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS1.SSS1" title="In 5.1 Evaluation for Generative Document Retrieval ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS1.SSS2" title="In 5.1 Evaluation for Generative Document Retrieval ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Benchmarks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS1.SSS3" title="In 5.1 Evaluation for Generative Document Retrieval ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.3 </span>Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS2" title="In 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span><span class="ltx_text ltx_font_italic">Evaluation for Response Generation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS2.SSS1" title="In 5.2 Evaluation for Response Generation ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS2.SSS2" title="In 5.2 Evaluation for Response Generation ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Benchmarks and Analysis</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6" title="In From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span><span class="ltx_text ltx_font_smallcaps">Challenges and Prospects</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1" title="In 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span><span class="ltx_text ltx_font_italic">Challenges on Generative Document Retrieval</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS1" title="In 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Scalability Issues</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS2" title="In 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Handling Dynamic Corpora</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS3" title="In 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Document Identifier</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS4" title="In 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.4 </span>Efficiency Concerns</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2" title="In 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span><span class="ltx_text ltx_font_italic">Challenges on Reliable Response Generation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2.SSS1" title="In 6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Improving Accuracy and Factuality</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2.SSS2" title="In 6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Real-time Properties of GenIR Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2.SSS3" title="In 6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.3 </span>Bias and Fairness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2.SSS4" title="In 6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.4 </span>Privacy and Security</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS3" title="In 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span><span class="ltx_text ltx_font_italic">Unified Framework</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS3.SSS1" title="In 6.3 Unified Framework ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Unified Framework for Retrieval and Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS3.SSS2" title="In 6.3 Unified Framework ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Towards End-to-End Framework for Various IR Tasks</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S7" title="In From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">{justify}</span>
</div>
<h1 class="ltx_title ltx_title_document">From Matching to Generation: A Survey on Generative Information Retrieval</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou
</span><span class="ltx_author_notes">
Github Repository: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/RUC-NLPIR/GenIR-Survey" title="">https://github.com/RUC-NLPIR/GenIR-Survey</a>
<br class="ltx_break"/>Zhicheng Dou is the corresponding author. All authors are from Gaoling School of Artificial Intelligence, Renmin University of China. 
<br class="ltx_break"/>Contact E-mail: xiaoxi_li@ruc.edu.cn, dou@ruc.edu.cn. 
<br class="ltx_break"/></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Information Retrieval (IR) systems are crucial tools for users to access information, widely applied in scenarios like search engines, question answering, and recommendation systems. Traditional IR methods, based on similarity matching to return ranked lists of documents, have been reliable means of information acquisition, dominating the IR field for years. With the advancement of pre-trained language models, generative information retrieval (GenIR) has emerged as a novel paradigm, gaining increasing attention in recent years.
Currently, research in GenIR can be categorized into two aspects: generative document retrieval (GR) and reliable response generation. GR leverages the generative model’s parameters for memorizing documents, enabling retrieval by directly generating relevant document identifiers without explicit indexing. Reliable response generation, on the other hand, employs language models to directly generate the information users seek, breaking the limitations of traditional IR in terms of document granularity and relevance matching, offering more flexibility, efficiency, and creativity, thus better meeting practical needs.
This paper aims to systematically review the latest research progress in GenIR. We will summarize the advancements in GR regarding model training, document identifier, incremental learning, downstream tasks adaptation, multi-modal GR and generative recommendation, as well as progress in reliable response generation in aspects of internal knowledge memorization, external knowledge augmentation, generating response with citations and personal information assistant. We also review the evaluation, challenges and future prospects in GenIR systems. This review aims to offer a comprehensive reference for researchers in the GenIR field, encouraging further development in this area.
</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Generative Information Retrieval; Generative Document Retrieval; Reliable Response Generation

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="322" id="S1.F1.g1" src="x1.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Exploring IR Evolution: From Traditional to Generative Methods - This diagram illustrates the shift from traditional similarity-based document matching (a) to GenIR techniques. Current GenIR methods can be categorized into two types: generative retrieval (b), which retrieves documents by directly generating relevant DocIDs constrained by a DocID prefix tree; and response generation (c), which directly generates reliable and user-centric answers.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In today’s digital landscape, information retrieval (IR) systems are crucial for navigating the vast sea of online information. From using search engines such as Google <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib1" title="">1</a>]</cite>, Bing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib2" title="">2</a>]</cite>, and Baidu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib3" title="">3</a>]</cite>, to engaging with question-answering or dialogue systems like ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib3" title="">3</a>]</cite> and Bing Chat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib4" title="">4</a>]</cite>, and discovering content via recommendation platforms like Amazon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib5" title="">5</a>]</cite> and YouTube <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib6" title="">6</a>]</cite>, IR technologies are integral to our everyday online experiences. These systems are not only reliable but also play a key role in spreading knowledge and ideas globally.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditional IR systems primarily rely on sparse retrieval methods based on word-level matching. These methods, which include Boolean Retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib7" title="">7</a>]</cite>, BM25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib8" title="">8</a>]</cite>, SPLADE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib9" title="">9</a>]</cite>, and UniCOIL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib10" title="">10</a>]</cite>, establish connections between vocabulary and documents, offering high retrieval efficiency and robust system performance. With the rise of deep learning, dense retrieval methods such as DPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib11" title="">11</a>]</cite> and ANCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib12" title="">12</a>]</cite>, based on the bidirectional encoding representations from the BERT model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib13" title="">13</a>]</cite>, capture the deep semantic information of documents, significantly improving retrieval precision. Although these methods have achieved leaps in accuracy, they rely on large-scale document indices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib15" title="">15</a>]</cite> and cannot be optimized in an end-to-end way. Moreover, when people search for information, what they really need is a precise and reliable answer. This document ranking list-based IR approach still requires users to spend time summarizing their required answers, which is not ideal enough for information seeking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">With the development of Transformer-based pre-trained language models such as T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib17" title="">17</a>]</cite>, BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib18" title="">18</a>]</cite>, and GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib19" title="">19</a>]</cite>, they have demonstrated their strong text generation capabilities. In recent years, large language models (LLMs) have brought about revolutionary changes in the field of AI-generated content (AIGC) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib21" title="">21</a>]</cite>. Based on large pre-training corpora and advanced training techniques like RLHF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib22" title="">22</a>]</cite>, LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib25" title="">25</a>]</cite> have made significant progress in natural language tasks, such as dialogue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib26" title="">26</a>]</cite> and question answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib28" title="">28</a>]</cite>. The rapid development of LLMs is transforming IR systems, giving rise to a new paradigm of generative information retrieval (GenIR), which achieves IR goals through generative approaches.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">As envisioned by Metzler et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib16" title="">16</a>]</cite>, in order to build an IR system that can respond like a domain expert, the system should not only provide accurate responses but also include source citations to improve credibility and transparency of the results. To achieve this, GenIR models must possess both sufficient memorized knowledge and the ability to recall the associations between knowledge and corresponding documents. Current research in GenIR is primarily focused on two main patterns: (1) generative document retrieval (GR), which involves retrieving documents by generating their identifiers; and (2) reliable response generation, which entails directly generating user-centric responses through strategies that enhance their reliability. These two patterns are the central topics of this survey.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Generative document retrieval, a new retrieval paradigm based on generative models, is garnering increasing attention. This approach leverages the parametric memory of generative models to directly generate document identifiers (DocIDs) related to the documents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib32" title="">32</a>]</cite>. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates this transition, where traditional IR systems match queries to documents based on indexed database (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>(a)), while generative methods use language models to retrieve by directly generating relevant document identifiers (Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>(b)).
Specifically, GR assigns a unique identifier to each document, which can be numeric-based or text-based, and then trains a generative retrieval model to learn the mapping from queries to the relevant document DocIDs. This allows the model to index documents using its internal parameters. During inference, GR models use constrained beam search to limit the generated DocIDs to be valid within the corpus, ranking them based on generation probability to produce a ranked list of DocIDs. This eliminates the need for large-scale document indexes in traditional methods, enabling end-to-end training of the model.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Recent studies on generative retrieval have delved into model training and structure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib34" title="">34</a>]</cite>, document identifier design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib36" title="">36</a>]</cite>, continual learning on dynamic corpora <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib39" title="">39</a>]</cite>, downstream task adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib42" title="">42</a>]</cite>, multi-modal generative retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib44" title="">44</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib45" title="">45</a>]</cite>, and generative recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib48" title="">48</a>]</cite>. The progress in GR is shifting retrieval systems from matching to generation.
It has also led to the emergence of workshops <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib49" title="">49</a>]</cite> and tutorials <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib50" title="">50</a>]</cite>. However, there is currently no comprehensive review that systematically organizes the research, challenges, and prospects of this emerging field.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Reliable response generation is also a promising direction in the IR field, offering user-centric and accurate answers that directly meet users’ needs.
Since LLMs are particularly adept at following instructions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib20" title="">20</a>]</cite>, capable of generating customized responses, and can even cite the knowledge sources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib52" title="">52</a>]</cite>, making direct response generation a new and intuitive way to access information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib55" title="">55</a>]</cite>.
As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>, the generative approach marks a significant shift from traditional IR systems, which return a ranked list of documents (as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>(a,b)). Instead, response generation methods (depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>(c)) offer a more dynamic form of information access by directly generating detailed, user-centric responses, thereby providing a richer and more immediate understanding of the information need behind the users’ queries.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">However, the responses generated by language models may not always be reliable. They have the potential to generate irrelevant answers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib56" title="">56</a>]</cite>, contradict factual information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib58" title="">58</a>]</cite>, provide outdated data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib59" title="">59</a>]</cite>, or generate toxic content <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib60" title="">60</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib61" title="">61</a>]</cite>. Consequently, these limitations render them unsuitable for many scenarios that require accurate and up-to-date information.
To address these challenges, the academic community has developed strategies across four key aspects: enhancing internal knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib70" title="">70</a>]</cite>; augmenting external knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib77" title="">77</a>]</cite>; generating responses with citation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib79" title="">79</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib80" title="">80</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib81" title="">81</a>]</cite>; and improving personal information assistance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib82" title="">82</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib83" title="">83</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib84" title="">84</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib85" title="">85</a>]</cite>. Despite these efforts, there is still a lack of a systematic review that organizes the existing research under this new paradigm of generative information access.</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.F2.1" style="width:575.0pt;height:1270.6pt;vertical-align:-1263.7pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S1.F2.1.1"><span class="ltx_text" id="S1.F2.1.1.1"><span class="ltx_ERROR undefined" id="S1.F2.1.1.1.1">{forest}</span>
forked edges,
for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=center,
font=<span class="ltx_text" id="S1.F2.1.1.1.2" style="font-size:120%;">,
rectangle,
draw=draw-leaf,
rounded corners,
align=left,
text centered,
minimum width=5em,
edge+=darkgray, line width=1pt,
s sep=3pt,
inner xsep=2pt,
inner ysep=3pt,
line width=1.2pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
</span>,
where level=0text width=8.5em,fill=fill_0,draw=draw_0,font=<span class="ltx_text" id="S1.F2.1.1.1.3" style="font-size:120%;">,</span>,
where level=1text width=10em,fill=fill_1,draw=draw_1,font=,,
where level=2text width=10.4em,fill=fill_2,draw=draw_2,font=,,
where level=3font=<span class="ltx_text" id="S1.F2.1.1.1.4" style="font-size:80%;">,</span>,
[
GenIR System, minimum height=2.8em[
Generative Document
<br class="ltx_break"/>Retrieval (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3" title="3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a>)[
Model Training and 
<br class="ltx_break"/>Structure (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS1" title="3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.1</span></a>)[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.5">Training</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS1.SSS1" title="3.1.1 Model Training ‣ 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>): DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>, DynamicRetriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib86" title="">86</a>]</cite>, NCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>]</cite>, DSI-QG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib87" title="">87</a>]</cite>,
<br class="ltx_break"/>Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib88" title="">88</a>]</cite>, LTRGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib89" title="">89</a>]</cite>, GenRRL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib34" title="">34</a>]</cite>, DGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib90" title="">90</a>]</cite>, ListGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib91" title="">91</a>]</cite>,, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.6">Structure</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS1.SSS2" title="3.1.2 Model Structure ‣ 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>): NCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>]</cite>, TOME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib92" title="">92</a>]</cite>, NP Decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib93" title="">93</a>]</cite>, MEVI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib94" title="">94</a>]</cite>,
<br class="ltx_break"/>DiffusionRet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib95" title="">95</a>]</cite>, GDR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib96" title="">96</a>]</cite>, Self-Retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib97" title="">97</a>]</cite>, PAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib98" title="">98</a>]</cite>, leaf, text width=31.8em
]
]
[
Document Identifier
<br class="ltx_break"/>(Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS2" title="3.2 Design of Document Identifiers ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.2</span></a>)[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.7">Numeric</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS2.SSS1" title="3.2.1 Numeric-based Identifiers ‣ 3.2 Design of Document Identifiers ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>): DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>, DynamicRetriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib86" title="">86</a>]</cite>, Ultron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib32" title="">32</a>]</cite>, GenRet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib35" title="">35</a>]</cite>,
<br class="ltx_break"/>Tied-Atomic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib99" title="">99</a>]</cite>, MEVI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib94" title="">94</a>]</cite>, LMIndexer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib100" title="">100</a>]</cite>, ASI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib36" title="">36</a>]</cite>, RIPOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib101" title="">101</a>]</cite>, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.8">Text</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS2.SSS2" title="3.2.2 Text-based Identifiers ‣ 3.2 Design of Document Identifiers ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>): GENRE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib29" title="">29</a>]</cite>, SEAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib102" title="">102</a>]</cite>, Ultron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib32" title="">32</a>]</cite>, LLM-URL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib103" title="">103</a>]</cite>,
<br class="ltx_break"/>UGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib104" title="">104</a>]</cite>, MINDER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib105" title="">105</a>]</cite>, AutoTSG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib106" title="">106</a>]</cite>, SE-DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib106" title="">106</a>]</cite>, NOVO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib107" title="">107</a>]</cite>, GLEN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib108" title="">108</a>]</cite>, leaf, text width=31.8em
]
]
[
Incremental Learning
<br class="ltx_break"/>(Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS3" title="3.3 Incremental Learning on Dynamic Corpora ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.3</span></a>)[
DSI++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib37" title="">37</a>]</cite>, IncDSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib38" title="">38</a>]</cite>, CLEVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib109" title="">109</a>]</cite>, CorpusBrain++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib39" title="">39</a>]</cite>, leaf, text width=31.8em
]
]
[
Downstream Task 
<br class="ltx_break"/>Adaptation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS4" title="3.4 Downstream Task Adaption ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.4</span></a>)[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.9">Separate Training</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS4.SSS1" title="3.4.1 Separate Training ‣ 3.4 Downstream Task Adaption ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.4.1</span></a>): GERE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib40" title="">40</a>]</cite>, CorpusBrain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib41" title="">41</a>]</cite>, GMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib110" title="">110</a>]</cite>, DearDR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib111" title="">111</a>]</cite>,
<br class="ltx_break"/>CodeDSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib112" title="">112</a>]</cite>, UGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib104" title="">104</a>]</cite>, GCoQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib113" title="">113</a>]</cite>, Re3val <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib114" title="">114</a>]</cite>, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.10">Joint Training</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS4.SSS2" title="3.4.2 Joint Training ‣ 3.4 Downstream Task Adaption ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.4.2</span></a>): UniGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib115" title="">115</a>]</cite>, CorpusLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib42" title="">42</a>]</cite>, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.11">Multi-Modal</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS4.SSS3" title="3.4.3 Multi-modal Generative Retrieval ‣ 3.4 Downstream Task Adaption ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.4.3</span></a>): IRGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib43" title="">43</a>]</cite>, GeMKR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib44" title="">44</a>]</cite>, GRACE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib45" title="">45</a>]</cite>, leaf, text width=31.8em
]
]
[
Generative Recommender
<br class="ltx_break"/>Systems (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS5" title="3.5 Generative Recommender Systems ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.5</span></a>), text width=11.9em[
P5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib46" title="">46</a>]</cite>, TIGER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib48" title="">48</a>]</cite>, SEATER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib116" title="">116</a>]</cite>, IDGenRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib117" title="">117</a>]</cite>, LC-Rec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib118" title="">118</a>]</cite>,
<br class="ltx_break"/>ColaRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib119" title="">119</a>]</cite>, leaf, text width=30.3em
]
]
]
[
Reliable Response
<br class="ltx_break"/>Generation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4" title="4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4</span></a>) [
Internal Knowledge
<br class="ltx_break"/>Memorization (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS1" title="4.1 Internal Knowledge Memorization ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.1</span></a>) [
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.12">Structure</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS1.SSS1" title="4.1.1 Model Structure ‣ 4.1 Internal Knowledge Memorization ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>):
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.13">Model Scaling:</span> GPT3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib62" title="">62</a>]</cite>, BLOOM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib63" title="">63</a>]</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib64" title="">64</a>]</cite>,
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.14">Model Structure:</span> PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib120" title="">120</a>]</cite>, Mixtral 8x7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib121" title="">121</a>]</cite>, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.15">Training and Inference</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS1.SSS2" title="4.1.2 Training and Inference ‣ 4.1 Internal Knowledge Memorization ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>):
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.16">Training:</span> Sadeq et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib122" title="">122</a>]</cite> FactTune <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib123" title="">123</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.17">Inference:</span> GenRead <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib124" title="">124</a>]</cite>, RECITE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib125" title="">125</a>]</cite>, DoLa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib67" title="">67</a>]</cite>, leaf, text width=31.8em]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.18">Knowledge Updating</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS1.SSS3" title="4.1.3 Knowledge Updating ‣ 4.1 Internal Knowledge Memorization ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.1.3</span></a>):
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.19">Incremental Learning:</span> Ernie 2.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib68" title="">68</a>]</cite>, DAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib69" title="">69</a>]</cite>;
<br class="ltx_break"/>DynaInst <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib126" title="">126</a>]</cite>,
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.20">Knowledge Editing:</span> KE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib127" title="">127</a>]</cite>, MEND <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib128" title="">128</a>]</cite>, ROME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib70" title="">70</a>]</cite>
, leaf, text width=31.8em
]
]
[
External Knowledge
<br class="ltx_break"/>Augmentation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS2" title="4.2 External Knowledge Augmentation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.2</span></a>) [
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.21">Retrieval</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS2.SSS1" title="4.2.1 Retrieval Augmentation ‣ 4.2 External Knowledge Augmentation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>):
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.22">Sequential:</span> RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib71" title="">71</a>]</cite>, RRR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib129" title="">129</a>]</cite>, ARL2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib130" title="">130</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.23">Branching:</span> TOC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib131" title="">131</a>]</cite>, BlendFilter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib132" title="">132</a>]</cite>, REPLUG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib133" title="">133</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.24">Conditional:</span> SKR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib134" title="">134</a>]</cite>, Self-DC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib72" title="">72</a>]</cite>, Rowen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib135" title="">135</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.25">Loop:</span> Iter-RetGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib136" title="">136</a>]</cite>, IR-COT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib137" title="">137</a>]</cite>, FLARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib76" title="">76</a>]</cite>, Self-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib73" title="">73</a>]</cite>
, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.26">Tool</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS2.SSS2" title="4.2.2 Tool Augmentation ‣ 4.2 External Knowledge Augmentation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>):
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.27">Search Engine:</span> ReAct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib74" title="">74</a>]</cite>, WebGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib51" title="">51</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.28">Knowledge Graph:</span> StructGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib75" title="">75</a>]</cite>, ToG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib138" title="">138</a>]</cite>, RoG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib139" title="">139</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.29">API-based Tools:</span> Toolformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib77" title="">77</a>]</cite>, ToolLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib140" title="">140</a>]</cite>, AssistGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib141" title="">141</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.30">Model-based Tools:</span> HuggingGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib142" title="">142</a>]</cite>, Visual ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib143" title="">143</a>]</cite>
, leaf, text width=31.8em
]
]
[
Generating Response
<br class="ltx_break"/>with Citation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS3" title="4.3 Generating Response with Citation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.3</span></a>) [
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.31">Direct Citation</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS3.SSS1" title="4.3.1 Direct Generating Response with Citation ‣ 4.3 Generating Response with Citation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>):
According-to Prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib78" title="">78</a>]</cite>, IFL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib79" title="">79</a>]</cite>, Fierro et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib144" title="">144</a>]</cite>,
<br class="ltx_break"/>Credible without Credit <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib145" title="">145</a>]</cite>, 1-PAGER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib146" title="">146</a>]</cite>, Khalifa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib147" title="">147</a>]</cite>
, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.32">Retrieval-based Citation</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS3.SSS2" title="4.3.2 Retrieval-based Response with Citation ‣ 4.3 Generating Response with Citation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.3.2</span></a>):
WebGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib51" title="">51</a>]</cite>, WebBrain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib52" title="">52</a>]</cite>, RARR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib148" title="">148</a>]</cite>,
<br class="ltx_break"/>SearChain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib149" title="">149</a>]</cite>, LLatrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib150" title="">150</a>]</cite>, VTG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib151" title="">151</a>]</cite>, CEG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib152" title="">152</a>]</cite>, APO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib81" title="">81</a>]</cite>
, leaf, text width=31.8em
]
]
[
Personal Information 
<br class="ltx_break"/>Assistant (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS4" title="4.4 Personal Information Assistant ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.4</span></a>) [
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.33">Personalized Dialogue</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS4.SSS1" title="4.4.1 Personalized Dialogue System ‣ 4.4 Personal Information Assistant ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.4.1</span></a>):
Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib153" title="">153</a>]</cite>, <math alttext="\mathcal{P}^{2}" class="ltx_Math" display="inline" id="S1.F2.1.1.1.m1.1"><semantics id="S1.F2.1.1.1.m1.1a"><msup id="S1.F2.1.1.1.m1.1.1" xref="S1.F2.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S1.F2.1.1.1.m1.1.1.2" xref="S1.F2.1.1.1.m1.1.1.2.cmml">𝒫</mi><mn id="S1.F2.1.1.1.m1.1.1.3" xref="S1.F2.1.1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S1.F2.1.1.1.m1.1b"><apply id="S1.F2.1.1.1.m1.1.1.cmml" xref="S1.F2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.F2.1.1.1.m1.1.1.1.cmml" xref="S1.F2.1.1.1.m1.1.1">superscript</csymbol><ci id="S1.F2.1.1.1.m1.1.1.2.cmml" xref="S1.F2.1.1.1.m1.1.1.2">𝒫</ci><cn id="S1.F2.1.1.1.m1.1.1.3.cmml" type="integer" xref="S1.F2.1.1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F2.1.1.1.m1.1c">\mathcal{P}^{2}</annotation><annotation encoding="application/x-llamapun" id="S1.F2.1.1.1.m1.1d">caligraphic_P start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>Bot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib82" title="">82</a>]</cite>, Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib154" title="">154</a>]</cite>, 
<br class="ltx_break"/>SAFARI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib83" title="">83</a>]</cite>, Personalized Soups <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib155" title="">155</a>]</cite>, OPPU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib156" title="">156</a>]</cite>
, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.34">Domain-specific</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS4.SSS2" title="4.4.2 Domain-specific Personalization ‣ 4.4 Personal Information Assistant ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.4.2</span></a>):
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.35">Healthcare:</span> Zhongjing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib157" title="">157</a>]</cite>, Mental-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib84" title="">84</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.36">Academic:</span> RevGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib85" title="">85</a>]</cite>, Pearl <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib158" title="">158</a>]</cite>;
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.37">Education:</span> EduChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib159" title="">159</a>]</cite>;
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.38">Recipe</span>, <span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.39">Robot</span>, etc.
, leaf, text width=31.8em
]
]
]
[
Evaluation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5" title="5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a>)[
Generative Document
<br class="ltx_break"/>Retrieval (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS1" title="5.1 Evaluation for Generative Document Retrieval ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5.1</span></a>)[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.40">Metrics</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS1.SSS1" title="5.1.1 Metrics ‣ 5.1 Evaluation for Generative Document Retrieval ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5.1.1</span></a>): Recall, MRR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib160" title="">160</a>]</cite>, R-Precision, MAP, nDCG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib161" title="">161</a>]</cite>, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.41">Benchmarks</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS1.SSS2" title="5.1.2 Benchmarks ‣ 5.1 Evaluation for Generative Document Retrieval ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5.1.2</span></a>): MS MARCO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib162" title="">162</a>]</cite>, NQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib163" title="">163</a>]</cite>, TriviaQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib164" title="">164</a>]</cite>, KILT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib165" title="">165</a>]</cite>,
<br class="ltx_break"/>TREC DL 19 &amp; 20 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib166" title="">166</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib167" title="">167</a>]</cite>, DynamicIR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib168" title="">168</a>]</cite>, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib169" title="">169</a>]</cite>, ExcluIR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib170" title="">170</a>]</cite>, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.42">Analysis</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS1.SSS3" title="5.1.3 Analysis ‣ 5.1 Evaluation for Generative Document Retrieval ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5.1.3</span></a>): Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib88" title="">88</a>]</cite>, Pradeep et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib171" title="">171</a>]</cite>, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib169" title="">169</a>]</cite>,
<br class="ltx_break"/>Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib172" title="">172</a>]</cite>, leaf, text width=31.8em
]
]
[
Reliable Response
<br class="ltx_break"/>Generation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS2" title="5.2 Evaluation for Response Generation ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5.2</span></a>)[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.43">Metrics</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS2.SSS1" title="5.2.1 Metrics ‣ 5.2 Evaluation for Response Generation ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5.2.1</span></a>): <span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.44">Rule-based:</span> EM, BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib173" title="">173</a>]</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib174" title="">174</a>]</cite>, Perplexity;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.45">Model-based:</span> BERTScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib175" title="">175</a>]</cite>, BLEURT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib176" title="">176</a>]</cite>, GPTScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib177" title="">177</a>]</cite>, FActScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib178" title="">178</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.46">Human Evaluation:</span> Comprehensibility, Relevance, Fluency
, leaf, text width=31.8em
]
[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.47">Benchmarks</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5.SS2.SSS2" title="5.2.2 Benchmarks and Analysis ‣ 5.2 Evaluation for Response Generation ‣ 5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5.2.2</span></a>):
<span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.48">General:</span> MMLU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib179" title="">179</a>]</cite>, BIG-bench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib180" title="">180</a>]</cite>, LLM-Eval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib181" title="">181</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.49">Tool:</span> API-Bank <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib182" title="">182</a>]</cite>, ToolBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib140" title="">140</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.50">Factuality:</span> TruthfulQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib183" title="">183</a>]</cite>, ALCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib150" title="">150</a>]</cite>, HaluEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib184" title="">184</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.51">Real Time:</span> RealTime QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib185" title="">185</a>]</cite>, FreshQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib59" title="">59</a>]</cite>;
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S1.F2.1.1.1.52">Trustworthy:</span> SafetyBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib186" title="">186</a>]</cite>, TrustGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib60" title="">60</a>]</cite>, TrustLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib61" title="">61</a>]</cite>
, leaf, text width=31.8em
]
]
]
[
Challenges and 
<br class="ltx_break"/>Prospects (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6" title="6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6</span></a>)[
Generative Document
<br class="ltx_break"/>Retrieval (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1" title="6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.1</span></a>)[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.53">Scalability</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS1" title="6.1.1 Scalability Issues ‣ 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>);
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.54">Dynamic Corpora</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS2" title="6.1.2 Handling Dynamic Corpora ‣ 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.1.2</span></a>);
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.55">Document Representation</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS3" title="6.1.3 Document Identifier ‣ 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.1.3</span></a>);
<br class="ltx_break"/><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.56">Efficiency</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS4" title="6.1.4 Efficiency Concerns ‣ 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.1.4</span></a>)
, leaf, text width=31.8em
]
]
[
Reliable Response
<br class="ltx_break"/>Generation (Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2" title="6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.2</span></a>)[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.57">Accuracy and Factuality</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2.SSS1" title="6.2.1 Improving Accuracy and Factuality ‣ 6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.2.1</span></a>);
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.58">Real-time Property</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2.SSS2" title="6.2.2 Real-time Properties of GenIR Systems ‣ 6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.2.2</span></a>);
<br class="ltx_break"/><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.59">Bias and Fairness</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2.SSS3" title="6.2.3 Bias and Fairness ‣ 6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.2.3</span></a>);
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.60">Privacy and Security</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS2.SSS4" title="6.2.4 Privacy and Security ‣ 6.2 Challenges on Reliable Response Generation ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.2.4</span></a>)
, leaf, text width=31.8em
]
]
[
Unified Framework
<br class="ltx_break"/>(Sec <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS3" title="6.3 Unified Framework ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.3</span></a>)[
<span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.61">Unified Framework for Retrieval and Generation</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS3.SSS1" title="6.3.1 Unified Framework for Retrieval and Generation ‣ 6.3 Unified Framework ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.3.1</span></a>); 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.F2.1.1.1.62">Towards End2end Framework for Various IR Tasks</span> (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS3.SSS2" title="6.3.2 Towards End-to-End Framework for Various IR Tasks ‣ 6.3 Unified Framework ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.3.2</span></a>)
, leaf, text width=31.8em
]
]
]
]</span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Taxonomy of research on generative information retrieval: investigating generative document retrieval, reliable response generation, evaluation, challenges and prospects.</figcaption>
</figure>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">This review will systematically review the latest research progress and future developments in the field of GenIR, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a>, which displays the classification of research related to the GenIR system. We will introduce background knowledge in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S2" title="2 Background and Preliminaries ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a>, generative document retrieval technologies in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3" title="3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a>, direct information accessing with generative language models in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4" title="4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4</span></a>, evaluation in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S5" title="5 Evaluation ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a>, current challenges and future directions in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6" title="6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6</span></a>, respectively. Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S7" title="7 Conclusion ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">7</span></a> will summarize the content of this review. This article is the first to systematically organize the research, evaluation, challenges and prospects of generative IR, while also looking forward to the potential and importance of GenIR’s future development. Through this review, readers will gain a deep understanding of the latest progress in developing GenIR systems and how it shapes the future of information access.</p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">The main contribution of this survey is as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">First comprehensive survey on generative information retrieval:</span> This survey is the first to comprehensively organize the techniques, evaluation, challenges, and prospects on the emerging field of GenIR, providing a deep understanding of the latest progress in developing GenIR systems and its future in shaping information access.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Systematic categorization and in-depth analysis:</span> The survey offers a systematic categorization of research related to GenIR systems, including generative document retrieval, reliable response generation. It provides an in-depth analysis, covering model training and structure, document identifier, etc. in generative document retrieval; internal knowledge memorization, external knowledge enhancement, etc. for reliable response generation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Comprehensive review of evaluation metrics and benchmarks:</span> The survey reviews a range of widely used evaluation metrics and benchmark datasets for accessing GenIR methods, alongside analysis on the effectiveness and weaknesses of existing GenIR methods.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Discussions of current challenges and future directions:</span> The survey identifies and discusses the current challenges faced in the GenIR field. We also provide potential solutions for each challenge and outline future research directions for building GenIR systems.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background and Preliminaries</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Information retrieval techniques aim at efficiently obtaining, processing, and understanding information from massive data. Technological advancements have driven the evolution of retrieval methods, from traditional keyword-based sparse retrieval, to deep learning-based dense retrieval, and further to recently emerged generative retrieval and large language models. Each advancement enhances retrieval accuracy and efficiency, catering to the complex and diverse query needs of users.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span class="ltx_text ltx_font_italic" id="S2.SS1.1.1">Traditional Information Retrieval</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">Sparse Retrieval.</span>
In the field of traditional information retrieval, sparse retrieval techniques implement fast and accurate document retrieval through the inverted index method. Inverted indexing technology maps each unique term to a list of all documents containing that term, providing an efficient means for information retrieval in large document collections. Among these methods, TF-IDF (Term Frequency-Inverse Document Frequency) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib187" title="">187</a>]</cite> is a particularly important statistical tool used to assess the importance of a word in a document collection, thereby widely applied in various traditional retrieval systems.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.7">The core of sparse retrieval technology lies in evaluating the relevance between documents and user queries. Specifically, given a document collection <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">caligraphic_D</annotation></semantics></math> and a user query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">italic_q</annotation></semantics></math>, traditional information retrieval systems identify and retrieve information by calculating the relevance <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.SS1.p2.3.m3.1"><semantics id="S2.SS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><ci id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.3.m3.1d">caligraphic_R</annotation></semantics></math> between document <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p2.4.m4.1"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.4.m4.1d">italic_d</annotation></semantics></math> and query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p2.5.m5.1"><semantics id="S2.SS1.p2.5.m5.1a"><mi id="S2.SS1.p2.5.m5.1.1" xref="S2.SS1.p2.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.5.m5.1b"><ci id="S2.SS1.p2.5.m5.1.1.cmml" xref="S2.SS1.p2.5.m5.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.5.m5.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.5.m5.1d">italic_q</annotation></semantics></math>. This relevance evaluation typically relies on the similarity measure between document <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p2.6.m6.1"><semantics id="S2.SS1.p2.6.m6.1a"><mi id="S2.SS1.p2.6.m6.1.1" xref="S2.SS1.p2.6.m6.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.6.m6.1b"><ci id="S2.SS1.p2.6.m6.1.1.cmml" xref="S2.SS1.p2.6.m6.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.6.m6.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.6.m6.1d">italic_d</annotation></semantics></math> and query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p2.7.m7.1"><semantics id="S2.SS1.p2.7.m7.1a"><mi id="S2.SS1.p2.7.m7.1.1" xref="S2.SS1.p2.7.m7.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.7.m7.1b"><ci id="S2.SS1.p2.7.m7.1.1.cmml" xref="S2.SS1.p2.7.m7.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.7.m7.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.7.m7.1d">italic_q</annotation></semantics></math>, as shown below:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}(q,d)=\sum_{t\in q\cap d}\text{tf-idf}(t,d)\cdot\text{tf-idf}(t,q)," class="ltx_Math" display="block" id="S2.E1.m1.7"><semantics id="S2.E1.m1.7a"><mrow id="S2.E1.m1.7.7.1" xref="S2.E1.m1.7.7.1.1.cmml"><mrow id="S2.E1.m1.7.7.1.1" xref="S2.E1.m1.7.7.1.1.cmml"><mrow id="S2.E1.m1.7.7.1.1.2" xref="S2.E1.m1.7.7.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.7.7.1.1.2.2" xref="S2.E1.m1.7.7.1.1.2.2.cmml">ℛ</mi><mo id="S2.E1.m1.7.7.1.1.2.1" xref="S2.E1.m1.7.7.1.1.2.1.cmml">⁢</mo><mrow id="S2.E1.m1.7.7.1.1.2.3.2" xref="S2.E1.m1.7.7.1.1.2.3.1.cmml"><mo id="S2.E1.m1.7.7.1.1.2.3.2.1" stretchy="false" xref="S2.E1.m1.7.7.1.1.2.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">q</mi><mo id="S2.E1.m1.7.7.1.1.2.3.2.2" xref="S2.E1.m1.7.7.1.1.2.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">d</mi><mo id="S2.E1.m1.7.7.1.1.2.3.2.3" stretchy="false" xref="S2.E1.m1.7.7.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.1.1.1" rspace="0.111em" xref="S2.E1.m1.7.7.1.1.1.cmml">=</mo><mrow id="S2.E1.m1.7.7.1.1.3" xref="S2.E1.m1.7.7.1.1.3.cmml"><munder id="S2.E1.m1.7.7.1.1.3.1" xref="S2.E1.m1.7.7.1.1.3.1.cmml"><mo id="S2.E1.m1.7.7.1.1.3.1.2" movablelimits="false" xref="S2.E1.m1.7.7.1.1.3.1.2.cmml">∑</mo><mrow id="S2.E1.m1.7.7.1.1.3.1.3" xref="S2.E1.m1.7.7.1.1.3.1.3.cmml"><mi id="S2.E1.m1.7.7.1.1.3.1.3.2" xref="S2.E1.m1.7.7.1.1.3.1.3.2.cmml">t</mi><mo id="S2.E1.m1.7.7.1.1.3.1.3.1" xref="S2.E1.m1.7.7.1.1.3.1.3.1.cmml">∈</mo><mrow id="S2.E1.m1.7.7.1.1.3.1.3.3" xref="S2.E1.m1.7.7.1.1.3.1.3.3.cmml"><mi id="S2.E1.m1.7.7.1.1.3.1.3.3.2" xref="S2.E1.m1.7.7.1.1.3.1.3.3.2.cmml">q</mi><mo id="S2.E1.m1.7.7.1.1.3.1.3.3.1" xref="S2.E1.m1.7.7.1.1.3.1.3.3.1.cmml">∩</mo><mi id="S2.E1.m1.7.7.1.1.3.1.3.3.3" xref="S2.E1.m1.7.7.1.1.3.1.3.3.3.cmml">d</mi></mrow></mrow></munder><mrow id="S2.E1.m1.7.7.1.1.3.2" xref="S2.E1.m1.7.7.1.1.3.2.cmml"><mrow id="S2.E1.m1.7.7.1.1.3.2.2" xref="S2.E1.m1.7.7.1.1.3.2.2.cmml"><mrow id="S2.E1.m1.7.7.1.1.3.2.2.2" xref="S2.E1.m1.7.7.1.1.3.2.2.2.cmml"><mtext id="S2.E1.m1.7.7.1.1.3.2.2.2.2" xref="S2.E1.m1.7.7.1.1.3.2.2.2.2a.cmml">tf-idf</mtext><mo id="S2.E1.m1.7.7.1.1.3.2.2.2.1" xref="S2.E1.m1.7.7.1.1.3.2.2.2.1.cmml">⁢</mo><mrow id="S2.E1.m1.7.7.1.1.3.2.2.2.3.2" xref="S2.E1.m1.7.7.1.1.3.2.2.2.3.1.cmml"><mo id="S2.E1.m1.7.7.1.1.3.2.2.2.3.2.1" stretchy="false" xref="S2.E1.m1.7.7.1.1.3.2.2.2.3.1.cmml">(</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">t</mi><mo id="S2.E1.m1.7.7.1.1.3.2.2.2.3.2.2" xref="S2.E1.m1.7.7.1.1.3.2.2.2.3.1.cmml">,</mo><mi id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">d</mi><mo id="S2.E1.m1.7.7.1.1.3.2.2.2.3.2.3" rspace="0.055em" stretchy="false" xref="S2.E1.m1.7.7.1.1.3.2.2.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.7.7.1.1.3.2.2.1" rspace="0.222em" xref="S2.E1.m1.7.7.1.1.3.2.2.1.cmml">⋅</mo><mtext id="S2.E1.m1.7.7.1.1.3.2.2.3" xref="S2.E1.m1.7.7.1.1.3.2.2.3a.cmml">tf-idf</mtext></mrow><mo id="S2.E1.m1.7.7.1.1.3.2.1" xref="S2.E1.m1.7.7.1.1.3.2.1.cmml">⁢</mo><mrow id="S2.E1.m1.7.7.1.1.3.2.3.2" xref="S2.E1.m1.7.7.1.1.3.2.3.1.cmml"><mo id="S2.E1.m1.7.7.1.1.3.2.3.2.1" stretchy="false" xref="S2.E1.m1.7.7.1.1.3.2.3.1.cmml">(</mo><mi id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml">t</mi><mo id="S2.E1.m1.7.7.1.1.3.2.3.2.2" xref="S2.E1.m1.7.7.1.1.3.2.3.1.cmml">,</mo><mi id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml">q</mi><mo id="S2.E1.m1.7.7.1.1.3.2.3.2.3" stretchy="false" xref="S2.E1.m1.7.7.1.1.3.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E1.m1.7.7.1.2" xref="S2.E1.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.7b"><apply id="S2.E1.m1.7.7.1.1.cmml" xref="S2.E1.m1.7.7.1"><eq id="S2.E1.m1.7.7.1.1.1.cmml" xref="S2.E1.m1.7.7.1.1.1"></eq><apply id="S2.E1.m1.7.7.1.1.2.cmml" xref="S2.E1.m1.7.7.1.1.2"><times id="S2.E1.m1.7.7.1.1.2.1.cmml" xref="S2.E1.m1.7.7.1.1.2.1"></times><ci id="S2.E1.m1.7.7.1.1.2.2.cmml" xref="S2.E1.m1.7.7.1.1.2.2">ℛ</ci><interval closure="open" id="S2.E1.m1.7.7.1.1.2.3.1.cmml" xref="S2.E1.m1.7.7.1.1.2.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝑞</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">𝑑</ci></interval></apply><apply id="S2.E1.m1.7.7.1.1.3.cmml" xref="S2.E1.m1.7.7.1.1.3"><apply id="S2.E1.m1.7.7.1.1.3.1.cmml" xref="S2.E1.m1.7.7.1.1.3.1"><csymbol cd="ambiguous" id="S2.E1.m1.7.7.1.1.3.1.1.cmml" xref="S2.E1.m1.7.7.1.1.3.1">subscript</csymbol><sum id="S2.E1.m1.7.7.1.1.3.1.2.cmml" xref="S2.E1.m1.7.7.1.1.3.1.2"></sum><apply id="S2.E1.m1.7.7.1.1.3.1.3.cmml" xref="S2.E1.m1.7.7.1.1.3.1.3"><in id="S2.E1.m1.7.7.1.1.3.1.3.1.cmml" xref="S2.E1.m1.7.7.1.1.3.1.3.1"></in><ci id="S2.E1.m1.7.7.1.1.3.1.3.2.cmml" xref="S2.E1.m1.7.7.1.1.3.1.3.2">𝑡</ci><apply id="S2.E1.m1.7.7.1.1.3.1.3.3.cmml" xref="S2.E1.m1.7.7.1.1.3.1.3.3"><intersect id="S2.E1.m1.7.7.1.1.3.1.3.3.1.cmml" xref="S2.E1.m1.7.7.1.1.3.1.3.3.1"></intersect><ci id="S2.E1.m1.7.7.1.1.3.1.3.3.2.cmml" xref="S2.E1.m1.7.7.1.1.3.1.3.3.2">𝑞</ci><ci id="S2.E1.m1.7.7.1.1.3.1.3.3.3.cmml" xref="S2.E1.m1.7.7.1.1.3.1.3.3.3">𝑑</ci></apply></apply></apply><apply id="S2.E1.m1.7.7.1.1.3.2.cmml" xref="S2.E1.m1.7.7.1.1.3.2"><times id="S2.E1.m1.7.7.1.1.3.2.1.cmml" xref="S2.E1.m1.7.7.1.1.3.2.1"></times><apply id="S2.E1.m1.7.7.1.1.3.2.2.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2"><ci id="S2.E1.m1.7.7.1.1.3.2.2.1.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2.1">⋅</ci><apply id="S2.E1.m1.7.7.1.1.3.2.2.2.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2.2"><times id="S2.E1.m1.7.7.1.1.3.2.2.2.1.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2.2.1"></times><ci id="S2.E1.m1.7.7.1.1.3.2.2.2.2a.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2.2.2"><mtext id="S2.E1.m1.7.7.1.1.3.2.2.2.2.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2.2.2">tf-idf</mtext></ci><interval closure="open" id="S2.E1.m1.7.7.1.1.3.2.2.2.3.1.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2.2.3.2"><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">𝑡</ci><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">𝑑</ci></interval></apply><ci id="S2.E1.m1.7.7.1.1.3.2.2.3a.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2.3"><mtext id="S2.E1.m1.7.7.1.1.3.2.2.3.cmml" xref="S2.E1.m1.7.7.1.1.3.2.2.3">tf-idf</mtext></ci></apply><interval closure="open" id="S2.E1.m1.7.7.1.1.3.2.3.1.cmml" xref="S2.E1.m1.7.7.1.1.3.2.3.2"><ci id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5">𝑡</ci><ci id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6">𝑞</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.7c">\mathcal{R}(q,d)=\sum_{t\in q\cap d}\text{tf-idf}(t,d)\cdot\text{tf-idf}(t,q),</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.7d">caligraphic_R ( italic_q , italic_d ) = ∑ start_POSTSUBSCRIPT italic_t ∈ italic_q ∩ italic_d end_POSTSUBSCRIPT tf-idf ( italic_t , italic_d ) ⋅ tf-idf ( italic_t , italic_q ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p2.15">where <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p2.8.m1.1"><semantics id="S2.SS1.p2.8.m1.1a"><mi id="S2.SS1.p2.8.m1.1.1" xref="S2.SS1.p2.8.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.8.m1.1b"><ci id="S2.SS1.p2.8.m1.1.1.cmml" xref="S2.SS1.p2.8.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.8.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.8.m1.1d">italic_t</annotation></semantics></math> represents the terms common to both query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p2.9.m2.1"><semantics id="S2.SS1.p2.9.m2.1a"><mi id="S2.SS1.p2.9.m2.1.1" xref="S2.SS1.p2.9.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.9.m2.1b"><ci id="S2.SS1.p2.9.m2.1.1.cmml" xref="S2.SS1.p2.9.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.9.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.9.m2.1d">italic_q</annotation></semantics></math> and document <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p2.10.m3.1"><semantics id="S2.SS1.p2.10.m3.1a"><mi id="S2.SS1.p2.10.m3.1.1" xref="S2.SS1.p2.10.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.10.m3.1b"><ci id="S2.SS1.p2.10.m3.1.1.cmml" xref="S2.SS1.p2.10.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.10.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.10.m3.1d">italic_d</annotation></semantics></math>, and <math alttext="\text{tf-idf}(t,d)" class="ltx_Math" display="inline" id="S2.SS1.p2.11.m4.2"><semantics id="S2.SS1.p2.11.m4.2a"><mrow id="S2.SS1.p2.11.m4.2.3" xref="S2.SS1.p2.11.m4.2.3.cmml"><mtext id="S2.SS1.p2.11.m4.2.3.2" xref="S2.SS1.p2.11.m4.2.3.2a.cmml">tf-idf</mtext><mo id="S2.SS1.p2.11.m4.2.3.1" xref="S2.SS1.p2.11.m4.2.3.1.cmml">⁢</mo><mrow id="S2.SS1.p2.11.m4.2.3.3.2" xref="S2.SS1.p2.11.m4.2.3.3.1.cmml"><mo id="S2.SS1.p2.11.m4.2.3.3.2.1" stretchy="false" xref="S2.SS1.p2.11.m4.2.3.3.1.cmml">(</mo><mi id="S2.SS1.p2.11.m4.1.1" xref="S2.SS1.p2.11.m4.1.1.cmml">t</mi><mo id="S2.SS1.p2.11.m4.2.3.3.2.2" xref="S2.SS1.p2.11.m4.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p2.11.m4.2.2" xref="S2.SS1.p2.11.m4.2.2.cmml">d</mi><mo id="S2.SS1.p2.11.m4.2.3.3.2.3" stretchy="false" xref="S2.SS1.p2.11.m4.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.11.m4.2b"><apply id="S2.SS1.p2.11.m4.2.3.cmml" xref="S2.SS1.p2.11.m4.2.3"><times id="S2.SS1.p2.11.m4.2.3.1.cmml" xref="S2.SS1.p2.11.m4.2.3.1"></times><ci id="S2.SS1.p2.11.m4.2.3.2a.cmml" xref="S2.SS1.p2.11.m4.2.3.2"><mtext id="S2.SS1.p2.11.m4.2.3.2.cmml" xref="S2.SS1.p2.11.m4.2.3.2">tf-idf</mtext></ci><interval closure="open" id="S2.SS1.p2.11.m4.2.3.3.1.cmml" xref="S2.SS1.p2.11.m4.2.3.3.2"><ci id="S2.SS1.p2.11.m4.1.1.cmml" xref="S2.SS1.p2.11.m4.1.1">𝑡</ci><ci id="S2.SS1.p2.11.m4.2.2.cmml" xref="S2.SS1.p2.11.m4.2.2">𝑑</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.11.m4.2c">\text{tf-idf}(t,d)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.11.m4.2d">tf-idf ( italic_t , italic_d )</annotation></semantics></math> and <math alttext="\text{tf-idf}(t,q)" class="ltx_Math" display="inline" id="S2.SS1.p2.12.m5.2"><semantics id="S2.SS1.p2.12.m5.2a"><mrow id="S2.SS1.p2.12.m5.2.3" xref="S2.SS1.p2.12.m5.2.3.cmml"><mtext id="S2.SS1.p2.12.m5.2.3.2" xref="S2.SS1.p2.12.m5.2.3.2a.cmml">tf-idf</mtext><mo id="S2.SS1.p2.12.m5.2.3.1" xref="S2.SS1.p2.12.m5.2.3.1.cmml">⁢</mo><mrow id="S2.SS1.p2.12.m5.2.3.3.2" xref="S2.SS1.p2.12.m5.2.3.3.1.cmml"><mo id="S2.SS1.p2.12.m5.2.3.3.2.1" stretchy="false" xref="S2.SS1.p2.12.m5.2.3.3.1.cmml">(</mo><mi id="S2.SS1.p2.12.m5.1.1" xref="S2.SS1.p2.12.m5.1.1.cmml">t</mi><mo id="S2.SS1.p2.12.m5.2.3.3.2.2" xref="S2.SS1.p2.12.m5.2.3.3.1.cmml">,</mo><mi id="S2.SS1.p2.12.m5.2.2" xref="S2.SS1.p2.12.m5.2.2.cmml">q</mi><mo id="S2.SS1.p2.12.m5.2.3.3.2.3" stretchy="false" xref="S2.SS1.p2.12.m5.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.12.m5.2b"><apply id="S2.SS1.p2.12.m5.2.3.cmml" xref="S2.SS1.p2.12.m5.2.3"><times id="S2.SS1.p2.12.m5.2.3.1.cmml" xref="S2.SS1.p2.12.m5.2.3.1"></times><ci id="S2.SS1.p2.12.m5.2.3.2a.cmml" xref="S2.SS1.p2.12.m5.2.3.2"><mtext id="S2.SS1.p2.12.m5.2.3.2.cmml" xref="S2.SS1.p2.12.m5.2.3.2">tf-idf</mtext></ci><interval closure="open" id="S2.SS1.p2.12.m5.2.3.3.1.cmml" xref="S2.SS1.p2.12.m5.2.3.3.2"><ci id="S2.SS1.p2.12.m5.1.1.cmml" xref="S2.SS1.p2.12.m5.1.1">𝑡</ci><ci id="S2.SS1.p2.12.m5.2.2.cmml" xref="S2.SS1.p2.12.m5.2.2">𝑞</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.12.m5.2c">\text{tf-idf}(t,q)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.12.m5.2d">tf-idf ( italic_t , italic_q )</annotation></semantics></math> represent the TF-IDF weights of term <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p2.13.m6.1"><semantics id="S2.SS1.p2.13.m6.1a"><mi id="S2.SS1.p2.13.m6.1.1" xref="S2.SS1.p2.13.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.13.m6.1b"><ci id="S2.SS1.p2.13.m6.1.1.cmml" xref="S2.SS1.p2.13.m6.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.13.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.13.m6.1d">italic_t</annotation></semantics></math> in document <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p2.14.m7.1"><semantics id="S2.SS1.p2.14.m7.1a"><mi id="S2.SS1.p2.14.m7.1.1" xref="S2.SS1.p2.14.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.14.m7.1b"><ci id="S2.SS1.p2.14.m7.1.1.cmml" xref="S2.SS1.p2.14.m7.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.14.m7.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.14.m7.1d">italic_d</annotation></semantics></math> and query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p2.15.m8.1"><semantics id="S2.SS1.p2.15.m8.1a"><mi id="S2.SS1.p2.15.m8.1.1" xref="S2.SS1.p2.15.m8.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.15.m8.1b"><ci id="S2.SS1.p2.15.m8.1.1.cmml" xref="S2.SS1.p2.15.m8.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.15.m8.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.15.m8.1d">italic_q</annotation></semantics></math>, respectively. Although sparse retrieval methods like TF-IDF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib187" title="">187</a>]</cite> and BM25 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib8" title="">8</a>]</cite> excel at fast retrieval, it struggles with complex queries involving synonyms, specialized terms, or context, as term matching and TF-IDF may not fully meet users’ information needs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib188" title="">188</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">Dense Retrieval.</span>

The advent of pre-trained language models like BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib13" title="">13</a>]</cite> has revolutionized information retrieval, leading to the development of dense retrieval methods, like DPR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib11" title="">11</a>]</cite>, ANCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib12" title="">12</a>]</cite>, E5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib189" title="">189</a>]</cite>, SimLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib190" title="">190</a>]</cite>. Unlike traditional sparse retrieval, these methods leverage Transformer-based encoders to create dense vector representations for both queries and documents. This approach enhances the capability to grasp the underlying semantics, thereby improving retrieval accuracy.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.9">The core of dense retrieval lies in converting documents and queries into vector representations. Given document <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p4.1.m1.1"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.1.m1.1d">italic_d</annotation></semantics></math> and query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p4.2.m2.1"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.2.m2.1d">italic_q</annotation></semantics></math> and their vector representations <math alttext="\mathbf{v}_{q}" class="ltx_Math" display="inline" id="S2.SS1.p4.3.m3.1"><semantics id="S2.SS1.p4.3.m3.1a"><msub id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml"><mi id="S2.SS1.p4.3.m3.1.1.2" xref="S2.SS1.p4.3.m3.1.1.2.cmml">𝐯</mi><mi id="S2.SS1.p4.3.m3.1.1.3" xref="S2.SS1.p4.3.m3.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><apply id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.3.m3.1.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p4.3.m3.1.1.2.cmml" xref="S2.SS1.p4.3.m3.1.1.2">𝐯</ci><ci id="S2.SS1.p4.3.m3.1.1.3.cmml" xref="S2.SS1.p4.3.m3.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">\mathbf{v}_{q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.3.m3.1d">bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math>, each document <math alttext="d" class="ltx_Math" display="inline" id="S2.SS1.p4.4.m4.1"><semantics id="S2.SS1.p4.4.m4.1a"><mi id="S2.SS1.p4.4.m4.1.1" xref="S2.SS1.p4.4.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.4.m4.1b"><ci id="S2.SS1.p4.4.m4.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.4.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.4.m4.1d">italic_d</annotation></semantics></math> is transformed into a dense vector <math alttext="\mathbf{v}_{d}" class="ltx_Math" display="inline" id="S2.SS1.p4.5.m5.1"><semantics id="S2.SS1.p4.5.m5.1a"><msub id="S2.SS1.p4.5.m5.1.1" xref="S2.SS1.p4.5.m5.1.1.cmml"><mi id="S2.SS1.p4.5.m5.1.1.2" xref="S2.SS1.p4.5.m5.1.1.2.cmml">𝐯</mi><mi id="S2.SS1.p4.5.m5.1.1.3" xref="S2.SS1.p4.5.m5.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.5.m5.1b"><apply id="S2.SS1.p4.5.m5.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.5.m5.1.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p4.5.m5.1.1.2.cmml" xref="S2.SS1.p4.5.m5.1.1.2">𝐯</ci><ci id="S2.SS1.p4.5.m5.1.1.3.cmml" xref="S2.SS1.p4.5.m5.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.5.m5.1c">\mathbf{v}_{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.5.m5.1d">bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> through a pre-trained language model, similarly, query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.p4.6.m6.1"><semantics id="S2.SS1.p4.6.m6.1a"><mi id="S2.SS1.p4.6.m6.1.1" xref="S2.SS1.p4.6.m6.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.6.m6.1b"><ci id="S2.SS1.p4.6.m6.1.1.cmml" xref="S2.SS1.p4.6.m6.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.6.m6.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.6.m6.1d">italic_q</annotation></semantics></math> is transformed into vector <math alttext="\mathbf{v}_{q}" class="ltx_Math" display="inline" id="S2.SS1.p4.7.m7.1"><semantics id="S2.SS1.p4.7.m7.1a"><msub id="S2.SS1.p4.7.m7.1.1" xref="S2.SS1.p4.7.m7.1.1.cmml"><mi id="S2.SS1.p4.7.m7.1.1.2" xref="S2.SS1.p4.7.m7.1.1.2.cmml">𝐯</mi><mi id="S2.SS1.p4.7.m7.1.1.3" xref="S2.SS1.p4.7.m7.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.7.m7.1b"><apply id="S2.SS1.p4.7.m7.1.1.cmml" xref="S2.SS1.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p4.7.m7.1.1.1.cmml" xref="S2.SS1.p4.7.m7.1.1">subscript</csymbol><ci id="S2.SS1.p4.7.m7.1.1.2.cmml" xref="S2.SS1.p4.7.m7.1.1.2">𝐯</ci><ci id="S2.SS1.p4.7.m7.1.1.3.cmml" xref="S2.SS1.p4.7.m7.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.7.m7.1c">\mathbf{v}_{q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.7.m7.1d">bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math>. Specifically, we can use encoder functions <math alttext="E_{d}(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p4.8.m8.1"><semantics id="S2.SS1.p4.8.m8.1a"><mrow id="S2.SS1.p4.8.m8.1.2" xref="S2.SS1.p4.8.m8.1.2.cmml"><msub id="S2.SS1.p4.8.m8.1.2.2" xref="S2.SS1.p4.8.m8.1.2.2.cmml"><mi id="S2.SS1.p4.8.m8.1.2.2.2" xref="S2.SS1.p4.8.m8.1.2.2.2.cmml">E</mi><mi id="S2.SS1.p4.8.m8.1.2.2.3" xref="S2.SS1.p4.8.m8.1.2.2.3.cmml">d</mi></msub><mo id="S2.SS1.p4.8.m8.1.2.1" xref="S2.SS1.p4.8.m8.1.2.1.cmml">⁢</mo><mrow id="S2.SS1.p4.8.m8.1.2.3.2" xref="S2.SS1.p4.8.m8.1.2.cmml"><mo id="S2.SS1.p4.8.m8.1.2.3.2.1" stretchy="false" xref="S2.SS1.p4.8.m8.1.2.cmml">(</mo><mo id="S2.SS1.p4.8.m8.1.1" lspace="0em" rspace="0em" xref="S2.SS1.p4.8.m8.1.1.cmml">⋅</mo><mo id="S2.SS1.p4.8.m8.1.2.3.2.2" stretchy="false" xref="S2.SS1.p4.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.8.m8.1b"><apply id="S2.SS1.p4.8.m8.1.2.cmml" xref="S2.SS1.p4.8.m8.1.2"><times id="S2.SS1.p4.8.m8.1.2.1.cmml" xref="S2.SS1.p4.8.m8.1.2.1"></times><apply id="S2.SS1.p4.8.m8.1.2.2.cmml" xref="S2.SS1.p4.8.m8.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p4.8.m8.1.2.2.1.cmml" xref="S2.SS1.p4.8.m8.1.2.2">subscript</csymbol><ci id="S2.SS1.p4.8.m8.1.2.2.2.cmml" xref="S2.SS1.p4.8.m8.1.2.2.2">𝐸</ci><ci id="S2.SS1.p4.8.m8.1.2.2.3.cmml" xref="S2.SS1.p4.8.m8.1.2.2.3">𝑑</ci></apply><ci id="S2.SS1.p4.8.m8.1.1.cmml" xref="S2.SS1.p4.8.m8.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.8.m8.1c">E_{d}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.8.m8.1d">italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> and <math alttext="E_{q}(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p4.9.m9.1"><semantics id="S2.SS1.p4.9.m9.1a"><mrow id="S2.SS1.p4.9.m9.1.2" xref="S2.SS1.p4.9.m9.1.2.cmml"><msub id="S2.SS1.p4.9.m9.1.2.2" xref="S2.SS1.p4.9.m9.1.2.2.cmml"><mi id="S2.SS1.p4.9.m9.1.2.2.2" xref="S2.SS1.p4.9.m9.1.2.2.2.cmml">E</mi><mi id="S2.SS1.p4.9.m9.1.2.2.3" xref="S2.SS1.p4.9.m9.1.2.2.3.cmml">q</mi></msub><mo id="S2.SS1.p4.9.m9.1.2.1" xref="S2.SS1.p4.9.m9.1.2.1.cmml">⁢</mo><mrow id="S2.SS1.p4.9.m9.1.2.3.2" xref="S2.SS1.p4.9.m9.1.2.cmml"><mo id="S2.SS1.p4.9.m9.1.2.3.2.1" stretchy="false" xref="S2.SS1.p4.9.m9.1.2.cmml">(</mo><mo id="S2.SS1.p4.9.m9.1.1" lspace="0em" rspace="0em" xref="S2.SS1.p4.9.m9.1.1.cmml">⋅</mo><mo id="S2.SS1.p4.9.m9.1.2.3.2.2" stretchy="false" xref="S2.SS1.p4.9.m9.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.9.m9.1b"><apply id="S2.SS1.p4.9.m9.1.2.cmml" xref="S2.SS1.p4.9.m9.1.2"><times id="S2.SS1.p4.9.m9.1.2.1.cmml" xref="S2.SS1.p4.9.m9.1.2.1"></times><apply id="S2.SS1.p4.9.m9.1.2.2.cmml" xref="S2.SS1.p4.9.m9.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p4.9.m9.1.2.2.1.cmml" xref="S2.SS1.p4.9.m9.1.2.2">subscript</csymbol><ci id="S2.SS1.p4.9.m9.1.2.2.2.cmml" xref="S2.SS1.p4.9.m9.1.2.2.2">𝐸</ci><ci id="S2.SS1.p4.9.m9.1.2.2.3.cmml" xref="S2.SS1.p4.9.m9.1.2.2.3">𝑞</ci></apply><ci id="S2.SS1.p4.9.m9.1.1.cmml" xref="S2.SS1.p4.9.m9.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.9.m9.1c">E_{q}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.9.m9.1d">italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> to represent the encoding process for documents and queries, respectively:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{v}_{d}=E_{d}(d),\quad\mathbf{v}_{q}=E_{q}(q)," class="ltx_Math" display="block" id="S2.E2.m1.3"><semantics id="S2.E2.m1.3a"><mrow id="S2.E2.m1.3.3.1"><mrow id="S2.E2.m1.3.3.1.1.2" xref="S2.E2.m1.3.3.1.1.3.cmml"><mrow id="S2.E2.m1.3.3.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.2" xref="S2.E2.m1.3.3.1.1.1.1.2.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.2.2" xref="S2.E2.m1.3.3.1.1.1.1.2.2.cmml">𝐯</mi><mi id="S2.E2.m1.3.3.1.1.1.1.2.3" xref="S2.E2.m1.3.3.1.1.1.1.2.3.cmml">d</mi></msub><mo id="S2.E2.m1.3.3.1.1.1.1.1" xref="S2.E2.m1.3.3.1.1.1.1.1.cmml">=</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.3" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml"><msub id="S2.E2.m1.3.3.1.1.1.1.3.2" xref="S2.E2.m1.3.3.1.1.1.1.3.2.cmml"><mi id="S2.E2.m1.3.3.1.1.1.1.3.2.2" xref="S2.E2.m1.3.3.1.1.1.1.3.2.2.cmml">E</mi><mi id="S2.E2.m1.3.3.1.1.1.1.3.2.3" xref="S2.E2.m1.3.3.1.1.1.1.3.2.3.cmml">d</mi></msub><mo id="S2.E2.m1.3.3.1.1.1.1.3.1" xref="S2.E2.m1.3.3.1.1.1.1.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.1.1.1.1.3.3.2" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml"><mo id="S2.E2.m1.3.3.1.1.1.1.3.3.2.1" stretchy="false" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">d</mi><mo id="S2.E2.m1.3.3.1.1.1.1.3.3.2.2" stretchy="false" xref="S2.E2.m1.3.3.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S2.E2.m1.3.3.1.1.2.3" rspace="1.167em" xref="S2.E2.m1.3.3.1.1.3a.cmml">,</mo><mrow id="S2.E2.m1.3.3.1.1.2.2" xref="S2.E2.m1.3.3.1.1.2.2.cmml"><msub id="S2.E2.m1.3.3.1.1.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.2.cmml"><mi id="S2.E2.m1.3.3.1.1.2.2.2.2" xref="S2.E2.m1.3.3.1.1.2.2.2.2.cmml">𝐯</mi><mi id="S2.E2.m1.3.3.1.1.2.2.2.3" xref="S2.E2.m1.3.3.1.1.2.2.2.3.cmml">q</mi></msub><mo id="S2.E2.m1.3.3.1.1.2.2.1" xref="S2.E2.m1.3.3.1.1.2.2.1.cmml">=</mo><mrow id="S2.E2.m1.3.3.1.1.2.2.3" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml"><msub id="S2.E2.m1.3.3.1.1.2.2.3.2" xref="S2.E2.m1.3.3.1.1.2.2.3.2.cmml"><mi id="S2.E2.m1.3.3.1.1.2.2.3.2.2" xref="S2.E2.m1.3.3.1.1.2.2.3.2.2.cmml">E</mi><mi id="S2.E2.m1.3.3.1.1.2.2.3.2.3" xref="S2.E2.m1.3.3.1.1.2.2.3.2.3.cmml">q</mi></msub><mo id="S2.E2.m1.3.3.1.1.2.2.3.1" xref="S2.E2.m1.3.3.1.1.2.2.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.3.3.1.1.2.2.3.3.2" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml"><mo id="S2.E2.m1.3.3.1.1.2.2.3.3.2.1" stretchy="false" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml">(</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">q</mi><mo id="S2.E2.m1.3.3.1.1.2.2.3.3.2.2" stretchy="false" xref="S2.E2.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E2.m1.3.3.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.3b"><apply id="S2.E2.m1.3.3.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.3a.cmml" xref="S2.E2.m1.3.3.1.1.2.3">formulae-sequence</csymbol><apply id="S2.E2.m1.3.3.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1"><eq id="S2.E2.m1.3.3.1.1.1.1.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.1"></eq><apply id="S2.E2.m1.3.3.1.1.1.1.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2.2">𝐯</ci><ci id="S2.E2.m1.3.3.1.1.1.1.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.2.3">𝑑</ci></apply><apply id="S2.E2.m1.3.3.1.1.1.1.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3"><times id="S2.E2.m1.3.3.1.1.1.1.3.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.1"></times><apply id="S2.E2.m1.3.3.1.1.1.1.3.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.1.1.3.2.1.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.1.1.3.2.2.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2.2">𝐸</ci><ci id="S2.E2.m1.3.3.1.1.1.1.3.2.3.cmml" xref="S2.E2.m1.3.3.1.1.1.1.3.2.3">𝑑</ci></apply><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝑑</ci></apply></apply><apply id="S2.E2.m1.3.3.1.1.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2"><eq id="S2.E2.m1.3.3.1.1.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.1"></eq><apply id="S2.E2.m1.3.3.1.1.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.2.2.2.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.2.2.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.2">𝐯</ci><ci id="S2.E2.m1.3.3.1.1.2.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.2.3">𝑞</ci></apply><apply id="S2.E2.m1.3.3.1.1.2.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.3"><times id="S2.E2.m1.3.3.1.1.2.2.3.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.3.1"></times><apply id="S2.E2.m1.3.3.1.1.2.2.3.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.3.3.1.1.2.2.3.2.1.cmml" xref="S2.E2.m1.3.3.1.1.2.2.3.2">subscript</csymbol><ci id="S2.E2.m1.3.3.1.1.2.2.3.2.2.cmml" xref="S2.E2.m1.3.3.1.1.2.2.3.2.2">𝐸</ci><ci id="S2.E2.m1.3.3.1.1.2.2.3.2.3.cmml" xref="S2.E2.m1.3.3.1.1.2.2.3.2.3">𝑞</ci></apply><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝑞</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.3c">\mathbf{v}_{d}=E_{d}(d),\quad\mathbf{v}_{q}=E_{q}(q),</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.3d">bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( italic_d ) , bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( italic_q ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p4.11">where <math alttext="E_{d}(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p4.10.m1.1"><semantics id="S2.SS1.p4.10.m1.1a"><mrow id="S2.SS1.p4.10.m1.1.2" xref="S2.SS1.p4.10.m1.1.2.cmml"><msub id="S2.SS1.p4.10.m1.1.2.2" xref="S2.SS1.p4.10.m1.1.2.2.cmml"><mi id="S2.SS1.p4.10.m1.1.2.2.2" xref="S2.SS1.p4.10.m1.1.2.2.2.cmml">E</mi><mi id="S2.SS1.p4.10.m1.1.2.2.3" xref="S2.SS1.p4.10.m1.1.2.2.3.cmml">d</mi></msub><mo id="S2.SS1.p4.10.m1.1.2.1" xref="S2.SS1.p4.10.m1.1.2.1.cmml">⁢</mo><mrow id="S2.SS1.p4.10.m1.1.2.3.2" xref="S2.SS1.p4.10.m1.1.2.cmml"><mo id="S2.SS1.p4.10.m1.1.2.3.2.1" stretchy="false" xref="S2.SS1.p4.10.m1.1.2.cmml">(</mo><mo id="S2.SS1.p4.10.m1.1.1" lspace="0em" rspace="0em" xref="S2.SS1.p4.10.m1.1.1.cmml">⋅</mo><mo id="S2.SS1.p4.10.m1.1.2.3.2.2" stretchy="false" xref="S2.SS1.p4.10.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.10.m1.1b"><apply id="S2.SS1.p4.10.m1.1.2.cmml" xref="S2.SS1.p4.10.m1.1.2"><times id="S2.SS1.p4.10.m1.1.2.1.cmml" xref="S2.SS1.p4.10.m1.1.2.1"></times><apply id="S2.SS1.p4.10.m1.1.2.2.cmml" xref="S2.SS1.p4.10.m1.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p4.10.m1.1.2.2.1.cmml" xref="S2.SS1.p4.10.m1.1.2.2">subscript</csymbol><ci id="S2.SS1.p4.10.m1.1.2.2.2.cmml" xref="S2.SS1.p4.10.m1.1.2.2.2">𝐸</ci><ci id="S2.SS1.p4.10.m1.1.2.2.3.cmml" xref="S2.SS1.p4.10.m1.1.2.2.3">𝑑</ci></apply><ci id="S2.SS1.p4.10.m1.1.1.cmml" xref="S2.SS1.p4.10.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.10.m1.1c">E_{d}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.10.m1.1d">italic_E start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> and <math alttext="E_{q}(\cdot)" class="ltx_Math" display="inline" id="S2.SS1.p4.11.m2.1"><semantics id="S2.SS1.p4.11.m2.1a"><mrow id="S2.SS1.p4.11.m2.1.2" xref="S2.SS1.p4.11.m2.1.2.cmml"><msub id="S2.SS1.p4.11.m2.1.2.2" xref="S2.SS1.p4.11.m2.1.2.2.cmml"><mi id="S2.SS1.p4.11.m2.1.2.2.2" xref="S2.SS1.p4.11.m2.1.2.2.2.cmml">E</mi><mi id="S2.SS1.p4.11.m2.1.2.2.3" xref="S2.SS1.p4.11.m2.1.2.2.3.cmml">q</mi></msub><mo id="S2.SS1.p4.11.m2.1.2.1" xref="S2.SS1.p4.11.m2.1.2.1.cmml">⁢</mo><mrow id="S2.SS1.p4.11.m2.1.2.3.2" xref="S2.SS1.p4.11.m2.1.2.cmml"><mo id="S2.SS1.p4.11.m2.1.2.3.2.1" stretchy="false" xref="S2.SS1.p4.11.m2.1.2.cmml">(</mo><mo id="S2.SS1.p4.11.m2.1.1" lspace="0em" rspace="0em" xref="S2.SS1.p4.11.m2.1.1.cmml">⋅</mo><mo id="S2.SS1.p4.11.m2.1.2.3.2.2" stretchy="false" xref="S2.SS1.p4.11.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.11.m2.1b"><apply id="S2.SS1.p4.11.m2.1.2.cmml" xref="S2.SS1.p4.11.m2.1.2"><times id="S2.SS1.p4.11.m2.1.2.1.cmml" xref="S2.SS1.p4.11.m2.1.2.1"></times><apply id="S2.SS1.p4.11.m2.1.2.2.cmml" xref="S2.SS1.p4.11.m2.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p4.11.m2.1.2.2.1.cmml" xref="S2.SS1.p4.11.m2.1.2.2">subscript</csymbol><ci id="S2.SS1.p4.11.m2.1.2.2.2.cmml" xref="S2.SS1.p4.11.m2.1.2.2.2">𝐸</ci><ci id="S2.SS1.p4.11.m2.1.2.2.3.cmml" xref="S2.SS1.p4.11.m2.1.2.2.3">𝑞</ci></apply><ci id="S2.SS1.p4.11.m2.1.1.cmml" xref="S2.SS1.p4.11.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.11.m2.1c">E_{q}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.11.m2.1d">italic_E start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> can be the same model or different models optimized for specific tasks.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.6">Dense retrieval methods evaluate relevance by calculating the similarity between the query vector and document vector, which can be measured by cosine similarity, expressed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}(q,d)=\text{cos}(\mathbf{v}_{q},\mathbf{v}_{d})=\frac{\mathbf{v}_{q%
}\cdot\mathbf{v}_{d}}{|\mathbf{v}_{q}||\mathbf{v}_{d}|}," class="ltx_Math" display="block" id="S2.E3.m1.5"><semantics id="S2.E3.m1.5a"><mrow id="S2.E3.m1.5.5.1" xref="S2.E3.m1.5.5.1.1.cmml"><mrow id="S2.E3.m1.5.5.1.1" xref="S2.E3.m1.5.5.1.1.cmml"><mrow id="S2.E3.m1.5.5.1.1.4" xref="S2.E3.m1.5.5.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E3.m1.5.5.1.1.4.2" xref="S2.E3.m1.5.5.1.1.4.2.cmml">ℛ</mi><mo id="S2.E3.m1.5.5.1.1.4.1" xref="S2.E3.m1.5.5.1.1.4.1.cmml">⁢</mo><mrow id="S2.E3.m1.5.5.1.1.4.3.2" xref="S2.E3.m1.5.5.1.1.4.3.1.cmml"><mo id="S2.E3.m1.5.5.1.1.4.3.2.1" stretchy="false" xref="S2.E3.m1.5.5.1.1.4.3.1.cmml">(</mo><mi id="S2.E3.m1.3.3" xref="S2.E3.m1.3.3.cmml">q</mi><mo id="S2.E3.m1.5.5.1.1.4.3.2.2" xref="S2.E3.m1.5.5.1.1.4.3.1.cmml">,</mo><mi id="S2.E3.m1.4.4" xref="S2.E3.m1.4.4.cmml">d</mi><mo id="S2.E3.m1.5.5.1.1.4.3.2.3" stretchy="false" xref="S2.E3.m1.5.5.1.1.4.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.5.5.1.1.5" xref="S2.E3.m1.5.5.1.1.5.cmml">=</mo><mrow id="S2.E3.m1.5.5.1.1.2" xref="S2.E3.m1.5.5.1.1.2.cmml"><mtext id="S2.E3.m1.5.5.1.1.2.4" xref="S2.E3.m1.5.5.1.1.2.4a.cmml">cos</mtext><mo id="S2.E3.m1.5.5.1.1.2.3" xref="S2.E3.m1.5.5.1.1.2.3.cmml">⁢</mo><mrow id="S2.E3.m1.5.5.1.1.2.2.2" xref="S2.E3.m1.5.5.1.1.2.2.3.cmml"><mo id="S2.E3.m1.5.5.1.1.2.2.2.3" stretchy="false" xref="S2.E3.m1.5.5.1.1.2.2.3.cmml">(</mo><msub id="S2.E3.m1.5.5.1.1.1.1.1.1" xref="S2.E3.m1.5.5.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.5.5.1.1.1.1.1.1.2" xref="S2.E3.m1.5.5.1.1.1.1.1.1.2.cmml">𝐯</mi><mi id="S2.E3.m1.5.5.1.1.1.1.1.1.3" xref="S2.E3.m1.5.5.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S2.E3.m1.5.5.1.1.2.2.2.4" xref="S2.E3.m1.5.5.1.1.2.2.3.cmml">,</mo><msub id="S2.E3.m1.5.5.1.1.2.2.2.2" xref="S2.E3.m1.5.5.1.1.2.2.2.2.cmml"><mi id="S2.E3.m1.5.5.1.1.2.2.2.2.2" xref="S2.E3.m1.5.5.1.1.2.2.2.2.2.cmml">𝐯</mi><mi id="S2.E3.m1.5.5.1.1.2.2.2.2.3" xref="S2.E3.m1.5.5.1.1.2.2.2.2.3.cmml">d</mi></msub><mo id="S2.E3.m1.5.5.1.1.2.2.2.5" stretchy="false" xref="S2.E3.m1.5.5.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.5.5.1.1.6" xref="S2.E3.m1.5.5.1.1.6.cmml">=</mo><mfrac id="S2.E3.m1.2.2" xref="S2.E3.m1.2.2.cmml"><mrow id="S2.E3.m1.2.2.4" xref="S2.E3.m1.2.2.4.cmml"><msub id="S2.E3.m1.2.2.4.2" xref="S2.E3.m1.2.2.4.2.cmml"><mi id="S2.E3.m1.2.2.4.2.2" xref="S2.E3.m1.2.2.4.2.2.cmml">𝐯</mi><mi id="S2.E3.m1.2.2.4.2.3" xref="S2.E3.m1.2.2.4.2.3.cmml">q</mi></msub><mo id="S2.E3.m1.2.2.4.1" lspace="0.222em" rspace="0.222em" xref="S2.E3.m1.2.2.4.1.cmml">⋅</mo><msub id="S2.E3.m1.2.2.4.3" xref="S2.E3.m1.2.2.4.3.cmml"><mi id="S2.E3.m1.2.2.4.3.2" xref="S2.E3.m1.2.2.4.3.2.cmml">𝐯</mi><mi id="S2.E3.m1.2.2.4.3.3" xref="S2.E3.m1.2.2.4.3.3.cmml">d</mi></msub></mrow><mrow id="S2.E3.m1.2.2.2" xref="S2.E3.m1.2.2.2.cmml"><mrow id="S2.E3.m1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.2.cmml"><mo id="S2.E3.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E3.m1.1.1.1.1.2.1.cmml">|</mo><msub id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2" xref="S2.E3.m1.1.1.1.1.1.1.2.cmml">𝐯</mi><mi id="S2.E3.m1.1.1.1.1.1.1.3" xref="S2.E3.m1.1.1.1.1.1.1.3.cmml">q</mi></msub><mo id="S2.E3.m1.1.1.1.1.1.3" stretchy="false" xref="S2.E3.m1.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="S2.E3.m1.2.2.2.3" xref="S2.E3.m1.2.2.2.3.cmml">⁢</mo><mrow id="S2.E3.m1.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.cmml"><mo id="S2.E3.m1.2.2.2.2.1.2" stretchy="false" xref="S2.E3.m1.2.2.2.2.2.1.cmml">|</mo><msub id="S2.E3.m1.2.2.2.2.1.1" xref="S2.E3.m1.2.2.2.2.1.1.cmml"><mi id="S2.E3.m1.2.2.2.2.1.1.2" xref="S2.E3.m1.2.2.2.2.1.1.2.cmml">𝐯</mi><mi id="S2.E3.m1.2.2.2.2.1.1.3" xref="S2.E3.m1.2.2.2.2.1.1.3.cmml">d</mi></msub><mo id="S2.E3.m1.2.2.2.2.1.3" stretchy="false" xref="S2.E3.m1.2.2.2.2.2.1.cmml">|</mo></mrow></mrow></mfrac></mrow><mo id="S2.E3.m1.5.5.1.2" xref="S2.E3.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.5b"><apply id="S2.E3.m1.5.5.1.1.cmml" xref="S2.E3.m1.5.5.1"><and id="S2.E3.m1.5.5.1.1a.cmml" xref="S2.E3.m1.5.5.1"></and><apply id="S2.E3.m1.5.5.1.1b.cmml" xref="S2.E3.m1.5.5.1"><eq id="S2.E3.m1.5.5.1.1.5.cmml" xref="S2.E3.m1.5.5.1.1.5"></eq><apply id="S2.E3.m1.5.5.1.1.4.cmml" xref="S2.E3.m1.5.5.1.1.4"><times id="S2.E3.m1.5.5.1.1.4.1.cmml" xref="S2.E3.m1.5.5.1.1.4.1"></times><ci id="S2.E3.m1.5.5.1.1.4.2.cmml" xref="S2.E3.m1.5.5.1.1.4.2">ℛ</ci><interval closure="open" id="S2.E3.m1.5.5.1.1.4.3.1.cmml" xref="S2.E3.m1.5.5.1.1.4.3.2"><ci id="S2.E3.m1.3.3.cmml" xref="S2.E3.m1.3.3">𝑞</ci><ci id="S2.E3.m1.4.4.cmml" xref="S2.E3.m1.4.4">𝑑</ci></interval></apply><apply id="S2.E3.m1.5.5.1.1.2.cmml" xref="S2.E3.m1.5.5.1.1.2"><times id="S2.E3.m1.5.5.1.1.2.3.cmml" xref="S2.E3.m1.5.5.1.1.2.3"></times><ci id="S2.E3.m1.5.5.1.1.2.4a.cmml" xref="S2.E3.m1.5.5.1.1.2.4"><mtext id="S2.E3.m1.5.5.1.1.2.4.cmml" xref="S2.E3.m1.5.5.1.1.2.4">cos</mtext></ci><interval closure="open" id="S2.E3.m1.5.5.1.1.2.2.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2"><apply id="S2.E3.m1.5.5.1.1.1.1.1.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.5.5.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.5.5.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.5.5.1.1.1.1.1.1.2">𝐯</ci><ci id="S2.E3.m1.5.5.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.5.5.1.1.1.1.1.1.3">𝑞</ci></apply><apply id="S2.E3.m1.5.5.1.1.2.2.2.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.5.5.1.1.2.2.2.2.1.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E3.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.2.2">𝐯</ci><ci id="S2.E3.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S2.E3.m1.5.5.1.1.2.2.2.2.3">𝑑</ci></apply></interval></apply></apply><apply id="S2.E3.m1.5.5.1.1c.cmml" xref="S2.E3.m1.5.5.1"><eq id="S2.E3.m1.5.5.1.1.6.cmml" xref="S2.E3.m1.5.5.1.1.6"></eq><share href="https://arxiv.org/html/2404.14851v3#S2.E3.m1.5.5.1.1.2.cmml" id="S2.E3.m1.5.5.1.1d.cmml" xref="S2.E3.m1.5.5.1"></share><apply id="S2.E3.m1.2.2.cmml" xref="S2.E3.m1.2.2"><divide id="S2.E3.m1.2.2.3.cmml" xref="S2.E3.m1.2.2"></divide><apply id="S2.E3.m1.2.2.4.cmml" xref="S2.E3.m1.2.2.4"><ci id="S2.E3.m1.2.2.4.1.cmml" xref="S2.E3.m1.2.2.4.1">⋅</ci><apply id="S2.E3.m1.2.2.4.2.cmml" xref="S2.E3.m1.2.2.4.2"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.4.2.1.cmml" xref="S2.E3.m1.2.2.4.2">subscript</csymbol><ci id="S2.E3.m1.2.2.4.2.2.cmml" xref="S2.E3.m1.2.2.4.2.2">𝐯</ci><ci id="S2.E3.m1.2.2.4.2.3.cmml" xref="S2.E3.m1.2.2.4.2.3">𝑞</ci></apply><apply id="S2.E3.m1.2.2.4.3.cmml" xref="S2.E3.m1.2.2.4.3"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.4.3.1.cmml" xref="S2.E3.m1.2.2.4.3">subscript</csymbol><ci id="S2.E3.m1.2.2.4.3.2.cmml" xref="S2.E3.m1.2.2.4.3.2">𝐯</ci><ci id="S2.E3.m1.2.2.4.3.3.cmml" xref="S2.E3.m1.2.2.4.3.3">𝑑</ci></apply></apply><apply id="S2.E3.m1.2.2.2.cmml" xref="S2.E3.m1.2.2.2"><times id="S2.E3.m1.2.2.2.3.cmml" xref="S2.E3.m1.2.2.2.3"></times><apply id="S2.E3.m1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1"><abs id="S2.E3.m1.1.1.1.1.2.1.cmml" xref="S2.E3.m1.1.1.1.1.1.2"></abs><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E3.m1.1.1.1.1.1.1.2.cmml" xref="S2.E3.m1.1.1.1.1.1.1.2">𝐯</ci><ci id="S2.E3.m1.1.1.1.1.1.1.3.cmml" xref="S2.E3.m1.1.1.1.1.1.1.3">𝑞</ci></apply></apply><apply id="S2.E3.m1.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.1"><abs id="S2.E3.m1.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.1.2"></abs><apply id="S2.E3.m1.2.2.2.2.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S2.E3.m1.2.2.2.2.1.1.1.cmml" xref="S2.E3.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S2.E3.m1.2.2.2.2.1.1.2.cmml" xref="S2.E3.m1.2.2.2.2.1.1.2">𝐯</ci><ci id="S2.E3.m1.2.2.2.2.1.1.3.cmml" xref="S2.E3.m1.2.2.2.2.1.1.3">𝑑</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.5c">\mathcal{R}(q,d)=\text{cos}(\mathbf{v}_{q},\mathbf{v}_{d})=\frac{\mathbf{v}_{q%
}\cdot\mathbf{v}_{d}}{|\mathbf{v}_{q}||\mathbf{v}_{d}|},</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.5d">caligraphic_R ( italic_q , italic_d ) = cos ( bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT , bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) = divide start_ARG bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ⋅ bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_ARG start_ARG | bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT | | bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT | end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS1.p5.5">where <math alttext="\mathbf{v}_{q}\cdot\mathbf{v}_{d}" class="ltx_Math" display="inline" id="S2.SS1.p5.1.m1.1"><semantics id="S2.SS1.p5.1.m1.1a"><mrow id="S2.SS1.p5.1.m1.1.1" xref="S2.SS1.p5.1.m1.1.1.cmml"><msub id="S2.SS1.p5.1.m1.1.1.2" xref="S2.SS1.p5.1.m1.1.1.2.cmml"><mi id="S2.SS1.p5.1.m1.1.1.2.2" xref="S2.SS1.p5.1.m1.1.1.2.2.cmml">𝐯</mi><mi id="S2.SS1.p5.1.m1.1.1.2.3" xref="S2.SS1.p5.1.m1.1.1.2.3.cmml">q</mi></msub><mo id="S2.SS1.p5.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p5.1.m1.1.1.1.cmml">⋅</mo><msub id="S2.SS1.p5.1.m1.1.1.3" xref="S2.SS1.p5.1.m1.1.1.3.cmml"><mi id="S2.SS1.p5.1.m1.1.1.3.2" xref="S2.SS1.p5.1.m1.1.1.3.2.cmml">𝐯</mi><mi id="S2.SS1.p5.1.m1.1.1.3.3" xref="S2.SS1.p5.1.m1.1.1.3.3.cmml">d</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.1.m1.1b"><apply id="S2.SS1.p5.1.m1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1"><ci id="S2.SS1.p5.1.m1.1.1.1.cmml" xref="S2.SS1.p5.1.m1.1.1.1">⋅</ci><apply id="S2.SS1.p5.1.m1.1.1.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p5.1.m1.1.1.2.1.cmml" xref="S2.SS1.p5.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS1.p5.1.m1.1.1.2.2.cmml" xref="S2.SS1.p5.1.m1.1.1.2.2">𝐯</ci><ci id="S2.SS1.p5.1.m1.1.1.2.3.cmml" xref="S2.SS1.p5.1.m1.1.1.2.3">𝑞</ci></apply><apply id="S2.SS1.p5.1.m1.1.1.3.cmml" xref="S2.SS1.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p5.1.m1.1.1.3.1.cmml" xref="S2.SS1.p5.1.m1.1.1.3">subscript</csymbol><ci id="S2.SS1.p5.1.m1.1.1.3.2.cmml" xref="S2.SS1.p5.1.m1.1.1.3.2">𝐯</ci><ci id="S2.SS1.p5.1.m1.1.1.3.3.cmml" xref="S2.SS1.p5.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.1.m1.1c">\mathbf{v}_{q}\cdot\mathbf{v}_{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.1.m1.1d">bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT ⋅ bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math> represents the dot product of query vector <math alttext="\mathbf{v}_{q}" class="ltx_Math" display="inline" id="S2.SS1.p5.2.m2.1"><semantics id="S2.SS1.p5.2.m2.1a"><msub id="S2.SS1.p5.2.m2.1.1" xref="S2.SS1.p5.2.m2.1.1.cmml"><mi id="S2.SS1.p5.2.m2.1.1.2" xref="S2.SS1.p5.2.m2.1.1.2.cmml">𝐯</mi><mi id="S2.SS1.p5.2.m2.1.1.3" xref="S2.SS1.p5.2.m2.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.2.m2.1b"><apply id="S2.SS1.p5.2.m2.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.2.m2.1.1.1.cmml" xref="S2.SS1.p5.2.m2.1.1">subscript</csymbol><ci id="S2.SS1.p5.2.m2.1.1.2.cmml" xref="S2.SS1.p5.2.m2.1.1.2">𝐯</ci><ci id="S2.SS1.p5.2.m2.1.1.3.cmml" xref="S2.SS1.p5.2.m2.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.2.m2.1c">\mathbf{v}_{q}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.2.m2.1d">bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> and document vector <math alttext="\mathbf{v}_{d}" class="ltx_Math" display="inline" id="S2.SS1.p5.3.m3.1"><semantics id="S2.SS1.p5.3.m3.1a"><msub id="S2.SS1.p5.3.m3.1.1" xref="S2.SS1.p5.3.m3.1.1.cmml"><mi id="S2.SS1.p5.3.m3.1.1.2" xref="S2.SS1.p5.3.m3.1.1.2.cmml">𝐯</mi><mi id="S2.SS1.p5.3.m3.1.1.3" xref="S2.SS1.p5.3.m3.1.1.3.cmml">d</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.3.m3.1b"><apply id="S2.SS1.p5.3.m3.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.3.m3.1.1.1.cmml" xref="S2.SS1.p5.3.m3.1.1">subscript</csymbol><ci id="S2.SS1.p5.3.m3.1.1.2.cmml" xref="S2.SS1.p5.3.m3.1.1.2">𝐯</ci><ci id="S2.SS1.p5.3.m3.1.1.3.cmml" xref="S2.SS1.p5.3.m3.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.3.m3.1c">\mathbf{v}_{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.3.m3.1d">bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="|\mathbf{v}_{q}|" class="ltx_Math" display="inline" id="S2.SS1.p5.4.m4.1"><semantics id="S2.SS1.p5.4.m4.1a"><mrow id="S2.SS1.p5.4.m4.1.1.1" xref="S2.SS1.p5.4.m4.1.1.2.cmml"><mo id="S2.SS1.p5.4.m4.1.1.1.2" stretchy="false" xref="S2.SS1.p5.4.m4.1.1.2.1.cmml">|</mo><msub id="S2.SS1.p5.4.m4.1.1.1.1" xref="S2.SS1.p5.4.m4.1.1.1.1.cmml"><mi id="S2.SS1.p5.4.m4.1.1.1.1.2" xref="S2.SS1.p5.4.m4.1.1.1.1.2.cmml">𝐯</mi><mi id="S2.SS1.p5.4.m4.1.1.1.1.3" xref="S2.SS1.p5.4.m4.1.1.1.1.3.cmml">q</mi></msub><mo id="S2.SS1.p5.4.m4.1.1.1.3" stretchy="false" xref="S2.SS1.p5.4.m4.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.4.m4.1b"><apply id="S2.SS1.p5.4.m4.1.1.2.cmml" xref="S2.SS1.p5.4.m4.1.1.1"><abs id="S2.SS1.p5.4.m4.1.1.2.1.cmml" xref="S2.SS1.p5.4.m4.1.1.1.2"></abs><apply id="S2.SS1.p5.4.m4.1.1.1.1.cmml" xref="S2.SS1.p5.4.m4.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.4.m4.1.1.1.1.1.cmml" xref="S2.SS1.p5.4.m4.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p5.4.m4.1.1.1.1.2.cmml" xref="S2.SS1.p5.4.m4.1.1.1.1.2">𝐯</ci><ci id="S2.SS1.p5.4.m4.1.1.1.1.3.cmml" xref="S2.SS1.p5.4.m4.1.1.1.1.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.4.m4.1c">|\mathbf{v}_{q}|</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.4.m4.1d">| bold_v start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT |</annotation></semantics></math> and <math alttext="|\mathbf{v}_{d}|" class="ltx_Math" display="inline" id="S2.SS1.p5.5.m5.1"><semantics id="S2.SS1.p5.5.m5.1a"><mrow id="S2.SS1.p5.5.m5.1.1.1" xref="S2.SS1.p5.5.m5.1.1.2.cmml"><mo id="S2.SS1.p5.5.m5.1.1.1.2" stretchy="false" xref="S2.SS1.p5.5.m5.1.1.2.1.cmml">|</mo><msub id="S2.SS1.p5.5.m5.1.1.1.1" xref="S2.SS1.p5.5.m5.1.1.1.1.cmml"><mi id="S2.SS1.p5.5.m5.1.1.1.1.2" xref="S2.SS1.p5.5.m5.1.1.1.1.2.cmml">𝐯</mi><mi id="S2.SS1.p5.5.m5.1.1.1.1.3" xref="S2.SS1.p5.5.m5.1.1.1.1.3.cmml">d</mi></msub><mo id="S2.SS1.p5.5.m5.1.1.1.3" stretchy="false" xref="S2.SS1.p5.5.m5.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p5.5.m5.1b"><apply id="S2.SS1.p5.5.m5.1.1.2.cmml" xref="S2.SS1.p5.5.m5.1.1.1"><abs id="S2.SS1.p5.5.m5.1.1.2.1.cmml" xref="S2.SS1.p5.5.m5.1.1.1.2"></abs><apply id="S2.SS1.p5.5.m5.1.1.1.1.cmml" xref="S2.SS1.p5.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p5.5.m5.1.1.1.1.1.cmml" xref="S2.SS1.p5.5.m5.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p5.5.m5.1.1.1.1.2.cmml" xref="S2.SS1.p5.5.m5.1.1.1.1.2">𝐯</ci><ci id="S2.SS1.p5.5.m5.1.1.1.1.3.cmml" xref="S2.SS1.p5.5.m5.1.1.1.1.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p5.5.m5.1c">|\mathbf{v}_{d}|</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p5.5.m5.1d">| bold_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT |</annotation></semantics></math> respectively represent the magnitudes of the query and document vector. Finally, documents are ranked based on these similarity scores to identify the most relevant ones for the user.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span class="ltx_text ltx_font_italic" id="S2.SS2.1.1">Generative Retrieval</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">With significant progress of language models, generative retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite> has emerged as a new direction in the field of information retrieval. Unlike traditional index-based retrieval methods, generative retrieval relies on pre-trained generative models, such as the T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib17" title="">17</a>]</cite> and BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib18" title="">18</a>]</cite>, to directly generate document identifiers (DocIDs) related to the query, thereby achieving end-to-end retrieval without relying on large-scale pre-built document indices.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.5">Formally, given a query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS2.p2.1.m1.1"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.m1.1d">italic_q</annotation></semantics></math>, a generative retrieval model aims to directly generate the document identifier <math alttext="d^{\prime}" class="ltx_Math" display="inline" id="S2.SS2.p2.2.m2.1"><semantics id="S2.SS2.p2.2.m2.1a"><msup id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml"><mi id="S2.SS2.p2.2.m2.1.1.2" xref="S2.SS2.p2.2.m2.1.1.2.cmml">d</mi><mo id="S2.SS2.p2.2.m2.1.1.3" xref="S2.SS2.p2.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><apply id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.2.m2.1.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="S2.SS2.p2.2.m2.1.1.2.cmml" xref="S2.SS2.p2.2.m2.1.1.2">𝑑</ci><ci id="S2.SS2.p2.2.m2.1.1.3.cmml" xref="S2.SS2.p2.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">d^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.2.m2.1d">italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> related to <math alttext="q" class="ltx_Math" display="inline" id="S2.SS2.p2.3.m3.1"><semantics id="S2.SS2.p2.3.m3.1a"><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.3.m3.1d">italic_q</annotation></semantics></math>. The model can be represented as a conditional probability distribution, where the probability of generating document identifier <math alttext="d^{\prime}" class="ltx_Math" display="inline" id="S2.SS2.p2.4.m4.1"><semantics id="S2.SS2.p2.4.m4.1a"><msup id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml"><mi id="S2.SS2.p2.4.m4.1.1.2" xref="S2.SS2.p2.4.m4.1.1.2.cmml">d</mi><mo id="S2.SS2.p2.4.m4.1.1.3" xref="S2.SS2.p2.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><apply id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.4.m4.1.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">superscript</csymbol><ci id="S2.SS2.p2.4.m4.1.1.2.cmml" xref="S2.SS2.p2.4.m4.1.1.2">𝑑</ci><ci id="S2.SS2.p2.4.m4.1.1.3.cmml" xref="S2.SS2.p2.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">d^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.4.m4.1d">italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> given query <math alttext="q" class="ltx_Math" display="inline" id="S2.SS2.p2.5.m5.1"><semantics id="S2.SS2.p2.5.m5.1a"><mi id="S2.SS2.p2.5.m5.1.1" xref="S2.SS2.p2.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.5.m5.1b"><ci id="S2.SS2.p2.5.m5.1.1.cmml" xref="S2.SS2.p2.5.m5.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.5.m5.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.5.m5.1d">italic_q</annotation></semantics></math> can be expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{R}(q,d)=P(d^{\prime}|q;\theta)=\prod_{i=1}^{T}P(d^{\prime}_{i}|d^{%
\prime}_{&lt;i},q;\theta)," class="ltx_Math" display="block" id="S2.E4.m1.7"><semantics id="S2.E4.m1.7a"><mrow id="S2.E4.m1.7.7.1" xref="S2.E4.m1.7.7.1.1.cmml"><mrow id="S2.E4.m1.7.7.1.1" xref="S2.E4.m1.7.7.1.1.cmml"><mrow id="S2.E4.m1.7.7.1.1.4" xref="S2.E4.m1.7.7.1.1.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E4.m1.7.7.1.1.4.2" xref="S2.E4.m1.7.7.1.1.4.2.cmml">ℛ</mi><mo id="S2.E4.m1.7.7.1.1.4.1" xref="S2.E4.m1.7.7.1.1.4.1.cmml">⁢</mo><mrow id="S2.E4.m1.7.7.1.1.4.3.2" xref="S2.E4.m1.7.7.1.1.4.3.1.cmml"><mo id="S2.E4.m1.7.7.1.1.4.3.2.1" stretchy="false" xref="S2.E4.m1.7.7.1.1.4.3.1.cmml">(</mo><mi id="S2.E4.m1.1.1" xref="S2.E4.m1.1.1.cmml">q</mi><mo id="S2.E4.m1.7.7.1.1.4.3.2.2" xref="S2.E4.m1.7.7.1.1.4.3.1.cmml">,</mo><mi id="S2.E4.m1.2.2" xref="S2.E4.m1.2.2.cmml">d</mi><mo id="S2.E4.m1.7.7.1.1.4.3.2.3" stretchy="false" xref="S2.E4.m1.7.7.1.1.4.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.7.7.1.1.5" xref="S2.E4.m1.7.7.1.1.5.cmml">=</mo><mrow id="S2.E4.m1.7.7.1.1.1" xref="S2.E4.m1.7.7.1.1.1.cmml"><mi id="S2.E4.m1.7.7.1.1.1.3" xref="S2.E4.m1.7.7.1.1.1.3.cmml">P</mi><mo id="S2.E4.m1.7.7.1.1.1.2" xref="S2.E4.m1.7.7.1.1.1.2.cmml">⁢</mo><mrow id="S2.E4.m1.7.7.1.1.1.1.1" xref="S2.E4.m1.7.7.1.1.1.1.1.1.cmml"><mo id="S2.E4.m1.7.7.1.1.1.1.1.2" stretchy="false" xref="S2.E4.m1.7.7.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.7.7.1.1.1.1.1.1" xref="S2.E4.m1.7.7.1.1.1.1.1.1.cmml"><msup id="S2.E4.m1.7.7.1.1.1.1.1.1.2" xref="S2.E4.m1.7.7.1.1.1.1.1.1.2.cmml"><mi id="S2.E4.m1.7.7.1.1.1.1.1.1.2.2" xref="S2.E4.m1.7.7.1.1.1.1.1.1.2.2.cmml">d</mi><mo id="S2.E4.m1.7.7.1.1.1.1.1.1.2.3" xref="S2.E4.m1.7.7.1.1.1.1.1.1.2.3.cmml">′</mo></msup><mo fence="false" id="S2.E4.m1.7.7.1.1.1.1.1.1.1" xref="S2.E4.m1.7.7.1.1.1.1.1.1.1.cmml">|</mo><mrow id="S2.E4.m1.7.7.1.1.1.1.1.1.3.2" xref="S2.E4.m1.7.7.1.1.1.1.1.1.3.1.cmml"><mi id="S2.E4.m1.3.3" xref="S2.E4.m1.3.3.cmml">q</mi><mo id="S2.E4.m1.7.7.1.1.1.1.1.1.3.2.1" xref="S2.E4.m1.7.7.1.1.1.1.1.1.3.1.cmml">;</mo><mi id="S2.E4.m1.4.4" xref="S2.E4.m1.4.4.cmml">θ</mi></mrow></mrow><mo id="S2.E4.m1.7.7.1.1.1.1.1.3" stretchy="false" xref="S2.E4.m1.7.7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.E4.m1.7.7.1.1.6" rspace="0.111em" xref="S2.E4.m1.7.7.1.1.6.cmml">=</mo><mrow id="S2.E4.m1.7.7.1.1.2" xref="S2.E4.m1.7.7.1.1.2.cmml"><munderover id="S2.E4.m1.7.7.1.1.2.2" xref="S2.E4.m1.7.7.1.1.2.2.cmml"><mo id="S2.E4.m1.7.7.1.1.2.2.2.2" movablelimits="false" xref="S2.E4.m1.7.7.1.1.2.2.2.2.cmml">∏</mo><mrow id="S2.E4.m1.7.7.1.1.2.2.2.3" xref="S2.E4.m1.7.7.1.1.2.2.2.3.cmml"><mi id="S2.E4.m1.7.7.1.1.2.2.2.3.2" xref="S2.E4.m1.7.7.1.1.2.2.2.3.2.cmml">i</mi><mo id="S2.E4.m1.7.7.1.1.2.2.2.3.1" xref="S2.E4.m1.7.7.1.1.2.2.2.3.1.cmml">=</mo><mn id="S2.E4.m1.7.7.1.1.2.2.2.3.3" xref="S2.E4.m1.7.7.1.1.2.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E4.m1.7.7.1.1.2.2.3" xref="S2.E4.m1.7.7.1.1.2.2.3.cmml">T</mi></munderover><mrow id="S2.E4.m1.7.7.1.1.2.1" xref="S2.E4.m1.7.7.1.1.2.1.cmml"><mi id="S2.E4.m1.7.7.1.1.2.1.3" xref="S2.E4.m1.7.7.1.1.2.1.3.cmml">P</mi><mo id="S2.E4.m1.7.7.1.1.2.1.2" xref="S2.E4.m1.7.7.1.1.2.1.2.cmml">⁢</mo><mrow id="S2.E4.m1.7.7.1.1.2.1.1.1" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.cmml"><mo id="S2.E4.m1.7.7.1.1.2.1.1.1.2" stretchy="false" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.cmml">(</mo><mrow id="S2.E4.m1.7.7.1.1.2.1.1.1.1" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.cmml"><msubsup id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.cmml"><mi id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.2" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.2.cmml">d</mi><mi id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.3" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.3.cmml">i</mi><mo id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.3" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.3.cmml">′</mo></msubsup><mo fence="false" id="S2.E4.m1.7.7.1.1.2.1.1.1.1.2" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.2.cmml">|</mo><mrow id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.2.cmml"><msubsup id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.2" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.2.cmml">d</mi><mrow id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.cmml"><mi id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.2" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.2.cmml"></mi><mo id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.1" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.1.cmml">&lt;</mo><mi id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.3" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.3.cmml">i</mi></mrow><mo id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.3" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.2" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.2.cmml">,</mo><mi id="S2.E4.m1.5.5" xref="S2.E4.m1.5.5.cmml">q</mi><mo id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.3" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.2.cmml">;</mo><mi id="S2.E4.m1.6.6" xref="S2.E4.m1.6.6.cmml">θ</mi></mrow></mrow><mo id="S2.E4.m1.7.7.1.1.2.1.1.1.3" stretchy="false" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S2.E4.m1.7.7.1.2" xref="S2.E4.m1.7.7.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E4.m1.7b"><apply id="S2.E4.m1.7.7.1.1.cmml" xref="S2.E4.m1.7.7.1"><and id="S2.E4.m1.7.7.1.1a.cmml" xref="S2.E4.m1.7.7.1"></and><apply id="S2.E4.m1.7.7.1.1b.cmml" xref="S2.E4.m1.7.7.1"><eq id="S2.E4.m1.7.7.1.1.5.cmml" xref="S2.E4.m1.7.7.1.1.5"></eq><apply id="S2.E4.m1.7.7.1.1.4.cmml" xref="S2.E4.m1.7.7.1.1.4"><times id="S2.E4.m1.7.7.1.1.4.1.cmml" xref="S2.E4.m1.7.7.1.1.4.1"></times><ci id="S2.E4.m1.7.7.1.1.4.2.cmml" xref="S2.E4.m1.7.7.1.1.4.2">ℛ</ci><interval closure="open" id="S2.E4.m1.7.7.1.1.4.3.1.cmml" xref="S2.E4.m1.7.7.1.1.4.3.2"><ci id="S2.E4.m1.1.1.cmml" xref="S2.E4.m1.1.1">𝑞</ci><ci id="S2.E4.m1.2.2.cmml" xref="S2.E4.m1.2.2">𝑑</ci></interval></apply><apply id="S2.E4.m1.7.7.1.1.1.cmml" xref="S2.E4.m1.7.7.1.1.1"><times id="S2.E4.m1.7.7.1.1.1.2.cmml" xref="S2.E4.m1.7.7.1.1.1.2"></times><ci id="S2.E4.m1.7.7.1.1.1.3.cmml" xref="S2.E4.m1.7.7.1.1.1.3">𝑃</ci><apply id="S2.E4.m1.7.7.1.1.1.1.1.1.cmml" xref="S2.E4.m1.7.7.1.1.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.7.7.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.7.7.1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.E4.m1.7.7.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.7.7.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.7.7.1.1.1.1.1.1.2">superscript</csymbol><ci id="S2.E4.m1.7.7.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.7.7.1.1.1.1.1.1.2.2">𝑑</ci><ci id="S2.E4.m1.7.7.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.7.7.1.1.1.1.1.1.2.3">′</ci></apply><list id="S2.E4.m1.7.7.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.7.7.1.1.1.1.1.1.3.2"><ci id="S2.E4.m1.3.3.cmml" xref="S2.E4.m1.3.3">𝑞</ci><ci id="S2.E4.m1.4.4.cmml" xref="S2.E4.m1.4.4">𝜃</ci></list></apply></apply></apply><apply id="S2.E4.m1.7.7.1.1c.cmml" xref="S2.E4.m1.7.7.1"><eq id="S2.E4.m1.7.7.1.1.6.cmml" xref="S2.E4.m1.7.7.1.1.6"></eq><share href="https://arxiv.org/html/2404.14851v3#S2.E4.m1.7.7.1.1.1.cmml" id="S2.E4.m1.7.7.1.1d.cmml" xref="S2.E4.m1.7.7.1"></share><apply id="S2.E4.m1.7.7.1.1.2.cmml" xref="S2.E4.m1.7.7.1.1.2"><apply id="S2.E4.m1.7.7.1.1.2.2.cmml" xref="S2.E4.m1.7.7.1.1.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.1.1.2.2.1.cmml" xref="S2.E4.m1.7.7.1.1.2.2">superscript</csymbol><apply id="S2.E4.m1.7.7.1.1.2.2.2.cmml" xref="S2.E4.m1.7.7.1.1.2.2"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.1.1.2.2.2.1.cmml" xref="S2.E4.m1.7.7.1.1.2.2">subscript</csymbol><csymbol cd="latexml" id="S2.E4.m1.7.7.1.1.2.2.2.2.cmml" xref="S2.E4.m1.7.7.1.1.2.2.2.2">product</csymbol><apply id="S2.E4.m1.7.7.1.1.2.2.2.3.cmml" xref="S2.E4.m1.7.7.1.1.2.2.2.3"><eq id="S2.E4.m1.7.7.1.1.2.2.2.3.1.cmml" xref="S2.E4.m1.7.7.1.1.2.2.2.3.1"></eq><ci id="S2.E4.m1.7.7.1.1.2.2.2.3.2.cmml" xref="S2.E4.m1.7.7.1.1.2.2.2.3.2">𝑖</ci><cn id="S2.E4.m1.7.7.1.1.2.2.2.3.3.cmml" type="integer" xref="S2.E4.m1.7.7.1.1.2.2.2.3.3">1</cn></apply></apply><ci id="S2.E4.m1.7.7.1.1.2.2.3.cmml" xref="S2.E4.m1.7.7.1.1.2.2.3">𝑇</ci></apply><apply id="S2.E4.m1.7.7.1.1.2.1.cmml" xref="S2.E4.m1.7.7.1.1.2.1"><times id="S2.E4.m1.7.7.1.1.2.1.2.cmml" xref="S2.E4.m1.7.7.1.1.2.1.2"></times><ci id="S2.E4.m1.7.7.1.1.2.1.3.cmml" xref="S2.E4.m1.7.7.1.1.2.1.3">𝑃</ci><apply id="S2.E4.m1.7.7.1.1.2.1.1.1.1.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1"><csymbol cd="latexml" id="S2.E4.m1.7.7.1.1.2.1.1.1.1.2.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.2">conditional</csymbol><apply id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.1.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3">subscript</csymbol><apply id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.1.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3">superscript</csymbol><ci id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.2.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.2">𝑑</ci><ci id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.3.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.2.3">′</ci></apply><ci id="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.3.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.3.3">𝑖</ci></apply><list id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.2.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1"><apply id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.2.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.2">𝑑</ci><ci id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.3.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.2.3">′</ci></apply><apply id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3"><lt id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.1"></lt><csymbol cd="latexml" id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.2">absent</csymbol><ci id="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E4.m1.7.7.1.1.2.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply><ci id="S2.E4.m1.5.5.cmml" xref="S2.E4.m1.5.5">𝑞</ci><ci id="S2.E4.m1.6.6.cmml" xref="S2.E4.m1.6.6">𝜃</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E4.m1.7c">\mathcal{R}(q,d)=P(d^{\prime}|q;\theta)=\prod_{i=1}^{T}P(d^{\prime}_{i}|d^{%
\prime}_{&lt;i},q;\theta),</annotation><annotation encoding="application/x-llamapun" id="S2.E4.m1.7d">caligraphic_R ( italic_q , italic_d ) = italic_P ( italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT | italic_q ; italic_θ ) = ∏ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_P ( italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT , italic_q ; italic_θ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS2.p2.12">where <math alttext="T" class="ltx_Math" display="inline" id="S2.SS2.p2.6.m1.1"><semantics id="S2.SS2.p2.6.m1.1a"><mi id="S2.SS2.p2.6.m1.1.1" xref="S2.SS2.p2.6.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.6.m1.1b"><ci id="S2.SS2.p2.6.m1.1.1.cmml" xref="S2.SS2.p2.6.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.6.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.6.m1.1d">italic_T</annotation></semantics></math> is the length of the generated document identifier <math alttext="d^{\prime}" class="ltx_Math" display="inline" id="S2.SS2.p2.7.m2.1"><semantics id="S2.SS2.p2.7.m2.1a"><msup id="S2.SS2.p2.7.m2.1.1" xref="S2.SS2.p2.7.m2.1.1.cmml"><mi id="S2.SS2.p2.7.m2.1.1.2" xref="S2.SS2.p2.7.m2.1.1.2.cmml">d</mi><mo id="S2.SS2.p2.7.m2.1.1.3" xref="S2.SS2.p2.7.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.7.m2.1b"><apply id="S2.SS2.p2.7.m2.1.1.cmml" xref="S2.SS2.p2.7.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.7.m2.1.1.1.cmml" xref="S2.SS2.p2.7.m2.1.1">superscript</csymbol><ci id="S2.SS2.p2.7.m2.1.1.2.cmml" xref="S2.SS2.p2.7.m2.1.1.2">𝑑</ci><ci id="S2.SS2.p2.7.m2.1.1.3.cmml" xref="S2.SS2.p2.7.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.7.m2.1c">d^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.7.m2.1d">italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="d^{\prime}_{i}" class="ltx_Math" display="inline" id="S2.SS2.p2.8.m3.1"><semantics id="S2.SS2.p2.8.m3.1a"><msubsup id="S2.SS2.p2.8.m3.1.1" xref="S2.SS2.p2.8.m3.1.1.cmml"><mi id="S2.SS2.p2.8.m3.1.1.2.2" xref="S2.SS2.p2.8.m3.1.1.2.2.cmml">d</mi><mi id="S2.SS2.p2.8.m3.1.1.3" xref="S2.SS2.p2.8.m3.1.1.3.cmml">i</mi><mo id="S2.SS2.p2.8.m3.1.1.2.3" xref="S2.SS2.p2.8.m3.1.1.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.8.m3.1b"><apply id="S2.SS2.p2.8.m3.1.1.cmml" xref="S2.SS2.p2.8.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.8.m3.1.1.1.cmml" xref="S2.SS2.p2.8.m3.1.1">subscript</csymbol><apply id="S2.SS2.p2.8.m3.1.1.2.cmml" xref="S2.SS2.p2.8.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.8.m3.1.1.2.1.cmml" xref="S2.SS2.p2.8.m3.1.1">superscript</csymbol><ci id="S2.SS2.p2.8.m3.1.1.2.2.cmml" xref="S2.SS2.p2.8.m3.1.1.2.2">𝑑</ci><ci id="S2.SS2.p2.8.m3.1.1.2.3.cmml" xref="S2.SS2.p2.8.m3.1.1.2.3">′</ci></apply><ci id="S2.SS2.p2.8.m3.1.1.3.cmml" xref="S2.SS2.p2.8.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.8.m3.1c">d^{\prime}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.8.m3.1d">italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the token at position <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p2.9.m4.1"><semantics id="S2.SS2.p2.9.m4.1a"><mi id="S2.SS2.p2.9.m4.1.1" xref="S2.SS2.p2.9.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.9.m4.1b"><ci id="S2.SS2.p2.9.m4.1.1.cmml" xref="S2.SS2.p2.9.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.9.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.9.m4.1d">italic_i</annotation></semantics></math>, <math alttext="d^{\prime}_{&lt;i}" class="ltx_Math" display="inline" id="S2.SS2.p2.10.m5.1"><semantics id="S2.SS2.p2.10.m5.1a"><msubsup id="S2.SS2.p2.10.m5.1.1" xref="S2.SS2.p2.10.m5.1.1.cmml"><mi id="S2.SS2.p2.10.m5.1.1.2.2" xref="S2.SS2.p2.10.m5.1.1.2.2.cmml">d</mi><mrow id="S2.SS2.p2.10.m5.1.1.3" xref="S2.SS2.p2.10.m5.1.1.3.cmml"><mi id="S2.SS2.p2.10.m5.1.1.3.2" xref="S2.SS2.p2.10.m5.1.1.3.2.cmml"></mi><mo id="S2.SS2.p2.10.m5.1.1.3.1" xref="S2.SS2.p2.10.m5.1.1.3.1.cmml">&lt;</mo><mi id="S2.SS2.p2.10.m5.1.1.3.3" xref="S2.SS2.p2.10.m5.1.1.3.3.cmml">i</mi></mrow><mo id="S2.SS2.p2.10.m5.1.1.2.3" xref="S2.SS2.p2.10.m5.1.1.2.3.cmml">′</mo></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.10.m5.1b"><apply id="S2.SS2.p2.10.m5.1.1.cmml" xref="S2.SS2.p2.10.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.10.m5.1.1.1.cmml" xref="S2.SS2.p2.10.m5.1.1">subscript</csymbol><apply id="S2.SS2.p2.10.m5.1.1.2.cmml" xref="S2.SS2.p2.10.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.10.m5.1.1.2.1.cmml" xref="S2.SS2.p2.10.m5.1.1">superscript</csymbol><ci id="S2.SS2.p2.10.m5.1.1.2.2.cmml" xref="S2.SS2.p2.10.m5.1.1.2.2">𝑑</ci><ci id="S2.SS2.p2.10.m5.1.1.2.3.cmml" xref="S2.SS2.p2.10.m5.1.1.2.3">′</ci></apply><apply id="S2.SS2.p2.10.m5.1.1.3.cmml" xref="S2.SS2.p2.10.m5.1.1.3"><lt id="S2.SS2.p2.10.m5.1.1.3.1.cmml" xref="S2.SS2.p2.10.m5.1.1.3.1"></lt><csymbol cd="latexml" id="S2.SS2.p2.10.m5.1.1.3.2.cmml" xref="S2.SS2.p2.10.m5.1.1.3.2">absent</csymbol><ci id="S2.SS2.p2.10.m5.1.1.3.3.cmml" xref="S2.SS2.p2.10.m5.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.10.m5.1c">d^{\prime}_{&lt;i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.10.m5.1d">italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the sequence of tokens generated before position <math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.p2.11.m6.1"><semantics id="S2.SS2.p2.11.m6.1a"><mi id="S2.SS2.p2.11.m6.1.1" xref="S2.SS2.p2.11.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.11.m6.1b"><ci id="S2.SS2.p2.11.m6.1.1.cmml" xref="S2.SS2.p2.11.m6.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.11.m6.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.11.m6.1d">italic_i</annotation></semantics></math>, and <math alttext="\theta" class="ltx_Math" display="inline" id="S2.SS2.p2.12.m7.1"><semantics id="S2.SS2.p2.12.m7.1a"><mi id="S2.SS2.p2.12.m7.1.1" xref="S2.SS2.p2.12.m7.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.12.m7.1b"><ci id="S2.SS2.p2.12.m7.1.1.cmml" xref="S2.SS2.p2.12.m7.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.12.m7.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.12.m7.1d">italic_θ</annotation></semantics></math> represents the model parameters.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Generative retrieval models demonstrate flexibility and deep semantic understanding capabilities. Compared to traditional methods, generative retrieval achieves end-to-end optimization of the retrieval process by directly generating DocIDs. Additionally, it reduces reliance on external indexing, lowering the system’s demand for storage resources.
However, its performance and efficiency are limited by model capacity and computational resources, indicating that future research might focus on finding more efficient model architectures and training methods to better balance performance, resource consumption, and response speed.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span><span class="ltx_text ltx_font_italic" id="S2.SS3.1.1">Large Language Models</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The evolution of Large Language Models (LLMs) marks a significant leap in natural language processing (NLP), rooted from early statistical and neural network-based language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib191" title="">191</a>]</cite>. These models, through pre-training on vast text corpora, learned deep semantic features of language, greatly enriching the understanding of text. Subsequently, generative language models, most notably the GPT series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib192" title="">192</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib62" title="">62</a>]</cite>, significantly improved text generation and understanding capabilities with improved model size and number of parameters.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">LLMs can be mainly divided into two categories: encoder-decoder models and decoder-only models. Encoder-decoder models, like T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib17" title="">17</a>]</cite> and BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib18" title="">18</a>]</cite>, convert input text into vector representations through their encoder, then the decoder generates output text based on these representations. This model perspective treats various NLP tasks as text-to-text conversion problems, solving them through text generation. On the other hand, decoder-only models, like the GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib19" title="">19</a>]</cite> and GPT-2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib192" title="">192</a>]</cite>, rely entirely on the Transformer decoder, generating text step by step through the self-attention mechanism. The introduction of GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib62" title="">62</a>]</cite>, with its 175 billion parameters, marked a significant milestone in this field and led to the creation of models like InstructGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib193" title="">193</a>]</cite>, Falcon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib194" title="">194</a>]</cite>, PaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib120" title="">120</a>]</cite> and LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib64" title="">64</a>]</cite>. These models, all using a decoder-only architecture, trained on large-scale datasets, have shown astonishing language processing capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">In the field of information retrieval, LLMs utilize two main strategies: In-Context Learning (ICL) and parameter-efficient fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib191" title="">191</a>]</cite>. ICL leverages the knowledge pre-trained in the model to understand and handle new tasks, requiring only a series of relevant examples and task descriptions to be provided to the model, enabling it to address specific information retrieval tasks without additional training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib62" title="">62</a>]</cite>. This method’s flexibility and cost-effectiveness are extremely high, allowing the model to quickly adapt to various scenarios.
On the other hand, parameter-efficient fine-tuning, such as LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib195" title="">195</a>]</cite>, PEFT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib196" title="">196</a>]</cite> and QLoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib197" title="">197</a>]</cite> technologies, adjusts a very small part of the model’s parameters to adapt to specific tasks, avoiding the need to retrain the entire model. These techniques, by fine-tuning key parts of the model, significantly reduce reliance on computational resources while maintaining performance.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Incorporating these strategies, LLMs have demonstrated exceptional potential across various information retrieval tasks, including but not limited to query rewriting, document retrieval, result re-ranking, document reading comprehension, and agent-based search mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib191" title="">191</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib198" title="">198</a>]</cite>. The application of LLMs extends beyond enhancing the relevance of search outcomes; they are instrumental in directly generating the precise information sought by users.
This capability marks a significant step towards a new era of generative information retrieval. In this era, the retrieval process is not solely about locating existing information but also about creating new content that meets the specific needs of users. This feature is especially advantageous in situations where users might not know how to phrase their queries or when they are in search of complex and highly personalized information, scenarios where traditional matching-based methods fall short.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers</span>
</h2>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S3.F3.g1" src="x2.png" width="828"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Timeline of research in generative retrieval: focus on model training and structure, document identifier design, incremental learning and downstream task adaptation.
</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In recent advancements in AIGC, generative retrieval (GR) has emerged as an promising approach in the field of information retrieval, garnering increasing interest from the academic community. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.F3" title="Figure 3 ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a> showcases a timeline of the GR methods. Initially, GENRE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib29" title="">29</a>]</cite> proposed to retrieve entities by generating their unique names through constrained beam search via a pre-built entity prefix tree, achieving advanced entity retrieval performance. Subsequently, Metzler et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib16" title="">16</a>]</cite> envisioned a model-based information retrieval framework aiming to combine the strengths of traditional document retrieval systems and pre-trained language models to create systems capable of providing expert-quality answers in various domains.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Following their lead, a diverse range of methods including DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>, DynamicRetriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib86" title="">86</a>]</cite>, SEAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib102" title="">102</a>]</cite>, NCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>]</cite>, etc., have been developed, with a continuously growing body of work. These methods explore various aspects such as model training and architectures, document identifiers, incremental learning, task-specific adaptation, and generative recommendations. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.F4" title="Figure 4 ‣ 3.1.1 Model Training ‣ 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4</span></a> presents an overview of the GR system and we’ll provide an in-depth discussion of each associated challenge in the following sections.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span class="ltx_text ltx_font_italic" id="S3.SS1.1.1">Model Training and Structure</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">One of the core components of GR is the model training and structure, aiming to enhance the model’s ability to memorize documents in the corpus.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1 </span>Model Training</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">To effectively train generative models for indexing documents, the standard approach is to train the mapping from queries to relevant DocIDs, based on standard sequence-to-sequence (seq2seq) training methods, as described in Equation (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S2.E2" title="In 2.1 Traditional Information Retrieval ‣ 2 Background and Preliminaries ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">2</span></a>). This method has been widely used in numerous GR research works, such as DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>, NCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>]</cite>, SEAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib102" title="">102</a>]</cite>, etc.
Moreover, a series of works have proposed various model training methods tailored for GR tasks to further enhance GR retrieval performance, such as sampling documents or generating queries based on document content to serve as pseudo queries for data augmentation; or including training objectives for document ranking.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">Specifically, DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite> proposed two training strategies: one is “indexing”, that is, training the model to associate document tokens with their corresponding DocIDs, where DocIDs are pre-built based on documents in corpus, which will be discussed in detail in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS2" title="3.2 Design of Document Identifiers ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.2</span></a>; the other is “retrieval”, using labeled query-DocID pairs to fine-tune the model. Notably, DSI was the first to realize a differentiable search index based on the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib199" title="">199</a>]</cite> structure, showing good performance in web search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib162" title="">162</a>]</cite> and question answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib163" title="">163</a>]</cite> scenarios.
Next, a series of methods propose training methods for data augmentation and improving GR model ranking ability</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.5"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p3.5.1">Sampling Document Pieces as Pseudo Queries.</span> In the same era, DynamicRetriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib86" title="">86</a>]</cite>, also based on the encoder-decoder model, constructed a model-based IR system by initializing the encoder with a pre-trained BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib13" title="">13</a>]</cite>. Besides, DynamicRetriever utilizes passages, sampled terms and N-grams to serve as pseudo queries to enhance the model’s memorization of DocIDs.
Formally, the training methods can be summarized as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S7.EGx1">
<tbody id="S3.E5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E5.2.1.1.1">Sampled Document</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle:d_{s_{i}}\longrightarrow\text{DocID},i\in\{1,...,k_{d_{s}}\}," class="ltx_Math" display="inline" id="S3.E5.m2.3"><semantics id="S3.E5.m2.3a"><mrow id="S3.E5.m2.3.3.1" xref="S3.E5.m2.3.3.1.1.cmml"><mrow id="S3.E5.m2.3.3.1.1" xref="S3.E5.m2.3.3.1.1.cmml"><mi id="S3.E5.m2.3.3.1.1.4" xref="S3.E5.m2.3.3.1.1.4.cmml"></mi><mo id="S3.E5.m2.3.3.1.1.3" lspace="0.278em" rspace="0.278em" xref="S3.E5.m2.3.3.1.1.3.cmml">:</mo><mrow id="S3.E5.m2.3.3.1.1.2.2" xref="S3.E5.m2.3.3.1.1.2.3.cmml"><mrow id="S3.E5.m2.3.3.1.1.1.1.1" xref="S3.E5.m2.3.3.1.1.1.1.1.cmml"><msub id="S3.E5.m2.3.3.1.1.1.1.1.2" xref="S3.E5.m2.3.3.1.1.1.1.1.2.cmml"><mi id="S3.E5.m2.3.3.1.1.1.1.1.2.2" xref="S3.E5.m2.3.3.1.1.1.1.1.2.2.cmml">d</mi><msub id="S3.E5.m2.3.3.1.1.1.1.1.2.3" xref="S3.E5.m2.3.3.1.1.1.1.1.2.3.cmml"><mi id="S3.E5.m2.3.3.1.1.1.1.1.2.3.2" xref="S3.E5.m2.3.3.1.1.1.1.1.2.3.2.cmml">s</mi><mi id="S3.E5.m2.3.3.1.1.1.1.1.2.3.3" xref="S3.E5.m2.3.3.1.1.1.1.1.2.3.3.cmml">i</mi></msub></msub><mo id="S3.E5.m2.3.3.1.1.1.1.1.1" stretchy="false" xref="S3.E5.m2.3.3.1.1.1.1.1.1.cmml">⟶</mo><mtext id="S3.E5.m2.3.3.1.1.1.1.1.3" xref="S3.E5.m2.3.3.1.1.1.1.1.3a.cmml">DocID</mtext></mrow><mo id="S3.E5.m2.3.3.1.1.2.2.3" xref="S3.E5.m2.3.3.1.1.2.3a.cmml">,</mo><mrow id="S3.E5.m2.3.3.1.1.2.2.2" xref="S3.E5.m2.3.3.1.1.2.2.2.cmml"><mi id="S3.E5.m2.3.3.1.1.2.2.2.3" xref="S3.E5.m2.3.3.1.1.2.2.2.3.cmml">i</mi><mo id="S3.E5.m2.3.3.1.1.2.2.2.2" xref="S3.E5.m2.3.3.1.1.2.2.2.2.cmml">∈</mo><mrow id="S3.E5.m2.3.3.1.1.2.2.2.1.1" xref="S3.E5.m2.3.3.1.1.2.2.2.1.2.cmml"><mo id="S3.E5.m2.3.3.1.1.2.2.2.1.1.2" stretchy="false" xref="S3.E5.m2.3.3.1.1.2.2.2.1.2.cmml">{</mo><mn id="S3.E5.m2.1.1" xref="S3.E5.m2.1.1.cmml">1</mn><mo id="S3.E5.m2.3.3.1.1.2.2.2.1.1.3" xref="S3.E5.m2.3.3.1.1.2.2.2.1.2.cmml">,</mo><mi id="S3.E5.m2.2.2" mathvariant="normal" xref="S3.E5.m2.2.2.cmml">…</mi><mo id="S3.E5.m2.3.3.1.1.2.2.2.1.1.4" xref="S3.E5.m2.3.3.1.1.2.2.2.1.2.cmml">,</mo><msub id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.cmml"><mi id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.2" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.2.cmml">k</mi><msub id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.cmml"><mi id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.2" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.2.cmml">d</mi><mi id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.3" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.3.cmml">s</mi></msub></msub><mo id="S3.E5.m2.3.3.1.1.2.2.2.1.1.5" stretchy="false" xref="S3.E5.m2.3.3.1.1.2.2.2.1.2.cmml">}</mo></mrow></mrow></mrow></mrow><mo id="S3.E5.m2.3.3.1.2" xref="S3.E5.m2.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m2.3b"><apply id="S3.E5.m2.3.3.1.1.cmml" xref="S3.E5.m2.3.3.1"><ci id="S3.E5.m2.3.3.1.1.3.cmml" xref="S3.E5.m2.3.3.1.1.3">:</ci><csymbol cd="latexml" id="S3.E5.m2.3.3.1.1.4.cmml" xref="S3.E5.m2.3.3.1.1.4">absent</csymbol><apply id="S3.E5.m2.3.3.1.1.2.3.cmml" xref="S3.E5.m2.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E5.m2.3.3.1.1.2.3a.cmml" xref="S3.E5.m2.3.3.1.1.2.2.3">formulae-sequence</csymbol><apply id="S3.E5.m2.3.3.1.1.1.1.1.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1"><ci id="S3.E5.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.1">⟶</ci><apply id="S3.E5.m2.3.3.1.1.1.1.1.2.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E5.m2.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E5.m2.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.2.2">𝑑</ci><apply id="S3.E5.m2.3.3.1.1.1.1.1.2.3.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E5.m2.3.3.1.1.1.1.1.2.3.1.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E5.m2.3.3.1.1.1.1.1.2.3.2.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.2.3.2">𝑠</ci><ci id="S3.E5.m2.3.3.1.1.1.1.1.2.3.3.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="S3.E5.m2.3.3.1.1.1.1.1.3a.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.3"><mtext id="S3.E5.m2.3.3.1.1.1.1.1.3.cmml" xref="S3.E5.m2.3.3.1.1.1.1.1.3">DocID</mtext></ci></apply><apply id="S3.E5.m2.3.3.1.1.2.2.2.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2"><in id="S3.E5.m2.3.3.1.1.2.2.2.2.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.2"></in><ci id="S3.E5.m2.3.3.1.1.2.2.2.3.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.3">𝑖</ci><set id="S3.E5.m2.3.3.1.1.2.2.2.1.2.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1"><cn id="S3.E5.m2.1.1.cmml" type="integer" xref="S3.E5.m2.1.1">1</cn><ci id="S3.E5.m2.2.2.cmml" xref="S3.E5.m2.2.2">…</ci><apply id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.2">𝑘</ci><apply id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.1.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.2.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.2">𝑑</ci><ci id="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.3.cmml" xref="S3.E5.m2.3.3.1.1.2.2.2.1.1.1.3.3">𝑠</ci></apply></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m2.3c">\displaystyle:d_{s_{i}}\longrightarrow\text{DocID},i\in\{1,...,k_{d_{s}}\},</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m2.3d">: italic_d start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ⟶ DocID , italic_i ∈ { 1 , … , italic_k start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
<tbody id="S3.E6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E6.2.1.1.1">Labeled Query</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle:q_{i}\longrightarrow\text{DocID},i\in\{1,...,k_{q}\}," class="ltx_Math" display="inline" id="S3.E6.m2.3"><semantics id="S3.E6.m2.3a"><mrow id="S3.E6.m2.3.3.1" xref="S3.E6.m2.3.3.1.1.cmml"><mrow id="S3.E6.m2.3.3.1.1" xref="S3.E6.m2.3.3.1.1.cmml"><mi id="S3.E6.m2.3.3.1.1.4" xref="S3.E6.m2.3.3.1.1.4.cmml"></mi><mo id="S3.E6.m2.3.3.1.1.3" lspace="0.278em" rspace="0.278em" xref="S3.E6.m2.3.3.1.1.3.cmml">:</mo><mrow id="S3.E6.m2.3.3.1.1.2.2" xref="S3.E6.m2.3.3.1.1.2.3.cmml"><mrow id="S3.E6.m2.3.3.1.1.1.1.1" xref="S3.E6.m2.3.3.1.1.1.1.1.cmml"><msub id="S3.E6.m2.3.3.1.1.1.1.1.2" xref="S3.E6.m2.3.3.1.1.1.1.1.2.cmml"><mi id="S3.E6.m2.3.3.1.1.1.1.1.2.2" xref="S3.E6.m2.3.3.1.1.1.1.1.2.2.cmml">q</mi><mi id="S3.E6.m2.3.3.1.1.1.1.1.2.3" xref="S3.E6.m2.3.3.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.E6.m2.3.3.1.1.1.1.1.1" stretchy="false" xref="S3.E6.m2.3.3.1.1.1.1.1.1.cmml">⟶</mo><mtext id="S3.E6.m2.3.3.1.1.1.1.1.3" xref="S3.E6.m2.3.3.1.1.1.1.1.3a.cmml">DocID</mtext></mrow><mo id="S3.E6.m2.3.3.1.1.2.2.3" xref="S3.E6.m2.3.3.1.1.2.3a.cmml">,</mo><mrow id="S3.E6.m2.3.3.1.1.2.2.2" xref="S3.E6.m2.3.3.1.1.2.2.2.cmml"><mi id="S3.E6.m2.3.3.1.1.2.2.2.3" xref="S3.E6.m2.3.3.1.1.2.2.2.3.cmml">i</mi><mo id="S3.E6.m2.3.3.1.1.2.2.2.2" xref="S3.E6.m2.3.3.1.1.2.2.2.2.cmml">∈</mo><mrow id="S3.E6.m2.3.3.1.1.2.2.2.1.1" xref="S3.E6.m2.3.3.1.1.2.2.2.1.2.cmml"><mo id="S3.E6.m2.3.3.1.1.2.2.2.1.1.2" stretchy="false" xref="S3.E6.m2.3.3.1.1.2.2.2.1.2.cmml">{</mo><mn id="S3.E6.m2.1.1" xref="S3.E6.m2.1.1.cmml">1</mn><mo id="S3.E6.m2.3.3.1.1.2.2.2.1.1.3" xref="S3.E6.m2.3.3.1.1.2.2.2.1.2.cmml">,</mo><mi id="S3.E6.m2.2.2" mathvariant="normal" xref="S3.E6.m2.2.2.cmml">…</mi><mo id="S3.E6.m2.3.3.1.1.2.2.2.1.1.4" xref="S3.E6.m2.3.3.1.1.2.2.2.1.2.cmml">,</mo><msub id="S3.E6.m2.3.3.1.1.2.2.2.1.1.1" xref="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.cmml"><mi id="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.2" xref="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.2.cmml">k</mi><mi id="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.3" xref="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.3.cmml">q</mi></msub><mo id="S3.E6.m2.3.3.1.1.2.2.2.1.1.5" stretchy="false" xref="S3.E6.m2.3.3.1.1.2.2.2.1.2.cmml">}</mo></mrow></mrow></mrow></mrow><mo id="S3.E6.m2.3.3.1.2" xref="S3.E6.m2.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m2.3b"><apply id="S3.E6.m2.3.3.1.1.cmml" xref="S3.E6.m2.3.3.1"><ci id="S3.E6.m2.3.3.1.1.3.cmml" xref="S3.E6.m2.3.3.1.1.3">:</ci><csymbol cd="latexml" id="S3.E6.m2.3.3.1.1.4.cmml" xref="S3.E6.m2.3.3.1.1.4">absent</csymbol><apply id="S3.E6.m2.3.3.1.1.2.3.cmml" xref="S3.E6.m2.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E6.m2.3.3.1.1.2.3a.cmml" xref="S3.E6.m2.3.3.1.1.2.2.3">formulae-sequence</csymbol><apply id="S3.E6.m2.3.3.1.1.1.1.1.cmml" xref="S3.E6.m2.3.3.1.1.1.1.1"><ci id="S3.E6.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.E6.m2.3.3.1.1.1.1.1.1">⟶</ci><apply id="S3.E6.m2.3.3.1.1.1.1.1.2.cmml" xref="S3.E6.m2.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m2.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E6.m2.3.3.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E6.m2.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E6.m2.3.3.1.1.1.1.1.2.2">𝑞</ci><ci id="S3.E6.m2.3.3.1.1.1.1.1.2.3.cmml" xref="S3.E6.m2.3.3.1.1.1.1.1.2.3">𝑖</ci></apply><ci id="S3.E6.m2.3.3.1.1.1.1.1.3a.cmml" xref="S3.E6.m2.3.3.1.1.1.1.1.3"><mtext id="S3.E6.m2.3.3.1.1.1.1.1.3.cmml" xref="S3.E6.m2.3.3.1.1.1.1.1.3">DocID</mtext></ci></apply><apply id="S3.E6.m2.3.3.1.1.2.2.2.cmml" xref="S3.E6.m2.3.3.1.1.2.2.2"><in id="S3.E6.m2.3.3.1.1.2.2.2.2.cmml" xref="S3.E6.m2.3.3.1.1.2.2.2.2"></in><ci id="S3.E6.m2.3.3.1.1.2.2.2.3.cmml" xref="S3.E6.m2.3.3.1.1.2.2.2.3">𝑖</ci><set id="S3.E6.m2.3.3.1.1.2.2.2.1.2.cmml" xref="S3.E6.m2.3.3.1.1.2.2.2.1.1"><cn id="S3.E6.m2.1.1.cmml" type="integer" xref="S3.E6.m2.1.1">1</cn><ci id="S3.E6.m2.2.2.cmml" xref="S3.E6.m2.2.2">…</ci><apply id="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.cmml" xref="S3.E6.m2.3.3.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E6.m2.3.3.1.1.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.2">𝑘</ci><ci id="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.3.cmml" xref="S3.E6.m2.3.3.1.1.2.2.2.1.1.1.3">𝑞</ci></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m2.3c">\displaystyle:q_{i}\longrightarrow\text{DocID},i\in\{1,...,k_{q}\},</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m2.3d">: italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ⟶ DocID , italic_i ∈ { 1 , … , italic_k start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.SSS1.p3.4">where <math alttext="d_{s_{i}}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.1.m1.1"><semantics id="S3.SS1.SSS1.p3.1.m1.1a"><msub id="S3.SS1.SSS1.p3.1.m1.1.1" xref="S3.SS1.SSS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p3.1.m1.1.1.2" xref="S3.SS1.SSS1.p3.1.m1.1.1.2.cmml">d</mi><msub id="S3.SS1.SSS1.p3.1.m1.1.1.3" xref="S3.SS1.SSS1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p3.1.m1.1.1.3.2" xref="S3.SS1.SSS1.p3.1.m1.1.1.3.2.cmml">s</mi><mi id="S3.SS1.SSS1.p3.1.m1.1.1.3.3" xref="S3.SS1.SSS1.p3.1.m1.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.1.m1.1b"><apply id="S3.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1.2">𝑑</ci><apply id="S3.SS1.SSS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1.3.2">𝑠</ci><ci id="S3.SS1.SSS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.p3.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.1.m1.1c">d_{s_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="q_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.2.m2.1"><semantics id="S3.SS1.SSS1.p3.2.m2.1a"><msub id="S3.SS1.SSS1.p3.2.m2.1.1" xref="S3.SS1.SSS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p3.2.m2.1.1.2" xref="S3.SS1.SSS1.p3.2.m2.1.1.2.cmml">q</mi><mi id="S3.SS1.SSS1.p3.2.m2.1.1.3" xref="S3.SS1.SSS1.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.2.m2.1b"><apply id="S3.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1.2">𝑞</ci><ci id="S3.SS1.SSS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.2.m2.1c">q_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.2.m2.1d">italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> denote each of the <math alttext="k_{d_{s}}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.3.m3.1"><semantics id="S3.SS1.SSS1.p3.3.m3.1a"><msub id="S3.SS1.SSS1.p3.3.m3.1.1" xref="S3.SS1.SSS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.SSS1.p3.3.m3.1.1.2" xref="S3.SS1.SSS1.p3.3.m3.1.1.2.cmml">k</mi><msub id="S3.SS1.SSS1.p3.3.m3.1.1.3" xref="S3.SS1.SSS1.p3.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS1.p3.3.m3.1.1.3.2" xref="S3.SS1.SSS1.p3.3.m3.1.1.3.2.cmml">d</mi><mi id="S3.SS1.SSS1.p3.3.m3.1.1.3.3" xref="S3.SS1.SSS1.p3.3.m3.1.1.3.3.cmml">s</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.3.m3.1b"><apply id="S3.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.2">𝑘</ci><apply id="S3.SS1.SSS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS1.p3.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.3.2">𝑑</ci><ci id="S3.SS1.SSS1.p3.3.m3.1.1.3.3.cmml" xref="S3.SS1.SSS1.p3.3.m3.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.3.m3.1c">k_{d_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.3.m3.1d">italic_k start_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> sampled document text and each of the <math alttext="k_{q}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p3.4.m4.1"><semantics id="S3.SS1.SSS1.p3.4.m4.1a"><msub id="S3.SS1.SSS1.p3.4.m4.1.1" xref="S3.SS1.SSS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.SSS1.p3.4.m4.1.1.2" xref="S3.SS1.SSS1.p3.4.m4.1.1.2.cmml">k</mi><mi id="S3.SS1.SSS1.p3.4.m4.1.1.3" xref="S3.SS1.SSS1.p3.4.m4.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p3.4.m4.1b"><apply id="S3.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.2">𝑘</ci><ci id="S3.SS1.SSS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.SSS1.p3.4.m4.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p3.4.m4.1c">k_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p3.4.m4.1d">italic_k start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> labeled query for the corresponding DocID, respectively.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p4">
<p class="ltx_p" id="S3.SS1.SSS1.p4.3"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p4.3.1">Generating Pseudo Queries from Documents.</span> Following DSI, the NCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>]</cite> model was trained using a combination of labeled query-document pairs and augmented pseudo query-document pairs. Specifically, NCI proposed two strategies: one using the DocT5Query <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib200" title="">200</a>]</cite> model as a query generator, generating pseudo queries for each document in the corpus through beam search; the other strategy directly uses the document as a query, as stated in Equation (<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.E5" title="In 3.1.1 Model Training ‣ 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">5</span></a>). Similarly, DSI-QG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib87" title="">87</a>]</cite> also proposed using a query generator to enhance training data, establishing a bridge between indexing and retrieval in DSI. This data augmentation method has been proven in subsequent works to be an effective method to enhance the model’s memorization for DocIDs, which can be expressed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Pseudo Query}:q_{s_{i}}\longrightarrow\text{DocID},i\in\{1,...,k_{q_{s}}\}," class="ltx_Math" display="block" id="S3.E7.m1.3"><semantics id="S3.E7.m1.3a"><mrow id="S3.E7.m1.3.3.1" xref="S3.E7.m1.3.3.1.1.cmml"><mrow id="S3.E7.m1.3.3.1.1" xref="S3.E7.m1.3.3.1.1.cmml"><mtext id="S3.E7.m1.3.3.1.1.4" xref="S3.E7.m1.3.3.1.1.4a.cmml">Pseudo Query</mtext><mo id="S3.E7.m1.3.3.1.1.3" lspace="0.278em" rspace="0.278em" xref="S3.E7.m1.3.3.1.1.3.cmml">:</mo><mrow id="S3.E7.m1.3.3.1.1.2.2" xref="S3.E7.m1.3.3.1.1.2.3.cmml"><mrow id="S3.E7.m1.3.3.1.1.1.1.1" xref="S3.E7.m1.3.3.1.1.1.1.1.cmml"><msub id="S3.E7.m1.3.3.1.1.1.1.1.2" xref="S3.E7.m1.3.3.1.1.1.1.1.2.cmml"><mi id="S3.E7.m1.3.3.1.1.1.1.1.2.2" xref="S3.E7.m1.3.3.1.1.1.1.1.2.2.cmml">q</mi><msub id="S3.E7.m1.3.3.1.1.1.1.1.2.3" xref="S3.E7.m1.3.3.1.1.1.1.1.2.3.cmml"><mi id="S3.E7.m1.3.3.1.1.1.1.1.2.3.2" xref="S3.E7.m1.3.3.1.1.1.1.1.2.3.2.cmml">s</mi><mi id="S3.E7.m1.3.3.1.1.1.1.1.2.3.3" xref="S3.E7.m1.3.3.1.1.1.1.1.2.3.3.cmml">i</mi></msub></msub><mo id="S3.E7.m1.3.3.1.1.1.1.1.1" stretchy="false" xref="S3.E7.m1.3.3.1.1.1.1.1.1.cmml">⟶</mo><mtext id="S3.E7.m1.3.3.1.1.1.1.1.3" xref="S3.E7.m1.3.3.1.1.1.1.1.3a.cmml">DocID</mtext></mrow><mo id="S3.E7.m1.3.3.1.1.2.2.3" xref="S3.E7.m1.3.3.1.1.2.3a.cmml">,</mo><mrow id="S3.E7.m1.3.3.1.1.2.2.2" xref="S3.E7.m1.3.3.1.1.2.2.2.cmml"><mi id="S3.E7.m1.3.3.1.1.2.2.2.3" xref="S3.E7.m1.3.3.1.1.2.2.2.3.cmml">i</mi><mo id="S3.E7.m1.3.3.1.1.2.2.2.2" xref="S3.E7.m1.3.3.1.1.2.2.2.2.cmml">∈</mo><mrow id="S3.E7.m1.3.3.1.1.2.2.2.1.1" xref="S3.E7.m1.3.3.1.1.2.2.2.1.2.cmml"><mo id="S3.E7.m1.3.3.1.1.2.2.2.1.1.2" stretchy="false" xref="S3.E7.m1.3.3.1.1.2.2.2.1.2.cmml">{</mo><mn id="S3.E7.m1.1.1" xref="S3.E7.m1.1.1.cmml">1</mn><mo id="S3.E7.m1.3.3.1.1.2.2.2.1.1.3" xref="S3.E7.m1.3.3.1.1.2.2.2.1.2.cmml">,</mo><mi id="S3.E7.m1.2.2" mathvariant="normal" xref="S3.E7.m1.2.2.cmml">…</mi><mo id="S3.E7.m1.3.3.1.1.2.2.2.1.1.4" xref="S3.E7.m1.3.3.1.1.2.2.2.1.2.cmml">,</mo><msub id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.cmml"><mi id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.2" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.2.cmml">k</mi><msub id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.cmml"><mi id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.2" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.2.cmml">q</mi><mi id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.3" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.3.cmml">s</mi></msub></msub><mo id="S3.E7.m1.3.3.1.1.2.2.2.1.1.5" stretchy="false" xref="S3.E7.m1.3.3.1.1.2.2.2.1.2.cmml">}</mo></mrow></mrow></mrow></mrow><mo id="S3.E7.m1.3.3.1.2" xref="S3.E7.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E7.m1.3b"><apply id="S3.E7.m1.3.3.1.1.cmml" xref="S3.E7.m1.3.3.1"><ci id="S3.E7.m1.3.3.1.1.3.cmml" xref="S3.E7.m1.3.3.1.1.3">:</ci><ci id="S3.E7.m1.3.3.1.1.4a.cmml" xref="S3.E7.m1.3.3.1.1.4"><mtext id="S3.E7.m1.3.3.1.1.4.cmml" xref="S3.E7.m1.3.3.1.1.4">Pseudo Query</mtext></ci><apply id="S3.E7.m1.3.3.1.1.2.3.cmml" xref="S3.E7.m1.3.3.1.1.2.2"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.2.3a.cmml" xref="S3.E7.m1.3.3.1.1.2.2.3">formulae-sequence</csymbol><apply id="S3.E7.m1.3.3.1.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1"><ci id="S3.E7.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.1">⟶</ci><apply id="S3.E7.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E7.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.2.2">𝑞</ci><apply id="S3.E7.m1.3.3.1.1.1.1.1.2.3.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.1.1.1.2.3.1.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.2.3">subscript</csymbol><ci id="S3.E7.m1.3.3.1.1.1.1.1.2.3.2.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.2.3.2">𝑠</ci><ci id="S3.E7.m1.3.3.1.1.1.1.1.2.3.3.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="S3.E7.m1.3.3.1.1.1.1.1.3a.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.3"><mtext id="S3.E7.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E7.m1.3.3.1.1.1.1.1.3">DocID</mtext></ci></apply><apply id="S3.E7.m1.3.3.1.1.2.2.2.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2"><in id="S3.E7.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.2"></in><ci id="S3.E7.m1.3.3.1.1.2.2.2.3.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.3">𝑖</ci><set id="S3.E7.m1.3.3.1.1.2.2.2.1.2.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1"><cn id="S3.E7.m1.1.1.cmml" type="integer" xref="S3.E7.m1.1.1">1</cn><ci id="S3.E7.m1.2.2.cmml" xref="S3.E7.m1.2.2">…</ci><apply id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.1.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1">subscript</csymbol><ci id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.2.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.2">𝑘</ci><apply id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.1.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3">subscript</csymbol><ci id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.2.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.2">𝑞</ci><ci id="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.3.cmml" xref="S3.E7.m1.3.3.1.1.2.2.2.1.1.1.3.3">𝑠</ci></apply></apply></set></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E7.m1.3c">\text{Pseudo Query}:q_{s_{i}}\longrightarrow\text{DocID},i\in\{1,...,k_{q_{s}}\},</annotation><annotation encoding="application/x-llamapun" id="S3.E7.m1.3d">Pseudo Query : italic_q start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ⟶ DocID , italic_i ∈ { 1 , … , italic_k start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.SSS1.p4.2">where <math alttext="q_{s_{i}}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.1.m1.1"><semantics id="S3.SS1.SSS1.p4.1.m1.1a"><msub id="S3.SS1.SSS1.p4.1.m1.1.1" xref="S3.SS1.SSS1.p4.1.m1.1.1.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.2.cmml">q</mi><msub id="S3.SS1.SSS1.p4.1.m1.1.1.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS1.p4.1.m1.1.1.3.2" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.2.cmml">s</mi><mi id="S3.SS1.SSS1.p4.1.m1.1.1.3.3" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3.cmml">i</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.1.m1.1b"><apply id="S3.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.2">𝑞</ci><apply id="S3.SS1.SSS1.p4.1.m1.1.1.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS1.p4.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.2">𝑠</ci><ci id="S3.SS1.SSS1.p4.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.1.m1.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.1.m1.1c">q_{s_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> represents each of the <math alttext="k_{q_{s}}" class="ltx_Math" display="inline" id="S3.SS1.SSS1.p4.2.m2.1"><semantics id="S3.SS1.SSS1.p4.2.m2.1a"><msub id="S3.SS1.SSS1.p4.2.m2.1.1" xref="S3.SS1.SSS1.p4.2.m2.1.1.cmml"><mi id="S3.SS1.SSS1.p4.2.m2.1.1.2" xref="S3.SS1.SSS1.p4.2.m2.1.1.2.cmml">k</mi><msub id="S3.SS1.SSS1.p4.2.m2.1.1.3" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.cmml"><mi id="S3.SS1.SSS1.p4.2.m2.1.1.3.2" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.2.cmml">q</mi><mi id="S3.SS1.SSS1.p4.2.m2.1.1.3.3" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3.cmml">s</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS1.p4.2.m2.1b"><apply id="S3.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.2.m2.1.1.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS1.p4.2.m2.1.1.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.2">𝑘</ci><apply id="S3.SS1.SSS1.p4.2.m2.1.1.3.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS1.p4.2.m2.1.1.3.1.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3">subscript</csymbol><ci id="S3.SS1.SSS1.p4.2.m2.1.1.3.2.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.2">𝑞</ci><ci id="S3.SS1.SSS1.p4.2.m2.1.1.3.3.cmml" xref="S3.SS1.SSS1.p4.2.m2.1.1.3.3">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS1.p4.2.m2.1c">k_{q_{s}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS1.p4.2.m2.1d">italic_k start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> generated pseudo query for the corresponding DocID.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="232" id="S3.F4.g1" src="x3.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>
A conceptual framework for a generative retrieval system, with a focus on challenges in incremental learning, identifier construction, model training and structure, and integration with downstream tasks and recommendation systems.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS1.p5">
<p class="ltx_p" id="S3.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS1.p5.1.1">Improving Ranking Capability.</span> Additionally, a series of methods focus on further optimizing the ranking capability of GR models. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib201" title="">201</a>]</cite> aimed at understanding the DSI model and its retrieval effectiveness, noting that the DSI model has flaws in exclusivity, completeness, and relevance ordering, proposing a multi-task distillation method to improve retrieval quality without changing the model structure, thereby obtaining better indexing and ranking capabilities. Meanwhile, LTRGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib89" title="">89</a>]</cite> proposed to directly learn how to rank paragraphs with GR models, not just generating paragraph identifiers. This framework is first trained with a sequence-to-sequence loss, then further trained with a ranking loss, allowing the model to learn ranking at the document level.
Subsequently, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib34" title="">34</a>]</cite> proposed GenRRL, which improves the overall ranking quality through reinforcement learning with relevance feedback, aligning token-level DocID generation with document-level relevance estimation after seq2seq supervised fine-tuning.
Moreover,  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib90" title="">90</a>]</cite> introduced DGR, which enhances GR through knowledge distillation. Specifically, DGR uses a cross-encoder as a teacher model, providing fine-grained passage ranking supervision signals, and then optimizes the GR model with a distilled RankNet loss, effectively transferring the capabilities of advanced ranking models to the GR model. ListGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib91" title="">91</a>]</cite> also focuses on enhancing the ranking ability of GR models, by defining positional conditional probabilities, emphasizing the importance of the generation order of each DocID in the list. In addition, ListGR employs relevance calibration that adjusts the generated list of DocIDs to better align with the labeled ranking list.
See Table <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.T1" title="TABLE I ‣ 3.1.2 Model Structure ‣ 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">I</span></a> for a detailed comparison of GR methods.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2 </span>Model Structure</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Basic generative retrieval models mostly use pre-trained encoder-decoder structured generative models, such as T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib17" title="">17</a>]</cite> and BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib18" title="">18</a>]</cite>, fine-tuned for the DocID generation task. To better adapt to the GR task, researchers have proposed a series of specifically designed model structures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib92" title="">92</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib93" title="">93</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib94" title="">94</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib95" title="">95</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib96" title="">96</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib97" title="">97</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p2.1.1">Model Decoding Methods.</span> For the semantic structured DocID proposed by DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>, NCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>]</cite> designed a Prefix-Aware Weight-Adaptive (PAWA) decoder structure. By perceiving the hierarchical prefix of DocID and adjusting the weights at different positions, this decoder can capture the semantic hierarchy of DocIDs.
To overcome the capacity limitations of GR models, NP-Decoding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib93" title="">93</a>]</cite> proposed a decoding method that uses non-parametric contextualized word embeddings (as external memory) instead of traditional word embeddings as the input to the decoder. This allows the GR model not only to utilize its own parameter space but also to access and use information in contextualized word embeddings.
Additionally, PAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib98" title="">98</a>]</cite> proposed a planning-ahead generation approach, constructing DocIDs based on both sets and sequences. PAG first decodes the set-based DocID to approximate document-level scores, and then continues to decode the sequence-based DocID on this basis.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p3.1.1">Combining Generative and Dense Retrieval Methods.</span> Combining seq2seq generative models with dual-encoder retrieval models, MEVI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib94" title="">94</a>]</cite> utilizes Residual Quantization (RQ) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib202" title="">202</a>]</cite> to organize documents into hierarchical clusters targeted by the generative model. This structure enables efficient retrieval of candidate clusters followed by precise document search within those clusters using Approximate Nearest Neighbor (ANN) search methods, enhancing both recall and low-latency efficiency. Similarly, Generative Dense Retrieval (GDR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib96" title="">96</a>]</cite> merges the advantages of generative and dense retrieval in a two-step process. It first broadly matches queries to document clusters, optimizing for interaction depth and memory efficiency, and then performs precise, cluster-specific document searches, boosting both recall and scalability. This dual approach effectively combines broad matching with targeted searching, offering an optimized solution for document retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p4">
<p class="ltx_p" id="S3.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.SSS2.p4.1.1">Utilizing Multiple Models.</span> TOME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib92" title="">92</a>]</cite> proposed a method that decomposes the GR task into two stages, first generating text paragraphs related to the query through an additional model, then using the GR model to generate the URL related to that paragraph, thereby improving the accuracy of model retrieval. DiffusionRet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib95" title="">95</a>]</cite> proposed a two-stage enhanced generative retrieval structure based on diffusion models. First, a sequence-to-sequence diffusion model (SeqDiffuSeq <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib203" title="">203</a>]</cite>) is used to generate a pseudo-document from a query, a process similar to semantic filling of text, where the generated pseudo-document is similar to real documents in length, format, and content, rich in semantic information; secondly, another sequence-to-sequence model is used for generative retrieval based on N-gram, similar to SEAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib102" title="">102</a>]</cite> during inference, generating sets of N-grams occurring in the corpus based on the FM-Index <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib204" title="">204</a>]</cite>.
Self-Retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib97" title="">97</a>]</cite> proposed an architecture that fully integrates indexing, retrieval, and evaluation into a single large language model. By successively generating natural language indices and document segments, and performing self-evaluation to score and rank the generated documents.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparisons of representative generative retrieval methods, focusing on document identifier, training data augmentation, and training objective.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1" rowspan="2" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.2" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.2.1">Document Identifier</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T1.1.1.3" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.3.1">Training Data Augmentation</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T1.1.1.4" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.4.1">Training Objective</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.1.1">State</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.2.1">Data Type</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.3" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.3.1">Order</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.4" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.4.1">Sample Doc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.5" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.5.1">Doc2Query</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.6" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.6.1">Seq2seq</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.7" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.7.1">DocID</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.8" style="padding:1pt 7.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.2.8.1">Ranking</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.1.3.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.1.1" style="background-color:#F0F0F0;">GENRE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib29" title="">29</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.3.1" style="background-color:#F0F0F0;">Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.6.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.3.9.1" style="background-color:#F0F0F0;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4">
<td class="ltx_td ltx_align_left" id="S3.T1.1.4.1" style="padding:1pt 7.0pt;">DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.2" style="padding:1pt 7.0pt;">Static</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.3" style="padding:1pt 7.0pt;">Numeric</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.5" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.6" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.8" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.4.9" style="padding:1pt 7.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.5.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.1.1" style="background-color:#F0F0F0;">DynamicRetriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib86" title="">86</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.3.1" style="background-color:#F0F0F0;">Numeric</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.6.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.5.9.1" style="background-color:#F0F0F0;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6">
<td class="ltx_td ltx_align_left" id="S3.T1.1.6.1" style="padding:1pt 7.0pt;">SEAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib102" title="">102</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.2" style="padding:1pt 7.0pt;">Static</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.3" style="padding:1pt 7.0pt;">Text</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.5" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.8" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.9" style="padding:1pt 7.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.7.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.1.1" style="background-color:#F0F0F0;">DSI-QG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib87" title="">87</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.3.1" style="background-color:#F0F0F0;">Numeric</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.7.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.7.9.1" style="background-color:#F0F0F0;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8">
<td class="ltx_td ltx_align_left" id="S3.T1.1.8.1" style="padding:1pt 7.0pt;">NCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.2" style="padding:1pt 7.0pt;">Static</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.3" style="padding:1pt 7.0pt;">Numeric</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.5" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.6" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.8" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.8.9" style="padding:1pt 7.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.9.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.1.1" style="background-color:#F0F0F0;">Ultron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib32" title="">32</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.3.1" style="background-color:#F0F0F0;">Numeric/Text</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.9.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.9.9.1" style="background-color:#F0F0F0;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10">
<td class="ltx_td ltx_align_left" id="S3.T1.1.10.1" style="padding:1pt 7.0pt;">CorpusBrain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib41" title="">41</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.2" style="padding:1pt 7.0pt;">Static</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.3" style="padding:1pt 7.0pt;">Text</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.5" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.6" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.8" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.10.9" style="padding:1pt 7.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.11.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.1.1" style="background-color:#F0F0F0;">GenRet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib35" title="">35</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.2.1" style="background-color:#F0F0F0;">Learnable</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.3.1" style="background-color:#F0F0F0;">Numeric</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.11.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.11.9.1" style="background-color:#F0F0F0;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12">
<td class="ltx_td ltx_align_left" id="S3.T1.1.12.1" style="padding:1pt 7.0pt;">AutoTSG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib106" title="">106</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.2" style="padding:1pt 7.0pt;">Static</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.3" style="padding:1pt 7.0pt;">Text</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.4" style="padding:1pt 7.0pt;">Set</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.5" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.6" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.8" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.12.9" style="padding:1pt 7.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.13.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.1.1" style="background-color:#F0F0F0;">SE-DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib205" title="">205</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.3.1" style="background-color:#F0F0F0;">Text</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.5.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.6.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.13.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.13.9.1" style="background-color:#F0F0F0;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14">
<td class="ltx_td ltx_align_left" id="S3.T1.1.14.1" style="padding:1pt 7.0pt;">Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib201" title="">201</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.2" style="padding:1pt 7.0pt;">Static</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.3" style="padding:1pt 7.0pt;">Numeric</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.5" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.6" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.8" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.14.9" style="padding:1pt 7.0pt;">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.15.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.1.1" style="background-color:#F0F0F0;">LLM-URL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib103" title="">103</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.3.1" style="background-color:#F0F0F0;">Text</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.6.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.7.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.15.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.15.9.1" style="background-color:#F0F0F0;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.16">
<td class="ltx_td ltx_align_left" id="S3.T1.1.16.1" style="padding:1pt 7.0pt;">MINDER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib105" title="">105</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.2" style="padding:1pt 7.0pt;">Static</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.3" style="padding:1pt 7.0pt;">Text</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.5" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.6" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.8" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.16.9" style="padding:1pt 7.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.17" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.17.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.1.1" style="background-color:#F0F0F0;">LTRGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib89" title="">89</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.3.1" style="background-color:#F0F0F0;">Text</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.17.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.17.9.1" style="background-color:#F0F0F0;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.18">
<td class="ltx_td ltx_align_left" id="S3.T1.1.18.1" style="padding:1pt 7.0pt;">NOVO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib107" title="">107</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.18.2" style="padding:1pt 7.0pt;">Learnable</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.18.3" style="padding:1pt 7.0pt;">Text</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.18.4" style="padding:1pt 7.0pt;">Set</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.18.5" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.18.6" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.18.7" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.18.8" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.18.9" style="padding:1pt 7.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.19" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.19.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.1.1" style="background-color:#F0F0F0;">GenRRL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib34" title="">34</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.19.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.19.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.3.1" style="background-color:#F0F0F0;">Text</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.19.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.19.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.19.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.19.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.19.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.19.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.19.9.1" style="background-color:#F0F0F0;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.20">
<td class="ltx_td ltx_align_left" id="S3.T1.1.20.1" style="padding:1pt 7.0pt;">LMIndexer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib100" title="">100</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.20.2" style="padding:1pt 7.0pt;">Learnable</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.20.3" style="padding:1pt 7.0pt;">Numeric</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.20.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.20.5" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.20.6" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.20.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.20.8" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.20.9" style="padding:1pt 7.0pt;">-</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.21" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.21.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.1.1" style="background-color:#F0F0F0;">ASI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib36" title="">36</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.21.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.2.1" style="background-color:#F0F0F0;">Learnable</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.21.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.3.1" style="background-color:#F0F0F0;">Numeric</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.21.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.21.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.21.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.21.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.21.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.21.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.21.9.1" style="background-color:#F0F0F0;">-</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.22">
<td class="ltx_td ltx_align_left" id="S3.T1.1.22.1" style="padding:1pt 7.0pt;">RIPOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib101" title="">101</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.22.2" style="padding:1pt 7.0pt;">Learnable</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.22.3" style="padding:1pt 7.0pt;">Numeric</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.22.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.22.5" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.22.6" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.22.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.22.8" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.22.9" style="padding:1pt 7.0pt;">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.23" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left" id="S3.T1.1.23.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.1.1" style="background-color:#F0F0F0;">GLEN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib108" title="">108</a>]</cite></span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.23.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.2.1" style="background-color:#F0F0F0;">Learnable</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.23.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.3.1" style="background-color:#F0F0F0;">Text</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.23.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.23.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.23.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.23.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.23.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.8.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.23.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.23.9.1" style="background-color:#F0F0F0;">✓</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.24">
<td class="ltx_td ltx_align_left" id="S3.T1.1.24.1" style="padding:1pt 7.0pt;">DGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib90" title="">90</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.24.2" style="padding:1pt 7.0pt;">Static</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.24.3" style="padding:1pt 7.0pt;">Text</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.24.4" style="padding:1pt 7.0pt;">Sequence</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.24.5" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.24.6" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.24.7" style="padding:1pt 7.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.24.8" style="padding:1pt 7.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.24.9" style="padding:1pt 7.0pt;">✓</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.25" style="background-color:#F0F0F0;">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.1.25.1" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.1.1" style="background-color:#F0F0F0;">ListGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib91" title="">91</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.25.2" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.2.1" style="background-color:#F0F0F0;">Static</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.25.3" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.3.1" style="background-color:#F0F0F0;">Numeric</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.25.4" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.4.1" style="background-color:#F0F0F0;">Sequence</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.25.5" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.5.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.25.6" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.6.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.25.7" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.7.1" style="background-color:#F0F0F0;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.25.8" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.8.1" style="background-color:#F0F0F0;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.25.9" style="padding:1pt 7.0pt;"><span class="ltx_text" id="S3.T1.1.25.9.1" style="background-color:#F0F0F0;">✓</span></td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span class="ltx_text ltx_font_italic" id="S3.SS2.1.1">Design of Document Identifiers</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Another essential component of generative retrieval is document representation, also known as document identifiers (DocIDs), which act as the target outputs for the GR model. Accurate document representations are crucial as they enable the model to more effectively memorize document information, leading to enhanced retrieval performance. Table <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.T1" title="TABLE I ‣ 3.1.2 Model Structure ‣ 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">I</span></a> provides a detailed comparison of the states, data types, and order of DocIDs across numerous GR methods.
In the following sections, we will explore the design of DocIDs from two categories: numeric-based identifiers and text-based identifiers.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Numeric-based Identifiers</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">An intuitive method to represent documents is by using a single number or a series of numbers, referred to as DocIDs. Existing methods have designed both static and learnable DocIDs.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.1">Static DocIDs.</span>
Initially, DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite> introduced three numeric DocIDs to represent documents: (1) Unstructured Atomic DocID: a unique integer identifier is randomly assigned to each document, containing no structure or semantic information. (2) Naively Structured String DocID: treating random integers as divisible strings, implementing character-level DocID decoding to replace large softmax output layers. (3) Semantically Structured DocID: introducing semantic structure through hierarchical <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p2.1.m1.1"><semantics id="S3.SS2.SSS1.p2.1.m1.1a"><mi id="S3.SS2.SSS1.p2.1.m1.1.1" xref="S3.SS2.SSS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p2.1.m1.1b"><ci id="S3.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p2.1.m1.1d">italic_k</annotation></semantics></math>-means method, allowing semantically similar documents to share prefixes in their identifiers, effectively reducing the search space. Concurrently, DynamicRetriever <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib86" title="">86</a>]</cite> also built a model-based IR system based on unstructured atomic DocID.
Subsequently, Ultron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib32" title="">32</a>]</cite> encoded documents into a latent semantic space using BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib13" title="">13</a>]</cite>, and compressed vectors into a smaller semantic space via Product Quantization (PQ) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib206" title="">206</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib207" title="">207</a>]</cite>, preserving semantic information. Each document’s PQ code serves as its semantic identifier, with additional digits added to ensure the uniqueness of each DocID if duplicates exist. MEVI clusters documents using Residual Quantization (RQ) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib202" title="">202</a>]</cite>, forming semantically similar clusters for efficient search. It utilizes dual-tower and seq2seq model embeddings for a balanced performance in large-scale document retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p3.1.1">Learnable DocIDs.</span>
Unlike previous static DocIDs, GenRet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib35" title="">35</a>]</cite> proposed learnable document representations, encoding documents into short discrete DocIDs using a discrete autoencoder. GenRet transforms documents into DocIDs through an encoder, then reconstructs documents from DocIDs using a decoder, trained to minimize reconstruction error. Furthermore, GenRet optimized the model through progressive training and diversity clustering.To ensure that DocID embeddings can reflect document content, Tied-Atomic <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib99" title="">99</a>]</cite> proposed to enhance DocID embeddings to reflect document content accurately. Tied-Atomic links document text with token embeddings and employs contrastive loss for DocID creation, merging generative and dense retrieval’s expressiveness and efficiency.
LMIndexer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib100" title="">100</a>]</cite> and ASI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib36" title="">36</a>]</cite> learned optimal DocIDs through semantic indexing, with LMIndexer using a reparameterization mechanism for unified optimization, facilitating efficient retrieval by aligning semantically similar documents under common DocIDs. ASI extends this by establishing an end-to-end retrieval framework, incorporating semantic loss functions and reparameterization to enable joint training. This approach ensures that documents with similar content are assigned similar DocIDs, enhancing retrieval accuracy and efficiency.
Furthermore, RIPOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib101" title="">101</a>]</cite> treats the GR model as a dense encoder to encode document content and using a start token for decoding. It then splits these representations into vectors via RQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib202" title="">202</a>]</cite>, creating unique DocID sequences. Furthermore, RIPOR implements a prefix-guided ranking optimization, increasing relevance scores for prefixes of pertinent DocIDs through margin decomposed pairwise loss during decoding. This ensures higher scores for relevant DocID prefixes in beam search steps, enhancing search precision.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1">In summary, numeric-based document representations can utilize the embeddings of dense retrievers, obtaining semantically meaningful DocID sequences through methods such as <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p4.1.m1.1"><semantics id="S3.SS2.SSS1.p4.1.m1.1a"><mi id="S3.SS2.SSS1.p4.1.m1.1.1" xref="S3.SS2.SSS1.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p4.1.m1.1b"><ci id="S3.SS2.SSS1.p4.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p4.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p4.1.m1.1d">italic_k</annotation></semantics></math>-means, PQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib206" title="">206</a>]</cite>, and RQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib202" title="">202</a>]</cite>; they can also combine encoder-decoder GR models with bi-encoder DR models to achieve complementary advantages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib99" title="">99</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib94" title="">94</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Text-based Identifiers</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Text-based DocIDs have the inherent advantage of effectively leveraging the strong capabilities of pre-trained language models and offering better interpretability.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.1">Document Titles.</span>
The most straightforward text-based identifier is the document title, which requires each title to uniquely represent a document in the corpus, otherwise, it would not be possible to accurately retrieve a specific document. The Wikipedia corpus used in the KILT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib165" title="">165</a>]</cite> benchmark, due to its well-regulated manual annotation, has a unique title corresponding to each document. Thus, GENRE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib29" title="">29</a>]</cite>, based on the title as DocID and leveraging the generative model BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib18" title="">18</a>]</cite> and pre-built DocID prefix, achieved superior retrieval performance across 11 datasets in KILT. Following GENRE, GERE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib40" title="">40</a>]</cite>, CorpusBrain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib41" title="">41</a>]</cite>, Re3val <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib114" title="">114</a>]</cite>, and CorpusBrain++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib39" title="">39</a>]</cite> also based their work on title DocIDs for Wikipedia-based tasks. Notably, LLM-URL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib103" title="">103</a>]</cite> directly generated URLs using ChatGPT prompts, achieving commendable performance after removing invalid URLs.
However, in the web search scenario <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib162" title="">162</a>]</cite>, document titles in the corpus often have significant duplication and many meaningless titles, making it unfeasible to use titles alone as DocIDs. Thus, Ultron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib32" title="">32</a>]</cite> effectively addressed this issue by combining URLs and titles as DocIDs, uniquely identifying documents through keywords in web page URLs and titles.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p3.1.1">Sub-strings of Documents.</span>
To increase the flexibility of DocIDs, SEAL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib102" title="">102</a>]</cite> proposed a sub-string identifier, representing documents with any N-grams within them. Using FM-Index (a compressed full-text sub-string index) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib204" title="">204</a>]</cite>, SEAL could generate N-grams present in the corpus to retrieve all documents containing those N-grams, scoring and ranking documents based on the frequency of N-grams in each document and the importance of N-grams in the query. Following SEAL, various GR models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib104" title="">104</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib105" title="">105</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib89" title="">89</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib90" title="">90</a>]</cite> also utilized sub-string DocIDs and FM-Index during inference. For a more comprehensive representation of documents, MINDER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib105" title="">105</a>]</cite> proposed multi-view identifiers, including generated pseudo queries from document content via DocT5Query <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib200" title="">200</a>]</cite>, titles, and sub-strings. However, compared to single DocID methods, multi-DocID GR methods required more memory use and inference time. This multi-DocID approach was also used in LTRGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib89" title="">89</a>]</cite> and DGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib90" title="">90</a>]</cite>. Concurrently, SE-DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib205" title="">205</a>]</cite> also demonstrated the effectiveness of using pseudo queries alone as DocIDs.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p4.1.1">Term Sets.</span>
Unlike the sequential DocIDs described earlier, AutoTSG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib106" title="">106</a>]</cite> proposed a term set-based document representation, using keywords extracted from titles and content, rather than predefined sequences, allowing for retrieval of the target document as long as the generated term set is included in the document’s identifiers. During inference, AutoTSG designed a constrained greedy search to ensure each term generated stepwise was an effective identifier belonging to some document. Recently, PAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib98" title="">98</a>]</cite> also constructed DocIDs based on sets of key terms, disregarding the order of terms, which allows for document-level scoring to be approximated in decoding.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p5">
<p class="ltx_p" id="S3.SS2.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p5.1.1">Learnable DocIDs.</span>
Text-based identifiers can also be learnable. Similarly based on term-sets, NOVO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib107" title="">107</a>]</cite> proposed learnable continuous N-grams constituting term-set DocIDs. Through the Denoising Query Modeling (DQM) task, the model learned to generate queries from documents with noise, thereby implicitly learning to filter out document N-grams more relevant to queries. NOVO also utilized the retrieval task to update the semantic representations of document identifiers, i.e., improving the document’s semantic representation by updating N-gram embeddings.
Later, GLEN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib108" title="">108</a>]</cite> designs dynamic lexical DocIDs and is trained through a two-phase index learning strategy. Firstly, the keyword-based DocID assignment phase, GLEN defines DocIDs by extracting keywords from documents using self-supervised signals and learns them. Secondly, the ranking-based DocID refinement phase, GLEN learns dynamic DocIDs by directly integrating query-document relevance through two loss functions. During inference, GLEN utilizes collision-free inference, ranking documents using DocID weights without incurring additional overhead.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span class="ltx_text ltx_font_italic" id="S3.SS3.1.1">Incremental Learning on Dynamic Corpora</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Prior studies have focused on generative retrieval from static document corpora. However, in reality, the documents available for retrieval are continuously updated and expanded. To address this challenge, researchers have developed a range of methods to optimize GR models for adapting to dynamic corpora.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Optimizer and Document Rehearsal.</span>
At first, DSI++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib37" title="">37</a>]</cite> aims to address the incremental learning challenges within dynamic document corpora encountered by DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>. DSI++ modifies the training by optimizing flat loss basins through the Sharpness-Aware Minimization (SAM) optimizer, thereby stabilizing the learning process of the model. It also introduces generative memory by employing DocT5Query <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib200" title="">200</a>]</cite> to generate pseudo queries for documents in the existing corpus as training data augmentation, mitigating the forgetting issue of GR models.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Constrained Optimization</span>
Addressing the scenario of real-time addition of new documents, such as news or scientific literature IR systems, IncDSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib38" title="">38</a>]</cite> views the addition of new documents as a constrained optimization problem to find optimal representations for the new documents. This approach aims to (1) ensure new documents can be correctly retrieved by their relevant queries, and (2) maintain the retrieval performance of existing documents unaffected. IncDSI manages to add each document within approximately 20-50ms, significantly reducing the time and computational resources required compared to full model retraining, while maintaining competitive retrieval performance.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Incremental Product Quantization.</span>
CLEVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib109" title="">109</a>]</cite>, based on Product Quantization (PQ) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib206" title="">206</a>]</cite>, proposes Incremental Product Quantization (IPQ) for generating PQ codes as DocIDs for documents. Compared to traditional PQ methods, IPQ designs two adaptive thresholds to update only a subset of centroids instead of all, maintaining the indices of updated centroids constant. This method reduces computational costs and allows the system to adapt flexibly to new documents. To mitigate forgetting, CLEVER employs a memory-enhanced learning mechanism, maintaining a dynamic memory bank to store example documents similar to the new ones.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">Fine-tuning Adatpers for Specific Tasks.</span>
CorpusBrain++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib39" title="">39</a>]</cite> introduces the KILT++ benchmark for continuously updated KILT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib165" title="">165</a>]</cite> tasks and designs a dynamic architecture paradigm with a backbone-adapter structure. By fixing a shared backbone model to provide basic retrieval capabilities while introducing task-specific adapters to incrementally learn new documents for each task, it effectively avoids catastrophic forgetting. During training, CorpusBrain++ generates pseudo queries for new document sets and continues to pre-train adapters for specific tasks. Moreover, it employs document clustering based on semantic similarity and a retraining strategy to maintain memory of older documents by revisiting them.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span><span class="ltx_text ltx_font_italic" id="S3.SS4.1.1">Downstream Task Adaption</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Generative retrieval methods, apart from addressing retrieval tasks individually, have been tailored to various downstream generative tasks. These include fact verification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib208" title="">208</a>]</cite>, entity linking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib209" title="">209</a>]</cite>, open-domain QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib163" title="">163</a>]</cite>, dialogue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib210" title="">210</a>]</cite>, slot filling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib211" title="">211</a>]</cite>, among others, as well as knowledge-intensive tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib165" title="">165</a>]</cite>, code <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib212" title="">212</a>]</cite>, conversational QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib213" title="">213</a>]</cite>, and multi-modal retrieval scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib214" title="">214</a>]</cite>, demonstrating superior performance and efficiency. These methods are discussed in terms of separate training, joint training, and multi-modal generative retrieval.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Separate Training</h4>
<div class="ltx_para" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">For fact verification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib208" title="">208</a>]</cite>, which involve determining the correctness of input claims, GERE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib40" title="">40</a>]</cite> proposed using an encoder-decoder-based GR model to replace traditional indexing-based methods. Specifically, GERE first utilizes a claim encoder to encode input claims, and then generates document titles related to the claim through a title decoder to obtain candidate sentences for corresponding documents. Finally, an evidence decoder generates evidence sentence identifiers, resulting in improvements in time, memory consumption, and performance.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p2.1.1">Knowledge-Intensive Language Tasks.</span>
For Knowledge-Intensive Language Tasks (KILT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib165" title="">165</a>]</cite>, CorpusBrain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib41" title="">41</a>]</cite> introduced three pre-training tasks to enhance the model’s understanding of query-document relationships at various granularities: Internal Sentence Selection, Leading Paragraph Selection, and Hyperlink Identifier Prediction.
Similarly, UGR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib104" title="">104</a>]</cite> proposed using different granularities of N-gram DocIDs to adapt to various downstream tasks, unifying different retrieval tasks into a single generative form. UGR achieves this by letting the GR model learn prompts specific to tasks, generating corresponding document, passage, sentence, or entity identifiers.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1">Futhermore, DearDR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib111" title="">111</a>]</cite> is an efficient data-centric self-regressive document retrieval method. It utilizes remote supervision and self-supervised learning techniques, using Wikipedia page titles and hyperlinks as training data. The model samples sentences from Wikipedia documents as input and trains a self-regressive model to decode page titles or hyperlinks, or both, without the need for manually labeled data. The model achieves zero-shot implementation of Wikipedia-based fact verification tasks and further optimizes performance through fine-tuning.
Re3val <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib114" title="">114</a>]</cite> proposes a retrieval framework combining generative reordering and reinforcement learning. It first reorders retrieved page titles using context information obtained from a dense retriever, then optimizes the reordering using the REINFORCE algorithm to maximize rewards generated by constrained decoding. By improving page title reordering and context selection, Re3val achieves more accurate information retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p4">
<p class="ltx_p" id="S3.SS4.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p4.1.1">Multi-hop retrieval.</span>
In multi-hop retrieval tasks, which require iterative document retrievals to gather adequate evidence for answering a query, GMR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib110" title="">110</a>]</cite> proposed to employ language model memory and multi-hop memory to train a generative retrieval model, enabling it to memorize the target corpus and simulate real retrieval scenarios through constructing pseudo multi-hop query data, achieving dynamic stopping and efficient performance in multi-hop retrieval tasks.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p5">
<p class="ltx_p" id="S3.SS4.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p5.1.1">Code Retrieval.</span>
CodeDSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib112" title="">112</a>]</cite> is an end-to-end generative code search method that directly maps queries to pre-stored code samples’ DocIDs instead of generating new code. Similar to DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>, it includes indexing and retrieval stages, learning to map code samples and real queries to their respective DocIDs. CodeDSI explores different DocID representation strategies, including direct and clustered representation, as well as numerical and character representations, showing superior performance compared to traditional methods on 1K and 10K scales, with numerical DocIDs performing better than alphabetic ones.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS1.p6">
<p class="ltx_p" id="S3.SS4.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS1.p6.1.1">Conversational Question Answering.</span>
GCoQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib113" title="">113</a>]</cite> is a generative retrieval method for conversational QA systems. Unlike traditional methods relying on bi-encoder architecture and similarity matching, GCoQA directly generates DocIDs for passage retrieval. This method focuses on key information in the dialogue context at each decoding step, achieving more precise and efficient passage retrieval and answer generation, thereby improving retrieval performance and overall system efficiency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Joint Training</h4>
<div class="ltx_para" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">The methods in the previous section involve separately training generative retrievers and downstream task generators. However, due to the inherent nature of GR models as generative models, a natural advantage lies in their ability to be jointly trained with downstream generators to obtain a unified model for retrieval and generation tasks.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p2.1.1">Multi-decoder Structure.</span>
UniGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib115" title="">115</a>]</cite> proposes a unified generation framework to integrate retrieval and question answering tasks, bridging the gap between query input and generation targets using connectors generated by large language models. UniGen employs shared encoders and task-specific decoders for retrieval and question answering, introducing iterative enhancement strategies to continuously improve the performance of both tasks. It demonstrates superior performance on both web search and question answering tasks.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS2.p3">
<p class="ltx_p" id="S3.SS4.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS2.p3.1.1">Multi-task Training.</span>
Later, CorpusLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib42" title="">42</a>]</cite> introduces a unified language model that integrates GR, closed-book generation, and retrieval-augmented generation to handle various knowledge-intensive tasks. The model adopts a multi-task learning approach and introduces ranking-guided DocID decoding strategies and continuous generation strategies to improve retrieval and generation performance. In addition, CorpusLM designs a series of auxiliary DocID understanding tasks to deepen the model’s understanding of DocID semantics. Experimental results validate the effectiveness and potential of CorpusLM (T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib17" title="">17</a>]</cite> and Llama2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib23" title="">23</a>]</cite> variants) for knowledge-intensive language tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Multi-modal Generative Retrieval</h4>
<div class="ltx_para" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">Generative retrieval methods can also leverage multi-modal data such as text, images, etc., to achieve end-to-end multi-modal retrieval.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p2">
<p class="ltx_p" id="S3.SS4.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.p2.1.1">Tokenizing Images to DocID Sequences.</span>
At first, IRGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib43" title="">43</a>]</cite> transforms image retrieval problems into generative problems, predicting relevant discrete visual tokens, i.e., image identifiers, through a seq2seq model given a query image. Its core innovation lies in its semantic image tokenizer, which converts global image features into short sequences capturing high-level semantic information. Unlike traditional methods that handle feature extraction and ANN search separately, IRGen achieves end-to-end differentiable search, optimizing directly from the final retrieval target, thereby enhancing retrieval accuracy and efficiency.</p>
</div>
<div class="ltx_para" id="S3.SS4.SSS3.p3">
<p class="ltx_p" id="S3.SS4.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.SSS3.p3.1.1">Advanced Model Training and Structure.</span>
Later, GeMKR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib44" title="">44</a>]</cite> combines LLMs’ generation capabilities with visual-text features, designing a generative knowledge retrieval framework. It first guides multi-granularity visual learning using object-aware prefix tuning techniques to align visual features with LLMs’ text feature space, achieving cross-modal interaction. GeMKR then employs a two-step retrieval process: generating knowledge clues closely related to the query and then retrieving corresponding documents based on these clues. It aims to improve knowledge retrieval efficiency and accuracy in multi-modal scenarios.
GRACE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib44" title="">44</a>]</cite> achieves generative cross-modal retrieval method by assigning unique identifier strings to images and training multi-modal large language models (MLLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib215" title="">215</a>]</cite> to memorize the association between images and their identifiers. The training process includes (1) learning to memorize images and their corresponding identifiers, and (2) learning to generate the target image identifiers from textual queries. GRACE explores various types of image identifiers, including strings, numbers, semantics, identifiers, and atomic identifiers, to adapt to different memory and retrieval requirements.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span><span class="ltx_text ltx_font_italic" id="S3.SS5.1.1">Generative Recommender Systems</span>
</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">Recommendation systems, as an integral part of the information retrieval domain, are currently undergoing a paradigm shift from discriminative models to generative models. Generative recommendation systems do not require the computation of ranking scores for each item followed by database indexing, but instead accomplish item recommendations through the direct generation of IDs. In this section, several seminal works, including P5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib46" title="">46</a>]</cite>, GPT4Rec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib216" title="">216</a>]</cite>, TIGER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib48" title="">48</a>]</cite>, SEATER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib116" title="">116</a>]</cite>, IDGenRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib117" title="">117</a>]</cite>, LC-Rec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib118" title="">118</a>]</cite> and ColaRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib119" title="">119</a>]</cite>, are summarized to outline the development trends in generative recommendations.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">P5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib46" title="">46</a>]</cite> transforms various recommendation tasks into different natural language sequences, designing a universal, shared framework for recommendation completion. This method, by setting unique training objectives, prompts, and prediction paradigms for each recommendation domain’s downstream tasks, serves well as a backbone model, accomplishing various recommendation tasks through generated text. This approach demonstrates the viability and flexibility of generative models in recommendation systems.
In generative retrieval, effective indexing identifiers have been proven to significantly enhance the performance of generative methods. Similarly, TIGER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib48" title="">48</a>]</cite> initially learns a residual quantized autoencoder to generate semantically informative indexing identifiers for different items. It then trains a transformer-based encoder-decoder model with this semantically informative indexing identifier sequence to generate item identifiers for recommending the next item based on historical sequences.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.1">Focusing solely on semantic information and overlooking the collaborative filtering information under the recommendation context might limit the further development of generative models. Therefore, after generating semantic indexing identifiers similar to TIGER using a residual quantized autoencoder with uniform semantic mapping, LC-Rec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib118" title="">118</a>]</cite> also engages in a series of alignment tasks, including sequential item prediction, explicit index-language alignment, and recommendation-oriented implicit alignment. Based on the learned item identifiers, it integrates semantic and collaborative information, enabling large language models to better adapt to sequence recommendation tasks.</p>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<p class="ltx_p" id="S3.SS5.p4.1">IDGenRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib117" title="">117</a>]</cite> innovatively combines generative recommendation systems with large language models by using human language tokens to generate unique, concise, semantically rich and platform-agnostic texual identifiers for recommended items. The framework includes a text ID generator trained on item metadata with a diversified ID generation algorithm, and an alternating training strategy that optimizes both the ID generator and the LLM-based recommendation model for improved performance and accuracy in sequential recommendations. The zero-shot performance of IDGenRec is comparable to, or even surpasses, certain traditional recommendation models that rely on supervised training, highlighting its potential as a foundational model for recommendation systems.
SEATER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib116" title="">116</a>]</cite> designs a balanced k-ary tree-structured indexes, using a constrained k-means clustering method to recursively cluster vectors encoded from item texts, obtaining equal-length identifiers. Compared to the method proposed by DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite>, this balanced k-ary tree index maintains semantic consistency at every level. It then trains a Transformer-based encoder-decoder model and enhances the semantics of each level of indexing through contrastive learning and multi-task learning.
ColaRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib119" title="">119</a>]</cite> integrates collaborative filtering signals and content information by deriving generative item identifiers from a pretrained recommendation model and representing users via aggregated item content. Then it uses an item indexing generation loss and contrastive loss to align content-based semantic spaces with collaborative interaction spaces, enhancing the model’s ability to recommend items in an end-to-end framework.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Reliable Response Generation: Direct Information Accessing with Generative Language Models</span>
</h2>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="391" id="S4.F5.g1" src="x4.png" width="706"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
An Illustration of strategies for enhancing language models to generate user-centric and reliable responses, including model internal knowledge memorization and external knowledge augmentation.
</figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The rapid advancement of large language models has positioned them as a novel form of IR system, capable of generating reliable responses directly aligned with users’ informational needs. This not only saves the time users would otherwise spend on collecting and integrating information but also provides personalized, user-centric answers tailored to individual users.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">However, challenges remain in creating a grounded system that delivers faithful answers, such as hallucination, prolonged inference time, and high operational costs. This section will discuss strategies for constructing a faithful GenIR system, focusing on both optimizing the model internally and enhancing it with external knowledge.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span class="ltx_text ltx_font_italic" id="S4.SS1.1.1">Internal Knowledge Memorization</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To develop an user-friendly and reliable IR system, the generative model should be equipped with comprehensive internal knowledge. Optimization of the backbone generative model can be categorized into three aspects: structural enhancements, training strategies, and inference techniques.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Model Structure</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">With the advent of generative models, a variety of methods have been introduced to improve model structure and enhance generative reliability. We aim to discuss the crucial technologies contributing to this advancement in this subsection.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1">(1) Model Scaling</span> Model parameter scaling is a pivotal factor influencing performance. Contemporary language models predominantly employ the Transformer architecture, which has been observed that scaling both the model parameters and the training data enhances the model’s capacity to retain knowledge and capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib217" title="">217</a>]</cite>. For instance, in the GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib192" title="">192</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib62" title="">62</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib218" title="">218</a>]</cite> series and LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib23" title="">23</a>]</cite> family, models with larger parameter sizes tend to perform better on diverse downstream tasks. These include few-shot learning, language understanding, and generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib120" title="">120</a>]</cite>. In addition, scaling the model contributes to improved instruction-following capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib219" title="">219</a>]</cite>, enabling a more adept comprehension of user intent and generating responses that better satisfy user requests.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.1">(2) Model integration</span> Model integration is an effective and intuitive method to enhance the reliability of generated outputs by capitalizing on diverse strengths inherent in various models. This diversity arises from differences in training data and architectural frameworks. The predominant approach to model integration is the ’Mixture of Experts’ (MoE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib220" title="">220</a>]</cite>, which utilizes a gating mechanism to selectively activate sections of network parameters during inference.
MoE facilitates the amalgamation of the strengths of multiple expert models, greatly increasing the effective parameters of the model without inflating inference costs. Consequently, this leads to enhanced performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib221" title="">221</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib222" title="">222</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib223" title="">223</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib121" title="">121</a>]</cite>. This method also boasts impressive scalability: the overall efficacy of the MoE model is augmented in tandem with the expanding parameter volume and the number of expert models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib224" title="">224</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">In contrast to the MoE mechanism that involves training multiple expert models and a gating system from scratch, the LLM-Blender framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib225" title="">225</a>]</cite> employs a ranker and a fuser module to organize and merge answers from diverse LLMs. This approach is applicable to a wide array of large-scale models, including black-box types; however, it presents challenges associated with high deployment costs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Training and Inference</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">In the model training stage, methods to enhance the reliability of answers can be categorized into two aspects: training data optimization and training methods optimization.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p2.1.1">(1) Training Data Optimization</span> The quality of training data substantially affects the reliability of model outputs. Noise, misinformation, and incomplete information can disrupt the learning process of models, leading to hallucation and other issues. Many studies have focused on the processing of model training data. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib226" title="">226</a>]</cite> used GPT3.5 to artificially create textbooks filled with examples and language descriptions as training data. They minorly fine-tuned the model with a small amount of data, resulting in significant improvements on downstream tasks. LIMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib227" title="">227</a>]</cite> used dialogues from community forums to construct a small-scale fine-tuning dataset, enhancing the model’s conversation capabilities during the alignment phase.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">Concerning the fact that a large part of training data comes from automatically crawled internet page data, which contains many redundancies, some works have attempted to deduplicate the training dataset. Using a combination of suffix array <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib228" title="">228</a>]</cite> and MinHash <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib229" title="">229</a>]</cite> techniques, Lee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib65" title="">65</a>]</cite> designed an approximate matching algorithm to identify redundant content originating from the same source, hence reducing the proportion of answers the model directly reproduces from training data.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p4.1.1">(2) Training Methods Optimization</span> In addition to conventional training methods, additional techniques have been proposed to improve the factuality of model outputs. Given that the autoregressive training objective of language models can lead to blind imitation of training data, MixCL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib230" title="">230</a>]</cite> incorporated contrastive learning into the training objective of language models. It used an external knowledge base to identify correct knowledge snippets, incorporated contrastive learning loss function to reduce the generation probability of tokens in incorrect knowledge snippets, thereby enhancing model reliability.
CaliNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib66" title="">66</a>]</cite> utilized a contrastive method to assess erroneous knowledge learned by the model and fine-tuned the parameters of the FFN layer to rectify these errors. FactTune <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib123" title="">123</a>]</cite> incorporated factuality assessment during the RLHF phase. It used several automatic evaluation methods, including Factscore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib178" title="">178</a>]</cite>, to rank the factuality of model outputs. Consequently, it utilized the DPO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib231" title="">231</a>]</cite> method to teach the model factuality preference ranking.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p5">
<p class="ltx_p" id="S4.SS1.SSS2.p5.1">Apart from enhancing the internal knowledge reliability of the model during the training phase, the inference stage significantly impacts the reliability of answers. The overall inference process consists of two parts: the inputting of corresponding instructions by the user to the model and the decoding of the corresponding response tokens by the model. The approach to increase generation reliability during the inference stage can also be divided into two parts: prompting engineering and decoding strategy.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p6">
<p class="ltx_p" id="S4.SS1.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p6.1.1">(3) Prompt Engineering</span> Prompting method plays a vital role in guiding the model. A well-designed prompt can better promote the model’s internal capabilities to provide more accurate answers. The Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib232" title="">232</a>]</cite> prompting method guides the model to explicitly decompose the question into a reasoning chain during the decoding process and to reason out the answer step by step according to the reasoning chain. This method can improve the response accuracy for each sub-question, grounding the final question on the most accurate intermediate step, thus enhancing the reliability of the answer. Further, CoT-SC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib233" title="">233</a>]</cite> builds upon this by sampling multiple answers and choosing the most consistent one as the final answer. The Tree of Thoughts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib234" title="">234</a>]</cite> expands the single reasoning path of CoT to multiple paths and synthesizes the reasoning outcomes of these paths to arrive at the final answer.
The Chain-of-Verification (CoVE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib235" title="">235</a>]</cite> introduces a self-reflection mechanism within the prompts of LLM. The prompt causes the LLM to generate a draft response, and then formulates a validation plan for each statement within the response to check for factual inaccuracies. If errors are found, corrections are carried out, effectively enhancing the factual accuracy of the response.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p7">
<p class="ltx_p" id="S4.SS1.SSS2.p7.1">Besides, some prompting methods enhance the grounding of answers on more accurate content by prompting the model to output relevant internal knowledge. For example, RECITE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib125" title="">125</a>]</cite> and GenRead <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib124" title="">124</a>]</cite> utilize a sampling method to prompt the model to output certain relevant knowledge fragments, which are then used to bolster the question-answering process.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p8">
<p class="ltx_p" id="S4.SS1.SSS2.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p8.1.1">(4) Decoding Strategy</span> Decoding strategies are another critical factor influencing the reliability of model-generated responses. An appropriate decoding method can maintain the reliability and diversity of a model’s response. In common gready search or beam search approaches, models may produce responses that are high in general applicability but lack diversity, compromising the quality of the generated content. Nucleus sampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib236" title="">236</a>]</cite> is proposed to tackle this issue. By sampling within a set probability range for tokens, it can ensure better diversity for each generated token, and achieve results that balance variety and reliability. Building on this, Lee et al. introduced Factual-Nucleus Sampling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib237" title="">237</a>]</cite>, which employs a dynamic threshold that decays for subsequent tokens, ensuring that later parts of a generated sentence are not adversely affected by earlier parts that may lack factual content. Wan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib238" title="">238</a>]</cite> proposed a faithfulness-aware decoding method to enhance the faithfulness of the beam-search approach. This method incorporates a Ranker to reorder the generated beam sequences and a lookahead method to avoid selecting tokens that may lead to unfaithfulness in further generation, thereby improving the faithfulness of the results.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p9">
<p class="ltx_p" id="S4.SS1.SSS2.p9.1">Apart from directly modifying the decoding method, numerous studies have influenced the model’s decoding distribution by utilizing or modifying the information in the hidden layers. DoLa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib67" title="">67</a>]</cite> uses the distributional differences between the hidden layer and the output layer as the distribution for the next token, capturing factual knowledge or key terms that are newly learned by the output layer relative to the base network, hence increasing the likelihood of generating these terms. Inference-Time Intervention (ITI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib239" title="">239</a>]</cite> focuses on identifying attention heads highly correlated with response correctness and adjusting their orientations, then moderating the activation of these attention heads. This method achieves more truthful generation with minimal interference to the model.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p10">
<p class="ltx_p" id="S4.SS1.SSS2.p10.1">In practical applications, prompts often contain additional textual content to provide extra information, which may conflict with the model’s inherent knowledge, leading to hallucinations. Shi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib240" title="">240</a>]</cite> proposed the CAD method, which compares the output distributions before and after the addition of extra information. This reduces the weight given to the model’s own knowledge, thus avoiding conflicts between the two that might result in factual inaccuracies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Knowledge Updating</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">In real-life scenarios, information is constantly evolving, and therefore, the GenIR system needs to continuously acquire the latest knowledge to meet users’ information needs. Since the model’s knowledge storage is limited, knowledge updating is necessary to ensure more reliable generated responses. In this section, we will discuss existing methods for knowledge updating from two perspectives: incremental learning and knowledge editing.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p2.1.1">(1) Incremental Learning</span>.
Incremental learning refers to the ability of machine learning models to continuously learn new skills and tasks while retaining previously acquired knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib241" title="">241</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib242" title="">242</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib243" title="">243</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib244" title="">244</a>]</cite>. In the GenIR system, it is crucial to enable the language model to memorize the latest information while preventing the forgetting of previous knowledge.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p3">
<p class="ltx_p" id="S4.SS1.SSS3.p3.1">One approach is <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p3.1.1">Incremental Pre-training</span>, which does not rely on supervised data but continues pre-training on continuously updated corpora to alleviate catastrophic forgetting. For example, Baidu proposed the ERNIE 2.0 framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib68" title="">68</a>]</cite>, which enhances language understanding through continuous multi-task learning. Jang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib245" title="">245</a>]</cite> introduced the concept of Continual Knowledge Learning (CKL) to explore how LLMs can update and retain knowledge in the face of rapidly changing world knowledge. They also created new benchmarks and evaluation metrics such as FUAR. Cossu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib246" title="">246</a>]</cite> studied continual pre-training for language and vision and found that self-supervised or unsupervised pre-training methods are more effective in retaining previous knowledge compared to supervised learning. Additionally, Ke et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib69" title="">69</a>]</cite> proposed Domain Adaptive Pre-training (DAP-training) to improve the model’s adaptability to new domains while preventing the forgetting of previously learned knowledge using techniques like soft masking and contrastive learning. For domain-specific model construction, Xie et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib247" title="">247</a>]</cite> introduced FinPythia-6.9B, an efficient continual pre-training method specifically designed for constructing large-scale language models in the financial domain.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p4">
<p class="ltx_p" id="S4.SS1.SSS3.p4.1">On the other hand, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p4.1.1">Incremental Fine-tuning</span> involves training the model using only labeled data. For instance, Progressive Prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib248" title="">248</a>]</cite> creates new soft prompts for each new task and concatenates them in the order of their occurrence, effectively transferring knowledge and mitigating forgetting. DynaInst <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib126" title="">126</a>]</cite> improves the lifelong learning performance of pre-trained language models using parameter regularization and experience replay. It employs dynamic instance selection and task selection mechanisms to optimize learning efficiency under resource-constrained settings. Jang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib249" title="">249</a>]</cite> challenge traditional multi-task prompt fine-tuning methods by fine-tuning expert language models on individual tasks. Suhr et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib250" title="">250</a>]</cite> adopt a feedback-driven continual learning approach applicable to instruction-following agents. In human-agent interaction, users control the agent using natural language and convert feedback into immediate rewards through contextual bandits, further optimizing the learning process. Furthermore, O-LoRA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib251" title="">251</a>]</cite> demonstrates its superiority in the field of continual learning by learning new tasks in low-rank subspaces while maintaining orthogonality between these subspaces, significantly reducing interference between tasks. Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib252" title="">252</a>]</cite> propose a scalable language model that dynamically adjusts parameters based on different task requirements, effectively reducing the forgetting of previous task knowledge during new task learning.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p5">
<p class="ltx_p" id="S4.SS1.SSS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p5.1.1">(2) Knowledge Editing</span>.
Knowledge editing refers to the process of modifying and updating existing knowledge within language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib243" title="">243</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib253" title="">253</a>]</cite>, which is distinct from incremental learning that focuses on adapting to new domains or tasks. Knowledge editing methods, by editing the weights or layers of a model, can correct erroneous facts and incorporate new knowledge. This makes it an important technology before deploying GenIR systems.
There are primarily three paradigms for internal knowledge editing within language models: adding trainable parameters, locate-then-edit, and meta-learning.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p6">
<p class="ltx_p" id="S4.SS1.SSS3.p6.1">One method of <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p6.1.1">Adding Trainable Parameters</span> is by integrating new single neurons (patches) in the final feed-forward neural network (FFN) layer, such as T-Patcher <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib254" title="">254</a>]</cite> and CaliNET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib66" title="">66</a>]</cite>. These neurons serve as trainable parameters that introduce new errors and adjust the model’s behavior. Alternatively, discrete code-book modules are introduced in the middle layers of the language model, as in GRACE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib255" title="">255</a>]</cite>, to adjust and correct information.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p7">
<p class="ltx_p" id="S4.SS1.SSS3.p7.1">Moerover, the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p7.1.1">Locate-then-Edit</span> method first identifies the parameters corresponding to specific knowledge and then updates these targeted parameters directly. Common techniques involve identifying key-value pairs in the FFN matrix, known as ”knowledge neurons,” and updating them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib256" title="">256</a>]</cite>. Techniques like ROME <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib70" title="">70</a>]</cite> use causal mediation analysis to pinpoint areas needing editing, and MEMIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib257" title="">257</a>]</cite> builds on ROME to implement synchronized editing in various scenarios. Moreover, methods such as PMET <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib258" title="">258</a>]</cite> employ attention mechanisms for editing, while BIRD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib259" title="">259</a>]</cite> has introduced a bidirectional inverse relation modeling approach.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p8">
<p class="ltx_p" id="S4.SS1.SSS3.p8.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p8.1.1">Meta Learning</span>, which is another paradigm, uses hyper-networks to generate the necessary updates for model editing. KE (Knowledge Editor) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib127" title="">127</a>]</cite> predicts weight updates for each data point using a hyper-network. MEND <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib128" title="">128</a>]</cite>, by taking low-order decomposition of gradients as input, learns to rapidly edit language models to enhance performance. Additionally, MALMEN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib260" title="">260</a>]</cite> manages to separate the computations of hyper-networks and language models, facilitating the editing of multiple facts under a limited memory budget. These meta-learning mechanisms enable models to swiftly adapt to new knowledge and tasks and perform the necessary edits.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span class="ltx_text ltx_font_italic" id="S4.SS2.1.1">External Knowledge Augmentation</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Although large language models have demonstrated significant effectiveness in response generation, issues such as susceptibility to hallucinations, difficulty to handle in-domain knowledge and challenges with knowledge updating persist. Augmenting the model’s generative process with external knowledge sources can serves as an effectively address to tackle these issues.
Based on the form of external knowledge employed, these approaches can be classified into retrieval augmentation and tool augmentation.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Retrieval Augmentation</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Retrieval-Augmented Generation (RAG) enhance the response of generative models by combining them with a retrieval mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib261" title="">261</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib262" title="">262</a>]</cite>.
By querying a large collection of documents, information that is relevant to the input query can be fetched and integrated into input of the generative model. RAG enables generative models to be grounded in existing reliable knowledge, significantly improving the reliability of model generation.
Typically, a RAG method involves a retriever and a generator. Based on the interaction flow between these two, RAG methods can be divided into four categories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib263" title="">263</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">(1) Sequential RAG: </span> Sequential RAG operates on a linear progression, where the retriever first retrieves relevant information and the generator utilizes information to directly complete the response generation process.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">The basic form of sequential RAG is a ”Retrieve-Read” framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib129" title="">129</a>]</cite>. Early works complete the response generation process through joint training of retriever and generator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib264" title="">264</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib265" title="">265</a>]</cite> or separate training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib261" title="">261</a>]</cite>. Although it has achieved good results, there is a need for pre-training of the language model, which is expensive and not conducive to generalization. The In-Context RALM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib266" title="">266</a>]</cite> solves this problem by directly using the retrieval document as input, utilizing the model’s in-context learning capability to understand the document without any training.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">With the widespread adoption of large language models, most subsequent works is built on the foundation of a frozen generator. AAR method <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib267" title="">267</a>]</cite> fine-tuned a general retriever to adapt to the information acquisition preferences of the generative model. LLM-embedder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib268" title="">268</a>]</cite> using reward produced by LLM to train an embedding model dedicated to retrieval augmentation. ARL2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib130" title="">130</a>]</cite> leverages LLM to annotate relevance score in training set, and train a retriever using contrastive learning.
Several works introduce pre-retrieval and post-retrieval process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib263" title="">263</a>]</cite> into the sequential pipeline to enhance the overall efficiency. In pre-retrieval process, the RRR model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib129" title="">129</a>]</cite> introduces a rewriter module before the retriever. The rewriter is trained by utilizing generator’s feedback and can enable the retrieval system to provide more suitable information for generation.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">In post-retrieval process, information compressors are proposed to filter out irrelevant content from documents, avoiding misleading generator’s response. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib269" title="">269</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib270" title="">270</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib271" title="">271</a>]</cite>
RECOMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib272" title="">272</a>]</cite> introduce both abstractive and extractive compressors to generate concise summary for retrieved documents. LLMLingua <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib273" title="">273</a>]</cite> calculates the importance of each token based on the perplexity provided by the generative model, while retaining important tokens. Furthermore, LongLLMLingua <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib274" title="">274</a>]</cite>introduces query-aware compression on its basis and rerank the retrieved documents based on the calculated importance score to alleviate the loss in the middle phenomenon <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib269" title="">269</a>]</cite>. PRCA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib275" title="">275</a>]</cite> employs reinforcement learning to train a text compressor adaptable to black-box LLMs and various retrievers, functioning as a versatile plug-in for multiple scenarios.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p6">
<p class="ltx_p" id="S4.SS2.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p6.1.1">(2) Branching RAG:</span> In Branching RAG framework, the input query is processed across multiple pipelines, and each pipeline may involve the entire process in the sequential pipeline.
The outputs from all pipelines merged to form the final response. Compared to the sequential RAG process, this approach allows for finer-grained handling of the query or retrieval results.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p7">
<p class="ltx_p" id="S4.SS2.SSS1.p7.1">In pre-retrieval stage, TOC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib131" title="">131</a>]</cite> applies few-shot prompting to recursively decompose complex, ambiguous questions into clear, disambiguated sub-questions. These sub-questions are structured in a tree structure, with relevant documents retrieved for each. Using all valid question nodes, a long-form answer covering all sub-questions is generated. BlendFilter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib132" title="">132</a>]</cite> leverages prompts to enhance the original query through both internal and external knowledge. The augmented queries are then used to retrieve related documents, which are subsequently merged. When facing complex questions from users, this type of method can break down the questions and then answer them, improving the comprehensiveness of the response.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p8">
<p class="ltx_p" id="S4.SS2.SSS1.p8.1">In post-retrieval stage, REPLUG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib133" title="">133</a>]</cite> inputs each retrieved document along with the query into a generator to obtain a predicted probability distribution. These individual distributions are then amalgamated to yield the ultimate predictive probability distribution.
GenRead <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib124" title="">124</a>]</cite> prompts LLM to generate related documents on its own while utilizing the retriever to retrieve documents.
These two sets of documents are then merged as input. The versatility in designing prompts for LLM to create related documents allows for a broader coverage of content, thus enhancing the likelihood of meeting the user’s query requirements more effectively.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p9">
<p class="ltx_p" id="S4.SS2.SSS1.p9.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p9.1.1">(3) Conditional RAG: </span> The Conditional RAG framework adapts to various query types through distinct processes, improving the system’s flexibility. Since there can be knowledge conflict between the knowledge from retrieved documents and the generator’s own knowledge, RAG’s effectiveness isn’t consistent across all scenarios. To address this, common conditional RAG methods include a decision-making module that determines whether to engage the retrieval process for each specific query.
SKR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib134" title="">134</a>]</cite> builds a binary classification model trained on a dataset compiled from questions that LLMs can and cannot answer. This model is leveraged in inference stage to discern whether a given query should utilize retrieval. Labeling for the training data is gathered by directly prompting the model to ascertain if external knowledge is required for generation.
Self-DC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib72" title="">72</a>]</cite> employs the confidence score of the model’s response to determine the necessity of retrieval. Based on the confidence score, queries are categorized into three groups: unknown, uncertainty, and known. Queries deemed unknown are processed through a sequential RAG pipeline, while those with uncertainty are broken down into sub-questions to generate answers.
Rowen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib135" title="">135</a>]</cite> introduces a multilingual detection module that transforms the original question into semantically equivalent perturbed questions and collects their responses. The decision to retrieve is then based on measuring the consistency across these responses.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p10">
<p class="ltx_p" id="S4.SS2.SSS1.p10.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p10.1.1">(4) Loop RAG:</span>
Loop RAG involves deep interactions between the retriever and generator components. Owing to multi-turn retrieval and generation processes, accompanied by comprehensive interactions, it excels at handling complex and diverse input queries, yielding superior results in response generation.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p11">
<p class="ltx_p" id="S4.SS2.SSS1.p11.1">ITER-RETGEN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib136" title="">136</a>]</cite> introduce an iterative retrieval and generation framework. For a given query, it initially performs retrieval-augmented generation, retrieving relevant documents and then generate an answer. Subsequently, it undertakes generation-augmented retrieval, where it continues to retrieve based on the content generated in the previous step, and synthesizes text using the newly retrieved documents. This process of alternating the two steps is repeated a fixed number of times to produce the final answer.
IR-COT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib137" title="">137</a>]</cite> has a similar overall procedure to ITER-RETGEN, but its iteration pause is contingent on the model’s own generative process.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p12">
<p class="ltx_p" id="S4.SS2.SSS1.p12.1">FLARE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib76" title="">76</a>]</cite> introduces a strategy of concurrent retrieval as responses are being generated, in contrast to conducting retrieval only after a complete response has been produced. For each new sentence generated, the framework evaluates the need for retrieval based on the LLM’s confidence score for that sentence. If needed, it formulates a query based on the sentence to retrieve relevant information, and then regenerates the sentence. This method dynamically supplements the LLM with the necessary information, enhancing the reliability of the generated content.
COG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib276" title="">276</a>]</cite> models the generation process as a continual retrieval of segments from an external corpus and subsequent copying. The generator’s primary role therein is to produce certain conjunction words to preserve sentence fluency.
Self-RAG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib73" title="">73</a>]</cite> adds special tokens into the vocabulary to allow the generator to decide whether to retrieve, the importance of the retrieved document, and whether to perform a critique.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p13">
<p class="ltx_p" id="S4.SS2.SSS1.p13.1">Some works focus on deconstructing complex inquiries into sub-questions, addressing these individually to produce a more dependable response. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib277" title="">277</a>]</cite> guides LLM to decompose complex questions into sub-questions, responds to each individually using retrieved results, and ultimately synthesizes the answers to all sub-questions to form the final response. Building upon this, RET-Robust <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib278" title="">278</a>]</cite> incorporates a NLI model to evaluate whether the retrieved documents can substantiate the answers to sub-questions, thereby minimizing the risk of the LLM being misled by irrelevant information.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Tool Augmentation</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Although retrieval-augmented techniques have significantly improved upon the blind spots of generator’s self-knowledge, these methods struggle with the rapid and flexible update of information since they rely on the existence of information within an external corpus of documents. Tool augmentation, on the other hand, excels in addressing this issue by invoking various tools that allow for the timely acquisition and usage of the latest data, including finance, news, and more. Moreover, tool augmentation expands the scope of responses a model can offer, such as language translation, image generation, and other tasks, to more comprehensively meet users’ information retrieval needs.
There are four categories of tools that can be utilized to construct a more reliable information retrieval system:</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p2.1.1">(1) Search Engine:</span> Common search engine tools like Google Search and Bing Search help answer frequent and time-sensitive queries effectively. Self-Ask <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib277" title="">277</a>]</cite> initially decomposes complex questions into multiple sub-questions, then uses search engine to answer each sub-question, and finally generating a comprehensive answer to the complex question. ReAct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib74" title="">74</a>]</cite> embeds search engine calls into the model’s reasoning process, allowing the generative model to determine when to make calls and what queries to input for more flexible reasoning. New Bing can automatically search relevant information from Bing based on user input, yielding reliable and detailed answers, including citation annotations in the generated content.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">Some works have also built advanced conversational systems based on tools like search engines. Internet-Augmented Generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib279" title="">279</a>]</cite> enhances the quality of conversational replies by using search engine during conversations. LaMDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib26" title="">26</a>]</cite> and BlenderBot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib280" title="">280</a>]</cite> combine search engines with conversational agents, constantly accessing internet information to enrich conversation factualness. WebGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib51" title="">51</a>]</cite> and WebCPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib28" title="">28</a>]</cite> directly teache models to perform human-like browser operations by generating commands such as Search, Click, and Quote, facilitating the automated retrieval and acquisition of information.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p4.1.1">(2) Knowledge Graph (KG):</span> Compared to search engine, KG is particularly useful for extracting structured, explicit knowledge. Relevant knowledge from a knowledge graph can be extracted and used as a prompt input to enhance the generative process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib138" title="">138</a>]</cite>.
StructGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib75" title="">75</a>]</cite> introduced an iterative reading-and-reasoning framework where the model can access a knowledge graph through a well-designed interface, continually acquiring information and reasoning until an answer is obtained. RoG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib139" title="">139</a>]</cite> generates plausible reasoning paths based on a KG before executing each path in parallel and integrating the outcomes for a final answer. The concept behind ToG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib138" title="">138</a>]</cite> is similar to RoG, but instead of pre-planning reasoning paths, it allows the model to explore potential entities and links first and then reason, continuously assessing the feasibility of the reasoning paths.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p5">
<p class="ltx_p" id="S4.SS2.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p5.1.1">(3) API-based Tools:</span>
An important part of the tools is the real-world APIs, which enable the model to obtain information from specific data sources, such as real-time stock information, movie services, code interpreters and so on.
However, the multitude and diversity of APIs, coupled with the adherence to certain operational protocols, make the teaching of API usage to models a focal point of this area.
Toolformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib77" title="">77</a>]</cite> trains language models in self-supervised manner to automatically call APIs when needed.
It commences by utilizing prompts to generate API calls within the text, subsequently executes these calls, and filters out ineffective ones based on the execution results to form the final dataset. By employing standard language modeling objectives on this dataset, it is possible to train a model capable of autonomously invoking APIs across various downstream tasks without losing its inherent language modeling capabilities.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p6">
<p class="ltx_p" id="S4.SS2.SSS2.p6.1">RestGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib281" title="">281</a>]</cite> has formulated a comprehensive framework for prompting LLMs to invoke RESTful APIs, comprising an online planner, an API selector, and an executor. ToolLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib140" title="">140</a>]</cite> capitalizes on a substantial corpus of scraped APIs to build a dataset for fine-tuning. Moreover, Gorilla <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib282" title="">282</a>]</cite> introduces an information retriever to provide the model with reference API documentation, which facilitates the teaching of retrieval-based information utilization during fine-tuning. ToolkenGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib283" title="">283</a>]</cite> incorporates each tool as a new token into the vocabulary. During the training process, the model learns the representation of each token, enabling it to invoke APIs in an inference phase as naturally as generating regular text.
Beyond learning to invoke APIs, CREATOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib284" title="">284</a>]</cite> proposes a framework to prompt models to write code based on actual problems as a new tool implementation. The generated tools function through a code interpreter and have demonstrated impressive outcomes on complex mathematical reasoning tasks.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p7">
<p class="ltx_p" id="S4.SS2.SSS2.p7.1">A part of the work additionally supports multimodal inputs, further broadening the application scope of the models. AssistGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib141" title="">141</a>]</cite> offers a comprehensive framework that includes modules such as Planner, Executor, Inspector, and Learner, utilizing both language and code to enable more intricate inference processes. ViperGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib285" title="">285</a>]</cite>, by feeding CodeX with user queries and visual API information, generates corresponding Python code to invoke APIs, successfully completing complex visual tasks.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p8">
<p class="ltx_p" id="S4.SS2.SSS2.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p8.1.1">(4) Model-based Tools:</span> With the swift expansion of diverse AI communities(i.e., Huggingface, ModelScope, Github), various types of AI models have become readily accessible for use, serving as a pivotal tool in enhancing generative retrieval systems. These AI models encompass a wide array of tasks each accompanied by comprehensive model descriptions and usage examples. Owing to their extensive training on specific tasks, these models often exhibit exceptional performance in those tasks.
HuggingGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib142" title="">142</a>]</cite> employs ChatGPT as a controller to deconstruct user queries into a sequence of tasks, subsequently determining which models to invoke for task execution. Similarly, Visual ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib143" title="">143</a>]</cite> integrates a visual foundation model with Large Language Models (LLMs), leveraging ChatGPT as a prompt manager to mobilize various visual foundation models, like BLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib286" title="">286</a>]</cite> and ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib287" title="">287</a>]</cite>. It is adept at processing users’ image-based requests and is more efficient compared to multi-modal models.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span class="ltx_text ltx_font_italic" id="S4.SS3.1.1">Generating Response with Citation</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To build a reliable GenIR system, generating responses with citations is a promising approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib288" title="">288</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib289" title="">289</a>]</cite>. Citations allow users to clearly understand the source of each piece of knowledge in the response, not only enhancing trust in the GenIR system but also facilitating its widespread adoption. Existing methods can be divided into directly generating responses with citations and using a retrieval module to enhance the generated content.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Direct Generating Response with Citation</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">This method uses the model’s intrinsic memory to generate source citations without relying on the retrieval module.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p2">
<p class="ltx_p" id="S4.SS3.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p2.1.1">(1) Model Intrinsic Knowledge</span>.
Leveraging the capabilities of the language model itself, according-to prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib78" title="">78</a>]</cite> guides LLMs to more accurately cite information from the pre-training data to reduce the generation of false information. By adding phrases like ”according to Wikipedia” in the model’s prompts, the model is guided to cite the corresponding knowledge sources.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p3">
<p class="ltx_p" id="S4.SS3.SSS1.p3.1">Furthermore, to improve the quality of citations, several methods are proposed. Iterative Feedback Learning (IFL) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib79" title="">79</a>]</cite> first uses a critique model to assess the generated text, then provides targeted feedback based on the assessment results, guiding LLMs to iteratively improve their performance. Through this method, the IFL approach can effectively enhance the accuracy of citations, content correctness, and linguistic fluency of LLMs, while maintaining a high standard of information accuracy. In addition, Fierro et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib144" title="">144</a>]</cite> proposed a plan-based approach, defining the generation plan as a series of questions that serve as a blueprint for generating content and its organization. They introduced two attribution models utilizing the blueprint: an abstract model, where questions are generated from scratch; and an extractive model, where questions are directly copied from the input. Experiments show that planning consistently improves the quality of citations.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="684" id="S4.F6.g1" src="x5.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Generating response with citation and personal information assistant are also crucial approaches for building a reliable and user-centric GenIR system.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.SSS1.p4">
<p class="ltx_p" id="S4.SS3.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p4.1.1">(2) Incorporating Generative Retrieval</span>.
As envisioned by Metzler et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib16" title="">16</a>]</cite>, to build an expert-level reliable IR system, allowing the model to directly generate responses with citations is a promising approach. Users do not need to search for answers from a list of returned documents like traditional IR systems but can directly receive reliable responses tailored to their information needs. Moreover, the cited document is also generated by the model through the generative retrieval approach described in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3" title="3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3</span></a>, directly generating corresponding DocIDs.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p5">
<p class="ltx_p" id="S4.SS3.SSS1.p5.1">Utilizing generative retrieval, 1-PAGER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib146" title="">146</a>]</cite> combines answer generation and evidence retrieval. 1-PAGER gradually generating N-gram DocIDs through constrained decoding using FM-Index <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib204" title="">204</a>]</cite>, thereby partitioning the retrieval corpus, selecting documents, and generating response step by step. 1-PAGER is comparable to existing retrieval-then-read methods in retrieval and answer accuracy and superior to pure closed-book QA models because it attributes predictions to specific evidence corpora. It provides a new scheme for integrating retrieval into seq2seq response generation.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS1.p6">
<p class="ltx_p" id="S4.SS3.SSS1.p6.1">Recently, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib147" title="">147</a>]</cite> proposes a source-aware training method. Specifically, they teach the model to associate DocIDs with knowledge during pre-training, then provide citations of supporting evidence during instruction tuning. Experiments have shown that this method can effectively achieve knowledge attribution of pre-training data, enhancing the verifiability of LLMs, and providing a promising research direction for building a trustworthy GenIR system.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Retrieval-based Response with Citation</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">To enhance the accuracy of citations, several methods have been developed based on retrieval techniques to fetch relevant documents, thereby improving the quality of responses with embedded citations.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p2">
<p class="ltx_p" id="S4.SS3.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p2.1.1">(1) Citation within Generation</span>.
Following retrieval, models directly generate responses that include citations. Initially, systems like WebGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib51" title="">51</a>]</cite>, LaMDA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib26" title="">26</a>]</cite>, and WebBrain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib52" title="">52</a>]</cite> were developed. These methods utilize web pages or Wikipedia to construct large-scale pre-training datasets, teaching models how to generate responses with citations.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p3">
<p class="ltx_p" id="S4.SS3.SSS2.p3.1">Subsequently, more advanced strategies for citation generation were proposed. For instance, Search-in-the-Chain (SearChain) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib149" title="">149</a>]</cite> introduces a method where it first generates a reasoning chain (Chain-of-Query, CoQ) through prompts to LLMs. The retrieval module interacts with each node of the CoQ for verification and completion tasks. After the interaction, SearChain performs tracing operations, generating the entire reasoning process and marking citations for each step of the inference.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p4">
<p class="ltx_p" id="S4.SS3.SSS2.p4.1">Following retrieval and answer generation, LLatrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib80" title="">80</a>]</cite> suggests continuously improving the retrieval results through an iterative updating process, verifying whether the retrieved documents can adequately support the generated answers until the verification is satisfied. Later, AGREE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib290" title="">290</a>]</cite> utilizes a Natural Language Inference (NLI) model to verify the consistency between the LLM-generated answers and the retrieved documents, employing a Test-Time Adaptation (TTA) strategy that allows LLMs to actively search and cite the most current information during the generation process, thus enhancing the accuracy and reliability of the responses. VTG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib151" title="">151</a>]</cite> integrates an evolved memory system and a dual-layer validator for generating verifiable text, combining long-term and short-term memories to adapt to dynamically changing content focuses and using an NLI model to evaluate the logical support between claims and potential evidences.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p5">
<p class="ltx_p" id="S4.SS3.SSS2.p5.1">Based on the graph of thoughts (GoT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib291" title="">291</a>]</cite>, HGOT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib292" title="">292</a>]</cite> improves context learning in retrieval-augmented settings by constructing a hierarchical GoT. This method leverages the LLM’s planning capabilities to break down complex queries into smaller sub-queries and introduces a scoring mechanism to assess the quality of retrieved paragraphs.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p6">
<p class="ltx_p" id="S4.SS3.SSS2.p6.1">Employing reinforcement learning, Huang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib293" title="">293</a>]</cite> introduce a fine-grained reward mechanism to train language models, allocating specific rewards for each generated sentence and citation, thus teaching models how to accurately cite external information sources. This approach uses rejection sampling and reinforcement learning algorithms to enhance model performance in generating citation-inclusive text through localized and specialized reward signals. APO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib81" title="">81</a>]</cite> reimagines the task of attributive text generation as a preference learning problem, automatically generating a large number of preference data pairs to reduce the cost of manual annotation. Through progressive preference optimization and experience replay techniques, this method reinforces the model’s preference signals at a fine-grained level, while avoiding overfitting and text degradation caused by automatic data generation.</p>
</div>
<div class="ltx_para" id="S4.SS3.SSS2.p7">
<p class="ltx_p" id="S4.SS3.SSS2.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.SSS2.p7.1.1">(2) Citation after Generation</span>.
This approach involves models first generating a response, then adding citations through models like NLI. RARR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib148" title="">148</a>]</cite> improves attributability by automatically finding external evidence for the language model’s output and post-editing to correct content while preserving the original output as much as possible. This method combines a few training examples, LLM, and retrieval to enhance its attribution capabilities without altering the existing model. Following that, PURR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib294" title="">294</a>]</cite> employs an unsupervised learning method by allowing LLMs to generate text noise themselves, then training an editor to eliminate this noise, achieving an efficient editing process. PURR leverages the generative capabilities of language models to create training data, which not only improves attribution performance but also significantly speeds up the generation process. CEG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib152" title="">152</a>]</cite> searches for supporting documents related to generated content and uses an NLI-based citation generation module to ensure each statement is supported by citations. ”Attribute First, then Generate” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib295" title="">295</a>]</cite> decomposes the generation process, first selecting relevant source text details and then generating based on these details, achieving localized attributability in text generation. This method offers a finer granularity of attribution, ensuring high-quality text with each sentence supported by a clear source, greatly reducing the workload of manual fact-checking.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span><span class="ltx_text ltx_font_italic" id="S4.SS4.1.1">Personal Information Assistant</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">The core of the GenIR system is the user, so understanding user intent is crucial. Researchers have explored various methods like personalized search <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib296" title="">296</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib297" title="">297</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib298" title="">298</a>, <span class="ltx_ref ltx_missing_citation ltx_ref_self">liu2023how_to_personalize</span>]</cite>, dialogue <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib153" title="">153</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib299" title="">299</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib82" title="">82</a>]</cite> and recommender <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib300" title="">300</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib301" title="">301</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib302" title="">302</a>]</cite> systems to explore the users’ interests. Specifically, personalized information assistants aims to better understand users’ personalities and preferences, generating personalized responses to better meet their information needs. This section reviews the progress in research on personalized dialogue and domain-specific personalization.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Personalized Dialogue System</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1">To better understand user needs, researchers have explored two main approaches: personalized prompt design and model fine-tuning.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.p2.1.1">(1) Personalized Prompt</span>.
For personalized prompt design, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib300" title="">300</a>]</cite> and Dai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib301" title="">301</a>]</cite> input users’ interaction history and rating history into ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib3" title="">3</a>]</cite> for in-context learning, effectively generating personalized responses. LaMP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib303" title="">303</a>]</cite> enhances the language model’s personalized output by retrieving personalized history from user profiles. Using long-term history, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib304" title="">304</a>]</cite> designs prompts describing users’ long-term interests, needs, and goals for input into LLMs. BookGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib302" title="">302</a>]</cite> uses LLM prompts, interactive querying methods, and result verification frameworks to obtain personalized book recommendations. PerSE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib305" title="">305</a>]</cite> infers preferences from several reviews by a specific reviewer and provides personalized evaluations for new story inputs.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p3">
<p class="ltx_p" id="S4.SS4.SSS1.p3.1">Using prompt rewriting, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib306" title="">306</a>]</cite> proposes a method combining supervised and reinforcement learning to better generate responses from frozen LLMs. Similarly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib307" title="">307</a>]</cite> rewrites user input prompts using extensive user text-to-image interaction history to align better with expected visual outputs.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p4">
<p class="ltx_p" id="S4.SS4.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS1.p4.1.1">(2) Personalized Fine-tuning</span>.
This line of work focuses on fine-tuning models for personalized response generation. To generate more engaging dialogues based on users’ personalities and interests, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib153" title="">153</a>]</cite> introduced a dataset with 5 million personas for dialogues, Persona-Chat, to train models to produce more personalized and appealing conversations. Mazaré et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib308" title="">308</a>]</cite> created a dataset of over 700 million conversations extracted from Reddit, demonstrating the effectiveness of training dialogue models on a large-scale personal profile dataset. <math alttext="\mathcal{P}^{2}" class="ltx_Math" display="inline" id="S4.SS4.SSS1.p4.1.m1.1"><semantics id="S4.SS4.SSS1.p4.1.m1.1a"><msup id="S4.SS4.SSS1.p4.1.m1.1.1" xref="S4.SS4.SSS1.p4.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS4.SSS1.p4.1.m1.1.1.2" xref="S4.SS4.SSS1.p4.1.m1.1.1.2.cmml">𝒫</mi><mn id="S4.SS4.SSS1.p4.1.m1.1.1.3" xref="S4.SS4.SSS1.p4.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS1.p4.1.m1.1b"><apply id="S4.SS4.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS4.SSS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.SSS1.p4.1.m1.1.1.1.cmml" xref="S4.SS4.SSS1.p4.1.m1.1.1">superscript</csymbol><ci id="S4.SS4.SSS1.p4.1.m1.1.1.2.cmml" xref="S4.SS4.SSS1.p4.1.m1.1.1.2">𝒫</ci><cn id="S4.SS4.SSS1.p4.1.m1.1.1.3.cmml" type="integer" xref="S4.SS4.SSS1.p4.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS1.p4.1.m1.1c">\mathcal{P}^{2}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS1.p4.1.m1.1d">caligraphic_P start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math>Bot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib82" title="">82</a>]</cite> generates more personalized and consistent dialogues by simulating the perception of personalities between conversation participants. DHAP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib299" title="">299</a>]</cite> designs a novel Transformer model structure to automatically learn implicit user profiles from users’ dialogue history without explicit personal information. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib154" title="">154</a>]</cite> propose a generative segmentation memory network to integrate diverse personal information. Fu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib309" title="">309</a>]</cite> developed a variational approach to model the relationship between personal memory and knowledge selection, with a bidirectional learning mechanism allowing mutual learning between personal memory fragments and knowledge selection.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p5">
<p class="ltx_p" id="S4.SS4.SSS1.p5.1">Using reinforcement learning, Cheng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib310" title="">310</a>]</cite> collected a domain-specific preference (DSP) dataset and proposed a three-stage reward model learning scheme, including base language model training, general preference fine-tuning, and customized preference fine-tuning. Jang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib155" title="">155</a>]</cite> developed a method called ”Personalized Soups,” first optimizing multiple policy models with different preferences independently using PPO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib311" title="">311</a>]</cite>, then dynamically combining these models’ parameters during the inference stage.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p6">
<p class="ltx_p" id="S4.SS4.SSS1.p6.1">Using retrieval-enhanced methods, LAPDOG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib312" title="">312</a>]</cite> retrieves relevant information from story documents to enhance personal profiles and generate better personalized responses. To effectively utilize multiple knowledge sources, SAFARI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib83" title="">83</a>]</cite> leverages LLMs’ capabilities in planning, understanding, and integrating knowledge under supervised and unsupervised training settings, effectively generating responses consistent with character settings and knowledge-enhanced. Inspired by writing education, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib313" title="">313</a>]</cite> proposed a multi-stage, multi-task framework to teach LLMs to generate personalized responses, including retrieval, ranking, summarization, synthesis, and generation. For subjective tasks, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib314" title="">314</a>]</cite> studied the superior performance of personalized fine-tuning in subjective text perception tasks compared to non-personalized models.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p7">
<p class="ltx_p" id="S4.SS4.SSS1.p7.1">To achieve a personalized information assistant for every user, OPPU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib156" title="">156</a>]</cite> uses personalized PEFT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib196" title="">196</a>]</cite> to store user-specific behavioral patterns and preferences, showing superior performance in handling changes in user behavior, modeling users with different activity levels, and among different PEFT methods.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p8">
<p class="ltx_p" id="S4.SS4.SSS1.p8.1">For multimodal scenarios, PMG <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib315" title="">315</a>]</cite> proposes a personalized multi-modal generation method that transforms user behavior into natural language, allowing LLMs to understand and extract user preferences.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Domain-specific Personalization</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Understanding users’ personalized information needs, the GenIR system has broad applications across various domains such as healthcare, academia, education, recipes, etc.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p2">
<p class="ltx_p" id="S4.SS4.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.p2.1.1">(1) Healthcare</span>.
In AI-assisted healthcare, personalization plays a crucial role. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib316" title="">316</a>]</cite> utilize few-shot tuning to process and infer based on time-series physiological and behavioral data. Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib317" title="">317</a>]</cite> implement specific medical diagnosis identification in databases and prospective diagnostic assistance using prompts from ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib3" title="">3</a>]</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib218" title="">218</a>]</cite>. Subsequently, Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib318" title="">318</a>]</cite> propose an LLM for traditional Chinese medicine called Zhongjing, based on LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib64" title="">64</a>]</cite>, which undergoes a complete training process from continued pre-training, supervised fine-tuning to reinforcement learning with human feedback (RLHF) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib22" title="">22</a>]</cite>. Abbasian et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib319" title="">319</a>]</cite> introduce an open-source LLM-based conversational health agent framework called openCHA, which collects necessary information through specific actions and generates personalized responses. MedAgents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib320" title="">320</a>]</cite> propose a multidisciplinary collaboration (MC) framework where LLM-based agents engage in multi-round cooperative discussions through role-playing to enhance the model’s expertise and reasoning capabilities.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p3">
<p class="ltx_p" id="S4.SS4.SSS2.p3.1">For mental healthcare, Mental-LLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib84" title="">84</a>]</cite> presents a new framework for using LLMs to predict mental health from social media text data, with prompting-based and finetuning-based methods for real-time monitoring and prediction of psychological issues such as depression and anxiety. Lai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib321" title="">321</a>]</cite> introduce a psychological consultation aid called Psy-LLM, combining pre-trained LLMs with real psychologist Q&amp;As and a large corpus of psychological articles.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p4">
<p class="ltx_p" id="S4.SS4.SSS2.p4.1">For medication suggestions, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib157" title="">157</a>]</cite> propose a framework called PharmacyGPT for generating personalized patient groups, formulating medication plans, and predicting patient outcomes.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p5">
<p class="ltx_p" id="S4.SS4.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.p5.1.1">(2) Academic</span>.
In the academic domain, RevGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib85" title="">85</a>]</cite> can automatically generate controllable and personalized user reviews based on users’ emotional tendencies and stylistic information. For writing assistants, Porsdam et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib322" title="">322</a>]</cite> explore the personalized enhancement of academic writing using LLMs like GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib62" title="">62</a>]</cite>, showing higher quality in format, style, overall quality, and novelty after training with published academic works of three authors. Similarly, to address the lack of personalized outputs for author communication styles and expertise in current LLM outputs, Mysore et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib158" title="">158</a>]</cite> propose Pearl, a personalized LLM writing assistant trained on data selected from users’ historical documents to optimize personalized text generation; they also develop a new KL divergence training objective to help retrievers more accurately track document contributions to personalized generation.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p6">
<p class="ltx_p" id="S4.SS4.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.p6.1.1">(3) Education</span>.
In the education domain, Cui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib323" title="">323</a>]</cite> propose a new adaptive and personalized exercise generation method that dynamically adjusts exercise difficulty to match students’ learning progress by combining knowledge tracing and controlled text generation. EduChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib159" title="">159</a>]</cite> learns education-specific functionalities, such as article evaluation and emotional support, through pre-training on educational corpora and fine-tuning on customized instructions, addressing the issues of delayed knowledge updates and lack of educational expertise traditionally faced by LLMs in the educational sector.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS2.p7">
<p class="ltx_p" id="S4.SS4.SSS2.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS2.p7.1.1">(4) Other Domains</span>.
For recipe generation tasks, traditional methods fail to consider users’ personal tastes and unfamiliar dishes. To address this, Majumder et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib324" title="">324</a>]</cite> propose a personalized generation model based on users’ historical recipe consumption. Using an encoder-decoder structure, the model utilizes users’ past recipe data to generate responses, enhancing the personalization and adaptability of the outputs. Subsequently, for personalized headline generation, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib325" title="">325</a>]</cite> simulate users’ interests based on their news browsing history and generate news headlines based on these interests. Salemi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib303" title="">303</a>]</cite> propose the LaMP benchmark, which includes various personalized generation tasks such as personalized news headline generation, personalized academic title generation, personalized email subject generation, and personalized tweet rewriting. Additionally, for personalized assistance with home cleaning robots in determining the correct storage locations for items, TidyBot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib326" title="">326</a>]</cite> proposes using LLMs to generalize from a small number of user-provided examples to infer broadly applicable user preference rules.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Evaluation</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section will provide a range of evaluation metrics and benchmarks for generative information retrieval methods, along with analysis and discussions on their performance.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span class="ltx_text ltx_font_italic" id="S5.SS1.1.1">Evaluation for Generative Document Retrieval</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1 </span>Metrics</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">In this section, we will discuss several core metrics for evaluating GR methods, including Recall <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib327" title="">327</a>]</cite>, R-Precision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib327" title="">327</a>]</cite>, Mean Reciprocal Rank (MRR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib160" title="">160</a>]</cite>, Mean Average Precision (MAP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib327" title="">327</a>]</cite>, and Normalized Discounted Cumulative Gain (nDCG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib161" title="">161</a>]</cite>. These metrics provide different perspectives on the effectiveness of a GR system, including its accuracy, efficiency, and the relevance of its results.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.2"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p2.2.1">Recall</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib327" title="">327</a>]</cite> is a metric that measures the proportion of relevant documents retrieved by the search system. For a given cutoff point <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.1.m1.1"><semantics id="S5.SS1.SSS1.p2.1.m1.1a"><mi id="S5.SS1.SSS1.p2.1.m1.1.1" xref="S5.SS1.SSS1.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.1.m1.1b"><ci id="S5.SS1.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.1.m1.1d">italic_k</annotation></semantics></math>, the recall <math alttext="\text{Recall}@k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.2.m2.1"><semantics id="S5.SS1.SSS1.p2.2.m2.1a"><mrow id="S5.SS1.SSS1.p2.2.m2.1.1" xref="S5.SS1.SSS1.p2.2.m2.1.1.cmml"><mtext id="S5.SS1.SSS1.p2.2.m2.1.1.2" xref="S5.SS1.SSS1.p2.2.m2.1.1.2a.cmml">Recall</mtext><mo id="S5.SS1.SSS1.p2.2.m2.1.1.1" xref="S5.SS1.SSS1.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.2.m2.1.1.3" mathvariant="normal" xref="S5.SS1.SSS1.p2.2.m2.1.1.3.cmml">@</mi><mo id="S5.SS1.SSS1.p2.2.m2.1.1.1a" xref="S5.SS1.SSS1.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.2.m2.1.1.4" xref="S5.SS1.SSS1.p2.2.m2.1.1.4.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.2.m2.1b"><apply id="S5.SS1.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p2.2.m2.1.1"><times id="S5.SS1.SSS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.SSS1.p2.2.m2.1.1.1"></times><ci id="S5.SS1.SSS1.p2.2.m2.1.1.2a.cmml" xref="S5.SS1.SSS1.p2.2.m2.1.1.2"><mtext id="S5.SS1.SSS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.SSS1.p2.2.m2.1.1.2">Recall</mtext></ci><ci id="S5.SS1.SSS1.p2.2.m2.1.1.3.cmml" xref="S5.SS1.SSS1.p2.2.m2.1.1.3">@</ci><ci id="S5.SS1.SSS1.p2.2.m2.1.1.4.cmml" xref="S5.SS1.SSS1.p2.2.m2.1.1.4">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.2.m2.1c">\text{Recall}@k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.2.m2.1d">Recall @ italic_k</annotation></semantics></math> is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E8">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Recall}@k=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{ret_{q,k}}{rel_{q}}," class="ltx_Math" display="block" id="S5.E8.m1.5"><semantics id="S5.E8.m1.5a"><mrow id="S5.E8.m1.5.5.1" xref="S5.E8.m1.5.5.1.1.cmml"><mrow id="S5.E8.m1.5.5.1.1" xref="S5.E8.m1.5.5.1.1.cmml"><mrow id="S5.E8.m1.5.5.1.1.2" xref="S5.E8.m1.5.5.1.1.2.cmml"><mtext id="S5.E8.m1.5.5.1.1.2.2" xref="S5.E8.m1.5.5.1.1.2.2a.cmml">Recall</mtext><mo id="S5.E8.m1.5.5.1.1.2.1" xref="S5.E8.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S5.E8.m1.5.5.1.1.2.3" mathvariant="normal" xref="S5.E8.m1.5.5.1.1.2.3.cmml">@</mi><mo id="S5.E8.m1.5.5.1.1.2.1a" xref="S5.E8.m1.5.5.1.1.2.1.cmml">⁢</mo><mi id="S5.E8.m1.5.5.1.1.2.4" xref="S5.E8.m1.5.5.1.1.2.4.cmml">k</mi></mrow><mo id="S5.E8.m1.5.5.1.1.1" xref="S5.E8.m1.5.5.1.1.1.cmml">=</mo><mrow id="S5.E8.m1.5.5.1.1.3" xref="S5.E8.m1.5.5.1.1.3.cmml"><mfrac id="S5.E8.m1.1.1" xref="S5.E8.m1.1.1.cmml"><mn id="S5.E8.m1.1.1.3" xref="S5.E8.m1.1.1.3.cmml">1</mn><mrow id="S5.E8.m1.1.1.1.3" xref="S5.E8.m1.1.1.1.2.cmml"><mo id="S5.E8.m1.1.1.1.3.1" stretchy="false" xref="S5.E8.m1.1.1.1.2.1.cmml">|</mo><mi id="S5.E8.m1.1.1.1.1" xref="S5.E8.m1.1.1.1.1.cmml">Q</mi><mo id="S5.E8.m1.1.1.1.3.2" stretchy="false" xref="S5.E8.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S5.E8.m1.5.5.1.1.3.1" xref="S5.E8.m1.5.5.1.1.3.1.cmml">⁢</mo><mrow id="S5.E8.m1.5.5.1.1.3.2" xref="S5.E8.m1.5.5.1.1.3.2.cmml"><munderover id="S5.E8.m1.5.5.1.1.3.2.1" xref="S5.E8.m1.5.5.1.1.3.2.1.cmml"><mo id="S5.E8.m1.5.5.1.1.3.2.1.2.2" movablelimits="false" xref="S5.E8.m1.5.5.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="S5.E8.m1.5.5.1.1.3.2.1.2.3" xref="S5.E8.m1.5.5.1.1.3.2.1.2.3.cmml"><mi id="S5.E8.m1.5.5.1.1.3.2.1.2.3.2" xref="S5.E8.m1.5.5.1.1.3.2.1.2.3.2.cmml">q</mi><mo id="S5.E8.m1.5.5.1.1.3.2.1.2.3.1" xref="S5.E8.m1.5.5.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S5.E8.m1.5.5.1.1.3.2.1.2.3.3" xref="S5.E8.m1.5.5.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mrow id="S5.E8.m1.2.2.1.3" xref="S5.E8.m1.2.2.1.2.cmml"><mo id="S5.E8.m1.2.2.1.3.1" stretchy="false" xref="S5.E8.m1.2.2.1.2.1.cmml">|</mo><mi id="S5.E8.m1.2.2.1.1" xref="S5.E8.m1.2.2.1.1.cmml">Q</mi><mo id="S5.E8.m1.2.2.1.3.2" stretchy="false" xref="S5.E8.m1.2.2.1.2.1.cmml">|</mo></mrow></munderover><mfrac id="S5.E8.m1.4.4" xref="S5.E8.m1.4.4.cmml"><mrow id="S5.E8.m1.4.4.2" xref="S5.E8.m1.4.4.2.cmml"><mi id="S5.E8.m1.4.4.2.4" xref="S5.E8.m1.4.4.2.4.cmml">r</mi><mo id="S5.E8.m1.4.4.2.3" xref="S5.E8.m1.4.4.2.3.cmml">⁢</mo><mi id="S5.E8.m1.4.4.2.5" xref="S5.E8.m1.4.4.2.5.cmml">e</mi><mo id="S5.E8.m1.4.4.2.3a" xref="S5.E8.m1.4.4.2.3.cmml">⁢</mo><msub id="S5.E8.m1.4.4.2.6" xref="S5.E8.m1.4.4.2.6.cmml"><mi id="S5.E8.m1.4.4.2.6.2" xref="S5.E8.m1.4.4.2.6.2.cmml">t</mi><mrow id="S5.E8.m1.4.4.2.2.2.4" xref="S5.E8.m1.4.4.2.2.2.3.cmml"><mi id="S5.E8.m1.3.3.1.1.1.1" xref="S5.E8.m1.3.3.1.1.1.1.cmml">q</mi><mo id="S5.E8.m1.4.4.2.2.2.4.1" xref="S5.E8.m1.4.4.2.2.2.3.cmml">,</mo><mi id="S5.E8.m1.4.4.2.2.2.2" xref="S5.E8.m1.4.4.2.2.2.2.cmml">k</mi></mrow></msub></mrow><mrow id="S5.E8.m1.4.4.4" xref="S5.E8.m1.4.4.4.cmml"><mi id="S5.E8.m1.4.4.4.2" xref="S5.E8.m1.4.4.4.2.cmml">r</mi><mo id="S5.E8.m1.4.4.4.1" xref="S5.E8.m1.4.4.4.1.cmml">⁢</mo><mi id="S5.E8.m1.4.4.4.3" xref="S5.E8.m1.4.4.4.3.cmml">e</mi><mo id="S5.E8.m1.4.4.4.1a" xref="S5.E8.m1.4.4.4.1.cmml">⁢</mo><msub id="S5.E8.m1.4.4.4.4" xref="S5.E8.m1.4.4.4.4.cmml"><mi id="S5.E8.m1.4.4.4.4.2" xref="S5.E8.m1.4.4.4.4.2.cmml">l</mi><mi id="S5.E8.m1.4.4.4.4.3" xref="S5.E8.m1.4.4.4.4.3.cmml">q</mi></msub></mrow></mfrac></mrow></mrow></mrow><mo id="S5.E8.m1.5.5.1.2" xref="S5.E8.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E8.m1.5b"><apply id="S5.E8.m1.5.5.1.1.cmml" xref="S5.E8.m1.5.5.1"><eq id="S5.E8.m1.5.5.1.1.1.cmml" xref="S5.E8.m1.5.5.1.1.1"></eq><apply id="S5.E8.m1.5.5.1.1.2.cmml" xref="S5.E8.m1.5.5.1.1.2"><times id="S5.E8.m1.5.5.1.1.2.1.cmml" xref="S5.E8.m1.5.5.1.1.2.1"></times><ci id="S5.E8.m1.5.5.1.1.2.2a.cmml" xref="S5.E8.m1.5.5.1.1.2.2"><mtext id="S5.E8.m1.5.5.1.1.2.2.cmml" xref="S5.E8.m1.5.5.1.1.2.2">Recall</mtext></ci><ci id="S5.E8.m1.5.5.1.1.2.3.cmml" xref="S5.E8.m1.5.5.1.1.2.3">@</ci><ci id="S5.E8.m1.5.5.1.1.2.4.cmml" xref="S5.E8.m1.5.5.1.1.2.4">𝑘</ci></apply><apply id="S5.E8.m1.5.5.1.1.3.cmml" xref="S5.E8.m1.5.5.1.1.3"><times id="S5.E8.m1.5.5.1.1.3.1.cmml" xref="S5.E8.m1.5.5.1.1.3.1"></times><apply id="S5.E8.m1.1.1.cmml" xref="S5.E8.m1.1.1"><divide id="S5.E8.m1.1.1.2.cmml" xref="S5.E8.m1.1.1"></divide><cn id="S5.E8.m1.1.1.3.cmml" type="integer" xref="S5.E8.m1.1.1.3">1</cn><apply id="S5.E8.m1.1.1.1.2.cmml" xref="S5.E8.m1.1.1.1.3"><abs id="S5.E8.m1.1.1.1.2.1.cmml" xref="S5.E8.m1.1.1.1.3.1"></abs><ci id="S5.E8.m1.1.1.1.1.cmml" xref="S5.E8.m1.1.1.1.1">𝑄</ci></apply></apply><apply id="S5.E8.m1.5.5.1.1.3.2.cmml" xref="S5.E8.m1.5.5.1.1.3.2"><apply id="S5.E8.m1.5.5.1.1.3.2.1.cmml" xref="S5.E8.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S5.E8.m1.5.5.1.1.3.2.1.1.cmml" xref="S5.E8.m1.5.5.1.1.3.2.1">superscript</csymbol><apply id="S5.E8.m1.5.5.1.1.3.2.1.2.cmml" xref="S5.E8.m1.5.5.1.1.3.2.1"><csymbol cd="ambiguous" id="S5.E8.m1.5.5.1.1.3.2.1.2.1.cmml" xref="S5.E8.m1.5.5.1.1.3.2.1">subscript</csymbol><sum id="S5.E8.m1.5.5.1.1.3.2.1.2.2.cmml" xref="S5.E8.m1.5.5.1.1.3.2.1.2.2"></sum><apply id="S5.E8.m1.5.5.1.1.3.2.1.2.3.cmml" xref="S5.E8.m1.5.5.1.1.3.2.1.2.3"><eq id="S5.E8.m1.5.5.1.1.3.2.1.2.3.1.cmml" xref="S5.E8.m1.5.5.1.1.3.2.1.2.3.1"></eq><ci id="S5.E8.m1.5.5.1.1.3.2.1.2.3.2.cmml" xref="S5.E8.m1.5.5.1.1.3.2.1.2.3.2">𝑞</ci><cn id="S5.E8.m1.5.5.1.1.3.2.1.2.3.3.cmml" type="integer" xref="S5.E8.m1.5.5.1.1.3.2.1.2.3.3">1</cn></apply></apply><apply id="S5.E8.m1.2.2.1.2.cmml" xref="S5.E8.m1.2.2.1.3"><abs id="S5.E8.m1.2.2.1.2.1.cmml" xref="S5.E8.m1.2.2.1.3.1"></abs><ci id="S5.E8.m1.2.2.1.1.cmml" xref="S5.E8.m1.2.2.1.1">𝑄</ci></apply></apply><apply id="S5.E8.m1.4.4.cmml" xref="S5.E8.m1.4.4"><divide id="S5.E8.m1.4.4.3.cmml" xref="S5.E8.m1.4.4"></divide><apply id="S5.E8.m1.4.4.2.cmml" xref="S5.E8.m1.4.4.2"><times id="S5.E8.m1.4.4.2.3.cmml" xref="S5.E8.m1.4.4.2.3"></times><ci id="S5.E8.m1.4.4.2.4.cmml" xref="S5.E8.m1.4.4.2.4">𝑟</ci><ci id="S5.E8.m1.4.4.2.5.cmml" xref="S5.E8.m1.4.4.2.5">𝑒</ci><apply id="S5.E8.m1.4.4.2.6.cmml" xref="S5.E8.m1.4.4.2.6"><csymbol cd="ambiguous" id="S5.E8.m1.4.4.2.6.1.cmml" xref="S5.E8.m1.4.4.2.6">subscript</csymbol><ci id="S5.E8.m1.4.4.2.6.2.cmml" xref="S5.E8.m1.4.4.2.6.2">𝑡</ci><list id="S5.E8.m1.4.4.2.2.2.3.cmml" xref="S5.E8.m1.4.4.2.2.2.4"><ci id="S5.E8.m1.3.3.1.1.1.1.cmml" xref="S5.E8.m1.3.3.1.1.1.1">𝑞</ci><ci id="S5.E8.m1.4.4.2.2.2.2.cmml" xref="S5.E8.m1.4.4.2.2.2.2">𝑘</ci></list></apply></apply><apply id="S5.E8.m1.4.4.4.cmml" xref="S5.E8.m1.4.4.4"><times id="S5.E8.m1.4.4.4.1.cmml" xref="S5.E8.m1.4.4.4.1"></times><ci id="S5.E8.m1.4.4.4.2.cmml" xref="S5.E8.m1.4.4.4.2">𝑟</ci><ci id="S5.E8.m1.4.4.4.3.cmml" xref="S5.E8.m1.4.4.4.3">𝑒</ci><apply id="S5.E8.m1.4.4.4.4.cmml" xref="S5.E8.m1.4.4.4.4"><csymbol cd="ambiguous" id="S5.E8.m1.4.4.4.4.1.cmml" xref="S5.E8.m1.4.4.4.4">subscript</csymbol><ci id="S5.E8.m1.4.4.4.4.2.cmml" xref="S5.E8.m1.4.4.4.4.2">𝑙</ci><ci id="S5.E8.m1.4.4.4.4.3.cmml" xref="S5.E8.m1.4.4.4.4.3">𝑞</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E8.m1.5c">\text{Recall}@k=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{ret_{q,k}}{rel_{q}},</annotation><annotation encoding="application/x-llamapun" id="S5.E8.m1.5d">Recall @ italic_k = divide start_ARG 1 end_ARG start_ARG | italic_Q | end_ARG ∑ start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_Q | end_POSTSUPERSCRIPT divide start_ARG italic_r italic_e italic_t start_POSTSUBSCRIPT italic_q , italic_k end_POSTSUBSCRIPT end_ARG start_ARG italic_r italic_e italic_l start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.SSS1.p2.8">where <math alttext="|Q|" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.3.m1.1"><semantics id="S5.SS1.SSS1.p2.3.m1.1a"><mrow id="S5.SS1.SSS1.p2.3.m1.1.2.2" xref="S5.SS1.SSS1.p2.3.m1.1.2.1.cmml"><mo id="S5.SS1.SSS1.p2.3.m1.1.2.2.1" stretchy="false" xref="S5.SS1.SSS1.p2.3.m1.1.2.1.1.cmml">|</mo><mi id="S5.SS1.SSS1.p2.3.m1.1.1" xref="S5.SS1.SSS1.p2.3.m1.1.1.cmml">Q</mi><mo id="S5.SS1.SSS1.p2.3.m1.1.2.2.2" stretchy="false" xref="S5.SS1.SSS1.p2.3.m1.1.2.1.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.3.m1.1b"><apply id="S5.SS1.SSS1.p2.3.m1.1.2.1.cmml" xref="S5.SS1.SSS1.p2.3.m1.1.2.2"><abs id="S5.SS1.SSS1.p2.3.m1.1.2.1.1.cmml" xref="S5.SS1.SSS1.p2.3.m1.1.2.2.1"></abs><ci id="S5.SS1.SSS1.p2.3.m1.1.1.cmml" xref="S5.SS1.SSS1.p2.3.m1.1.1">𝑄</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.3.m1.1c">|Q|</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.3.m1.1d">| italic_Q |</annotation></semantics></math> is the number of queries in the set, <math alttext="ret_{q,k}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.4.m2.2"><semantics id="S5.SS1.SSS1.p2.4.m2.2a"><mrow id="S5.SS1.SSS1.p2.4.m2.2.3" xref="S5.SS1.SSS1.p2.4.m2.2.3.cmml"><mi id="S5.SS1.SSS1.p2.4.m2.2.3.2" xref="S5.SS1.SSS1.p2.4.m2.2.3.2.cmml">r</mi><mo id="S5.SS1.SSS1.p2.4.m2.2.3.1" xref="S5.SS1.SSS1.p2.4.m2.2.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.4.m2.2.3.3" xref="S5.SS1.SSS1.p2.4.m2.2.3.3.cmml">e</mi><mo id="S5.SS1.SSS1.p2.4.m2.2.3.1a" xref="S5.SS1.SSS1.p2.4.m2.2.3.1.cmml">⁢</mo><msub id="S5.SS1.SSS1.p2.4.m2.2.3.4" xref="S5.SS1.SSS1.p2.4.m2.2.3.4.cmml"><mi id="S5.SS1.SSS1.p2.4.m2.2.3.4.2" xref="S5.SS1.SSS1.p2.4.m2.2.3.4.2.cmml">t</mi><mrow id="S5.SS1.SSS1.p2.4.m2.2.2.2.4" xref="S5.SS1.SSS1.p2.4.m2.2.2.2.3.cmml"><mi id="S5.SS1.SSS1.p2.4.m2.1.1.1.1" xref="S5.SS1.SSS1.p2.4.m2.1.1.1.1.cmml">q</mi><mo id="S5.SS1.SSS1.p2.4.m2.2.2.2.4.1" xref="S5.SS1.SSS1.p2.4.m2.2.2.2.3.cmml">,</mo><mi id="S5.SS1.SSS1.p2.4.m2.2.2.2.2" xref="S5.SS1.SSS1.p2.4.m2.2.2.2.2.cmml">k</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.4.m2.2b"><apply id="S5.SS1.SSS1.p2.4.m2.2.3.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.3"><times id="S5.SS1.SSS1.p2.4.m2.2.3.1.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.3.1"></times><ci id="S5.SS1.SSS1.p2.4.m2.2.3.2.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.3.2">𝑟</ci><ci id="S5.SS1.SSS1.p2.4.m2.2.3.3.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.3.3">𝑒</ci><apply id="S5.SS1.SSS1.p2.4.m2.2.3.4.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.3.4"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p2.4.m2.2.3.4.1.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.3.4">subscript</csymbol><ci id="S5.SS1.SSS1.p2.4.m2.2.3.4.2.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.3.4.2">𝑡</ci><list id="S5.SS1.SSS1.p2.4.m2.2.2.2.3.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.2.2.4"><ci id="S5.SS1.SSS1.p2.4.m2.1.1.1.1.cmml" xref="S5.SS1.SSS1.p2.4.m2.1.1.1.1">𝑞</ci><ci id="S5.SS1.SSS1.p2.4.m2.2.2.2.2.cmml" xref="S5.SS1.SSS1.p2.4.m2.2.2.2.2">𝑘</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.4.m2.2c">ret_{q,k}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.4.m2.2d">italic_r italic_e italic_t start_POSTSUBSCRIPT italic_q , italic_k end_POSTSUBSCRIPT</annotation></semantics></math> is the number of relevant documents retrieved for the <math alttext="q" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.5.m3.1"><semantics id="S5.SS1.SSS1.p2.5.m3.1a"><mi id="S5.SS1.SSS1.p2.5.m3.1.1" xref="S5.SS1.SSS1.p2.5.m3.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.5.m3.1b"><ci id="S5.SS1.SSS1.p2.5.m3.1.1.cmml" xref="S5.SS1.SSS1.p2.5.m3.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.5.m3.1c">q</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.5.m3.1d">italic_q</annotation></semantics></math>-th query within the top <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.6.m4.1"><semantics id="S5.SS1.SSS1.p2.6.m4.1a"><mi id="S5.SS1.SSS1.p2.6.m4.1.1" xref="S5.SS1.SSS1.p2.6.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.6.m4.1b"><ci id="S5.SS1.SSS1.p2.6.m4.1.1.cmml" xref="S5.SS1.SSS1.p2.6.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.6.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.6.m4.1d">italic_k</annotation></semantics></math> results, and <math alttext="rel_{q}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.7.m5.1"><semantics id="S5.SS1.SSS1.p2.7.m5.1a"><mrow id="S5.SS1.SSS1.p2.7.m5.1.1" xref="S5.SS1.SSS1.p2.7.m5.1.1.cmml"><mi id="S5.SS1.SSS1.p2.7.m5.1.1.2" xref="S5.SS1.SSS1.p2.7.m5.1.1.2.cmml">r</mi><mo id="S5.SS1.SSS1.p2.7.m5.1.1.1" xref="S5.SS1.SSS1.p2.7.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p2.7.m5.1.1.3" xref="S5.SS1.SSS1.p2.7.m5.1.1.3.cmml">e</mi><mo id="S5.SS1.SSS1.p2.7.m5.1.1.1a" xref="S5.SS1.SSS1.p2.7.m5.1.1.1.cmml">⁢</mo><msub id="S5.SS1.SSS1.p2.7.m5.1.1.4" xref="S5.SS1.SSS1.p2.7.m5.1.1.4.cmml"><mi id="S5.SS1.SSS1.p2.7.m5.1.1.4.2" xref="S5.SS1.SSS1.p2.7.m5.1.1.4.2.cmml">l</mi><mi id="S5.SS1.SSS1.p2.7.m5.1.1.4.3" xref="S5.SS1.SSS1.p2.7.m5.1.1.4.3.cmml">q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.7.m5.1b"><apply id="S5.SS1.SSS1.p2.7.m5.1.1.cmml" xref="S5.SS1.SSS1.p2.7.m5.1.1"><times id="S5.SS1.SSS1.p2.7.m5.1.1.1.cmml" xref="S5.SS1.SSS1.p2.7.m5.1.1.1"></times><ci id="S5.SS1.SSS1.p2.7.m5.1.1.2.cmml" xref="S5.SS1.SSS1.p2.7.m5.1.1.2">𝑟</ci><ci id="S5.SS1.SSS1.p2.7.m5.1.1.3.cmml" xref="S5.SS1.SSS1.p2.7.m5.1.1.3">𝑒</ci><apply id="S5.SS1.SSS1.p2.7.m5.1.1.4.cmml" xref="S5.SS1.SSS1.p2.7.m5.1.1.4"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p2.7.m5.1.1.4.1.cmml" xref="S5.SS1.SSS1.p2.7.m5.1.1.4">subscript</csymbol><ci id="S5.SS1.SSS1.p2.7.m5.1.1.4.2.cmml" xref="S5.SS1.SSS1.p2.7.m5.1.1.4.2">𝑙</ci><ci id="S5.SS1.SSS1.p2.7.m5.1.1.4.3.cmml" xref="S5.SS1.SSS1.p2.7.m5.1.1.4.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.7.m5.1c">rel_{q}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.7.m5.1d">italic_r italic_e italic_l start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> is the total number of relevant documents for the <math alttext="q" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p2.8.m6.1"><semantics id="S5.SS1.SSS1.p2.8.m6.1a"><mi id="S5.SS1.SSS1.p2.8.m6.1.1" xref="S5.SS1.SSS1.p2.8.m6.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p2.8.m6.1b"><ci id="S5.SS1.SSS1.p2.8.m6.1.1.cmml" xref="S5.SS1.SSS1.p2.8.m6.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p2.8.m6.1c">q</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p2.8.m6.1d">italic_q</annotation></semantics></math>-th query.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p3.2.1">R-Precision</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib327" title="">327</a>]</cite> measures the precision at the rank position <math alttext="R" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.1.m1.1"><semantics id="S5.SS1.SSS1.p3.1.m1.1a"><mi id="S5.SS1.SSS1.p3.1.m1.1.1" xref="S5.SS1.SSS1.p3.1.m1.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.1.m1.1b"><ci id="S5.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p3.1.m1.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.1.m1.1c">R</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.1.m1.1d">italic_R</annotation></semantics></math>, which corresponds to the number of relevant documents for a given query <math alttext="q" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.2.m2.1"><semantics id="S5.SS1.SSS1.p3.2.m2.1a"><mi id="S5.SS1.SSS1.p3.2.m2.1.1" xref="S5.SS1.SSS1.p3.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.2.m2.1b"><ci id="S5.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p3.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.2.m2.1d">italic_q</annotation></semantics></math>. It is calculated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E9">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{R-Precision}=\frac{ret_{q,R}}{rel_{q}}," class="ltx_Math" display="block" id="S5.E9.m1.3"><semantics id="S5.E9.m1.3a"><mrow id="S5.E9.m1.3.3.1" xref="S5.E9.m1.3.3.1.1.cmml"><mrow id="S5.E9.m1.3.3.1.1" xref="S5.E9.m1.3.3.1.1.cmml"><mtext id="S5.E9.m1.3.3.1.1.2" xref="S5.E9.m1.3.3.1.1.2a.cmml">R-Precision</mtext><mo id="S5.E9.m1.3.3.1.1.1" xref="S5.E9.m1.3.3.1.1.1.cmml">=</mo><mfrac id="S5.E9.m1.2.2" xref="S5.E9.m1.2.2.cmml"><mrow id="S5.E9.m1.2.2.2" xref="S5.E9.m1.2.2.2.cmml"><mi id="S5.E9.m1.2.2.2.4" xref="S5.E9.m1.2.2.2.4.cmml">r</mi><mo id="S5.E9.m1.2.2.2.3" xref="S5.E9.m1.2.2.2.3.cmml">⁢</mo><mi id="S5.E9.m1.2.2.2.5" xref="S5.E9.m1.2.2.2.5.cmml">e</mi><mo id="S5.E9.m1.2.2.2.3a" xref="S5.E9.m1.2.2.2.3.cmml">⁢</mo><msub id="S5.E9.m1.2.2.2.6" xref="S5.E9.m1.2.2.2.6.cmml"><mi id="S5.E9.m1.2.2.2.6.2" xref="S5.E9.m1.2.2.2.6.2.cmml">t</mi><mrow id="S5.E9.m1.2.2.2.2.2.4" xref="S5.E9.m1.2.2.2.2.2.3.cmml"><mi id="S5.E9.m1.1.1.1.1.1.1" xref="S5.E9.m1.1.1.1.1.1.1.cmml">q</mi><mo id="S5.E9.m1.2.2.2.2.2.4.1" xref="S5.E9.m1.2.2.2.2.2.3.cmml">,</mo><mi id="S5.E9.m1.2.2.2.2.2.2" xref="S5.E9.m1.2.2.2.2.2.2.cmml">R</mi></mrow></msub></mrow><mrow id="S5.E9.m1.2.2.4" xref="S5.E9.m1.2.2.4.cmml"><mi id="S5.E9.m1.2.2.4.2" xref="S5.E9.m1.2.2.4.2.cmml">r</mi><mo id="S5.E9.m1.2.2.4.1" xref="S5.E9.m1.2.2.4.1.cmml">⁢</mo><mi id="S5.E9.m1.2.2.4.3" xref="S5.E9.m1.2.2.4.3.cmml">e</mi><mo id="S5.E9.m1.2.2.4.1a" xref="S5.E9.m1.2.2.4.1.cmml">⁢</mo><msub id="S5.E9.m1.2.2.4.4" xref="S5.E9.m1.2.2.4.4.cmml"><mi id="S5.E9.m1.2.2.4.4.2" xref="S5.E9.m1.2.2.4.4.2.cmml">l</mi><mi id="S5.E9.m1.2.2.4.4.3" xref="S5.E9.m1.2.2.4.4.3.cmml">q</mi></msub></mrow></mfrac></mrow><mo id="S5.E9.m1.3.3.1.2" xref="S5.E9.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E9.m1.3b"><apply id="S5.E9.m1.3.3.1.1.cmml" xref="S5.E9.m1.3.3.1"><eq id="S5.E9.m1.3.3.1.1.1.cmml" xref="S5.E9.m1.3.3.1.1.1"></eq><ci id="S5.E9.m1.3.3.1.1.2a.cmml" xref="S5.E9.m1.3.3.1.1.2"><mtext id="S5.E9.m1.3.3.1.1.2.cmml" xref="S5.E9.m1.3.3.1.1.2">R-Precision</mtext></ci><apply id="S5.E9.m1.2.2.cmml" xref="S5.E9.m1.2.2"><divide id="S5.E9.m1.2.2.3.cmml" xref="S5.E9.m1.2.2"></divide><apply id="S5.E9.m1.2.2.2.cmml" xref="S5.E9.m1.2.2.2"><times id="S5.E9.m1.2.2.2.3.cmml" xref="S5.E9.m1.2.2.2.3"></times><ci id="S5.E9.m1.2.2.2.4.cmml" xref="S5.E9.m1.2.2.2.4">𝑟</ci><ci id="S5.E9.m1.2.2.2.5.cmml" xref="S5.E9.m1.2.2.2.5">𝑒</ci><apply id="S5.E9.m1.2.2.2.6.cmml" xref="S5.E9.m1.2.2.2.6"><csymbol cd="ambiguous" id="S5.E9.m1.2.2.2.6.1.cmml" xref="S5.E9.m1.2.2.2.6">subscript</csymbol><ci id="S5.E9.m1.2.2.2.6.2.cmml" xref="S5.E9.m1.2.2.2.6.2">𝑡</ci><list id="S5.E9.m1.2.2.2.2.2.3.cmml" xref="S5.E9.m1.2.2.2.2.2.4"><ci id="S5.E9.m1.1.1.1.1.1.1.cmml" xref="S5.E9.m1.1.1.1.1.1.1">𝑞</ci><ci id="S5.E9.m1.2.2.2.2.2.2.cmml" xref="S5.E9.m1.2.2.2.2.2.2">𝑅</ci></list></apply></apply><apply id="S5.E9.m1.2.2.4.cmml" xref="S5.E9.m1.2.2.4"><times id="S5.E9.m1.2.2.4.1.cmml" xref="S5.E9.m1.2.2.4.1"></times><ci id="S5.E9.m1.2.2.4.2.cmml" xref="S5.E9.m1.2.2.4.2">𝑟</ci><ci id="S5.E9.m1.2.2.4.3.cmml" xref="S5.E9.m1.2.2.4.3">𝑒</ci><apply id="S5.E9.m1.2.2.4.4.cmml" xref="S5.E9.m1.2.2.4.4"><csymbol cd="ambiguous" id="S5.E9.m1.2.2.4.4.1.cmml" xref="S5.E9.m1.2.2.4.4">subscript</csymbol><ci id="S5.E9.m1.2.2.4.4.2.cmml" xref="S5.E9.m1.2.2.4.4.2">𝑙</ci><ci id="S5.E9.m1.2.2.4.4.3.cmml" xref="S5.E9.m1.2.2.4.4.3">𝑞</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E9.m1.3c">\text{R-Precision}=\frac{ret_{q,R}}{rel_{q}},</annotation><annotation encoding="application/x-llamapun" id="S5.E9.m1.3d">R-Precision = divide start_ARG italic_r italic_e italic_t start_POSTSUBSCRIPT italic_q , italic_R end_POSTSUBSCRIPT end_ARG start_ARG italic_r italic_e italic_l start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.SSS1.p3.6">where <math alttext="ret_{q,R}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.3.m1.2"><semantics id="S5.SS1.SSS1.p3.3.m1.2a"><mrow id="S5.SS1.SSS1.p3.3.m1.2.3" xref="S5.SS1.SSS1.p3.3.m1.2.3.cmml"><mi id="S5.SS1.SSS1.p3.3.m1.2.3.2" xref="S5.SS1.SSS1.p3.3.m1.2.3.2.cmml">r</mi><mo id="S5.SS1.SSS1.p3.3.m1.2.3.1" xref="S5.SS1.SSS1.p3.3.m1.2.3.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p3.3.m1.2.3.3" xref="S5.SS1.SSS1.p3.3.m1.2.3.3.cmml">e</mi><mo id="S5.SS1.SSS1.p3.3.m1.2.3.1a" xref="S5.SS1.SSS1.p3.3.m1.2.3.1.cmml">⁢</mo><msub id="S5.SS1.SSS1.p3.3.m1.2.3.4" xref="S5.SS1.SSS1.p3.3.m1.2.3.4.cmml"><mi id="S5.SS1.SSS1.p3.3.m1.2.3.4.2" xref="S5.SS1.SSS1.p3.3.m1.2.3.4.2.cmml">t</mi><mrow id="S5.SS1.SSS1.p3.3.m1.2.2.2.4" xref="S5.SS1.SSS1.p3.3.m1.2.2.2.3.cmml"><mi id="S5.SS1.SSS1.p3.3.m1.1.1.1.1" xref="S5.SS1.SSS1.p3.3.m1.1.1.1.1.cmml">q</mi><mo id="S5.SS1.SSS1.p3.3.m1.2.2.2.4.1" xref="S5.SS1.SSS1.p3.3.m1.2.2.2.3.cmml">,</mo><mi id="S5.SS1.SSS1.p3.3.m1.2.2.2.2" xref="S5.SS1.SSS1.p3.3.m1.2.2.2.2.cmml">R</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.3.m1.2b"><apply id="S5.SS1.SSS1.p3.3.m1.2.3.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.3"><times id="S5.SS1.SSS1.p3.3.m1.2.3.1.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.3.1"></times><ci id="S5.SS1.SSS1.p3.3.m1.2.3.2.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.3.2">𝑟</ci><ci id="S5.SS1.SSS1.p3.3.m1.2.3.3.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.3.3">𝑒</ci><apply id="S5.SS1.SSS1.p3.3.m1.2.3.4.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.3.4"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p3.3.m1.2.3.4.1.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.3.4">subscript</csymbol><ci id="S5.SS1.SSS1.p3.3.m1.2.3.4.2.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.3.4.2">𝑡</ci><list id="S5.SS1.SSS1.p3.3.m1.2.2.2.3.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.2.2.4"><ci id="S5.SS1.SSS1.p3.3.m1.1.1.1.1.cmml" xref="S5.SS1.SSS1.p3.3.m1.1.1.1.1">𝑞</ci><ci id="S5.SS1.SSS1.p3.3.m1.2.2.2.2.cmml" xref="S5.SS1.SSS1.p3.3.m1.2.2.2.2">𝑅</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.3.m1.2c">ret_{q,R}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.3.m1.2d">italic_r italic_e italic_t start_POSTSUBSCRIPT italic_q , italic_R end_POSTSUBSCRIPT</annotation></semantics></math> is the number of relevant documents retrieved within the top <math alttext="R" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.4.m2.1"><semantics id="S5.SS1.SSS1.p3.4.m2.1a"><mi id="S5.SS1.SSS1.p3.4.m2.1.1" xref="S5.SS1.SSS1.p3.4.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.4.m2.1b"><ci id="S5.SS1.SSS1.p3.4.m2.1.1.cmml" xref="S5.SS1.SSS1.p3.4.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.4.m2.1c">R</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.4.m2.1d">italic_R</annotation></semantics></math> positions, and <math alttext="R" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.5.m3.1"><semantics id="S5.SS1.SSS1.p3.5.m3.1a"><mi id="S5.SS1.SSS1.p3.5.m3.1.1" xref="S5.SS1.SSS1.p3.5.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.5.m3.1b"><ci id="S5.SS1.SSS1.p3.5.m3.1.1.cmml" xref="S5.SS1.SSS1.p3.5.m3.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.5.m3.1c">R</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.5.m3.1d">italic_R</annotation></semantics></math> is equivalent to <math alttext="rel_{q}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p3.6.m4.1"><semantics id="S5.SS1.SSS1.p3.6.m4.1a"><mrow id="S5.SS1.SSS1.p3.6.m4.1.1" xref="S5.SS1.SSS1.p3.6.m4.1.1.cmml"><mi id="S5.SS1.SSS1.p3.6.m4.1.1.2" xref="S5.SS1.SSS1.p3.6.m4.1.1.2.cmml">r</mi><mo id="S5.SS1.SSS1.p3.6.m4.1.1.1" xref="S5.SS1.SSS1.p3.6.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p3.6.m4.1.1.3" xref="S5.SS1.SSS1.p3.6.m4.1.1.3.cmml">e</mi><mo id="S5.SS1.SSS1.p3.6.m4.1.1.1a" xref="S5.SS1.SSS1.p3.6.m4.1.1.1.cmml">⁢</mo><msub id="S5.SS1.SSS1.p3.6.m4.1.1.4" xref="S5.SS1.SSS1.p3.6.m4.1.1.4.cmml"><mi id="S5.SS1.SSS1.p3.6.m4.1.1.4.2" xref="S5.SS1.SSS1.p3.6.m4.1.1.4.2.cmml">l</mi><mi id="S5.SS1.SSS1.p3.6.m4.1.1.4.3" xref="S5.SS1.SSS1.p3.6.m4.1.1.4.3.cmml">q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p3.6.m4.1b"><apply id="S5.SS1.SSS1.p3.6.m4.1.1.cmml" xref="S5.SS1.SSS1.p3.6.m4.1.1"><times id="S5.SS1.SSS1.p3.6.m4.1.1.1.cmml" xref="S5.SS1.SSS1.p3.6.m4.1.1.1"></times><ci id="S5.SS1.SSS1.p3.6.m4.1.1.2.cmml" xref="S5.SS1.SSS1.p3.6.m4.1.1.2">𝑟</ci><ci id="S5.SS1.SSS1.p3.6.m4.1.1.3.cmml" xref="S5.SS1.SSS1.p3.6.m4.1.1.3">𝑒</ci><apply id="S5.SS1.SSS1.p3.6.m4.1.1.4.cmml" xref="S5.SS1.SSS1.p3.6.m4.1.1.4"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p3.6.m4.1.1.4.1.cmml" xref="S5.SS1.SSS1.p3.6.m4.1.1.4">subscript</csymbol><ci id="S5.SS1.SSS1.p3.6.m4.1.1.4.2.cmml" xref="S5.SS1.SSS1.p3.6.m4.1.1.4.2">𝑙</ci><ci id="S5.SS1.SSS1.p3.6.m4.1.1.4.3.cmml" xref="S5.SS1.SSS1.p3.6.m4.1.1.4.3">𝑞</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p3.6.m4.1c">rel_{q}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p3.6.m4.1d">italic_r italic_e italic_l start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p4">
<p class="ltx_p" id="S5.SS1.SSS1.p4.3"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p4.3.1">MRR</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib160" title="">160</a>]</cite> reflects the average rank position of the first relevant document returned in the search results. It is computed as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E10">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{MRR}=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{1}{\text{rank}_{q}}," class="ltx_Math" display="block" id="S5.E10.m1.3"><semantics id="S5.E10.m1.3a"><mrow id="S5.E10.m1.3.3.1" xref="S5.E10.m1.3.3.1.1.cmml"><mrow id="S5.E10.m1.3.3.1.1" xref="S5.E10.m1.3.3.1.1.cmml"><mtext id="S5.E10.m1.3.3.1.1.2" xref="S5.E10.m1.3.3.1.1.2a.cmml">MRR</mtext><mo id="S5.E10.m1.3.3.1.1.1" xref="S5.E10.m1.3.3.1.1.1.cmml">=</mo><mrow id="S5.E10.m1.3.3.1.1.3" xref="S5.E10.m1.3.3.1.1.3.cmml"><mfrac id="S5.E10.m1.1.1" xref="S5.E10.m1.1.1.cmml"><mn id="S5.E10.m1.1.1.3" xref="S5.E10.m1.1.1.3.cmml">1</mn><mrow id="S5.E10.m1.1.1.1.3" xref="S5.E10.m1.1.1.1.2.cmml"><mo id="S5.E10.m1.1.1.1.3.1" stretchy="false" xref="S5.E10.m1.1.1.1.2.1.cmml">|</mo><mi id="S5.E10.m1.1.1.1.1" xref="S5.E10.m1.1.1.1.1.cmml">Q</mi><mo id="S5.E10.m1.1.1.1.3.2" stretchy="false" xref="S5.E10.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S5.E10.m1.3.3.1.1.3.1" xref="S5.E10.m1.3.3.1.1.3.1.cmml">⁢</mo><mrow id="S5.E10.m1.3.3.1.1.3.2" xref="S5.E10.m1.3.3.1.1.3.2.cmml"><munderover id="S5.E10.m1.3.3.1.1.3.2.1" xref="S5.E10.m1.3.3.1.1.3.2.1.cmml"><mo id="S5.E10.m1.3.3.1.1.3.2.1.2.2" movablelimits="false" xref="S5.E10.m1.3.3.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="S5.E10.m1.3.3.1.1.3.2.1.2.3" xref="S5.E10.m1.3.3.1.1.3.2.1.2.3.cmml"><mi id="S5.E10.m1.3.3.1.1.3.2.1.2.3.2" xref="S5.E10.m1.3.3.1.1.3.2.1.2.3.2.cmml">q</mi><mo id="S5.E10.m1.3.3.1.1.3.2.1.2.3.1" xref="S5.E10.m1.3.3.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="S5.E10.m1.3.3.1.1.3.2.1.2.3.3" xref="S5.E10.m1.3.3.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mrow id="S5.E10.m1.2.2.1.3" xref="S5.E10.m1.2.2.1.2.cmml"><mo id="S5.E10.m1.2.2.1.3.1" stretchy="false" xref="S5.E10.m1.2.2.1.2.1.cmml">|</mo><mi id="S5.E10.m1.2.2.1.1" xref="S5.E10.m1.2.2.1.1.cmml">Q</mi><mo id="S5.E10.m1.2.2.1.3.2" stretchy="false" xref="S5.E10.m1.2.2.1.2.1.cmml">|</mo></mrow></munderover><mfrac id="S5.E10.m1.3.3.1.1.3.2.2" xref="S5.E10.m1.3.3.1.1.3.2.2.cmml"><mn id="S5.E10.m1.3.3.1.1.3.2.2.2" xref="S5.E10.m1.3.3.1.1.3.2.2.2.cmml">1</mn><msub id="S5.E10.m1.3.3.1.1.3.2.2.3" xref="S5.E10.m1.3.3.1.1.3.2.2.3.cmml"><mtext id="S5.E10.m1.3.3.1.1.3.2.2.3.2" xref="S5.E10.m1.3.3.1.1.3.2.2.3.2a.cmml">rank</mtext><mi id="S5.E10.m1.3.3.1.1.3.2.2.3.3" xref="S5.E10.m1.3.3.1.1.3.2.2.3.3.cmml">q</mi></msub></mfrac></mrow></mrow></mrow><mo id="S5.E10.m1.3.3.1.2" xref="S5.E10.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E10.m1.3b"><apply id="S5.E10.m1.3.3.1.1.cmml" xref="S5.E10.m1.3.3.1"><eq id="S5.E10.m1.3.3.1.1.1.cmml" xref="S5.E10.m1.3.3.1.1.1"></eq><ci id="S5.E10.m1.3.3.1.1.2a.cmml" xref="S5.E10.m1.3.3.1.1.2"><mtext id="S5.E10.m1.3.3.1.1.2.cmml" xref="S5.E10.m1.3.3.1.1.2">MRR</mtext></ci><apply id="S5.E10.m1.3.3.1.1.3.cmml" xref="S5.E10.m1.3.3.1.1.3"><times id="S5.E10.m1.3.3.1.1.3.1.cmml" xref="S5.E10.m1.3.3.1.1.3.1"></times><apply id="S5.E10.m1.1.1.cmml" xref="S5.E10.m1.1.1"><divide id="S5.E10.m1.1.1.2.cmml" xref="S5.E10.m1.1.1"></divide><cn id="S5.E10.m1.1.1.3.cmml" type="integer" xref="S5.E10.m1.1.1.3">1</cn><apply id="S5.E10.m1.1.1.1.2.cmml" xref="S5.E10.m1.1.1.1.3"><abs id="S5.E10.m1.1.1.1.2.1.cmml" xref="S5.E10.m1.1.1.1.3.1"></abs><ci id="S5.E10.m1.1.1.1.1.cmml" xref="S5.E10.m1.1.1.1.1">𝑄</ci></apply></apply><apply id="S5.E10.m1.3.3.1.1.3.2.cmml" xref="S5.E10.m1.3.3.1.1.3.2"><apply id="S5.E10.m1.3.3.1.1.3.2.1.cmml" xref="S5.E10.m1.3.3.1.1.3.2.1"><csymbol cd="ambiguous" id="S5.E10.m1.3.3.1.1.3.2.1.1.cmml" xref="S5.E10.m1.3.3.1.1.3.2.1">superscript</csymbol><apply id="S5.E10.m1.3.3.1.1.3.2.1.2.cmml" xref="S5.E10.m1.3.3.1.1.3.2.1"><csymbol cd="ambiguous" id="S5.E10.m1.3.3.1.1.3.2.1.2.1.cmml" xref="S5.E10.m1.3.3.1.1.3.2.1">subscript</csymbol><sum id="S5.E10.m1.3.3.1.1.3.2.1.2.2.cmml" xref="S5.E10.m1.3.3.1.1.3.2.1.2.2"></sum><apply id="S5.E10.m1.3.3.1.1.3.2.1.2.3.cmml" xref="S5.E10.m1.3.3.1.1.3.2.1.2.3"><eq id="S5.E10.m1.3.3.1.1.3.2.1.2.3.1.cmml" xref="S5.E10.m1.3.3.1.1.3.2.1.2.3.1"></eq><ci id="S5.E10.m1.3.3.1.1.3.2.1.2.3.2.cmml" xref="S5.E10.m1.3.3.1.1.3.2.1.2.3.2">𝑞</ci><cn id="S5.E10.m1.3.3.1.1.3.2.1.2.3.3.cmml" type="integer" xref="S5.E10.m1.3.3.1.1.3.2.1.2.3.3">1</cn></apply></apply><apply id="S5.E10.m1.2.2.1.2.cmml" xref="S5.E10.m1.2.2.1.3"><abs id="S5.E10.m1.2.2.1.2.1.cmml" xref="S5.E10.m1.2.2.1.3.1"></abs><ci id="S5.E10.m1.2.2.1.1.cmml" xref="S5.E10.m1.2.2.1.1">𝑄</ci></apply></apply><apply id="S5.E10.m1.3.3.1.1.3.2.2.cmml" xref="S5.E10.m1.3.3.1.1.3.2.2"><divide id="S5.E10.m1.3.3.1.1.3.2.2.1.cmml" xref="S5.E10.m1.3.3.1.1.3.2.2"></divide><cn id="S5.E10.m1.3.3.1.1.3.2.2.2.cmml" type="integer" xref="S5.E10.m1.3.3.1.1.3.2.2.2">1</cn><apply id="S5.E10.m1.3.3.1.1.3.2.2.3.cmml" xref="S5.E10.m1.3.3.1.1.3.2.2.3"><csymbol cd="ambiguous" id="S5.E10.m1.3.3.1.1.3.2.2.3.1.cmml" xref="S5.E10.m1.3.3.1.1.3.2.2.3">subscript</csymbol><ci id="S5.E10.m1.3.3.1.1.3.2.2.3.2a.cmml" xref="S5.E10.m1.3.3.1.1.3.2.2.3.2"><mtext id="S5.E10.m1.3.3.1.1.3.2.2.3.2.cmml" xref="S5.E10.m1.3.3.1.1.3.2.2.3.2">rank</mtext></ci><ci id="S5.E10.m1.3.3.1.1.3.2.2.3.3.cmml" xref="S5.E10.m1.3.3.1.1.3.2.2.3.3">𝑞</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E10.m1.3c">\text{MRR}=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\frac{1}{\text{rank}_{q}},</annotation><annotation encoding="application/x-llamapun" id="S5.E10.m1.3d">MRR = divide start_ARG 1 end_ARG start_ARG | italic_Q | end_ARG ∑ start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_Q | end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG rank start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(10)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.SSS1.p4.2">where <math alttext="\text{rank}_{q}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p4.1.m1.1"><semantics id="S5.SS1.SSS1.p4.1.m1.1a"><msub id="S5.SS1.SSS1.p4.1.m1.1.1" xref="S5.SS1.SSS1.p4.1.m1.1.1.cmml"><mtext id="S5.SS1.SSS1.p4.1.m1.1.1.2" xref="S5.SS1.SSS1.p4.1.m1.1.1.2a.cmml">rank</mtext><mi id="S5.SS1.SSS1.p4.1.m1.1.1.3" xref="S5.SS1.SSS1.p4.1.m1.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p4.1.m1.1b"><apply id="S5.SS1.SSS1.p4.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p4.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p4.1.m1.1.1.2a.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1.2"><mtext id="S5.SS1.SSS1.p4.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1.2">rank</mtext></ci><ci id="S5.SS1.SSS1.p4.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p4.1.m1.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p4.1.m1.1c">\text{rank}_{q}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p4.1.m1.1d">rank start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math> is the rank of the first relevant document returned for the <math alttext="q" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p4.2.m2.1"><semantics id="S5.SS1.SSS1.p4.2.m2.1a"><mi id="S5.SS1.SSS1.p4.2.m2.1.1" xref="S5.SS1.SSS1.p4.2.m2.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p4.2.m2.1b"><ci id="S5.SS1.SSS1.p4.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p4.2.m2.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p4.2.m2.1c">q</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p4.2.m2.1d">italic_q</annotation></semantics></math>-th query.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p5">
<p class="ltx_p" id="S5.SS1.SSS1.p5.6"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p5.6.1">MAP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib327" title="">327</a>]</cite> calculates the average precision across multiple queries. It considers the exact position of all relevant documents and is calculated using the following formula:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E11">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{MAP}=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\left(\frac{1}{rel_{q}}\sum_{k=1}^{n_{%
q}}\text{P}@k\times I(q,k)\right)," class="ltx_Math" display="block" id="S5.E11.m1.5"><semantics id="S5.E11.m1.5a"><mrow id="S5.E11.m1.5.5.1" xref="S5.E11.m1.5.5.1.1.cmml"><mrow id="S5.E11.m1.5.5.1.1" xref="S5.E11.m1.5.5.1.1.cmml"><mtext id="S5.E11.m1.5.5.1.1.3" xref="S5.E11.m1.5.5.1.1.3a.cmml">MAP</mtext><mo id="S5.E11.m1.5.5.1.1.2" xref="S5.E11.m1.5.5.1.1.2.cmml">=</mo><mrow id="S5.E11.m1.5.5.1.1.1" xref="S5.E11.m1.5.5.1.1.1.cmml"><mfrac id="S5.E11.m1.1.1" xref="S5.E11.m1.1.1.cmml"><mn id="S5.E11.m1.1.1.3" xref="S5.E11.m1.1.1.3.cmml">1</mn><mrow id="S5.E11.m1.1.1.1.3" xref="S5.E11.m1.1.1.1.2.cmml"><mo id="S5.E11.m1.1.1.1.3.1" stretchy="false" xref="S5.E11.m1.1.1.1.2.1.cmml">|</mo><mi id="S5.E11.m1.1.1.1.1" xref="S5.E11.m1.1.1.1.1.cmml">Q</mi><mo id="S5.E11.m1.1.1.1.3.2" stretchy="false" xref="S5.E11.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S5.E11.m1.5.5.1.1.1.2" xref="S5.E11.m1.5.5.1.1.1.2.cmml">⁢</mo><mrow id="S5.E11.m1.5.5.1.1.1.1" xref="S5.E11.m1.5.5.1.1.1.1.cmml"><munderover id="S5.E11.m1.5.5.1.1.1.1.2" xref="S5.E11.m1.5.5.1.1.1.1.2.cmml"><mo id="S5.E11.m1.5.5.1.1.1.1.2.2.2" movablelimits="false" rspace="0em" xref="S5.E11.m1.5.5.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.E11.m1.5.5.1.1.1.1.2.2.3" xref="S5.E11.m1.5.5.1.1.1.1.2.2.3.cmml"><mi id="S5.E11.m1.5.5.1.1.1.1.2.2.3.2" xref="S5.E11.m1.5.5.1.1.1.1.2.2.3.2.cmml">q</mi><mo id="S5.E11.m1.5.5.1.1.1.1.2.2.3.1" xref="S5.E11.m1.5.5.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E11.m1.5.5.1.1.1.1.2.2.3.3" xref="S5.E11.m1.5.5.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mrow id="S5.E11.m1.2.2.1.3" xref="S5.E11.m1.2.2.1.2.cmml"><mo id="S5.E11.m1.2.2.1.3.1" stretchy="false" xref="S5.E11.m1.2.2.1.2.1.cmml">|</mo><mi id="S5.E11.m1.2.2.1.1" xref="S5.E11.m1.2.2.1.1.cmml">Q</mi><mo id="S5.E11.m1.2.2.1.3.2" stretchy="false" xref="S5.E11.m1.2.2.1.2.1.cmml">|</mo></mrow></munderover><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.cmml"><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1.1" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.cmml"><mfrac id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.cmml"><mn id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.2.cmml">1</mn><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.cmml"><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.2.cmml">r</mi><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.1" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.3.cmml">e</mi><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.1a" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.1.cmml">⁢</mo><msub id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.cmml"><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.2.cmml">l</mi><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.3.cmml">q</mi></msub></mrow></mfrac><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.1" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.cmml"><munderover id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.cmml"><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.2" movablelimits="false" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.2.cmml">∑</mo><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.cmml"><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.2.cmml">k</mi><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.1" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.3.cmml">1</mn></mrow><msub id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.cmml"><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.2.cmml">n</mi><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.3.cmml">q</mi></msub></munderover><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.cmml"><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.cmml"><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.cmml"><mtext id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.2a.cmml">P</mtext><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.1" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.3" mathvariant="normal" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.3.cmml">@</mi><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.1a" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.4" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.4.cmml">k</mi></mrow><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.1.cmml">×</mo><mi id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.3.cmml">I</mi></mrow><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.1" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.1.cmml">⁢</mo><mrow id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.1.cmml"><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.2.1" stretchy="false" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.1.cmml">(</mo><mi id="S5.E11.m1.3.3" xref="S5.E11.m1.3.3.cmml">q</mi><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.2.2" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.1.cmml">,</mo><mi id="S5.E11.m1.4.4" xref="S5.E11.m1.4.4.cmml">k</mi><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.2.3" stretchy="false" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S5.E11.m1.5.5.1.1.1.1.1.1.3" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S5.E11.m1.5.5.1.2" xref="S5.E11.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E11.m1.5b"><apply id="S5.E11.m1.5.5.1.1.cmml" xref="S5.E11.m1.5.5.1"><eq id="S5.E11.m1.5.5.1.1.2.cmml" xref="S5.E11.m1.5.5.1.1.2"></eq><ci id="S5.E11.m1.5.5.1.1.3a.cmml" xref="S5.E11.m1.5.5.1.1.3"><mtext id="S5.E11.m1.5.5.1.1.3.cmml" xref="S5.E11.m1.5.5.1.1.3">MAP</mtext></ci><apply id="S5.E11.m1.5.5.1.1.1.cmml" xref="S5.E11.m1.5.5.1.1.1"><times id="S5.E11.m1.5.5.1.1.1.2.cmml" xref="S5.E11.m1.5.5.1.1.1.2"></times><apply id="S5.E11.m1.1.1.cmml" xref="S5.E11.m1.1.1"><divide id="S5.E11.m1.1.1.2.cmml" xref="S5.E11.m1.1.1"></divide><cn id="S5.E11.m1.1.1.3.cmml" type="integer" xref="S5.E11.m1.1.1.3">1</cn><apply id="S5.E11.m1.1.1.1.2.cmml" xref="S5.E11.m1.1.1.1.3"><abs id="S5.E11.m1.1.1.1.2.1.cmml" xref="S5.E11.m1.1.1.1.3.1"></abs><ci id="S5.E11.m1.1.1.1.1.cmml" xref="S5.E11.m1.1.1.1.1">𝑄</ci></apply></apply><apply id="S5.E11.m1.5.5.1.1.1.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1"><apply id="S5.E11.m1.5.5.1.1.1.1.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E11.m1.5.5.1.1.1.1.2.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.2">superscript</csymbol><apply id="S5.E11.m1.5.5.1.1.1.1.2.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E11.m1.5.5.1.1.1.1.2.2.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.2">subscript</csymbol><sum id="S5.E11.m1.5.5.1.1.1.1.2.2.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.2.2.2"></sum><apply id="S5.E11.m1.5.5.1.1.1.1.2.2.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.2.2.3"><eq id="S5.E11.m1.5.5.1.1.1.1.2.2.3.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.2.2.3.1"></eq><ci id="S5.E11.m1.5.5.1.1.1.1.2.2.3.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.2.2.3.2">𝑞</ci><cn id="S5.E11.m1.5.5.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S5.E11.m1.5.5.1.1.1.1.2.2.3.3">1</cn></apply></apply><apply id="S5.E11.m1.2.2.1.2.cmml" xref="S5.E11.m1.2.2.1.3"><abs id="S5.E11.m1.2.2.1.2.1.cmml" xref="S5.E11.m1.2.2.1.3.1"></abs><ci id="S5.E11.m1.2.2.1.1.cmml" xref="S5.E11.m1.2.2.1.1">𝑄</ci></apply></apply><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1"><times id="S5.E11.m1.5.5.1.1.1.1.1.1.1.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.1"></times><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2"><divide id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2"></divide><cn id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.2.cmml" type="integer" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.2">1</cn><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3"><times id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.1"></times><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.2">𝑟</ci><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.3">𝑒</ci><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4"><csymbol cd="ambiguous" id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4">subscript</csymbol><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.2">𝑙</ci><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.2.3.4.3">𝑞</ci></apply></apply></apply><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3"><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1">superscript</csymbol><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1">subscript</csymbol><sum id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.2"></sum><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3"><eq id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.1"></eq><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.2">𝑘</ci><cn id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.3.cmml" type="integer" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.2.3.3">1</cn></apply></apply><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3"><csymbol cd="ambiguous" id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3">subscript</csymbol><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.2">𝑛</ci><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.1.3.3">𝑞</ci></apply></apply><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2"><times id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.1"></times><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2"><times id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.1"></times><apply id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2"><times id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.1"></times><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.2a.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.2"><mtext id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.2.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.2">P</mtext></ci><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.3">@</ci><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.4.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.2.4">𝑘</ci></apply><ci id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.3.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.2.3">𝐼</ci></apply><interval closure="open" id="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S5.E11.m1.5.5.1.1.1.1.1.1.1.3.2.3.2"><ci id="S5.E11.m1.3.3.cmml" xref="S5.E11.m1.3.3">𝑞</ci><ci id="S5.E11.m1.4.4.cmml" xref="S5.E11.m1.4.4">𝑘</ci></interval></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E11.m1.5c">\text{MAP}=\frac{1}{|Q|}\sum_{q=1}^{|Q|}\left(\frac{1}{rel_{q}}\sum_{k=1}^{n_{%
q}}\text{P}@k\times I(q,k)\right),</annotation><annotation encoding="application/x-llamapun" id="S5.E11.m1.5d">MAP = divide start_ARG 1 end_ARG start_ARG | italic_Q | end_ARG ∑ start_POSTSUBSCRIPT italic_q = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_Q | end_POSTSUPERSCRIPT ( divide start_ARG 1 end_ARG start_ARG italic_r italic_e italic_l start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_POSTSUPERSCRIPT P @ italic_k × italic_I ( italic_q , italic_k ) ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(11)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.SSS1.p5.5">where <math alttext="\text{P}@k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p5.1.m1.1"><semantics id="S5.SS1.SSS1.p5.1.m1.1a"><mrow id="S5.SS1.SSS1.p5.1.m1.1.1" xref="S5.SS1.SSS1.p5.1.m1.1.1.cmml"><mtext id="S5.SS1.SSS1.p5.1.m1.1.1.2" xref="S5.SS1.SSS1.p5.1.m1.1.1.2a.cmml">P</mtext><mo id="S5.SS1.SSS1.p5.1.m1.1.1.1" xref="S5.SS1.SSS1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p5.1.m1.1.1.3" mathvariant="normal" xref="S5.SS1.SSS1.p5.1.m1.1.1.3.cmml">@</mi><mo id="S5.SS1.SSS1.p5.1.m1.1.1.1a" xref="S5.SS1.SSS1.p5.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p5.1.m1.1.1.4" xref="S5.SS1.SSS1.p5.1.m1.1.1.4.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.1.m1.1b"><apply id="S5.SS1.SSS1.p5.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1"><times id="S5.SS1.SSS1.p5.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1.1"></times><ci id="S5.SS1.SSS1.p5.1.m1.1.1.2a.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1.2"><mtext id="S5.SS1.SSS1.p5.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1.2">P</mtext></ci><ci id="S5.SS1.SSS1.p5.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1.3">@</ci><ci id="S5.SS1.SSS1.p5.1.m1.1.1.4.cmml" xref="S5.SS1.SSS1.p5.1.m1.1.1.4">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.1.m1.1c">\text{P}@k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p5.1.m1.1d">P @ italic_k</annotation></semantics></math> is the precision at cutoff <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p5.2.m2.1"><semantics id="S5.SS1.SSS1.p5.2.m2.1a"><mi id="S5.SS1.SSS1.p5.2.m2.1.1" xref="S5.SS1.SSS1.p5.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.2.m2.1b"><ci id="S5.SS1.SSS1.p5.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p5.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p5.2.m2.1d">italic_k</annotation></semantics></math>, and <math alttext="I(q,k)" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p5.3.m3.2"><semantics id="S5.SS1.SSS1.p5.3.m3.2a"><mrow id="S5.SS1.SSS1.p5.3.m3.2.3" xref="S5.SS1.SSS1.p5.3.m3.2.3.cmml"><mi id="S5.SS1.SSS1.p5.3.m3.2.3.2" xref="S5.SS1.SSS1.p5.3.m3.2.3.2.cmml">I</mi><mo id="S5.SS1.SSS1.p5.3.m3.2.3.1" xref="S5.SS1.SSS1.p5.3.m3.2.3.1.cmml">⁢</mo><mrow id="S5.SS1.SSS1.p5.3.m3.2.3.3.2" xref="S5.SS1.SSS1.p5.3.m3.2.3.3.1.cmml"><mo id="S5.SS1.SSS1.p5.3.m3.2.3.3.2.1" stretchy="false" xref="S5.SS1.SSS1.p5.3.m3.2.3.3.1.cmml">(</mo><mi id="S5.SS1.SSS1.p5.3.m3.1.1" xref="S5.SS1.SSS1.p5.3.m3.1.1.cmml">q</mi><mo id="S5.SS1.SSS1.p5.3.m3.2.3.3.2.2" xref="S5.SS1.SSS1.p5.3.m3.2.3.3.1.cmml">,</mo><mi id="S5.SS1.SSS1.p5.3.m3.2.2" xref="S5.SS1.SSS1.p5.3.m3.2.2.cmml">k</mi><mo id="S5.SS1.SSS1.p5.3.m3.2.3.3.2.3" stretchy="false" xref="S5.SS1.SSS1.p5.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.3.m3.2b"><apply id="S5.SS1.SSS1.p5.3.m3.2.3.cmml" xref="S5.SS1.SSS1.p5.3.m3.2.3"><times id="S5.SS1.SSS1.p5.3.m3.2.3.1.cmml" xref="S5.SS1.SSS1.p5.3.m3.2.3.1"></times><ci id="S5.SS1.SSS1.p5.3.m3.2.3.2.cmml" xref="S5.SS1.SSS1.p5.3.m3.2.3.2">𝐼</ci><interval closure="open" id="S5.SS1.SSS1.p5.3.m3.2.3.3.1.cmml" xref="S5.SS1.SSS1.p5.3.m3.2.3.3.2"><ci id="S5.SS1.SSS1.p5.3.m3.1.1.cmml" xref="S5.SS1.SSS1.p5.3.m3.1.1">𝑞</ci><ci id="S5.SS1.SSS1.p5.3.m3.2.2.cmml" xref="S5.SS1.SSS1.p5.3.m3.2.2">𝑘</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.3.m3.2c">I(q,k)</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p5.3.m3.2d">italic_I ( italic_q , italic_k )</annotation></semantics></math> is an indicator function that is 1 if the document at position <math alttext="k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p5.4.m4.1"><semantics id="S5.SS1.SSS1.p5.4.m4.1a"><mi id="S5.SS1.SSS1.p5.4.m4.1.1" xref="S5.SS1.SSS1.p5.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.4.m4.1b"><ci id="S5.SS1.SSS1.p5.4.m4.1.1.cmml" xref="S5.SS1.SSS1.p5.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p5.4.m4.1d">italic_k</annotation></semantics></math> is relevant to the <math alttext="q" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p5.5.m5.1"><semantics id="S5.SS1.SSS1.p5.5.m5.1a"><mi id="S5.SS1.SSS1.p5.5.m5.1.1" xref="S5.SS1.SSS1.p5.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p5.5.m5.1b"><ci id="S5.SS1.SSS1.p5.5.m5.1.1.cmml" xref="S5.SS1.SSS1.p5.5.m5.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p5.5.m5.1c">q</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p5.5.m5.1d">italic_q</annotation></semantics></math>-th query and 0 otherwise.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p6">
<p class="ltx_p" id="S5.SS1.SSS1.p6.6"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS1.p6.6.1">nDCG</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib161" title="">161</a>]</cite> takes into account not only the relevance of the documents returned but also their positions in the result list, which is defined by:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E12">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{DCG}@k=\sum_{i=1}^{k}\frac{2^{\text{rel}_{i}}-1}{\log_{2}(i+1)}," class="ltx_Math" display="block" id="S5.E12.m1.3"><semantics id="S5.E12.m1.3a"><mrow id="S5.E12.m1.3.3.1" xref="S5.E12.m1.3.3.1.1.cmml"><mrow id="S5.E12.m1.3.3.1.1" xref="S5.E12.m1.3.3.1.1.cmml"><mrow id="S5.E12.m1.3.3.1.1.2" xref="S5.E12.m1.3.3.1.1.2.cmml"><mtext id="S5.E12.m1.3.3.1.1.2.2" xref="S5.E12.m1.3.3.1.1.2.2a.cmml">DCG</mtext><mo id="S5.E12.m1.3.3.1.1.2.1" xref="S5.E12.m1.3.3.1.1.2.1.cmml">⁢</mo><mi id="S5.E12.m1.3.3.1.1.2.3" mathvariant="normal" xref="S5.E12.m1.3.3.1.1.2.3.cmml">@</mi><mo id="S5.E12.m1.3.3.1.1.2.1a" xref="S5.E12.m1.3.3.1.1.2.1.cmml">⁢</mo><mi id="S5.E12.m1.3.3.1.1.2.4" xref="S5.E12.m1.3.3.1.1.2.4.cmml">k</mi></mrow><mo id="S5.E12.m1.3.3.1.1.1" rspace="0.111em" xref="S5.E12.m1.3.3.1.1.1.cmml">=</mo><mrow id="S5.E12.m1.3.3.1.1.3" xref="S5.E12.m1.3.3.1.1.3.cmml"><munderover id="S5.E12.m1.3.3.1.1.3.1" xref="S5.E12.m1.3.3.1.1.3.1.cmml"><mo id="S5.E12.m1.3.3.1.1.3.1.2.2" movablelimits="false" xref="S5.E12.m1.3.3.1.1.3.1.2.2.cmml">∑</mo><mrow id="S5.E12.m1.3.3.1.1.3.1.2.3" xref="S5.E12.m1.3.3.1.1.3.1.2.3.cmml"><mi id="S5.E12.m1.3.3.1.1.3.1.2.3.2" xref="S5.E12.m1.3.3.1.1.3.1.2.3.2.cmml">i</mi><mo id="S5.E12.m1.3.3.1.1.3.1.2.3.1" xref="S5.E12.m1.3.3.1.1.3.1.2.3.1.cmml">=</mo><mn id="S5.E12.m1.3.3.1.1.3.1.2.3.3" xref="S5.E12.m1.3.3.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S5.E12.m1.3.3.1.1.3.1.3" xref="S5.E12.m1.3.3.1.1.3.1.3.cmml">k</mi></munderover><mfrac id="S5.E12.m1.2.2" xref="S5.E12.m1.2.2.cmml"><mrow id="S5.E12.m1.2.2.4" xref="S5.E12.m1.2.2.4.cmml"><msup id="S5.E12.m1.2.2.4.2" xref="S5.E12.m1.2.2.4.2.cmml"><mn id="S5.E12.m1.2.2.4.2.2" xref="S5.E12.m1.2.2.4.2.2.cmml">2</mn><msub id="S5.E12.m1.2.2.4.2.3" xref="S5.E12.m1.2.2.4.2.3.cmml"><mtext id="S5.E12.m1.2.2.4.2.3.2" xref="S5.E12.m1.2.2.4.2.3.2a.cmml">rel</mtext><mi id="S5.E12.m1.2.2.4.2.3.3" xref="S5.E12.m1.2.2.4.2.3.3.cmml">i</mi></msub></msup><mo id="S5.E12.m1.2.2.4.1" xref="S5.E12.m1.2.2.4.1.cmml">−</mo><mn id="S5.E12.m1.2.2.4.3" xref="S5.E12.m1.2.2.4.3.cmml">1</mn></mrow><mrow id="S5.E12.m1.2.2.2.2" xref="S5.E12.m1.2.2.2.3.cmml"><msub id="S5.E12.m1.1.1.1.1.1" xref="S5.E12.m1.1.1.1.1.1.cmml"><mi id="S5.E12.m1.1.1.1.1.1.2" xref="S5.E12.m1.1.1.1.1.1.2.cmml">log</mi><mn id="S5.E12.m1.1.1.1.1.1.3" xref="S5.E12.m1.1.1.1.1.1.3.cmml">2</mn></msub><mo id="S5.E12.m1.2.2.2.2a" xref="S5.E12.m1.2.2.2.3.cmml">⁡</mo><mrow id="S5.E12.m1.2.2.2.2.2" xref="S5.E12.m1.2.2.2.3.cmml"><mo id="S5.E12.m1.2.2.2.2.2.2" stretchy="false" xref="S5.E12.m1.2.2.2.3.cmml">(</mo><mrow id="S5.E12.m1.2.2.2.2.2.1" xref="S5.E12.m1.2.2.2.2.2.1.cmml"><mi id="S5.E12.m1.2.2.2.2.2.1.2" xref="S5.E12.m1.2.2.2.2.2.1.2.cmml">i</mi><mo id="S5.E12.m1.2.2.2.2.2.1.1" xref="S5.E12.m1.2.2.2.2.2.1.1.cmml">+</mo><mn id="S5.E12.m1.2.2.2.2.2.1.3" xref="S5.E12.m1.2.2.2.2.2.1.3.cmml">1</mn></mrow><mo id="S5.E12.m1.2.2.2.2.2.3" stretchy="false" xref="S5.E12.m1.2.2.2.3.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow><mo id="S5.E12.m1.3.3.1.2" xref="S5.E12.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E12.m1.3b"><apply id="S5.E12.m1.3.3.1.1.cmml" xref="S5.E12.m1.3.3.1"><eq id="S5.E12.m1.3.3.1.1.1.cmml" xref="S5.E12.m1.3.3.1.1.1"></eq><apply id="S5.E12.m1.3.3.1.1.2.cmml" xref="S5.E12.m1.3.3.1.1.2"><times id="S5.E12.m1.3.3.1.1.2.1.cmml" xref="S5.E12.m1.3.3.1.1.2.1"></times><ci id="S5.E12.m1.3.3.1.1.2.2a.cmml" xref="S5.E12.m1.3.3.1.1.2.2"><mtext id="S5.E12.m1.3.3.1.1.2.2.cmml" xref="S5.E12.m1.3.3.1.1.2.2">DCG</mtext></ci><ci id="S5.E12.m1.3.3.1.1.2.3.cmml" xref="S5.E12.m1.3.3.1.1.2.3">@</ci><ci id="S5.E12.m1.3.3.1.1.2.4.cmml" xref="S5.E12.m1.3.3.1.1.2.4">𝑘</ci></apply><apply id="S5.E12.m1.3.3.1.1.3.cmml" xref="S5.E12.m1.3.3.1.1.3"><apply id="S5.E12.m1.3.3.1.1.3.1.cmml" xref="S5.E12.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S5.E12.m1.3.3.1.1.3.1.1.cmml" xref="S5.E12.m1.3.3.1.1.3.1">superscript</csymbol><apply id="S5.E12.m1.3.3.1.1.3.1.2.cmml" xref="S5.E12.m1.3.3.1.1.3.1"><csymbol cd="ambiguous" id="S5.E12.m1.3.3.1.1.3.1.2.1.cmml" xref="S5.E12.m1.3.3.1.1.3.1">subscript</csymbol><sum id="S5.E12.m1.3.3.1.1.3.1.2.2.cmml" xref="S5.E12.m1.3.3.1.1.3.1.2.2"></sum><apply id="S5.E12.m1.3.3.1.1.3.1.2.3.cmml" xref="S5.E12.m1.3.3.1.1.3.1.2.3"><eq id="S5.E12.m1.3.3.1.1.3.1.2.3.1.cmml" xref="S5.E12.m1.3.3.1.1.3.1.2.3.1"></eq><ci id="S5.E12.m1.3.3.1.1.3.1.2.3.2.cmml" xref="S5.E12.m1.3.3.1.1.3.1.2.3.2">𝑖</ci><cn id="S5.E12.m1.3.3.1.1.3.1.2.3.3.cmml" type="integer" xref="S5.E12.m1.3.3.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S5.E12.m1.3.3.1.1.3.1.3.cmml" xref="S5.E12.m1.3.3.1.1.3.1.3">𝑘</ci></apply><apply id="S5.E12.m1.2.2.cmml" xref="S5.E12.m1.2.2"><divide id="S5.E12.m1.2.2.3.cmml" xref="S5.E12.m1.2.2"></divide><apply id="S5.E12.m1.2.2.4.cmml" xref="S5.E12.m1.2.2.4"><minus id="S5.E12.m1.2.2.4.1.cmml" xref="S5.E12.m1.2.2.4.1"></minus><apply id="S5.E12.m1.2.2.4.2.cmml" xref="S5.E12.m1.2.2.4.2"><csymbol cd="ambiguous" id="S5.E12.m1.2.2.4.2.1.cmml" xref="S5.E12.m1.2.2.4.2">superscript</csymbol><cn id="S5.E12.m1.2.2.4.2.2.cmml" type="integer" xref="S5.E12.m1.2.2.4.2.2">2</cn><apply id="S5.E12.m1.2.2.4.2.3.cmml" xref="S5.E12.m1.2.2.4.2.3"><csymbol cd="ambiguous" id="S5.E12.m1.2.2.4.2.3.1.cmml" xref="S5.E12.m1.2.2.4.2.3">subscript</csymbol><ci id="S5.E12.m1.2.2.4.2.3.2a.cmml" xref="S5.E12.m1.2.2.4.2.3.2"><mtext id="S5.E12.m1.2.2.4.2.3.2.cmml" mathsize="70%" xref="S5.E12.m1.2.2.4.2.3.2">rel</mtext></ci><ci id="S5.E12.m1.2.2.4.2.3.3.cmml" xref="S5.E12.m1.2.2.4.2.3.3">𝑖</ci></apply></apply><cn id="S5.E12.m1.2.2.4.3.cmml" type="integer" xref="S5.E12.m1.2.2.4.3">1</cn></apply><apply id="S5.E12.m1.2.2.2.3.cmml" xref="S5.E12.m1.2.2.2.2"><apply id="S5.E12.m1.1.1.1.1.1.cmml" xref="S5.E12.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.E12.m1.1.1.1.1.1.1.cmml" xref="S5.E12.m1.1.1.1.1.1">subscript</csymbol><log id="S5.E12.m1.1.1.1.1.1.2.cmml" xref="S5.E12.m1.1.1.1.1.1.2"></log><cn id="S5.E12.m1.1.1.1.1.1.3.cmml" type="integer" xref="S5.E12.m1.1.1.1.1.1.3">2</cn></apply><apply id="S5.E12.m1.2.2.2.2.2.1.cmml" xref="S5.E12.m1.2.2.2.2.2.1"><plus id="S5.E12.m1.2.2.2.2.2.1.1.cmml" xref="S5.E12.m1.2.2.2.2.2.1.1"></plus><ci id="S5.E12.m1.2.2.2.2.2.1.2.cmml" xref="S5.E12.m1.2.2.2.2.2.1.2">𝑖</ci><cn id="S5.E12.m1.2.2.2.2.2.1.3.cmml" type="integer" xref="S5.E12.m1.2.2.2.2.2.1.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E12.m1.3c">\text{DCG}@k=\sum_{i=1}^{k}\frac{2^{\text{rel}_{i}}-1}{\log_{2}(i+1)},</annotation><annotation encoding="application/x-llamapun" id="S5.E12.m1.3d">DCG @ italic_k = ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT divide start_ARG 2 start_POSTSUPERSCRIPT rel start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT - 1 end_ARG start_ARG roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_i + 1 ) end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(12)</span></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S5.E13">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{nDCG}@k=\frac{\text{DCG}@k}{\text{IDCG}@k}," class="ltx_Math" display="block" id="S5.E13.m1.1"><semantics id="S5.E13.m1.1a"><mrow id="S5.E13.m1.1.1.1" xref="S5.E13.m1.1.1.1.1.cmml"><mrow id="S5.E13.m1.1.1.1.1" xref="S5.E13.m1.1.1.1.1.cmml"><mrow id="S5.E13.m1.1.1.1.1.2" xref="S5.E13.m1.1.1.1.1.2.cmml"><mtext id="S5.E13.m1.1.1.1.1.2.2" xref="S5.E13.m1.1.1.1.1.2.2a.cmml">nDCG</mtext><mo id="S5.E13.m1.1.1.1.1.2.1" xref="S5.E13.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S5.E13.m1.1.1.1.1.2.3" mathvariant="normal" xref="S5.E13.m1.1.1.1.1.2.3.cmml">@</mi><mo id="S5.E13.m1.1.1.1.1.2.1a" xref="S5.E13.m1.1.1.1.1.2.1.cmml">⁢</mo><mi id="S5.E13.m1.1.1.1.1.2.4" xref="S5.E13.m1.1.1.1.1.2.4.cmml">k</mi></mrow><mo id="S5.E13.m1.1.1.1.1.1" xref="S5.E13.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S5.E13.m1.1.1.1.1.3" xref="S5.E13.m1.1.1.1.1.3.cmml"><mrow id="S5.E13.m1.1.1.1.1.3.2" xref="S5.E13.m1.1.1.1.1.3.2.cmml"><mtext id="S5.E13.m1.1.1.1.1.3.2.2" xref="S5.E13.m1.1.1.1.1.3.2.2a.cmml">DCG</mtext><mo id="S5.E13.m1.1.1.1.1.3.2.1" xref="S5.E13.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.E13.m1.1.1.1.1.3.2.3" mathvariant="normal" xref="S5.E13.m1.1.1.1.1.3.2.3.cmml">@</mi><mo id="S5.E13.m1.1.1.1.1.3.2.1a" xref="S5.E13.m1.1.1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.E13.m1.1.1.1.1.3.2.4" xref="S5.E13.m1.1.1.1.1.3.2.4.cmml">k</mi></mrow><mrow id="S5.E13.m1.1.1.1.1.3.3" xref="S5.E13.m1.1.1.1.1.3.3.cmml"><mtext id="S5.E13.m1.1.1.1.1.3.3.2" xref="S5.E13.m1.1.1.1.1.3.3.2a.cmml">IDCG</mtext><mo id="S5.E13.m1.1.1.1.1.3.3.1" xref="S5.E13.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.E13.m1.1.1.1.1.3.3.3" mathvariant="normal" xref="S5.E13.m1.1.1.1.1.3.3.3.cmml">@</mi><mo id="S5.E13.m1.1.1.1.1.3.3.1a" xref="S5.E13.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.E13.m1.1.1.1.1.3.3.4" xref="S5.E13.m1.1.1.1.1.3.3.4.cmml">k</mi></mrow></mfrac></mrow><mo id="S5.E13.m1.1.1.1.2" xref="S5.E13.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E13.m1.1b"><apply id="S5.E13.m1.1.1.1.1.cmml" xref="S5.E13.m1.1.1.1"><eq id="S5.E13.m1.1.1.1.1.1.cmml" xref="S5.E13.m1.1.1.1.1.1"></eq><apply id="S5.E13.m1.1.1.1.1.2.cmml" xref="S5.E13.m1.1.1.1.1.2"><times id="S5.E13.m1.1.1.1.1.2.1.cmml" xref="S5.E13.m1.1.1.1.1.2.1"></times><ci id="S5.E13.m1.1.1.1.1.2.2a.cmml" xref="S5.E13.m1.1.1.1.1.2.2"><mtext id="S5.E13.m1.1.1.1.1.2.2.cmml" xref="S5.E13.m1.1.1.1.1.2.2">nDCG</mtext></ci><ci id="S5.E13.m1.1.1.1.1.2.3.cmml" xref="S5.E13.m1.1.1.1.1.2.3">@</ci><ci id="S5.E13.m1.1.1.1.1.2.4.cmml" xref="S5.E13.m1.1.1.1.1.2.4">𝑘</ci></apply><apply id="S5.E13.m1.1.1.1.1.3.cmml" xref="S5.E13.m1.1.1.1.1.3"><divide id="S5.E13.m1.1.1.1.1.3.1.cmml" xref="S5.E13.m1.1.1.1.1.3"></divide><apply id="S5.E13.m1.1.1.1.1.3.2.cmml" xref="S5.E13.m1.1.1.1.1.3.2"><times id="S5.E13.m1.1.1.1.1.3.2.1.cmml" xref="S5.E13.m1.1.1.1.1.3.2.1"></times><ci id="S5.E13.m1.1.1.1.1.3.2.2a.cmml" xref="S5.E13.m1.1.1.1.1.3.2.2"><mtext id="S5.E13.m1.1.1.1.1.3.2.2.cmml" xref="S5.E13.m1.1.1.1.1.3.2.2">DCG</mtext></ci><ci id="S5.E13.m1.1.1.1.1.3.2.3.cmml" xref="S5.E13.m1.1.1.1.1.3.2.3">@</ci><ci id="S5.E13.m1.1.1.1.1.3.2.4.cmml" xref="S5.E13.m1.1.1.1.1.3.2.4">𝑘</ci></apply><apply id="S5.E13.m1.1.1.1.1.3.3.cmml" xref="S5.E13.m1.1.1.1.1.3.3"><times id="S5.E13.m1.1.1.1.1.3.3.1.cmml" xref="S5.E13.m1.1.1.1.1.3.3.1"></times><ci id="S5.E13.m1.1.1.1.1.3.3.2a.cmml" xref="S5.E13.m1.1.1.1.1.3.3.2"><mtext id="S5.E13.m1.1.1.1.1.3.3.2.cmml" xref="S5.E13.m1.1.1.1.1.3.3.2">IDCG</mtext></ci><ci id="S5.E13.m1.1.1.1.1.3.3.3.cmml" xref="S5.E13.m1.1.1.1.1.3.3.3">@</ci><ci id="S5.E13.m1.1.1.1.1.3.3.4.cmml" xref="S5.E13.m1.1.1.1.1.3.3.4">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E13.m1.1c">\text{nDCG}@k=\frac{\text{DCG}@k}{\text{IDCG}@k},</annotation><annotation encoding="application/x-llamapun" id="S5.E13.m1.1d">nDCG @ italic_k = divide start_ARG DCG @ italic_k end_ARG start_ARG IDCG @ italic_k end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(13)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS1.SSS1.p6.5">where <math alttext="\text{rel}_{i}" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p6.1.m1.1"><semantics id="S5.SS1.SSS1.p6.1.m1.1a"><msub id="S5.SS1.SSS1.p6.1.m1.1.1" xref="S5.SS1.SSS1.p6.1.m1.1.1.cmml"><mtext id="S5.SS1.SSS1.p6.1.m1.1.1.2" xref="S5.SS1.SSS1.p6.1.m1.1.1.2a.cmml">rel</mtext><mi id="S5.SS1.SSS1.p6.1.m1.1.1.3" xref="S5.SS1.SSS1.p6.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.1.m1.1b"><apply id="S5.SS1.SSS1.p6.1.m1.1.1.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.SSS1.p6.1.m1.1.1.1.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.SSS1.p6.1.m1.1.1.2a.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1.2"><mtext id="S5.SS1.SSS1.p6.1.m1.1.1.2.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1.2">rel</mtext></ci><ci id="S5.SS1.SSS1.p6.1.m1.1.1.3.cmml" xref="S5.SS1.SSS1.p6.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.1.m1.1c">\text{rel}_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p6.1.m1.1d">rel start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the graded relevance of the <math alttext="i" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p6.2.m2.1"><semantics id="S5.SS1.SSS1.p6.2.m2.1a"><mi id="S5.SS1.SSS1.p6.2.m2.1.1" xref="S5.SS1.SSS1.p6.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.2.m2.1b"><ci id="S5.SS1.SSS1.p6.2.m2.1.1.cmml" xref="S5.SS1.SSS1.p6.2.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p6.2.m2.1d">italic_i</annotation></semantics></math>-th document, <math alttext="\text{DCG}@k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p6.3.m3.1"><semantics id="S5.SS1.SSS1.p6.3.m3.1a"><mrow id="S5.SS1.SSS1.p6.3.m3.1.1" xref="S5.SS1.SSS1.p6.3.m3.1.1.cmml"><mtext id="S5.SS1.SSS1.p6.3.m3.1.1.2" xref="S5.SS1.SSS1.p6.3.m3.1.1.2a.cmml">DCG</mtext><mo id="S5.SS1.SSS1.p6.3.m3.1.1.1" xref="S5.SS1.SSS1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p6.3.m3.1.1.3" mathvariant="normal" xref="S5.SS1.SSS1.p6.3.m3.1.1.3.cmml">@</mi><mo id="S5.SS1.SSS1.p6.3.m3.1.1.1a" xref="S5.SS1.SSS1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p6.3.m3.1.1.4" xref="S5.SS1.SSS1.p6.3.m3.1.1.4.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.3.m3.1b"><apply id="S5.SS1.SSS1.p6.3.m3.1.1.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1"><times id="S5.SS1.SSS1.p6.3.m3.1.1.1.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1.1"></times><ci id="S5.SS1.SSS1.p6.3.m3.1.1.2a.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1.2"><mtext id="S5.SS1.SSS1.p6.3.m3.1.1.2.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1.2">DCG</mtext></ci><ci id="S5.SS1.SSS1.p6.3.m3.1.1.3.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1.3">@</ci><ci id="S5.SS1.SSS1.p6.3.m3.1.1.4.cmml" xref="S5.SS1.SSS1.p6.3.m3.1.1.4">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.3.m3.1c">\text{DCG}@k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p6.3.m3.1d">DCG @ italic_k</annotation></semantics></math> is the discounted cumulative gain, and <math alttext="\text{IDCG}@k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p6.4.m4.1"><semantics id="S5.SS1.SSS1.p6.4.m4.1a"><mrow id="S5.SS1.SSS1.p6.4.m4.1.1" xref="S5.SS1.SSS1.p6.4.m4.1.1.cmml"><mtext id="S5.SS1.SSS1.p6.4.m4.1.1.2" xref="S5.SS1.SSS1.p6.4.m4.1.1.2a.cmml">IDCG</mtext><mo id="S5.SS1.SSS1.p6.4.m4.1.1.1" xref="S5.SS1.SSS1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p6.4.m4.1.1.3" mathvariant="normal" xref="S5.SS1.SSS1.p6.4.m4.1.1.3.cmml">@</mi><mo id="S5.SS1.SSS1.p6.4.m4.1.1.1a" xref="S5.SS1.SSS1.p6.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p6.4.m4.1.1.4" xref="S5.SS1.SSS1.p6.4.m4.1.1.4.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.4.m4.1b"><apply id="S5.SS1.SSS1.p6.4.m4.1.1.cmml" xref="S5.SS1.SSS1.p6.4.m4.1.1"><times id="S5.SS1.SSS1.p6.4.m4.1.1.1.cmml" xref="S5.SS1.SSS1.p6.4.m4.1.1.1"></times><ci id="S5.SS1.SSS1.p6.4.m4.1.1.2a.cmml" xref="S5.SS1.SSS1.p6.4.m4.1.1.2"><mtext id="S5.SS1.SSS1.p6.4.m4.1.1.2.cmml" xref="S5.SS1.SSS1.p6.4.m4.1.1.2">IDCG</mtext></ci><ci id="S5.SS1.SSS1.p6.4.m4.1.1.3.cmml" xref="S5.SS1.SSS1.p6.4.m4.1.1.3">@</ci><ci id="S5.SS1.SSS1.p6.4.m4.1.1.4.cmml" xref="S5.SS1.SSS1.p6.4.m4.1.1.4">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.4.m4.1c">\text{IDCG}@k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p6.4.m4.1d">IDCG @ italic_k</annotation></semantics></math> represents the maximum possible <math alttext="\text{DCG}@k" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p6.5.m5.1"><semantics id="S5.SS1.SSS1.p6.5.m5.1a"><mrow id="S5.SS1.SSS1.p6.5.m5.1.1" xref="S5.SS1.SSS1.p6.5.m5.1.1.cmml"><mtext id="S5.SS1.SSS1.p6.5.m5.1.1.2" xref="S5.SS1.SSS1.p6.5.m5.1.1.2a.cmml">DCG</mtext><mo id="S5.SS1.SSS1.p6.5.m5.1.1.1" xref="S5.SS1.SSS1.p6.5.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p6.5.m5.1.1.3" mathvariant="normal" xref="S5.SS1.SSS1.p6.5.m5.1.1.3.cmml">@</mi><mo id="S5.SS1.SSS1.p6.5.m5.1.1.1a" xref="S5.SS1.SSS1.p6.5.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS1.SSS1.p6.5.m5.1.1.4" xref="S5.SS1.SSS1.p6.5.m5.1.1.4.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p6.5.m5.1b"><apply id="S5.SS1.SSS1.p6.5.m5.1.1.cmml" xref="S5.SS1.SSS1.p6.5.m5.1.1"><times id="S5.SS1.SSS1.p6.5.m5.1.1.1.cmml" xref="S5.SS1.SSS1.p6.5.m5.1.1.1"></times><ci id="S5.SS1.SSS1.p6.5.m5.1.1.2a.cmml" xref="S5.SS1.SSS1.p6.5.m5.1.1.2"><mtext id="S5.SS1.SSS1.p6.5.m5.1.1.2.cmml" xref="S5.SS1.SSS1.p6.5.m5.1.1.2">DCG</mtext></ci><ci id="S5.SS1.SSS1.p6.5.m5.1.1.3.cmml" xref="S5.SS1.SSS1.p6.5.m5.1.1.3">@</ci><ci id="S5.SS1.SSS1.p6.5.m5.1.1.4.cmml" xref="S5.SS1.SSS1.p6.5.m5.1.1.4">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p6.5.m5.1c">\text{DCG}@k</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p6.5.m5.1d">DCG @ italic_k</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2 </span>Benchmarks</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Evaluating the effectiveness of GR methods relies on high-quality and challenging benchmark datasets. Here are several benchmark datasets that are widely used in the field:</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p2.1.1">MS MARCO</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib162" title="">162</a>]</cite> (Microsoft Machine Reading Comprehension) is a large dataset for evaluating machine reading comprehension, retrieval, and question-answering capabilities in web search scenarios, containing two benchmarks: document ranking and passage ranking, with a total of 3.2 million documents and 8.8 million passages. It is compiled from real user queries extracted from Microsoft Bing’s search logs, each accompanied by annotated relevant documents. This dataset covers a diverse range of question types and document genres, aiming to assess the performance of GR systems in complex web search scenarios.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p3">
<p class="ltx_p" id="S5.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p3.1.1">NQ</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib163" title="">163</a>]</cite> (Natural Questions) is a question-answering dataset introduced by Google, using Wikipedia as the corpus, which includes 3.2 million documents, each document being a Wikipedia page. It contains numerous natural user queries and their corresponding answers extracted from web pages in Google search results, which could be utilzed to evaluate the retrieval performance of GR systems in addressing real-world questions.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p4">
<p class="ltx_p" id="S5.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p4.1.1">TriviaQA</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib164" title="">164</a>]</cite> is a QA dataset that includes a large number of factual questions and answers from various sources, including Wikipedia and web-based QA forums, totaling about 0.66M evidence documents. TriviaQA aims to assess the generalization ability of information retrieval systems across different domains and question types.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p5">
<p class="ltx_p" id="S5.SS1.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p5.1.1">KILT</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib165" title="">165</a>]</cite> (Knowledge Intensive Language Tasks) is a comprehensive benchmark dataset integrating 5 categories of knowledge-intensive tasks, including fact checking (FEVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib208" title="">208</a>]</cite>), entity linking (AIDA CoNLL-YAGO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib209" title="">209</a>]</cite>, WNED-WIKI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib328" title="">328</a>]</cite>, WNED-CWEB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib328" title="">328</a>]</cite>), slot filling (T-REx <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib329" title="">329</a>]</cite>, Zero Shot RE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib211" title="">211</a>]</cite>), open-domain QA (Natural Questions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib163" title="">163</a>]</cite>, HotpotQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib330" title="">330</a>]</cite>, TriviaQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib164" title="">164</a>]</cite>, ELI5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib331" title="">331</a>]</cite>), and dialogue (Wizard of Wikipedia <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib210" title="">210</a>]</cite>). The KILT benchmark also uses Wikipedia as its corpus, comprising 5.9 million wiki pages, which aims to evaluate the effectiveness of information retrieval systems in handling complex language tasks that require extensive background knowledge.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p6">
<p class="ltx_p" id="S5.SS1.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p6.1.1">TREC Deep Learning Track 2019 &amp; 2020</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib166" title="">166</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib167" title="">167</a>]</cite> focus on using deep learning to enhance the efficiency of information retrieval, with primary tasks including document and passage ranking. These evaluation campaigns use the MS MARCO dataset to simulate real-world search queries, providing a standardized environment for assessing different retrieval techniques.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p7">
<p class="ltx_p" id="S5.SS1.SSS2.p7.1">It is noteworthy that, due to the challenge of indexing large-scale document corpus by GR models, some methods like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib86" title="">86</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib87" title="">87</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib88" title="">88</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib100" title="">100</a>]</cite> have adopted the approach of evaluating retrieval performance on smaller subsets of documents, ranging from 10K to 300K, along with their corresponding training and test pairs.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p8">
<p class="ltx_p" id="S5.SS1.SSS2.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p8.1.1">DynamicIR.</span>
For dynamic corpora, DynamicIR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib168" title="">168</a>]</cite> proposes a task framework based on StreamingQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib332" title="">332</a>]</cite> benchmark for evaluating IR models within dynamically updated corpora. Through experimental analysis, DynamicIR revealed that GR systems are superior in adapting to evolving knowledge, handling temporally informed data, and are more efficient in terms of memory, indexing time, and FLOPs compared to dense retrieval systems.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p9">
<p class="ltx_p" id="S5.SS1.SSS2.p9.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS2.p9.1.1">ExcluIR.</span>
For exclusionary retrieval tasks, where users explicitly indicate in their queries that they do not want certain information, ExcluIR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib170" title="">170</a>]</cite> provides a set of resources. This includes an evaluation benchmark and a training set to help retrieval models understand and process exclusionary queries.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.3 </span>Analysis</h4>
<div class="ltx_para" id="S5.SS1.SSS3.p1">
<p class="ltx_p" id="S5.SS1.SSS3.p1.1">In addition to the benchmarks and metrics for evaluating the performance of GR methods, there is a series of works that have conducted detailed analyses and discussions to study the behavior of GR models.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p2">
<p class="ltx_p" id="S5.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS3.p2.1.1">Understanding Generative Retrieval.</span>
To understand the performance of DSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>]</cite> in text retrieval, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib88" title="">88</a>]</cite> examines uniqueness, completeness, and relevance ordering. These respectively reflect the system’s ability to distinguish between different documents, retrieve all relevant documents, and accurately rank documents by relevance. Experimental analysis find that DSI excels in remembering the mapping from pseudo queries to DocIDs, indicating a strong capability to recall specific DocIDs from particular queries. However, the study also pointed out DSI’s deficiency in distinguishing relevant documents from random ones, negatively impacting its retrieval effectiveness.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p3">
<p class="ltx_p" id="S5.SS1.SSS3.p3.1">Exploring the connection between generative and dense retrieval, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib99" title="">99</a>]</cite> demonstrates that they can be considered as bi-encoders in dense retrieval. Specifically, the authors analyze the computation of dot products during the generative retrieval process, which is similar to the calculation of dot products between query vectors and document vectors in dense retrieval. Following this, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib172" title="">172</a>]</cite> revisits generative retrieval from the perspective of multi-vector dense retrieval (MVDR), revealing a common framework in computing document-query relevance between the two methods. This work also analyzes their differences in document encoding and alignment strategies, further confirming through experiments the phenomenon of term matching in the alignment matrices and their commonalities in retrieval.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p4">
<p class="ltx_p" id="S5.SS1.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS3.p4.1.1">Large-scale Experimental Analysis.</span>
Later, Pradeep et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib171" title="">171</a>]</cite> conduct the first comprehensive experimental study on GR techniques over large document sets, such as the 8.8M MS MARCO passages, which results in significant computational demands. It was found that among all the techniques examined, using generated pseudo queries to augment training data remains the only effective method on large document corpus. The strongest result in the experiments was achieved by using a training task that only utilized synthetic queries to Naive DocIDs, expanding the model to T5-XL (3B parameters) to achieve an MRR@10 of 26.7. Surprisingly, increasing the parameters to T5 XXL (11B) in the same setup did not improve performance but rather led to a decline. These findings suggest that more research and in-depth analysis are needed in the GR field, and possibly additional improvements to the paradigm, to fully leverage larger language models.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS3.p5">
<p class="ltx_p" id="S5.SS1.SSS3.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.SSS3.p5.1.1">Out-of-distribution Perspective.</span>
For out-of-distribution (OOD) robustness of GR models, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib169" title="">169</a>]</cite> investigate three aspects: query variations, new query types, and new tasks. Their study showed that all types of retrieval models suffer from performance drops with query variations, indicating sensitivity to query quality and structure. However, when dealing with new query types and tasks, GR models showed different levels of adaptability, with pre-training enhancing their flexibility. The research highlights the critical need for OOD robustness in GR models for dealing with ever-changing real-world information sources.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span class="ltx_text ltx_font_italic" id="S5.SS2.1.1">Evaluation for Response Generation</span>
</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Metrics</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">Evaluating the quality of generated responses involves multiple aspects, including accuracy, fluency, relevance, etc. In this section, we’ll introduce the main metrics for evaluating reliable response generation, categorized into rule-based, model-based, and human evaluation metrics.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p2.1.1">(1) Rule-based Metrics</span>.
Exact Match (EM) is a straightforward evaluation method requiring the model’s output to be completely identical to the reference answer at the word level. This full character-level matching is stringent, often used in tasks requiring precise and concise answers, such as question answering systems, e.g., NQ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib163" title="">163</a>]</cite>, TriviaQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib164" title="">164</a>]</cite>, SQuAD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib333" title="">333</a>]</cite>, etc. It simply calculates the ratio of perfectly matched instances to the total number of instances.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p3">
<p class="ltx_p" id="S5.SS2.SSS1.p3.1">For the generation of longer text sequences, BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib173" title="">173</a>]</cite> is a common metric initially used to evaluate the quality of machine translation. It compares the similarity between the model’s output and a set of reference texts by calculating the overlap of n-grams, thereby deriving a score. This method assumes that high-quality generation should have a high lexical overlap with the labeled answer.
Optimized from BLEU, METEOR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib334" title="">334</a>]</cite> is an alignment-based metric that considers not only exact word matches but also synonyms and stem matches. Additionally, METEOR introduces considerations for word order and syntactic structure to better assess the fluency and consistency of the generated text.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p4">
<p class="ltx_p" id="S5.SS2.SSS1.p4.1">ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib174" title="">174</a>]</cite>, also a commonly used metric for evaluating longer texts, by measuring the extent of overlap in words, sentences, n-grams, and so forth, between the generated text and a collection of reference texts. It focuses on recall, meaning it evaluates how much of the information in the reference text is covered by the generated text. ROUGE comes in various forms, including ROUGE-N, which evaluates based on n-gram overlap, and ROUGE-L, which considers the longest common subsequence, catering to diverse evaluation requirements.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p5">
<p class="ltx_p" id="S5.SS2.SSS1.p5.2">Perplexity (PPL) is a metric for evaluating the performance of language models, defined as the exponentiation of the average negative log-likelihood, reflecting the model’s average predictive ability for a given corpus of text sequences. The lower the perplexity, the stronger the model’s predictive ability. Specifically, given a sequence of words <math alttext="W=w_{1},w_{2},\ldots,w_{N}" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p5.1.m1.4"><semantics id="S5.SS2.SSS1.p5.1.m1.4a"><mrow id="S5.SS2.SSS1.p5.1.m1.4.4" xref="S5.SS2.SSS1.p5.1.m1.4.4.cmml"><mi id="S5.SS2.SSS1.p5.1.m1.4.4.5" xref="S5.SS2.SSS1.p5.1.m1.4.4.5.cmml">W</mi><mo id="S5.SS2.SSS1.p5.1.m1.4.4.4" xref="S5.SS2.SSS1.p5.1.m1.4.4.4.cmml">=</mo><mrow id="S5.SS2.SSS1.p5.1.m1.4.4.3.3" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.4.cmml"><msub id="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1" xref="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.cmml"><mi id="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.2" xref="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.2.cmml">w</mi><mn id="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.3" xref="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.4" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.4.cmml">,</mo><msub id="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2" xref="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.cmml"><mi id="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.2" xref="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.2.cmml">w</mi><mn id="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.3" xref="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.5" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.4.cmml">,</mo><mi id="S5.SS2.SSS1.p5.1.m1.1.1" mathvariant="normal" xref="S5.SS2.SSS1.p5.1.m1.1.1.cmml">…</mi><mo id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.6" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.4.cmml">,</mo><msub id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.cmml"><mi id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.2" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.2.cmml">w</mi><mi id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.3" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.3.cmml">N</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.1.m1.4b"><apply id="S5.SS2.SSS1.p5.1.m1.4.4.cmml" xref="S5.SS2.SSS1.p5.1.m1.4.4"><eq id="S5.SS2.SSS1.p5.1.m1.4.4.4.cmml" xref="S5.SS2.SSS1.p5.1.m1.4.4.4"></eq><ci id="S5.SS2.SSS1.p5.1.m1.4.4.5.cmml" xref="S5.SS2.SSS1.p5.1.m1.4.4.5">𝑊</ci><list id="S5.SS2.SSS1.p5.1.m1.4.4.3.4.cmml" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.3"><apply id="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.cmml" xref="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.1.cmml" xref="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.2.cmml" xref="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.2">𝑤</ci><cn id="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S5.SS2.SSS1.p5.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.cmml" xref="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.1.cmml" xref="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.2.cmml" xref="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.2">𝑤</ci><cn id="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S5.SS2.SSS1.p5.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S5.SS2.SSS1.p5.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p5.1.m1.1.1">…</ci><apply id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.cmml" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.1.cmml" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.2.cmml" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.2">𝑤</ci><ci id="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.3.cmml" xref="S5.SS2.SSS1.p5.1.m1.4.4.3.3.3.3">𝑁</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.1.m1.4c">W=w_{1},w_{2},\ldots,w_{N}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p5.1.m1.4d">italic_W = italic_w start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_w start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_w start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p5.2.m2.1"><semantics id="S5.SS2.SSS1.p5.2.m2.1a"><mi id="S5.SS2.SSS1.p5.2.m2.1.1" xref="S5.SS2.SSS1.p5.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.2.m2.1b"><ci id="S5.SS2.SSS1.p5.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p5.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p5.2.m2.1d">italic_N</annotation></semantics></math> is the total number of words in the sequence, PPL can be expressed as:</p>
<table class="ltx_equation ltx_eqn_table" id="S5.E14">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{PPL}(W)=\exp\left\{-\frac{1}{N}\sum_{i=1}^{N}\log p(w_{i}|w_{&lt;i})\right\}," class="ltx_Math" display="block" id="S5.E14.m1.3"><semantics id="S5.E14.m1.3a"><mrow id="S5.E14.m1.3.3.1" xref="S5.E14.m1.3.3.1.1.cmml"><mrow id="S5.E14.m1.3.3.1.1" xref="S5.E14.m1.3.3.1.1.cmml"><mrow id="S5.E14.m1.3.3.1.1.3" xref="S5.E14.m1.3.3.1.1.3.cmml"><mtext id="S5.E14.m1.3.3.1.1.3.2" xref="S5.E14.m1.3.3.1.1.3.2a.cmml">PPL</mtext><mo id="S5.E14.m1.3.3.1.1.3.1" xref="S5.E14.m1.3.3.1.1.3.1.cmml">⁢</mo><mrow id="S5.E14.m1.3.3.1.1.3.3.2" xref="S5.E14.m1.3.3.1.1.3.cmml"><mo id="S5.E14.m1.3.3.1.1.3.3.2.1" stretchy="false" xref="S5.E14.m1.3.3.1.1.3.cmml">(</mo><mi id="S5.E14.m1.1.1" xref="S5.E14.m1.1.1.cmml">W</mi><mo id="S5.E14.m1.3.3.1.1.3.3.2.2" stretchy="false" xref="S5.E14.m1.3.3.1.1.3.cmml">)</mo></mrow></mrow><mo id="S5.E14.m1.3.3.1.1.2" xref="S5.E14.m1.3.3.1.1.2.cmml">=</mo><mrow id="S5.E14.m1.3.3.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.2.cmml"><mi id="S5.E14.m1.2.2" xref="S5.E14.m1.2.2.cmml">exp</mi><mo id="S5.E14.m1.3.3.1.1.1.1a" xref="S5.E14.m1.3.3.1.1.1.2.cmml">⁡</mo><mrow id="S5.E14.m1.3.3.1.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.2.cmml"><mo id="S5.E14.m1.3.3.1.1.1.1.1.2" xref="S5.E14.m1.3.3.1.1.1.2.cmml">{</mo><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S5.E14.m1.3.3.1.1.1.1.1.1a" xref="S5.E14.m1.3.3.1.1.1.1.1.1.cmml">−</mo><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.cmml"><mfrac id="S5.E14.m1.3.3.1.1.1.1.1.1.1.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mn id="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.2.cmml">1</mn><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">N</mi></mfrac><mo id="S5.E14.m1.3.3.1.1.1.1.1.1.1.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.cmml"><munderover id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.cmml"><mo id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml">∑</mo><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml">N</mi></munderover><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml">log</mi><mo id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3a" lspace="0.167em" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml">⁡</mo><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml">p</mi></mrow><mo id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">w</mi><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">|</mo><msub id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">w</mi><mrow id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S5.E14.m1.3.3.1.1.1.1.1.3" xref="S5.E14.m1.3.3.1.1.1.2.cmml">}</mo></mrow></mrow></mrow><mo id="S5.E14.m1.3.3.1.2" xref="S5.E14.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.E14.m1.3b"><apply id="S5.E14.m1.3.3.1.1.cmml" xref="S5.E14.m1.3.3.1"><eq id="S5.E14.m1.3.3.1.1.2.cmml" xref="S5.E14.m1.3.3.1.1.2"></eq><apply id="S5.E14.m1.3.3.1.1.3.cmml" xref="S5.E14.m1.3.3.1.1.3"><times id="S5.E14.m1.3.3.1.1.3.1.cmml" xref="S5.E14.m1.3.3.1.1.3.1"></times><ci id="S5.E14.m1.3.3.1.1.3.2a.cmml" xref="S5.E14.m1.3.3.1.1.3.2"><mtext id="S5.E14.m1.3.3.1.1.3.2.cmml" xref="S5.E14.m1.3.3.1.1.3.2">PPL</mtext></ci><ci id="S5.E14.m1.1.1.cmml" xref="S5.E14.m1.1.1">𝑊</ci></apply><apply id="S5.E14.m1.3.3.1.1.1.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1"><exp id="S5.E14.m1.2.2.cmml" xref="S5.E14.m1.2.2"></exp><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1"><minus id="S5.E14.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1"></minus><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1"><times id="S5.E14.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.2"></times><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.3"><divide id="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.3"></divide><cn id="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.2">1</cn><ci id="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.3.3">𝑁</ci></apply><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1"><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.2"></sum><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3"><eq id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.2.3">𝑁</ci></apply><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1"><times id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.2"></times><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3"><log id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.1"></log><ci id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.3.2">𝑝</ci></apply><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.1">conditional</csymbol><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.2">𝑤</ci><ci id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑤</ci><apply id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3"><lt id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S5.E14.m1.3.3.1.1.1.1.1.1.1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.E14.m1.3c">\text{PPL}(W)=\exp\left\{-\frac{1}{N}\sum_{i=1}^{N}\log p(w_{i}|w_{&lt;i})\right\},</annotation><annotation encoding="application/x-llamapun" id="S5.E14.m1.3d">PPL ( italic_W ) = roman_exp { - divide start_ARG 1 end_ARG start_ARG italic_N end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT roman_log italic_p ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT ) } ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(14)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S5.SS2.SSS1.p5.6">where <math alttext="p(w_{i}|w_{&lt;i})" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p5.3.m1.1"><semantics id="S5.SS2.SSS1.p5.3.m1.1a"><mrow id="S5.SS2.SSS1.p5.3.m1.1.1" xref="S5.SS2.SSS1.p5.3.m1.1.1.cmml"><mi id="S5.SS2.SSS1.p5.3.m1.1.1.3" xref="S5.SS2.SSS1.p5.3.m1.1.1.3.cmml">p</mi><mo id="S5.SS2.SSS1.p5.3.m1.1.1.2" xref="S5.SS2.SSS1.p5.3.m1.1.1.2.cmml">⁢</mo><mrow id="S5.SS2.SSS1.p5.3.m1.1.1.1.1" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.cmml"><mo id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.2" stretchy="false" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.cmml"><msub id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.cmml"><mi id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.2" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.2.cmml">w</mi><mi id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.3" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo fence="false" id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.1" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.1.cmml">|</mo><msub id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.cmml"><mi id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.2" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.2.cmml">w</mi><mrow id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.cmml"><mi id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.2" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.2.cmml"></mi><mo id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.1" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.1.cmml">&lt;</mo><mi id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.3" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.3.cmml">i</mi></mrow></msub></mrow><mo id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.3" stretchy="false" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.3.m1.1b"><apply id="S5.SS2.SSS1.p5.3.m1.1.1.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1"><times id="S5.SS2.SSS1.p5.3.m1.1.1.2.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.2"></times><ci id="S5.SS2.SSS1.p5.3.m1.1.1.3.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.3">𝑝</ci><apply id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1"><csymbol cd="latexml" id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.1.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.1.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.2.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.2">𝑤</ci><ci id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.3.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.1.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.2.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.2">𝑤</ci><apply id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3"><lt id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.1.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.1"></lt><csymbol cd="latexml" id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.2.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.2">absent</csymbol><ci id="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.3.cmml" xref="S5.SS2.SSS1.p5.3.m1.1.1.1.1.1.3.3.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.3.m1.1c">p(w_{i}|w_{&lt;i})</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p5.3.m1.1d">italic_p ( italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_w start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> represents the pre-trained language model’s probability of predicting the <math alttext="i" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p5.4.m2.1"><semantics id="S5.SS2.SSS1.p5.4.m2.1a"><mi id="S5.SS2.SSS1.p5.4.m2.1.1" xref="S5.SS2.SSS1.p5.4.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.4.m2.1b"><ci id="S5.SS2.SSS1.p5.4.m2.1.1.cmml" xref="S5.SS2.SSS1.p5.4.m2.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.4.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p5.4.m2.1d">italic_i</annotation></semantics></math>-th word <math alttext="w_{i}" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p5.5.m3.1"><semantics id="S5.SS2.SSS1.p5.5.m3.1a"><msub id="S5.SS2.SSS1.p5.5.m3.1.1" xref="S5.SS2.SSS1.p5.5.m3.1.1.cmml"><mi id="S5.SS2.SSS1.p5.5.m3.1.1.2" xref="S5.SS2.SSS1.p5.5.m3.1.1.2.cmml">w</mi><mi id="S5.SS2.SSS1.p5.5.m3.1.1.3" xref="S5.SS2.SSS1.p5.5.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.5.m3.1b"><apply id="S5.SS2.SSS1.p5.5.m3.1.1.cmml" xref="S5.SS2.SSS1.p5.5.m3.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p5.5.m3.1.1.1.cmml" xref="S5.SS2.SSS1.p5.5.m3.1.1">subscript</csymbol><ci id="S5.SS2.SSS1.p5.5.m3.1.1.2.cmml" xref="S5.SS2.SSS1.p5.5.m3.1.1.2">𝑤</ci><ci id="S5.SS2.SSS1.p5.5.m3.1.1.3.cmml" xref="S5.SS2.SSS1.p5.5.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.5.m3.1c">w_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p5.5.m3.1d">italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> given the previous words <math alttext="w_{&lt;i}" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p5.6.m4.1"><semantics id="S5.SS2.SSS1.p5.6.m4.1a"><msub id="S5.SS2.SSS1.p5.6.m4.1.1" xref="S5.SS2.SSS1.p5.6.m4.1.1.cmml"><mi id="S5.SS2.SSS1.p5.6.m4.1.1.2" xref="S5.SS2.SSS1.p5.6.m4.1.1.2.cmml">w</mi><mrow id="S5.SS2.SSS1.p5.6.m4.1.1.3" xref="S5.SS2.SSS1.p5.6.m4.1.1.3.cmml"><mi id="S5.SS2.SSS1.p5.6.m4.1.1.3.2" xref="S5.SS2.SSS1.p5.6.m4.1.1.3.2.cmml"></mi><mo id="S5.SS2.SSS1.p5.6.m4.1.1.3.1" xref="S5.SS2.SSS1.p5.6.m4.1.1.3.1.cmml">&lt;</mo><mi id="S5.SS2.SSS1.p5.6.m4.1.1.3.3" xref="S5.SS2.SSS1.p5.6.m4.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p5.6.m4.1b"><apply id="S5.SS2.SSS1.p5.6.m4.1.1.cmml" xref="S5.SS2.SSS1.p5.6.m4.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS1.p5.6.m4.1.1.1.cmml" xref="S5.SS2.SSS1.p5.6.m4.1.1">subscript</csymbol><ci id="S5.SS2.SSS1.p5.6.m4.1.1.2.cmml" xref="S5.SS2.SSS1.p5.6.m4.1.1.2">𝑤</ci><apply id="S5.SS2.SSS1.p5.6.m4.1.1.3.cmml" xref="S5.SS2.SSS1.p5.6.m4.1.1.3"><lt id="S5.SS2.SSS1.p5.6.m4.1.1.3.1.cmml" xref="S5.SS2.SSS1.p5.6.m4.1.1.3.1"></lt><csymbol cd="latexml" id="S5.SS2.SSS1.p5.6.m4.1.1.3.2.cmml" xref="S5.SS2.SSS1.p5.6.m4.1.1.3.2">absent</csymbol><ci id="S5.SS2.SSS1.p5.6.m4.1.1.3.3.cmml" xref="S5.SS2.SSS1.p5.6.m4.1.1.3.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p5.6.m4.1c">w_{&lt;i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p5.6.m4.1d">italic_w start_POSTSUBSCRIPT &lt; italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p6">
<p class="ltx_p" id="S5.SS2.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p6.1.1">(2) Model-based Metrics</span>.
With the rise of pre-trained language models, a series of model-based evaluation metrics have emerged. These metrics utilize neural models to capture the deep semantic relationships between texts.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p7">
<p class="ltx_p" id="S5.SS2.SSS1.p7.1">Unlike traditional rule-based metrics, BERTScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib175" title="">175</a>]</cite> utilizes the contextual embeddings of BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib13" title="">13</a>]</cite> to capture the deep semantics of words, evaluating the similarity between candidate and reference sentences through the cosine similarity of embeddings. BERTScore employs a greedy matching strategy to optimize word-level matching and uses optional inverse document frequency weighting to emphasize important words, ultimately providing a comprehensive evaluation through a combination of recall, precision, and F1 score. BERTScore captures not only surface lexical overlap but also a deeper understanding of the semantic content of sentences.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p8">
<p class="ltx_p" id="S5.SS2.SSS1.p8.1">Similarly based on BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib13" title="">13</a>]</cite>, BLEURT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib176" title="">176</a>]</cite> designed multiple pre-training tasks, enhancing the model’s ability to recognize textual differences with millions of synthetic training pairs. These pre-training tasks include automatic evaluation metrics (such as BLEU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib173" title="">173</a>]</cite>, ROUGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib174" title="">174</a>]</cite>, and BERTScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib175" title="">175</a>]</cite>), back-translation likelihood, textual entailment, etc. Each task provides different signals to help the model learn how to evaluate the quality of text generation.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p9">
<p class="ltx_p" id="S5.SS2.SSS1.p9.1">BARTScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib335" title="">335</a>]</cite>, based on the pre-trained seq2seq generative model BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib18" title="">18</a>]</cite>, treats the evaluation of generated text as a text generation problem. Specifically, BARTScore determines the quality of text based on the transition probability between the generated text and reference text. BARTScore does not require additional parameters or labeled data and can flexibly evaluate generated text from multiple perspectives (such as informativeness, fluency, factuality, etc.) and further enhance evaluation performance through text prompts or fine-tuning for specific tasks.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p10">
<p class="ltx_p" id="S5.SS2.SSS1.p10.1">FActScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib178" title="">178</a>]</cite> focuses on the factual accuracy of each independent information point in long texts. It calculates a score representing factual accuracy by decomposing the text into atomic facts and verifying whether these facts are supported by reliable knowledge sources. This method provides a more detailed evaluation than traditional binary judgments and can be implemented efficiently and accurately through human evaluation and automated models (combining retrieval and powerful language models).</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p11">
<p class="ltx_p" id="S5.SS2.SSS1.p11.1">GPTScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib177" title="">177</a>]</cite> is a flexible, multi-faceted evaluation tool that allows users to evaluate text using natural language instructions without the need for complex training processes or costly annotations. GPTScore constructs an evaluation protocol dynamically through task specification and aspect definition and utilizes the zero-shot capability of pre-trained language models to evaluate text quality, optionally using demonstration samples to improve evaluation accuracy.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p12">
<p class="ltx_p" id="S5.SS2.SSS1.p12.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p12.1.1">(3) Human Evaluation Metrics</span>.
Human evaluation is an important method for assessing the performance of language models, especially in complex tasks where automated evaluation tools struggle to provide accurate assessments. Compared to rule-based and model-based metrics, human evaluation is more accurate and reliable in real-world applications. This evaluation method requires human evaluators (such as experts, researchers, or everyday users) to provide comprehensive assessments of the model-generated content based on their intuition and knowledge.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p13">
<p class="ltx_p" id="S5.SS2.SSS1.p13.1">Human evaluation measures the quality of language model outputs by integrating multiple assessment criteria, following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib336" title="">336</a>]</cite>: Accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib337" title="">337</a>]</cite> primarily evaluates the correctness of information and its correspondence with facts; Relevance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib338" title="">338</a>]</cite> focuses on whether the model’s output is pertinent to the specific context and user query; Fluency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib339" title="">339</a>]</cite> examines whether the text is coherent, natural, and facilitates smooth communication with users; Safety <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib340" title="">340</a>]</cite> scrutinizes whether the content may lead to potential adverse consequences or harm. These indicators collectively provide a comprehensive assessment of the model’s performance in real-world settings, ensuring its effectiveness and applicability.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Benchmarks and Analysis</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">In this section, we explore various benchmarks for evaluating the performance of language models in generating reliable responses. These benchmarks assess language understanding, factual accuracy, reliability, and the ability to provide timely information.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p2.1.1">(1) General Evaluation</span>.
To comprehensively assess the language models’ understanding capabilities across a wide range of scenarios, MMLU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib179" title="">179</a>]</cite> utilizes a multiple-choice format covering 57 different tasks, from basic mathematics to American history, computer science, and law. This benchmark spans evaluations in humanities, social science, and science, technology, engineering, and mathematics, providing a comprehensive and challenging test. It has been widely used in the evaluation of Large Language Models (LLMs) in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p3">
<p class="ltx_p" id="S5.SS2.SSS2.p3.1">Furthermore, BIG-bench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib180" title="">180</a>]</cite> introduces a large-scale and diverse benchmark designed to measure and understand the capabilities and limitations of LLMs across a broad range of tasks. Including 204 tasks contributed by 450 authors from 132 institutions, it covers areas such as linguistics, mathematics, and common sense reasoning. It focuses on tasks beyond the capabilities of language models, exploring how model performance and societal biases evolve with scale and complexity.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p4">
<p class="ltx_p" id="S5.SS2.SSS2.p4.1">LLM-Eval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib181" title="">181</a>]</cite> offers a unified multi-dimensional automatic evaluation method for open-domain dialogue of LLMs, eliminating the need for manual annotation. The performance of LLM-Eval across various datasets demonstrates its effectiveness, efficiency, and adaptability, improving over existing evaluation methods. The research also analyzes the impact of different LLMs and decoding strategies on the evaluation outcomes, underscoring the importance of selecting suitable LLMs and decoding strategies.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p5">
<p class="ltx_p" id="S5.SS2.SSS2.p5.1">For Chinese, C-Eval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib341" title="">341</a>]</cite> aims to comprehensively evaluate LLMs’ advanced knowledge and reasoning capabilities in the Chinese context. It is based on a multiple-choice format, covering four difficulty levels and 52 different academic fields from secondary school to professional levels. C-Eval also introduces C-Eval Hard, a subset containing highly challenging subjects to test the models’ advanced reasoning capabilities. Through evaluating state-of-the-art English and Chinese LLMs, C-Eval reveals areas where current models still fall short in handling complex tasks, guiding the development and optimization of Chinese LLMs.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p6">
<p class="ltx_p" id="S5.SS2.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p6.1.1">(2) Tool Evaluation</span>.
To assess the ability of language models to utilize tools, API-Bank <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib182" title="">182</a>]</cite> provides a comprehensive evaluation framework containing 73 APIs and 314 tool usage dialogs, along with a rich training dataset of 1,888 dialogs covering 1,000 domains to improve LLMs’ tool usage capabilities. Experiments show that different LLMs perform variably in tool usage, highlighting their strengths and areas for improvement.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p7">
<p class="ltx_p" id="S5.SS2.SSS2.p7.1">Later, ToolBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib140" title="">140</a>]</cite> developed a comprehensive framework including a dataset and evaluation tools to facilitate and assess the ability of LLMs to use over 16,000 real-world APIs. It enhances reasoning capabilities by automatically generating diverse instruction and API usage scenario paths, introducing a decision tree based on depth-first search. ToolBench significantly enhances LLMs’ performance in executing complex instructions and in their ability to generalize to unseen APIs. ToolLLaMA, an LLM fine-tuned from LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib64" title="">64</a>]</cite>, exhibits remarkable zero-shot capabilities and performance comparable to state-of-the-art LLMs like ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib3" title="">3</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p8">
<p class="ltx_p" id="S5.SS2.SSS2.p8.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p8.1.1">(3) Factuality Evaluation</span>.
TruthfulQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib183" title="">183</a>]</cite> measures the truthfulness of language models in answering questions. This benchmark consists of 817 questions covering 38 categories, including health, law, finance, and politics. This evaluation reveals that, even in optimal conditions, the truthfulness of model responses only reaches 58%, in stark contrast to human performance at 94%. Moreover, they proposed an automated evaluation metric named GPT-judge, which classifies the truthfulness of answers by fine-tuning the GPT-3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib62" title="">62</a>]</cite> model, achieving 90-96% accuracy in predicting human evaluations.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p9">
<p class="ltx_p" id="S5.SS2.SSS2.p9.1">HaluEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib184" title="">184</a>]</cite> is a benchmark for evaluating LLM illusions, constructed using a dataset containing 35K illusion samples, employing a combination of automated generation and manual annotation. This provides effective tools and methods for assessing and enhancing large language models’ capabilities in identifying and reducing illusions. For Chinese scenarios, HalluQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib342" title="">342</a>]</cite> designs 450 meticulously selected adversarial questions to assess the illusion phenomenon in Chinese LLMs, covering multiple domains and reflecting Chinese culture and history, identifying two main types of illusions: imitative falsehoods and factual errors.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p10">
<p class="ltx_p" id="S5.SS2.SSS2.p10.1">To evaluate the ability of LLMs to generate answers with cited text, ALCE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib150" title="">150</a>]</cite> builds an end-to-end system for retrieving relevant text passages and generating answers with citations. ALCE contains three datasets, covering different types of questions, and evaluates the generated text’s quality from ’fluency’, ’correctness’, and ’citation quality’ dimensions, combining human evaluation to verify the effectiveness of the evaluation metrics. The experimental results show that while LLMs excel at generating fluent text, there is significant room for improvement in ensuring content factual correctness and citation quality, especially on the ELI5 dataset where the best model was incomplete in citation support half of the time.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p11">
<p class="ltx_p" id="S5.SS2.SSS2.p11.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p11.1.1">(4) Real-Time Evaluation</span>.
RealTime QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib185" title="">185</a>]</cite> created a dynamic question-and-answer platform that regularly releases questions and evaluates systems weekly to ask and answer questions about the latest events or information. It challenges the static assumption of traditional QA datasets aiming for immediate application. Experiments based on LLMs like GPT-3 and T5 found that models could effectively update their generated results based on newly retrieved documents. However, when the retrieved documents failed to provide sufficient information, models tended to return outdated answers.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p12">
<p class="ltx_p" id="S5.SS2.SSS2.p12.1">Furthermore, FreshQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib59" title="">59</a>]</cite> evaluates large language models’ performance in challenges involving time-sensitive and erroneous premise questions by creating a new benchmark containing questions of this nature. Evaluating various open and closed-source LLMs revealed significant limitations in handling questions involving rapidly changing knowledge and erroneous premises. Based on these findings, the study proposed a simple in-context learning method, FreshPrompt, significantly improving LLMs’ performance on FreshQA by integrating relevant and up-to-date information sourced from search engines into the prompt.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p13">
<p class="ltx_p" id="S5.SS2.SSS2.p13.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.p13.1.1">(5) Safety, Ethic, and Trustworthiness</span>.
To comprehensively evaluate the safety of LLMs, SafetyBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib186" title="">186</a>]</cite> implements an efficient and accurate evaluation of LLMs’ safety through 11,435 multiple-choice questions covering 7 safety categories in multiple languages (Chinese and English). The diversity of question types and the broad data sources ensure rigorous testing of LLMs in various safety-related scenarios. Comparing the performance of 25 popular LLMs, SafetyBench revealed GPT-4’s significant advantage and pointed out the areas where current models need improvements in safety to promote the rapid development of safer LLMs.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p14">
<p class="ltx_p" id="S5.SS2.SSS2.p14.1">For ethics, TrustGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib60" title="">60</a>]</cite> aims to assess LLMs’ ethical performance from toxicity, bias, and value alignment, three key dimensions. The benchmark uses predefined prompt templates based on social norms to guide LLMs in generating content and employs multiple metrics to quantitatively assess the toxicity, bias, and value consistency of these contents. Experimental analysis revealed that even the most advanced LLMs still have significant issues and potential risks in these ethical considerations.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p15">
<p class="ltx_p" id="S5.SS2.SSS2.p15.1">For trustworthiness, TrustLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib61" title="">61</a>]</cite> explores principles and benchmarks including truthfulness, safety, fairness, robustness, privacy, and machine ethics across six dimensions. Extensive experiments, including assessing 16 mainstream LLMs’ performance on 30 datasets, found that trustworthiness usually positively correlates with functional effectiveness. While proprietary models typically outperform open-source models in trustworthiness, some open-source models like Llama2 showed comparable high performance.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p16">
<p class="ltx_p" id="S5.SS2.SSS2.p16.1">These benchmarks provide important tools and metrics for evaluating and improving the capabilities of language models, contributing to the development of more accurate, reliable, safe, and timely GenIR systems. For further understanding of the evaluation works, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib343" title="">343</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib336" title="">336</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib344" title="">344</a>]</cite> offer more detailed introductions.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Challenges and Prospects</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This section discusses the key challenges faced in the fields of generative document retrieval and reliable response generation, as well as potential directions for future research.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span><span class="ltx_text ltx_font_italic" id="S6.SS1.1.1">Challenges on Generative Document Retrieval</span>
</h3>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Scalability Issues</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">As extensively studied by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib171" title="">171</a>]</cite>, generative retrieval demonstrates significantly lower retrieval accuracy compared to dense retrieval when handling million-level document corpora in web search scenarios. Merely increasing the model size does not yield stable performance improvements. However, GR outperforms dense retrieval in document collections smaller than 300K, posing a question: What impedes GR methods from scaling to large document sizes? This issue encompasses several aspects:</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS1.p2.1.1">Training Data</span>. Current LLMs are pre-trained on huge datasets ranging from hundreds of billions to several trillion tokens, covering vast knowledge sources such as the internet, books, and news articles, consuming substantial computational power <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib20" title="">20</a>]</cite>. They are then extensively fine-tuned with high-quality, human-annotated data to achieve substantial generalization capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib193" title="">193</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib64" title="">64</a>]</cite>. In contrast, generative retrieval (GR) models often begin with a pre-trained language model and are fine-tuned on labeled data comprising ¡query, DocID¿ pairs, which does not sufficiently prepare them to fully grasp GR tasks. For numeric-based DocIDs, the models, having not encountered these numbers in their pre-training phase, tend to rote memorize the DocIDs seen during training, struggling to predict unseen ones effectively. Similarly, if text-based DocIDs fail to precisely represent the documents, the model also tend to rote learning.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p3">
<p class="ltx_p" id="S6.SS1.SSS1.p3.1">A potential solution is to create a large-scale pre-training dataset for generative retrieval on a general corpus, possibly including a variety of common DocIDs such as URLs, titles, and numerical sequences. We can utilize instructions to distinguish generation targets for various DocIDs. Then we can pre-train a Transformer-based GR model from scratch, the model can understand generative retrieval across diverse domains. This method could bridge the gap between language model pre-training data and GR tasks, enhancing the generalization ability of GR models across different corpora.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p4">
<p class="ltx_p" id="S6.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS1.p4.1.1">Training Method</span>.
As described in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS1.SSS1" title="3.1.1 Model Training ‣ 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>, existing training methods explore various training objectives, including seq2seq training, learning DocID, and ranking capabilities. Other methods involve knowledge distillation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib201" title="">201</a>]</cite>, reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib34" title="">34</a>]</cite>, etc. Is there a better training method to enable GR models to master generating DocID ranking lists? For example, RLHF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib22" title="">22</a>]</cite> has been effectively used to train LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib193" title="">193</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib23" title="">23</a>]</cite>, though at a high cost. Exploring RLHF in the GR field is also worthwhile.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p5">
<p class="ltx_p" id="S6.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS1.p5.1.1">Model Structure</span>.
As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS1.SSS2" title="3.1.2 Model Structure ‣ 3.1 Model Training and Structure ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>, most current GR models are based on encoder-decoder Transformers structures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib32" title="">32</a>]</cite>, such as T5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib17" title="">17</a>]</cite> and BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib18" title="">18</a>]</cite>. Some GR methods like CorpusLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib42" title="">42</a>]</cite> have experimented with a decoder-only structure of the LLM Llama2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib23" title="">23</a>]</cite>, requiring more training computational power but not significantly improving performance. Research is needed to determine which structure is more suitable for generative retrieval. Additionally, whether increasing model and data size could lead to emergent phenomena similar to those observed in LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib345" title="">345</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib346" title="">346</a>]</cite> is also a promising research direction.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Handling Dynamic Corpora</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">Real-world applications often involve dynamically changing corpora, such as the web and news archives, where incremental learning is essential. However, for language models, indexing new documents inevitably leads to forgetting old ones, posing a challenge for GR systems. Existing methods like DSI++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib37" title="">37</a>]</cite>, IncDSI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib38" title="">38</a>]</cite>, CLEVER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib109" title="">109</a>]</cite>, and CorpusBrain++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib39" title="">39</a>]</cite> propose solutions such as experience replay, constrained optimization, incremental product quantization, and continual generative pre-training frameworks to address incremental learning issues. Yet, these methods have their specific applicable scenarios, and more effective and universally applicable incremental learning strategies remain a key area for exploration.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>Document Identifier</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">Accurately representing a document with high-quality DocIDs is crucial for generative retrieval.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p2">
<p class="ltx_p" id="S6.SS1.SSS3.p2.1">For example, the KILT dataset based on the Wikipedia corpus, which includes 5.9 million documents, demonstrates optimistic retrieval performance for GR methods using titles as DocIDs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib42" title="">42</a>]</cite>. This is because each document in Wikipedia has a unique manually annotated title that represents the core entity discussed in that page. However, in the web search scenario, such as in the MS MARCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib162" title="">162</a>]</cite>, many documents lack a unique title, are overlapping, and the titles do not accurately represent the core content of the documents. Thus, GR performance significantly declines in the MS MARCO corpus of 8.8 million passages.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p3">
<p class="ltx_p" id="S6.SS1.SSS3.p3.1">Therefore, how to construct high-quality titles (or other types of DocIDs) in general corpora, similar to those in Wikipedia, that not only accurately represent documents but also are lightweight, is a critical factor for implementing GR methods and warrants in-depth research.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p4">
<p class="ltx_p" id="S6.SS1.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS3.p4.1.1">Text or Numeric?</span>
As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S3.SS2" title="3.2 Design of Document Identifiers ‣ 3 Generative Document Retrieval: From Similarity Matching to Generating Document Identifiers ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">3.2</span></a>, current methods include text-based and numeric-based DocIDs, each with their advantages and disadvantages. Text-based DocIDs effectively leverage the linguistic capabilities of pre-trained generative language models and offer better interpretability. Numeric-based DocIDs can utilize dense retriever embeddings to obtain semantic DocID sequences; they can also complement dense retrievers to achieve synergistic benefits.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p5">
<p class="ltx_p" id="S6.SS1.SSS3.p5.1">However, to ensure good generalization ability of GR models without extensive pre-training, it is essential to utilize the inherent pre-trained parameters of the model. Coherent textual DocIDs can naturally leverage this aspect, but they also need to capture key document semantics and maintain linguistic sequence characteristics. Numeric DocIDs, however, do not offer this advantage. Thus, as mentioned in <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S6.SS1.SSS1" title="6.1.1 Scalability Issues ‣ 6.1 Challenges on Generative Document Retrieval ‣ 6 Challenges and Prospects ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>, extensive pre-training is necessary to enable models to fully understand the meanings behind these numerical strings, which is a costly endeavor.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p6">
<p class="ltx_p" id="S6.SS1.SSS3.p6.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS3.p6.1.1">Do We Need a Unique ID for Each Document?</span>
Most current GR methods use a unique DocID to uniquely identify a document. However, as the number of documents in a corpus increases, maintaining a unique DocID becomes increasingly challenging. Even if a unique DocID is maintained, it is difficult to differentiate significantly from other DocIDs semantically, leading to reduced retrieval precision. Some methods, such as using sub-string as DocIDs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib102" title="">102</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib104" title="">104</a>]</cite>, have proven effective. These methods utilize the FM-Index <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib204" title="">204</a>]</cite> to ensure the generated sub-string exists in the corpus and use the number of generated sub-strings in different documents to rank documents, demonstrating good performance and generalization ability.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p7">
<p class="ltx_p" id="S6.SS1.SSS3.p7.1">However, since this method is based on FM-Index, its inference latency is high, which is an issue that needs addressing. Furthermore, exploring other more efficient alternatives to FM-Index and even considering not using constrained search but freely generating a DocID sequence followed by a lightweight matching and scoring module to efficiently return a document ranking list are also worthy of exploration.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4 </span>Efficiency Concerns</h4>
<div class="ltx_para" id="S6.SS1.SSS4.p1">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1">Current GR methods generally rely on constrained beam search to generate multiple DocID sequences during inference, resulting in high latency. This is particularly severe when returning 100 or more documents, with latencies reaching several hundred milliseconds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib31" title="">31</a>]</cite>, which is unacceptable for low-latency IR systems. Therefore, designing more efficient inference methods is crucial. To reduce inference latency, the length of the DocID sequence should not be too long; 16 tokens or fewer is an efficient range. This necessitates designing DocIDs that are precise and concise enough to represent documents while maintaining performance and improving efficiency. Additionally, developing more efficient decoding strategies is a valuable research direction for the future.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span><span class="ltx_text ltx_font_italic" id="S6.SS2.1.1">Challenges on Reliable Response Generation</span>
</h3>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Improving Accuracy and Factuality</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">In GenIR systems, ensuring content accuracy and factuality is crucial. To achieve this, as mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4" title="4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4</span></a>, there are two main areas of improvement:</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p2">
<p class="ltx_p" id="S6.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS1.p2.1.1">Internal Knowledge Memorization.</span>
Firstly, training stronger generative models is critical for building reliable GenIR systems. Various commercial LLMs continue to progress, utilizing vast training data and computational resources, but exploring better model structures is also worthwhile. Recent research such as Retentive Networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib347" title="">347</a>]</cite>, Mamba <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib348" title="">348</a>]</cite>, and others, have shown potential to challenge the performance and efficiency of Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib199" title="">199</a>]</cite>. However, whether these can scale and truly surpass Transformer-based LLMs in generation quality is still an open question. Moreover, what types of training data and methods can consistently produce models capable of generating high-quality, reliable text also deserve thorough investigation and summary. The mechanisms by which language models recall knowledge during inference are not yet clear and need to be fully understood to better serve user information needs.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p3">
<p class="ltx_p" id="S6.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS1.p3.1.1">External Knowledge Enhancement.</span>
As described in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS2.SSS1" title="4.2.1 Retrieval Augmentation ‣ 4.2 External Knowledge Augmentation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>, retrieval-augmented generation is an effective method widely applied in LLMs. However, there is still room for improvement. For example, whether inserting retrieved documents directly into generative models via prompts is the best method, or if there are better ways, such as inputting embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib349" title="">349</a>]</cite>, needs exploration. Additionally, whether models can autonomously decide whether to perform retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib350" title="">350</a>]</cite>, and when in the generation process to perform it <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib76" title="">76</a>]</cite>, are topics worth further exploration.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p4">
<p class="ltx_p" id="S6.SS2.SSS1.p4.1">Tool-augmented generation, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#S4.SS2.SSS2" title="4.2.2 Tool Augmentation ‣ 4.2 External Knowledge Augmentation ‣ 4 Reliable Response Generation: Direct Information Accessing with Generative Language Models ‣ From Matching to Generation: A Survey on Generative Information Retrieval"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>, is also a popular method for endowing LLMs with fine-grained world knowledge and performing complex tasks. Recent research has raised questions, such as ”Should tools always be used?” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib351" title="">351</a>]</cite>. More specifically, whether the performance improvements brought by using tools justify the extra computational costs incurred during model training or the inference costs during testing. Existing work mainly focuses on task accuracy, but studying the cost-effectiveness of these methods is also a valuable topic.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Real-time Properties of GenIR Systems</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">Timeliness is critical for GenIR systems, as well as traditional IR systems, to provide users with the most up-to-date information. However, since the knowledge of pre-trained generative models is fixed after training, methods like retrieval and tool augmentation are needed to acquire new external knowledge. Research on real-time knowledge acquisition remains limited, making it a valuable area for investigation.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p2">
<p class="ltx_p" id="S6.SS2.SSS2.p2.1">Moreover, continually relying on outdated knowledge from language models is inadequate, as models cannot comprehend the significance of given contexts or backgrounds in the current era, thus reducing the reliability of the generated content. Therefore, updating the information in language models while avoiding the forgetting of existing knowledge, such as through continual learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib352" title="">352</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib241" title="">241</a>]</cite>, knowledge editing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib243" title="">243</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib353" title="">353</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib354" title="">354</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib253" title="">253</a>]</cite>, etc., is a topic worth further exploring.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3 </span>Bias and Fairness</h4>
<div class="ltx_para" id="S6.SS2.SSS3.p1">
<p class="ltx_p" id="S6.SS2.SSS3.p1.1">Since LLMs are often trained on large, unfiltered datasets, GenIR systems may propagate stereotypes and biases present in the data regarding race, culture, and other aspects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib355" title="">355</a>]</cite>. Researchers have explored various methods to enhance the fairness of generated content during training data selection, training methods, generation techniques, and rewriting phases. However, biases have not been eradicated and require a thorough understanding of the mechanisms by which generative models produce biases, to design methods to solve them and build fair GenIR systems that further the practical application of GenIR.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.4 </span>Privacy and Security</h4>
<div class="ltx_para" id="S6.SS2.SSS4.p1">
<p class="ltx_p" id="S6.SS2.SSS4.p1.1">Firstly, the content generated by GenIR systems risks plagiarism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib356" title="">356</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib357" title="">357</a>]</cite>. For instance, studies such as <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib358" title="">358</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib359" title="">359</a>]</cite> indicate that pre-trained language models can reproduce large segments of their training data, leading to inadvertent plagiarism and causing academic dishonesty or copyright issues. On one hand, legal regulations regarding the copyright of AI-generated content will gradually emerge and evolve. On the other hand, technical research aimed at reducing plagiarism by generative models, such as generating text with correct citations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib360" title="">360</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib288" title="">288</a>]</cite>, is a promising research direction for reliable GenIR that has received increasing attention in recent years.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS4.p2">
<p class="ltx_p" id="S6.SS2.SSS4.p2.1">Moreover, due to the unclear mechanisms of memory and generation in pre-trained language models, GenIR systems inevitably return unsafe content. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib361" title="">361</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib358" title="">358</a>]</cite> show that when attacked, LLMs may return private information of users seen in training data. Therefore, understanding the mechanisms by which LLMs recall training data and designing effective defense mechanisms to enhance security are crucial for the widespread use of GenIR systems. Additionally, developing effective detection methods for content generated by LLMs is essential for enhancing the security of GenIR systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib362" title="">362</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span class="ltx_text ltx_font_italic" id="S6.SS3.1.1">Unified Framework</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">This article discusses two mainstream forms of GenIR: generative document retrieval and reliable response generation. However, each approach has its advantages and limitations. Generative document retrieval still returns a list of documents, whereas the reliable response generation model itself cannot effectively capture document-level relationships. Therefore, integrating these two approaches is a promising research direction.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1 </span>Unified Framework for Retrieval and Generation</h4>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.1">Given that both generative retrieval and downstream generation tasks can be based on generative language models, could a single model perform both retrieval and generation tasks? Indeed, it could.</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS1.p2">
<p class="ltx_p" id="S6.SS3.SSS1.p2.1">Current attempts, such as UniGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib115" title="">115</a>]</cite>, use a shared encoder and two decoders for GR and QA tasks respectively, and show superior performance on small-scale retrieval and QA datasets. However, they struggle to generalize across multiple downstream tasks and to integrate with powerful LLMs. Additionally, CorpusLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib42" title="">42</a>]</cite> uses a multi-task training approach to obtain a universal model for GR, QA, and RAG. Yet, merely merging training data does not significantly improve retrieval and generation performance, and CorpusLM remains limited to the Wikipedia corpus. Facing a broader internet corpus presents significant challenges.</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS1.p3">
<p class="ltx_p" id="S6.SS3.SSS1.p3.1">In the future, can we construct a large search model (LSM) that allows an LLM to have the capability to generate DocIDs and reliable responses autonomously? Even LSM could decide when to generate DocIDs to access the required knowledge before continuing generation. Unlike the large search model defined in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib363" title="">363</a>]</cite>, which unifies models beyond the first-stage retrieval (such as re-ranking, snippet, and answer models), we aim to integrate the first-stage retrieval as well, enabling the LSM to fully understand the meaning of retrieval and its connection with various downstream generation tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2 </span>Towards End-to-End Framework for Various IR Tasks</h4>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.1">As envisioned by Metzler et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.14851v3#bib.bib16" title="">16</a>]</cite>, propose an expert-level corpus model that not only possesses linguistic capabilities but also understands document-level DocIDs and knows the sources of its own knowledge. Such a model could not only solve the issue of hallucinations common in traditional language models but could also generate texts with references pointing to the source documents, thus achieving a reliable end-to-end GenIR model. By understanding DocIDs and knowledge sources, this end-to-end system could also perform additional IR tasks, such as returning the main content of a document given its DocID or returning other related document DocIDs, as well as enabling multi-lingual and multi-modal retrieval.</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS2.p2">
<p class="ltx_p" id="S6.SS3.SSS2.p2.1">Current methods, as discussed in this GenIR survey, primarily focus on generative document retrieval (GR) and response generation as separate entities. GR models excel at comprehending document identifiers at the document-level, while downstream models demonstrate powerful task generation capabilities. However, existing methods face challenges when it comes to effectively integrating these two generative abilities, limiting the overall performance and effectiveness of the GenIR system. The integration of these generative abilities in a seamless and efficient manner remains a key challenge in the field.</p>
</div>
<div class="ltx_para" id="S6.SS3.SSS2.p3">
<p class="ltx_p" id="S6.SS3.SSS2.p3.1">In the future, we can design training methods that align knowledge and DocIDs and construct high-quality training datasets for generating answers with references, to train such an end-to-end GenIR model. Achieving this goal remains challenging and requires the collaborative efforts of researchers to contribute to building the next generation of GenIR systems.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this survey, we explore the latest research developments, evaluations, current challenges, and future directions in generative information retrieval (GenIR). We discuss two main directions in the GenIR field: generative document retrieval (GR) and reliable response generation. Specifically, we systematically review the progress of GR covering model training, document identifier design, incremental learning, adaptability to downstream tasks, multi-modal GR, and generative recommendation systems; as well as advancements in reliable response generation in terms of internal knowledge memorization, external knowledge enhancement, generating responses with citations, and personal information assistance.
Additionally, we have sorted out the existing evaluation methods and benchmarks for GR and response generation. We organize the current limitations and future directions of GR systems, addressing scalability, handling dynamic corpora, document representation, and efficiency challenges. Furthermore, we identify challenges in reliable response generation, such as accuracy, real-time capabilities, bias and fairness, privacy, and security. We propose potential solutions and future research directions to tackle these challenges.
Finally, we also envision a unified framework, including unified retrieval and generation tasks, and even building an end-to-end framework capable of handling various information retrieval tasks. Through this review, we hope to provide a comprehensive reference for researchers in the GenIR field to further promote the development of this area.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google [2023a]</span>
<span class="ltx_bibblock">
Google, “Google,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">https://www.google.com</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Microsoft [2023a]</span>
<span class="ltx_bibblock">
Microsoft, “Bing,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">https://www.bing.com</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2022]</span>
<span class="ltx_bibblock">
OpenAI, “Introducing chatgpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">https://openai.com/blog/chatgpt</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Microsoft [2023b]</span>
<span class="ltx_bibblock">
Microsoft, “Bing chat,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">https://www.bing.com/new</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amazon [2023]</span>
<span class="ltx_bibblock">
Amazon, “Amazon,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">https://www.amazon.com</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Google [2023b]</span>
<span class="ltx_bibblock">
Google, “Youtube,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">https://www.youtube.com</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salton et al. [1983]</span>
<span class="ltx_bibblock">
G. Salton, E. A. Fox, and H. Wu, “Extended boolean information retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Commun. ACM</em>, vol. 26, no. 11, pp. 1022–1036, 1983.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson and Zaragoza [2009]</span>
<span class="ltx_bibblock">
S. E. Robertson and H. Zaragoza, “The probabilistic relevance framework: BM25 and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Found. Trends Inf. Retr.</em>, vol. 3, no. 4, pp. 333–389, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Formal et al. [2021]</span>
<span class="ltx_bibblock">
T. Formal, B. Piwowarski, and S. Clinchant, “SPLADE: sparse lexical and expansion model for first stage ranking,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">SIGIR ’21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021</em>.   ACM, 2021, pp. 2288–2292.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Ma [2021]</span>
<span class="ltx_bibblock">
J. Lin and X. Ma, “A few brief notes on deepimpact, coil, and a conceptual framework for information retrieval techniques,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, vol. abs/2106.14807, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin et al. [2020]</span>
<span class="ltx_bibblock">
V. Karpukhin, B. Oguz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih, “Dense passage retrieval for open-domain question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">EMNLP</em>, 2020, pp. 6769–6781.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al. [2020]</span>
<span class="ltx_bibblock">
L. Xiong, C. Xiong, Y. Li, K.-F. Tang, J. Liu, P. N. Bennett, J. Ahmed, and A. Overwijk, “Approximate nearest neighbor negative contrastive learning for dense text retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">ICLR</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenton and Toutanova [2019]</span>
<span class="ltx_bibblock">
J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">NAACL-HLT</em>, 2019, pp. 4171–4186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Douze et al. [2024]</span>
<span class="ltx_bibblock">
M. Douze, A. Guzhva, C. Deng, J. Johnson, G. Szilvasy, P.-E. Mazaré, M. Lomeli, L. Hosseini, and H. Jégou, “The faiss library,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">CoRR</em>, vol. abs/2401.08281, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malkov and Yashunin [2020]</span>
<span class="ltx_bibblock">
Y. A. Malkov and D. A. Yashunin, “Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</em>, vol. 42, no. 4, pp. 824–836, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Metzler et al. [2021]</span>
<span class="ltx_bibblock">
D. Metzler, Y. Tay, D. Bahri, and M. Najork, “Rethinking search: making domain experts out of dilettantes,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">ACM SIGIR Forum</em>, vol. 55, no. 1.   ACM New York, NY, USA, 2021, pp. 1–27.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. [2020]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-text transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">J. Mach. Learn. Res.</em>, vol. 21, pp. 140:1–140:67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. [2020a]</span>
<span class="ltx_bibblock">
M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ACL</em>, 2020, pp. 7871–7880.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2018]</span>
<span class="ltx_bibblock">
A. Radford, K. Narasimhan, T. Salimans, I. Sutskever <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">et al.</em>, “Improving language understanding by generative pre-training,” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. [2023]</span>
<span class="ltx_bibblock">
W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong, Y. Du, C. Yang, Y. Chen, Z. Chen, J. Jiang, R. Ren, Y. Li, X. Tang, Z. Liu, P. Liu, J.-Y. Nie, and J.-R. Wen, “A survey of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">CoRR</em>, vol. abs/2303.18223, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2023]</span>
<span class="ltx_bibblock">
Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, and L. Sun, “A comprehensive survey of ai-generated content (AIGC): A history of generative AI from GAN to chatgpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">CoRR</em>, vol. abs/2303.04226, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christiano et al. [2017]</span>
<span class="ltx_bibblock">
P. F. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei, “Deep reinforcement learning from human preferences,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, 2017, pp. 4299–4307.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023a]</span>
<span class="ltx_bibblock">
H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et al.</em>, “Llama 2: Open foundation and fine-tuned chat models,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. [2023]</span>
<span class="ltx_bibblock">
J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang, B. Hui, L. Ji, M. Li, J. Lin, R. Lin, D. Liu, G. Liu, C. Lu, K. Lu, J. Ma, R. Men, X. Ren, X. Ren, C. Tan, S. Tan, J. Tu, P. Wang, S. Wang, W. Wang, S. Wu, B. Xu, J. Xu, A. Yang, H. Yang, J. Yang, S. Yang, Y. Yao, B. Yu, H. Yuan, Z. Yuan, J. Zhang, X. Zhang, Y. Zhang, Z. Zhang, C. Zhou, J. Zhou, X. Zhou, and T. Zhu, “Qwen technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">CoRR</em>, vol. abs/2309.16609, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023a]</span>
<span class="ltx_bibblock">
A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. de Las Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, L. R. Lavaud, M.-A. Lachaux, P. Stock, T. L. Scao, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, “Mistral 7b,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">CoRR</em>, vol. abs/2310.06825, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thoppilan et al. [2022]</span>
<span class="ltx_bibblock">
R. Thoppilan, D. D. Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du, Y. Li, H. Lee, H. S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, K. S. Meier-Hellstern, M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John, J. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R. Kurzweil, B. A. y Arcas, C. Cui, M. Croak, E. H. Chi, and Q. Le, “Lamda: Language models for dialog applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">CoRR</em>, vol. abs/2201.08239, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023a]</span>
<span class="ltx_bibblock">
X. Liu, H. Lai, H. Yu, Y. Xu, A. Zeng, Z. Du, P. Zhang, Y. Dong, and J. Tang, “Webglm: Towards an efficient web-enhanced question answering system with human preferences,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023</em>.   ACM, 2023, pp. 4549–4560.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. [2023a]</span>
<span class="ltx_bibblock">
Y. Qin, Z. Cai, D. Jin, L. Yan, S. Liang, K. Zhu, Y. Lin, X. Han, N. Ding, H. Wang, R. Xie, F. Qi, Z. Liu, M. Sun, and J. Zhou, “WebCPM: Interactive web search for Chinese long-form question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.   Toronto, Canada: Association for Computational Linguistics, Jul. 2023, pp. 8968–8988. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.acl-long.499" title="">https://aclanthology.org/2023.acl-long.499</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2021a]</span>
<span class="ltx_bibblock">
N. D. Cao, G. Izacard, S. Riedel, and F. Petroni, “Autoregressive entity retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>.   OpenReview.net, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al. [2022]</span>
<span class="ltx_bibblock">
Y. Tay, V. Tran, M. Dehghani, J. Ni, D. Bahri, H. Mehta, Z. Qin, K. Hui, Z. Zhao, J. P. Gupta, T. Schuster, W. W. Cohen, and D. Metzler, “Transformer memory as a differentiable search index,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022a]</span>
<span class="ltx_bibblock">
Y. Wang, Y. Hou, H. Wang, Z. Miao, S. Wu, H. Sun, Q. Chen, Y. Xia, C. Chi, G. Zhao, Z. Liu, X. Xie, H. A. Sun, W. Deng, Q. Zhang, and M. Yang, “A neural corpus indexer for document retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">CoRR</em>, vol. abs/2206.02743, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2022]</span>
<span class="ltx_bibblock">
Y. Zhou, J. Yao, Z. Dou, L. Wu, P. Zhang, and J.-R. Wen, “Ultron: An ultimate retriever on corpus with a model-based indexer,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">CoRR</em>, vol. abs/2208.09257, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2023a]</span>
<span class="ltx_bibblock">
Y. Zhou, J. Yao, L. Wu, Z. Dou, and J.-R. Wen, “Webultron: An ultimate retriever on webpages under the model-centric paradigm,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">IEEE Transactions on Knowledge and Data Engineering</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2023b]</span>
<span class="ltx_bibblock">
Y. Zhou, Z. Dou, and J.-R. Wen, “Enhancing generative retrieval with reinforcement learning from relevance feedback,” in <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 12 481–12 490.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023a]</span>
<span class="ltx_bibblock">
W. Sun, L. Yan, Z. Chen, S. Wang, H. Zhu, P. Ren, Z. Chen, D. Yin, M. de Rijke, and Z. Ren, “Learning to tokenize for generative retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">CoRR</em>, vol. abs/2304.04171, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023a]</span>
<span class="ltx_bibblock">
T. Yang, M. Song, Z. Zhang, H. Huang, W. Deng, F. Sun, and Q. Zhang, “Auto search indexer for end-to-end document retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 6955–6970.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehta et al. [2022]</span>
<span class="ltx_bibblock">
S. V. Mehta, J. Gupta, Y. Tay, M. Dehghani, V. Q. Tran, J. Rao, M. Najork, E. Strubell, and D. Metzler, “Dsi++: Updating transformer memory with new documents,” <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2212.09744</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kishore et al. [2023]</span>
<span class="ltx_bibblock">
V. Kishore, C. Wan, J. Lovelace, Y. Artzi, and K. Q. Weinberger, “Incdsi: Incrementally updatable document retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA</em>, ser. Proceedings of Machine Learning Research, vol. 202.   PMLR, 2023, pp. 17 122–17 134.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. [2024]</span>
<span class="ltx_bibblock">
J. Guo, C. Zhou, R. Zhang, J. Chen, M. de Rijke, Y. Fan, and X. Cheng, “Corpusbrain++: A continual generative pre-training framework for knowledge-intensive language tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2402.16767</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022a]</span>
<span class="ltx_bibblock">
J. Chen, R. Zhang, J. Guo, Y. Fan, and X. Cheng, “Gere: Generative evidence retrieval for fact verification,” <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2204.05511</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022b]</span>
<span class="ltx_bibblock">
J. Chen, R. Zhang, J. Guo, Y. Liu, Y. Fan, and X. Cheng, “Corpusbrain: Pre-train a generative retrieval model for knowledge-intensive language tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management, Atlanta, GA, USA, October 17-21, 2022</em>.   ACM, 2022, pp. 191–200.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024a]</span>
<span class="ltx_bibblock">
X. Li, Z. Dou, Y. Zhou, and F. Liu, “Towards a unified language model for knowledge-intensive tasks utilizing external corpus,” <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">CoRR</em>, vol. abs/2402.01176, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023a]</span>
<span class="ltx_bibblock">
Y. Zhang, T. Zhang, D. Chen, Y. Wang, Q. Chen, X. Xie, H. Sun, W. Deng, Q. Zhang, F. Yang, M. Yang, Q. Liao, and B. Guo, “Irgen: Generative modeling for image retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">CoRR</em>, vol. abs/2303.10126, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long et al. [2024]</span>
<span class="ltx_bibblock">
X. Long, J. Zeng, F. Meng, Z. Ma, K. Zhang, B. Zhou, and J. Zhou, “Generative multi-modal knowledge retrieval with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">CoRR</em>, vol. abs/2401.08206, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024b]</span>
<span class="ltx_bibblock">
Y. Li, W. Wang, L. Qu, L. Nie, W. Li, and T.-S. Chua, “Generative cross-modal retrieval: Memorizing images in multimodal language models for retrieval and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">CoRR</em>, vol. abs/2402.10805, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al. [2022]</span>
<span class="ltx_bibblock">
S. Geng, S. Liu, Z. Fu, Y. Ge, and Y. Zhang, “Recommendation as language processing (RLP): A unified pretrain, personalized prompt &amp; predict paradigm (P5),” in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">RecSys ’22: Sixteenth ACM Conference on Recommender Systems, Seattle, WA, USA, September 18 - 23, 2022</em>.   ACM, 2022, pp. 299–315.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023a]</span>
<span class="ltx_bibblock">
W. Wang, X. Lin, F. Feng, X. He, and T.-S. Chua, “Generative recommendation: Towards next-generation recommender paradigm,” <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">CoRR</em>, vol. abs/2304.03516, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajput et al. [2023]</span>
<span class="ltx_bibblock">
S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt, L. Hong, Y. Tay, V. Q. Tran, J. Samost, M. Kula, E. H. Chi, and M. Sathiamoorthy, “Recommender systems with generative retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bénédict et al. [2023]</span>
<span class="ltx_bibblock">
G. Bénédict, R. Zhang, and D. Metzler, “Gen-ir@ sigir 2023: The first workshop on generative information retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2023, pp. 3460–3463.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023a]</span>
<span class="ltx_bibblock">
Y. Tang, R. Zhang, J. Guo, and M. de Rijke, “Recent advances in generative information retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region, SIGIR-AP 2023, Beijing, China, November 26-28, 2023</em>.   ACM, 2023, pp. 294–297.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et al. [2021]</span>
<span class="ltx_bibblock">
R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight, B. Chess, and J. Schulman, “Webgpt: Browser-assisted question-answering with human feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">CoRR</em>, vol. abs/2112.09332, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al. [2023a]</span>
<span class="ltx_bibblock">
H. Qian, Y. Zhu, Z. Dou, H. Gu, X. Zhang, Z. Liu, R. Lai, Z. Cao, J.-Y. Nie, and J.-R. Wen, “Webbrain: Learning to generate factually correct articles for queries by grounding on large web corpus,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">CoRR</em>, vol. abs/2304.04358, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sallam et al. [2023]</span>
<span class="ltx_bibblock">
M. Sallam, N. A. Salim, A. B. Al-Tammemi, M. M. Barakat, D. Fayyad, S. Hallit, H. Harapan, R. Hallit, and A. Mahafzah, “Chatgpt output regarding compulsory vaccination and covid-19 vaccine conspiracy: A descriptive study at the outset of a paradigm shift in online search for information,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Cureus</em>, vol. 15, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:256897987" title="">https://api.semanticscholar.org/CorpusID:256897987</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gienapp et al. [2023]</span>
<span class="ltx_bibblock">
L. Gienapp, H. Scells, N. Deckers, J. Bevendorff, S. Wang, J. Kiesel, S. Syed, M. Frobe, G. Zucoon, B. Stein, M. Hagen, and M. Potthast, “Evaluating generative ad hoc information retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">ArXiv</em>, vol. abs/2311.04694, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:265050661" title="">https://api.semanticscholar.org/CorpusID:265050661</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White [2023]</span>
<span class="ltx_bibblock">
R. W. White, “Tasks, copilots, and the future of search: A keynote at SIGIR 2023,” <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">SIGIR Forum</em>, vol. 57, no. 2, pp. 4:1–4:8, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hersh [2023]</span>
<span class="ltx_bibblock">
W. R. Hersh, “Search still matters: Information retrieval in the era of generative AI,” <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">CoRR</em>, vol. abs/2311.18550, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. [2023a]</span>
<span class="ltx_bibblock">
Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">ACM Computing Surveys</em>, vol. 55, no. 12, pp. 1–38, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023a]</span>
<span class="ltx_bibblock">
L. Huang, W. Yu, W. Ma, W. Zhong, Z. Feng, H. Wang, Q. Chen, W. Peng, X. Feng, B. Qin, and T. Liu, “A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions,” <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">CoRR</em>, vol. abs/2311.05232, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al. [2023]</span>
<span class="ltx_bibblock">
T. Vu, M. Iyyer, X. Wang, N. Constant, J. W. Wei, J. Wei, C. Tar, Y.-H. Sung, D. Zhou, Q. V. Le, and T. Luong, “Freshllms: Refreshing large language models with search engine augmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">CoRR</em>, vol. abs/2310.03214, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023b]</span>
<span class="ltx_bibblock">
Y. Huang, Q. Zhang, P. S. Yu, and L. Sun, “Trustgpt: A benchmark for trustworthy and responsible large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">CoRR</em>, vol. abs/2306.11507, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2024]</span>
<span class="ltx_bibblock">
L. Sun, Y. Huang, H. Wang, S. Wu, Q. Zhang, C. Gao, Y. Huang, W. Lyu, Y. Zhang, X. Li, Z. Liu, Y. Liu, Y. Wang, Z. Zhang, B. Kailkhura, C. Xiong, C. Xiao, C. Li, E. P. Xing, F. Huang, H. Liu, H. Ji, H. Wang, H. Zhang, H. Yao, M. Kellis, M. Zitnik, M. Jiang, M. Bansal, J. Zou, J. Pei, J. Liu, J. Gao, J. Han, J. Zhao, J. Tang, J. Wang, J. Mitchell, K. Shu, K. Xu, K.-W. Chang, L. He, L. Huang, M. Backes, N. Z. Gong, P. S. Yu, P.-Y. Chen, Q. Gu, R. Xu, R. Ying, S. Ji, S. Jana, T. Chen, T. Liu, T. Zhou, W. Wang, X. Li, X. Zhang, X. Wang, X. Xie, X. Chen, X. Wang, Y. Liu, Y. Ye, Y. Cao, and Y. Zhao, “Trustllm: Trustworthiness in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">CoRR</em>, vol. abs/2401.05561, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. [2020]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">et al.</em>, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib62.2.2">Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et al. [2022]</span>
<span class="ltx_bibblock">
T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow, R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman, A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Laurençon, Y. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I. Adelani, and et al., “BLOOM: A 176b-parameter open-access multilingual language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">CoRR</em>, vol. abs/2211.05100, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023b]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">et al.</em>, “Llama: Open and efficient foundation language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.2.2">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2022a]</span>
<span class="ltx_bibblock">
K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini, “Deduplicating training data makes language models better,” in <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>.   Association for Computational Linguistics, 2022, pp. 8424–8445.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al. [2022]</span>
<span class="ltx_bibblock">
Q. Dong, D. Dai, Y. Song, J. Xu, Z. Sui, and L. Li, “Calibrating factual knowledge in pretrained language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>.   Association for Computational Linguistics, 2022, pp. 5937–5947.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chuang et al. [2023]</span>
<span class="ltx_bibblock">
Y.-S. Chuang, Y. Xie, H. Luo, Y. Kim, J. R. Glass, and P. He, “Dola: Decoding by contrasting layers improves factuality in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">CoRR</em>, vol. abs/2309.03883, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2020]</span>
<span class="ltx_bibblock">
Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and H. Wang, “Ernie 2.0: A continual pre-training framework for language understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 34, no. 05, 2020, pp. 8968–8975.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al. [2023]</span>
<span class="ltx_bibblock">
Z. Ke, Y. Shao, H. Lin, T. Konishi, G. Kim, and B. Liu, “Continual pre-training of language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2302.03241</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. [2022]</span>
<span class="ltx_bibblock">
K. Meng, D. Bau, A. Andonian, and Y. Belinkov, “Locating and editing factual associations in GPT,” in <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. [2020b]</span>
<span class="ltx_bibblock">
P. S. H. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W. tau Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-augmented generation for knowledge-intensive NLP tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024a]</span>
<span class="ltx_bibblock">
H. Wang, B. Xue, B. Zhou, T. Zhang, C. Wang, G. Chen, H. Wang, and K.-f. Wong, “Self-dc: When to retrieve and when to generate? self divide-and-conquer for compositional unknown questions,” <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2402.13514</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. [2023]</span>
<span class="ltx_bibblock">
A. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, “Self-rag: Learning to retrieve, generate, and critique through self-reflection,” <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">CoRR</em>, vol. abs/2310.11511, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2022]</span>
<span class="ltx_bibblock">
S. Yao, J. Zhao, D. Yu, I. Shafran, K. R. Narasimhan, and Y. Cao, “React: Synergizing reasoning and acting in language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">NeurIPS 2022 Foundation Models for Decision Making Workshop</em>, 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=tvI4u1ylcqs" title="">https://openreview.net/forum?id=tvI4u1ylcqs</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023b]</span>
<span class="ltx_bibblock">
J. Jiang, K. Zhou, Z. Dong, K. Ye, X. Zhao, and J.-R. Wen, “StructGPT: A general framework for large language model to reason over structured data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.   Singapore: Association for Computational Linguistics, Dec. 2023, pp. 9237–9251. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.574" title="">https://aclanthology.org/2023.emnlp-main.574</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023c]</span>
<span class="ltx_bibblock">
Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y. Yang, J. Callan, and G. Neubig, “Active retrieval augmented generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 7969–7992.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. [2023]</span>
<span class="ltx_bibblock">
T. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom, “Toolformer: Language models can teach themselves to use tools,” <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2302.04761</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weller et al. [2023]</span>
<span class="ltx_bibblock">
O. Weller, M. Marone, N. Weir, D. Lawrie, D. Khashabi, and B. Van Durme, “” according to…” prompting language models improves quoting from pre-training data,” <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2305.13252</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2023a]</span>
<span class="ltx_bibblock">
D. Lee, T. Whang, C. Lee, and H. Lim, “Towards reliable and fluent large language models: Incorporating feedback learning loops in QA systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">CoRR</em>, vol. abs/2309.06384, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023a]</span>
<span class="ltx_bibblock">
X. Li, C. Zhu, L. Li, Z. Yin, T. Sun, and X. Qiu, “Llatrieval: Llm-verified retrieval for verifiable generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">CoRR</em>, vol. abs/2311.07838, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024c]</span>
<span class="ltx_bibblock">
D. Li, Z. Sun, B. Hu, Z. Liu, X. Hu, X. Liu, and M. Zhang, “Improving attributed text generation of large language models via preference learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">CoRR</em>, vol. abs/2403.18381, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2020]</span>
<span class="ltx_bibblock">
Q. Liu, Y. Chen, B. Chen, J.-G. Lou, Z. Chen, B. Zhou, and D. Zhang, “You impress me: Dialogue generation via mutual persona perception,” in <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020</em>.   Association for Computational Linguistics, 2020, pp. 1417–1427.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023b]</span>
<span class="ltx_bibblock">
H. Wang, M. Hu, Y. Deng, R. Wang, F. Mi, W. Wang, Y. Wang, W.-C. Kwan, I. King, and K.-F. Wong, “Large language models as source planner for personalized knowledge-grounded dialogues,” in <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 9556–9569.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023a]</span>
<span class="ltx_bibblock">
X. Xu, B. Yao, Y. Dong, H. Yu, J. Hendler, A. K. Dey, and D. Wang, “Leveraging large language models for mental health prediction via online text data,” <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2307.14385</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li and Tuzhilin [2019]</span>
<span class="ltx_bibblock">
P. Li and A. Tuzhilin, “Towards controllable and personalized review generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>.   Association for Computational Linguistics, 2019, pp. 3235–3243.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2023c]</span>
<span class="ltx_bibblock">
Y. Zhou, J. Yao, Z. Dou, L. Wu, and J.-R. Wen, “Dynamicretriever: A pre-trained model-based IR system without an explicit index,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Mach. Intell. Res.</em>, vol. 20, no. 2, pp. 276–288, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al. [2022]</span>
<span class="ltx_bibblock">
S. Zhuang, H. Ren, L. Shou, J. Pei, M. Gong, G. Zuccon, and D. Jiang, “Bridging the gap between indexing and retrieval for differentiable search index with query generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">CoRR</em>, vol. abs/2206.10128, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023a]</span>
<span class="ltx_bibblock">
X. Chen, Y. Liu, B. He, L. Sun, and Y. Sun, “Understanding differential search index for text retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>.   Association for Computational Linguistics, 2023, pp. 10 701–10 717.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023b]</span>
<span class="ltx_bibblock">
Y. Li, N. Yang, L. Wang, F. Wei, and W. Li, “Learning to rank in generative retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">CoRR</em>, vol. abs/2306.15222, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024d]</span>
<span class="ltx_bibblock">
Y. Li, Z. Zhang, W. Wang, L. Nie, W. Li, and T.-S. Chua, “Distillation enhanced generative retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:2402.10769</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2024a]</span>
<span class="ltx_bibblock">
Y. Tang, R. Zhang, J. Guo, M. de Rijke, W. Chen, and X. Cheng, “Listwise generative retrieval models via a sequential learning process,” <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">ACM Transactions on Information Systems</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. [2023]</span>
<span class="ltx_bibblock">
R. Ren, W. X. Zhao, J. Liu, H. Wu, J.-R. Wen, and H. Wang, “TOME: A two-stage approach for model-based retrieval,” pp. 6102–6114, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2023b]</span>
<span class="ltx_bibblock">
H. Lee, J. Kim, H. Chang, H. Oh, S. Yang, V. Karpukhin, Y. Lu, and M. Seo, “Nonparametric decoding for generative retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>.   Association for Computational Linguistics, 2023, pp. 12 642–12 661.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023b]</span>
<span class="ltx_bibblock">
H. Zhang, Y. Wang, Q. Chen, R. Chang, T. Zhang, Z. Miao, Y. Hou, Y. Ding, X. Miao, H. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">et al.</em>, “Model-enhanced vector index,” <em class="ltx_emph ltx_font_italic" id="bib.bib94.2.2">arXiv preprint arXiv:2309.13335</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiao et al. [2023]</span>
<span class="ltx_bibblock">
S. Qiao, X. Liu, and S.-H. Na, “Diffusionret: Diffusion-enhanced generative retriever using constrained decoding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 9515–9529.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2024]</span>
<span class="ltx_bibblock">
P. Yuan, X. Wang, S. Feng, B. Pan, Y. Li, H. Wang, X. Miao, and K. Li, “Generative dense retrieval: Memory can be a burden,” <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">CoRR</em>, vol. abs/2401.10487, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2024b]</span>
<span class="ltx_bibblock">
Q. Tang, J. Chen, B. Yu, Y. Lu, C. Fu, H. Yu, H. Lin, F. Huang, B. He, X. Han <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">et al.</em>, “Self-retrieval: Building an information retrieval system with one large language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib97.2.2">arXiv preprint arXiv:2403.00801</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2024]</span>
<span class="ltx_bibblock">
H. Zeng, C. Luo, and H. Zamani, “Planning ahead in generative retrieval: Guiding autoregressive generation through simultaneous decoding,” <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">arXiv preprint arXiv:2404.14600</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen and Yates [2023]</span>
<span class="ltx_bibblock">
T. Nguyen and A. Yates, “Generative retrieval as dense retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">CoRR</em>, vol. abs/2306.11397, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2023]</span>
<span class="ltx_bibblock">
B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li, Y. Li, H. Lu <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">et al.</em>, “Language models as semantic indexers,” <em class="ltx_emph ltx_font_italic" id="bib.bib100.2.2">arXiv preprint arXiv:2310.07815</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2023]</span>
<span class="ltx_bibblock">
H. Zeng, C. Luo, B. Jin, S. M. Sarwar, T. Wei, and H. Zamani, “Scalable and effective generative information retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">CoRR</em>, vol. abs/2311.09134, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bevilacqua et al. [2022]</span>
<span class="ltx_bibblock">
M. Bevilacqua, G. Ottaviano, P. S. H. Lewis, S. Yih, S. Riedel, and F. Petroni, “Autoregressive search engines: Generating substrings as document identifiers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems et al. [2023]</span>
<span class="ltx_bibblock">
N. Ziems, W. Yu, Z. Zhang, and M. Jiang, “Large language models are built-in autoregressive search engines,” in <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Findings of the Association for Computational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023</em>.   Association for Computational Linguistics, 2023, pp. 2666–2678.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023b]</span>
<span class="ltx_bibblock">
J. Chen, R. Zhang, J. Guo, M. de Rijke, Y. Liu, Y. Fan, and X. Cheng, “A unified generative retriever for knowledge-intensive language tasks via prompt learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023</em>.   ACM, 2023, pp. 1448–1457.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023c]</span>
<span class="ltx_bibblock">
Y. Li, N. Yang, L. Wang, F. Wei, and W. Li, “Multiview identifiers enhanced generative retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>.   Association for Computational Linguistics, 2023, pp. 6636–6648.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023c]</span>
<span class="ltx_bibblock">
P. Zhang, Z. Liu, Y. Zhou, Z. Dou, and Z. Cao, “Term-sets can be strong document identifiers for auto-regressive search engines,” <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">CoRR</em>, vol. abs/2305.13859, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023c]</span>
<span class="ltx_bibblock">
Z. Wang, Y. Zhou, Y. Tu, and Z. Dou, “NOVO: learnable and interpretable document identifiers for model-based IR,” in <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023, Birmingham, United Kingdom, October 21-25, 2023</em>.   ACM, 2023, pp. 2656–2665.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2023c]</span>
<span class="ltx_bibblock">
S. Lee, M. Choi, and J. Lee, “GLEN: generative retrieval via lexical index learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 7693–7704.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023c]</span>
<span class="ltx_bibblock">
J. Chen, R. Zhang, J. Guo, M. de Rijke, W. Chen, Y. Fan, and X. Cheng, “Continual learning for generative retrieval over dynamic corpora,” in <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>, 2023, pp. 306–315.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2022b]</span>
<span class="ltx_bibblock">
H. Lee, S. Yang, H. Oh, and M. Seo, “Generative multi-hop retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>.   Association for Computational Linguistics, 2022, pp. 1417–1436.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne [2022]</span>
<span class="ltx_bibblock">
J. Thorne, “Data-efficient autoregressive document retrieval for fact verification,” <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">CoRR</em>, vol. abs/2211.09388, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nadeem et al. [2022]</span>
<span class="ltx_bibblock">
U. Nadeem, N. Ziems, and S. Wu, “Codedsi: Differentiable code search,” <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">CoRR</em>, vol. abs/2210.00328, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023d]</span>
<span class="ltx_bibblock">
Y. Li, N. Yang, L. Wang, F. Wei, and W. Li, “Generative retrieval for conversational question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Inf. Process. Manag.</em>, vol. 60, no. 5, p. 103475, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2024]</span>
<span class="ltx_bibblock">
E. Song, S. Kim, H. Lee, J. Kim, and J. Thorne, “Re3val: Reinforced and reranked generative retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">CoRR</em>, vol. abs/2401.16979, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024e]</span>
<span class="ltx_bibblock">
X. Li, Y. Zhou, and Z. Dou, “Unigen: A unified generative framework for retrieval and question answering with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 8, 2024, pp. 8688–8696.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Si et al. [2023]</span>
<span class="ltx_bibblock">
Z. Si, Z. Sun, J. Chen, G. Chen, X. Zang, K. Zheng, Y. Song, X. Zhang, and J. Xu, “Generative retrieval with semantic tree-structured item identifiers via contrastive learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">CoRR</em>, vol. abs/2309.13375, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. [2024a]</span>
<span class="ltx_bibblock">
J. Tan, S. Xu, W. Hua, Y. Ge, Z. Li, and Y. Zhang, “Towards llm-recsys alignment with textual id learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">arXiv preprint arXiv:2403.19021</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2023]</span>
<span class="ltx_bibblock">
B. Zheng, Y. Hou, H. Lu, Y. Chen, W. X. Zhao, and J.-R. Wen, “Adapting large language models by integrating collaborative semantics for recommendation,” <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">arXiv preprint arXiv:2311.09049</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024b]</span>
<span class="ltx_bibblock">
Y. Wang, Z. Ren, W. Sun, J. Yang, Z. Liang, X. Chen, R. Xie, S. Yan, X. Zhang, P. Ren <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">et al.</em>, “Enhanced generative recommendation via content and collaboration integration,” <em class="ltx_emph ltx_font_italic" id="bib.bib119.2.2">arXiv preprint arXiv:2403.18480</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. [2023]</span>
<span class="ltx_bibblock">
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel, “Palm: Scaling language modeling with pathways,” <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">J. Mach. Learn. Res.</em>, vol. 24, pp. 240:1–240:113, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2024]</span>
<span class="ltx_bibblock">
A. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary, C. Bamford, D. S. Chaplot, D. de Las Casas, E. B. Hanna, F. Bressand, G. Lengyel, G. Bour, G. Lample, L. R. Lavaud, L. Saulnier, M.-A. Lachaux, P. Stock, S. Subramanian, S. Yang, S. Antoniak, T. L. Scao, T. Gervet, T. Lavril, T. Wang, T. Lacroix, and W. E. Sayed, “Mixtral of experts,” <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">CoRR</em>, vol. abs/2401.04088, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sadeq et al. [2023]</span>
<span class="ltx_bibblock">
N. Sadeq, B. Kang, P. Lamba, and J. J. McAuley, “Unsupervised improvement of factual knowledge in language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023</em>.   Association for Computational Linguistics, 2023, pp. 2952–2961.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovadia et al. [2023a]</span>
<span class="ltx_bibblock">
O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or retrieval? comparing knowledge injection in llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">CoRR</em>, vol. abs/2312.05934, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2022]</span>
<span class="ltx_bibblock">
W. Yu, D. Iter, S. Wang, Y. Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng, and M. Jiang, “Generate rather than retrieve: Large language models are strong context generators,” <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">arXiv preprint arXiv:2209.10063</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2022a]</span>
<span class="ltx_bibblock">
Z. Sun, X. Wang, Y. Tay, Y. Yang, and D. Zhou, “Recitation-augmented language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">arXiv preprint arXiv:2210.01296</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mok et al. [2023]</span>
<span class="ltx_bibblock">
J. Mok, J. Do, S. Lee, T. Taghavi, S. Yu, and S. Yoon, “Large-scale lifelong learning of in-context instructions and how to tackle it,” in <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 2023, pp. 12 573–12 589.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al. [2021b]</span>
<span class="ltx_bibblock">
N. D. Cao, W. Aziz, and I. Titov, “Editing factual knowledge in language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021</em>.   Association for Computational Linguistics, 2021, pp. 6491–6506.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitchell et al. [2022]</span>
<span class="ltx_bibblock">
E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning, “Fast model editing at scale,” in <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.   OpenReview.net, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2023a]</span>
<span class="ltx_bibblock">
X. Ma, Y. Gong, P. He, H. Zhao, and N. Duan, “Query rewriting in retrieval-augmented large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.   Singapore: Association for Computational Linguistics, Dec. 2023, pp. 5303–5315. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.322" title="">https://aclanthology.org/2023.emnlp-main.322</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024a]</span>
<span class="ltx_bibblock">
L. Zhang, Y. Yu, K. Wang, and C. Zhang, “Arl2: Aligning retrievers for black-box large language models via self-guided adaptive relevance labeling,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2023]</span>
<span class="ltx_bibblock">
G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifications: Answering ambiguous questions with retrieval-augmented large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.   Singapore: Association for Computational Linguistics, Dec. 2023, pp. 996–1009. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.63" title="">https://aclanthology.org/2023.emnlp-main.63</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024c]</span>
<span class="ltx_bibblock">
H. Wang, T. Zhao, and J. Gao, “Blendfilter: Advancing retrieval-augmented large language models via query generation blending and knowledge filtering,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2023a]</span>
<span class="ltx_bibblock">
W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W. tau Yih, “REPLUG: retrieval-augmented black-box language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">CoRR</em>, vol. abs/2301.12652, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023d]</span>
<span class="ltx_bibblock">
Y. Wang, P. Li, M. Sun, and Y. Liu, “Self-knowledge guided retrieval augmentation for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">arXiv preprint arXiv:2310.05002</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. [2024]</span>
<span class="ltx_bibblock">
H. Ding, L. Pang, Z. Wei, H. Shen, and X. Cheng, “Retrieve only when it needs: Adaptive retrieval augmentation for hallucination mitigation in large language models,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. [2023]</span>
<span class="ltx_bibblock">
Z. Shao, Y. Gong, Y. Shen, M. Huang, N. Duan, and W. Chen, “Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al. [2022]</span>
<span class="ltx_bibblock">
H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions,” <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">arXiv preprint arXiv:2212.10509</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023b]</span>
<span class="ltx_bibblock">
J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, H.-Y. Shum, and J. Guo, “Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2024]</span>
<span class="ltx_bibblock">
L. Luo, Y.-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful and interpretable large language model reasoning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. [2023b]</span>
<span class="ltx_bibblock">
Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun, “Toolllm: Facilitating large language models to master 16000+ real-world apis,” <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">CoRR</em>, vol. abs/2307.16789, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2023a]</span>
<span class="ltx_bibblock">
D. Gao, L. Ji, L. Zhou, K. Q. Lin, J. Chen, Z. Fan, and M. Z. Shou, “Assistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. [2023]</span>
<span class="ltx_bibblock">
Y. Shen, K. Song, X. Tan, D. Li, W. Lu, and Y. Zhuang, “Hugginggpt: Solving AI tasks with chatgpt and its friends in hugging face,” in <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023a]</span>
<span class="ltx_bibblock">
C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan, “Visual chatgpt: Talking, drawing and editing with visual foundation models,” <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">arXiv preprint arXiv:2303.04671</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fierro et al. [2024]</span>
<span class="ltx_bibblock">
C. Fierro, R. K. Amplayo, F. Huot, N. De Cao, J. Maynez, S. Narayan, and M. Lapata, “Learning to plan and generate text with citations,” <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">arXiv preprint arXiv:2404.03381</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peskoff and Stewart [2023]</span>
<span class="ltx_bibblock">
D. Peskoff and B. Stewart, “Credible without credit: Domain experts assess generative language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>.   Association for Computational Linguistics, 2023, pp. 427–438.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. [2023]</span>
<span class="ltx_bibblock">
P. Jain, L. Soares, and T. Kwiatkowski, “1-pager: One pass answer generation and evidence retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 14 529–14 543.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khalifa et al. [2024]</span>
<span class="ltx_bibblock">
M. Khalifa, D. Wadden, E. Strubell, H. Lee, L. Wang, I. Beltagy, and H. Peng, “Source-aware training enables knowledge attribution in language models,” 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:268819100" title="">https://api.semanticscholar.org/CorpusID:268819100</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2023b]</span>
<span class="ltx_bibblock">
L. Gao, Z. Dai, P. Pasupat, A. Chen, A. T. Chaganty, Y. Fan, V. Y. Zhao, N. Lao, H. Lee, D.-C. Juan, and K. Guu, “RARR: researching and revising what language models say, using language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>.   Association for Computational Linguistics, 2023, pp. 16 477–16 508.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023b]</span>
<span class="ltx_bibblock">
S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-chain: Towards the accurate, credible and traceable content generation for complex knowledge-intensive tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">CoRR</em>, vol. abs/2304.14732, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2023c]</span>
<span class="ltx_bibblock">
T. Gao, H. Yen, J. Yu, and D. Chen, “Enabling large language models to generate text with citations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 6465–6488.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023c]</span>
<span class="ltx_bibblock">
H. Sun, H. Cai, B. Wang, Y. Hou, X. Wei, S. Wang, Y. Zhang, and D. Yin, “Towards verifiable text generation with evolving memory and self-reflection,” <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">CoRR</em>, vol. abs/2312.09075, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024f]</span>
<span class="ltx_bibblock">
W. Li, J. Li, W. Ma, and Y. Liu, “Citation-enhanced generation for llm-based chatbots,” <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">CoRR</em>, vol. abs/2402.16063, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2018]</span>
<span class="ltx_bibblock">
S. Zhang, E. Dinan, J. Urbanek, A. Szlam, D. Kiela, and J. Weston, “Personalizing dialogue agents: I have a dog, do you have pets too?” 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2021]</span>
<span class="ltx_bibblock">
Y. Wu, X. Ma, and D. Yang, “Personalized response generation via generative split memory network,” in <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, 2021, pp. 1956–1970.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al. [2023a]</span>
<span class="ltx_bibblock">
J. Jang, S. Kim, B. Y. Lin, Y. Wang, J. Hessel, L. Zettlemoyer, H. Hajishirzi, Y. Choi, and P. Ammanabrolu, “Personalized soups: Personalized large language model alignment via post-hoc parameter merging,” <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">CoRR</em>, vol. abs/2310.11564, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. [2024b]</span>
<span class="ltx_bibblock">
Z. Tan, Q. Zeng, Y. Tian, Z. Liu, B. Yin, and M. Jiang, “Democratizing large language models via personalized parameter-efficient fine-tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">CoRR</em>, vol. abs/2402.04401, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023b]</span>
<span class="ltx_bibblock">
Z. Liu, Z. Wu, M. Hu, B. Zhao, L. Zhao, T. Zhang, H. Dai, X. Chen, Y. Shen, S. Li <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">et al.</em>, “Pharmacygpt: The ai pharmacist,” <em class="ltx_emph ltx_font_italic" id="bib.bib157.2.2">arXiv preprint arXiv:2307.10432</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mysore et al. [2023]</span>
<span class="ltx_bibblock">
S. Mysore, Z. Lu, M. Wan, L. Yang, S. Menezes, T. Baghaee, E. B. Gonzalez, J. Neville, and T. Safavi, “PEARL: personalizing large language model writing assistants with generation-calibrated retrievers,” <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">CoRR</em>, vol. abs/2311.09180, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dan et al. [2023]</span>
<span class="ltx_bibblock">
Y. Dan, Z. Lei, Y. Gu, Y. Li, J. Yin, J. Lin, L. Ye, Z. Tie, Y. Zhou, Y. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">et al.</em>, “Educhat: A large-scale language model-based chatbot system for intelligent education,” <em class="ltx_emph ltx_font_italic" id="bib.bib159.2.2">arXiv preprint arXiv:2308.02773</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell [2009]</span>
<span class="ltx_bibblock">
N. Craswell, “Mean reciprocal rank,” in <em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">Encyclopedia of Database Systems</em>.   Springer US, 2009, p. 1703.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Järvelin and Kekäläinen [2002]</span>
<span class="ltx_bibblock">
K. Järvelin and J. Kekäläinen, “Cumulated gain-based evaluation of IR techniques,” <em class="ltx_emph ltx_font_italic" id="bib.bib161.1.1">ACM Trans. Inf. Syst.</em>, vol. 20, no. 4, pp. 422–446, 2002.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2016]</span>
<span class="ltx_bibblock">
T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder, and L. Deng, “MS MARCO: A human generated machine reading comprehension dataset,” in <em class="ltx_emph ltx_font_italic" id="bib.bib162.1.1">Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain, December 9, 2016</em>, ser. CEUR Workshop Proceedings, vol. 1773.   CEUR-WS.org, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. [2019]</span>
<span class="ltx_bibblock">
T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee <em class="ltx_emph ltx_font_italic" id="bib.bib163.1.1">et al.</em>, “Natural questions: a benchmark for question answering research,” <em class="ltx_emph ltx_font_italic" id="bib.bib163.2.2">Transactions of the Association for Computational Linguistics</em>, vol. 7, pp. 453–466, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi et al. [2017]</span>
<span class="ltx_bibblock">
M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer, “TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension,” in <em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">ACL</em>.   Vancouver, Canada: Association for Computational Linguistics, Jul. 2017, pp. 1601–1611.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. [2021]</span>
<span class="ltx_bibblock">
F. Petroni, A. Piktus, A. Fan, P. Lewis, M. Yazdani, N. De Cao, J. Thorne, Y. Jernite, V. Karpukhin, J. Maillard, V. Plachouras, T. Rocktäschel, and S. Riedel, “KILT: a benchmark for knowledge intensive language tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>.   Online: Association for Computational Linguistics, Jun. 2021, pp. 2523–2544. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.200" title="">https://aclanthology.org/2021.naacl-main.200</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell et al. [2020]</span>
<span class="ltx_bibblock">
N. Craswell, B. Mitra, E. Yilmaz, D. Campos, and E. M. Voorhees, “Overview of the TREC 2019 deep learning track,” <em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">CoRR</em>, vol. abs/2003.07820, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell et al. [2021]</span>
<span class="ltx_bibblock">
N. Craswell, B. Mitra, E. Yilmaz, and D. Campos, “Overview of the TREC 2020 deep learning track,” <em class="ltx_emph ltx_font_italic" id="bib.bib167.1.1">CoRR</em>, vol. abs/2102.07662, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoon et al. [2023]</span>
<span class="ltx_bibblock">
S. Yoon, C. Kim, H. Lee, J. Jang, and M. Seo, “Exploring the practicality of generative retrieval on dynamic corpora,” 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258967398" title="">https://api.semanticscholar.org/CorpusID:258967398</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023c]</span>
<span class="ltx_bibblock">
Y.-A. Liu, R. Zhang, J. Guo, W. Chen, and X. Cheng, “On the robustness of generative retrieval models: An out-of-distribution perspective,” <em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">CoRR</em>, vol. abs/2306.12756, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024b]</span>
<span class="ltx_bibblock">
W. Zhang, M. Zhang, S. Wu, J. Pei, Z. Ren, M. de Rijke, Z. Chen, and P. Ren, “Excluir: Exclusionary neural information retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib170.1.1">arXiv preprint arXiv:2404.17288</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pradeep et al. [2023]</span>
<span class="ltx_bibblock">
R. Pradeep, K. Hui, J. Gupta, Á. D. Lelkes, H. Zhuang, J. Lin, D. Metzler, and V. Q. Tran, “How does generative retrieval scale to millions of passages?” in <em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 1305–1321.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2024a]</span>
<span class="ltx_bibblock">
S. Wu, W. Wei, M. Zhang, Z. Chen, J. Ma, Z. Ren, M. de Rijke, and P. Ren, “Generative retrieval as multi-vector dense retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">arXiv preprint arXiv:2404.00684</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. [2002]</span>
<span class="ltx_bibblock">
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA</em>.   ACL, 2002, pp. 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin [2004]</span>
<span class="ltx_bibblock">
C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,” in <em class="ltx_emph ltx_font_italic" id="bib.bib174.1.1">Annual Meeting of the Association for Computational Linguistics</em>, 2004. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:964287" title="">https://api.semanticscholar.org/CorpusID:964287</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2019]</span>
<span class="ltx_bibblock">
T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, “Bertscore: Evaluating text generation with bert,” 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. [2020]</span>
<span class="ltx_bibblock">
T. Sellam, D. Das, and A. Parikh, “BLEURT: Learning robust metrics for text generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>.   Online: Association for Computational Linguistics, Jul. 2020, pp. 7881–7892. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.704" title="">https://aclanthology.org/2020.acl-main.704</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. [2023]</span>
<span class="ltx_bibblock">
J. Fu, S.-K. Ng, Z. Jiang, and P. Liu, “Gptscore: Evaluate as you desire,” <em class="ltx_emph ltx_font_italic" id="bib.bib177.1.1">CoRR</em>, vol. abs/2302.04166, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. [2023]</span>
<span class="ltx_bibblock">
S. Min, K. Krishna, X. Lyu, M. Lewis, W. tau Yih, P. W. Koh, M. Iyyer, L. Zettlemoyer, and H. Hajishirzi, “Factscore: Fine-grained atomic evaluation of factual precision in long form text generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 12 076–12 100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. [2021]</span>
<span class="ltx_bibblock">
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, “Measuring massive multitask language understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>.   OpenReview.net, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. [2022]</span>
<span class="ltx_bibblock">
A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch, A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz, A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv, A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Rahane, A. S. Iyer, A. Andreassen, A. Santilli, A. Stuhlmüller, A. M. Dai, A. La, A. K. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi, A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan, A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakas, and et al., “Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">CoRR</em>, vol. abs/2206.04615, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Chen [2023]</span>
<span class="ltx_bibblock">
Y.-T. Lin and Y.-N. Chen, “Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">CoRR</em>, vol. abs/2305.13711, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023e]</span>
<span class="ltx_bibblock">
M. Li, Y. Zhao, B. Yu, F. Song, H. Li, H. Yu, Z. Li, F. Huang, and Y. Li, “Api-bank: A comprehensive benchmark for tool-augmented llms,” in <em class="ltx_emph ltx_font_italic" id="bib.bib182.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 3102–3116.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2022]</span>
<span class="ltx_bibblock">
S. Lin, J. Hilton, and O. Evans, “Truthfulqa: Measuring how models mimic human falsehoods,” in <em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>.   Association for Computational Linguistics, 2022, pp. 3214–3252.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023f]</span>
<span class="ltx_bibblock">
J. Li, X. Cheng, X. Zhao, J.-Y. Nie, and J.-R. Wen, “Halueval: A large-scale hallucination evaluation benchmark for large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib184.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 6449–6464.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasai et al. [2023]</span>
<span class="ltx_bibblock">
J. Kasai, K. Sakaguchi, Y. Takahashi, R. L. Bras, A. Asai, X. Yu, D. Radev, N. A. Smith, Y. Choi, and K. Inui, “Realtime QA: what’s the answer right now?” in <em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023d]</span>
<span class="ltx_bibblock">
Z. Zhang, L. Lei, L. Wu, R. Sun, Y. Huang, C. Long, X. Liu, X. Lei, J. Tang, and M. Huang, “Safetybench: Evaluating the safety of large language models with multiple choice questions,” <em class="ltx_emph ltx_font_italic" id="bib.bib186.1.1">CoRR</em>, vol. abs/2309.07045, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramos et al. [2003]</span>
<span class="ltx_bibblock">
J. Ramos <em class="ltx_emph ltx_font_italic" id="bib.bib187.1.1">et al.</em>, “Using tf-idf to determine word relevance in document queries,” in <em class="ltx_emph ltx_font_italic" id="bib.bib187.2.2">Proceedings of the first instructional conference on machine learning</em>, vol. 242, no. 1.   Citeseer, 2003, pp. 29–48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luan et al. [2021]</span>
<span class="ltx_bibblock">
Y. Luan, J. Eisenstein, K. Toutanova, and M. Collins, “Sparse, dense, and attentional representations for text retrieval,” <em class="ltx_emph ltx_font_italic" id="bib.bib188.1.1">Trans. Assoc. Comput. Linguistics</em>, vol. 9, pp. 329–345, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022b]</span>
<span class="ltx_bibblock">
L. Wang, N. Yang, X. Huang, B. Jiao, L. Yang, D. Jiang, R. Majumder, and F. Wei, “Text embeddings by weakly-supervised contrastive pre-training,” <em class="ltx_emph ltx_font_italic" id="bib.bib189.1.1">CoRR</em>, vol. abs/2212.03533, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023e]</span>
<span class="ltx_bibblock">
——, “Simlm: Pre-training with representation bottleneck for dense passage retrieval,” in <em class="ltx_emph ltx_font_italic" id="bib.bib190.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>.   Association for Computational Linguistics, 2023, pp. 2244–2258.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023]</span>
<span class="ltx_bibblock">
Y. Zhu, H. Yuan, S. Wang, J. Liu, W. Liu, C. Deng, Z. Dou, and J.-R. Wen, “Large language models for information retrieval: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">arXiv preprint arXiv:2308.07107</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2019]</span>
<span class="ltx_bibblock">
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever <em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">et al.</em>, “Language models are unsupervised multitask learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib192.2.2">OpenAI blog</em>, vol. 1, no. 8, p. 9, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. [2022]</span>
<span class="ltx_bibblock">
L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe, “Training language models to follow instructions with human feedback,” in <em class="ltx_emph ltx_font_italic" id="bib.bib193.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penedo et al. [2023]</span>
<span class="ltx_bibblock">
G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, H. Alobeidli, A. Cappelli, B. Pannier, E. Almazrouei, and J. Launay, “The refinedweb dataset for falcon LLM: outperforming curated corpora with web data only,” in <em class="ltx_emph ltx_font_italic" id="bib.bib194.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2022]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, “Lora: Low-rank adaptation of large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.   OpenReview.net, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al. [2023]</span>
<span class="ltx_bibblock">
N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C.-M. Chan, W. Chen, J. Yi, W. Zhao, X. Wang, Z. Liu, H.-T. Zheng, J. Chen, Y. Liu, J. Tang, J. Li, and M. Sun, “Parameter-efficient fine-tuning of large-scale pre-trained language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib196.1.1">Nat. Mac. Intell.</em>, vol. 5, no. 3, pp. 220–235, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. [2023]</span>
<span class="ltx_bibblock">
T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient finetuning of quantized llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib197.1.1">CoRR</em>, vol. abs/2305.14314, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2024]</span>
<span class="ltx_bibblock">
Y. Zhu, P. Zhang, C. Zhang, Y. Chen, B. Xie, Z. Dou, Z. Liu, and J.-R. Wen, “INTERS: unlocking the power of large language models in search with instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib198.1.1">CoRR</em>, vol. abs/2401.06532, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. [2017]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in <em class="ltx_emph ltx_font_italic" id="bib.bib199.1.1">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA</em>, 2017, pp. 5998–6008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nogueira et al. [2019]</span>
<span class="ltx_bibblock">
R. Nogueira, J. Lin, and A. Epistemic, “From doc2query to doctttttquery,” <em class="ltx_emph ltx_font_italic" id="bib.bib200.1.1">Online preprint</em>, vol. 6, p. 2, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023d]</span>
<span class="ltx_bibblock">
X. Chen, Y. Liu, B. He, L. Sun, and Y. Sun, “Understanding differential search index for text retrieval,” pp. 10 701–10 717, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez et al. [2014]</span>
<span class="ltx_bibblock">
J. Martinez, H. H. Hoos, and J. J. Little, “Stacked quantizers for compositional vector compression,” <em class="ltx_emph ltx_font_italic" id="bib.bib202.1.1">CoRR</em>, vol. abs/1411.2173, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2022]</span>
<span class="ltx_bibblock">
H. Yuan, Z. Yuan, C. Tan, F. Huang, and S. Huang, “Seqdiffuseq: Text diffusion with encoder-decoder transformers,” <em class="ltx_emph ltx_font_italic" id="bib.bib203.1.1">CoRR</em>, vol. abs/2212.10325, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ferragina and Manzini [2000]</span>
<span class="ltx_bibblock">
P. Ferragina and G. Manzini, “Opportunistic data structures with applications,” in <em class="ltx_emph ltx_font_italic" id="bib.bib204.1.1">41st Annual Symposium on Foundations of Computer Science, FOCS 2000, 12-14 November 2000, Redondo Beach, California, USA</em>.   IEEE Computer Society, 2000, pp. 390–398.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023b]</span>
<span class="ltx_bibblock">
Y. Tang, R. Zhang, J. Guo, J. Chen, Z. Zhu, S. Wang, D. Yin, and X. Cheng, “Semantic-enhanced differentiable search index inspired by learning strategies,” in <em class="ltx_emph ltx_font_italic" id="bib.bib205.1.1">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2023, Long Beach, CA, USA, August 6-10, 2023</em>.   ACM, 2023, pp. 4904–4913.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jégou et al. [2011]</span>
<span class="ltx_bibblock">
H. Jégou, M. Douze, and C. Schmid, “Product quantization for nearest neighbor search,” <em class="ltx_emph ltx_font_italic" id="bib.bib206.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</em>, vol. 33, no. 1, pp. 117–128, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ge et al. [2014]</span>
<span class="ltx_bibblock">
T. Ge, K. He, Q. Ke, and J. Sun, “Optimized product quantization,” <em class="ltx_emph ltx_font_italic" id="bib.bib207.1.1">IEEE Trans. Pattern Anal. Mach. Intell.</em>, vol. 36, no. 4, pp. 744–755, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorne et al. [2018]</span>
<span class="ltx_bibblock">
J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a large-scale dataset for fact extraction and verification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib208.1.1">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</em>, 2018, pp. 809–819.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffart et al. [2011]</span>
<span class="ltx_bibblock">
J. Hoffart, M. A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater, and G. Weikum, “Robust disambiguation of named entities in text,” in <em class="ltx_emph ltx_font_italic" id="bib.bib209.1.1">EMNLP</em>, 2011, pp. 782–792.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dinan et al. [2018]</span>
<span class="ltx_bibblock">
E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston, “Wizard of wikipedia: Knowledge-powered conversational agents,” in <em class="ltx_emph ltx_font_italic" id="bib.bib210.1.1">International Conference on Learning Representations</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levy et al. [2017]</span>
<span class="ltx_bibblock">
O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation extraction via reading comprehension,” in <em class="ltx_emph ltx_font_italic" id="bib.bib211.1.1">CoNLL</em>, 2017, pp. 333–342.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. [2021]</span>
<span class="ltx_bibblock">
S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. B. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou, L. Zhou, M. Tufano, M. Gong, M. Zhou, N. Duan, N. Sundaresan, S. K. Deng, S. Fu, and S. Liu, “Codexglue: A machine learning benchmark dataset for code understanding and generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib212.1.1">Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adlakha et al. [2022]</span>
<span class="ltx_bibblock">
V. Adlakha, S. Dhuliawala, K. Suleman, H. de Vries, and S. Reddy, “Topiocqa: Open-domain conversational question answering with topic switching,” <em class="ltx_emph ltx_font_italic" id="bib.bib213.1.1">Trans. Assoc. Comput. Linguistics</em>, vol. 10, pp. 468–483, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2014]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. J. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick, “Microsoft COCO: common objects in context,” in <em class="ltx_emph ltx_font_italic" id="bib.bib214.1.1">Computer Vision - ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V</em>, ser. Lecture Notes in Computer Science, vol. 8693.   Springer, 2014, pp. 740–755.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awadalla et al. [2023]</span>
<span class="ltx_bibblock">
A. Awadalla, I. Gao, J. Gardner, J. Hessel, Y. Hanafy, W. Zhu, K. Marathe, Y. Bitton, S. Y. Gadre, S. Sagawa, J. Jitsev, S. Kornblith, P. W. Koh, G. Ilharco, M. Wortsman, and L. Schmidt, “Openflamingo: An open-source framework for training large autoregressive vision-language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib215.1.1">CoRR</em>, vol. abs/2308.01390, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023g]</span>
<span class="ltx_bibblock">
J. Li, W. Zhang, T. Wang, G. Xiong, A. Lu, and G. Medioni, “Gpt4rec: A generative framework for personalized recommendation and user interests interpretation,” <em class="ltx_emph ltx_font_italic" id="bib.bib216.1.1">arXiv preprint arXiv:2304.03879</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al. [2020]</span>
<span class="ltx_bibblock">
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws for neural language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib217.1.1">CoRR</em>, vol. abs/2001.08361, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. [2023]</span>
<span class="ltx_bibblock">
J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat <em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">et al.</em>, “Gpt-4 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib218.2.2">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. [2024]</span>
<span class="ltx_bibblock">
Y. Qin, K. Song, Y. Hu, W. Yao, S. Cho, X. Wang, X. Wu, F. Liu, P. Liu, and D. Yu, “Infobench: Evaluating instruction following ability in large language models,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jacobs et al. [1991]</span>
<span class="ltx_bibblock">
R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, “Adaptive mixtures of local experts,” <em class="ltx_emph ltx_font_italic" id="bib.bib220.1.1">Neural computation</em>, vol. 3, no. 1, pp. 79–87, 1991.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fedus et al. [2022]</span>
<span class="ltx_bibblock">
W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: scaling to trillion parameter models with simple and efficient sparsity,” <em class="ltx_emph ltx_font_italic" id="bib.bib221.1.1">J. Mach. Learn. Res.</em>, vol. 23, no. 1, jan 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lepikhin et al. [2020]</span>
<span class="ltx_bibblock">
D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer, and Z. Chen, “Gshard: Scaling giant models with conditional computation and automatic sharding,” <em class="ltx_emph ltx_font_italic" id="bib.bib222.1.1">CoRR</em>, vol. abs/2006.16668, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. [2021]</span>
<span class="ltx_bibblock">
N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu, O. Firat, B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat, K. Robinson, K. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and C. Cui, “Glam: Efficient scaling of language models with mixture-of-experts,” <em class="ltx_emph ltx_font_italic" id="bib.bib223.1.1">CoRR</em>, vol. abs/2112.06905, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. [2022]</span>
<span class="ltx_bibblock">
A. Clark, D. de Las Casas, A. Guy, A. Mensch, M. Paganini, J. Hoffmann, B. Damoc, B. A. Hechtman, T. Cai, S. Borgeaud, G. van den Driessche, E. Rutherford, T. Hennigan, M. J. Johnson, K. Millican, A. Cassirer, C. Jones, E. Buchatskaya, D. Budden, L. Sifre, S. Osindero, O. Vinyals, J. W. Rae, E. Elsen, K. Kavukcuoglu, and K. Simonyan, “Unified scaling laws for routed language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib224.1.1">CoRR</em>, vol. abs/2202.01169, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023d]</span>
<span class="ltx_bibblock">
D. Jiang, X. Ren, and B. Y. Lin, “Llm-blender: Ensembling large language models with pairwise comparison and generative fusion,” in <em class="ltx_emph ltx_font_italic" id="bib.bib225.1.1">Proceedings of the 61th Annual Meeting of the Association for Computational Linguistics (ACL 2023)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gunasekar et al. [2023]</span>
<span class="ltx_bibblock">
S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa, O. Saarikivi, A. Salim, S. Shah, H. S. Behl, X. Wang, S. Bubeck, R. Eldan, A. T. Kalai, Y. T. Lee, and Y. Li, “Textbooks are all you need,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2024a]</span>
<span class="ltx_bibblock">
C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat, P. Yu, L. Yu <em class="ltx_emph ltx_font_italic" id="bib.bib227.1.1">et al.</em>, “Lima: Less is more for alignment,” vol. 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Manber and Myers [1990]</span>
<span class="ltx_bibblock">
U. Manber and G. Myers, “Suffix arrays: a new method for on-line string searches,” in <em class="ltx_emph ltx_font_italic" id="bib.bib228.1.1">Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms</em>, ser. SODA ’90.   USA: Society for Industrial and Applied Mathematics, 1990, p. 319–327.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Broder [1997]</span>
<span class="ltx_bibblock">
A. Broder, “On the resemblance and containment of documents,” in <em class="ltx_emph ltx_font_italic" id="bib.bib229.1.1">Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171)</em>, 1997, pp. 21–29.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2022b]</span>
<span class="ltx_bibblock">
W. Sun, Z. Shi, S. Gao, P. Ren, M. de Rijke, and Z. Ren, “Contrastive learning reduces hallucination in conversations,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al. [2023]</span>
<span class="ltx_bibblock">
R. Rafailov, A. Sharma, E. Mitchell, C. D. Manning, S. Ermon, and C. Finn, “Direct preference optimization: Your language model is secretly a reward model,” in <em class="ltx_emph ltx_font_italic" id="bib.bib231.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=HPuSIXJaa9" title="">https://openreview.net/forum?id=HPuSIXJaa9</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2022a]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, and D. Zhou, “Chain of thought prompting elicits reasoning in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib232.1.1">CoRR</em>, vol. abs/2201.11903, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023f]</span>
<span class="ltx_bibblock">
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency improves chain of thought reasoning in language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2023a]</span>
<span class="ltx_bibblock">
S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan, “Tree of Thoughts: Deliberate problem solving with large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhuliawala et al. [2023]</span>
<span class="ltx_bibblock">
S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz, and J. Weston, “Chain-of-verification reduces hallucination in large language models,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2022c]</span>
<span class="ltx_bibblock">
N. Lee, W. Ping, P. Xu, M. Patwary, P. Fung, M. Shoeybi, and B. Catanzaro, “Factuality enhanced language models for open-ended text generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib236.1.1">Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2023d]</span>
<span class="ltx_bibblock">
——, “Factuality enhanced language models for open-ended text generation,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al. [2023]</span>
<span class="ltx_bibblock">
D. Wan, M. Liu, K. McKeown, M. Dreyer, and M. Bansal, “Faithfulness-aware decoding strategies for abstractive summarization,” <em class="ltx_emph ltx_font_italic" id="bib.bib238.1.1">arXiv preprint arXiv:2303.03278</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023h]</span>
<span class="ltx_bibblock">
K. Li, O. Patel, F. Viégas, H. Pfister, and M. Wattenberg, “Inference-time intervention: Eliciting truthful answers from a language model,” in <em class="ltx_emph ltx_font_italic" id="bib.bib239.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=aLLuYpn83y" title="">https://openreview.net/forum?id=aLLuYpn83y</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al. [2023b]</span>
<span class="ltx_bibblock">
W. Shi, X. Han, M. Lewis, Y. Tsvetkov, L. Zettlemoyer, and S. W. tau Yih, “Trusting your evidence: Hallucinate less with context-aware decoding,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023g]</span>
<span class="ltx_bibblock">
L. Wang, X. Zhang, H. Su, and J. Zhu, “A comprehensive survey of continual learning: Theory, method and application,” <em class="ltx_emph ltx_font_italic" id="bib.bib241.1.1">CoRR</em>, vol. abs/2302.00487, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2024b]</span>
<span class="ltx_bibblock">
T. Wu, L. Luo, Y.-F. Li, S. Pan, T.-T. Vu, and G. Haffari, “Continual learning for large language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib242.1.1">arXiv preprint arXiv:2402.01364</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023h]</span>
<span class="ltx_bibblock">
S. Wang, Y. Zhu, H. Liu, Z. Zheng, C. Chen, and J. Li, “Knowledge editing for large language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib243.1.1">CoRR</em>, vol. abs/2310.16218, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2024c]</span>
<span class="ltx_bibblock">
N. Zhang, Y. Yao, B. Tian, P. Wang, S. Deng, M. Wang, Z. Xi, S. Mao, J. Zhang, Y. Ni <em class="ltx_emph ltx_font_italic" id="bib.bib244.1.1">et al.</em>, “A comprehensive study of knowledge editing for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib244.2.2">arXiv preprint arXiv:2401.01286</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al. [2021]</span>
<span class="ltx_bibblock">
J. Jang, S. Ye, S. Yang, J. Shin, J. Han, G. Kim, S. J. Choi, and M. Seo, “Towards continual knowledge learning of language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib245.1.1">arXiv preprint arXiv:2110.03215</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cossu et al. [2022]</span>
<span class="ltx_bibblock">
A. Cossu, T. Tuytelaars, A. Carta, L. Passaro, V. Lomonaco, and D. Bacciu, “Continual pre-training mitigates forgetting in language and vision,” <em class="ltx_emph ltx_font_italic" id="bib.bib246.1.1">arXiv preprint arXiv:2205.09357</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. [2023]</span>
<span class="ltx_bibblock">
Y. Xie, K. Aggarwal, and A. Ahmad, “Efficient continual pre-training for building domain specific large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib247.1.1">arXiv preprint arXiv:2311.08545</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Razdaibiedina et al. [2023]</span>
<span class="ltx_bibblock">
A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, and A. Almahairi, “Progressive prompts: Continual learning for language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib248.1.1">arXiv preprint arXiv:2301.12314</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jang et al. [2023b]</span>
<span class="ltx_bibblock">
J. Jang, S. Kim, S. Ye, D. Kim, L. Logeswaran, M. Lee, K. Lee, and M. Seo, “Exploring the benefits of training expert language models over instruction tuning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib249.1.1">International Conference on Machine Learning</em>.   PMLR, 2023, pp. 14 702–14 729.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suhr and Artzi [2024]</span>
<span class="ltx_bibblock">
A. Suhr and Y. Artzi, “Continual learning for instruction following from realtime feedback,” <em class="ltx_emph ltx_font_italic" id="bib.bib250.1.1">Advances in Neural Information Processing Systems</em>, vol. 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023i]</span>
<span class="ltx_bibblock">
X. Wang, T. Chen, Q. Ge, H. Xia, R. Bao, R. Zheng, Q. Zhang, T. Gui, and X. Huang, “Orthogonal subspace learning for language model continual learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib251.1.1">arXiv preprint arXiv:2310.14152</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al. [2024]</span>
<span class="ltx_bibblock">
B. Peng, Z. Tian, S. Liu, M. Yang, and J. Jia, “Scalable language model with generalized continual learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib252.1.1">arXiv preprint arXiv:2404.07470</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mazzia et al. [2023]</span>
<span class="ltx_bibblock">
V. Mazzia, A. Pedrani, A. Caciolai, K. Rottmann, and D. Bernardi, “A survey on knowledge editing of neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib253.1.1">CoRR</em>, vol. abs/2310.19704, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023c]</span>
<span class="ltx_bibblock">
Z. Huang, Y. Shen, X. Zhang, J. Zhou, W. Rong, and Z. Xiong, “Transformer-patcher: One mistake worth one neuron,” in <em class="ltx_emph ltx_font_italic" id="bib.bib254.1.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>.   OpenReview.net, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartvigsen et al. [2023]</span>
<span class="ltx_bibblock">
T. Hartvigsen, S. Sankaranarayanan, H. Palangi, Y. Kim, and M. Ghassemi, “Aging with GRACE: lifelong model editing with discrete key-value adaptors,” in <em class="ltx_emph ltx_font_italic" id="bib.bib255.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2022]</span>
<span class="ltx_bibblock">
D. Dai, L. Dong, Y. Hao, Z. Sui, B. Chang, and F. Wei, “Knowledge neurons in pretrained transformers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib256.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>.   Association for Computational Linguistics, 2022, pp. 8493–8502.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. [2023]</span>
<span class="ltx_bibblock">
K. Meng, A. S. Sharma, A. J. Andonian, Y. Belinkov, and D. Bau, “Mass-editing memory in a transformer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib257.1.1">The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023</em>.   OpenReview.net, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024g]</span>
<span class="ltx_bibblock">
X. Li, S. Li, S. Song, J. Yang, J. Ma, and J. Yu, “Pmet: Precise model editing in a transformer,” in <em class="ltx_emph ltx_font_italic" id="bib.bib258.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 17, 2024, pp. 18 564–18 572.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2023b]</span>
<span class="ltx_bibblock">
J.-Y. Ma, J.-C. Gu, Z.-H. Ling, Q. Liu, and C. Liu, “Untying the reversal curse via bidirectional language model editing,” <em class="ltx_emph ltx_font_italic" id="bib.bib259.1.1">CoRR</em>, vol. abs/2310.10322, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. [2023]</span>
<span class="ltx_bibblock">
C. Tan, G. Zhang, and J. Fu, “Massive editing for large language models via meta learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib260.1.1">CoRR</em>, vol. abs/2311.04661, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Izacard and Grave [2020]</span>
<span class="ltx_bibblock">
G. Izacard and E. Grave, “Leveraging passage retrieval with generative models for open domain question answering,” <em class="ltx_emph ltx_font_italic" id="bib.bib261.1.1">arXiv preprint arXiv:2007.01282</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2024b]</span>
<span class="ltx_bibblock">
Y. Zhou, Z. Liu, J. Jin, J.-Y. Nie, and Z. Dou, “Metacognitive retrieval-augmented large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib262.1.1">CoRR</em>, vol. abs/2402.11626, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2024]</span>
<span class="ltx_bibblock">
Y. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, Q. Guo, M. Wang, and H. Wang, “Retrieval-augmented generation for large language models: A survey,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu et al. [2020]</span>
<span class="ltx_bibblock">
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang, “Retrieval augmented language model pre-training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib264.1.1">International conference on machine learning</em>.   PMLR, 2020, pp. 3929–3938.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borgeaud et al. [2022]</span>
<span class="ltx_bibblock">
S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. van den Driessche, J.-B. Lespiau, B. Damoc, A. Clark, D. de Las Casas, A. Guy, J. Menick, R. Ring, T. Hennigan, S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals, S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre, “Improving language models by retrieving from trillions of tokens,” in <em class="ltx_emph ltx_font_italic" id="bib.bib265.1.1">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, ser. Proceedings of Machine Learning Research, vol. 162.   PMLR, 2022, pp. 2206–2240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram et al. [2023]</span>
<span class="ltx_bibblock">
O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-Brown, and Y. Shoham, “In-context retrieval-augmented language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib266.1.1">arXiv preprint arXiv:2302.00083</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. [2023]</span>
<span class="ltx_bibblock">
Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever improves generalization of language models as generic plug-in,” <em class="ltx_emph ltx_font_italic" id="bib.bib267.1.1">arXiv preprint arXiv:2305.17331</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023e]</span>
<span class="ltx_bibblock">
P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y. Nie, “Retrieve anything to augment large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib268.1.1">CoRR</em>, vol. abs/2310.07554, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023d]</span>
<span class="ltx_bibblock">
N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang, “Lost in the middle: How language models use long contexts,” 2023, arXiv:2307.03172.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuconasu et al. [2024]</span>
<span class="ltx_bibblock">
F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano, Y. Maarek, N. Tonellotto, and F. Silvestri, “The power of noise: Redefining retrieval for rag systems,” 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2024]</span>
<span class="ltx_bibblock">
J. Jin, Y. Zhu, Y. Zhou, and Z. Dou, “BIDER: bridging knowledge inconsistency for efficient retrieval-augmented llms via key supporting evidence,” <em class="ltx_emph ltx_font_italic" id="bib.bib271.1.1">CoRR</em>, vol. abs/2402.12174, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023c]</span>
<span class="ltx_bibblock">
F. Xu, W. Shi, and E. Choi, “RECOMP: improving retrieval-augmented lms with compression and selective augmentation,” <em class="ltx_emph ltx_font_italic" id="bib.bib272.1.1">CoRR</em>, vol. abs/2310.04408, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023e]</span>
<span class="ltx_bibblock">
H. Jiang, Q. Wu, C.-Y. Lin, Y. Yang, and L. Qiu, “LLMLingua: Compressing prompts for accelerated inference of large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib273.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>.   Association for Computational Linguistics, Dec. 2023, pp. 13 358–13 376. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.825" title="">https://aclanthology.org/2023.emnlp-main.825</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023f]</span>
<span class="ltx_bibblock">
H. Jiang, Q. Wu, , X. Luo, D. Li, C.-Y. Lin, Y. Yang, and L. Qiu, “Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression,” <em class="ltx_emph ltx_font_italic" id="bib.bib274.1.1">ArXiv preprint</em>, vol. abs/2310.06839, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.06839" title="">https://arxiv.org/abs/2310.06839</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023b]</span>
<span class="ltx_bibblock">
H. Yang, Z. Li, Y. Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “PRCA: fitting black-box large language models for retrieval question answering via pluggable reward-driven contextual adapter,” in <em class="ltx_emph ltx_font_italic" id="bib.bib275.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 5364–5375.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lan et al. [2023]</span>
<span class="ltx_bibblock">
T. Lan, D. Cai, Y. Wang, H. Huang, and X.-L. Mao, “Copy is all you need,” in <em class="ltx_emph ltx_font_italic" id="bib.bib276.1.1">The Eleventh International Conference on Learning Representations</em>, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=CROlOA9Nd8C" title="">https://openreview.net/forum?id=CROlOA9Nd8C</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Press et al. [2023]</span>
<span class="ltx_bibblock">
O. Press, M. Zhang, S. Min, L. Schmidt, N. Smith, and M. Lewis, “Measuring and narrowing the compositionality gap in language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib277.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>.   Singapore: Association for Computational Linguistics, Dec. 2023, pp. 5687–5711. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-emnlp.378" title="">https://aclanthology.org/2023.findings-emnlp.378</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib278">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoran et al. [2023]</span>
<span class="ltx_bibblock">
O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-augmented language models robust to irrelevant context,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib279">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Komeili et al. [2022]</span>
<span class="ltx_bibblock">
M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib279.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>.   Dublin, Ireland: Association for Computational Linguistics, May 2022, pp. 8460–8478. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.579" title="">https://aclanthology.org/2022.acl-long.579</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib280">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al. [2022]</span>
<span class="ltx_bibblock">
K. Shuster, J. Xu, M. Komeili, D. Ju, E. M. Smith, S. Roller, M. Ung, M. Chen, K. Arora, J. Lane, M. Behrooz, W. Ngan, S. Poff, N. Goyal, A. Szlam, Y.-L. Boureau, M. Kambadur, and J. Weston, “Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib281">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. [2023]</span>
<span class="ltx_bibblock">
Y. Song, W. Xiong, D. Zhu, W. Wu, H. Qian, M. Song, H. Huang, C. Li, K. Wang, R. Yao, Y. Tian, and S. Li, “Restgpt: Connecting large language models with real-world restful apis,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib282">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patil et al. [2023]</span>
<span class="ltx_bibblock">
S. G. Patil, T. Zhang, X. Wang, and J. E. Gonzalez, “Gorilla: Large language model connected with massive apis,” 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib283">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al. [2023]</span>
<span class="ltx_bibblock">
S. Hao, T. Liu, Z. Wang, and Z. Hu, “Toolkengpt: Augmenting frozen language models with massive tools via tool embeddings,” in <em class="ltx_emph ltx_font_italic" id="bib.bib283.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib284">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al. [2023b]</span>
<span class="ltx_bibblock">
C. Qian, C. Han, Y. Fung, Y. Qin, Z. Liu, and H. Ji, “CREATOR: Tool creation for disentangling abstract and concrete reasoning of large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib284.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>.   Singapore: Association for Computational Linguistics, Dec. 2023, pp. 6922–6939. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-emnlp.462" title="">https://aclanthology.org/2023.findings-emnlp.462</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib285">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Surís et al. [2023]</span>
<span class="ltx_bibblock">
D. Surís, S. Menon, and C. Vondrick, “Vipergpt: Visual inference via python execution for reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib285.1.1">arXiv preprint arXiv:2303.08128</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib286">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022]</span>
<span class="ltx_bibblock">
J. Li, D. Li, C. Xiong, and S. Hoi, “Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib286.1.1">International conference on machine learning</em>.   PMLR, 2022, pp. 12 888–12 900.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib287">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023f]</span>
<span class="ltx_bibblock">
L. Zhang, A. Rao, and M. Agrawala, “Adding conditional control to text-to-image diffusion models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib287.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</em>, 2023, pp. 3836–3847.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib288">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Chang [2023]</span>
<span class="ltx_bibblock">
J. Huang and K. C.-C. Chang, “Citation: A key to building responsible and accountable large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib288.1.1">CoRR</em>, vol. abs/2307.02185, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib289">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Litschko et al. [2023]</span>
<span class="ltx_bibblock">
R. Litschko, M. Müller-Eberstein, R. van der Goot, L. Weber-Genzel, and B. Plank, “Establishing trustworthiness: Rethinking tasks and model evaluation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib289.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 193–203.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib290">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2023]</span>
<span class="ltx_bibblock">
X. Ye, R. Sun, S. Ö. Arik, and T. Pfister, “Effective large language model adaptation for improved grounding,” <em class="ltx_emph ltx_font_italic" id="bib.bib290.1.1">CoRR</em>, vol. abs/2311.09533, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib291">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besta et al. [2024a]</span>
<span class="ltx_bibblock">
M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk <em class="ltx_emph ltx_font_italic" id="bib.bib291.1.1">et al.</em>, “Graph of thoughts: Solving elaborate problems with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib291.2.2">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 16, 2024, pp. 17 682–17 690.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib292">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fang et al. [2024]</span>
<span class="ltx_bibblock">
Y. Fang, S. W. Thomas, and X. Zhu, “HGOT: hierarchical graph of thoughts for retrieval-augmented in-context learning in factuality evaluation,” <em class="ltx_emph ltx_font_italic" id="bib.bib292.1.1">CoRR</em>, vol. abs/2402.09390, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib293">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2024]</span>
<span class="ltx_bibblock">
C. Huang, Z. Wu, Y. Hu, and W. Wang, “Training language models to generate text with citations via fine-grained rewards,” <em class="ltx_emph ltx_font_italic" id="bib.bib293.1.1">CoRR</em>, vol. abs/2402.04315, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib294">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023e]</span>
<span class="ltx_bibblock">
A. Chen, P. Pasupat, S. Singh, H. Lee, and K. Guu, “PURR: efficiently editing language model hallucinations by denoising language model corruptions,” <em class="ltx_emph ltx_font_italic" id="bib.bib294.1.1">CoRR</em>, vol. abs/2305.14908, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib295">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Slobodkin et al. [2024]</span>
<span class="ltx_bibblock">
A. Slobodkin, E. Hirsch, A. Cattan, T. Schuster, and I. Dagan, “Attribute first, then generate: Locally-attributable grounded text generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib295.1.1">CoRR</em>, vol. abs/2403.17104, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib296">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2020]</span>
<span class="ltx_bibblock">
Y. Zhou, Z. Dou, and J.-R. Wen, “Encoding history with context-aware representation learning for personalized search,” in <em class="ltx_emph ltx_font_italic" id="bib.bib296.1.1">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event, China, July 25-30, 2020</em>.   ACM, 2020, pp. 1111–1120.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib297">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023j]</span>
<span class="ltx_bibblock">
S. Wang, Z. Dou, J. Yao, Y. Zhou, and J.-R. Wen, “Incorporating explicit subtopics in personalized search,” in <em class="ltx_emph ltx_font_italic" id="bib.bib297.1.1">Proceedings of the ACM Web Conference 2023, WWW 2023, Austin, TX, USA, 30 April 2023 - 4 May 2023</em>.   ACM, 2023, pp. 3364–3374.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib298">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2024c]</span>
<span class="ltx_bibblock">
Y. Zhou, Q. Zhu, J. Jin, and Z. Dou, “Cognitive personalized search integrating large language models with an efficient memory mechanism,” <em class="ltx_emph ltx_font_italic" id="bib.bib298.1.1">CoRR</em>, vol. abs/2402.10548, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib299">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2021]</span>
<span class="ltx_bibblock">
Z. Ma, Z. Dou, Y. Zhu, H. Zhong, and J.-R. Wen, “One chatbot per person: Creating personalized chatbots based on implicit user profiles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib299.1.1">SIGIR ’21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021</em>.   ACM, 2021, pp. 555–564.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib300">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023e]</span>
<span class="ltx_bibblock">
J. Liu, C. Liu, R. Lv, K. Zhou, and Y. Zhang, “Is chatgpt a good recommender? A preliminary study,” <em class="ltx_emph ltx_font_italic" id="bib.bib300.1.1">CoRR</em>, vol. abs/2304.10149, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib301">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2023]</span>
<span class="ltx_bibblock">
S. Dai, N. Shao, H. Zhao, W. Yu, Z. Si, C. Xu, Z. Sun, X. Zhang, and J. Xu, “Uncovering chatgpt’s capabilities in recommender systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib301.1.1">Proceedings of the 17th ACM Conference on Recommender Systems, RecSys 2023, Singapore, Singapore, September 18-22, 2023</em>.   ACM, 2023, pp. 1126–1132.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib302">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhiyuli et al. [2023]</span>
<span class="ltx_bibblock">
A. Zhiyuli, Y. Chen, X. Zhang, and X. Liang, “Bookgpt: A general framework for book recommendation empowered by large language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib302.1.1">CoRR</em>, vol. abs/2305.15673, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib303">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salemi et al. [2023]</span>
<span class="ltx_bibblock">
A. Salemi, S. Mysore, M. Bendersky, and H. Zamani, “Lamp: When large language models meet personalization,” <em class="ltx_emph ltx_font_italic" id="bib.bib303.1.1">CoRR</em>, vol. abs/2304.11406, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib304">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou et al. [2023]</span>
<span class="ltx_bibblock">
K. Christakopoulou, A. Lalama, C. Adams, I. Qu, Y. Amir, S. Chucri, P. Vollucci, F. Soldo, D. Bseiso, S. Scodel, L. Dixon, E. H. Chi, and M. Chen, “Large language models for user interest journeys,” <em class="ltx_emph ltx_font_italic" id="bib.bib304.1.1">CoRR</em>, vol. abs/2305.15498, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib305">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023k]</span>
<span class="ltx_bibblock">
D. Wang, K. Yang, H. Zhu, X. Yang, A. Cohen, L. Li, and Y. Tian, “Learning personalized story evaluation,” <em class="ltx_emph ltx_font_italic" id="bib.bib305.1.1">CoRR</em>, vol. abs/2310.03304, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib306">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023i]</span>
<span class="ltx_bibblock">
C. Li, M. Zhang, Q. Mei, W. Kong, and M. Bendersky, “Automatic prompt rewriting for personalized text generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib306.1.1">CoRR</em>, vol. abs/2310.00152, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib307">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2023f]</span>
<span class="ltx_bibblock">
Z. Chen, L. Zhang, F. Weng, L. Pan, and Z. Lan, “Tailored visions: Enhancing text-to-image generation with personalized prompt rewriting,” <em class="ltx_emph ltx_font_italic" id="bib.bib307.1.1">CoRR</em>, vol. abs/2310.08129, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib308">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mazaré et al. [2018]</span>
<span class="ltx_bibblock">
P.-E. Mazaré, S. Humeau, M. Raison, and A. Bordes, “Training millions of personalized dialogue agents,” in <em class="ltx_emph ltx_font_italic" id="bib.bib308.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018</em>.   Association for Computational Linguistics, 2018, pp. 2775–2779.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib309">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. [2022]</span>
<span class="ltx_bibblock">
T. Fu, X. Zhao, C. Tao, J.-R. Wen, and R. Yan, “There are a thousand hamlets in a thousand people’s eyes: Enhancing knowledge-grounded dialogue with personal memory,” in <em class="ltx_emph ltx_font_italic" id="bib.bib309.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022</em>.   Association for Computational Linguistics, 2022, pp. 3901–3913.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib310">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2023a]</span>
<span class="ltx_bibblock">
P. Cheng, J. Xie, K. Bai, Y. Dai, and N. Du, “Everyone deserves A reward: Learning customized human preferences,” <em class="ltx_emph ltx_font_italic" id="bib.bib310.1.1">CoRR</em>, vol. abs/2309.03126, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib311">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et al. [2017]</span>
<span class="ltx_bibblock">
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” <em class="ltx_emph ltx_font_italic" id="bib.bib311.1.1">CoRR</em>, vol. abs/1707.06347, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib312">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023d]</span>
<span class="ltx_bibblock">
Q. Huang, S. Fu, X. Liu, W. Wang, T. Ko, Y. Zhang, and L. H. Y. Tang, “Learning retrieval augmentation for personalized dialogue generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib312.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 2523–2540.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib313">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023j]</span>
<span class="ltx_bibblock">
C. Li, M. Zhang, Q. Mei, Y. Wang, S. A. Hombaiah, Y. Liang, and M. Bendersky, “Teach llms to personalize - an approach inspired by writing education,” <em class="ltx_emph ltx_font_italic" id="bib.bib313.1.1">CoRR</em>, vol. abs/2308.07968, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib314">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wozniak et al. [2024]</span>
<span class="ltx_bibblock">
S. Wozniak, B. Koptyra, A. Janz, P. Kazienko, and J. Kocon, “Personalized large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib314.1.1">CoRR</em>, vol. abs/2402.09269, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib315">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. [2024]</span>
<span class="ltx_bibblock">
X. Shen, R. Zhang, X. Zhao, J. Zhu, and X. Xiao, “Pmg: Personalized multimodal generation with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib315.1.1">arXiv preprint arXiv:2404.08677</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib316">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023f]</span>
<span class="ltx_bibblock">
X. Liu, D. McDuff, G. Kovacs, I. R. Galatzer-Levy, J. E. Sunshine, J. Zhan, M.-Z. Poh, S. Liao, P. D. Achille, and S. N. Patel, “Large language models are few-shot health learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib316.1.1">CoRR</em>, vol. abs/2305.15525, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib317">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023g]</span>
<span class="ltx_bibblock">
J. Zhang, K. Sun, A. Jagadeesh, M. Ghahfarokhi, D. Gupta, A. Gupta, V. Gupta, and Y. Guo, “The potential and pitfalls of using a large language model such as chatgpt or GPT-4 as a clinical assistant,” <em class="ltx_emph ltx_font_italic" id="bib.bib317.1.1">CoRR</em>, vol. abs/2307.08152, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib318">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besta et al. [2024b]</span>
<span class="ltx_bibblock">
M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk <em class="ltx_emph ltx_font_italic" id="bib.bib318.1.1">et al.</em>, “Graph of thoughts: Solving elaborate problems with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib318.2.2">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 16, 2024, pp. 17 682–17 690.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib319">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abbasian et al. [2023]</span>
<span class="ltx_bibblock">
M. Abbasian, I. Azimi, A. M. Rahmani, and R. C. Jain, “Conversational health agents: A personalized llm-powered agent framework,” <em class="ltx_emph ltx_font_italic" id="bib.bib319.1.1">CoRR</em>, vol. abs/2310.02374, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib320">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023c]</span>
<span class="ltx_bibblock">
X. Tang, A. Zou, Z. Zhang, Y. Zhao, X. Zhang, A. Cohan, and M. Gerstein, “Medagents: Large language models as collaborators for zero-shot medical reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib320.1.1">CoRR</em>, vol. abs/2311.10537, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib321">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. [2023]</span>
<span class="ltx_bibblock">
T. Lai, Y. Shi, Z. Du, J. Wu, K. Fu, Y. Dou, and Z. Wang, “Psy-llm: Scaling up global mental health psychological services with ai-based large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib321.1.1">CoRR</em>, vol. abs/2307.11991, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib322">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Porsdam Mann et al. [2023]</span>
<span class="ltx_bibblock">
S. Porsdam Mann, B. D. Earp, N. Møller, S. Vynn, and J. Savulescu, “Autogen: A personalized large language model for academic enhancement—ethics and proof of principle,” <em class="ltx_emph ltx_font_italic" id="bib.bib322.1.1">The American Journal of Bioethics</em>, vol. 23, no. 10, pp. 28–41, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib323">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui and Sachan [2023]</span>
<span class="ltx_bibblock">
P. Cui and M. Sachan, “Adaptive and personalized exercise generation for online language learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib323.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023</em>.   Association for Computational Linguistics, 2023, pp. 10 184–10 198.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib324">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majumder et al. [2019]</span>
<span class="ltx_bibblock">
B. P. Majumder, S. Li, J. Ni, and J. J. McAuley, “Generating personalized recipes from historical user preferences,” in <em class="ltx_emph ltx_font_italic" id="bib.bib324.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019</em>.   Association for Computational Linguistics, 2019, pp. 5975–5981.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib325">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022]</span>
<span class="ltx_bibblock">
K. Zhang, G. Lu, G. Zhang, Z. Lei, and L. Wu, “Personalized headline generation with enhanced user interest perception,” in <em class="ltx_emph ltx_font_italic" id="bib.bib325.1.1">International Conference on Artificial Neural Networks</em>.   Springer, 2022, pp. 797–809.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib326">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023b]</span>
<span class="ltx_bibblock">
J. Wu, R. Antonova, A. Kan, M. Lepert, A. Zeng, S. Song, J. Bohg, S. Rusinkiewicz, and T. A. Funkhouser, “Tidybot: personalized robot assistance with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib326.1.1">Auton. Robots</em>, vol. 47, no. 8, pp. 1087–1102, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib327">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu [2004]</span>
<span class="ltx_bibblock">
M. Zhu, “Recall, precision and average precision,” <em class="ltx_emph ltx_font_italic" id="bib.bib327.1.1">Department of Statistics and Actuarial Science, University of Waterloo, Waterloo</em>, vol. 2, no. 30, p. 6, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib328">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo and Barbosa [2018]</span>
<span class="ltx_bibblock">
Z. Guo and D. Barbosa, “Robust named entity disambiguation with random walks,” <em class="ltx_emph ltx_font_italic" id="bib.bib328.1.1">Semantic Web</em>, vol. 9, no. 4, pp. 459–479, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib329">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elsahar et al. [2018]</span>
<span class="ltx_bibblock">
H. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest, and E. Simperl, “T-rex: A large scale alignment of natural language with knowledge base triples,” in <em class="ltx_emph ltx_font_italic" id="bib.bib329.1.1">LREC</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib330">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2018]</span>
<span class="ltx_bibblock">
Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. Cohen, R. Salakhutdinov, and C. D. Manning, “HotpotQA: A dataset for diverse, explainable multi-hop question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib330.1.1">EMNLP</em>.   Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. 2018, pp. 2369–2380. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D18-1259" title="">https://aclanthology.org/D18-1259</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib331">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. [2019]</span>
<span class="ltx_bibblock">
A. Fan, Y. Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “ELI5: Long form question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib331.1.1">ACL</em>.   Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 3558–3567. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1346" title="">https://aclanthology.org/P19-1346</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib332">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liska et al. [2022]</span>
<span class="ltx_bibblock">
A. Liska, T. Kociský, E. Gribovskaya, T. Terzi, E. Sezener, D. Agrawal, C. de Masson d’Autume, T. Scholtes, M. Zaheer, S. Young, E. Gilsenan-McMahon, S. Austin, P. Blunsom, and A. Lazaridou, “Streamingqa: A benchmark for adaptation to new knowledge over time in question answering models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib332.1.1">International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA</em>, ser. Proceedings of Machine Learning Research, vol. 162.   PMLR, 2022, pp. 13 604–13 622.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib333">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar et al. [2018]</span>
<span class="ltx_bibblock">
P. Rajpurkar, R. Jia, and P. Liang, “Know what you don’t know: Unanswerable questions for squad,” in <em class="ltx_emph ltx_font_italic" id="bib.bib333.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers</em>.   Association for Computational Linguistics, 2018, pp. 784–789.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib334">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie [2005]</span>
<span class="ltx_bibblock">
S. Banerjee and A. Lavie, “METEOR: an automatic metric for MT evaluation with improved correlation with human judgments,” in <em class="ltx_emph ltx_font_italic" id="bib.bib334.1.1">Proceedings of the Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization@ACL 2005, Ann Arbor, Michigan, USA, June 29, 2005</em>.   Association for Computational Linguistics, 2005, pp. 65–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib335">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. [2021]</span>
<span class="ltx_bibblock">
W. Yuan, G. Neubig, and P. Liu, “Bartscore: Evaluating generated text as text generation,” vol. 34, 2021, pp. 27 263–27 277.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib336">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. [2023]</span>
<span class="ltx_bibblock">
Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang, Y. Wang, W. Ye, Y. Zhang, Y. Chang, P. S. Yu, Q. Yang, and X. Xie, “A survey on evaluation of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib336.1.1">CoRR</em>, vol. abs/2307.03109, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib337">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. [2022]</span>
<span class="ltx_bibblock">
K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. K. Tanwani, H. Cole-Lewis, S. Pfohl, P. Payne, M. Seneviratne, P. Gamble, C. Kelly, N. Schärli, A. Chowdhery, P. A. Mansfield, B. A. y Arcas, D. R. Webster, G. S. Corrado, Y. Matias, K. Chou, J. Gottweis, N. Tomasev, Y. Liu, A. Rajkomar, J. K. Barral, C. Semturs, A. Karthikesalingam, and V. Natarajan, “Large language models encode clinical knowledge,” <em class="ltx_emph ltx_font_italic" id="bib.bib337.1.1">CoRR</em>, vol. abs/2212.13138, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib338">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al. [2022]</span>
<span class="ltx_bibblock">
M. Zhong, Y. Liu, D. Yin, Y. Mao, Y. Jiao, P. Liu, C. Zhu, H. Ji, and J. Han, “Towards a unified multi-dimensional evaluator for text generation,” 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib339">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van der Lee et al. [2019]</span>
<span class="ltx_bibblock">
C. van der Lee, A. Gatt, E. van Miltenburg, S. Wubben, and E. Krahmer, “Best practices for the human evaluation of automatically generated text,” in <em class="ltx_emph ltx_font_italic" id="bib.bib339.1.1">Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, Tokyo, Japan, October 29 - November 1, 2019</em>.   Association for Computational Linguistics, 2019, pp. 355–368.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib340">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. [2023b]</span>
<span class="ltx_bibblock">
J. Ji, M. Liu, J. Dai, X. Pan, C. Zhang, C. Bian, B. Chen, R. Sun, Y. Wang, and Y. Yang, “Beavertails: Towards improved safety alignment of LLM via a human-preference dataset,” in <em class="ltx_emph ltx_font_italic" id="bib.bib340.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib341">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2023e]</span>
<span class="ltx_bibblock">
Y. Huang, Y. Bai, Z. Zhu, J. Zhang, J. Zhang, T. Su, J. Liu, C. Lv, Y. Zhang, J. Lei, Y. Fu, M. Sun, and J. He, “C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib341.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib342">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cheng et al. [2023b]</span>
<span class="ltx_bibblock">
Q. Cheng, T. Sun, W. Zhang, S. Wang, X. Liu, M. Zhang, J. He, M. Huang, Z. Yin, K. Chen, and X. Qiu, “Evaluating hallucinations in chinese large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib342.1.1">CoRR</em>, vol. abs/2310.03368, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib343">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023l]</span>
<span class="ltx_bibblock">
C. Wang, X. Liu, Y. Yue, X. Tang, T. Zhang, J. Cheng, Y. Yao, W. Gao, X. Hu, Z. Qi, Y. Wang, L. Yang, J. Wang, X. Xie, Z. Zhang, and Y. Zhang, “Survey on factuality in large language models: Knowledge, retrieval and domain-specificity,” <em class="ltx_emph ltx_font_italic" id="bib.bib343.1.1">CoRR</em>, vol. abs/2310.07521, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib344">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. [2024]</span>
<span class="ltx_bibblock">
S. Dai, C. Xu, S. Xu, L. Pang, Z. Dong, and J. Xu, “Unifying bias and unfairness in information retrieval: A survey of challenges and opportunities with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib344.1.1">arXiv preprint arXiv:2404.11457</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib345">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. [2022b]</span>
<span class="ltx_bibblock">
J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus, “Emergent abilities of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib345.1.1">Trans. Mach. Learn. Res.</em>, vol. 2022, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib346">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et al. [2023]</span>
<span class="ltx_bibblock">
R. Schaeffer, B. Miranda, and S. Koyejo, “Are emergent abilities of large language models a mirage?” in <em class="ltx_emph ltx_font_italic" id="bib.bib346.1.1">Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib347">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. [2023d]</span>
<span class="ltx_bibblock">
Y. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and F. Wei, “Retentive network: A successor to transformer for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib347.1.1">CoRR</em>, vol. abs/2307.08621, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib348">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu and Dao [2023]</span>
<span class="ltx_bibblock">
A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with selective state spaces,” <em class="ltx_emph ltx_font_italic" id="bib.bib348.1.1">CoRR</em>, vol. abs/2312.00752, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib349">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yen et al. [2024]</span>
<span class="ltx_bibblock">
H. Yen, T. Gao, and D. Chen, “Long-context language modeling with parallel context encoding,” <em class="ltx_emph ltx_font_italic" id="bib.bib349.1.1">CoRR</em>, vol. abs/2402.16617, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib350">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. [2024c]</span>
<span class="ltx_bibblock">
J. Tan, Z. Dou, Y. Zhu, P. Guo, K. Fang, and J.-R. Wen, “Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib350.1.1">CoRR</em>, vol. abs/2402.12052, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib351">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2024d]</span>
<span class="ltx_bibblock">
Z. Wang, Z. Cheng, H. Zhu, D. Fried, and G. Neubig, “What are tools anyway? A survey from the language model perspective,” <em class="ltx_emph ltx_font_italic" id="bib.bib351.1.1">CoRR</em>, vol. abs/2403.15452, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib352">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2022]</span>
<span class="ltx_bibblock">
T. Wu, M. Caccia, Z. Li, Y.-F. Li, G. Qi, and G. Haffari, “Pretrained language model in continual learning: A comparative study,” in <em class="ltx_emph ltx_font_italic" id="bib.bib352.1.1">The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.   OpenReview.net, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib353">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ovadia et al. [2023b]</span>
<span class="ltx_bibblock">
O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or retrieval? comparing knowledge injection in llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib353.1.1">CoRR</em>, vol. abs/2312.05934, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib354">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. [2023b]</span>
<span class="ltx_bibblock">
Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen, and N. Zhang, “Editing large language models: Problems, methods, and opportunities,” in <em class="ltx_emph ltx_font_italic" id="bib.bib354.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 10 222–10 240.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib355">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gallegos et al. [2023]</span>
<span class="ltx_bibblock">
I. O. Gallegos, R. A. Rossi, J. Barrow, M. M. Tanjim, S. Kim, F. Dernoncourt, T. Yu, R. Zhang, and N. K. Ahmed, “Bias and fairness in large language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib355.1.1">CoRR</em>, vol. abs/2309.00770, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib356">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dien [2023]</span>
<span class="ltx_bibblock">
J. Dien, “Generative artificial intelligence as a plagiarism problem,” p. 108621, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib357">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenthapadi et al. [2023]</span>
<span class="ltx_bibblock">
K. Kenthapadi, H. Lakkaraju, and N. Rajani, “Generative ai meets responsible ai: Practical challenges and opportunities,” in <em class="ltx_emph ltx_font_italic" id="bib.bib357.1.1">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>, 2023, pp. 5805–5806.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib358">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. [2021]</span>
<span class="ltx_bibblock">
N. Carlini, F. Tramèr, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. B. Brown, D. Song, Ú. Erlingsson, A. Oprea, and C. Raffel, “Extracting training data from large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib358.1.1">30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021</em>.   USENIX Association, 2021, pp. 2633–2650.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib359">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2022]</span>
<span class="ltx_bibblock">
J. Huang, H. Shao, and K. C.-C. Chang, “Are large pre-trained language models leaking your personal information?” in <em class="ltx_emph ltx_font_italic" id="bib.bib359.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022</em>.   Association for Computational Linguistics, 2022, pp. 2038–2047.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib360">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023g]</span>
<span class="ltx_bibblock">
N. F. Liu, T. Zhang, and P. Liang, “Evaluating verifiability in generative search engines,” in <em class="ltx_emph ltx_font_italic" id="bib.bib360.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023</em>.   Association for Computational Linguistics, 2023, pp. 7001–7025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib361">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. [2019]</span>
<span class="ltx_bibblock">
N. Carlini, C. Liu, Ú. Erlingsson, J. Kos, and D. Song, “The secret sharer: Evaluating and testing unintended memorization in neural networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib361.1.1">28th USENIX Security Symposium, USENIX Security 2019, Santa Clara, CA, USA, August 14-16, 2019</em>.   USENIX Association, 2019, pp. 267–284.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib362">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023c]</span>
<span class="ltx_bibblock">
X. Yang, L. Pan, X. Zhao, H. Chen, L. R. Petzold, W. Y. Wang, and W. Cheng, “A survey on detection of llms-generated content,” <em class="ltx_emph ltx_font_italic" id="bib.bib362.1.1">CoRR</em>, vol. abs/2310.15654, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib363">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2023m]</span>
<span class="ltx_bibblock">
L. Wang, N. Yang, X. Huang, L. Yang, R. Majumder, and F. Wei, “Large search model: Redefining search stack in the era of llms,” <em class="ltx_emph ltx_font_italic" id="bib.bib363.1.1">SIGIR Forum</em>, vol. 57, no. 2, pp. 23:1–23:16, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu May 16 03:31:32 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
