<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1809.08697] Textually Enriched Neural Module Networks for Visual Question Answering</title><meta property="og:description" content="Problems at the intersection of language and vision, like visual question answering, have recently been gaining a lot of attention in the field of multi-modal machine learning as computer vision research moves beyond tâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Textually Enriched Neural Module Networks for Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Textually Enriched Neural Module Networks for Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1809.08697">

<!--Generated on Sun Mar 17 01:12:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Textually Enriched Neural Module Networks for Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Khyathi Chandu, Mary Arpita Pyreddy, Matthieu Felix, Narendra Nath Joshi
<br class="ltx_break">
<br class="ltx_break">Language Technologies Institute, School of Computer Science
<br class="ltx_break">Carnegie Mellon University, Pittsburgh, PA
<br class="ltx_break">
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">{kchandu, mpyreddy, matthief, nnj}@andrew.cmu.edu</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">Problems at the intersection of language and vision, like visual question answering, have recently been gaining a lot of attention in the field of multi-modal machine learning as computer vision research moves beyond traditional recognition tasks. There has been recent success in visual question answering using deep neural network models which use the linguistic structure of the questions to dynamically instantiate network layouts. In the process of converting the question to a network layout, the question is simplified, which results in loss of information in the model. In this paper, we enrich the image information with textual data using image captions and external knowledge bases to generate more coherent answers. We achieve 57.1% overall accuracy on the test-dev open-ended questions from the visual question answering (VQA 1.0) real image dataset.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Visual question answering (VQA), introduced by <cite class="ltx_cite ltx_citemacro_citet">Antol etÂ al., (<a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite>, has generated a lot of interest in recent computer vision research. Given an image and a question in natural language about the image, the task of VQA is to provide an accurate natural language answer (for instance, a simple question could be â€œHow many people are in the picture?â€). Achieving consistently good performance in VQA could have a tremendous impact on both the research community and society in general: it can be used, for instance, for the assistance of the visually impaired for daily navigation, as well as lead us to deeper understanding and better representations for other computer vision tasks like image search. We believe it can have a high impact on multi-modal conversational agent research as well.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib2" title="" class="ltx_ref">Andreas etÂ al., 2016b </a></cite> and <cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib1" title="" class="ltx_ref">Andreas etÂ al., 2016a </a></cite> propose neural module networks (NMN) for VQA where a network layout is generated by putting together neural modules based on a natural language dependency parse (that is, a tree structure representing relationship between the words) of the question. One limitation in this model is that some of the information present in the question is destroyed when the question is converted to a network layout: NMNs obtain better results with short parse trees, which represent only the main elements of the original question.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this paper, we propose two approaches to textually enrich NMNs for VQA by leveraging the image captions. The first approach is to incorporate image caption (see <cite class="ltx_cite ltx_citemacro_citet">Karpathy and Fei-Fei, (<a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite>, for instance) information into the model, making the model resilient to information deletion stemming from incorrect or simplified parses. The second approach is a modification of the first approach where we attend to the caption to pick only the useful parts instead of using the caption as a whole. This is to ensure that irrelevant captions do not introduce noise into the system. We also propose a third approach where we leverage information from external knowledge sources to provide better answers to questions that might benefit from additional knowledge. Additionally, we implement the Measure module as proposed by <cite class="ltx_cite ltx_citemacro_citet"><a href="#bib.bib2" title="" class="ltx_ref">Andreas etÂ al., 2016b </a></cite>, predominantly for answering yes/no questions.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p id="S1.p4.1" class="ltx_p">We first introduce the related work in the next section, propose our approaches and describe our experimental setup, and report our results and discuss our analysis. Finally, we conclude by discussing some possible future directions.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual Question Answering</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.1" class="ltx_p">We review here those that we have deemed most representative of the current approaches in VQA.</p>
</div>
<section id="S2.SS1.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Late Fusion</h5>

<div id="S2.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px1.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Antol etÂ al., (<a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite>, <cite class="ltx_cite ltx_citemacro_citet">Malinowski etÂ al., (<a href="#bib.bib19" title="" class="ltx_ref">2015</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citet">Gao etÂ al., (<a href="#bib.bib8" title="" class="ltx_ref">2015</a>)</cite> use a model where a long short-term memory network (LSTM, <cite class="ltx_cite ltx_citemacro_citet">Hochreiter and Schmidhuber, (<a href="#bib.bib10" title="" class="ltx_ref">1997</a>)</cite>) and a convolutional neural network (CNN, <cite class="ltx_cite ltx_citemacro_citet">LeCun etÂ al., (<a href="#bib.bib15" title="" class="ltx_ref">1989</a>)</cite>), both pre-trained, run independently on the questions and the images, respectively. The image embedding is transformed to a smaller one (for instance, 1024 dimensions) by a fully-connected layer with <math id="S2.SS1.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="\tanh" display="inline"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">tanh</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.1.m1.1b"><tanh id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1"></tanh></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.1c">\tanh</annotation></semantics></math> nonlinearity. The output of both networks are fused via element-wise multiplication, and a final fully connected softmax layer is added. <cite class="ltx_cite ltx_citemacro_citet">Ren etÂ al., (<a href="#bib.bib22" title="" class="ltx_ref">2015</a>)</cite> perform asymmetric fusion by feeding the output of the CNN into the first LSTM, as though it were the first word of the sentence. This performs somewhat better than naive late fusion.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Attention Models</h5>

<div id="S2.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px2.p1.1" class="ltx_p">This family of models assign weights to different parts of the image, in order to filter out irrelevant information that could generate noise in the answer. The model developed by <cite class="ltx_cite ltx_citemacro_citet">Shih etÂ al., (<a href="#bib.bib24" title="" class="ltx_ref">2016</a>)</cite> does this by computing the dot-products of text features extracted from the question and region-by-region features extracted by a CNN. The model then weighs information from each region by the corresponding dot-product value to produce a final answer (information for each region is generated much like in the late-fusion approach).</p>
</div>
<div id="S2.SS1.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Lu etÂ al., (<a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Nam etÂ al., (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> argue that it is equally important to model â€œwhich words to listen toâ€ (question attention). They present a co-attention model for VQA that jointly performs image and question attention. In addition, their model attends to the question (and consequently the image via the co-attention mechanism) in a hierarchical fashion via a 1-dimensional CNN. <cite class="ltx_cite ltx_citemacro_citet">Nam etÂ al., (<a href="#bib.bib20" title="" class="ltx_ref">2016</a>)</cite> show that Dual Attention Networks (DANs) jointly leverage visual and textual attention mechanisms to capture fine-grained interplay between vision and language, and that DANs attend to specific regions in images and words in text through multiple steps and gather essential information from both modalities.</p>
</div>
<div id="S2.SS1.SSS0.Px2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px2.p3.2" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Fukui etÂ al., (<a href="#bib.bib7" title="" class="ltx_ref">2016</a>)</cite> perform fusion between visual and text modalities by computing the tensor product of the two feature vectors. Since this operation would create a very large <math id="S2.SS1.SSS0.Px2.p3.1.m1.1" class="ltx_Math" alttext="n^{2}" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.1.m1.1a"><msup id="S2.SS1.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.2" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.2.cmml">n</mi><mn id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.3" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.1.m1.1b"><apply id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.2">ğ‘›</ci><cn type="integer" id="S2.SS1.SSS0.Px2.p3.1.m1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p3.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p3.1.m1.1c">n^{2}</annotation></semantics></math>-sized vector for <math id="S2.SS1.SSS0.Px2.p3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.SSS0.Px2.p3.2.m2.1a"><mi id="S2.SS1.SSS0.Px2.p3.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p3.2.m2.1b"><ci id="S2.SS1.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p3.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p3.2.m2.1c">n</annotation></semantics></math>-sized input vectors, several tricks are performed to compute a smaller approximation of this product, as suggested by <cite class="ltx_cite ltx_citemacro_citet">Gao etÂ al., (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>. The general architecture of the model is then set up as in most attention models, with this compact bilinear pooling used to generate both the attention maps and the final features.</p>
</div>
</section>
<section id="S2.SS1.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Addition of a Knowledge Base</h5>

<div id="S2.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS0.Px3.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Wu etÂ al., (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> have proposed using an external knowledge base in a VQA system. Their approach is to obtain attributes from an image using an image labeling model (<cite class="ltx_cite ltx_citemacro_cite">Ã…strÃ¶m etÂ al., (<a href="#bib.bib4" title="" class="ltx_ref">2016</a>)</cite>), query an external knowledge base, and use that information, along with image features generated from a CNN, to seed the initial hidden state of an LSTM. The question is then fed to the LSTM and an answer is generated.</p>
</div>
</section>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Image Captioning</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p id="S2.SS2.p1.1" class="ltx_p">Image captioning is closely related to VQA: both tasks try to generate textual data from visual inputs, and captions have been used to provide supplemental information in VQA models (as in <cite class="ltx_cite ltx_citemacro_cite">Wu etÂ al., (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite>). In particular, <cite class="ltx_cite ltx_citemacro_citet">Karpathy and Fei-Fei, (<a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite> have proposed a widely-used captioning model that is composed of two main parts. In this model, a CNN is used to obtain image region embeddings, an RNN to obtain a caption representation, and an alignment is performed between them using a Markov random field. These aligned pairs are then fed to the second model, which uses an RNN to generate a caption for each region of the image, using features computed from that region with a CNN as the initial hidden state of the RNN. <cite class="ltx_cite ltx_citemacro_citet">Xu etÂ al., (<a href="#bib.bib27" title="" class="ltx_ref">2015</a>)</cite> propose another model where image features are generated with a CNN, and passed to an LSTM with attention to generate the caption.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Counting with Neural Networks</h3>

<div id="S2.SS3.p1" class="ltx_para ltx_noindent">
<p id="S2.SS3.p1.1" class="ltx_p">VQA models generally perform significantly worse on counting questions than on other types of questions (this is the case in all reviewed prior work: <cite class="ltx_cite ltx_citemacro_cite">Antol etÂ al., (<a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Shih etÂ al., (<a href="#bib.bib24" title="" class="ltx_ref">2016</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite">Gao etÂ al., (<a href="#bib.bib9" title="" class="ltx_ref">2016</a>)</cite>, <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">Andreas etÂ al., 2016b </a></cite> for instance). Yet approaches to counting with neural networks have been proposed. <cite class="ltx_cite ltx_citemacro_citet">SeguÃ­ etÂ al., (<a href="#bib.bib23" title="" class="ltx_ref">2015</a>)</cite> use a straightforward model to count even digits or pedestrians in images. Their system uses two or more convolutional layers, followed by a one or several fully-connected layers.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Approach</h2>

<div id="S3.p1" class="ltx_para ltx_noindent">
<p id="S3.p1.1" class="ltx_p">In this work, we extend the baseline of Neural Module Networks (NMN) proposed by <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">Andreas etÂ al., 2016b </a></cite>.
This model is trained with triples of (question, image, answer) from which it dynamically learns to assemble a neural network in which individual modules perform specific tasks. The modules to use are selected based on the dependency parse tree<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This work uses Stanford dependency parser <cite class="ltx_cite ltx_citemacro_citep">(Chen and Manning,, <a href="#bib.bib6" title="" class="ltx_ref">2014</a>)</cite> for this purpose.</span></span></span> of the question, and the type of the question word (i.e. the first or first few words in the question). Essentially, this combines the good performance of neural networks in image recognition and captioning with the power of classical NLP methods, by assembling the network using linguistic information.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p">Specifically, a question <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{Q}" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\mathbf{Q}</annotation></semantics></math> is mapped to a logical representation of meaning, from which all the nouns, verbs and prepositional phrases that are directly related to the <span id="S3.p2.1.1" class="ltx_text ltx_font_italic">â€˜whâ€™</span> word (â€œwhatâ€, â€œwhereâ€, â€œhowâ€, â€œhow manyâ€, etc.) are collected. The common nouns and verbs are mapped to a <span id="S3.p2.1.2" class="ltx_text ltx_font_italic">find</span> module. The modules can then be combined using <span id="S3.p2.1.3" class="ltx_text ltx_font_italic">and</span> modules, and a <span id="S3.p2.1.4" class="ltx_text ltx_font_italic">measure</span> or a <span id="S3.p2.1.5" class="ltx_text ltx_font_italic">describe</span> module is inserted at the top. Table <a href="#S3.T1" title="Table 1 â€£ 3 Proposed Approach â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> lists the roles and implementations of these modules.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<p id="S3.T1.1" class="ltx_p"><span id="S3.T1.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S3.T1.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:235.7pt;height:5437.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.T1.1.1.1.1" class="ltx_p"><span id="S3.T1.1.1.1.1.1" class="ltx_text">
<span id="S3.T1.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S3.T1.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Module</span>
<span id="S3.T1.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:227.6pt;">
<span id="S3.T1.1.1.1.1.1.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1.1.1.2.1.1" class="ltx_p">Description</span>
</span></span>
<span id="S3.T1.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Input</span>
<span id="S3.T1.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Output</span></span>
</span>
<span class="ltx_tbody">
<span id="S3.T1.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Find</span>
<span id="S3.T1.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:227.6pt;">
<span id="S3.T1.1.1.1.1.1.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1.2.1.2.1.1" class="ltx_p">Convolves every position in the input image with a weight vector to produce an attention map.</span>
</span></span>
<span id="S3.T1.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">Image</span>
<span id="S3.T1.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_left ltx_border_t">Attention</span></span>
<span id="S3.T1.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_left">And</span>
<span id="S3.T1.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:227.6pt;">
<span id="S3.T1.1.1.1.1.1.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1.3.2.2.1.1" class="ltx_p">Merges two attention maps into a single attention map using element wise product.</span>
</span></span>
<span id="S3.T1.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_left">Attention x Attention</span>
<span id="S3.T1.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_left">Attention</span></span>
<span id="S3.T1.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left">Describe</span>
<span id="S3.T1.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_justify ltx_align_middle" style="width:227.6pt;">
<span id="S3.T1.1.1.1.1.1.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1.4.3.2.1.1" class="ltx_p">Computes an average over image features weighted by the attention, and then passes through a single fully-connected layer.</span>
</span></span>
<span id="S3.T1.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_left">Image x Attention</span>
<span id="S3.T1.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_left">Label</span></span>
<span id="S3.T1.1.1.1.1.1.1.5.4" class="ltx_tr">
<span id="S3.T1.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_border_b">Measure</span>
<span id="S3.T1.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b" style="width:227.6pt;">
<span id="S3.T1.1.1.1.1.1.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1.1.5.4.2.1.1" class="ltx_p">Passes the attention through a fully connected layer, ReLU nonlinearity, fully connected layer and a softmax to generate a distribution over labels.</span>
</span></span>
<span id="S3.T1.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_b">Attention</span>
<span id="S3.T1.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_left ltx_border_b">Label</span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Different NMN modules</figcaption>
</figure>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p">In any case, answer prediction is formulated as a classification problem where we are selecting the answer from 2000 most common answers that were encountered in the training dataset. This approach (with varying sizes for the number of answers considered) seems very common in prior work, and works well because the majority of answers are short and occur several times in the dataset.</p>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p">The main contribution of our work is the enrichment of the features through text. We incorporate this information in three ways, two of which primarily depend on the information from captions and the third is based on the information that can be incorporated from external knowledge bases.
The caption information that we incorporate is obtained from a pre-trained image captioning model from <cite class="ltx_cite ltx_citemacro_citet">Karpathy and Fei-Fei, (<a href="#bib.bib12" title="" class="ltx_ref">2015</a>)</cite>, which is described in the prior art section and is trained on the MSCOCO <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al.,, <a href="#bib.bib16" title="" class="ltx_ref">2014</a>)</cite> dataset.</p>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.3" class="ltx_p">We now describe the three proposed approaches in detail. In the following, let <math id="S3.p5.1.m1.1" class="ltx_Math" alttext="\mathbf{q_{i}}" display="inline"><semantics id="S3.p5.1.m1.1a"><msub id="S3.p5.1.m1.1.1" xref="S3.p5.1.m1.1.1.cmml"><mi id="S3.p5.1.m1.1.1.2" xref="S3.p5.1.m1.1.1.2.cmml">ğª</mi><mi id="S3.p5.1.m1.1.1.3" xref="S3.p5.1.m1.1.1.3.cmml">ğ¢</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.1.m1.1b"><apply id="S3.p5.1.m1.1.1.cmml" xref="S3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p5.1.m1.1.1.1.cmml" xref="S3.p5.1.m1.1.1">subscript</csymbol><ci id="S3.p5.1.m1.1.1.2.cmml" xref="S3.p5.1.m1.1.1.2">ğª</ci><ci id="S3.p5.1.m1.1.1.3.cmml" xref="S3.p5.1.m1.1.1.3">ğ¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.1.m1.1c">\mathbf{q_{i}}</annotation></semantics></math> represent the word embedding for word <math id="S3.p5.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p5.2.m2.1a"><mi id="S3.p5.2.m2.1.1" xref="S3.p5.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p5.2.m2.1b"><ci id="S3.p5.2.m2.1.1.cmml" xref="S3.p5.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.2.m2.1c">i</annotation></semantics></math> and <math id="S3.p5.3.m3.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.p5.3.m3.1a"><mi id="S3.p5.3.m3.1.1" xref="S3.p5.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.p5.3.m3.1b"><ci id="S3.p5.3.m3.1.1.cmml" xref="S3.p5.3.m3.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.3.m3.1c">T</annotation></semantics></math> be the maximum number of words in the question in that batch. Hence the entire question is represented as</p>
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.4" class="ltx_Math" alttext="\displaystyle\mathbf{Q}=(\mathbf{q}_{1},\mathbf{q}_{2},\dots,\mathbf{q}_{T})" display="inline"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml"><mi id="S3.Ex1.m1.4.4.5" xref="S3.Ex1.m1.4.4.5.cmml">ğ</mi><mo id="S3.Ex1.m1.4.4.4" xref="S3.Ex1.m1.4.4.4.cmml">=</mo><mrow id="S3.Ex1.m1.4.4.3.3" xref="S3.Ex1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.3.3.4" xref="S3.Ex1.m1.4.4.3.4.cmml">(</mo><msub id="S3.Ex1.m1.2.2.1.1.1" xref="S3.Ex1.m1.2.2.1.1.1.cmml"><mi id="S3.Ex1.m1.2.2.1.1.1.2" xref="S3.Ex1.m1.2.2.1.1.1.2.cmml">ğª</mi><mn id="S3.Ex1.m1.2.2.1.1.1.3" xref="S3.Ex1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.Ex1.m1.4.4.3.3.5" xref="S3.Ex1.m1.4.4.3.4.cmml">,</mo><msub id="S3.Ex1.m1.3.3.2.2.2" xref="S3.Ex1.m1.3.3.2.2.2.cmml"><mi id="S3.Ex1.m1.3.3.2.2.2.2" xref="S3.Ex1.m1.3.3.2.2.2.2.cmml">ğª</mi><mn id="S3.Ex1.m1.3.3.2.2.2.3" xref="S3.Ex1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.Ex1.m1.4.4.3.3.6" xref="S3.Ex1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">â€¦</mi><mo id="S3.Ex1.m1.4.4.3.3.7" xref="S3.Ex1.m1.4.4.3.4.cmml">,</mo><msub id="S3.Ex1.m1.4.4.3.3.3" xref="S3.Ex1.m1.4.4.3.3.3.cmml"><mi id="S3.Ex1.m1.4.4.3.3.3.2" xref="S3.Ex1.m1.4.4.3.3.3.2.cmml">ğª</mi><mi id="S3.Ex1.m1.4.4.3.3.3.3" xref="S3.Ex1.m1.4.4.3.3.3.3.cmml">T</mi></msub><mo stretchy="false" id="S3.Ex1.m1.4.4.3.3.8" xref="S3.Ex1.m1.4.4.3.4.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4"><eq id="S3.Ex1.m1.4.4.4.cmml" xref="S3.Ex1.m1.4.4.4"></eq><ci id="S3.Ex1.m1.4.4.5.cmml" xref="S3.Ex1.m1.4.4.5">ğ</ci><vector id="S3.Ex1.m1.4.4.3.4.cmml" xref="S3.Ex1.m1.4.4.3.3"><apply id="S3.Ex1.m1.2.2.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.2.2.1.1.1.1.cmml" xref="S3.Ex1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.2.2.1.1.1.2">ğª</ci><cn type="integer" id="S3.Ex1.m1.2.2.1.1.1.3.cmml" xref="S3.Ex1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.Ex1.m1.3.3.2.2.2.cmml" xref="S3.Ex1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.2.2.2.1.cmml" xref="S3.Ex1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.Ex1.m1.3.3.2.2.2.2.cmml" xref="S3.Ex1.m1.3.3.2.2.2.2">ğª</ci><cn type="integer" id="S3.Ex1.m1.3.3.2.2.2.3.cmml" xref="S3.Ex1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">â€¦</ci><apply id="S3.Ex1.m1.4.4.3.3.3.cmml" xref="S3.Ex1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.3.3.3.1.cmml" xref="S3.Ex1.m1.4.4.3.3.3">subscript</csymbol><ci id="S3.Ex1.m1.4.4.3.3.3.2.cmml" xref="S3.Ex1.m1.4.4.3.3.3.2">ğª</ci><ci id="S3.Ex1.m1.4.4.3.3.3.3.cmml" xref="S3.Ex1.m1.4.4.3.3.3.3">ğ‘‡</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">\displaystyle\mathbf{Q}=(\mathbf{q}_{1},\mathbf{q}_{2},\dots,\mathbf{q}_{T})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.p5.6" class="ltx_p">In a similar way, let the maximum number of words in a caption be <math id="S3.p5.4.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S3.p5.4.m1.1a"><mi id="S3.p5.4.m1.1.1" xref="S3.p5.4.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.p5.4.m1.1b"><ci id="S3.p5.4.m1.1.1.cmml" xref="S3.p5.4.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.4.m1.1c">N</annotation></semantics></math> and let <math id="S3.p5.5.m2.1" class="ltx_Math" alttext="C_{i}" display="inline"><semantics id="S3.p5.5.m2.1a"><msub id="S3.p5.5.m2.1.1" xref="S3.p5.5.m2.1.1.cmml"><mi id="S3.p5.5.m2.1.1.2" xref="S3.p5.5.m2.1.1.2.cmml">C</mi><mi id="S3.p5.5.m2.1.1.3" xref="S3.p5.5.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p5.5.m2.1b"><apply id="S3.p5.5.m2.1.1.cmml" xref="S3.p5.5.m2.1.1"><csymbol cd="ambiguous" id="S3.p5.5.m2.1.1.1.cmml" xref="S3.p5.5.m2.1.1">subscript</csymbol><ci id="S3.p5.5.m2.1.1.2.cmml" xref="S3.p5.5.m2.1.1.2">ğ¶</ci><ci id="S3.p5.5.m2.1.1.3.cmml" xref="S3.p5.5.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.5.m2.1c">C_{i}</annotation></semantics></math> represent the word embedding of word <math id="S3.p5.6.m3.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p5.6.m3.1a"><mi id="S3.p5.6.m3.1.1" xref="S3.p5.6.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p5.6.m3.1b"><ci id="S3.p5.6.m3.1.1.cmml" xref="S3.p5.6.m3.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p5.6.m3.1c">i</annotation></semantics></math> in the caption. Hence the caption is represented as</p>
<table id="S6.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex2.m1.2" class="ltx_Math" alttext="\displaystyle\mathbf{C}=(\mathbf{c}_{1},\mathbf{c}_{2},\dots,\mathbf{c}_{N})." display="inline"><semantics id="S3.Ex2.m1.2a"><mrow id="S3.Ex2.m1.2.2.1" xref="S3.Ex2.m1.2.2.1.1.cmml"><mrow id="S3.Ex2.m1.2.2.1.1" xref="S3.Ex2.m1.2.2.1.1.cmml"><mi id="S3.Ex2.m1.2.2.1.1.5" xref="S3.Ex2.m1.2.2.1.1.5.cmml">ğ‚</mi><mo id="S3.Ex2.m1.2.2.1.1.4" xref="S3.Ex2.m1.2.2.1.1.4.cmml">=</mo><mrow id="S3.Ex2.m1.2.2.1.1.3.3" xref="S3.Ex2.m1.2.2.1.1.3.4.cmml"><mo stretchy="false" id="S3.Ex2.m1.2.2.1.1.3.3.4" xref="S3.Ex2.m1.2.2.1.1.3.4.cmml">(</mo><msub id="S3.Ex2.m1.2.2.1.1.1.1.1" xref="S3.Ex2.m1.2.2.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.2.2.1.1.1.1.1.2" xref="S3.Ex2.m1.2.2.1.1.1.1.1.2.cmml">ğœ</mi><mn id="S3.Ex2.m1.2.2.1.1.1.1.1.3" xref="S3.Ex2.m1.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.Ex2.m1.2.2.1.1.3.3.5" xref="S3.Ex2.m1.2.2.1.1.3.4.cmml">,</mo><msub id="S3.Ex2.m1.2.2.1.1.2.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.2.cmml"><mi id="S3.Ex2.m1.2.2.1.1.2.2.2.2" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2.cmml">ğœ</mi><mn id="S3.Ex2.m1.2.2.1.1.2.2.2.3" xref="S3.Ex2.m1.2.2.1.1.2.2.2.3.cmml">2</mn></msub><mo id="S3.Ex2.m1.2.2.1.1.3.3.6" xref="S3.Ex2.m1.2.2.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.Ex2.m1.1.1" xref="S3.Ex2.m1.1.1.cmml">â€¦</mi><mo id="S3.Ex2.m1.2.2.1.1.3.3.7" xref="S3.Ex2.m1.2.2.1.1.3.4.cmml">,</mo><msub id="S3.Ex2.m1.2.2.1.1.3.3.3" xref="S3.Ex2.m1.2.2.1.1.3.3.3.cmml"><mi id="S3.Ex2.m1.2.2.1.1.3.3.3.2" xref="S3.Ex2.m1.2.2.1.1.3.3.3.2.cmml">ğœ</mi><mi id="S3.Ex2.m1.2.2.1.1.3.3.3.3" xref="S3.Ex2.m1.2.2.1.1.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S3.Ex2.m1.2.2.1.1.3.3.8" xref="S3.Ex2.m1.2.2.1.1.3.4.cmml">)</mo></mrow></mrow><mo lspace="0em" id="S3.Ex2.m1.2.2.1.2" xref="S3.Ex2.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.2b"><apply id="S3.Ex2.m1.2.2.1.1.cmml" xref="S3.Ex2.m1.2.2.1"><eq id="S3.Ex2.m1.2.2.1.1.4.cmml" xref="S3.Ex2.m1.2.2.1.1.4"></eq><ci id="S3.Ex2.m1.2.2.1.1.5.cmml" xref="S3.Ex2.m1.2.2.1.1.5">ğ‚</ci><vector id="S3.Ex2.m1.2.2.1.1.3.4.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3"><apply id="S3.Ex2.m1.2.2.1.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.2">ğœ</ci><cn type="integer" id="S3.Ex2.m1.2.2.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.2.2.1.1.1.1.1.3">1</cn></apply><apply id="S3.Ex2.m1.2.2.1.1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.2.2.2.1.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.2">ğœ</ci><cn type="integer" id="S3.Ex2.m1.2.2.1.1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.1.1.2.2.2.3">2</cn></apply><ci id="S3.Ex2.m1.1.1.cmml" xref="S3.Ex2.m1.1.1">â€¦</ci><apply id="S3.Ex2.m1.2.2.1.1.3.3.3.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.2.2.1.1.3.3.3.1.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex2.m1.2.2.1.1.3.3.3.2.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.3.2">ğœ</ci><ci id="S3.Ex2.m1.2.2.1.1.3.3.3.3.cmml" xref="S3.Ex2.m1.2.2.1.1.3.3.3.3">ğ‘</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.2c">\displaystyle\mathbf{C}=(\mathbf{c}_{1},\mathbf{c}_{2},\dots,\mathbf{c}_{N}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Caption Information</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In this subsection, we describe the methodology of incorporating the entire caption information to assist the prediction of NMNs. Figure <a href="#S3.F1.sf1" title="In Figure 1 â€£ 3.1 Caption Information â€£ 3 Proposed Approach â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(a)</span></a> represents the architecture of this approach.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.08697/assets/x1.png" id="S3.F1.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(a) </span>Textual enrichment of NMN through caption information</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/1809.08697/assets/x2.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">(b) </span>Textual enrichment of NMN through attention on captions</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Schematic representation of our two captioning-based approaches</figcaption>
</figure>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.9" class="ltx_p"><math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{Q}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mi id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">ğ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">ğ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">\mathbf{Q}</annotation></semantics></math> is processed through a single layer LSTM and a fully connected layer from which a question context vector <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{m_{Q}}" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><msub id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">ğ¦</mi><mi id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">ğ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">ğ¦</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">ğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathbf{m_{Q}}</annotation></semantics></math> is obtained. The same procedure is applied to <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{C}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">ğ‚</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">ğ‚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathbf{C}</annotation></semantics></math> using another LSTM and a fully connected layer to obtain the caption context vector <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{m_{C}}" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><msub id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">ğ¦</mi><mi id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">ğ‚</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">ğ¦</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">ğ‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">\mathbf{m_{C}}</annotation></semantics></math>. The image features along with the dependency parse are provided as input to the NMN. Let the output vector of NMN be represented as <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{Pred_{NMN}}" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><msub id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">ğğ«ğğ</mi><mi id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">ğğŒğ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">ğğ«ğğ</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">ğğŒğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\mathbf{Pred_{NMN}}</annotation></semantics></math>. An elementwise addition is performed on <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{Pred_{NMN}}" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><msub id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">ğğ«ğğ</mi><mi id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">ğğŒğ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">ğğ«ğğ</ci><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">ğğŒğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">\mathbf{Pred_{NMN}}</annotation></semantics></math>, the question context vector, and the caption context vector, followed by a rectified linear unit (ReLU) nonlinearity and finally another fully connected layer (with <math id="S3.SS1.p2.7.m7.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S3.SS1.p2.7.m7.1a"><mi id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">ğ–</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">ğ–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">\mathbf{W}</annotation></semantics></math> as the weight matrix). To obtain the final answer distribution, we compute a <math id="S3.SS1.p2.8.m8.1" class="ltx_Math" alttext="\mathit{softmax}" display="inline"><semantics id="S3.SS1.p2.8.m8.1a"><mi id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">\mathit{softmax}</annotation></semantics></math> over this vector call the output <math id="S3.SS1.p2.9.m9.1" class="ltx_Math" alttext="\mathbf{Pred_{V}}" display="inline"><semantics id="S3.SS1.p2.9.m9.1a"><msub id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml"><mi id="S3.SS1.p2.9.m9.1.1.2" xref="S3.SS1.p2.9.m9.1.1.2.cmml">ğğ«ğğ</mi><mi id="S3.SS1.p2.9.m9.1.1.3" xref="S3.SS1.p2.9.m9.1.1.3.cmml">ğ•</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.1b"><apply id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.1.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">subscript</csymbol><ci id="S3.SS1.p2.9.m9.1.1.2.cmml" xref="S3.SS1.p2.9.m9.1.1.2">ğğ«ğğ</ci><ci id="S3.SS1.p2.9.m9.1.1.3.cmml" xref="S3.SS1.p2.9.m9.1.1.3">ğ•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.1c">\mathbf{Pred_{V}}</annotation></semantics></math>. The final answer is the word corresponding to the maximum value in this vector.
These steps are mathematically represented as the following equations:</p>
<table id="S6.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex3.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{Pred_{V}}" display="inline"><semantics id="S3.Ex3.m1.1a"><msub id="S3.Ex3.m1.1.1" xref="S3.Ex3.m1.1.1.cmml"><mi id="S3.Ex3.m1.1.1.2" xref="S3.Ex3.m1.1.1.2.cmml">ğğ«ğğ</mi><mi id="S3.Ex3.m1.1.1.3" xref="S3.Ex3.m1.1.1.3.cmml">ğ•</mi></msub><annotation-xml encoding="MathML-Content" id="S3.Ex3.m1.1b"><apply id="S3.Ex3.m1.1.1.cmml" xref="S3.Ex3.m1.1.1"><csymbol cd="ambiguous" id="S3.Ex3.m1.1.1.1.cmml" xref="S3.Ex3.m1.1.1">subscript</csymbol><ci id="S3.Ex3.m1.1.1.2.cmml" xref="S3.Ex3.m1.1.1.2">ğğ«ğğ</ci><ci id="S3.Ex3.m1.1.1.3.cmml" xref="S3.Ex3.m1.1.1.3">ğ•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m1.1c">\displaystyle\mathbf{Pred_{V}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex3.m2.3" class="ltx_Math" alttext="\displaystyle=\mathit{softmax}(\mathbf{W}*\max(0,(\mathbf{Pred_{NMN}}\oplus\mathbf{m_{Q}})\oplus\mathbf{m_{C}}))" display="inline"><semantics id="S3.Ex3.m2.3a"><mrow id="S3.Ex3.m2.3.3" xref="S3.Ex3.m2.3.3.cmml"><mi id="S3.Ex3.m2.3.3.3" xref="S3.Ex3.m2.3.3.3.cmml"></mi><mo id="S3.Ex3.m2.3.3.2" xref="S3.Ex3.m2.3.3.2.cmml">=</mo><mrow id="S3.Ex3.m2.3.3.1" xref="S3.Ex3.m2.3.3.1.cmml"><mi id="S3.Ex3.m2.3.3.1.3" xref="S3.Ex3.m2.3.3.1.3.cmml">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</mi><mo lspace="0em" rspace="0em" id="S3.Ex3.m2.3.3.1.2" xref="S3.Ex3.m2.3.3.1.2.cmml">â€‹</mo><mrow id="S3.Ex3.m2.3.3.1.1.1" xref="S3.Ex3.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex3.m2.3.3.1.1.1.2" xref="S3.Ex3.m2.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.Ex3.m2.3.3.1.1.1.1" xref="S3.Ex3.m2.3.3.1.1.1.1.cmml"><mi id="S3.Ex3.m2.3.3.1.1.1.1.3" xref="S3.Ex3.m2.3.3.1.1.1.1.3.cmml">ğ–</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex3.m2.3.3.1.1.1.1.2" xref="S3.Ex3.m2.3.3.1.1.1.1.2.cmml">âˆ—</mo><mrow id="S3.Ex3.m2.3.3.1.1.1.1.1.1" xref="S3.Ex3.m2.3.3.1.1.1.1.1.2.cmml"><mi id="S3.Ex3.m2.1.1" xref="S3.Ex3.m2.1.1.cmml">max</mi><mo id="S3.Ex3.m2.3.3.1.1.1.1.1.1a" xref="S3.Ex3.m2.3.3.1.1.1.1.1.2.cmml">â¡</mo><mrow id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1" xref="S3.Ex3.m2.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.2" xref="S3.Ex3.m2.3.3.1.1.1.1.1.2.cmml">(</mo><mn id="S3.Ex3.m2.2.2" xref="S3.Ex3.m2.2.2.cmml">0</mn><mo id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.3" xref="S3.Ex3.m2.3.3.1.1.1.1.1.2.cmml">,</mo><mrow id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğğ«ğğ</mi><mi id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">ğğŒğ</mi></msub><mo id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âŠ•</mo><msub id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">ğ¦</mi><mi id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">ğ</mi></msub></mrow><mo stretchy="false" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.2" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.2.cmml">âŠ•</mo><msub id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.2.cmml">ğ¦</mi><mi id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.3.cmml">ğ‚</mi></msub></mrow><mo stretchy="false" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.4" xref="S3.Ex3.m2.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.Ex3.m2.3.3.1.1.1.3" xref="S3.Ex3.m2.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex3.m2.3b"><apply id="S3.Ex3.m2.3.3.cmml" xref="S3.Ex3.m2.3.3"><eq id="S3.Ex3.m2.3.3.2.cmml" xref="S3.Ex3.m2.3.3.2"></eq><csymbol cd="latexml" id="S3.Ex3.m2.3.3.3.cmml" xref="S3.Ex3.m2.3.3.3">absent</csymbol><apply id="S3.Ex3.m2.3.3.1.cmml" xref="S3.Ex3.m2.3.3.1"><times id="S3.Ex3.m2.3.3.1.2.cmml" xref="S3.Ex3.m2.3.3.1.2"></times><ci id="S3.Ex3.m2.3.3.1.3.cmml" xref="S3.Ex3.m2.3.3.1.3">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</ci><apply id="S3.Ex3.m2.3.3.1.1.1.1.cmml" xref="S3.Ex3.m2.3.3.1.1.1"><times id="S3.Ex3.m2.3.3.1.1.1.1.2.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.2"></times><ci id="S3.Ex3.m2.3.3.1.1.1.1.3.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.3">ğ–</ci><apply id="S3.Ex3.m2.3.3.1.1.1.1.1.2.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1"><max id="S3.Ex3.m2.1.1.cmml" xref="S3.Ex3.m2.1.1"></max><cn type="integer" id="S3.Ex3.m2.2.2.cmml" xref="S3.Ex3.m2.2.2">0</cn><apply id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.2">direct-sum</csymbol><apply id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.1">direct-sum</csymbol><apply id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">ğğ«ğğ</ci><ci id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">ğğŒğ</ci></apply><apply id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¦</ci><ci id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ</ci></apply></apply><apply id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.2">ğ¦</ci><ci id="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex3.m2.3.3.1.1.1.1.1.1.1.1.3.3">ğ‚</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex3.m2.3c">\displaystyle=\mathit{softmax}(\mathbf{W}*\max(0,(\mathbf{Pred_{NMN}}\oplus\mathbf{m_{Q}})\oplus\mathbf{m_{C}}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex4.m1.1" class="ltx_Math" alttext="\displaystyle\mathit{answer}" display="inline"><semantics id="S3.Ex4.m1.1a"><mi id="S3.Ex4.m1.1.1" xref="S3.Ex4.m1.1.1.cmml">ğ‘ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.Ex4.m1.1b"><ci id="S3.Ex4.m1.1.1.cmml" xref="S3.Ex4.m1.1.1">ğ‘ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m1.1c">\displaystyle\mathit{answer}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex4.m2.1" class="ltx_Math" alttext="\displaystyle=\operatorname{arg\,max}_{i}\hskip 5.69046pt\mathbf{Pred_{V}}[i]" display="inline"><semantics id="S3.Ex4.m2.1a"><mrow id="S3.Ex4.m2.1.2" xref="S3.Ex4.m2.1.2.cmml"><mi id="S3.Ex4.m2.1.2.2" xref="S3.Ex4.m2.1.2.2.cmml"></mi><mo id="S3.Ex4.m2.1.2.1" xref="S3.Ex4.m2.1.2.1.cmml">=</mo><mrow id="S3.Ex4.m2.1.2.3" xref="S3.Ex4.m2.1.2.3.cmml"><mrow id="S3.Ex4.m2.1.2.3.2" xref="S3.Ex4.m2.1.2.3.2.cmml"><msub id="S3.Ex4.m2.1.2.3.2.1" xref="S3.Ex4.m2.1.2.3.2.1.cmml"><mrow id="S3.Ex4.m2.1.2.3.2.1.2" xref="S3.Ex4.m2.1.2.3.2.1.2.cmml"><mi id="S3.Ex4.m2.1.2.3.2.1.2.2" xref="S3.Ex4.m2.1.2.3.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.Ex4.m2.1.2.3.2.1.2.1" xref="S3.Ex4.m2.1.2.3.2.1.2.1.cmml">â€‹</mo><mi id="S3.Ex4.m2.1.2.3.2.1.2.3" xref="S3.Ex4.m2.1.2.3.2.1.2.3.cmml">max</mi></mrow><mi id="S3.Ex4.m2.1.2.3.2.1.3" xref="S3.Ex4.m2.1.2.3.2.1.3.cmml">i</mi></msub><mo id="S3.Ex4.m2.1.2.3.2a" xref="S3.Ex4.m2.1.2.3.2.cmml">â¡</mo><msub id="S3.Ex4.m2.1.2.3.2.2" xref="S3.Ex4.m2.1.2.3.2.2.cmml"><mi id="S3.Ex4.m2.1.2.3.2.2.2" xref="S3.Ex4.m2.1.2.3.2.2.2.cmml">ğğ«ğğ</mi><mi id="S3.Ex4.m2.1.2.3.2.2.3" xref="S3.Ex4.m2.1.2.3.2.2.3.cmml">ğ•</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex4.m2.1.2.3.1" xref="S3.Ex4.m2.1.2.3.1.cmml">â€‹</mo><mrow id="S3.Ex4.m2.1.2.3.3.2" xref="S3.Ex4.m2.1.2.3.3.1.cmml"><mo stretchy="false" id="S3.Ex4.m2.1.2.3.3.2.1" xref="S3.Ex4.m2.1.2.3.3.1.1.cmml">[</mo><mi id="S3.Ex4.m2.1.1" xref="S3.Ex4.m2.1.1.cmml">i</mi><mo stretchy="false" id="S3.Ex4.m2.1.2.3.3.2.2" xref="S3.Ex4.m2.1.2.3.3.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex4.m2.1b"><apply id="S3.Ex4.m2.1.2.cmml" xref="S3.Ex4.m2.1.2"><eq id="S3.Ex4.m2.1.2.1.cmml" xref="S3.Ex4.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.Ex4.m2.1.2.2.cmml" xref="S3.Ex4.m2.1.2.2">absent</csymbol><apply id="S3.Ex4.m2.1.2.3.cmml" xref="S3.Ex4.m2.1.2.3"><times id="S3.Ex4.m2.1.2.3.1.cmml" xref="S3.Ex4.m2.1.2.3.1"></times><apply id="S3.Ex4.m2.1.2.3.2.cmml" xref="S3.Ex4.m2.1.2.3.2"><apply id="S3.Ex4.m2.1.2.3.2.1.cmml" xref="S3.Ex4.m2.1.2.3.2.1"><csymbol cd="ambiguous" id="S3.Ex4.m2.1.2.3.2.1.1.cmml" xref="S3.Ex4.m2.1.2.3.2.1">subscript</csymbol><apply id="S3.Ex4.m2.1.2.3.2.1.2.cmml" xref="S3.Ex4.m2.1.2.3.2.1.2"><times id="S3.Ex4.m2.1.2.3.2.1.2.1.cmml" xref="S3.Ex4.m2.1.2.3.2.1.2.1"></times><ci id="S3.Ex4.m2.1.2.3.2.1.2.2.cmml" xref="S3.Ex4.m2.1.2.3.2.1.2.2">arg</ci><ci id="S3.Ex4.m2.1.2.3.2.1.2.3.cmml" xref="S3.Ex4.m2.1.2.3.2.1.2.3">max</ci></apply><ci id="S3.Ex4.m2.1.2.3.2.1.3.cmml" xref="S3.Ex4.m2.1.2.3.2.1.3">ğ‘–</ci></apply><apply id="S3.Ex4.m2.1.2.3.2.2.cmml" xref="S3.Ex4.m2.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.Ex4.m2.1.2.3.2.2.1.cmml" xref="S3.Ex4.m2.1.2.3.2.2">subscript</csymbol><ci id="S3.Ex4.m2.1.2.3.2.2.2.cmml" xref="S3.Ex4.m2.1.2.3.2.2.2">ğğ«ğğ</ci><ci id="S3.Ex4.m2.1.2.3.2.2.3.cmml" xref="S3.Ex4.m2.1.2.3.2.2.3">ğ•</ci></apply></apply><apply id="S3.Ex4.m2.1.2.3.3.1.cmml" xref="S3.Ex4.m2.1.2.3.3.2"><csymbol cd="latexml" id="S3.Ex4.m2.1.2.3.3.1.1.cmml" xref="S3.Ex4.m2.1.2.3.3.2.1">delimited-[]</csymbol><ci id="S3.Ex4.m2.1.1.cmml" xref="S3.Ex4.m2.1.1">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex4.m2.1c">\displaystyle=\operatorname{arg\,max}_{i}\hskip 5.69046pt\mathbf{Pred_{V}}[i]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Caption Attention</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">In the previous approach, we notice that there are cases where the entire caption may not be helpful in answering the question being asked. Instead, an end-to-end back propagation of the error by attending to the necessary information from the caption after combining with the respective question and prediction from the NMNs can help localize on the answer space. Figure <a href="#S3.F1.sf2" title="In Figure 1 â€£ 3.1 Caption Information â€£ 3 Proposed Approach â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1(b)</span></a> outlines the architecture of this approach.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.7" class="ltx_p">In order to capture attention on the caption words, the caption word embeddings and the question context vector are individually passed through a fully connected layer with a <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathit{sigmoid}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\mathit{sigmoid}</annotation></semantics></math> (<math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">Ïƒ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\sigma</annotation></semantics></math>) activation function. The weight matrix used for all caption word embeddings (<math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\mathbf{c}_{i}" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">ğœ</mi><mi id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">ğœ</ci><ci id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\mathbf{c}_{i}</annotation></semantics></math>) and question context vector (<math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\mathbf{m_{Q}}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">ğ¦</mi><mi id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">ğ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">ğ¦</ci><ci id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">ğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\mathbf{m_{Q}}</annotation></semantics></math>) are <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{W_{C}}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">ğ–</mi><mi id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml">ğ‚</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">ğ–</ci><ci id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3">ğ‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\mathbf{W_{C}}</annotation></semantics></math> and <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="\mathbf{W_{Q}}" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><msub id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mi id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">ğ–</mi><mi id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">ğ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">ğ–</ci><ci id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">ğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">\mathbf{W_{Q}}</annotation></semantics></math> respectively.
An elementwise dot product is then performed on the two resulting vectors. The hidden representation <math id="S3.SS2.p2.7.m7.1" class="ltx_Math" alttext="\mathbf{H}" display="inline"><semantics id="S3.SS2.p2.7.m7.1a"><mi id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml">ğ‡</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><ci id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1">ğ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\mathbf{H}</annotation></semantics></math> is:</p>
<table id="S6.EGx4" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex5"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex5.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{H}" display="inline"><semantics id="S3.Ex5.m1.1a"><mi id="S3.Ex5.m1.1.1" xref="S3.Ex5.m1.1.1.cmml">ğ‡</mi><annotation-xml encoding="MathML-Content" id="S3.Ex5.m1.1b"><ci id="S3.Ex5.m1.1.1.cmml" xref="S3.Ex5.m1.1.1">ğ‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m1.1c">\displaystyle\mathbf{H}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex5.m2.3" class="ltx_Math" alttext="\displaystyle=(\mathbf{h}_{1},\mathbf{h}_{2},\dots,\mathbf{h}_{N})\in\mathbb{R}^{k\times N},\text{ where }\mathbf{h}_{i}=\sigma(\mathbf{W_{Q}m_{Q}})\odot\sigma(\mathbf{W_{C}}\mathbf{c}_{i})" display="inline"><semantics id="S3.Ex5.m2.3a"><mrow id="S3.Ex5.m2.3.3.2" xref="S3.Ex5.m2.3.3.3.cmml"><mrow id="S3.Ex5.m2.2.2.1.1" xref="S3.Ex5.m2.2.2.1.1.cmml"><mi id="S3.Ex5.m2.2.2.1.1.5" xref="S3.Ex5.m2.2.2.1.1.5.cmml"></mi><mo id="S3.Ex5.m2.2.2.1.1.6" xref="S3.Ex5.m2.2.2.1.1.6.cmml">=</mo><mrow id="S3.Ex5.m2.2.2.1.1.3.3" xref="S3.Ex5.m2.2.2.1.1.3.4.cmml"><mo stretchy="false" id="S3.Ex5.m2.2.2.1.1.3.3.4" xref="S3.Ex5.m2.2.2.1.1.3.4.cmml">(</mo><msub id="S3.Ex5.m2.2.2.1.1.1.1.1" xref="S3.Ex5.m2.2.2.1.1.1.1.1.cmml"><mi id="S3.Ex5.m2.2.2.1.1.1.1.1.2" xref="S3.Ex5.m2.2.2.1.1.1.1.1.2.cmml">ğ¡</mi><mn id="S3.Ex5.m2.2.2.1.1.1.1.1.3" xref="S3.Ex5.m2.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.Ex5.m2.2.2.1.1.3.3.5" xref="S3.Ex5.m2.2.2.1.1.3.4.cmml">,</mo><msub id="S3.Ex5.m2.2.2.1.1.2.2.2" xref="S3.Ex5.m2.2.2.1.1.2.2.2.cmml"><mi id="S3.Ex5.m2.2.2.1.1.2.2.2.2" xref="S3.Ex5.m2.2.2.1.1.2.2.2.2.cmml">ğ¡</mi><mn id="S3.Ex5.m2.2.2.1.1.2.2.2.3" xref="S3.Ex5.m2.2.2.1.1.2.2.2.3.cmml">2</mn></msub><mo id="S3.Ex5.m2.2.2.1.1.3.3.6" xref="S3.Ex5.m2.2.2.1.1.3.4.cmml">,</mo><mi mathvariant="normal" id="S3.Ex5.m2.1.1" xref="S3.Ex5.m2.1.1.cmml">â€¦</mi><mo id="S3.Ex5.m2.2.2.1.1.3.3.7" xref="S3.Ex5.m2.2.2.1.1.3.4.cmml">,</mo><msub id="S3.Ex5.m2.2.2.1.1.3.3.3" xref="S3.Ex5.m2.2.2.1.1.3.3.3.cmml"><mi id="S3.Ex5.m2.2.2.1.1.3.3.3.2" xref="S3.Ex5.m2.2.2.1.1.3.3.3.2.cmml">ğ¡</mi><mi id="S3.Ex5.m2.2.2.1.1.3.3.3.3" xref="S3.Ex5.m2.2.2.1.1.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S3.Ex5.m2.2.2.1.1.3.3.8" xref="S3.Ex5.m2.2.2.1.1.3.4.cmml">)</mo></mrow><mo id="S3.Ex5.m2.2.2.1.1.7" xref="S3.Ex5.m2.2.2.1.1.7.cmml">âˆˆ</mo><msup id="S3.Ex5.m2.2.2.1.1.8" xref="S3.Ex5.m2.2.2.1.1.8.cmml"><mi id="S3.Ex5.m2.2.2.1.1.8.2" xref="S3.Ex5.m2.2.2.1.1.8.2.cmml">â„</mi><mrow id="S3.Ex5.m2.2.2.1.1.8.3" xref="S3.Ex5.m2.2.2.1.1.8.3.cmml"><mi id="S3.Ex5.m2.2.2.1.1.8.3.2" xref="S3.Ex5.m2.2.2.1.1.8.3.2.cmml">k</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex5.m2.2.2.1.1.8.3.1" xref="S3.Ex5.m2.2.2.1.1.8.3.1.cmml">Ã—</mo><mi id="S3.Ex5.m2.2.2.1.1.8.3.3" xref="S3.Ex5.m2.2.2.1.1.8.3.3.cmml">N</mi></mrow></msup></mrow><mo id="S3.Ex5.m2.3.3.2.3" xref="S3.Ex5.m2.3.3.3a.cmml">,</mo><mrow id="S3.Ex5.m2.3.3.2.2" xref="S3.Ex5.m2.3.3.2.2.cmml"><mrow id="S3.Ex5.m2.3.3.2.2.4" xref="S3.Ex5.m2.3.3.2.2.4.cmml"><mtext id="S3.Ex5.m2.3.3.2.2.4.2" xref="S3.Ex5.m2.3.3.2.2.4.2a.cmml">Â whereÂ </mtext><mo lspace="0em" rspace="0em" id="S3.Ex5.m2.3.3.2.2.4.1" xref="S3.Ex5.m2.3.3.2.2.4.1.cmml">â€‹</mo><msub id="S3.Ex5.m2.3.3.2.2.4.3" xref="S3.Ex5.m2.3.3.2.2.4.3.cmml"><mi id="S3.Ex5.m2.3.3.2.2.4.3.2" xref="S3.Ex5.m2.3.3.2.2.4.3.2.cmml">ğ¡</mi><mi id="S3.Ex5.m2.3.3.2.2.4.3.3" xref="S3.Ex5.m2.3.3.2.2.4.3.3.cmml">i</mi></msub></mrow><mo id="S3.Ex5.m2.3.3.2.2.3" xref="S3.Ex5.m2.3.3.2.2.3.cmml">=</mo><mrow id="S3.Ex5.m2.3.3.2.2.2" xref="S3.Ex5.m2.3.3.2.2.2.cmml"><mrow id="S3.Ex5.m2.3.3.2.2.1.1" xref="S3.Ex5.m2.3.3.2.2.1.1.cmml"><mrow id="S3.Ex5.m2.3.3.2.2.1.1.1" xref="S3.Ex5.m2.3.3.2.2.1.1.1.cmml"><mi id="S3.Ex5.m2.3.3.2.2.1.1.1.3" xref="S3.Ex5.m2.3.3.2.2.1.1.1.3.cmml">Ïƒ</mi><mo lspace="0em" rspace="0em" id="S3.Ex5.m2.3.3.2.2.1.1.1.2" xref="S3.Ex5.m2.3.3.2.2.1.1.1.2.cmml">â€‹</mo><mrow id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.2" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.cmml"><msub id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.2" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.2.cmml">ğ–</mi><mi id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.3" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.3.cmml">ğ</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.1" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.1.cmml">â€‹</mo><msub id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.2" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.2.cmml">ğ¦</mi><mi id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.3" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.3.cmml">ğ</mi></msub></mrow><mo rspace="0.055em" stretchy="false" id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.3" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S3.Ex5.m2.3.3.2.2.1.1.2" xref="S3.Ex5.m2.3.3.2.2.1.1.2.cmml">âŠ™</mo><mi id="S3.Ex5.m2.3.3.2.2.1.1.3" xref="S3.Ex5.m2.3.3.2.2.1.1.3.cmml">Ïƒ</mi></mrow><mo lspace="0em" rspace="0em" id="S3.Ex5.m2.3.3.2.2.2.3" xref="S3.Ex5.m2.3.3.2.2.2.3.cmml">â€‹</mo><mrow id="S3.Ex5.m2.3.3.2.2.2.2.1" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.cmml"><mo stretchy="false" id="S3.Ex5.m2.3.3.2.2.2.2.1.2" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.Ex5.m2.3.3.2.2.2.2.1.1" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.cmml"><msub id="S3.Ex5.m2.3.3.2.2.2.2.1.1.2" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.cmml"><mi id="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.2" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.2.cmml">ğ–</mi><mi id="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.3" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.3.cmml">ğ‚</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex5.m2.3.3.2.2.2.2.1.1.1" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.1.cmml">â€‹</mo><msub id="S3.Ex5.m2.3.3.2.2.2.2.1.1.3" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.cmml"><mi id="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.2" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.2.cmml">ğœ</mi><mi id="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.3" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S3.Ex5.m2.3.3.2.2.2.2.1.3" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex5.m2.3b"><apply id="S3.Ex5.m2.3.3.3.cmml" xref="S3.Ex5.m2.3.3.2"><csymbol cd="ambiguous" id="S3.Ex5.m2.3.3.3a.cmml" xref="S3.Ex5.m2.3.3.2.3">formulae-sequence</csymbol><apply id="S3.Ex5.m2.2.2.1.1.cmml" xref="S3.Ex5.m2.2.2.1.1"><and id="S3.Ex5.m2.2.2.1.1a.cmml" xref="S3.Ex5.m2.2.2.1.1"></and><apply id="S3.Ex5.m2.2.2.1.1b.cmml" xref="S3.Ex5.m2.2.2.1.1"><eq id="S3.Ex5.m2.2.2.1.1.6.cmml" xref="S3.Ex5.m2.2.2.1.1.6"></eq><csymbol cd="latexml" id="S3.Ex5.m2.2.2.1.1.5.cmml" xref="S3.Ex5.m2.2.2.1.1.5">absent</csymbol><vector id="S3.Ex5.m2.2.2.1.1.3.4.cmml" xref="S3.Ex5.m2.2.2.1.1.3.3"><apply id="S3.Ex5.m2.2.2.1.1.1.1.1.cmml" xref="S3.Ex5.m2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex5.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex5.m2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex5.m2.2.2.1.1.1.1.1.2.cmml" xref="S3.Ex5.m2.2.2.1.1.1.1.1.2">ğ¡</ci><cn type="integer" id="S3.Ex5.m2.2.2.1.1.1.1.1.3.cmml" xref="S3.Ex5.m2.2.2.1.1.1.1.1.3">1</cn></apply><apply id="S3.Ex5.m2.2.2.1.1.2.2.2.cmml" xref="S3.Ex5.m2.2.2.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex5.m2.2.2.1.1.2.2.2.1.cmml" xref="S3.Ex5.m2.2.2.1.1.2.2.2">subscript</csymbol><ci id="S3.Ex5.m2.2.2.1.1.2.2.2.2.cmml" xref="S3.Ex5.m2.2.2.1.1.2.2.2.2">ğ¡</ci><cn type="integer" id="S3.Ex5.m2.2.2.1.1.2.2.2.3.cmml" xref="S3.Ex5.m2.2.2.1.1.2.2.2.3">2</cn></apply><ci id="S3.Ex5.m2.1.1.cmml" xref="S3.Ex5.m2.1.1">â€¦</ci><apply id="S3.Ex5.m2.2.2.1.1.3.3.3.cmml" xref="S3.Ex5.m2.2.2.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex5.m2.2.2.1.1.3.3.3.1.cmml" xref="S3.Ex5.m2.2.2.1.1.3.3.3">subscript</csymbol><ci id="S3.Ex5.m2.2.2.1.1.3.3.3.2.cmml" xref="S3.Ex5.m2.2.2.1.1.3.3.3.2">ğ¡</ci><ci id="S3.Ex5.m2.2.2.1.1.3.3.3.3.cmml" xref="S3.Ex5.m2.2.2.1.1.3.3.3.3">ğ‘</ci></apply></vector></apply><apply id="S3.Ex5.m2.2.2.1.1c.cmml" xref="S3.Ex5.m2.2.2.1.1"><in id="S3.Ex5.m2.2.2.1.1.7.cmml" xref="S3.Ex5.m2.2.2.1.1.7"></in><share href="#S3.Ex5.m2.2.2.1.1.3.cmml" id="S3.Ex5.m2.2.2.1.1d.cmml" xref="S3.Ex5.m2.2.2.1.1"></share><apply id="S3.Ex5.m2.2.2.1.1.8.cmml" xref="S3.Ex5.m2.2.2.1.1.8"><csymbol cd="ambiguous" id="S3.Ex5.m2.2.2.1.1.8.1.cmml" xref="S3.Ex5.m2.2.2.1.1.8">superscript</csymbol><ci id="S3.Ex5.m2.2.2.1.1.8.2.cmml" xref="S3.Ex5.m2.2.2.1.1.8.2">â„</ci><apply id="S3.Ex5.m2.2.2.1.1.8.3.cmml" xref="S3.Ex5.m2.2.2.1.1.8.3"><times id="S3.Ex5.m2.2.2.1.1.8.3.1.cmml" xref="S3.Ex5.m2.2.2.1.1.8.3.1"></times><ci id="S3.Ex5.m2.2.2.1.1.8.3.2.cmml" xref="S3.Ex5.m2.2.2.1.1.8.3.2">ğ‘˜</ci><ci id="S3.Ex5.m2.2.2.1.1.8.3.3.cmml" xref="S3.Ex5.m2.2.2.1.1.8.3.3">ğ‘</ci></apply></apply></apply></apply><apply id="S3.Ex5.m2.3.3.2.2.cmml" xref="S3.Ex5.m2.3.3.2.2"><eq id="S3.Ex5.m2.3.3.2.2.3.cmml" xref="S3.Ex5.m2.3.3.2.2.3"></eq><apply id="S3.Ex5.m2.3.3.2.2.4.cmml" xref="S3.Ex5.m2.3.3.2.2.4"><times id="S3.Ex5.m2.3.3.2.2.4.1.cmml" xref="S3.Ex5.m2.3.3.2.2.4.1"></times><ci id="S3.Ex5.m2.3.3.2.2.4.2a.cmml" xref="S3.Ex5.m2.3.3.2.2.4.2"><mtext id="S3.Ex5.m2.3.3.2.2.4.2.cmml" xref="S3.Ex5.m2.3.3.2.2.4.2">Â whereÂ </mtext></ci><apply id="S3.Ex5.m2.3.3.2.2.4.3.cmml" xref="S3.Ex5.m2.3.3.2.2.4.3"><csymbol cd="ambiguous" id="S3.Ex5.m2.3.3.2.2.4.3.1.cmml" xref="S3.Ex5.m2.3.3.2.2.4.3">subscript</csymbol><ci id="S3.Ex5.m2.3.3.2.2.4.3.2.cmml" xref="S3.Ex5.m2.3.3.2.2.4.3.2">ğ¡</ci><ci id="S3.Ex5.m2.3.3.2.2.4.3.3.cmml" xref="S3.Ex5.m2.3.3.2.2.4.3.3">ğ‘–</ci></apply></apply><apply id="S3.Ex5.m2.3.3.2.2.2.cmml" xref="S3.Ex5.m2.3.3.2.2.2"><times id="S3.Ex5.m2.3.3.2.2.2.3.cmml" xref="S3.Ex5.m2.3.3.2.2.2.3"></times><apply id="S3.Ex5.m2.3.3.2.2.1.1.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1"><csymbol cd="latexml" id="S3.Ex5.m2.3.3.2.2.1.1.2.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.2">direct-product</csymbol><apply id="S3.Ex5.m2.3.3.2.2.1.1.1.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1"><times id="S3.Ex5.m2.3.3.2.2.1.1.1.2.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.2"></times><ci id="S3.Ex5.m2.3.3.2.2.1.1.1.3.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.3">ğœ</ci><apply id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1"><times id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.1"></times><apply id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.2">ğ–</ci><ci id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.2.3">ğ</ci></apply><apply id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.2">ğ¦</ci><ci id="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.1.1.1.1.3.3">ğ</ci></apply></apply></apply><ci id="S3.Ex5.m2.3.3.2.2.1.1.3.cmml" xref="S3.Ex5.m2.3.3.2.2.1.1.3">ğœ</ci></apply><apply id="S3.Ex5.m2.3.3.2.2.2.2.1.1.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1"><times id="S3.Ex5.m2.3.3.2.2.2.2.1.1.1.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.1"></times><apply id="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.1.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.2.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.2">ğ–</ci><ci id="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.3.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.2.3">ğ‚</ci></apply><apply id="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.1.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.2.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.2">ğœ</ci><ci id="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.3.cmml" xref="S3.Ex5.m2.3.3.2.2.2.2.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex5.m2.3c">\displaystyle=(\mathbf{h}_{1},\mathbf{h}_{2},\dots,\mathbf{h}_{N})\in\mathbb{R}^{k\times N},\text{ where }\mathbf{h}_{i}=\sigma(\mathbf{W_{Q}m_{Q}})\odot\sigma(\mathbf{W_{C}}\mathbf{c}_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.11" class="ltx_p">The elementwise multiplication propagates the error that does not maximize the importance of the words in the captions with respect to the question. The projected space is thus brought back into the dimensions of the caption length and a normalized probability distribution is computed over this vector using a <math id="S3.SS2.p2.8.m1.1" class="ltx_Math" alttext="softmax" display="inline"><semantics id="S3.SS2.p2.8.m1.1a"><mrow id="S3.SS2.p2.8.m1.1.1" xref="S3.SS2.p2.8.m1.1.1.cmml"><mi id="S3.SS2.p2.8.m1.1.1.2" xref="S3.SS2.p2.8.m1.1.1.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m1.1.1.1" xref="S3.SS2.p2.8.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.8.m1.1.1.3" xref="S3.SS2.p2.8.m1.1.1.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m1.1.1.1a" xref="S3.SS2.p2.8.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.8.m1.1.1.4" xref="S3.SS2.p2.8.m1.1.1.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m1.1.1.1b" xref="S3.SS2.p2.8.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.8.m1.1.1.5" xref="S3.SS2.p2.8.m1.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m1.1.1.1c" xref="S3.SS2.p2.8.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.8.m1.1.1.6" xref="S3.SS2.p2.8.m1.1.1.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m1.1.1.1d" xref="S3.SS2.p2.8.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.8.m1.1.1.7" xref="S3.SS2.p2.8.m1.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.8.m1.1.1.1e" xref="S3.SS2.p2.8.m1.1.1.1.cmml">â€‹</mo><mi id="S3.SS2.p2.8.m1.1.1.8" xref="S3.SS2.p2.8.m1.1.1.8.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m1.1b"><apply id="S3.SS2.p2.8.m1.1.1.cmml" xref="S3.SS2.p2.8.m1.1.1"><times id="S3.SS2.p2.8.m1.1.1.1.cmml" xref="S3.SS2.p2.8.m1.1.1.1"></times><ci id="S3.SS2.p2.8.m1.1.1.2.cmml" xref="S3.SS2.p2.8.m1.1.1.2">ğ‘ </ci><ci id="S3.SS2.p2.8.m1.1.1.3.cmml" xref="S3.SS2.p2.8.m1.1.1.3">ğ‘œ</ci><ci id="S3.SS2.p2.8.m1.1.1.4.cmml" xref="S3.SS2.p2.8.m1.1.1.4">ğ‘“</ci><ci id="S3.SS2.p2.8.m1.1.1.5.cmml" xref="S3.SS2.p2.8.m1.1.1.5">ğ‘¡</ci><ci id="S3.SS2.p2.8.m1.1.1.6.cmml" xref="S3.SS2.p2.8.m1.1.1.6">ğ‘š</ci><ci id="S3.SS2.p2.8.m1.1.1.7.cmml" xref="S3.SS2.p2.8.m1.1.1.7">ğ‘</ci><ci id="S3.SS2.p2.8.m1.1.1.8.cmml" xref="S3.SS2.p2.8.m1.1.1.8">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m1.1c">softmax</annotation></semantics></math> function (with <math id="S3.SS2.p2.9.m2.1" class="ltx_Math" alttext="\mathbf{W_{h}}" display="inline"><semantics id="S3.SS2.p2.9.m2.1a"><msub id="S3.SS2.p2.9.m2.1.1" xref="S3.SS2.p2.9.m2.1.1.cmml"><mi id="S3.SS2.p2.9.m2.1.1.2" xref="S3.SS2.p2.9.m2.1.1.2.cmml">ğ–</mi><mi id="S3.SS2.p2.9.m2.1.1.3" xref="S3.SS2.p2.9.m2.1.1.3.cmml">ğ¡</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m2.1b"><apply id="S3.SS2.p2.9.m2.1.1.cmml" xref="S3.SS2.p2.9.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m2.1.1.1.cmml" xref="S3.SS2.p2.9.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m2.1.1.2.cmml" xref="S3.SS2.p2.9.m2.1.1.2">ğ–</ci><ci id="S3.SS2.p2.9.m2.1.1.3.cmml" xref="S3.SS2.p2.9.m2.1.1.3">ğ¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m2.1c">\mathbf{W_{h}}</annotation></semantics></math> as weight parameter) to get the attention vector <math id="S3.SS2.p2.10.m3.1" class="ltx_Math" alttext="\mathbf{a}" display="inline"><semantics id="S3.SS2.p2.10.m3.1a"><mi id="S3.SS2.p2.10.m3.1.1" xref="S3.SS2.p2.10.m3.1.1.cmml">ğš</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m3.1b"><ci id="S3.SS2.p2.10.m3.1.1.cmml" xref="S3.SS2.p2.10.m3.1.1">ğš</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m3.1c">\mathbf{a}</annotation></semantics></math> for the captions. The caption attention context vector <math id="S3.SS2.p2.11.m4.1" class="ltx_Math" alttext="\hat{\mathbf{C}}" display="inline"><semantics id="S3.SS2.p2.11.m4.1a"><mover accent="true" id="S3.SS2.p2.11.m4.1.1" xref="S3.SS2.p2.11.m4.1.1.cmml"><mi id="S3.SS2.p2.11.m4.1.1.2" xref="S3.SS2.p2.11.m4.1.1.2.cmml">ğ‚</mi><mo id="S3.SS2.p2.11.m4.1.1.1" xref="S3.SS2.p2.11.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m4.1b"><apply id="S3.SS2.p2.11.m4.1.1.cmml" xref="S3.SS2.p2.11.m4.1.1"><ci id="S3.SS2.p2.11.m4.1.1.1.cmml" xref="S3.SS2.p2.11.m4.1.1.1">^</ci><ci id="S3.SS2.p2.11.m4.1.1.2.cmml" xref="S3.SS2.p2.11.m4.1.1.2">ğ‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m4.1c">\hat{\mathbf{C}}</annotation></semantics></math> is then the average of the caption word embeddings weighted by the attention as shown below:</p>
<table id="S6.EGx5" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex6"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex6.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{\hat{C}}" display="inline"><semantics id="S3.Ex6.m1.1a"><mover accent="true" id="S3.Ex6.m1.1.1" xref="S3.Ex6.m1.1.1.cmml"><mi id="S3.Ex6.m1.1.1.2" xref="S3.Ex6.m1.1.1.2.cmml">ğ‚</mi><mo id="S3.Ex6.m1.1.1.1" xref="S3.Ex6.m1.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.Ex6.m1.1b"><apply id="S3.Ex6.m1.1.1.cmml" xref="S3.Ex6.m1.1.1"><ci id="S3.Ex6.m1.1.1.1.cmml" xref="S3.Ex6.m1.1.1.1">^</ci><ci id="S3.Ex6.m1.1.1.2.cmml" xref="S3.Ex6.m1.1.1.2">ğ‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex6.m1.1c">\displaystyle\mathbf{\hat{C}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex6.m2.2" class="ltx_Math" alttext="\displaystyle=\sum_{i=1}^{N}a_{i}\mathbf{c_{i}},\hskip 5.69046pta_{i}\in\mathbf{a},\text{ where }\mathbf{a}=\mathit{softmax}(\mathbf{W_{h}H})" display="inline"><semantics id="S3.Ex6.m2.2a"><mrow id="S3.Ex6.m2.2.2.2" xref="S3.Ex6.m2.2.2.3.cmml"><mrow id="S3.Ex6.m2.1.1.1.1" xref="S3.Ex6.m2.1.1.1.1.cmml"><mi id="S3.Ex6.m2.1.1.1.1.2" xref="S3.Ex6.m2.1.1.1.1.2.cmml"></mi><mo id="S3.Ex6.m2.1.1.1.1.1" xref="S3.Ex6.m2.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex6.m2.1.1.1.1.3" xref="S3.Ex6.m2.1.1.1.1.3.cmml"><mstyle displaystyle="true" id="S3.Ex6.m2.1.1.1.1.3.1" xref="S3.Ex6.m2.1.1.1.1.3.1.cmml"><munderover id="S3.Ex6.m2.1.1.1.1.3.1a" xref="S3.Ex6.m2.1.1.1.1.3.1.cmml"><mo movablelimits="false" id="S3.Ex6.m2.1.1.1.1.3.1.2.2" xref="S3.Ex6.m2.1.1.1.1.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex6.m2.1.1.1.1.3.1.2.3" xref="S3.Ex6.m2.1.1.1.1.3.1.2.3.cmml"><mi id="S3.Ex6.m2.1.1.1.1.3.1.2.3.2" xref="S3.Ex6.m2.1.1.1.1.3.1.2.3.2.cmml">i</mi><mo id="S3.Ex6.m2.1.1.1.1.3.1.2.3.1" xref="S3.Ex6.m2.1.1.1.1.3.1.2.3.1.cmml">=</mo><mn id="S3.Ex6.m2.1.1.1.1.3.1.2.3.3" xref="S3.Ex6.m2.1.1.1.1.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.Ex6.m2.1.1.1.1.3.1.3" xref="S3.Ex6.m2.1.1.1.1.3.1.3.cmml">N</mi></munderover></mstyle><mrow id="S3.Ex6.m2.1.1.1.1.3.2" xref="S3.Ex6.m2.1.1.1.1.3.2.cmml"><msub id="S3.Ex6.m2.1.1.1.1.3.2.2" xref="S3.Ex6.m2.1.1.1.1.3.2.2.cmml"><mi id="S3.Ex6.m2.1.1.1.1.3.2.2.2" xref="S3.Ex6.m2.1.1.1.1.3.2.2.2.cmml">a</mi><mi id="S3.Ex6.m2.1.1.1.1.3.2.2.3" xref="S3.Ex6.m2.1.1.1.1.3.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex6.m2.1.1.1.1.3.2.1" xref="S3.Ex6.m2.1.1.1.1.3.2.1.cmml">â€‹</mo><msub id="S3.Ex6.m2.1.1.1.1.3.2.3" xref="S3.Ex6.m2.1.1.1.1.3.2.3.cmml"><mi id="S3.Ex6.m2.1.1.1.1.3.2.3.2" xref="S3.Ex6.m2.1.1.1.1.3.2.3.2.cmml">ğœ</mi><mi id="S3.Ex6.m2.1.1.1.1.3.2.3.3" xref="S3.Ex6.m2.1.1.1.1.3.2.3.3.cmml">ğ¢</mi></msub></mrow></mrow></mrow><mo rspace="0.737em" id="S3.Ex6.m2.2.2.2.3" xref="S3.Ex6.m2.2.2.3a.cmml">,</mo><mrow id="S3.Ex6.m2.2.2.2.2.2" xref="S3.Ex6.m2.2.2.2.2.3.cmml"><mrow id="S3.Ex6.m2.2.2.2.2.1.1" xref="S3.Ex6.m2.2.2.2.2.1.1.cmml"><msub id="S3.Ex6.m2.2.2.2.2.1.1.2" xref="S3.Ex6.m2.2.2.2.2.1.1.2.cmml"><mi id="S3.Ex6.m2.2.2.2.2.1.1.2.2" xref="S3.Ex6.m2.2.2.2.2.1.1.2.2.cmml">a</mi><mi id="S3.Ex6.m2.2.2.2.2.1.1.2.3" xref="S3.Ex6.m2.2.2.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S3.Ex6.m2.2.2.2.2.1.1.1" xref="S3.Ex6.m2.2.2.2.2.1.1.1.cmml">âˆˆ</mo><mi id="S3.Ex6.m2.2.2.2.2.1.1.3" xref="S3.Ex6.m2.2.2.2.2.1.1.3.cmml">ğš</mi></mrow><mo id="S3.Ex6.m2.2.2.2.2.2.3" xref="S3.Ex6.m2.2.2.2.2.3a.cmml">,</mo><mrow id="S3.Ex6.m2.2.2.2.2.2.2" xref="S3.Ex6.m2.2.2.2.2.2.2.cmml"><mrow id="S3.Ex6.m2.2.2.2.2.2.2.3" xref="S3.Ex6.m2.2.2.2.2.2.2.3.cmml"><mtext id="S3.Ex6.m2.2.2.2.2.2.2.3.2" xref="S3.Ex6.m2.2.2.2.2.2.2.3.2a.cmml">Â whereÂ </mtext><mo lspace="0em" rspace="0em" id="S3.Ex6.m2.2.2.2.2.2.2.3.1" xref="S3.Ex6.m2.2.2.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S3.Ex6.m2.2.2.2.2.2.2.3.3" xref="S3.Ex6.m2.2.2.2.2.2.2.3.3.cmml">ğš</mi></mrow><mo id="S3.Ex6.m2.2.2.2.2.2.2.2" xref="S3.Ex6.m2.2.2.2.2.2.2.2.cmml">=</mo><mrow id="S3.Ex6.m2.2.2.2.2.2.2.1" xref="S3.Ex6.m2.2.2.2.2.2.2.1.cmml"><mi id="S3.Ex6.m2.2.2.2.2.2.2.1.3" xref="S3.Ex6.m2.2.2.2.2.2.2.1.3.cmml">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</mi><mo lspace="0em" rspace="0em" id="S3.Ex6.m2.2.2.2.2.2.2.1.2" xref="S3.Ex6.m2.2.2.2.2.2.2.1.2.cmml">â€‹</mo><mrow id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.2" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.cmml"><msub id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.cmml"><mi id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.2" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.2.cmml">ğ–</mi><mi id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.3" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.3.cmml">ğ¡</mi></msub><mo lspace="0em" rspace="0em" id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.1" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.1.cmml">â€‹</mo><mi id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.3" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.3.cmml">ğ‡</mi></mrow><mo stretchy="false" id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.3" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex6.m2.2b"><apply id="S3.Ex6.m2.2.2.3.cmml" xref="S3.Ex6.m2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex6.m2.2.2.3a.cmml" xref="S3.Ex6.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.Ex6.m2.1.1.1.1.cmml" xref="S3.Ex6.m2.1.1.1.1"><eq id="S3.Ex6.m2.1.1.1.1.1.cmml" xref="S3.Ex6.m2.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.Ex6.m2.1.1.1.1.2.cmml" xref="S3.Ex6.m2.1.1.1.1.2">absent</csymbol><apply id="S3.Ex6.m2.1.1.1.1.3.cmml" xref="S3.Ex6.m2.1.1.1.1.3"><apply id="S3.Ex6.m2.1.1.1.1.3.1.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.Ex6.m2.1.1.1.1.3.1.1.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1">superscript</csymbol><apply id="S3.Ex6.m2.1.1.1.1.3.1.2.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.Ex6.m2.1.1.1.1.3.1.2.1.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1">subscript</csymbol><sum id="S3.Ex6.m2.1.1.1.1.3.1.2.2.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1.2.2"></sum><apply id="S3.Ex6.m2.1.1.1.1.3.1.2.3.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1.2.3"><eq id="S3.Ex6.m2.1.1.1.1.3.1.2.3.1.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1.2.3.1"></eq><ci id="S3.Ex6.m2.1.1.1.1.3.1.2.3.2.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1.2.3.2">ğ‘–</ci><cn type="integer" id="S3.Ex6.m2.1.1.1.1.3.1.2.3.3.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1.2.3.3">1</cn></apply></apply><ci id="S3.Ex6.m2.1.1.1.1.3.1.3.cmml" xref="S3.Ex6.m2.1.1.1.1.3.1.3">ğ‘</ci></apply><apply id="S3.Ex6.m2.1.1.1.1.3.2.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2"><times id="S3.Ex6.m2.1.1.1.1.3.2.1.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.1"></times><apply id="S3.Ex6.m2.1.1.1.1.3.2.2.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.Ex6.m2.1.1.1.1.3.2.2.1.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.2">subscript</csymbol><ci id="S3.Ex6.m2.1.1.1.1.3.2.2.2.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.2.2">ğ‘</ci><ci id="S3.Ex6.m2.1.1.1.1.3.2.2.3.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.2.3">ğ‘–</ci></apply><apply id="S3.Ex6.m2.1.1.1.1.3.2.3.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.Ex6.m2.1.1.1.1.3.2.3.1.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.3">subscript</csymbol><ci id="S3.Ex6.m2.1.1.1.1.3.2.3.2.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.3.2">ğœ</ci><ci id="S3.Ex6.m2.1.1.1.1.3.2.3.3.cmml" xref="S3.Ex6.m2.1.1.1.1.3.2.3.3">ğ¢</ci></apply></apply></apply></apply><apply id="S3.Ex6.m2.2.2.2.2.3.cmml" xref="S3.Ex6.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex6.m2.2.2.2.2.3a.cmml" xref="S3.Ex6.m2.2.2.2.2.2.3">formulae-sequence</csymbol><apply id="S3.Ex6.m2.2.2.2.2.1.1.cmml" xref="S3.Ex6.m2.2.2.2.2.1.1"><in id="S3.Ex6.m2.2.2.2.2.1.1.1.cmml" xref="S3.Ex6.m2.2.2.2.2.1.1.1"></in><apply id="S3.Ex6.m2.2.2.2.2.1.1.2.cmml" xref="S3.Ex6.m2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.Ex6.m2.2.2.2.2.1.1.2.1.cmml" xref="S3.Ex6.m2.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.Ex6.m2.2.2.2.2.1.1.2.2.cmml" xref="S3.Ex6.m2.2.2.2.2.1.1.2.2">ğ‘</ci><ci id="S3.Ex6.m2.2.2.2.2.1.1.2.3.cmml" xref="S3.Ex6.m2.2.2.2.2.1.1.2.3">ğ‘–</ci></apply><ci id="S3.Ex6.m2.2.2.2.2.1.1.3.cmml" xref="S3.Ex6.m2.2.2.2.2.1.1.3">ğš</ci></apply><apply id="S3.Ex6.m2.2.2.2.2.2.2.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2"><eq id="S3.Ex6.m2.2.2.2.2.2.2.2.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.2"></eq><apply id="S3.Ex6.m2.2.2.2.2.2.2.3.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.3"><times id="S3.Ex6.m2.2.2.2.2.2.2.3.1.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.3.1"></times><ci id="S3.Ex6.m2.2.2.2.2.2.2.3.2a.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.3.2"><mtext id="S3.Ex6.m2.2.2.2.2.2.2.3.2.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.3.2">Â whereÂ </mtext></ci><ci id="S3.Ex6.m2.2.2.2.2.2.2.3.3.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.3.3">ğš</ci></apply><apply id="S3.Ex6.m2.2.2.2.2.2.2.1.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1"><times id="S3.Ex6.m2.2.2.2.2.2.2.1.2.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.2"></times><ci id="S3.Ex6.m2.2.2.2.2.2.2.1.3.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.3">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</ci><apply id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1"><times id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.1.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.1"></times><apply id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.1.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.2.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.2">ğ–</ci><ci id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.3.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.2.3">ğ¡</ci></apply><ci id="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.3.cmml" xref="S3.Ex6.m2.2.2.2.2.2.2.1.1.1.1.3">ğ‡</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex6.m2.2c">\displaystyle=\sum_{i=1}^{N}a_{i}\mathbf{c_{i}},\hskip 5.69046pta_{i}\in\mathbf{a},\text{ where }\mathbf{a}=\mathit{softmax}(\mathbf{W_{h}H})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S3.SS2.p2.16" class="ltx_p"><math id="S3.SS2.p2.12.m1.1" class="ltx_Math" alttext="\mathbf{Pred_{NMN}}" display="inline"><semantics id="S3.SS2.p2.12.m1.1a"><msub id="S3.SS2.p2.12.m1.1.1" xref="S3.SS2.p2.12.m1.1.1.cmml"><mi id="S3.SS2.p2.12.m1.1.1.2" xref="S3.SS2.p2.12.m1.1.1.2.cmml">ğğ«ğğ</mi><mi id="S3.SS2.p2.12.m1.1.1.3" xref="S3.SS2.p2.12.m1.1.1.3.cmml">ğğŒğ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.12.m1.1b"><apply id="S3.SS2.p2.12.m1.1.1.cmml" xref="S3.SS2.p2.12.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.12.m1.1.1.1.cmml" xref="S3.SS2.p2.12.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.12.m1.1.1.2.cmml" xref="S3.SS2.p2.12.m1.1.1.2">ğğ«ğğ</ci><ci id="S3.SS2.p2.12.m1.1.1.3.cmml" xref="S3.SS2.p2.12.m1.1.1.3">ğğŒğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.12.m1.1c">\mathbf{Pred_{NMN}}</annotation></semantics></math>, <math id="S3.SS2.p2.13.m2.1" class="ltx_Math" alttext="\mathbf{m_{Q}}" display="inline"><semantics id="S3.SS2.p2.13.m2.1a"><msub id="S3.SS2.p2.13.m2.1.1" xref="S3.SS2.p2.13.m2.1.1.cmml"><mi id="S3.SS2.p2.13.m2.1.1.2" xref="S3.SS2.p2.13.m2.1.1.2.cmml">ğ¦</mi><mi id="S3.SS2.p2.13.m2.1.1.3" xref="S3.SS2.p2.13.m2.1.1.3.cmml">ğ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.13.m2.1b"><apply id="S3.SS2.p2.13.m2.1.1.cmml" xref="S3.SS2.p2.13.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.13.m2.1.1.1.cmml" xref="S3.SS2.p2.13.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.13.m2.1.1.2.cmml" xref="S3.SS2.p2.13.m2.1.1.2">ğ¦</ci><ci id="S3.SS2.p2.13.m2.1.1.3.cmml" xref="S3.SS2.p2.13.m2.1.1.3">ğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.13.m2.1c">\mathbf{m_{Q}}</annotation></semantics></math>, and the caption attention context vector <math id="S3.SS2.p2.14.m3.1" class="ltx_Math" alttext="\mathbf{\hat{C}}" display="inline"><semantics id="S3.SS2.p2.14.m3.1a"><mover accent="true" id="S3.SS2.p2.14.m3.1.1" xref="S3.SS2.p2.14.m3.1.1.cmml"><mi id="S3.SS2.p2.14.m3.1.1.2" xref="S3.SS2.p2.14.m3.1.1.2.cmml">ğ‚</mi><mo id="S3.SS2.p2.14.m3.1.1.1" xref="S3.SS2.p2.14.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.14.m3.1b"><apply id="S3.SS2.p2.14.m3.1.1.cmml" xref="S3.SS2.p2.14.m3.1.1"><ci id="S3.SS2.p2.14.m3.1.1.1.cmml" xref="S3.SS2.p2.14.m3.1.1.1">^</ci><ci id="S3.SS2.p2.14.m3.1.1.2.cmml" xref="S3.SS2.p2.14.m3.1.1.2">ğ‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.14.m3.1c">\mathbf{\hat{C}}</annotation></semantics></math> are added element wise, followed by ReLU nonlinearity and a fully connected layer (with <math id="S3.SS2.p2.15.m4.1" class="ltx_Math" alttext="\mathbf{W}" display="inline"><semantics id="S3.SS2.p2.15.m4.1a"><mi id="S3.SS2.p2.15.m4.1.1" xref="S3.SS2.p2.15.m4.1.1.cmml">ğ–</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.15.m4.1b"><ci id="S3.SS2.p2.15.m4.1.1.cmml" xref="S3.SS2.p2.15.m4.1.1">ğ–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.15.m4.1c">\mathbf{W}</annotation></semantics></math> as weight parameter) over which we obtain the probability distribution in the answer space using a <math id="S3.SS2.p2.16.m5.1" class="ltx_Math" alttext="\mathit{softmax}" display="inline"><semantics id="S3.SS2.p2.16.m5.1a"><mi id="S3.SS2.p2.16.m5.1.1" xref="S3.SS2.p2.16.m5.1.1.cmml">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.16.m5.1b"><ci id="S3.SS2.p2.16.m5.1.1.cmml" xref="S3.SS2.p2.16.m5.1.1">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.16.m5.1c">\mathit{softmax}</annotation></semantics></math> function, as before.</p>
<table id="S6.EGx6" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.Ex7"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex7.m1.1" class="ltx_Math" alttext="\displaystyle\mathbf{Pred_{V}}" display="inline"><semantics id="S3.Ex7.m1.1a"><msub id="S3.Ex7.m1.1.1" xref="S3.Ex7.m1.1.1.cmml"><mi id="S3.Ex7.m1.1.1.2" xref="S3.Ex7.m1.1.1.2.cmml">ğğ«ğğ</mi><mi id="S3.Ex7.m1.1.1.3" xref="S3.Ex7.m1.1.1.3.cmml">ğ•</mi></msub><annotation-xml encoding="MathML-Content" id="S3.Ex7.m1.1b"><apply id="S3.Ex7.m1.1.1.cmml" xref="S3.Ex7.m1.1.1"><csymbol cd="ambiguous" id="S3.Ex7.m1.1.1.1.cmml" xref="S3.Ex7.m1.1.1">subscript</csymbol><ci id="S3.Ex7.m1.1.1.2.cmml" xref="S3.Ex7.m1.1.1.2">ğğ«ğğ</ci><ci id="S3.Ex7.m1.1.1.3.cmml" xref="S3.Ex7.m1.1.1.3">ğ•</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex7.m1.1c">\displaystyle\mathbf{Pred_{V}}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex7.m2.3" class="ltx_Math" alttext="\displaystyle=\mathit{softmax}(\mathbf{W}*\max(0,(\mathbf{Pred_{NMN}}\oplus\mathbf{m_{Q}})\oplus\mathbf{\hat{C}}))" display="inline"><semantics id="S3.Ex7.m2.3a"><mrow id="S3.Ex7.m2.3.3" xref="S3.Ex7.m2.3.3.cmml"><mi id="S3.Ex7.m2.3.3.3" xref="S3.Ex7.m2.3.3.3.cmml"></mi><mo id="S3.Ex7.m2.3.3.2" xref="S3.Ex7.m2.3.3.2.cmml">=</mo><mrow id="S3.Ex7.m2.3.3.1" xref="S3.Ex7.m2.3.3.1.cmml"><mi id="S3.Ex7.m2.3.3.1.3" xref="S3.Ex7.m2.3.3.1.3.cmml">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</mi><mo lspace="0em" rspace="0em" id="S3.Ex7.m2.3.3.1.2" xref="S3.Ex7.m2.3.3.1.2.cmml">â€‹</mo><mrow id="S3.Ex7.m2.3.3.1.1.1" xref="S3.Ex7.m2.3.3.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex7.m2.3.3.1.1.1.2" xref="S3.Ex7.m2.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.Ex7.m2.3.3.1.1.1.1" xref="S3.Ex7.m2.3.3.1.1.1.1.cmml"><mi id="S3.Ex7.m2.3.3.1.1.1.1.3" xref="S3.Ex7.m2.3.3.1.1.1.1.3.cmml">ğ–</mi><mo lspace="0.222em" rspace="0.222em" id="S3.Ex7.m2.3.3.1.1.1.1.2" xref="S3.Ex7.m2.3.3.1.1.1.1.2.cmml">âˆ—</mo><mrow id="S3.Ex7.m2.3.3.1.1.1.1.1.1" xref="S3.Ex7.m2.3.3.1.1.1.1.1.2.cmml"><mi id="S3.Ex7.m2.1.1" xref="S3.Ex7.m2.1.1.cmml">max</mi><mo id="S3.Ex7.m2.3.3.1.1.1.1.1.1a" xref="S3.Ex7.m2.3.3.1.1.1.1.1.2.cmml">â¡</mo><mrow id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1" xref="S3.Ex7.m2.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.2" xref="S3.Ex7.m2.3.3.1.1.1.1.1.2.cmml">(</mo><mn id="S3.Ex7.m2.2.2" xref="S3.Ex7.m2.2.2.cmml">0</mn><mo id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.3" xref="S3.Ex7.m2.3.3.1.1.1.1.1.2.cmml">,</mo><mrow id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">ğğ«ğğ</mi><mi id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">ğğŒğ</mi></msub><mo id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml">âŠ•</mo><msub id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">ğ¦</mi><mi id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">ğ</mi></msub></mrow><mo stretchy="false" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.2" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.2.cmml">âŠ•</mo><mover accent="true" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.2" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.2.cmml">ğ‚</mi><mo id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.1" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.1.cmml">^</mo></mover></mrow><mo stretchy="false" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.4" xref="S3.Ex7.m2.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo stretchy="false" id="S3.Ex7.m2.3.3.1.1.1.3" xref="S3.Ex7.m2.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex7.m2.3b"><apply id="S3.Ex7.m2.3.3.cmml" xref="S3.Ex7.m2.3.3"><eq id="S3.Ex7.m2.3.3.2.cmml" xref="S3.Ex7.m2.3.3.2"></eq><csymbol cd="latexml" id="S3.Ex7.m2.3.3.3.cmml" xref="S3.Ex7.m2.3.3.3">absent</csymbol><apply id="S3.Ex7.m2.3.3.1.cmml" xref="S3.Ex7.m2.3.3.1"><times id="S3.Ex7.m2.3.3.1.2.cmml" xref="S3.Ex7.m2.3.3.1.2"></times><ci id="S3.Ex7.m2.3.3.1.3.cmml" xref="S3.Ex7.m2.3.3.1.3">ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥</ci><apply id="S3.Ex7.m2.3.3.1.1.1.1.cmml" xref="S3.Ex7.m2.3.3.1.1.1"><times id="S3.Ex7.m2.3.3.1.1.1.1.2.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.2"></times><ci id="S3.Ex7.m2.3.3.1.1.1.1.3.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.3">ğ–</ci><apply id="S3.Ex7.m2.3.3.1.1.1.1.1.2.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1"><max id="S3.Ex7.m2.1.1.cmml" xref="S3.Ex7.m2.1.1"></max><cn type="integer" id="S3.Ex7.m2.2.2.cmml" xref="S3.Ex7.m2.2.2">0</cn><apply id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.2">direct-sum</csymbol><apply id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.1">direct-sum</csymbol><apply id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.2">ğğ«ğğ</ci><ci id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.2.3">ğğŒğ</ci></apply><apply id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ¦</ci><ci id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.1.1.1.3.3">ğ</ci></apply></apply><apply id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3"><ci id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.1">^</ci><ci id="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex7.m2.3.3.1.1.1.1.1.1.1.1.3.2">ğ‚</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex7.m2.3c">\displaystyle=\mathit{softmax}(\mathbf{W}*\max(0,(\mathbf{Pred_{NMN}}\oplus\mathbf{m_{Q}})\oplus\mathbf{\hat{C}}))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.Ex8"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex8.m1.1" class="ltx_Math" alttext="\displaystyle\mathit{answer}" display="inline"><semantics id="S3.Ex8.m1.1a"><mi id="S3.Ex8.m1.1.1" xref="S3.Ex8.m1.1.1.cmml">ğ‘ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.Ex8.m1.1b"><ci id="S3.Ex8.m1.1.1.cmml" xref="S3.Ex8.m1.1.1">ğ‘ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex8.m1.1c">\displaystyle\mathit{answer}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex8.m2.1" class="ltx_Math" alttext="\displaystyle=\operatorname{arg\,max}_{i}\hskip 5.69046pt\mathbf{Pred_{V}}[i]" display="inline"><semantics id="S3.Ex8.m2.1a"><mrow id="S3.Ex8.m2.1.2" xref="S3.Ex8.m2.1.2.cmml"><mi id="S3.Ex8.m2.1.2.2" xref="S3.Ex8.m2.1.2.2.cmml"></mi><mo id="S3.Ex8.m2.1.2.1" xref="S3.Ex8.m2.1.2.1.cmml">=</mo><mrow id="S3.Ex8.m2.1.2.3" xref="S3.Ex8.m2.1.2.3.cmml"><mrow id="S3.Ex8.m2.1.2.3.2" xref="S3.Ex8.m2.1.2.3.2.cmml"><msub id="S3.Ex8.m2.1.2.3.2.1" xref="S3.Ex8.m2.1.2.3.2.1.cmml"><mrow id="S3.Ex8.m2.1.2.3.2.1.2" xref="S3.Ex8.m2.1.2.3.2.1.2.cmml"><mi id="S3.Ex8.m2.1.2.3.2.1.2.2" xref="S3.Ex8.m2.1.2.3.2.1.2.2.cmml">arg</mi><mo lspace="0.170em" rspace="0em" id="S3.Ex8.m2.1.2.3.2.1.2.1" xref="S3.Ex8.m2.1.2.3.2.1.2.1.cmml">â€‹</mo><mi id="S3.Ex8.m2.1.2.3.2.1.2.3" xref="S3.Ex8.m2.1.2.3.2.1.2.3.cmml">max</mi></mrow><mi id="S3.Ex8.m2.1.2.3.2.1.3" xref="S3.Ex8.m2.1.2.3.2.1.3.cmml">i</mi></msub><mo id="S3.Ex8.m2.1.2.3.2a" xref="S3.Ex8.m2.1.2.3.2.cmml">â¡</mo><msub id="S3.Ex8.m2.1.2.3.2.2" xref="S3.Ex8.m2.1.2.3.2.2.cmml"><mi id="S3.Ex8.m2.1.2.3.2.2.2" xref="S3.Ex8.m2.1.2.3.2.2.2.cmml">ğğ«ğğ</mi><mi id="S3.Ex8.m2.1.2.3.2.2.3" xref="S3.Ex8.m2.1.2.3.2.2.3.cmml">ğ•</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.Ex8.m2.1.2.3.1" xref="S3.Ex8.m2.1.2.3.1.cmml">â€‹</mo><mrow id="S3.Ex8.m2.1.2.3.3.2" xref="S3.Ex8.m2.1.2.3.3.1.cmml"><mo stretchy="false" id="S3.Ex8.m2.1.2.3.3.2.1" xref="S3.Ex8.m2.1.2.3.3.1.1.cmml">[</mo><mi id="S3.Ex8.m2.1.1" xref="S3.Ex8.m2.1.1.cmml">i</mi><mo stretchy="false" id="S3.Ex8.m2.1.2.3.3.2.2" xref="S3.Ex8.m2.1.2.3.3.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex8.m2.1b"><apply id="S3.Ex8.m2.1.2.cmml" xref="S3.Ex8.m2.1.2"><eq id="S3.Ex8.m2.1.2.1.cmml" xref="S3.Ex8.m2.1.2.1"></eq><csymbol cd="latexml" id="S3.Ex8.m2.1.2.2.cmml" xref="S3.Ex8.m2.1.2.2">absent</csymbol><apply id="S3.Ex8.m2.1.2.3.cmml" xref="S3.Ex8.m2.1.2.3"><times id="S3.Ex8.m2.1.2.3.1.cmml" xref="S3.Ex8.m2.1.2.3.1"></times><apply id="S3.Ex8.m2.1.2.3.2.cmml" xref="S3.Ex8.m2.1.2.3.2"><apply id="S3.Ex8.m2.1.2.3.2.1.cmml" xref="S3.Ex8.m2.1.2.3.2.1"><csymbol cd="ambiguous" id="S3.Ex8.m2.1.2.3.2.1.1.cmml" xref="S3.Ex8.m2.1.2.3.2.1">subscript</csymbol><apply id="S3.Ex8.m2.1.2.3.2.1.2.cmml" xref="S3.Ex8.m2.1.2.3.2.1.2"><times id="S3.Ex8.m2.1.2.3.2.1.2.1.cmml" xref="S3.Ex8.m2.1.2.3.2.1.2.1"></times><ci id="S3.Ex8.m2.1.2.3.2.1.2.2.cmml" xref="S3.Ex8.m2.1.2.3.2.1.2.2">arg</ci><ci id="S3.Ex8.m2.1.2.3.2.1.2.3.cmml" xref="S3.Ex8.m2.1.2.3.2.1.2.3">max</ci></apply><ci id="S3.Ex8.m2.1.2.3.2.1.3.cmml" xref="S3.Ex8.m2.1.2.3.2.1.3">ğ‘–</ci></apply><apply id="S3.Ex8.m2.1.2.3.2.2.cmml" xref="S3.Ex8.m2.1.2.3.2.2"><csymbol cd="ambiguous" id="S3.Ex8.m2.1.2.3.2.2.1.cmml" xref="S3.Ex8.m2.1.2.3.2.2">subscript</csymbol><ci id="S3.Ex8.m2.1.2.3.2.2.2.cmml" xref="S3.Ex8.m2.1.2.3.2.2.2">ğğ«ğğ</ci><ci id="S3.Ex8.m2.1.2.3.2.2.3.cmml" xref="S3.Ex8.m2.1.2.3.2.2.3">ğ•</ci></apply></apply><apply id="S3.Ex8.m2.1.2.3.3.1.cmml" xref="S3.Ex8.m2.1.2.3.3.2"><csymbol cd="latexml" id="S3.Ex8.m2.1.2.3.3.1.1.cmml" xref="S3.Ex8.m2.1.2.3.3.2.1">delimited-[]</csymbol><ci id="S3.Ex8.m2.1.1.cmml" xref="S3.Ex8.m2.1.1">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex8.m2.1c">\displaystyle=\operatorname{arg\,max}_{i}\hskip 5.69046pt\mathbf{Pred_{V}}[i]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>External Knowledge Sources</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">A fraction of the questions in the dataset seem to call for some external general knowledge. Two examples are given in figure <a href="#S3.F2" title="Figure 2 â€£ 3.3 External Knowledge Sources â€£ 3 Proposed Approach â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. In <a href="#S3.F2.sf1" title="In Figure 2 â€£ 3.3 External Knowledge Sources â€£ 3 Proposed Approach â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>, the top answer (â€œmeatâ€) is present in the Wikipedia abstract of the article titled â€œPizzaâ€. In <a href="#S3.F2.sf2" title="In Figure 2 â€£ 3.3 External Knowledge Sources â€£ 3 Proposed Approach â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>, one can easily argue that the image is not necessary to answer the question, while access to information about what a helmet is useful for giving a direct answer. While it is difficult to objectively evaluate how many questions can benefit from the addition of an external knowledge source, a cursory look at 100 images from the VQA 1.0 dataset suggests that about one image in every fifteen would be helped by the addition of general knowledge.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S3.F2.1" class="ltx_p ltx_figure_panel"><span id="S3.F2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S3.F2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:439.3pt;height:348pt;vertical-align:-348.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S3.F2.1.1.1.1" class="ltx_p"><span id="S3.F2.1.1.1.1.1" class="ltx_text">
Â  Â Â Â Â Â Â Â Â Â 

</span></span>
</span></span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel">
<table id="S3.F2.sf1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F2.sf1.1.1" class="ltx_tr">
<td id="S3.F2.sf1.1.1.1" class="ltx_td ltx_align_left">
<span id="S3.F2.sf1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:198.7pt;"><img src="/html/1809.08697/assets/pizza.png" id="S3.F2.sf1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="548" height="411" alt="Refer to caption">
</span>
</td>
<td id="S3.F2.sf1.1.1.2" class="ltx_td ltx_align_right">
<span id="S3.F2.sf1.1.1.2.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:198.7pt;">
<span id="S3.F2.sf1.1.1.2.1.1" class="ltx_p"><span id="S3.F2.sf1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Question</span>: What toppings are on the pizza?</span>
<span id="S3.F2.sf1.1.1.2.1.2" class="ltx_p"><span id="S3.F2.sf1.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Answers</span>: meat; onion; peppers</span>
<span id="S3.F2.sf1.1.1.2.1.3" class="ltx_p"><span id="S3.F2.sf1.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Excerpt from Wikipedia article â€œpizzaâ€</span>: It is commonly topped with a selection of meats, vegetables and condiments.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel">
<table id="S3.F2.sf2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F2.sf2.1.1" class="ltx_tr">
<td id="S3.F2.sf2.1.1.1" class="ltx_td ltx_align_left">
<span id="S3.F2.sf2.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:198.7pt;"><img src="/html/1809.08697/assets/helmet.png" id="S3.F2.sf2.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="548" height="411" alt="Refer to caption">
</span>
</td>
<td id="S3.F2.sf2.1.1.2" class="ltx_td ltx_align_right">
<span id="S3.F2.sf2.1.1.2.1" class="ltx_inline-block ltx_minipage ltx_align_middle" style="width:198.7pt;">
<span id="S3.F2.sf2.1.1.2.1.1" class="ltx_p"><span id="S3.F2.sf2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Question</span>: Why are the people wearing helmets?</span>
<span id="S3.F2.sf2.1.1.2.1.2" class="ltx_p"><span id="S3.F2.sf2.1.1.2.1.2.1" class="ltx_text ltx_font_bold">Answers (top 3)</span>: so they donâ€™t injure their heads; for protection; safety</span>
<span id="S3.F2.sf2.1.1.2.1.3" class="ltx_p"><span id="S3.F2.sf2.1.1.2.1.3.1" class="ltx_text ltx_font_bold">Excerpt from Wikipedia article â€œhelmetâ€</span>: A helmet is a form of protective gear worn to protect the head from injuries.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Examples of images that could be helped by the addition of an external knowledge source</figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p">To alleviate this problem, we add support for external knowledge bases in NMN. As suggested by <cite class="ltx_cite ltx_citemacro_citet">Wu etÂ al., (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite>, we use information extracted from our knowledge base as the seed for the hidden state of the LSTM that will parse the question. While it is known that LSTMs tend to forget their initial state if their input is too long (<cite class="ltx_cite ltx_citemacro_cite">Neubig, (<a href="#bib.bib21" title="" class="ltx_ref">2017</a>)</cite> for instance), we do not believe this to be an issue here since questions are generally under ten words in length (3% of all questions exceed this length).</p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<p id="S3.SS3.p3.1" class="ltx_p">The novelty of our approach essentially resides in the selection and the preprocessing of the knowledge base. We use the DBPedia collection of English Wikipedia abstracts <cite class="ltx_cite ltx_citemacro_citep">(Auer etÂ al.,, <a href="#bib.bib5" title="" class="ltx_ref">2007</a>)</cite> as our knowledge base. Articles are filtered to keep only those that correspond to proper or common nouns, in order to remove the many observed false matches on movie or song titles. This allows us to match articles in the content of the abstract, instead of the title or meta-information only, potentially giving access to more information if no direct matches are found, as well as being able to take in more information, since Wikipedia abstracts are more detailed than the DBPedia ontology elements used by <cite class="ltx_cite ltx_citemacro_citet">Wu etÂ al., (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite>. A potential drawback of this approach is that longer abstracts may not be harder to exploit as expected if they contain more information that is irrelevant to the question. Essentially, our hypothesis for this part is that giving the model access to more external information should be beneficial. The entire collection of abstracts is indexed in an Apache Lucene<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://lucene.apache.org/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lucene.apache.org/</a></span></span></span> database. Each question is then parsed to select only nouns using NLTK (Natural Language Toolkit) <cite class="ltx_cite ltx_citemacro_citep">(Loper and Bird,, <a href="#bib.bib17" title="" class="ltx_ref">2002</a>)</cite>, and a query is run against the database using the nouns extracted from the question. All selected articles are then turned into 300-dimensional vectors using Doc2Vec <cite class="ltx_cite ltx_citemacro_citep">(Le and Mikolov,, <a href="#bib.bib14" title="" class="ltx_ref">2014</a>)</cite>. We use a pre-trained model from <cite class="ltx_cite ltx_citemacro_citet">Lau and Baldwin, (<a href="#bib.bib13" title="" class="ltx_ref">2016</a>)</cite> for this task.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>

<div id="S4.p1" class="ltx_para ltx_noindent">
<p id="S4.p1.1" class="ltx_p">We evaluate the performance of our model on the VQA 1.0 dataset <cite class="ltx_cite ltx_citemacro_citep">(Antol etÂ al.,, <a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite>. This dataset is widely used for VQA tasks, comprising of around 200,000 images from
MSCOCO <cite class="ltx_cite ltx_citemacro_cite">Lin etÂ al., (<a href="#bib.bib16" title="" class="ltx_ref">2014</a>)</cite>. Each of these images is associated with three different questions along with ten answers to each of these questions which were generated by human annotators. We train our model using the standard train/val/test split. The <span id="S4.p1.1.1" class="ltx_text ltx_font_typewriter">conv5</span> layer after performing max pooling, from a 16-layer VGGNet (a deep CNN, <cite class="ltx_cite ltx_citemacro_cite">Simonyan and Zisserman, (<a href="#bib.bib25" title="" class="ltx_ref">2014</a>)</cite>) is used generate the visual features, that are normalized to have a mean of 0 and a standard deviation of 1, used by NMN.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.2" class="ltx_p">We use ApolloCaffe <cite class="ltx_cite ltx_citemacro_citep">(Jia etÂ al.,, <a href="#bib.bib11" title="" class="ltx_ref">2014</a>)</cite> to develop our model. Since we treat the problem as a classification problem, categorical cross entropy is used as a loss function. We use the ADADELTA optimizer with standard parameter settings <cite class="ltx_cite ltx_citemacro_citep">(Zeiler,, <a href="#bib.bib28" title="" class="ltx_ref">2012</a>)</cite>. We set a batch size to be 100 and train up to 12 epochs with early stopping if the validation accuracy has not improved. In Caption Attention Model, the hidden layer size of <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{W_{Q}}" display="inline"><semantics id="S4.p2.1.m1.1a"><msub id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">ğ–</mi><mi id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">ğ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">ğ–</ci><ci id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">ğ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\mathbf{W_{Q}}</annotation></semantics></math> and <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{W_{C}}" display="inline"><semantics id="S4.p2.2.m2.1a"><msub id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">ğ–</mi><mi id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">ğ‚</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">ğ–</ci><ci id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">ğ‚</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\mathbf{W_{C}}</annotation></semantics></math> is set to 200. The question context vector is generated through a single layer LSTM of 1000 hidden units and the caption context vector in Caption Attention Model is generated through a single layer LSTM of 500 hidden units.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">The two baselines we explore are hierarchical co-attention models <cite class="ltx_cite ltx_citemacro_citep">(Lu etÂ al.,, <a href="#bib.bib18" title="" class="ltx_ref">2016</a>)</cite> and NMNs <cite class="ltx_cite ltx_citemacro_citep">(<a href="#bib.bib2" title="" class="ltx_ref">Andreas etÂ al., 2016b, </a>)</cite>. Table <a href="#S5.T2" title="Table 2 â€£ 5 Results and Discussion â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the results that are replicated by training these models with train+val as the training set.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<p id="S5.T2.1" class="ltx_p"><span id="S5.T2.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S5.T2.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:389.1pt;height:72pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.T2.1.1.1.1" class="ltx_p"><span id="S5.T2.1.1.1.1.1" class="ltx_text">
<span id="S5.T2.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T2.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T2.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Models</span>
<span id="S5.T2.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Yes/No</span>
<span id="S5.T2.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Number</span>
<span id="S5.T2.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Other</span>
<span id="S5.T2.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Overall</span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T2.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S5.T2.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Hierarchical Question-Image Co-Attention</span>
<span id="S5.T2.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">79.6</span>
<span id="S5.T2.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">38.4</span>
<span id="S5.T2.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">49.1</span>
<span id="S5.T2.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">60.5</span></span>
<span id="S5.T2.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="S5.T2.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_left">Neural Module Networks</span>
<span id="S5.T2.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center">80.8</span>
<span id="S5.T2.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center">36.1</span>
<span id="S5.T2.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center">43.7</span>
<span id="S5.T2.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_center">58.1</span></span>
<span id="S5.T2.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="S5.T2.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_border_b">Neural Module Networks (Reported in paper)</span>
<span id="S5.T2.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">81.2</span>
<span id="S5.T2.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b">35.2</span>
<span id="S5.T2.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b">43.3</span>
<span id="S5.T2.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b">58.0</span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of baseline models on test-dev open-ended questions. The published implementation of NMN2 performs somewhat better than reported in the paper, and both do worse than Hierarchical Co-Attention.</figcaption>
</figure>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">The major focus of our work lies in improving the NMNs by combining some advantages from the attention model. Henceforth, we present the results of our system obtained by improving the baseline of NMNs. The reason for this choice though it is not the state of the art model, is the intuitive and interesting combination of dependency trees to dynamically adapt the neural networks by assembling different neural modules.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Experimental results</h3>

<figure id="S5.T3" class="ltx_table">
<p id="S5.T3.1" class="ltx_p"><span id="S5.T3.1.1" class="ltx_text ltx_inline-block" style="width:433.6pt;">
<span id="S5.T3.1.1.1" class="ltx_inline-block ltx_transformed_outer" style="width:476.6pt;height:109pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span id="S5.T3.1.1.1.1" class="ltx_p"><span id="S5.T3.1.1.1.1.1" class="ltx_text">
<span id="S5.T3.1.1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_thead">
<span id="S5.T3.1.1.1.1.1.1.1.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t">Model Name</span>
<span id="S5.T3.1.1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Train Acc</span>
<span id="S5.T3.1.1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Validation Acc</span>
<span id="S5.T3.1.1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Yes/No</span>
<span id="S5.T3.1.1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Number</span>
<span id="S5.T3.1.1.1.1.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Other</span>
<span id="S5.T3.1.1.1.1.1.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Overall</span></span>
</span>
<span class="ltx_tbody">
<span id="S5.T3.1.1.1.1.1.1.2.1" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Neural Module Networks (NMN)</span>
<span id="S5.T3.1.1.1.1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">60.0</span>
<span id="S5.T3.1.1.1.1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">55.2</span>
<span id="S5.T3.1.1.1.1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">79.0</span>
<span id="S5.T3.1.1.1.1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">37.5</span>
<span id="S5.T3.1.1.1.1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">42.4</span>
<span id="S5.T3.1.1.1.1.1.1.2.1.7" class="ltx_td ltx_align_center ltx_border_t">56.9</span></span>
<span id="S5.T3.1.1.1.1.1.1.3.2" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.3.2.1" class="ltx_td ltx_align_left">Caption Attention Only</span>
<span id="S5.T3.1.1.1.1.1.1.3.2.2" class="ltx_td ltx_align_center">50.0</span>
<span id="S5.T3.1.1.1.1.1.1.3.2.3" class="ltx_td ltx_align_center">48.2</span>
<span id="S5.T3.1.1.1.1.1.1.3.2.4" class="ltx_td ltx_align_center">78.4</span>
<span id="S5.T3.1.1.1.1.1.1.3.2.5" class="ltx_td ltx_align_center">36.5</span>
<span id="S5.T3.1.1.1.1.1.1.3.2.6" class="ltx_td ltx_align_center">27.8</span>
<span id="S5.T3.1.1.1.1.1.1.3.2.7" class="ltx_td ltx_align_center">49.5</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.3" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.4.3.1" class="ltx_td ltx_align_left"><span id="S5.T3.1.1.1.1.1.1.4.3.1.1" class="ltx_text ltx_font_bold">NMN + Caption Information</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.3.2.1" class="ltx_text ltx_font_bold">61.9</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.3.3" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.3.3.1" class="ltx_text ltx_font_bold">56.4</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.3.4.1" class="ltx_text ltx_font_bold">79.8</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.3.5" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.3.5.1" class="ltx_text ltx_font_bold">37.4</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.3.6" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.3.6.1" class="ltx_text ltx_font_bold">42.1</span></span>
<span id="S5.T3.1.1.1.1.1.1.4.3.7" class="ltx_td ltx_align_center"><span id="S5.T3.1.1.1.1.1.1.4.3.7.1" class="ltx_text ltx_font_bold">57.1</span></span></span>
<span id="S5.T3.1.1.1.1.1.1.5.4" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.5.4.1" class="ltx_td ltx_align_left">NMN + Caption Attention</span>
<span id="S5.T3.1.1.1.1.1.1.5.4.2" class="ltx_td ltx_align_center">60.3</span>
<span id="S5.T3.1.1.1.1.1.1.5.4.3" class="ltx_td ltx_align_center">55.2</span>
<span id="S5.T3.1.1.1.1.1.1.5.4.4" class="ltx_td ltx_align_center">79.2</span>
<span id="S5.T3.1.1.1.1.1.1.5.4.5" class="ltx_td ltx_align_center">35.8</span>
<span id="S5.T3.1.1.1.1.1.1.5.4.6" class="ltx_td ltx_align_center">42.1</span>
<span id="S5.T3.1.1.1.1.1.1.5.4.7" class="ltx_td ltx_align_center">56.6</span></span>
<span id="S5.T3.1.1.1.1.1.1.6.5" class="ltx_tr">
<span id="S5.T3.1.1.1.1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_border_b">NMN + External Knowledge Source</span>
<span id="S5.T3.1.1.1.1.1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b">61.3</span>
<span id="S5.T3.1.1.1.1.1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b">-</span>
<span id="S5.T3.1.1.1.1.1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b">79.2</span>
<span id="S5.T3.1.1.1.1.1.1.6.5.5" class="ltx_td ltx_align_center ltx_border_b">36.4</span>
<span id="S5.T3.1.1.1.1.1.1.6.5.6" class="ltx_td ltx_align_center ltx_border_b">42.2</span>
<span id="S5.T3.1.1.1.1.1.1.6.5.7" class="ltx_td ltx_align_center ltx_border_b">56.8</span></span>
</span>
</span></span></span>
</span></span></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of our different models and the baseline on test-dev open-ended questions</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p id="S5.SS1.p1.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 â€£ 5.1 Experimental results â€£ 5 Results and Discussion â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> uses train set whereas table <a href="#S5.T4" title="Table 4 â€£ 5.1 Experimental results â€£ 5 Results and Discussion â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> uses train+val as the training set.
As we observe in table <a href="#S5.T3" title="Table 3 â€£ 5.1 Experimental results â€£ 5 Results and Discussion â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, the train and test accuracy in the second row, which corresponds to the experiment by removing the output from the NMN completely and just relying on the attentions from the captions is about 50%. This indicates that the model is able to learn to predict some answers from the captions to the images.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p">Adding caption information increases the train and test accuracy by 1.6 percentage points and 1.2 points respectively. However, our caption attention model degrades NMN performance. One possible reason for this could be that we are forcing the model to attend to some words in the caption in cases where the caption is not relevant to the question being asked.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.1" class="ltx_p">To combat information loss when the question parse is simplified into a network layout, we have experimented with using larger parse trees. For example consider the question, â€˜What is this person playing?â€™. The shorter version of the parse that baseline NMN uses is â€˜<span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_italic">Describe(Find(Person))</span>. This parse does not include any information about â€˜playingâ€™. Hence we consider tree from longer parse <span id="S5.SS1.p3.1.2" class="ltx_text ltx_font_italic">Describe(And(Find(Person), Find(Playing)))</span>. In addition, we implement a <span id="S5.SS1.p3.1.3" class="ltx_text ltx_font_bold ltx_font_italic">measure</span> module to address counting type questions specifically<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>The <span id="footnote3.1" class="ltx_text ltx_font_italic">measure</span> module is described in <cite class="ltx_cite ltx_citemacro_cite"><a href="#bib.bib2" title="" class="ltx_ref">Andreas etÂ al., 2016b </a></cite> but is not implemented in the published code. We implement this module as outlined in the paper; it is very similar to the counting system in <cite class="ltx_cite ltx_citemacro_citet">SeguÃ­ etÂ al., (<a href="#bib.bib23" title="" class="ltx_ref">2015</a>)</cite></span></span></span>.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">Model Name</th>
<th id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Train Acc</th>
<th id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Yes/No</th>
<th id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Number</th>
<th id="S5.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Other</th>
<th id="S5.T4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">Overall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.1.2.1" class="ltx_tr">
<th id="S5.T4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Neural Module Networks (NMN)</th>
<td id="S5.T4.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">62.9</td>
<td id="S5.T4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">80.8</td>
<td id="S5.T4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">36.1</td>
<td id="S5.T4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">43.7</td>
<td id="S5.T4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">58.1</td>
</tr>
<tr id="S5.T4.1.3.2" class="ltx_tr">
<th id="S5.T4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Longest Parse</th>
<td id="S5.T4.1.3.2.2" class="ltx_td ltx_align_center">61.4</td>
<td id="S5.T4.1.3.2.3" class="ltx_td ltx_align_center">79.7</td>
<td id="S5.T4.1.3.2.4" class="ltx_td ltx_align_center">35.8</td>
<td id="S5.T4.1.3.2.5" class="ltx_td ltx_align_center">42.3</td>
<td id="S5.T4.1.3.2.6" class="ltx_td ltx_align_center">57.0</td>
</tr>
<tr id="S5.T4.1.4.3" class="ltx_tr">
<th id="S5.T4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b">Measure Module</th>
<td id="S5.T4.1.4.3.2" class="ltx_td ltx_align_center ltx_border_b">64.0</td>
<td id="S5.T4.1.4.3.3" class="ltx_td ltx_align_center ltx_border_b">79.6</td>
<td id="S5.T4.1.4.3.4" class="ltx_td ltx_align_center ltx_border_b">34.8</td>
<td id="S5.T4.1.4.3.5" class="ltx_td ltx_align_center ltx_border_b">41.7</td>
<td id="S5.T4.1.4.3.6" class="ltx_td ltx_align_center ltx_border_b">56.5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of results using some simple tweaks to the NMN model. All of our changes lead to worse results, so we do not use them for our other experiments.</figcaption>
</figure>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p">As we can see from table <a href="#S5.T4" title="Table 4 â€£ 5.1 Experimental results â€£ 5 Results and Discussion â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, performing less simplification on the parse tree decreases the scores. This could be due to a higher chance of error in complex parses, which are consequently not mapped to appropriate neural modules.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para ltx_noindent">
<p id="S5.SS1.p5.1" class="ltx_p">Adding a <span id="S5.SS1.p5.1.1" class="ltx_text ltx_font_italic">measure</span> module at the top of the parse tree instead of a <span id="S5.SS1.p5.1.2" class="ltx_text ltx_font_italic">describe</span> module for counting questions (those that start with â€œhow manyâ€) does not improve the results either. This could be due to the errors in the attention maps from the <span id="S5.SS1.p5.1.3" class="ltx_text ltx_font_italic">find</span> module, or to the fact that while the <span id="S5.SS1.p5.1.4" class="ltx_text ltx_font_italic">describe</span> module takes both image and attention maps as inputs, the <span id="S5.SS1.p5.1.5" class="ltx_text ltx_font_italic">measure</span> module only takes the attention maps, so some information from image data might have been lost.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparative Qualitative Analysis</h3>

<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p id="S5.SS2.p1.1" class="ltx_p">To analyze the performance of our approach with respect to the baseline of NMNs, we randomly select some images from the validation set and compare the answers produced by our system with correct answers and those produced with NMNs. We present this analysis in table <a href="#S5.T5" title="Table 5 â€£ 5.2 Comparative Qualitative Analysis â€£ 5 Results and Discussion â€£ Textually Enriched Neural Module Networks for Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The notations â€˜Qâ€™ and â€˜Câ€™ represent corresponding questions and captions. â€œPredicted Answer NMNâ€ is the output from the NMN model and â€œPredicted Answer NMN+CAâ€ is the answer predicted by our model after adding the attention on captions with respect to the question.</p>
</div>
<figure id="S5.T5" class="ltx_table">
<table id="S5.T5.6" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T5.3.3" class="ltx_tr">
<td id="S5.T5.1.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.1.1.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1809.08697/assets/COCO_val2014_000000246425.jpg" id="S5.T5.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="165" height="86" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S5.T5.2.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.2.2.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1809.08697/assets/COCO_val2014_000000089367.jpg" id="S5.T5.2.2.2.1.1.g1" class="ltx_graphics ltx_img_portrait" width="110" height="147" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S5.T5.3.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.3.3.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1809.08697/assets/COCO_val2014_000000080172.jpg" id="S5.T5.3.3.3.1.1.g1" class="ltx_graphics ltx_img_portrait" width="110" height="147" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S5.T5.6.7.1" class="ltx_tr">
<td id="S5.T5.6.7.1.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.7.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.7.1.1.1.1" class="ltx_p" style="width:113.8pt;">Q: How many planes are flying?</span>
</span>
</td>
<td id="S5.T5.6.7.1.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.7.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.7.1.2.1.1" class="ltx_p" style="width:113.8pt;">Q: What is the food called?</span>
</span>
</td>
<td id="S5.T5.6.7.1.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.7.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.7.1.3.1.1" class="ltx_p" style="width:113.8pt;">Q: What is the child holding?</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.8.2" class="ltx_tr">
<td id="S5.T5.6.8.2.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.8.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.8.2.1.1.1" class="ltx_p" style="width:113.8pt;">C: two planes flying in the sky in the sky</span>
</span>
</td>
<td id="S5.T5.6.8.2.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.8.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.8.2.2.1.1" class="ltx_p" style="width:113.8pt;">C: a little boy sitting at a table with a pizza</span>
</span>
</td>
<td id="S5.T5.6.8.2.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.8.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.8.2.3.1.1" class="ltx_p" style="width:113.8pt;">C: a young boy brushing his teeth in the bathroom</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.9.3" class="ltx_tr">
<td id="S5.T5.6.9.3.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.9.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.9.3.1.1.1" class="ltx_p" style="width:113.8pt;">Correct Answer: 2</span>
</span>
</td>
<td id="S5.T5.6.9.3.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.9.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.9.3.2.1.1" class="ltx_p" style="width:113.8pt;">Correct Answer: pizza</span>
</span>
</td>
<td id="S5.T5.6.9.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.9.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.9.3.3.1.1" class="ltx_p" style="width:113.8pt;">Correct Answer: toothbrush</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.10.4" class="ltx_tr">
<td id="S5.T5.6.10.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.10.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.10.4.1.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN: 4</span>
</span>
</td>
<td id="S5.T5.6.10.4.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.10.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.10.4.2.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN: pizza</span>
</span>
</td>
<td id="S5.T5.6.10.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.10.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.10.4.3.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN: phone</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.11.5" class="ltx_tr">
<td id="S5.T5.6.11.5.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.11.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.11.5.1.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN+CA: 2</span>
</span>
</td>
<td id="S5.T5.6.11.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.11.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.11.5.2.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN+CA: pizza</span>
</span>
</td>
<td id="S5.T5.6.11.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.11.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.11.5.3.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN+CA: toothbrush</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.12.6" class="ltx_tr">
<td id="S5.T5.6.12.6.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.12.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.12.6.1.1.1" class="ltx_p" style="width:113.8pt;">(a)</span>
</span>
</td>
<td id="S5.T5.6.12.6.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.12.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.12.6.2.1.1" class="ltx_p" style="width:113.8pt;">(b)</span>
</span>
</td>
<td id="S5.T5.6.12.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.12.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.12.6.3.1.1" class="ltx_p" style="width:113.8pt;">(c)</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.6" class="ltx_tr">
<td id="S5.T5.4.4.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.4.4.1.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1809.08697/assets/COCO_val2014_000000018480.jpg" id="S5.T5.4.4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="143" height="96" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S5.T5.5.5.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.5.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.5.5.2.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1809.08697/assets/COCO_val2014_000000351875.jpg" id="S5.T5.5.5.2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="143" height="96" alt="[Uncaptioned image]"></span>
</span>
</td>
<td id="S5.T5.6.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.6.3.1.1" class="ltx_p" style="width:113.8pt;"><img src="/html/1809.08697/assets/COCO_val2014_000000413247.jpg" id="S5.T5.6.6.3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="143" height="95" alt="[Uncaptioned image]"></span>
</span>
</td>
</tr>
<tr id="S5.T5.6.13.7" class="ltx_tr">
<td id="S5.T5.6.13.7.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.13.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.13.7.1.1.1" class="ltx_p" style="width:113.8pt;">Q: How many people are standing?</span>
</span>
</td>
<td id="S5.T5.6.13.7.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.13.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.13.7.2.1.1" class="ltx_p" style="width:113.8pt;">Q: What color is the sign?</span>
</span>
</td>
<td id="S5.T5.6.13.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.13.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.13.7.3.1.1" class="ltx_p" style="width:113.8pt;">Q: Is there a tree on the desk?</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.14.8" class="ltx_tr">
<td id="S5.T5.6.14.8.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.14.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.14.8.1.1.1" class="ltx_p" style="width:113.8pt;">C: a group of people in a field with a frisbee</span>
</span>
</td>
<td id="S5.T5.6.14.8.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.14.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.14.8.2.1.1" class="ltx_p" style="width:113.8pt;">C: a group of people riding motorcycles down a street</span>
</span>
</td>
<td id="S5.T5.6.14.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.14.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.14.8.3.1.1" class="ltx_p" style="width:113.8pt;">C: a laptop computer sitting on top of a desk</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.15.9" class="ltx_tr">
<td id="S5.T5.6.15.9.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.15.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.15.9.1.1.1" class="ltx_p" style="width:113.8pt;">Correct Answer: 6</span>
</span>
</td>
<td id="S5.T5.6.15.9.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.15.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.15.9.2.1.1" class="ltx_p" style="width:113.8pt;">Correct Answer: black and white</span>
</span>
</td>
<td id="S5.T5.6.15.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.15.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.15.9.3.1.1" class="ltx_p" style="width:113.8pt;">Correct Answer: no</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.16.10" class="ltx_tr">
<td id="S5.T5.6.16.10.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.16.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.16.10.1.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN: 2</span>
</span>
</td>
<td id="S5.T5.6.16.10.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.16.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.16.10.2.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN: red</span>
</span>
</td>
<td id="S5.T5.6.16.10.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.16.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.16.10.3.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN: yes</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.17.11" class="ltx_tr">
<td id="S5.T5.6.17.11.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.17.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.17.11.1.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN+CA: 5</span>
</span>
</td>
<td id="S5.T5.6.17.11.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.17.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.17.11.2.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN+CA: red</span>
</span>
</td>
<td id="S5.T5.6.17.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.17.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.17.11.3.1.1" class="ltx_p" style="width:113.8pt;">Predicted Answer NMN+CA: yes</span>
</span>
</td>
</tr>
<tr id="S5.T5.6.18.12" class="ltx_tr">
<td id="S5.T5.6.18.12.1" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.18.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.18.12.1.1.1" class="ltx_p" style="width:113.8pt;">(d)</span>
</span>
</td>
<td id="S5.T5.6.18.12.2" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.18.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.18.12.2.1.1" class="ltx_p" style="width:113.8pt;">(e)</span>
</span>
</td>
<td id="S5.T5.6.18.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="S5.T5.6.18.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S5.T5.6.18.12.3.1.1" class="ltx_p" style="width:113.8pt;">(f)</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparative performance analysis of before and after adding Caption Attention to the NMNs. The first row of examples show the questions correctly by our model. The second row of examples show questions where both NMN and our model predict the wrong answer.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p">In example (a), the information about â€˜two planesâ€™ is clearly mentioned in the caption and hence this additional information helps our model capture this answer from the embedded context vector when the caption is passed through an LSTM. Similarly the answer words â€˜pizzaâ€™ and â€˜tooth brushâ€™ in (b) and (c) are explicitly present in the caption. An obvious drawback to our approach is when the caption produced for the image is not relevant to the question being asked.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p">For the examples in the second row, the captions are not exactly relevant to the question and hence attention over the captions has not been particularly helpful. The predicted answers by our model exactly match that from NMN in (e) and (f). We observed an interesting type of error that our model makes by mapping generic terms to frequent occurrences in the captions. This is depicted in (d). The caption for this image has the word â€˜a group of peopleâ€™ that correspond to the number of people in the image. While NMN predicted the answer as 2, the model with caption attentions predicted the answer as 5. This could be due to frequent associations mapping the word in the caption â€˜groupâ€™ to the answer â€˜5â€™ in the training data and this is learnt by our model during end-to-end error backpropagation. Though the final predicted answer is wrong, common sense information that â€˜two peopleâ€™ are not called group and a relatively higher number is required to be called a group is learnt by the model.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Directions</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">We show that incorporating the information from captions improves results slightly (by 1.9% on training and by 1.2% on testing), especially in cases where the caption is relevant to the question being asked. However, our model fails when the generated caption is not relevant to the question and hence one future direction is towards generating captions with certain required words. This approach also learns appropriate mappings to generic terms mapped to more probable associations from the cations in the training data. An interesting direction to explore in the future would be the analysis of the irrelevant captions generated for an image for better fitting attention models.</p>
</div>
<section id="S6.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S6.SS0.SSSx1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSSx1.p1.1" class="ltx_p">We thank Professor Louis-Phillipe Morency, Dr. Tadas Baltrusaitis, Amir Zadeh and Chaitanya Ahuja for providing us the guidance, constant timely feedback and resources required for this project.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
Andreas, J., Rohrbach, M., Darrell, T., and Klein, D. (2016a).

</span>
<span class="ltx_bibblock">Learning to compose neural networks for question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</span>. Association for Computational Linguistics (ACL).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(2)</span>
<span class="ltx_bibblock">
Andreas, J., Rohrbach, M., Darrell, T., and Klein, D. (2016b).

</span>
<span class="ltx_bibblock">Neural module networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 39â€“48.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol etÂ al.,  (2015)</span>
<span class="ltx_bibblock">
Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.Â L., and
Parikh, D. (2015).

</span>
<span class="ltx_bibblock">VQA: Visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2015 IEEE International Conference on Computer Vision
(ICCV)</span>. Institute of Electrical and Electronics Engineers (IEEE).

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ã…strÃ¶m etÂ al.,  (2016)</span>
<span class="ltx_bibblock">
Ã…strÃ¶m, F., Petra, S., Schmitzer, B., and SchnÃ¶rr, C. (2016).

</span>
<span class="ltx_bibblock">Image labeling by assignment.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Journal of Mathematical Imaging and Vision</span>, pages 1â€“28.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer etÂ al.,  (2007)</span>
<span class="ltx_bibblock">
Auer, S., Bizer, C., Kobilarov, G., Lehmann, J., Cyganiak, R., and Ives, Z.
(2007).

</span>
<span class="ltx_bibblock">Dbpedia: A nucleus for a web of open data.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">The semantic web</span>, pages 722â€“735.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Manning,  (2014)</span>
<span class="ltx_bibblock">
Chen, D. and Manning, C.Â D. (2014).

</span>
<span class="ltx_bibblock">A fast and accurate dependency parser using neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">EMNLP</span>, pages 740â€“750.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fukui etÂ al.,  (2016)</span>
<span class="ltx_bibblock">
Fukui, A., Park, D.Â H., Yang, D., Rohrbach, A., Darrell, T., and Rohrbach, M.
(2016).

</span>
<span class="ltx_bibblock">Multimodal compact bilinear pooling for visual question answering and
visual grounding.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1606.01847</span>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al.,  (2015)</span>
<span class="ltx_bibblock">
Gao, H., Mao, J., Zhou, J., Huang, Z., Wang, L., and Xu, W. (2015).

</span>
<span class="ltx_bibblock">Are you talking to a machine? dataset and methods for multilingual
image question.

</span>
<span class="ltx_bibblock">In <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
2296â€“2304.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al.,  (2016)</span>
<span class="ltx_bibblock">
Gao, Y., Beijbom, O., Zhang, N., and Darrell, T. (2016).

</span>
<span class="ltx_bibblock">Compact bilinear pooling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 317â€“326.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber,  (1997)</span>
<span class="ltx_bibblock">
Hochreiter, S. and Schmidhuber, J. (1997).

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Neural computation</span>, 9(8):1735â€“1780.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia etÂ al.,  (2014)</span>
<span class="ltx_bibblock">
Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R.,
Guadarrama, S., and Darrell, T. (2014).

</span>
<span class="ltx_bibblock">Caffe: Convolutional architecture for fast feature embedding.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1408.5093</span>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpathy and Fei-Fei,  (2015)</span>
<span class="ltx_bibblock">
Karpathy, A. and Fei-Fei, L. (2015).

</span>
<span class="ltx_bibblock">Deep visual-semantic alignments for generating image descriptions.

</span>
<span class="ltx_bibblock">In <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 3128â€“3137.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lau and Baldwin,  (2016)</span>
<span class="ltx_bibblock">
Lau, J.Â H. and Baldwin, T. (2016).

</span>
<span class="ltx_bibblock">An empirical evaluation of doc2vec with practical insights into
document embedding generation.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1607.05368</span>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le and Mikolov,  (2014)</span>
<span class="ltx_bibblock">
Le, Q.Â V. and Mikolov, T. (2014).

</span>
<span class="ltx_bibblock">Distributed representations of sentences and documents.

</span>
<span class="ltx_bibblock">In <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">ICML</span>, volumeÂ 14, pages 1188â€“1196.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun etÂ al.,  (1989)</span>
<span class="ltx_bibblock">
LeCun, Y., Boser, B., Denker, J.Â S., Henderson, D., Howard, R.Â E., Hubbard, W.,
and Jackel, L.Â D. (1989).

</span>
<span class="ltx_bibblock">Backpropagation applied to handwritten zip code recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Neural computation</span>, 1(4):541â€“551.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al.,  (2014)</span>
<span class="ltx_bibblock">
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
DollÃ¡r, P., and Zitnick, C.Â L. (2014).

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">European Conference on Computer Vision</span>, pages 740â€“755.
Springer.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loper and Bird,  (2002)</span>
<span class="ltx_bibblock">
Loper, E. and Bird, S. (2002).

</span>
<span class="ltx_bibblock">Nltk: The natural language toolkit.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACL-02 Workshop on Effective tools and
methodologies for teaching natural language processing and computational
linguistics-Volume 1</span>, pages 63â€“70. Association for Computational
Linguistics.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al.,  (2016)</span>
<span class="ltx_bibblock">
Lu, J., Yang, J., Batra, D., and Parikh, D. (2016).

</span>
<span class="ltx_bibblock">Hierarchical question-image co-attention for visual question
answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Advances In Neural Information Processing Systems</span>, pages
289â€“297.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malinowski etÂ al.,  (2015)</span>
<span class="ltx_bibblock">
Malinowski, M., Rohrbach, M., and Fritz, M. (2015).

</span>
<span class="ltx_bibblock">Ask your neurons: A neural-based approach to answering questions
about images.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2015 IEEE International Conference on Computer Vision
(ICCV)</span>. Institute of Electrical and Electronics Engineers (IEEE).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nam etÂ al.,  (2016)</span>
<span class="ltx_bibblock">
Nam, H., Ha, J.-W., and Kim, J. (2016).

</span>
<span class="ltx_bibblock">Dual attention networks for multimodal reasoning and matching.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.00471</span>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neubig,  (2017)</span>
<span class="ltx_bibblock">
Neubig, G. (2017).

</span>
<span class="ltx_bibblock">Neural machine translation lecture notes: Attentional neural machine
translation.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren etÂ al.,  (2015)</span>
<span class="ltx_bibblock">
Ren, M., Kiros, R., and Zemel, R. (2015).

</span>
<span class="ltx_bibblock">Exploring models and data for image question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, pages
2953â€“2961.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SeguÃ­ etÂ al.,  (2015)</span>
<span class="ltx_bibblock">
SeguÃ­, S., Pujol, O., and Vitria, J. (2015).

</span>
<span class="ltx_bibblock">Learning to count with deep object features.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops</span>, pages 90â€“96.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shih etÂ al.,  (2016)</span>
<span class="ltx_bibblock">
Shih, K.Â J., Singh, S., and Hoiem, D. (2016).

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">2016 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span>. Institute of Electrical and Electronics Engineers
(IEEE).

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simonyan and Zisserman,  (2014)</span>
<span class="ltx_bibblock">
Simonyan, K. and Zisserman, A. (2014).

</span>
<span class="ltx_bibblock">Very deep convolutional networks for large-scale image recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1409.1556.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al.,  (2016)</span>
<span class="ltx_bibblock">
Wu, Q., Wang, P., Shen, C., Dick, A., and vanÂ den Hengel, A. (2016).

</span>
<span class="ltx_bibblock">Ask me anything: Free-form visual question answering based on
knowledge from external sources.

</span>
<span class="ltx_bibblock">In <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span>, pages 4622â€“4630.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al.,  (2015)</span>
<span class="ltx_bibblock">
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R.,
and Bengio, Y. (2015).

</span>
<span class="ltx_bibblock">Show, attend and tell: Neural image caption generation with visual
attention.

</span>
<span class="ltx_bibblock">In <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
2048â€“2057.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeiler,  (2012)</span>
<span class="ltx_bibblock">
Zeiler, M.Â D. (2012).

</span>
<span class="ltx_bibblock">Adadelta: an adaptive learning rate method.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1212.5701</span>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1809.08696" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1809.08697" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1809.08697">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1809.08697" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1809.08698" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Mar 17 01:12:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
