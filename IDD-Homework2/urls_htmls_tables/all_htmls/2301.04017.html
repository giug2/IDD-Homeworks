<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2301.04017] Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄</title><meta property="og:description" content="Federated learning (FL) is a framework for users to jointly train a machine learning model.
FL is promoted as a privacy-enhancing technology (PET) that provides data minimization: data never “leaves” personal devices a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2301.04017">

<!--Generated on Fri Mar  1 03:50:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Reconstructing Individual Data Points in Federated Learning 
<br class="ltx_break">Hardened with Differential Privacy and Secure Aggregation<sup id="id2.id1" class="ltx_sup">⋄</sup>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Franziska Boenisch1,
Adam Dziedzic12<sup id="id3.1.id1" class="ltx_sup">§</sup>,
Roei Schuster1<sup id="id4.2.id2" class="ltx_sup">§</sup>,
<br class="ltx_break">Ali Shahin Shamsabadi13<sup id="id5.3.id3" class="ltx_sup">§</sup>,
Ilia Shumailov1<sup id="id6.4.id4" class="ltx_sup">§</sup>, and
Nicolas Papernot12
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">1Vector Institute 2University of Toronto 3The Alan Turing Institute
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">Federated learning (FL) is a framework for users to jointly train a machine learning model.
FL is promoted as a privacy-enhancing technology (PET) that provides data minimization: data never “leaves” personal devices and users share only model updates with a server (e.g., a company) coordinating the distributed training.
While prior work showed that in vanilla FL a malicious server can extract users’ private data from the model updates, in this work we take it further and demonstrate that a malicious server can reconstruct user data even in hardened versions of the protocol.
More precisely, we propose an attack against FL protected with distributed differential privacy (DDP) and secure aggregation (SA).
Our attack method is based on the introduction of sybil devices that deviate from the protocol to expose individual users’ data for reconstruction by the server.
The underlying root cause for the vulnerability to our attack is a power imbalance:
the server orchestrates the whole protocol and users are given little guarantees about the selection of other users participating in the protocol.
Moving forward, we discuss requirements for privacy guarantees in FL.
We conclude that users should only participate in the protocol when they trust the server or they apply local primitives such as local DP, shifting power away from the server.
Yet, the latter approaches come at significant overhead in terms of performance degradation of the trained model, making them less likely to be deployed in practice.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">§</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">§</sup><span class="ltx_note_type">footnotetext: </span>Equal contribution. 
<br class="ltx_break"><sup id="footnotex1.1" class="ltx_sup">⋄</sup>. Accepted at the 8th IEEE European Symposium on Security and Privacy (IEEE Euro S&amp;P).</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is a widely deployed protocol for collaborative machine learning (ML).
It allows a server to train an ML model on data of different users without requiring direct access to that data.
For this reason, FL is often promoted as data minimizing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>:
instead of sharing their data, users calculate model updates, usually gradients, on a shared model obtained from the server.
These model updates are then aggregated and applied iteratively to train this shared model.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Prior work has demonstrated that the model updates leak sensitive information on the users’ local training data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>.
This is to be expected since nothing in the design of FL prevents information leakage.
Indeed, the root cause of why FL is inherently vulnerable to data reconstruction attacks is that it is designed to provide confidentiality (data does not leave user devices) rather than privacy (outputs of the computation do not leak sensitive attributes from the users’ input). Without additional privacy measures, FL cannot protect users from the server reconstructing their data.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Fortunately, <em id="S1.p3.1.1" class="ltx_emph ltx_font_italic">if</em> the server is trusted to follow the protocol as prescribed, a relatively cost-effective mitigation exists:
the server can add noise to the model updates and, thereby, implement differential privacy (DP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> during the aggregation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>. This degrades the performance of learned models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, but adds strong privacy guarantees. Unfortunately, the server cannot always be trusted. Worse yet, by observing local users’ model updates, an untrusted server can mount powerful attacks to reconstruct users’ training data points <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. Contemporary attacks exploit the fact that neurons compute a weighted sum of their input; thus the corresponding gradients contain rescaled versions of the input.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2301.04017/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="292" height="90" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.25.12.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.22.11" class="ltx_text ltx_font_bold" style="font-size:90%;">Course of our attack against FL protected by SA and DDP.<span id="S1.F1.22.11.11" class="ltx_text ltx_font_medium">  <svg id="S1.F1.12.1.1.pic1" class="ltx_picture" height="14.02" overflow="visible" version="1.1" width="14.02"><g transform="translate(0,14.02) matrix(1 0 0 -1 0 0) translate(7.01,0) translate(0,7.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.01 0 C 7.01 3.87 3.87 7.01 0 7.01 C -3.87 7.01 -7.01 3.87 -7.01 0 C -7.01 -3.87 -3.87 -7.01 0 -7.01 C 3.87 -7.01 7.01 -3.87 7.01 0 Z M 0 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.12.1.1.pic1.1.1.1.1.1" class="ltx_text" style="color:#FFFFFF;">1</span></foreignObject></g></g></svg> The server introduces a small fraction of sybil users into the FL application.  <svg id="S1.F1.13.2.2.pic2" class="ltx_picture" height="14.02" overflow="visible" version="1.1" width="14.02"><g transform="translate(0,14.02) matrix(1 0 0 -1 0 0) translate(7.01,0) translate(0,7.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.01 0 C 7.01 3.87 3.87 7.01 0 7.01 C -3.87 7.01 -7.01 3.87 -7.01 0 C -7.01 -3.87 -3.87 -7.01 0 -7.01 C 3.87 -7.01 7.01 -3.87 7.01 0 Z M 0 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.13.2.2.pic2.1.1.1.1.1" class="ltx_text" style="color:#FFFFFF;">2</span></foreignObject></g></g></svg> The server selects <math id="S1.F1.14.3.3.m1.1" class="ltx_Math" alttext="\mathsf{M}" display="inline"><semantics id="S1.F1.14.3.3.m1.1b"><mi id="S1.F1.14.3.3.m1.1.1" xref="S1.F1.14.3.3.m1.1.1.cmml">𝖬</mi><annotation-xml encoding="MathML-Content" id="S1.F1.14.3.3.m1.1c"><ci id="S1.F1.14.3.3.m1.1.1.cmml" xref="S1.F1.14.3.3.m1.1.1">𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.14.3.3.m1.1d">\mathsf{M}</annotation></semantics></math> users for participation in training round <math id="S1.F1.15.4.4.m2.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S1.F1.15.4.4.m2.1b"><mi id="S1.F1.15.4.4.m2.1.1" xref="S1.F1.15.4.4.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S1.F1.15.4.4.m2.1c"><ci id="S1.F1.15.4.4.m2.1.1.cmml" xref="S1.F1.15.4.4.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.15.4.4.m2.1d">t</annotation></semantics></math>: one target user and <math id="S1.F1.16.5.5.m3.1" class="ltx_Math" alttext="\mathsf{M}-1" display="inline"><semantics id="S1.F1.16.5.5.m3.1b"><mrow id="S1.F1.16.5.5.m3.1.1" xref="S1.F1.16.5.5.m3.1.1.cmml"><mi id="S1.F1.16.5.5.m3.1.1.2" xref="S1.F1.16.5.5.m3.1.1.2.cmml">𝖬</mi><mo id="S1.F1.16.5.5.m3.1.1.1" xref="S1.F1.16.5.5.m3.1.1.1.cmml">−</mo><mn id="S1.F1.16.5.5.m3.1.1.3" xref="S1.F1.16.5.5.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S1.F1.16.5.5.m3.1c"><apply id="S1.F1.16.5.5.m3.1.1.cmml" xref="S1.F1.16.5.5.m3.1.1"><minus id="S1.F1.16.5.5.m3.1.1.1.cmml" xref="S1.F1.16.5.5.m3.1.1.1"></minus><ci id="S1.F1.16.5.5.m3.1.1.2.cmml" xref="S1.F1.16.5.5.m3.1.1.2">𝖬</ci><cn type="integer" id="S1.F1.16.5.5.m3.1.1.3.cmml" xref="S1.F1.16.5.5.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.16.5.5.m3.1d">\mathsf{M}-1</annotation></semantics></math> sybils.  <svg id="S1.F1.17.6.6.pic3" class="ltx_picture" height="14.02" overflow="visible" version="1.1" width="14.02"><g transform="translate(0,14.02) matrix(1 0 0 -1 0 0) translate(7.01,0) translate(0,7.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.01 0 C 7.01 3.87 3.87 7.01 0 7.01 C -3.87 7.01 -7.01 3.87 -7.01 0 C -7.01 -3.87 -3.87 -7.01 0 -7.01 C 3.87 -7.01 7.01 -3.87 7.01 0 Z M 0 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.17.6.6.pic3.1.1.1.1.1" class="ltx_text" style="color:#FFFFFF;">3</span></foreignObject></g></g></svg> The server manipulates the shared model with trap weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and sends it out to the selected users.  <svg id="S1.F1.18.7.7.pic4" class="ltx_picture" height="14.02" overflow="visible" version="1.1" width="14.02"><g transform="translate(0,14.02) matrix(1 0 0 -1 0 0) translate(7.01,0) translate(0,7.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.01 0 C 7.01 3.87 3.87 7.01 0 7.01 C -3.87 7.01 -7.01 3.87 -7.01 0 C -7.01 -3.87 -3.87 -7.01 0 -7.01 C 3.87 -7.01 7.01 -3.87 7.01 0 Z M 0 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.18.7.7.pic4.1.1.1.1.1" class="ltx_text" style="color:#FFFFFF;">4</span></foreignObject></g></g></svg> The target user locally calculates its gradients on the manipulated model while the sybil users return zero or constant value gradients that are known by the server.  <svg id="S1.F1.19.8.8.pic5" class="ltx_picture" height="14.02" overflow="visible" version="1.1" width="14.02"><g transform="translate(0,14.02) matrix(1 0 0 -1 0 0) translate(7.01,0) translate(0,7.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.01 0 C 7.01 3.87 3.87 7.01 0 7.01 C -3.87 7.01 -7.01 3.87 -7.01 0 C -7.01 -3.87 -3.87 -7.01 0 -7.01 C 3.87 -7.01 7.01 -3.87 7.01 0 Z M 0 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.19.8.8.pic5.1.1.1.1.1" class="ltx_text" style="color:#FFFFFF;">5</span></foreignObject></g></g></svg> Only the target user locally applies a small amount of noise to its gradients to implement DDP.  <svg id="S1.F1.20.9.9.pic6" class="ltx_picture" height="14.02" overflow="visible" version="1.1" width="14.02"><g transform="translate(0,14.02) matrix(1 0 0 -1 0 0) translate(7.01,0) translate(0,7.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.01 0 C 7.01 3.87 3.87 7.01 0 7.01 C -3.87 7.01 -7.01 3.87 -7.01 0 C -7.01 -3.87 -3.87 -7.01 0 -7.01 C 3.87 -7.01 7.01 -3.87 7.01 0 Z M 0 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.20.9.9.pic6.1.1.1.1.1" class="ltx_text" style="color:#FFFFFF;">6</span></foreignObject></g></g></svg> The target user’s local noised gradients are aggregated with the sybil users’ values.  <svg id="S1.F1.21.10.10.pic7" class="ltx_picture" height="14.02" overflow="visible" version="1.1" width="14.02"><g transform="translate(0,14.02) matrix(1 0 0 -1 0 0) translate(7.01,0) translate(0,7.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.01 0 C 7.01 3.87 3.87 7.01 0 7.01 C -3.87 7.01 -7.01 3.87 -7.01 0 C -7.01 -3.87 -3.87 -7.01 0 -7.01 C 3.87 -7.01 7.01 -3.87 7.01 0 Z M 0 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.21.10.10.pic7.1.1.1.1.1" class="ltx_text" style="color:#FFFFFF;">7</span></foreignObject></g></g></svg> The resulting aggregate which effectively contains solely the target user’s gradients is sent to the server.
 <svg id="S1.F1.22.11.11.pic8" class="ltx_picture" height="14.02" overflow="visible" version="1.1" width="14.02"><g transform="translate(0,14.02) matrix(1 0 0 -1 0 0) translate(7.01,0) translate(0,7.01)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#000000"><path d="M 7.01 0 C 7.01 3.87 3.87 7.01 0 7.01 C -3.87 7.01 -7.01 3.87 -7.01 0 C -7.01 -3.87 -3.87 -7.01 0 -7.01 C 3.87 -7.01 7.01 -3.87 7.01 0 Z M 0 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)" fill="#000000" stroke="#000000"><foreignObject width="6.23" height="8.03" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><span id="S1.F1.22.11.11.pic8.1.1.1.1.1" class="ltx_text" style="color:#FFFFFF;">8</span></foreignObject></g></g></svg> The server extracts the target user’s training data from the received gradients.
</span></span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">We go a step beyond these prior extraction attacks that operate in vanilla FL protocols and mount an attack against FL combined with secure aggregation (SA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and distributed differential privacy (DDP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
In SA, gradient aggregation is performed via a decentralized multiparty computation protocol. In DDP, each user adds a small amount of noise to their gradient updates. Through the aggregation of user updates, the cumulative noise of DDP provides sufficiently high privacy guarantees to protect user data from leaking sensitive information.
The two techniques were designed to decrease the amount of trust placed by users in the central party in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Yet, our attack (see <a href="#S1.F1" title="In 1 Introduction ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>) shows that an untrusted server that exerts their full capabilities is able to still reconstruct individual user data points in this setup.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">To be clear, we are, of course, not claiming that the cryptographic primitives behind SA and DDP are broken.
We merely notice that the trust model that they assume, where in each round enough honest users contribute noised gradient updates, is not necessarily realized in practice within FL.
In reality, the server is entrusted with <em id="S1.p5.1.1" class="ltx_emph ltx_font_italic">provisioning</em> users and <em id="S1.p5.1.2" class="ltx_emph ltx_font_italic">sampling</em> them in each round.
A powerful untrusted server can inject an arbitrary number of malicious sybils under their control into any round, as demonstrated by industry actors actively deploying FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
By sampling a target user together with a group of sybils that act maliciously, the server can acquire direct access to the target user’s non-aggregated update.
This allows the server to eliminate any effect of SA, and effectively reduces the protection of DDP to a minimum because the noise added by a single user is not designed to offer the claimed privacy guarantees.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Next, we observe that the server is also typically entrusted with <em id="S1.p6.1.1" class="ltx_emph ltx_font_italic">controlling the shared model</em>.
Prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> has shown that this ability enables the server to extract large amounts of the users’ individual training data points from gradients in vanilla FL.
By integrating the trap weight approach from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> into our attack, we show that an untrusted server can extract high-fidelity user data points for common learning tasks despite DDP and SA.
This highlights that <em id="S1.p6.1.2" class="ltx_emph ltx_font_italic">even elaborate combinations of techniques like DDP and SA with FL can be attacked to perform data reconstruction when the server’s real-world capabilities are taken into account</em>.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">While exploring data reconstruction under DP, we make another observation that advances our understanding of which users are more vulnerable to data reconstruction attacks: the noise addition does not guarantee equal protection over all model gradients.
As we visualize in <a href="#S5.F3" title="In 5.2 Noisy Data Extraction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>, some data points can be extracted with higher fidelity than others. This is despite the fact that all corresponding gradients were protected with the same clipping and noising operations.
We identify the gradient norm as the reason behind disparate protection.
For small-norm gradients, noise dominates the signal of the extracted data more than for gradients with a large norm.
In principle, clipping in DP for ML is supposed to bound the gradient norm to control for this.
However, the gradient norm is calculated globally whereas data is extracted locally from the components of the gradient that correspond to the weighted input of a single neuron. When the gradient corresponding to a neuron is large but all other neurons in that layer have small gradients, the overall gradient can still be below the clipping norm.
Therefore, no clipping is performed, the same amount of noise is added to neurons with large and small gradients, and data from the neurons with larger gradients can be extracted with higher fidelity.
We also sketch a construction that amplifies this effect and makes a few individual data points nearly perfectly extractable, even under noise addition.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">We then discuss the centralization and resulting power imbalance between the server and users as the root cause of FL’s vulnerability against attacks like the one proposed in this work.
This motivates us to explore the requirements for building a variant of FL that practically prevents attacks by a malicious server.
We consider three approaches based on (1) decentralization, (2) user verification, and (3) the support of specialized hardware.
We find that one promising direction is to add adequate amounts of noise to users’ gradients via a cryptographic protocol such as secure multiparty computation (SMCP).
However as of yet, due to the gradients’ high dimensionality, known constructions’ communication costs are prohibitive.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">As an alternative direction, users in FL can take responsibility for implementing their full privacy protection locally, for example, by adding enough noise to individually implement strong privacy guarantees.
This approach is commonly referred to as local DP (LDP).
As a last resort, users can decide not to participate in FL protocols if they do not trust the server.
However, the last two options are only available if users have the required control over their participation in the protocol, which is not always the case.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">In summary, we make the following contributions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We design an attack that enables a malicious server that holds the power of introducing sybil devices to reconstruct individual training data points from users when FL is protected by SA and DDP. See <a href="#S1.F1" title="In 1 Introduction ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> for an illustration of our attack flow.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We experimentally validate the attack’s ability to reconstruct image and textual data with high fidelity in different DDP setups. We propose a proof of concept construction that allows to increase the fidelity of the reconstruction of users’ data points by reducing the effect of additive noise at the level of individual neurons.
We thus observe disparate leakage over gradients under DDP.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We discuss centralization in FL and its resulting power-imbalance between the server and the users as the root cause of FL’s vulnerability and consider potential mitigations.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Background</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">This section provides background on FL, describes user-data extraction from gradients in the vanilla version of the protocol and introduces SA and DP—extensions implementing dedicated defenses against privacy leakage.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span id="S2.SS1.1.1" class="ltx_text ltx_font_italic">Federated Learning</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.8" class="ltx_p">FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is a communication protocol that allows a group of <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathsf{N}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">𝖭</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\mathsf{N}</annotation></semantics></math> users to jointly train an ML model <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="f" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">f</annotation></semantics></math>, such that data never leaves the respective users’ devices. FL involves a <span id="S2.SS1.p1.8.1" class="ltx_text ltx_font_italic">server</span>, who coordinates the training in an iterative process, as follows: at round <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="t=0" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml">t</mi><mo id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><eq id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"></eq><ci id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2">𝑡</ci><cn type="integer" id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">t=0</annotation></semantics></math>, the server initializes the shared model <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{W}^{[0]}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><msup id="S2.SS1.p1.4.m4.1.2" xref="S2.SS1.p1.4.m4.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.4.m4.1.2.2" xref="S2.SS1.p1.4.m4.1.2.2.cmml">𝒲</mi><mrow id="S2.SS1.p1.4.m4.1.1.1.3" xref="S2.SS1.p1.4.m4.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.4.m4.1.1.1.3.1" xref="S2.SS1.p1.4.m4.1.1.1.2.1.cmml">[</mo><mn id="S2.SS1.p1.4.m4.1.1.1.1" xref="S2.SS1.p1.4.m4.1.1.1.1.cmml">0</mn><mo stretchy="false" id="S2.SS1.p1.4.m4.1.1.1.3.2" xref="S2.SS1.p1.4.m4.1.1.1.2.1.cmml">]</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.2.cmml" xref="S2.SS1.p1.4.m4.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.2.1.cmml" xref="S2.SS1.p1.4.m4.1.2">superscript</csymbol><ci id="S2.SS1.p1.4.m4.1.2.2.cmml" xref="S2.SS1.p1.4.m4.1.2.2">𝒲</ci><apply id="S2.SS1.p1.4.m4.1.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.1.3"><csymbol cd="latexml" id="S2.SS1.p1.4.m4.1.1.1.2.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1.3.1">delimited-[]</csymbol><cn type="integer" id="S2.SS1.p1.4.m4.1.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1.1.1">0</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\mathcal{W}^{[0]}</annotation></semantics></math> at random, typically following common weight-initialization practice <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. At every round <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">t</annotation></semantics></math>, the server chooses a subset of <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="\mathsf{M}\leq\mathsf{N}" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">𝖬</mi><mo id="S2.SS1.p1.6.m6.1.1.1" xref="S2.SS1.p1.6.m6.1.1.1.cmml">≤</mo><mi id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml">𝖭</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><leq id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1.1"></leq><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">𝖬</ci><ci id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3">𝖭</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\mathsf{M}\leq\mathsf{N}</annotation></semantics></math> users to contribute to the learning round. The server then sends each of these users the model <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="f_{\mathcal{W}^{[t]}}" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><msub id="S2.SS1.p1.7.m7.1.2" xref="S2.SS1.p1.7.m7.1.2.cmml"><mi id="S2.SS1.p1.7.m7.1.2.2" xref="S2.SS1.p1.7.m7.1.2.2.cmml">f</mi><msup id="S2.SS1.p1.7.m7.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.7.m7.1.1.1.3" xref="S2.SS1.p1.7.m7.1.1.1.3.cmml">𝒲</mi><mrow id="S2.SS1.p1.7.m7.1.1.1.1.1.3" xref="S2.SS1.p1.7.m7.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.7.m7.1.1.1.1.1.3.1" xref="S2.SS1.p1.7.m7.1.1.1.1.1.2.1.cmml">[</mo><mi id="S2.SS1.p1.7.m7.1.1.1.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p1.7.m7.1.1.1.1.1.3.2" xref="S2.SS1.p1.7.m7.1.1.1.1.1.2.1.cmml">]</mo></mrow></msup></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.2.cmml" xref="S2.SS1.p1.7.m7.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.2.1.cmml" xref="S2.SS1.p1.7.m7.1.2">subscript</csymbol><ci id="S2.SS1.p1.7.m7.1.2.2.cmml" xref="S2.SS1.p1.7.m7.1.2.2">𝑓</ci><apply id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.1.3">𝒲</ci><apply id="S2.SS1.p1.7.m7.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.1.1.1.3"><csymbol cd="latexml" id="S2.SS1.p1.7.m7.1.1.1.1.1.2.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1.1.1.3.1">delimited-[]</csymbol><ci id="S2.SS1.p1.7.m7.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1.1.1.1">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">f_{\mathcal{W}^{[t]}}</annotation></semantics></math>. Each user chooses a subsample of their training data termed a <span id="S2.SS1.p1.8.2" class="ltx_text ltx_font_italic">mini-batch</span>, computes the gradients of an objective function over <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="f_{\mathcal{W}^{[t]}}" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.2" xref="S2.SS1.p1.8.m8.1.2.cmml"><mi id="S2.SS1.p1.8.m8.1.2.2" xref="S2.SS1.p1.8.m8.1.2.2.cmml">f</mi><msup id="S2.SS1.p1.8.m8.1.1.1" xref="S2.SS1.p1.8.m8.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.8.m8.1.1.1.3" xref="S2.SS1.p1.8.m8.1.1.1.3.cmml">𝒲</mi><mrow id="S2.SS1.p1.8.m8.1.1.1.1.1.3" xref="S2.SS1.p1.8.m8.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.SS1.p1.8.m8.1.1.1.1.1.3.1" xref="S2.SS1.p1.8.m8.1.1.1.1.1.2.1.cmml">[</mo><mi id="S2.SS1.p1.8.m8.1.1.1.1.1.1" xref="S2.SS1.p1.8.m8.1.1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="S2.SS1.p1.8.m8.1.1.1.1.1.3.2" xref="S2.SS1.p1.8.m8.1.1.1.1.1.2.1.cmml">]</mo></mrow></msup></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.2.cmml" xref="S2.SS1.p1.8.m8.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.2.1.cmml" xref="S2.SS1.p1.8.m8.1.2">subscript</csymbol><ci id="S2.SS1.p1.8.m8.1.2.2.cmml" xref="S2.SS1.p1.8.m8.1.2.2">𝑓</ci><apply id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.8.m8.1.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.1.3">𝒲</ci><apply id="S2.SS1.p1.8.m8.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.1.1.1.3"><csymbol cd="latexml" id="S2.SS1.p1.8.m8.1.1.1.1.1.2.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1.1.1.3.1">delimited-[]</csymbol><ci id="S2.SS1.p1.8.m8.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1.1.1.1.1">𝑡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">f_{\mathcal{W}^{[t]}}</annotation></semantics></math> for each of these samples, and returns these gradients, which we call a (local) <span id="S2.SS1.p1.8.3" class="ltx_text ltx_font_italic">update</span>, to the server. To conclude the round, the server updates the shared model by aggregating the received gradients, multiplying them by a learning-rate parameter and applying the change to the shared model.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For readers unfamiliar with gradient optimization: such gradient-based weight updates are intended to prescribe which direction <math id="footnote1.m1.1" class="ltx_Math" alttext="\mathcal{W}^{[t]}" display="inline"><semantics id="footnote1.m1.1b"><msup id="footnote1.m1.1.2" xref="footnote1.m1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="footnote1.m1.1.2.2" xref="footnote1.m1.1.2.2.cmml">𝒲</mi><mrow id="footnote1.m1.1.1.1.3" xref="footnote1.m1.1.1.1.2.cmml"><mo stretchy="false" id="footnote1.m1.1.1.1.3.1" xref="footnote1.m1.1.1.1.2.1.cmml">[</mo><mi id="footnote1.m1.1.1.1.1" xref="footnote1.m1.1.1.1.1.cmml">t</mi><mo stretchy="false" id="footnote1.m1.1.1.1.3.2" xref="footnote1.m1.1.1.1.2.1.cmml">]</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><apply id="footnote1.m1.1.2.cmml" xref="footnote1.m1.1.2"><csymbol cd="ambiguous" id="footnote1.m1.1.2.1.cmml" xref="footnote1.m1.1.2">superscript</csymbol><ci id="footnote1.m1.1.2.2.cmml" xref="footnote1.m1.1.2.2">𝒲</ci><apply id="footnote1.m1.1.1.1.2.cmml" xref="footnote1.m1.1.1.1.3"><csymbol cd="latexml" id="footnote1.m1.1.1.1.2.1.cmml" xref="footnote1.m1.1.1.1.3.1">delimited-[]</csymbol><ci id="footnote1.m1.1.1.1.1.cmml" xref="footnote1.m1.1.1.1.1">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">\mathcal{W}^{[t]}</annotation></semantics></math> needs to move towards, for the objective to be minimized. Typically, the objective is a value measuring the model’s level of prediction error across a given mini-batch.</span></span></span></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">It follows from the description above that users’ training data significantly affects the values of local updates.
This enables a variety of privacy attacks that extract individual user-data points directly from the model updates, as highlighted in the following section.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span id="S2.SS2.1.1" class="ltx_text ltx_font_italic">Data Extraction from Vanilla FL</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> has shown that an untrusted server can directly extract user-data from the model gradients.
In these attacks, the server leverages its control over the shared model.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>There also exist optimization-based data reconstruction attacks operating on model updates.
These attacks can be conducted by a passive attacker solely observing the gradients.
However, computation is expensive and the reconstructed data is not necessarily of high-fidelity.
We provide a brief overview of this type of attack in <a href="#S7" title="7 Related Work ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">7</span></a>.</span></span></span>
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the server exploits this ability to insert a fully-connected model layer as an extraction module where individual data points can be directly extracted.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> manipulates the model weights with an attack they call trap weights.
This attack increases natural data leakage from fully-connected model layers.
In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, the server instead modifies model parameters to extract single data points by increasing their gradient contribution.
The attack operates in several iterations of the protocol to collect multiple gradient updates from the same user.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">The presence of such data extraction attacks highlights the vulnerability of vanilla FL to privacy-leakage.
To prevent training data from leaking onto updates and straight to the hands of the server, various extensions have been proposed, as we now briefly describe.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span><span id="S2.SS3.1.1" class="ltx_text ltx_font_italic">Secure Aggregation</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In SA, due to Bonawitz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, users do not send their individual updates to the server. Instead, they perform, along with the server, a multiparty computation (MPC) protocol that ensures the server only receives the average of all updates in the round.
Various improvements of the original protocol were suggested, for example, to allow the server to prove the correctness of the aggregate computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>,
increase robustness against malicious users’ manipulated gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, or improve communication efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span><span id="S2.SS4.1.1" class="ltx_text ltx_font_italic">Differential Privacy in FL</span>
</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Nothing in the design of FL prevents information leakage: FL is designed to provide confidentiality (data does not leave user devices) rather than privacy (outputs of the computation do not leak sensitive attributes from the users’ input).
As discussed in <a href="#S2.SS2" title="2.2 Data Extraction from Vanilla FL ‣ 2 Background ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>, this leaves vanilla FL vulnerable to data reconstruction attacks.
To ensure the privacy of users’ sensitive training data, it is natural to consider DP approaches that work by adding noise to user updates. DP is a gold standard in privacy technology because proper application of it comes with a theoretical bound on the probability of an adversary being able to distinguish adjacent datasets, <em id="S2.SS4.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S2.SS4.p1.1.2" class="ltx_text"></span> datasets that differ solely in one data point.
This implies a bound on the probability of data point extraction.
In other words, a DP approach properly applied to FL updates could, in principle, ensure that individual user data points are not revealed to whoever observes the noised updates. See Appendix <a href="#A1" title="Appendix A Differential Privacy and DPSGD ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a> for more background on DP and its integration to ML.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">One possible approach to integrate DP into FL is <span id="S2.SS4.p2.1.1" class="ltx_text ltx_font_italic">centralized DP</span> (CDP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, where the server adds noise to the mini-batch gradients received by users. CDP assumes that the server is trusted to add noise, which is not true in the threat model of this work, see Section <a href="#S3" title="3 Threat Model and Assumptions ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
To address this, <span id="S2.SS4.p2.1.2" class="ltx_text ltx_font_italic">local DP</span> (LDP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> was proposed, where each user adds noise to its local update before sending it out for aggregation, in a way that ensures the user’s own dataset is protected from extraction. Unfortunately, LDP generally results in degraded model utility due to the addition of large amounts of noise to every user’s update <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>. <span id="S2.SS4.p2.1.3" class="ltx_text ltx_font_italic">Distributed DP (DDP)</span> was proposed as a popular middle ground between CDP and LDP, where multiple users independently add noise to their update, that is sufficient to ensure their datasets are protected from an extraction adversary, but only as long as their updates are aggregated before the adversary observes them. Through combination with SA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> or similar aggregation methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, where the server can only view aggregated updates, DDP ostensibly ensures that the server cannot extract individual data points. But, importantly, this assumes that a large fraction of users participating in the FL round are honest and add their share of the noise.
We discuss the applicability of this assumption in the real world in <a href="#S3.SS2" title="3.2 Adversarial Model ‣ 3 Threat Model and Assumptions ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Threat Model and Assumptions</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We characterize our threat model and assumptions in terms of our adversary and the considered FL setup.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_italic">Adversary</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.2" class="ltx_p">Our adversary is the server, and their goal is to infer individual users’ local sensitive data points. Note that the background in <a href="#S2.SS1" title="2.1 Federated Learning ‣ 2 Background ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.1</span></a> implies that the server can—whenever they choose to—(1) control the weights of the shared model, (2) select which of the <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathsf{N}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathsf{N}</annotation></semantics></math> users participate in each round, and (3) provision new users into the pool (including <span id="S3.SS1.p1.2.1" class="ltx_text ltx_font_italic">sybils</span> controlled by the server).<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Capabilities (2) and (3) have been demonstrated in the real world as Google researchers introduced 189 sybils devices into the Gboard FL system and made them participate in the protocol along with real users <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</span></span></span> We assume that the total fraction of sybils out of the <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathsf{N}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">𝖭</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝖭</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathsf{N}</annotation></semantics></math> users is small.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">We, furthermore, assume the server is <em id="S3.SS1.p2.1.1" class="ltx_emph ltx_font_italic">occasionally malicious</em> (OM), meaning they behave maliciously in only a few rounds of the protocol.
When this happens, they can exert the above capabilities (1-3) adversarially.
Do note that the server here does not deviate from the protocol and restricts themselves to only use valid operations (1-3) in the FL protocol.
An OM server ensures the attack remains stealthy, and also allows the server to train a model that has high utility over the non-malicious rounds, which is an expected product of FL.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_italic">Adversarial Model</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">By assuming a server’s ability of introducing hundreds of sybil devices into the pool of active users, this work relies on a strong adversarial model.
Yet, industry actors that deploy FL protocols in real-world applications have shown that this scenario is a realistic threat <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
For the purpose of a research project, they successfully introduced hundreds of sybil devices in the real-world FL training of the Google keyboard and let them train along the real users for some time to manipulate the model training.
Since obtaining and introducing sybil devices does not only require configuration but also a non-negligible financial overhead, we expect this type of manipulation to be reserved to adversaries that can afford it.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Another setting in which we think our attack is practical is the one of colluding employees. Since prior research has documented that it generally takes a small number (e.g., 2-3) of employees to approve changes to a company’s code base <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, we could imagine that these employees would collude. Jointly, these employees could then maliciously exploit the servers’ abilities of user sampling and controlling the shared model. We account for this scenario by our OM threat model, and argue that by preventing this type of attack, a company can protect itself against actions that might harm their reputation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span id="S3.SS3.1.1" class="ltx_text ltx_font_italic">FL Setup</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Our departure point are FL protocols deployed in real-world applications, such as the one described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. These protocols initially focused on data minimization only.
We extend them with SA and DDP, two defenses dedicated to additionally providing privacy protection for the FL protocol.
We note that we chose to study an instantiation of FL with SA and DDP because it is the combination of published techniques that holds the strongest promise in the presence of an untrusted server.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Indeed, the key alternative to FL with SA and DDP would be FL with LDP (local DP guarantees) but this alternative is not appealing because it comes at a significant utility cost for the server.</span></span></span>
Note, however, that FL with SA and DDP is not as widely deployed as vanilla FL is.
This is mainly due to some increased communication costs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> and the computational overhead of adapted mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Attacking FL under SA and DDP</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we study FL extended by DDP and SA—considered as a strongly privacy-protective instantiation of the protocol—and show that the server can still reconstruct sensitive information about the users’ training data.
We also forge an intuition of what factors contribute most to the leakage.
Based on our findings, in <a href="#S8" title="8 Discussion: Privacy-Preserving FL ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">8</span></a>, we discuss future research and implementation towards private FL.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">For successful data reconstruction under DDP and SA, the server has to make use of the following three capabilities which it naturally holds in FL:</p>
<ol id="S4.I1" class="ltx_enumerate">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p"><em id="S4.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">Introducing sybil devices:</em> The server needs to be able to introduce a fraction of manipulated devices in the FL protocol (see <a href="#S3.SS2" title="3.2 Adversarial Model ‣ 3 Threat Model and Assumptions ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>). These devices can return arbitrary gradients, chosen by the server. In particular, they can contribute zero gradients to the SA.</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p"><em id="S4.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">Controlling the user sampling:</em> To ensure that the sybil devices are sampled for SA together with a target user, the server needs to control the user sampling.</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S4.I1.i3.p1" class="ltx_para">
<p id="S4.I1.i3.p1.1" class="ltx_p"><em id="S4.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Manipulating the model weights:</em> For improved data reconstruction performance, the server can manipulate the shared model’s weights, for example, relying on one of the methods discussed in <a href="#S2.SS2" title="2.2 Data Extraction from Vanilla FL ‣ 2 Background ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
</li>
</ol>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">While the first two capabilities enable the server to circumvent SA and to leave the gradients of a target user with insufficient amount of noise for privacy protection under DDP, the third capability increases the amount of individual training data that can be reconstructed and extends the attack to other model architecture types.</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><span id="S4.p4.1.1" class="ltx_text ltx_font_bold">Attack flow.</span>

Our attack aims at reconstructing the private data of one target user per malicious round in the FL training.
To do so, the attack needs to circumvent the SA (<a href="#S4.SS1" title="4.1 Circumventing SA ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>), then exploit the weak privacy guarantees of DDP from a user’s perspective (<a href="#S4.SS2" title="4.2 Exploiting DDP Guarantees ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>), and finally reconstruct the target user’s individual training data points (<a href="#S4.SS3" title="4.3 Reconstructing Data ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>). See <a href="#S1.F1" title="In 1 Introduction ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> for the course of our attack.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span id="S4.SS1.1.1" class="ltx_text ltx_font_italic">Circumventing SA</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.2" class="ltx_p">In our attack, the server circumvents SA by sampling the target user together with maliciously controlled sybil devices for the given training round.
Since for each round, <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="\mathsf{M}" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">𝖬</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">\mathsf{M}</annotation></semantics></math> users are sampled for participation, the server needs to control at least <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathsf{M}-1" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">𝖬</mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><minus id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></minus><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">𝖬</ci><cn type="integer" id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\mathsf{M}-1</annotation></semantics></math> sybil devices.
It has been shown in previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> that inserting an arbitrary number of sybil devices into real-world FL deployments is practically feasible as we discuss in <a href="#S3.SS2" title="3.2 Adversarial Model ‣ 3 Threat Model and Assumptions ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Since SA-protocols provide their guarantees under the assumption that a certain fraction of users is honest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, it follows naturally that in the presence of the sybil devices, no guarantees can be provided to the target user.
This is because when the gradients are aggregated over multiple users, and all but the target user contribute arbitrary gradients, known to the server, the server can extract the target user’s gradients perfectly.
In the easiest case, the sybil devices contribute all zero-gradients, such that the final aggregate directly only contains the target user’s gradients.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">Note that we do not claim that the SA or any of the underlying cryptographic primitives are broken.
We solely observe that SA relies on the assumption that the clients participating in the execution are real clients and not maliciously controlled by the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
However, in FL with an untrusted server, the users cannot verify this assumption.
We show through our attack that this has severe implications on their privacy guarantees.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Pasquini <em id="S4.SS1.p4.1.1" class="ltx_emph ltx_font_italic">et al</em>.<span id="S4.SS1.p4.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> describe a different way to circumvent SA based on the server sending out different models to different users.
While the models for non-target users produce zero-gradients, the target user’s model produces non-zero gradients which can be exploited for data reconstruction.
An advantage of this method is that it does not require the server to control the user sampling, or to manipulate a fraction of users.
Note however that in their scenario, DDP can still be efficiently applied if every user adds some noise to their (potentially zero) gradients.
As a consequence, the total amount of noise can be sufficient to protect the gradients of the target user.
Therefore, in our attack, we rely on the controlled sybil devices to circumvent the SA.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.1" class="ltx_p">Note that also alternative mechanisms to implement DDP, such as shuffling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> which can be put into place instead of SA, can be circumvented by our approach of inserting sybil devices with server-controlled gradients into the protocol.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span id="S4.SS2.1.1" class="ltx_text ltx_font_italic">Exploiting DDP Guarantees</span>
</h3>

<figure id="S4.F2" class="ltx_figure"><img src="/html/2301.04017/assets/x2.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="298" height="127" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.19.8.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.14.7" class="ltx_text" style="font-size:90%;">
<span id="S4.F2.14.7.1" class="ltx_text ltx_font_bold">Privacy vs. utility trade-offs under DDP/LDP.</span> Each point on the <span id="S4.F2.14.7.2" class="ltx_text" style="color:#0000FF;">blue line</span> corresponds to a model trained on CIFAR10 with a clipping parameter <math id="S4.F2.8.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S4.F2.8.1.m1.1b"><mrow id="S4.F2.8.1.m1.1.1" xref="S4.F2.8.1.m1.1.1.cmml"><mi id="S4.F2.8.1.m1.1.1.2" xref="S4.F2.8.1.m1.1.1.2.cmml">c</mi><mo id="S4.F2.8.1.m1.1.1.1" xref="S4.F2.8.1.m1.1.1.1.cmml">=</mo><mn id="S4.F2.8.1.m1.1.1.3" xref="S4.F2.8.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.8.1.m1.1c"><apply id="S4.F2.8.1.m1.1.1.cmml" xref="S4.F2.8.1.m1.1.1"><eq id="S4.F2.8.1.m1.1.1.1.cmml" xref="S4.F2.8.1.m1.1.1.1"></eq><ci id="S4.F2.8.1.m1.1.1.2.cmml" xref="S4.F2.8.1.m1.1.1.2">𝑐</ci><cn type="integer" id="S4.F2.8.1.m1.1.1.3.cmml" xref="S4.F2.8.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.8.1.m1.1d">c=1</annotation></semantics></math>, and the total noise scale <math id="S4.F2.9.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.F2.9.2.m2.1b"><mi id="S4.F2.9.2.m2.1.1" xref="S4.F2.9.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.F2.9.2.m2.1c"><ci id="S4.F2.9.2.m2.1.1.cmml" xref="S4.F2.9.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.9.2.m2.1d">\sigma</annotation></semantics></math> indicated on the x-axis.
Training was conducted over 100 epochs, the resulting privacy guarantees <math id="S4.F2.10.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S4.F2.10.3.m3.1b"><mi id="S4.F2.10.3.m3.1.1" xref="S4.F2.10.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.F2.10.3.m3.1c"><ci id="S4.F2.10.3.m3.1.1.cmml" xref="S4.F2.10.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.10.3.m3.1d">\epsilon</annotation></semantics></math> are reported.
For the non-private baseline <math id="S4.F2.11.4.m4.1" class="ltx_Math" alttext="\epsilon=\infty" display="inline"><semantics id="S4.F2.11.4.m4.1b"><mrow id="S4.F2.11.4.m4.1.1" xref="S4.F2.11.4.m4.1.1.cmml"><mi id="S4.F2.11.4.m4.1.1.2" xref="S4.F2.11.4.m4.1.1.2.cmml">ϵ</mi><mo id="S4.F2.11.4.m4.1.1.1" xref="S4.F2.11.4.m4.1.1.1.cmml">=</mo><mi mathvariant="normal" id="S4.F2.11.4.m4.1.1.3" xref="S4.F2.11.4.m4.1.1.3.cmml">∞</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F2.11.4.m4.1c"><apply id="S4.F2.11.4.m4.1.1.cmml" xref="S4.F2.11.4.m4.1.1"><eq id="S4.F2.11.4.m4.1.1.1.cmml" xref="S4.F2.11.4.m4.1.1.1"></eq><ci id="S4.F2.11.4.m4.1.1.2.cmml" xref="S4.F2.11.4.m4.1.1.2">italic-ϵ</ci><infinity id="S4.F2.11.4.m4.1.1.3.cmml" xref="S4.F2.11.4.m4.1.1.3"></infinity></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.11.4.m4.1d">\epsilon=\infty</annotation></semantics></math> (<span id="S4.F2.14.7.3" class="ltx_text" style="color:#FF0000;">red line</span>).
We report accuracy loss with respect to this non-private baseline.
The images depicted below the line plot display the rescaled noisy gradients of one data point of an individual user with noise calculated according to <a href="#S4.E2" title="In 4.2 Exploiting DDP Guarantees ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a> as a function of <math id="S4.F2.12.5.m5.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.F2.12.5.m5.1b"><mi id="S4.F2.12.5.m5.1.1" xref="S4.F2.12.5.m5.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.F2.12.5.m5.1c"><ci id="S4.F2.12.5.m5.1.1.cmml" xref="S4.F2.12.5.m5.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.12.5.m5.1d">\sigma</annotation></semantics></math>, <math id="S4.F2.13.6.m6.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.F2.13.6.m6.1b"><mi id="S4.F2.13.6.m6.1.1" xref="S4.F2.13.6.m6.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.F2.13.6.m6.1c"><ci id="S4.F2.13.6.m6.1.1.cmml" xref="S4.F2.13.6.m6.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.13.6.m6.1d">c</annotation></semantics></math>, and <math id="S4.F2.14.7.m7.1" class="ltx_Math" alttext="\mathsf{M}" display="inline"><semantics id="S4.F2.14.7.m7.1b"><mi id="S4.F2.14.7.m7.1.1" xref="S4.F2.14.7.m7.1.1.cmml">𝖬</mi><annotation-xml encoding="MathML-Content" id="S4.F2.14.7.m7.1c"><ci id="S4.F2.14.7.m7.1.1.cmml" xref="S4.F2.14.7.m7.1.1">𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F2.14.7.m7.1d">\mathsf{M}</annotation></semantics></math> in the training round.
The more users participate, the less noise every user needs to add because, during aggregation, the total noise is determined by the sum of the individual noises.
If, however, other users do not add noise, the locally added noise is the only privacy protection every individual user has (DDP reduces to LDP with weak privacy guarantees).
As a consequence, there is a discrepancy between the privacy the user believes to get and the privacy they actually get:
The images above the black line visualize the user’s belief on what the server can extract from their gradient, the images below the black line visualize what the server can actually extract under our attack.</span></figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">If DDP is in place, the gradients of the target user will be slightly noisy—even with successful circumvention of SA.
However, by design of DDP, the amount of noise added by each user is typically insufficient to provide a meaningful privacy guarantee <span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_italic">from the user’s perspective</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. By meaningful privacy guarantees we mean, guarantees equivalent to what one would obtain in the LDP definition.
This is in fact how DDP obtains a utility gain over LDP, which would have inserted sufficient noise to obtain per-user privacy guarantees that are independent of other users:
DDP assumes all users will add enough noise so that the <em id="S4.SS2.p1.1.2" class="ltx_emph ltx_font_italic">aggregate</em> is sufficiently noised whereas LDP assumes each user adds enough noise to obtain <em id="S4.SS2.p1.1.3" class="ltx_emph ltx_font_italic">privacy in isolation</em>.
As a consequence, in DDP, each user can add less noise locally than required for the desired total privacy level, resulting in more utility.
In contrast, the guarantee provided by LDP allows the user to not trust the server or other users.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.2" class="ltx_p">Concretely, in an LDP version of FL, the noise added by each user depends solely on the noise scale <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">\sigma</annotation></semantics></math> and the clipping parameter <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">c</annotation></semantics></math> of the application.
As a consequence, the local noise is sampled from a Gaussian distribution according to</p>
<table id="A2.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E1.m1.2" class="ltx_Math" alttext="\displaystyle\mathcal{N}(0,\sigma^{2}c^{2})\text{.}" display="inline"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.3" xref="S4.E1.m1.2.2.3.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2.cmml">​</mo><mrow id="S4.E1.m1.2.2.1.1" xref="S4.E1.m1.2.2.1.2.cmml"><mo stretchy="false" id="S4.E1.m1.2.2.1.1.2" xref="S4.E1.m1.2.2.1.2.cmml">(</mo><mn id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">0</mn><mo id="S4.E1.m1.2.2.1.1.3" xref="S4.E1.m1.2.2.1.2.cmml">,</mo><mrow id="S4.E1.m1.2.2.1.1.1" xref="S4.E1.m1.2.2.1.1.1.cmml"><msup id="S4.E1.m1.2.2.1.1.1.2" xref="S4.E1.m1.2.2.1.1.1.2.cmml"><mi id="S4.E1.m1.2.2.1.1.1.2.2" xref="S4.E1.m1.2.2.1.1.1.2.2.cmml">σ</mi><mn id="S4.E1.m1.2.2.1.1.1.2.3" xref="S4.E1.m1.2.2.1.1.1.2.3.cmml">2</mn></msup><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.1.1.1.1" xref="S4.E1.m1.2.2.1.1.1.1.cmml">​</mo><msup id="S4.E1.m1.2.2.1.1.1.3" xref="S4.E1.m1.2.2.1.1.1.3.cmml"><mi id="S4.E1.m1.2.2.1.1.1.3.2" xref="S4.E1.m1.2.2.1.1.1.3.2.cmml">c</mi><mn id="S4.E1.m1.2.2.1.1.1.3.3" xref="S4.E1.m1.2.2.1.1.1.3.3.cmml">2</mn></msup></mrow><mo stretchy="false" id="S4.E1.m1.2.2.1.1.4" xref="S4.E1.m1.2.2.1.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E1.m1.2.2.2a" xref="S4.E1.m1.2.2.2.cmml">​</mo><mtext id="S4.E1.m1.2.2.4" xref="S4.E1.m1.2.2.4a.cmml">.</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><times id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"></times><ci id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2.3">𝒩</ci><interval closure="open" id="S4.E1.m1.2.2.1.2.cmml" xref="S4.E1.m1.2.2.1.1"><cn type="integer" id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">0</cn><apply id="S4.E1.m1.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1"><times id="S4.E1.m1.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.1.1.1.1"></times><apply id="S4.E1.m1.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.1.1.1.2">superscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.1.1.1.2.2">𝜎</ci><cn type="integer" id="S4.E1.m1.2.2.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.1.1.1.2.3">2</cn></apply><apply id="S4.E1.m1.2.2.1.1.1.3.cmml" xref="S4.E1.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.1.1.1.3">superscript</csymbol><ci id="S4.E1.m1.2.2.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.1.1.1.3.2">𝑐</ci><cn type="integer" id="S4.E1.m1.2.2.1.1.1.3.3.cmml" xref="S4.E1.m1.2.2.1.1.1.3.3">2</cn></apply></apply></interval><ci id="S4.E1.m1.2.2.4a.cmml" xref="S4.E1.m1.2.2.4"><mtext id="S4.E1.m1.2.2.4.cmml" xref="S4.E1.m1.2.2.4">.</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\displaystyle\mathcal{N}(0,\sigma^{2}c^{2})\text{.}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">In contrast, in DDP, the amount of noise added by each individual user additionally takes the number of users who participate in the round into account <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>.
Assuming that <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathsf{M}" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mi id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">𝖬</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><ci id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">\mathsf{M}</annotation></semantics></math> users are sampled for participation, this results in a local addition of Gaussian noise sampled from</p>
<table id="A2.EGx2" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.2" class="ltx_Math" alttext="\displaystyle\mathcal{N}\left(0,\frac{\sigma^{2}}{\mathsf{M}-1}c^{2}\right)\text{~{}\cite[cite]{[\@@bibref{}{truex2019hybrid}{}{}]}.}" display="inline"><semantics id="S4.E2.m1.2a"><mrow id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.2.2.3" xref="S4.E2.m1.2.2.3.cmml">𝒩</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.2.cmml">​</mo><mrow id="S4.E2.m1.2.2.1.1" xref="S4.E2.m1.2.2.1.2.cmml"><mo id="S4.E2.m1.2.2.1.1.2" xref="S4.E2.m1.2.2.1.2.cmml">(</mo><mn id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">0</mn><mo id="S4.E2.m1.2.2.1.1.3" xref="S4.E2.m1.2.2.1.2.cmml">,</mo><mrow id="S4.E2.m1.2.2.1.1.1" xref="S4.E2.m1.2.2.1.1.1.cmml"><mstyle displaystyle="true" id="S4.E2.m1.2.2.1.1.1.2" xref="S4.E2.m1.2.2.1.1.1.2.cmml"><mfrac id="S4.E2.m1.2.2.1.1.1.2a" xref="S4.E2.m1.2.2.1.1.1.2.cmml"><msup id="S4.E2.m1.2.2.1.1.1.2.2" xref="S4.E2.m1.2.2.1.1.1.2.2.cmml"><mi id="S4.E2.m1.2.2.1.1.1.2.2.2" xref="S4.E2.m1.2.2.1.1.1.2.2.2.cmml">σ</mi><mn id="S4.E2.m1.2.2.1.1.1.2.2.3" xref="S4.E2.m1.2.2.1.1.1.2.2.3.cmml">2</mn></msup><mrow id="S4.E2.m1.2.2.1.1.1.2.3" xref="S4.E2.m1.2.2.1.1.1.2.3.cmml"><mi id="S4.E2.m1.2.2.1.1.1.2.3.2" xref="S4.E2.m1.2.2.1.1.1.2.3.2.cmml">𝖬</mi><mo id="S4.E2.m1.2.2.1.1.1.2.3.1" xref="S4.E2.m1.2.2.1.1.1.2.3.1.cmml">−</mo><mn id="S4.E2.m1.2.2.1.1.1.2.3.3" xref="S4.E2.m1.2.2.1.1.1.2.3.3.cmml">1</mn></mrow></mfrac></mstyle><mo lspace="0em" rspace="0em" id="S4.E2.m1.2.2.1.1.1.1" xref="S4.E2.m1.2.2.1.1.1.1.cmml">​</mo><msup id="S4.E2.m1.2.2.1.1.1.3" xref="S4.E2.m1.2.2.1.1.1.3.cmml"><mi id="S4.E2.m1.2.2.1.1.1.3.2" xref="S4.E2.m1.2.2.1.1.1.3.2.cmml">c</mi><mn id="S4.E2.m1.2.2.1.1.1.3.3" xref="S4.E2.m1.2.2.1.1.1.3.3.cmml">2</mn></msup></mrow><mo id="S4.E2.m1.2.2.1.1.4" xref="S4.E2.m1.2.2.1.2.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S4.E2.m1.2.2.2a" xref="S4.E2.m1.2.2.2.cmml">​</mo><mrow id="S4.E2.m1.2.2.4" xref="S4.E2.m1.2.2.4f.cmml"><mtext id="S4.E2.m1.2.2.4a" xref="S4.E2.m1.2.2.4f.cmml"> </mtext><mtext class="ltx_citemacro_cite" id="S4.E2.m1.2.2.4b" xref="S4.E2.m1.2.2.4f.cmml"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></mtext><mtext id="S4.E2.m1.2.2.4e" xref="S4.E2.m1.2.2.4f.cmml">.</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.2b"><apply id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2"><times id="S4.E2.m1.2.2.2.cmml" xref="S4.E2.m1.2.2.2"></times><ci id="S4.E2.m1.2.2.3.cmml" xref="S4.E2.m1.2.2.3">𝒩</ci><interval closure="open" id="S4.E2.m1.2.2.1.2.cmml" xref="S4.E2.m1.2.2.1.1"><cn type="integer" id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1">0</cn><apply id="S4.E2.m1.2.2.1.1.1.cmml" xref="S4.E2.m1.2.2.1.1.1"><times id="S4.E2.m1.2.2.1.1.1.1.cmml" xref="S4.E2.m1.2.2.1.1.1.1"></times><apply id="S4.E2.m1.2.2.1.1.1.2.cmml" xref="S4.E2.m1.2.2.1.1.1.2"><divide id="S4.E2.m1.2.2.1.1.1.2.1.cmml" xref="S4.E2.m1.2.2.1.1.1.2"></divide><apply id="S4.E2.m1.2.2.1.1.1.2.2.cmml" xref="S4.E2.m1.2.2.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.1.1.1.2.2.1.cmml" xref="S4.E2.m1.2.2.1.1.1.2.2">superscript</csymbol><ci id="S4.E2.m1.2.2.1.1.1.2.2.2.cmml" xref="S4.E2.m1.2.2.1.1.1.2.2.2">𝜎</ci><cn type="integer" id="S4.E2.m1.2.2.1.1.1.2.2.3.cmml" xref="S4.E2.m1.2.2.1.1.1.2.2.3">2</cn></apply><apply id="S4.E2.m1.2.2.1.1.1.2.3.cmml" xref="S4.E2.m1.2.2.1.1.1.2.3"><minus id="S4.E2.m1.2.2.1.1.1.2.3.1.cmml" xref="S4.E2.m1.2.2.1.1.1.2.3.1"></minus><ci id="S4.E2.m1.2.2.1.1.1.2.3.2.cmml" xref="S4.E2.m1.2.2.1.1.1.2.3.2">𝖬</ci><cn type="integer" id="S4.E2.m1.2.2.1.1.1.2.3.3.cmml" xref="S4.E2.m1.2.2.1.1.1.2.3.3">1</cn></apply></apply><apply id="S4.E2.m1.2.2.1.1.1.3.cmml" xref="S4.E2.m1.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.2.2.1.1.1.3.1.cmml" xref="S4.E2.m1.2.2.1.1.1.3">superscript</csymbol><ci id="S4.E2.m1.2.2.1.1.1.3.2.cmml" xref="S4.E2.m1.2.2.1.1.1.3.2">𝑐</ci><cn type="integer" id="S4.E2.m1.2.2.1.1.1.3.3.cmml" xref="S4.E2.m1.2.2.1.1.1.3.3">2</cn></apply></apply></interval><ci id="S4.E2.m1.2.2.4f.cmml" xref="S4.E2.m1.2.2.4"><mrow id="S4.E2.m1.2.2.4.cmml" xref="S4.E2.m1.2.2.4"><mtext id="S4.E2.m1.2.2.4a.cmml" xref="S4.E2.m1.2.2.4"> </mtext><mtext class="ltx_citemacro_cite" id="S4.E2.m1.2.2.4b.cmml" xref="S4.E2.m1.2.2.4"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></mtext><mtext id="S4.E2.m1.2.2.4e.cmml" xref="S4.E2.m1.2.2.4">.</mtext></mrow></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.2c">\displaystyle\mathcal{N}\left(0,\frac{\sigma^{2}}{\mathsf{M}-1}c^{2}\right)\text{~{}\cite[cite]{[\@@bibref{}{truex2019hybrid}{}{}]}.}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p">In <a href="#S4.F2" title="In 4.2 Exploiting DDP Guarantees ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, we present the privacy-utility trade-offs resulting from training models on the CIFAR10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> dataset as a function of the total noise scale <math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mi id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><ci id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">\sigma</annotation></semantics></math> and the resulting models’ accuracy on a test set.
We train the private models with a state-of-the-art framework for DP-training<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a target="_blank" href="https://github.com/ftramer/Handcrafted-DP" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/ftramer/Handcrafted-DP</a>. Note, however, that our reported accuracy and achieved privacy levels <math id="footnote5.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="footnote5.m1.1b"><mi id="footnote5.m1.1.1" xref="footnote5.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="footnote5.m1.1c"><ci id="footnote5.m1.1.1.cmml" xref="footnote5.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m1.1d">\epsilon</annotation></semantics></math> cannot directly be compared with the values reported in the repository. This is because we use different noise scales than they do and train the model for <math id="footnote5.m2.1" class="ltx_Math" alttext="100" display="inline"><semantics id="footnote5.m2.1b"><mn id="footnote5.m2.1.1" xref="footnote5.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="footnote5.m2.1c"><cn type="integer" id="footnote5.m2.1.1.cmml" xref="footnote5.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m2.1d">100</annotation></semantics></math> epochs while they only train for <math id="footnote5.m3.1" class="ltx_Math" alttext="30" display="inline"><semantics id="footnote5.m3.1b"><mn id="footnote5.m3.1.1" xref="footnote5.m3.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="footnote5.m3.1c"><cn type="integer" id="footnote5.m3.1.1.cmml" xref="footnote5.m3.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m3.1d">30</annotation></semantics></math> epochs.</span></span></span> in which all hyperparameters and model architecture are tuned for the task.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><a href="#S4.F2" title="In 4.2 Exploiting DDP Guarantees ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> provides two main insights.
First, unsurprisingly, given the privacy-utility trade-offs mentioned above, the model utility decreases when the total noise scale <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mi id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><ci id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">\sigma</annotation></semantics></math> increases.
Second, the figure shows that the more users participate in a given training round, the less noise each user needs to add locally.
This results from <a href="#S4.E2" title="In 4.2 Exploiting DDP Guarantees ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a> which relies on the total noise being aggregated over all participating users before sharing the aggregated gradients with the server.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p">DDP assumes that each user is <em id="S4.SS2.p6.1.1" class="ltx_emph ltx_font_italic">honest</em> and adds the required noise to their gradients.
However, if even one of the users adds less than the amount of noise it should add, the desired total privacy guarantees cannot be reached.
Even worse, if, as described in <a href="#S4" title="4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4</span></a>, a target user in FL is sampled for participation solely with controlled sybil devices that do not provide any noise for aggregation, the local noise added by the target user represents the only protection for its gradients.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p">These results mean that there is a tension between (1) the guarantee claimed by the server (and other users) in DDP and (2) the guarantee that a user who does not trust this server can rely on. This will lead the server optimizing for model utility to request that users add less noise to their gradients than what is needed for individual users to protect their data from leaking to an untrusted server.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span id="S4.SS3.1.1" class="ltx_text ltx_font_italic">Reconstructing Data</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In <a href="#S2.SS2" title="2.2 Data Extraction from Vanilla FL ‣ 2 Background ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.2</span></a>, we presented different attacks that rely on manipulations of the shared model to extract individual users’ training data points.
In principle, each of these attacks can be included to perform data reconstruction in our FL+SA+DDP setup.
However, the attack by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> extracts individual data points over several rounds of the FL protocol.
In our setup, due to the server’s OM nature, single-round attacks are preferable.
These allow the adversary to stay more inconspicuous and to train a more meaningful shared model throughout the benign rounds.
The attack by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> relies on manipulations of the model architecture, which are more detectable than manipulations of model parameters, such as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Finally, our experimental evaluation highlighted a significant advantage of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> in comparison to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> for data reconstruction under noise.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>’s trap weights yield redundancy in the extracted data, <em id="S4.SS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S4.SS3.p1.1.2" class="ltx_text"></span> the same data point can be extracted multiple times from gradients of different weight rows.
We thoroughly investigate this effect in <a href="#S5.SS3" title="5.3 Redundancy in Extracted Data ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>.
The redundancy of extracted noisy data can be exploited to average out the effect of the noise and yield higher-fidelity data reconstruction, as we will show in <a href="#S5.SS4" title="5.4 Improving Noisy Data Reconstruction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.4</span></a>.
In contrast, due to the nature of their attack, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, each data point is only extractable once.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Evaluation of the Attack against SA+DDP</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we present a practical evaluation of our attack against FL protected with SA and DDP.
We first present our experimental setup.
Then, we evaluate direct extraction of noisy gradients under DDP.
We move on to experimentally evaluate the redundancy of extracted data with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>’s trap weight approach, and discover how this redundancy can be exploited for higher-fidelity data reconstruction under noise.
We illustrate this with empirical results reconstructing image and text data.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span><span id="S5.SS1.1.1" class="ltx_text ltx_font_italic">Experimental Setup</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">We operate in a cross-device FL setup and perform training on the CIFAR10 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> dataset.
We evaluate extraction on different mini-batch sizes <math id="S5.SS1.p1.1.m1.3" class="ltx_Math" alttext="B\in\{10,20,100\}" display="inline"><semantics id="S5.SS1.p1.1.m1.3a"><mrow id="S5.SS1.p1.1.m1.3.4" xref="S5.SS1.p1.1.m1.3.4.cmml"><mi id="S5.SS1.p1.1.m1.3.4.2" xref="S5.SS1.p1.1.m1.3.4.2.cmml">B</mi><mo id="S5.SS1.p1.1.m1.3.4.1" xref="S5.SS1.p1.1.m1.3.4.1.cmml">∈</mo><mrow id="S5.SS1.p1.1.m1.3.4.3.2" xref="S5.SS1.p1.1.m1.3.4.3.1.cmml"><mo stretchy="false" id="S5.SS1.p1.1.m1.3.4.3.2.1" xref="S5.SS1.p1.1.m1.3.4.3.1.cmml">{</mo><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">10</mn><mo id="S5.SS1.p1.1.m1.3.4.3.2.2" xref="S5.SS1.p1.1.m1.3.4.3.1.cmml">,</mo><mn id="S5.SS1.p1.1.m1.2.2" xref="S5.SS1.p1.1.m1.2.2.cmml">20</mn><mo id="S5.SS1.p1.1.m1.3.4.3.2.3" xref="S5.SS1.p1.1.m1.3.4.3.1.cmml">,</mo><mn id="S5.SS1.p1.1.m1.3.3" xref="S5.SS1.p1.1.m1.3.3.cmml">100</mn><mo stretchy="false" id="S5.SS1.p1.1.m1.3.4.3.2.4" xref="S5.SS1.p1.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.3b"><apply id="S5.SS1.p1.1.m1.3.4.cmml" xref="S5.SS1.p1.1.m1.3.4"><in id="S5.SS1.p1.1.m1.3.4.1.cmml" xref="S5.SS1.p1.1.m1.3.4.1"></in><ci id="S5.SS1.p1.1.m1.3.4.2.cmml" xref="S5.SS1.p1.1.m1.3.4.2">𝐵</ci><set id="S5.SS1.p1.1.m1.3.4.3.1.cmml" xref="S5.SS1.p1.1.m1.3.4.3.2"><cn type="integer" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">10</cn><cn type="integer" id="S5.SS1.p1.1.m1.2.2.cmml" xref="S5.SS1.p1.1.m1.2.2">20</cn><cn type="integer" id="S5.SS1.p1.1.m1.3.3.cmml" xref="S5.SS1.p1.1.m1.3.3">100</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.3c">B\in\{10,20,100\}</annotation></semantics></math>.
We split the CIFAR10 training data at random and iid between the users.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> shows that their trap weights’ extraction success is equal for iid and non-iid distribution, even for the most extreme scenario where every data point in a given mini-batch stems from same class.
Following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, we also experiment with the IMDB dataset for sentiment analysis and the and distribute data the same way.
An extended experimental evaluation on two additional image datasets (MNIST and ImageNet), and two textual spam classification datasets (Spam Mails and SMS Spam Collection) can be found in Appendix <a href="#A2" title="Appendix B Extended Experimental Evaluation ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">To evaluate different setups for DDP, we select <math id="S5.SS1.p2.1.m1.3" class="ltx_Math" alttext="\{10,100,1000\}" display="inline"><semantics id="S5.SS1.p2.1.m1.3a"><mrow id="S5.SS1.p2.1.m1.3.4.2" xref="S5.SS1.p2.1.m1.3.4.1.cmml"><mo stretchy="false" id="S5.SS1.p2.1.m1.3.4.2.1" xref="S5.SS1.p2.1.m1.3.4.1.cmml">{</mo><mn id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">10</mn><mo id="S5.SS1.p2.1.m1.3.4.2.2" xref="S5.SS1.p2.1.m1.3.4.1.cmml">,</mo><mn id="S5.SS1.p2.1.m1.2.2" xref="S5.SS1.p2.1.m1.2.2.cmml">100</mn><mo id="S5.SS1.p2.1.m1.3.4.2.3" xref="S5.SS1.p2.1.m1.3.4.1.cmml">,</mo><mn id="S5.SS1.p2.1.m1.3.3" xref="S5.SS1.p2.1.m1.3.3.cmml">1000</mn><mo stretchy="false" id="S5.SS1.p2.1.m1.3.4.2.4" xref="S5.SS1.p2.1.m1.3.4.1.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.3b"><set id="S5.SS1.p2.1.m1.3.4.1.cmml" xref="S5.SS1.p2.1.m1.3.4.2"><cn type="integer" id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">10</cn><cn type="integer" id="S5.SS1.p2.1.m1.2.2.cmml" xref="S5.SS1.p2.1.m1.2.2">100</cn><cn type="integer" id="S5.SS1.p2.1.m1.3.3.cmml" xref="S5.SS1.p2.1.m1.3.3">1000</cn></set></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.3c">\{10,100,1000\}</annotation></semantics></math> users for participation in a given round of the FL protocol.
To circumvent the SA, as described in the previous section, we sample one target user together with sybil devices which all return zero gradients.
Other than that, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>’s experimental setup, use their six-layer fully-connected neural network and embedding architecture (their Table 7 and 8), initialize the first fully-connected layer with their trap weights for individual data point extractability.
To project received gradients of the first fully-connected layer’s weight matrix back to their input domain, we rely on <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>’s Equation (5) which shows that it is sufficient to rescale the gradient of the weights with the inverse of the gradient of the bias for perfect data extraction.
Note that in our case, due to DDP, noise is added not only to the gradient of the weights but also to the gradient of the bias.
Therefore, not only the extracted gradients but also our scaling factor are noisy, resulting in the rescaled gradients not being a perfect reconstruction of the original input data.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.3" class="ltx_p">We report the DDP-setup per-round through three parameters required to determine the noise magnitude according to <a href="#S4.E2" title="In 4.2 Exploiting DDP Guarantees ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a>, namely the DP clip norm <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mi id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><ci id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">c</annotation></semantics></math>, the DP noise multiplier <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mi id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">\sigma</annotation></semantics></math>, and the number of selected users in this round <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathsf{M}" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mi id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">𝖬</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><ci id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">𝖬</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">\mathsf{M}</annotation></semantics></math>. This enables us to understand how sensitive the attack is to choices for these hyperparameters without making any assumptions about other hyperparameters of the training run (e.g., the number of training steps).</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span><span id="S5.SS2.1.1" class="ltx_text ltx_font_italic">Noisy Data Extraction</span>
</h3>

<figure id="S5.F3" class="ltx_figure"><img src="/html/2301.04017/assets/x3.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="104" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.9.4.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S5.F3.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Directly Extracted Data under DDP.<span id="S5.F3.6.3.3" class="ltx_text ltx_font_medium"> Rescaled clipped and noised gradients from a mini-batch with 20 data points from CIFAR10 dataset.
DDP setup: <math id="S5.F3.4.1.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S5.F3.4.1.1.m1.1b"><mrow id="S5.F3.4.1.1.m1.1.1" xref="S5.F3.4.1.1.m1.1.1.cmml"><mi id="S5.F3.4.1.1.m1.1.1.2" xref="S5.F3.4.1.1.m1.1.1.2.cmml">c</mi><mo id="S5.F3.4.1.1.m1.1.1.1" xref="S5.F3.4.1.1.m1.1.1.1.cmml">=</mo><mn id="S5.F3.4.1.1.m1.1.1.3" xref="S5.F3.4.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.4.1.1.m1.1c"><apply id="S5.F3.4.1.1.m1.1.1.cmml" xref="S5.F3.4.1.1.m1.1.1"><eq id="S5.F3.4.1.1.m1.1.1.1.cmml" xref="S5.F3.4.1.1.m1.1.1.1"></eq><ci id="S5.F3.4.1.1.m1.1.1.2.cmml" xref="S5.F3.4.1.1.m1.1.1.2">𝑐</ci><cn type="integer" id="S5.F3.4.1.1.m1.1.1.3.cmml" xref="S5.F3.4.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.4.1.1.m1.1d">c=1</annotation></semantics></math>, <math id="S5.F3.5.2.2.m2.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="S5.F3.5.2.2.m2.1b"><mrow id="S5.F3.5.2.2.m2.1.1" xref="S5.F3.5.2.2.m2.1.1.cmml"><mi id="S5.F3.5.2.2.m2.1.1.2" xref="S5.F3.5.2.2.m2.1.1.2.cmml">σ</mi><mo id="S5.F3.5.2.2.m2.1.1.1" xref="S5.F3.5.2.2.m2.1.1.1.cmml">=</mo><mn id="S5.F3.5.2.2.m2.1.1.3" xref="S5.F3.5.2.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.5.2.2.m2.1c"><apply id="S5.F3.5.2.2.m2.1.1.cmml" xref="S5.F3.5.2.2.m2.1.1"><eq id="S5.F3.5.2.2.m2.1.1.1.cmml" xref="S5.F3.5.2.2.m2.1.1.1"></eq><ci id="S5.F3.5.2.2.m2.1.1.2.cmml" xref="S5.F3.5.2.2.m2.1.1.2">𝜎</ci><cn type="float" id="S5.F3.5.2.2.m2.1.1.3.cmml" xref="S5.F3.5.2.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.5.2.2.m2.1d">\sigma=0.1</annotation></semantics></math>, and <math id="S5.F3.6.3.3.m3.1" class="ltx_Math" alttext="\mathsf{M}=100" display="inline"><semantics id="S5.F3.6.3.3.m3.1b"><mrow id="S5.F3.6.3.3.m3.1.1" xref="S5.F3.6.3.3.m3.1.1.cmml"><mi id="S5.F3.6.3.3.m3.1.1.2" xref="S5.F3.6.3.3.m3.1.1.2.cmml">𝖬</mi><mo id="S5.F3.6.3.3.m3.1.1.1" xref="S5.F3.6.3.3.m3.1.1.1.cmml">=</mo><mn id="S5.F3.6.3.3.m3.1.1.3" xref="S5.F3.6.3.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F3.6.3.3.m3.1c"><apply id="S5.F3.6.3.3.m3.1.1.cmml" xref="S5.F3.6.3.3.m3.1.1"><eq id="S5.F3.6.3.3.m3.1.1.1.cmml" xref="S5.F3.6.3.3.m3.1.1.1"></eq><ci id="S5.F3.6.3.3.m3.1.1.2.cmml" xref="S5.F3.6.3.3.m3.1.1.2">𝖬</ci><cn type="integer" id="S5.F3.6.3.3.m3.1.1.3.cmml" xref="S5.F3.6.3.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F3.6.3.3.m3.1d">\mathsf{M}=100</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.5" class="ltx_p">We first perform direct extraction from noisy gradients.
The extraction of data points works exactly the same way as for vanilla FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, by initializing the model with trap weights before sending it to the target user, and then projecting their received gradients back to the input domain.
<a href="#S5.F3" title="In 5.2 Noisy Data Extraction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the full resulting extracted data in a setup with a mini-batch of 20 data points, <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">c</mi><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><eq id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></eq><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑐</ci><cn type="integer" id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">c=1</annotation></semantics></math>, <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">σ</mi><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><eq id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></eq><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">𝜎</ci><cn type="float" id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\sigma=0.1</annotation></semantics></math>, and <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="\mathsf{M}=100" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">𝖬</mi><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><eq id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1"></eq><ci id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">𝖬</ci><cn type="integer" id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\mathsf{M}=100</annotation></semantics></math>.
While some noisy reconstructed data points resemble the original training data, other are less recognizable because they are an overlay of multiple data points, or are dominated by the added noise. DDP setup: <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mrow id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mi id="S5.SS2.p1.4.m4.1.1.2" xref="S5.SS2.p1.4.m4.1.1.2.cmml">c</mi><mo id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.4.m4.1.1.3" xref="S5.SS2.p1.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><eq id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1"></eq><ci id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2">𝑐</ci><cn type="integer" id="S5.SS2.p1.4.m4.1.1.3.cmml" xref="S5.SS2.p1.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">c=1</annotation></semantics></math>, <math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><mrow id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml"><mi id="S5.SS2.p1.5.m5.1.1.2" xref="S5.SS2.p1.5.m5.1.1.2.cmml">σ</mi><mo id="S5.SS2.p1.5.m5.1.1.1" xref="S5.SS2.p1.5.m5.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.5.m5.1.1.3" xref="S5.SS2.p1.5.m5.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><apply id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1"><eq id="S5.SS2.p1.5.m5.1.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1.1"></eq><ci id="S5.SS2.p1.5.m5.1.1.2.cmml" xref="S5.SS2.p1.5.m5.1.1.2">𝜎</ci><cn type="float" id="S5.SS2.p1.5.m5.1.1.3.cmml" xref="S5.SS2.p1.5.m5.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">\sigma=0.1</annotation></semantics></math>.
We report further results for MNIST and ImageNet in Appendix <a href="#A2.SS2" title="B.2 Experimental Results on Image Data ‣ Appendix B Extended Experimental Evaluation ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2</span></a>.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.4" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T1.4.1" class="ltx_tr">
<td id="S5.T1.4.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.4.1.1.1" class="ltx_text ltx_font_bold">Number of</span></td>
<td id="S5.T1.4.1.2" class="ltx_td ltx_align_center ltx_border_tt">% <span id="S5.T1.4.1.2.1" class="ltx_text ltx_font_bold">Individually</span>
</td>
<td id="S5.T1.4.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S5.T1.4.1.3.1" class="ltx_text ltx_font_bold">Reconstruction</span></td>
</tr>
<tr id="S5.T1.4.2" class="ltx_tr">
<td id="S5.T1.4.2.1" class="ltx_td ltx_align_center"><span id="S5.T1.4.2.1.1" class="ltx_text ltx_font_bold">Benign Users</span></td>
<td id="S5.T1.4.2.2" class="ltx_td ltx_align_center"><span id="S5.T1.4.2.2.1" class="ltx_text ltx_font_bold">Reconstructable Data</span></td>
<td id="S5.T1.4.2.3" class="ltx_td ltx_align_center"><span id="S5.T1.4.2.3.1" class="ltx_text ltx_font_bold">SNRs</span></td>
</tr>
<tr id="S5.T1.4.3" class="ltx_tr">
<td id="S5.T1.4.3.1" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S5.T1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">0.867</td>
<td id="S5.T1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">0.015</td>
</tr>
<tr id="S5.T1.4.4" class="ltx_tr">
<td id="S5.T1.4.4.1" class="ltx_td ltx_align_center">2</td>
<td id="S5.T1.4.4.2" class="ltx_td ltx_align_center">0.800</td>
<td id="S5.T1.4.4.3" class="ltx_td ltx_align_center">0.012</td>
</tr>
<tr id="S5.T1.4.5" class="ltx_tr">
<td id="S5.T1.4.5.1" class="ltx_td ltx_align_center">3</td>
<td id="S5.T1.4.5.2" class="ltx_td ltx_align_center">0.733</td>
<td id="S5.T1.4.5.3" class="ltx_td ltx_align_center">0.011</td>
</tr>
<tr id="S5.T1.4.6" class="ltx_tr">
<td id="S5.T1.4.6.1" class="ltx_td ltx_align_center">4</td>
<td id="S5.T1.4.6.2" class="ltx_td ltx_align_center">0.717</td>
<td id="S5.T1.4.6.3" class="ltx_td ltx_align_center">0.010</td>
</tr>
<tr id="S5.T1.4.7" class="ltx_tr">
<td id="S5.T1.4.7.1" class="ltx_td ltx_align_center">5</td>
<td id="S5.T1.4.7.2" class="ltx_td ltx_align_center">0.633</td>
<td id="S5.T1.4.7.3" class="ltx_td ltx_align_center">0.010</td>
</tr>
<tr id="S5.T1.4.8" class="ltx_tr">
<td id="S5.T1.4.8.1" class="ltx_td ltx_align_center">10</td>
<td id="S5.T1.4.8.2" class="ltx_td ltx_align_center">0.400</td>
<td id="S5.T1.4.8.3" class="ltx_td ltx_align_center">0.008</td>
</tr>
<tr id="S5.T1.4.9" class="ltx_tr">
<td id="S5.T1.4.9.1" class="ltx_td ltx_align_center">15</td>
<td id="S5.T1.4.9.2" class="ltx_td ltx_align_center">0.317</td>
<td id="S5.T1.4.9.3" class="ltx_td ltx_align_center">0.008</td>
</tr>
<tr id="S5.T1.4.10" class="ltx_tr">
<td id="S5.T1.4.10.1" class="ltx_td ltx_align_center">20</td>
<td id="S5.T1.4.10.2" class="ltx_td ltx_align_center">0.250</td>
<td id="S5.T1.4.10.3" class="ltx_td ltx_align_center">0.008</td>
</tr>
<tr id="S5.T1.4.11" class="ltx_tr">
<td id="S5.T1.4.11.1" class="ltx_td ltx_align_center">30</td>
<td id="S5.T1.4.11.2" class="ltx_td ltx_align_center">0.133</td>
<td id="S5.T1.4.11.3" class="ltx_td ltx_align_center">0.007</td>
</tr>
<tr id="S5.T1.4.12" class="ltx_tr">
<td id="S5.T1.4.12.1" class="ltx_td ltx_align_center">40</td>
<td id="S5.T1.4.12.2" class="ltx_td ltx_align_center">0.000</td>
<td id="S5.T1.4.12.3" class="ltx_td ltx_align_center">0.007</td>
</tr>
<tr id="S5.T1.4.13" class="ltx_tr">
<td id="S5.T1.4.13.1" class="ltx_td ltx_align_center ltx_border_bb">50</td>
<td id="S5.T1.4.13.2" class="ltx_td ltx_align_center ltx_border_bb">0.000</td>
<td id="S5.T1.4.13.3" class="ltx_td ltx_align_center ltx_border_bb">0.007</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.6.2.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S5.T1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Influence of Fraction of Sybil Devices.<span id="S5.T1.2.1.1" class="ltx_text ltx_font_medium"> Results for FL hardened by SSA and DDP with 50 participants, each holding <math id="S5.T1.2.1.1.m1.1" class="ltx_Math" alttext="B=20" display="inline"><semantics id="S5.T1.2.1.1.m1.1b"><mrow id="S5.T1.2.1.1.m1.1.1" xref="S5.T1.2.1.1.m1.1.1.cmml"><mi id="S5.T1.2.1.1.m1.1.1.2" xref="S5.T1.2.1.1.m1.1.1.2.cmml">B</mi><mo id="S5.T1.2.1.1.m1.1.1.1" xref="S5.T1.2.1.1.m1.1.1.1.cmml">=</mo><mn id="S5.T1.2.1.1.m1.1.1.3" xref="S5.T1.2.1.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.1.1.m1.1c"><apply id="S5.T1.2.1.1.m1.1.1.cmml" xref="S5.T1.2.1.1.m1.1.1"><eq id="S5.T1.2.1.1.m1.1.1.1.cmml" xref="S5.T1.2.1.1.m1.1.1.1"></eq><ci id="S5.T1.2.1.1.m1.1.1.2.cmml" xref="S5.T1.2.1.1.m1.1.1.2">𝐵</ci><cn type="integer" id="S5.T1.2.1.1.m1.1.1.3.cmml" xref="S5.T1.2.1.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.1.1.m1.1d">B=20</annotation></semantics></math> data points. We replaced a varying fraction of users by sybil devices and measured number of individually extractable data points from the target user and average SNRs over all their reconstructions. The more benign users participate in the round, the less effective data reconstruction becomes.</span></span></figcaption>
</figure>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2301.04017/assets/x4.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F4.5.2.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S5.F4.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Influence of Fraction of Sybil Devices.<span id="S5.F4.2.1.1" class="ltx_text ltx_font_medium"> Visual depiction of the results presented in <a href="#S5.T1" title="In 5.2 Noisy Data Extraction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a> for FL hardened by SSA and DDP with 50 participants, each holding <math id="S5.F4.2.1.1.m1.1" class="ltx_Math" alttext="B=20" display="inline"><semantics id="S5.F4.2.1.1.m1.1b"><mrow id="S5.F4.2.1.1.m1.1.1" xref="S5.F4.2.1.1.m1.1.1.cmml"><mi id="S5.F4.2.1.1.m1.1.1.2" xref="S5.F4.2.1.1.m1.1.1.2.cmml">B</mi><mo id="S5.F4.2.1.1.m1.1.1.1" xref="S5.F4.2.1.1.m1.1.1.1.cmml">=</mo><mn id="S5.F4.2.1.1.m1.1.1.3" xref="S5.F4.2.1.1.m1.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F4.2.1.1.m1.1c"><apply id="S5.F4.2.1.1.m1.1.1.cmml" xref="S5.F4.2.1.1.m1.1.1"><eq id="S5.F4.2.1.1.m1.1.1.1.cmml" xref="S5.F4.2.1.1.m1.1.1.1"></eq><ci id="S5.F4.2.1.1.m1.1.1.2.cmml" xref="S5.F4.2.1.1.m1.1.1.2">𝐵</ci><cn type="integer" id="S5.F4.2.1.1.m1.1.1.3.cmml" xref="S5.F4.2.1.1.m1.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F4.2.1.1.m1.1d">B=20</annotation></semantics></math> data points.</span></span></figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Effect of Fraction of Sybils.</span>
We, furthermore, conducted experiments to quantify the effect of not replacing all other <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="\mathsf{M}-1" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">𝖬</mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">−</mo><mn id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><minus id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></minus><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">𝖬</ci><cn type="integer" id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">\mathsf{M}-1</annotation></semantics></math> users by sybil devices, but only a fraction of the other users.
Therefore, we conducted experiments with extracting data from one round of the FL protocol with 50 participants, each holding a mini-batch of 20 data points.
Our goal was to study what privacy gain the presence of other benign users incurs on the target user.
We visualize our results in <a href="#S5.T1" title="In 5.2 Noisy Data Extraction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">I</span></a> and <a href="#S5.F4" title="In 5.2 Noisy Data Extraction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>.
They suggest that when the target user is sampled solely with sybil devices (row 1), the server is able to extract 95% of their individual training data points individually, protected solely by the noise added locally according to DDP.
The more benign users participate in the protocol round, the lower SNRs of the reconstructed data from the target user, and the fewer of the target user’s individual data points can be individually extracted.
This effect stems from the aggregation within the SA, which overlays gradients from all users before sharing them with the server.
Our results are aligned with findings by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> who showed that averaging over several mini-batches (which is precisely the effect of the SA) degrades extraction success.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">Sufficiently Protective Noise.</span>
Deciding at which point, <em id="S5.SS2.p3.1.2" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.SS2.p3.1.3" class="ltx_text"></span>, under the influence of how much noise, the reconstruction of a data point is sufficiently close to the original data point is orthogonal to this work.
In particular, it will depend on the specific domain, task, and user-preference.
However, users in FL should assume that the server can extract individual data points such as the ones depicted in <a href="#S5.F3" title="In 5.2 Noisy Data Extraction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> from their gradients.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">In the following, we will show how the server can improve the fidelity of extraction by leveraging the redundancy of extractable data due to the trap weights.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span><span id="S5.SS3.1.1" class="ltx_text ltx_font_italic">Redundancy in Extracted Data</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">This section studies redundancy in extracted data of the trap weights method and their effect on the fidelity of reconstructed data.</p>
</div>
<figure id="S5.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.04017/assets/x5.png" id="S5.F5.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="320" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Random weights.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.04017/assets/x6.png" id="S5.F5.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="318" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F5.sf2.3.2" class="ltx_text" style="font-size:90%;">Trap weights.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F5.3.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S5.F5.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Number of Activations per Data Point.<span id="S5.F5.4.2.1" class="ltx_text ltx_font_medium"> The number of times each of the 100 data points is individually extractable from the model gradients.
The same data points are individually extractable from many more different gradients when using trap weights which enable us to use redundancy for better data reconstruction.
Results are averaged over five different random and trap weight model initializations.
</span></span></figcaption>
</figure>
<div id="S5.SS3.p2" class="ltx_para ltx_noindent">
<p id="S5.SS3.p2.1" class="ltx_p"><span id="S5.SS3.p2.1.1" class="ltx_text ltx_font_bold">Redundancy in Extractable Data Points.</span>

We first study direct redundancy by analyzing <em id="S5.SS3.p2.1.2" class="ltx_emph ltx_font_italic">how often</em> each data point in a mini-batch with <math id="S5.SS3.p2.1.m1.1" class="ltx_Math" alttext="B=100" display="inline"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">B</mi><mo id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><eq id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></eq><ci id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">𝐵</ci><cn type="integer" id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">B=100</annotation></semantics></math> is individually extractable from the rescaled gradients.
The results depicted in <a href="#S5.F5" title="In 5.3 Redundancy in Extracted Data ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> suggest that the trap weights, first of all, make more data points individually extractable in contrast to random model initializations, but also cause the same data points to be individually extractable from many more different weight rows’ gradients (up to 70 times over the 1000 neurons and their respective weight rows).</p>
</div>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.04017/assets/x7.png" id="S5.F6.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F6.sf1.3.2" class="ltx_text" style="font-size:90%;">Random weights.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.04017/assets/x8.png" id="S5.F6.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="311" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F6.sf2.3.2" class="ltx_text" style="font-size:90%;">Trap weights.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Number of Activations per Neuron.<span id="S5.F6.4.2.1" class="ltx_text ltx_font_medium"> Number of data points that activate each one of the 1000 neurons.
Individual neurons are activated by fewer data points (less overlay) when using trap weights which enable better data reconstruction.
Results are averaged over five different random and trap weight model initializations.
</span></span></figcaption>
</figure>
<div id="S5.SS3.p3" class="ltx_para ltx_noindent">
<p id="S5.SS3.p3.1" class="ltx_p"><span id="S5.SS3.p3.1.1" class="ltx_text ltx_font_bold">Sparsity in Extractability.</span>

We, furthermore, evaluate <em id="S5.SS3.p3.1.2" class="ltx_emph ltx_font_italic">by how many data points</em> each neuron gets activated.
This is equivalent to the question how many data points cause a positive input to each neuron.
<a href="#S5.F6" title="In 5.3 Redundancy in Extracted Data ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> highlights that with randomly initialized weights, neurons are activated by many more data points than with the trap weights, which causes that many data points overlay in a single gradient and we cannot extract them individually.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">To improve fidelity of reconstruction, we can leverage both the redundancy of extractable data and the sparsity in the extracted data.
By averaging redundant noisy data points, the signal-to-noise ratio (SNR) of reconstructed data increases as noise averages out.
We visualize this effect in <a href="#S5.F7" title="In 5.3 Redundancy in Extracted Data ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>.
Also sparsity can be exploited.
The fewer data points activate a neuron, the fewer data points contained in the overlay of the rescaled gradient.
Hence, each individual data point’s signal is more clearly present and identifiable in the rescaled gradients.
In the following section, we will show how this can be used to yield higher-fidelity reconstruction in the image domain through clustering.</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2301.04017/assets/x9.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="300" height="204" alt="Refer to caption"></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img src="/html/2301.04017/assets/x10.png" id="S5.F7.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="300" height="85" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.9.4.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Averaging out Noise.<span id="S5.F7.6.3.3" class="ltx_text ltx_font_medium"> Mean value over #-many noisy reconstructions of the same data point (bottom); corresponding mean image’s SNR (top).
DDP setup: <math id="S5.F7.4.1.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S5.F7.4.1.1.m1.1b"><mrow id="S5.F7.4.1.1.m1.1.1" xref="S5.F7.4.1.1.m1.1.1.cmml"><mi id="S5.F7.4.1.1.m1.1.1.2" xref="S5.F7.4.1.1.m1.1.1.2.cmml">c</mi><mo id="S5.F7.4.1.1.m1.1.1.1" xref="S5.F7.4.1.1.m1.1.1.1.cmml">=</mo><mn id="S5.F7.4.1.1.m1.1.1.3" xref="S5.F7.4.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F7.4.1.1.m1.1c"><apply id="S5.F7.4.1.1.m1.1.1.cmml" xref="S5.F7.4.1.1.m1.1.1"><eq id="S5.F7.4.1.1.m1.1.1.1.cmml" xref="S5.F7.4.1.1.m1.1.1.1"></eq><ci id="S5.F7.4.1.1.m1.1.1.2.cmml" xref="S5.F7.4.1.1.m1.1.1.2">𝑐</ci><cn type="integer" id="S5.F7.4.1.1.m1.1.1.3.cmml" xref="S5.F7.4.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F7.4.1.1.m1.1d">c=1</annotation></semantics></math>, <math id="S5.F7.5.2.2.m2.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="S5.F7.5.2.2.m2.1b"><mrow id="S5.F7.5.2.2.m2.1.1" xref="S5.F7.5.2.2.m2.1.1.cmml"><mi id="S5.F7.5.2.2.m2.1.1.2" xref="S5.F7.5.2.2.m2.1.1.2.cmml">σ</mi><mo id="S5.F7.5.2.2.m2.1.1.1" xref="S5.F7.5.2.2.m2.1.1.1.cmml">=</mo><mn id="S5.F7.5.2.2.m2.1.1.3" xref="S5.F7.5.2.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F7.5.2.2.m2.1c"><apply id="S5.F7.5.2.2.m2.1.1.cmml" xref="S5.F7.5.2.2.m2.1.1"><eq id="S5.F7.5.2.2.m2.1.1.1.cmml" xref="S5.F7.5.2.2.m2.1.1.1"></eq><ci id="S5.F7.5.2.2.m2.1.1.2.cmml" xref="S5.F7.5.2.2.m2.1.1.2">𝜎</ci><cn type="float" id="S5.F7.5.2.2.m2.1.1.3.cmml" xref="S5.F7.5.2.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F7.5.2.2.m2.1d">\sigma=0.1</annotation></semantics></math>, and <math id="S5.F7.6.3.3.m3.1" class="ltx_Math" alttext="\mathsf{M}=100" display="inline"><semantics id="S5.F7.6.3.3.m3.1b"><mrow id="S5.F7.6.3.3.m3.1.1" xref="S5.F7.6.3.3.m3.1.1.cmml"><mi id="S5.F7.6.3.3.m3.1.1.2" xref="S5.F7.6.3.3.m3.1.1.2.cmml">𝖬</mi><mo id="S5.F7.6.3.3.m3.1.1.1" xref="S5.F7.6.3.3.m3.1.1.1.cmml">=</mo><mn id="S5.F7.6.3.3.m3.1.1.3" xref="S5.F7.6.3.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F7.6.3.3.m3.1c"><apply id="S5.F7.6.3.3.m3.1.1.cmml" xref="S5.F7.6.3.3.m3.1.1"><eq id="S5.F7.6.3.3.m3.1.1.1.cmml" xref="S5.F7.6.3.3.m3.1.1.1"></eq><ci id="S5.F7.6.3.3.m3.1.1.2.cmml" xref="S5.F7.6.3.3.m3.1.1.2">𝖬</ci><cn type="integer" id="S5.F7.6.3.3.m3.1.1.3.cmml" xref="S5.F7.6.3.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F7.6.3.3.m3.1d">\mathsf{M}=100</annotation></semantics></math>.
Over an increasing number of reconstructions, the local noise averages out, yielding higher-fidelity images and increased SNR.</span></span></figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span><span id="S5.SS4.1.1" class="ltx_text ltx_font_italic">Improving Noisy Data Reconstruction</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The previous sections highlight that DDP reduces to LDP with weak privacy guarantees from an individual user’s perspective when other users are untrusted with their noise addition.
In this section, we show how we can even improve data reconstruction in this setup, further amplifying the small signal in the extracted gradients.
We evaluate improvements for data reconstruction from noisy gradients computed under DDP on <span id="S5.SS4.p1.1.1" class="ltx_text ltx_font_italic">image and textual data</span>.
All improvements solely rely on post-processing steps to reduce the effect of the noise.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para ltx_noindent">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Image Data.</span>
Due to the local clipping and noise addition by the users, the data points extracted from the gradients are not perfect reconstructions of the original data points.
We can still improve reconstruction quality by leveraging redundancy and sparsity in the gradients to average out the added noise, as highlighted in the previous section.
However, without knowledge of the users data, the server has no means of determining which data points activate which neurons a priori.
Therefore, it is unclear which rescaled gradients need to be averaged to improve reconstruction fidelity.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2301.04017/assets/x11.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="266" height="125" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.7.3.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Similarity Clustering<span id="S5.F8.4.2.2" class="ltx_text ltx_font_medium"> to improve noisy data extraction. Original data points and average clusters obtained from the rescaled gradients depicted in <a href="#S5.F3" title="In 5.2 Noisy Data Extraction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>. First 10 original training data points from the CIFAR10 dataset (top row).
Averaged clusters of 10 data points reconstructed from the gradients for mini-batch size <math id="S5.F8.3.1.1.m1.1" class="ltx_Math" alttext="B=10" display="inline"><semantics id="S5.F8.3.1.1.m1.1b"><mrow id="S5.F8.3.1.1.m1.1.1" xref="S5.F8.3.1.1.m1.1.1.cmml"><mi id="S5.F8.3.1.1.m1.1.1.2" xref="S5.F8.3.1.1.m1.1.1.2.cmml">B</mi><mo id="S5.F8.3.1.1.m1.1.1.1" xref="S5.F8.3.1.1.m1.1.1.1.cmml">=</mo><mn id="S5.F8.3.1.1.m1.1.1.3" xref="S5.F8.3.1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F8.3.1.1.m1.1c"><apply id="S5.F8.3.1.1.m1.1.1.cmml" xref="S5.F8.3.1.1.m1.1.1"><eq id="S5.F8.3.1.1.m1.1.1.1.cmml" xref="S5.F8.3.1.1.m1.1.1.1"></eq><ci id="S5.F8.3.1.1.m1.1.1.2.cmml" xref="S5.F8.3.1.1.m1.1.1.2">𝐵</ci><cn type="integer" id="S5.F8.3.1.1.m1.1.1.3.cmml" xref="S5.F8.3.1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F8.3.1.1.m1.1d">B=10</annotation></semantics></math> (mid row), and <math id="S5.F8.4.2.2.m2.1" class="ltx_Math" alttext="B=20" display="inline"><semantics id="S5.F8.4.2.2.m2.1b"><mrow id="S5.F8.4.2.2.m2.1.1" xref="S5.F8.4.2.2.m2.1.1.cmml"><mi id="S5.F8.4.2.2.m2.1.1.2" xref="S5.F8.4.2.2.m2.1.1.2.cmml">B</mi><mo id="S5.F8.4.2.2.m2.1.1.1" xref="S5.F8.4.2.2.m2.1.1.1.cmml">=</mo><mn id="S5.F8.4.2.2.m2.1.1.3" xref="S5.F8.4.2.2.m2.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.F8.4.2.2.m2.1c"><apply id="S5.F8.4.2.2.m2.1.1.cmml" xref="S5.F8.4.2.2.m2.1.1"><eq id="S5.F8.4.2.2.m2.1.1.1.cmml" xref="S5.F8.4.2.2.m2.1.1.1"></eq><ci id="S5.F8.4.2.2.m2.1.1.2.cmml" xref="S5.F8.4.2.2.m2.1.1.2">𝐵</ci><cn type="integer" id="S5.F8.4.2.2.m2.1.1.3.cmml" xref="S5.F8.4.2.2.m2.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F8.4.2.2.m2.1d">B=20</annotation></semantics></math> with the first 10 examples depicted (bottom row).
The numbers above the images indicate how many noisy reconstructions were averaged to obtain that image.</span></span></figcaption>
</figure>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.5" class="ltx_p">To overcome this limitation, we employ <span id="S5.SS4.p3.5.1" class="ltx_text ltx_font_italic">similarity clustering</span>.
In this approach, the server first filters out extracted data points with a SNR below <math id="S5.SS4.p3.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S5.SS4.p3.1.m1.1a"><mn id="S5.SS4.p3.1.m1.1.1" xref="S5.SS4.p3.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.m1.1b"><cn type="integer" id="S5.SS4.p3.1.m1.1.1.cmml" xref="S5.SS4.p3.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.m1.1c">1</annotation></semantics></math>.
This prevents too noisy instances from degrading performance.
In the following <a href="#S6" title="6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we will discuss why different extracted data points have different SNRs.
Then, the server runs a simple <math id="S5.SS4.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.p3.2.m2.1a"><mi id="S5.SS4.p3.2.m2.1.1" xref="S5.SS4.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.2.m2.1b"><ci id="S5.SS4.p3.2.m2.1.1.cmml" xref="S5.SS4.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.2.m2.1c">k</annotation></semantics></math>-means clustering on the extracted data, and finally averages all per-cluster data points.
Thereby, we do not only leverage redundancy in individually extracted data points, but also the sparsity.
The signal from gradients that represent an overlay of very few data points can meaningfully contribute to the improved signal.
We evaluate this approach with different noise scales and mini-batch sizes <math id="S5.SS4.p3.3.m3.1" class="ltx_Math" alttext="B" display="inline"><semantics id="S5.SS4.p3.3.m3.1a"><mi id="S5.SS4.p3.3.m3.1.1" xref="S5.SS4.p3.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.3.m3.1b"><ci id="S5.SS4.p3.3.m3.1.1.cmml" xref="S5.SS4.p3.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.3.m3.1c">B</annotation></semantics></math>.
Note that the number <math id="S5.SS4.p3.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS4.p3.4.m4.1a"><mi id="S5.SS4.p3.4.m4.1.1" xref="S5.SS4.p3.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.4.m4.1b"><ci id="S5.SS4.p3.4.m4.1.1.cmml" xref="S5.SS4.p3.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.4.m4.1c">k</annotation></semantics></math> of clusters has to be chosen in accordance with the mini-batch size if we want to be able to reconstruct every data point.
Our evaluation suggests that clustering works best when <math id="S5.SS4.p3.5.m5.1" class="ltx_Math" alttext="k\geq 2B" display="inline"><semantics id="S5.SS4.p3.5.m5.1a"><mrow id="S5.SS4.p3.5.m5.1.1" xref="S5.SS4.p3.5.m5.1.1.cmml"><mi id="S5.SS4.p3.5.m5.1.1.2" xref="S5.SS4.p3.5.m5.1.1.2.cmml">k</mi><mo id="S5.SS4.p3.5.m5.1.1.1" xref="S5.SS4.p3.5.m5.1.1.1.cmml">≥</mo><mrow id="S5.SS4.p3.5.m5.1.1.3" xref="S5.SS4.p3.5.m5.1.1.3.cmml"><mn id="S5.SS4.p3.5.m5.1.1.3.2" xref="S5.SS4.p3.5.m5.1.1.3.2.cmml">2</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p3.5.m5.1.1.3.1" xref="S5.SS4.p3.5.m5.1.1.3.1.cmml">​</mo><mi id="S5.SS4.p3.5.m5.1.1.3.3" xref="S5.SS4.p3.5.m5.1.1.3.3.cmml">B</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.5.m5.1b"><apply id="S5.SS4.p3.5.m5.1.1.cmml" xref="S5.SS4.p3.5.m5.1.1"><geq id="S5.SS4.p3.5.m5.1.1.1.cmml" xref="S5.SS4.p3.5.m5.1.1.1"></geq><ci id="S5.SS4.p3.5.m5.1.1.2.cmml" xref="S5.SS4.p3.5.m5.1.1.2">𝑘</ci><apply id="S5.SS4.p3.5.m5.1.1.3.cmml" xref="S5.SS4.p3.5.m5.1.1.3"><times id="S5.SS4.p3.5.m5.1.1.3.1.cmml" xref="S5.SS4.p3.5.m5.1.1.3.1"></times><cn type="integer" id="S5.SS4.p3.5.m5.1.1.3.2.cmml" xref="S5.SS4.p3.5.m5.1.1.3.2">2</cn><ci id="S5.SS4.p3.5.m5.1.1.3.3.cmml" xref="S5.SS4.p3.5.m5.1.1.3.3">𝐵</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.5.m5.1c">k\geq 2B</annotation></semantics></math>.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.3" class="ltx_p">In <a href="#S5.F8" title="In 5.4 Improving Noisy Data Reconstruction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a>, we depict the results of our clustering on data points from the CIFAR10 dataset with a DDP setup with <math id="S5.SS4.p4.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S5.SS4.p4.1.m1.1a"><mrow id="S5.SS4.p4.1.m1.1.1" xref="S5.SS4.p4.1.m1.1.1.cmml"><mi id="S5.SS4.p4.1.m1.1.1.2" xref="S5.SS4.p4.1.m1.1.1.2.cmml">c</mi><mo id="S5.SS4.p4.1.m1.1.1.1" xref="S5.SS4.p4.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS4.p4.1.m1.1.1.3" xref="S5.SS4.p4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.1.m1.1b"><apply id="S5.SS4.p4.1.m1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1"><eq id="S5.SS4.p4.1.m1.1.1.1.cmml" xref="S5.SS4.p4.1.m1.1.1.1"></eq><ci id="S5.SS4.p4.1.m1.1.1.2.cmml" xref="S5.SS4.p4.1.m1.1.1.2">𝑐</ci><cn type="integer" id="S5.SS4.p4.1.m1.1.1.3.cmml" xref="S5.SS4.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.1.m1.1c">c=1</annotation></semantics></math>, <math id="S5.SS4.p4.2.m2.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="S5.SS4.p4.2.m2.1a"><mrow id="S5.SS4.p4.2.m2.1.1" xref="S5.SS4.p4.2.m2.1.1.cmml"><mi id="S5.SS4.p4.2.m2.1.1.2" xref="S5.SS4.p4.2.m2.1.1.2.cmml">σ</mi><mo id="S5.SS4.p4.2.m2.1.1.1" xref="S5.SS4.p4.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS4.p4.2.m2.1.1.3" xref="S5.SS4.p4.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.2.m2.1b"><apply id="S5.SS4.p4.2.m2.1.1.cmml" xref="S5.SS4.p4.2.m2.1.1"><eq id="S5.SS4.p4.2.m2.1.1.1.cmml" xref="S5.SS4.p4.2.m2.1.1.1"></eq><ci id="S5.SS4.p4.2.m2.1.1.2.cmml" xref="S5.SS4.p4.2.m2.1.1.2">𝜎</ci><cn type="float" id="S5.SS4.p4.2.m2.1.1.3.cmml" xref="S5.SS4.p4.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.2.m2.1c">\sigma=0.1</annotation></semantics></math> and <math id="S5.SS4.p4.3.m3.1" class="ltx_Math" alttext="\mathsf{M}=100" display="inline"><semantics id="S5.SS4.p4.3.m3.1a"><mrow id="S5.SS4.p4.3.m3.1.1" xref="S5.SS4.p4.3.m3.1.1.cmml"><mi id="S5.SS4.p4.3.m3.1.1.2" xref="S5.SS4.p4.3.m3.1.1.2.cmml">𝖬</mi><mo id="S5.SS4.p4.3.m3.1.1.1" xref="S5.SS4.p4.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS4.p4.3.m3.1.1.3" xref="S5.SS4.p4.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p4.3.m3.1b"><apply id="S5.SS4.p4.3.m3.1.1.cmml" xref="S5.SS4.p4.3.m3.1.1"><eq id="S5.SS4.p4.3.m3.1.1.1.cmml" xref="S5.SS4.p4.3.m3.1.1.1"></eq><ci id="S5.SS4.p4.3.m3.1.1.2.cmml" xref="S5.SS4.p4.3.m3.1.1.2">𝖬</ci><cn type="integer" id="S5.SS4.p4.3.m3.1.1.3.cmml" xref="S5.SS4.p4.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p4.3.m3.1c">\mathsf{M}=100</annotation></semantics></math>.
The top row depicts 10 original data points, the mid and bottom rows show the closest averaged clusters for mini-batches of size 10, and 20 respectively.
The more instances are available for averaging, the better the resulting per-cluster averages.</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2301.04017/assets/x12.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.5.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Textual Data Extraction under Noise.<span id="S5.F9.6.2.1" class="ltx_text ltx_font_medium"> Extraction performance under noise for DDP from language model on the IMDB dataset. Extraction remains successful, even in presence of noise. Occasional drops in performance occur because of near-zero gradients resulted from correct data classification, <em id="S5.F9.6.2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S5.F9.6.2.1.2" class="ltx_text"></span> data points with very low original loss. Error bars correspond to a single standard deviation.</span></span></figcaption>
</figure>
<div id="S5.SS4.p5" class="ltx_para ltx_noindent">
<p id="S5.SS4.p5.1" class="ltx_p"><span id="S5.SS4.p5.1.1" class="ltx_text ltx_font_bold">Textual Data.</span>

For the text classifier on IMDB, we initialize the weights of the embedding layer with a random uniform distribution (minimum=0.0,maximum=1.0) to create the inputs for the fully-connected layer, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
We then adversarially initialize this fully-connected-layer’s weights with the trap weights to perform extraction of the embeddings and invert the embeddings back to tokens using a lookup dictionary.
In the presence of noise introduced for DDP, the extracted embeddings are slightly noisy.
To overcome this, in presence of noise we perform the lookup by searching for the token with the closest embedding measured through the <math id="S5.SS4.p5.1.m1.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S5.SS4.p5.1.m1.1a"><msub id="S5.SS4.p5.1.m1.1.1" xref="S5.SS4.p5.1.m1.1.1.cmml"><mi mathvariant="normal" id="S5.SS4.p5.1.m1.1.1.2" xref="S5.SS4.p5.1.m1.1.1.2.cmml">ℓ</mi><mn id="S5.SS4.p5.1.m1.1.1.3" xref="S5.SS4.p5.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p5.1.m1.1b"><apply id="S5.SS4.p5.1.m1.1.1.cmml" xref="S5.SS4.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p5.1.m1.1.1.1.cmml" xref="S5.SS4.p5.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.p5.1.m1.1.1.2.cmml" xref="S5.SS4.p5.1.m1.1.1.2">ℓ</ci><cn type="integer" id="S5.SS4.p5.1.m1.1.1.3.cmml" xref="S5.SS4.p5.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p5.1.m1.1c">\ell_{2}</annotation></semantics></math> distance.
<a href="#S5.F9" title="In 5.4 Improving Noisy Data Reconstruction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a> shows performance of a single mini-batch language extraction in presence of DP. Just as with image data, here an attacker is capable of extracting the original sentence of the users, despite the applied noise.
We do observe however that there is stochasticity involved—when parametrization does well on the data point by default, extraction gets low performance since the received gradient has an extremely low magnitude and the corresponding signal gets dominated by the noise. We turn to this phenomenon in the next section.
Supplementary results for the two additional text datasets can be found in Appendix <a href="#A2.SS3" title="B.3 Experimental Results on Textual Data ‣ Appendix B Extended Experimental Evaluation ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>.

<br class="ltx_break"></p>
</div>
<div id="S5.SS4.p6" class="ltx_para ltx_noindent">
<p id="S5.SS4.p6.1" class="ltx_p">To summarize the results on image and textual data, we find that:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">The trap weights <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> cause input data-diversity and redundancy in resulting gradients, which can be used to cancel out some of the applied noise.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">NLP is not safe from attacks described in this paper, despite a more sophisticated input-embeddings mapping.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Despite using DDP, an attacker often can reconstruct semantic information on the individual user data points. This is because in the presence of untrusted other users, DDP reduces to LDP with weak privacy guarantees from the perspective of an individual user.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">As shown in <a href="#S4.F2" title="In 4.2 Exploiting DDP Guarantees ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, having users add more noise locally, without additional improvements of the protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> comes with a significant decrease in utility which makes the solution less practical.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Disparate Leakage over Model Gradients</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Throughout our experiments, we observe that with the exact same scale of noise added to all gradients, some extracted data points have a significantly higher SNR than others.
This effect translates into different levels of semantic similarity in the extracted data with respect to the original data as we show in <a href="#S5.F3" title="In 5.2 Noisy Data Extraction ‣ 5 Evaluation of the Attack against SA+DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>.
In this section, we explain this observation and sketch how it can be leveraged by the adversary to better extract data in the presence of noise.</p>
</div>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2301.04017/assets/x13.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F10.11.4.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S6.F10.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Gradient Norm vs. SNR.<span id="S6.F10.6.3.3" class="ltx_text ltx_font_medium"> Norm of the clipped and noised gradients of 1000 weight rows against SNR in corresponding extracted data point, <em id="S6.F10.6.3.3.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S6.F10.6.3.3.2" class="ltx_text"></span> the rescaled gradients. With higher gradient norms, the SNR in the extracted data increases. DDP setup: <math id="S6.F10.4.1.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S6.F10.4.1.1.m1.1b"><mrow id="S6.F10.4.1.1.m1.1.1" xref="S6.F10.4.1.1.m1.1.1.cmml"><mi id="S6.F10.4.1.1.m1.1.1.2" xref="S6.F10.4.1.1.m1.1.1.2.cmml">c</mi><mo id="S6.F10.4.1.1.m1.1.1.1" xref="S6.F10.4.1.1.m1.1.1.1.cmml">=</mo><mn id="S6.F10.4.1.1.m1.1.1.3" xref="S6.F10.4.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F10.4.1.1.m1.1c"><apply id="S6.F10.4.1.1.m1.1.1.cmml" xref="S6.F10.4.1.1.m1.1.1"><eq id="S6.F10.4.1.1.m1.1.1.1.cmml" xref="S6.F10.4.1.1.m1.1.1.1"></eq><ci id="S6.F10.4.1.1.m1.1.1.2.cmml" xref="S6.F10.4.1.1.m1.1.1.2">𝑐</ci><cn type="integer" id="S6.F10.4.1.1.m1.1.1.3.cmml" xref="S6.F10.4.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F10.4.1.1.m1.1d">c=1</annotation></semantics></math>, <math id="S6.F10.5.2.2.m2.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="S6.F10.5.2.2.m2.1b"><mrow id="S6.F10.5.2.2.m2.1.1" xref="S6.F10.5.2.2.m2.1.1.cmml"><mi id="S6.F10.5.2.2.m2.1.1.2" xref="S6.F10.5.2.2.m2.1.1.2.cmml">σ</mi><mo id="S6.F10.5.2.2.m2.1.1.1" xref="S6.F10.5.2.2.m2.1.1.1.cmml">=</mo><mn id="S6.F10.5.2.2.m2.1.1.3" xref="S6.F10.5.2.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F10.5.2.2.m2.1c"><apply id="S6.F10.5.2.2.m2.1.1.cmml" xref="S6.F10.5.2.2.m2.1.1"><eq id="S6.F10.5.2.2.m2.1.1.1.cmml" xref="S6.F10.5.2.2.m2.1.1.1"></eq><ci id="S6.F10.5.2.2.m2.1.1.2.cmml" xref="S6.F10.5.2.2.m2.1.1.2">𝜎</ci><cn type="float" id="S6.F10.5.2.2.m2.1.1.3.cmml" xref="S6.F10.5.2.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F10.5.2.2.m2.1d">\sigma=0.1</annotation></semantics></math>, and <math id="S6.F10.6.3.3.m3.1" class="ltx_Math" alttext="\mathsf{M}=100" display="inline"><semantics id="S6.F10.6.3.3.m3.1b"><mrow id="S6.F10.6.3.3.m3.1.1" xref="S6.F10.6.3.3.m3.1.1.cmml"><mi id="S6.F10.6.3.3.m3.1.1.2" xref="S6.F10.6.3.3.m3.1.1.2.cmml">𝖬</mi><mo id="S6.F10.6.3.3.m3.1.1.1" xref="S6.F10.6.3.3.m3.1.1.1.cmml">=</mo><mn id="S6.F10.6.3.3.m3.1.1.3" xref="S6.F10.6.3.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F10.6.3.3.m3.1c"><apply id="S6.F10.6.3.3.m3.1.1.cmml" xref="S6.F10.6.3.3.m3.1.1"><eq id="S6.F10.6.3.3.m3.1.1.1.cmml" xref="S6.F10.6.3.3.m3.1.1.1"></eq><ci id="S6.F10.6.3.3.m3.1.1.2.cmml" xref="S6.F10.6.3.3.m3.1.1.2">𝖬</ci><cn type="integer" id="S6.F10.6.3.3.m3.1.1.3.cmml" xref="S6.F10.6.3.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F10.6.3.3.m3.1d">\mathsf{M}=100</annotation></semantics></math>.</span></span></figcaption>
</figure>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span><span id="S6.SS1.1.1" class="ltx_text ltx_font_italic">Impact of Gradient Norm</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">We find that the SNR of an extracted data point is tightly bound to the magnitude, <em id="S6.SS1.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S6.SS1.p1.1.2" class="ltx_text"></span> the norm, of the respective gradients.
In <a href="#S6.F10" title="In 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">10</span></a>, we depict the SNR in the rescaled clipped and noised gradients, <em id="S6.SS1.p1.1.3" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S6.SS1.p1.1.4" class="ltx_text"></span>, the extracted data points, against their respective gradient norms.
The figure shows that with higher magnitude gradients, the same amount of noise has less impact on the signal, whereas, with smaller magnitude gradients, the same amount of noise largely dominates the signal.
Therefore, increased magnitude of model gradients results in an increased data leakage.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">The norm of a weight row’s gradients in the model depends on the model’s loss.
In general, higher loss results in higher magnitude gradients, in particular for the weight rows that most contribute to the loss.
Intuitively, to increase data leakage from noisy gradients, the server could, therefore, manipulate the shared model to produce higher loss.
In the best case, the loss would be caused by all weight rows in the fully-connected layer used for extraction with the trap weights.
This ensures high-magnitude gradients at all the weight rows’ gradients and, thereby, enables enhanced extraction at all of them.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span><span id="S6.SS2.1.1" class="ltx_text ltx_font_italic">Global vs. Local Effect of Clipping</span>
</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.3" class="ltx_p">However, in DDP, before noise addition, users perform a clipping step, bounding the maximum per-layer gradient norm, and hence the extractable signal from a gradient update.
More precisely, clipping bounds the total norm of a model layer’s gradients to the clipping parameter <math id="S6.SS2.p1.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S6.SS2.p1.1.m1.1a"><mi id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><ci id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">c</annotation></semantics></math>.
If all weight rows have high gradients, their joint norm will exceed <math id="S6.SS2.p1.2.m2.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S6.SS2.p1.2.m2.1a"><mi id="S6.SS2.p1.2.m2.1.1" xref="S6.SS2.p1.2.m2.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.2.m2.1b"><ci id="S6.SS2.p1.2.m2.1.1.cmml" xref="S6.SS2.p1.2.m2.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.2.m2.1c">c</annotation></semantics></math>, and therefore, all of them will have to be scaled down to reduce the total norm to <math id="S6.SS2.p1.3.m3.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S6.SS2.p1.3.m3.1a"><mi id="S6.SS2.p1.3.m3.1.1" xref="S6.SS2.p1.3.m3.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.3.m3.1b"><ci id="S6.SS2.p1.3.m3.1.1.cmml" xref="S6.SS2.p1.3.m3.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.3.m3.1c">c</annotation></semantics></math>.
The effect is visualized in the middle row of <a href="#S6.F11" title="In 6.2 Global vs. Local Effect of Clipping ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>.
It shows that in this scenario, the extracted data over all gradients has a relatively low signal, which yields low-fidelity reconstruction.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Even though with DP and clipping, it is not possible to have high magnitude gradients over <span id="S6.SS2.p2.1.1" class="ltx_text ltx_font_italic">all</span> weight rows, we note that the clipping is performed <span id="S6.SS2.p2.1.2" class="ltx_text ltx_font_italic">globally</span> per-model layer.
Hence, if only a few weight rows <span id="S6.SS2.p2.1.3" class="ltx_text ltx_font_italic">locally</span> have a high magnitude gradient but all other weight rows have a low magnitude gradient, then their joint norm can be below <math id="S6.SS2.p2.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S6.SS2.p2.1.m1.1a"><mi id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><ci id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">c</annotation></semantics></math>.
As a consequence, no clipping will be performed.
The effect of this scenario is visualized in the bottom row of <a href="#S6.F11" title="In 6.2 Global vs. Local Effect of Clipping ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>.
It highlights that while most gradients yield pure-noise reconstruction, a few gradients contain a very high-fidelity reconstruction of the input data.
These gradients correspond to individual neurons whose gradients were less affected by the clipping operation due to the local vs. global effect we described.
This higher vulnerability of certain neurons is desirable for improved data reconstruction under DDP.</p>
</div>
<figure id="S6.F11" class="ltx_figure"><img src="/html/2301.04017/assets/x14.png" id="S6.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="332" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F11.12.4.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S6.F11.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">Extraction Success with Additional Model Manipulations.<span id="S6.F11.6.3.4" class="ltx_text ltx_font_medium">
</span>Row 1 (top):<span id="S6.F11.6.3.5" class="ltx_text ltx_font_medium"> Under trap weights only (baseline), gradients at different weight rows have varying SNRs under the same amount of added noise, depending on their magnitudes. </span>Row 2 (middle):<span id="S6.F11.6.3.6" class="ltx_text ltx_font_medium"> When the shared model is further manipulated (<a href="#S6.SS3" title="6.3 Exploiting Global Clipping for Increased Extractability ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">6.3</span></a>) and all weight rows contribute equally to a high loss, their gradients will be clipped, which results in equal information loss for all of them.
</span>Row 3 (bottom):<span id="S6.F11.6.3.3" class="ltx_text ltx_font_medium"> When only a few weight rows contribute to a high loss, their gradients preserve a high magnitude over clipping, which allows for higher fidelity extraction. DDP setup: <math id="S6.F11.4.1.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S6.F11.4.1.1.m1.1b"><mrow id="S6.F11.4.1.1.m1.1.1" xref="S6.F11.4.1.1.m1.1.1.cmml"><mi id="S6.F11.4.1.1.m1.1.1.2" xref="S6.F11.4.1.1.m1.1.1.2.cmml">c</mi><mo id="S6.F11.4.1.1.m1.1.1.1" xref="S6.F11.4.1.1.m1.1.1.1.cmml">=</mo><mn id="S6.F11.4.1.1.m1.1.1.3" xref="S6.F11.4.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F11.4.1.1.m1.1c"><apply id="S6.F11.4.1.1.m1.1.1.cmml" xref="S6.F11.4.1.1.m1.1.1"><eq id="S6.F11.4.1.1.m1.1.1.1.cmml" xref="S6.F11.4.1.1.m1.1.1.1"></eq><ci id="S6.F11.4.1.1.m1.1.1.2.cmml" xref="S6.F11.4.1.1.m1.1.1.2">𝑐</ci><cn type="integer" id="S6.F11.4.1.1.m1.1.1.3.cmml" xref="S6.F11.4.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F11.4.1.1.m1.1d">c=1</annotation></semantics></math>, <math id="S6.F11.5.2.2.m2.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="S6.F11.5.2.2.m2.1b"><mrow id="S6.F11.5.2.2.m2.1.1" xref="S6.F11.5.2.2.m2.1.1.cmml"><mi id="S6.F11.5.2.2.m2.1.1.2" xref="S6.F11.5.2.2.m2.1.1.2.cmml">σ</mi><mo id="S6.F11.5.2.2.m2.1.1.1" xref="S6.F11.5.2.2.m2.1.1.1.cmml">=</mo><mn id="S6.F11.5.2.2.m2.1.1.3" xref="S6.F11.5.2.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F11.5.2.2.m2.1c"><apply id="S6.F11.5.2.2.m2.1.1.cmml" xref="S6.F11.5.2.2.m2.1.1"><eq id="S6.F11.5.2.2.m2.1.1.1.cmml" xref="S6.F11.5.2.2.m2.1.1.1"></eq><ci id="S6.F11.5.2.2.m2.1.1.2.cmml" xref="S6.F11.5.2.2.m2.1.1.2">𝜎</ci><cn type="float" id="S6.F11.5.2.2.m2.1.1.3.cmml" xref="S6.F11.5.2.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F11.5.2.2.m2.1d">\sigma=0.1</annotation></semantics></math>, and <math id="S6.F11.6.3.3.m3.1" class="ltx_Math" alttext="\mathsf{M}=10" display="inline"><semantics id="S6.F11.6.3.3.m3.1b"><mrow id="S6.F11.6.3.3.m3.1.1" xref="S6.F11.6.3.3.m3.1.1.cmml"><mi id="S6.F11.6.3.3.m3.1.1.2" xref="S6.F11.6.3.3.m3.1.1.2.cmml">𝖬</mi><mo id="S6.F11.6.3.3.m3.1.1.1" xref="S6.F11.6.3.3.m3.1.1.1.cmml">=</mo><mn id="S6.F11.6.3.3.m3.1.1.3" xref="S6.F11.6.3.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F11.6.3.3.m3.1c"><apply id="S6.F11.6.3.3.m3.1.1.cmml" xref="S6.F11.6.3.3.m3.1.1"><eq id="S6.F11.6.3.3.m3.1.1.1.cmml" xref="S6.F11.6.3.3.m3.1.1.1"></eq><ci id="S6.F11.6.3.3.m3.1.1.2.cmml" xref="S6.F11.6.3.3.m3.1.1.2">𝖬</ci><cn type="integer" id="S6.F11.6.3.3.m3.1.1.3.cmml" xref="S6.F11.6.3.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F11.6.3.3.m3.1d">\mathsf{M}=10</annotation></semantics></math>.</span></span></figcaption>
</figure>
<figure id="S6.F12" class="ltx_figure"><img src="/html/2301.04017/assets/x15.png" id="S6.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F12.3.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S6.F12.4.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Amplification of Gradient Magnitudes<span id="S6.F12.4.2.1" class="ltx_text ltx_font_medium">. The misclassification of an example to the added class increases the magnitude of its gradient for weight rows that contribute to the loss.</span></span></figcaption>
</figure>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span id="S6.SS3.1.1" class="ltx_text ltx_font_italic">Exploiting Global Clipping for Increased Extractability</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">The previous section highlights that the <span id="S6.SS3.p1.1.1" class="ltx_text ltx_font_italic">global</span> per-layer clipping in DP still allows <span id="S6.SS3.p1.1.2" class="ltx_text ltx_font_italic">local</span> parts of the gradients to be large.
This can be exploited for higher-fidelity data extraction from neurons corresponding to these gradients.
In this section, we sketch the idea for a possible model manipulation that gives an attacker the control on local gradient magnitudes to amplify this effect.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">We base our manipulation on a fully-connected neural network with two layers.
The first layer is initialized with the trap weights for extraction and has a ReLU activation function.
The second classification layer is modified to yield high loss (without knowledge of the user data).
Therefore, we add an additional neuron to the classification layer, <em id="S6.SS3.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S6.SS3.p2.1.2" class="ltx_text"></span>, an additional class that does not occur in the data distribution.
Then, we set most of the weights connecting the output of the previous layers’ neurons to this additional class to very small values, <em id="S6.SS3.p2.1.3" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.SS3.p2.1.4" class="ltx_text"></span> zero, and the weights for a few neurons’ output to high values, <em id="S6.SS3.p2.1.5" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S6.SS3.p2.1.6" class="ltx_text"></span> one.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">Due to the ReLU activation of the first layer, this layer’s outputs are positive.
High weights, connecting neurons to the added class in the second layer, ”attribute” the loss of the misclassification to a few trap weight rows.
As a consequence, only the gradients of these few trap weight rows obtain a high magnitude (as illustrated in <a href="#S6.F12" title="In 6.2 Global vs. Local Effect of Clipping ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a> for the <span id="S6.SS3.p3.1.1" class="ltx_text ltx_font_italic">Trap Weight Row 2</span>).
All other trap weight rows (row 1 and n in <a href="#S6.F12" title="In 6.2 Global vs. Local Effect of Clipping ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">12</span></a>) have low-magnitude gradients.
The overall norm of the layer’s gradients will stay below the clipping parameter <math id="S6.SS3.p3.1.m1.1" class="ltx_Math" alttext="c" display="inline"><semantics id="S6.SS3.p3.1.m1.1a"><mi id="S6.SS3.p3.1.m1.1.1" xref="S6.SS3.p3.1.m1.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S6.SS3.p3.1.m1.1b"><ci id="S6.SS3.p3.1.m1.1.1.cmml" xref="S6.SS3.p3.1.m1.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p3.1.m1.1c">c</annotation></semantics></math>, hence no clipping will be applied, enabling high-fidelity data extraction from the <span id="S6.SS3.p3.1.2" class="ltx_text ltx_font_italic">Trap Weight Row 2</span>.</p>
</div>
<div id="S6.SS3.p4" class="ltx_para">
<p id="S6.SS3.p4.1" class="ltx_p">We instantiated the above construction with a fully-connected neural network consisting of 1000 neurons in the first layer and eleven neurons in the second layer for experimental evaluation.
The first layer’s weights were initialized with trap weights, while the second layer was initialized with a Glorot uniform distribution.
We then manipulated the weights connecting to the eleventh (added) class and set varying fractions (10%, 30%, 50%, and 100%) of them to one and the rest to zero.
Using the CIFAR10 dataset, we performed data reconstruction.</p>
</div>
<div id="S6.SS3.p5" class="ltx_para">
<p id="S6.SS3.p5.1" class="ltx_p">In <a href="#S6.F11" title="In 6.2 Global vs. Local Effect of Clipping ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>, we visualize the extracted data.
For the top row, all weights of the second layer are initialized at random.
In the middle row, 100% of the weights connecting to the added class are set to one, and in the bottom row, 10% of these weights are set to one and the rest to zero.</p>
</div>
<figure id="S6.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S6.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.04017/assets/x16.png" id="S6.F13.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F13.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S6.F13.sf1.3.2" class="ltx_text" style="font-size:90%;">100</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S6.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.04017/assets/x17.png" id="S6.F13.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F13.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S6.F13.sf2.3.2" class="ltx_text" style="font-size:90%;">300</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S6.F13.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.04017/assets/x18.png" id="S6.F13.sf3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F13.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S6.F13.sf3.3.2" class="ltx_text" style="font-size:90%;">500</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<figure id="S6.F13.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2301.04017/assets/x19.png" id="S6.F13.sf4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="302" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F13.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S6.F13.sf4.3.2" class="ltx_text" style="font-size:90%;">1000</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S6.F13.12.4.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S6.F13.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">SNRs of Rescaled Clipped and Noised Gradients,<span id="S6.F13.6.3.3" class="ltx_text ltx_font_medium"> <em id="S6.F13.6.3.3.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S6.F13.6.3.3.2" class="ltx_text"></span> the extracted data points. The first fully-connected layer used for extraction consists of 1000 neurons. The respective weight rows are initialized with our trap weights. The second layer is manipulated by adding an additional class neuron, and setting {100, 300, 500, 1000} of the 1000 weights that are connected to this neuron to one. The remaining weights that go to this neuron are set to zero. The <span id="S6.F13.6.3.3.3" class="ltx_text ltx_font_italic">original</span> baseline consists in randomly initialized weights for second fully-connected model layer. Noise with scale <math id="S6.F13.4.1.1.m1.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S6.F13.4.1.1.m1.1b"><mn id="S6.F13.4.1.1.m1.1.1" xref="S6.F13.4.1.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S6.F13.4.1.1.m1.1c"><cn type="float" id="S6.F13.4.1.1.m1.1.1.cmml" xref="S6.F13.4.1.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F13.4.1.1.m1.1d">0.001</annotation></semantics></math> is added to all gradients; clipping parameter <math id="S6.F13.5.2.2.m2.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="S6.F13.5.2.2.m2.1b"><mrow id="S6.F13.5.2.2.m2.1.1" xref="S6.F13.5.2.2.m2.1.1.cmml"><mi id="S6.F13.5.2.2.m2.1.1.2" xref="S6.F13.5.2.2.m2.1.1.2.cmml">c</mi><mo id="S6.F13.5.2.2.m2.1.1.1" xref="S6.F13.5.2.2.m2.1.1.1.cmml">=</mo><mn id="S6.F13.5.2.2.m2.1.1.3" xref="S6.F13.5.2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F13.5.2.2.m2.1c"><apply id="S6.F13.5.2.2.m2.1.1.cmml" xref="S6.F13.5.2.2.m2.1.1"><eq id="S6.F13.5.2.2.m2.1.1.1.cmml" xref="S6.F13.5.2.2.m2.1.1.1"></eq><ci id="S6.F13.5.2.2.m2.1.1.2.cmml" xref="S6.F13.5.2.2.m2.1.1.2">𝑐</ci><cn type="integer" id="S6.F13.5.2.2.m2.1.1.3.cmml" xref="S6.F13.5.2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F13.5.2.2.m2.1d">c=1</annotation></semantics></math>.
When fewer weight rows (in this case 100) contribute to the high loss, the data points extracted from the respective rescaled gradients have the highest SNR (<math id="S6.F13.6.3.3.m3.1" class="ltx_Math" alttext="&gt;1.5" display="inline"><semantics id="S6.F13.6.3.3.m3.1b"><mrow id="S6.F13.6.3.3.m3.1.1" xref="S6.F13.6.3.3.m3.1.1.cmml"><mi id="S6.F13.6.3.3.m3.1.1.2" xref="S6.F13.6.3.3.m3.1.1.2.cmml"></mi><mo id="S6.F13.6.3.3.m3.1.1.1" xref="S6.F13.6.3.3.m3.1.1.1.cmml">&gt;</mo><mn id="S6.F13.6.3.3.m3.1.1.3" xref="S6.F13.6.3.3.m3.1.1.3.cmml">1.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F13.6.3.3.m3.1c"><apply id="S6.F13.6.3.3.m3.1.1.cmml" xref="S6.F13.6.3.3.m3.1.1"><gt id="S6.F13.6.3.3.m3.1.1.1.cmml" xref="S6.F13.6.3.3.m3.1.1.1"></gt><csymbol cd="latexml" id="S6.F13.6.3.3.m3.1.1.2.cmml" xref="S6.F13.6.3.3.m3.1.1.2">absent</csymbol><cn type="float" id="S6.F13.6.3.3.m3.1.1.3.cmml" xref="S6.F13.6.3.3.m3.1.1.3">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F13.6.3.3.m3.1d">&gt;1.5</annotation></semantics></math>), which allows for higher fidelity extraction (compare to Row 3 (bottom) in <a href="#S6.F11" title="In 6.2 Global vs. Local Effect of Clipping ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>).
</span></span></figcaption>
</figure>
<div id="S6.SS3.p6" class="ltx_para">
<p id="S6.SS3.p6.1" class="ltx_p">We furthermore measured the SNRs of the extracted data and depict the results in <a href="#S6.F13" title="In 6.3 Exploiting Global Clipping for Increased Extractability ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13</span></a>.
The results are consistent to the observations from <a href="#S6.F11" title="In 6.2 Global vs. Local Effect of Clipping ‣ 6 Disparate Leakage over Model Gradients ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">11</span></a>:
When few weight rows contribute to the high loss, their respective rescaled gradients, <em id="S6.SS3.p6.1.1" class="ltx_emph ltx_font_italic">i.e</em>.<span id="S6.SS3.p6.1.2" class="ltx_text"></span> the extracted data points, have a higher SNR.
This allows for higher-fidelity extraction.
In contrast, when all weight rows contribute equally to the high loss, all their rescaled gradients have a similar (lower) SNR.
This is because of all their gradients being (equally) affected by the clipping.
In general, the more weight rows contribute to a high loss, the lower their individual SNRs.</p>
</div>
<div id="S6.SS3.p7" class="ltx_para">
<p id="S6.SS3.p7.1" class="ltx_p">Our two-layer construction naturally integrates with other architectures starting with convolutional and embedding layers described in this work.
We leave fine-tuning and the extension of our construction to architectures that end with more than two fully-connected layers for future work.
However, the approach highlights that even when DDP is in place, the server can initialize a model to increase the likelihood of reconstructing points with high fidelity.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">We survey related work on privacy attacks against model gradients, in particular in the setup of FL.</p>
</div>
<div id="S7.p2" class="ltx_para ltx_noindent">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Passive Attacks against Vanilla FL.</span>

Phong <em id="S7.p2.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p2.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> were the first to show how gradients leak information that can be used to recover training data at single neurons or linear layers. Recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite> proposed exploiting this leakage for data reconstruction, for example, through Generative Adversarial Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> (GANs), or by solving a second order optimization problem.</p>
</div>
<div id="S7.p3" class="ltx_para ltx_noindent">
<p id="S7.p3.1" class="ltx_p"><span id="S7.p3.1.1" class="ltx_text ltx_font_bold">Active Attacks against Vanilla FL.</span>
Melis <em id="S7.p3.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p3.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> proposed membership <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite> and property inference <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> attacks based on periodically analyzing the model updates in an FL setup.
They consider both passive and active attackers in vanilla FL.
Nasr <em id="S7.p3.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p3.1.5" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> assume active attackers (both users and server) for membership inference.
Similarly to Melis <em id="S7.p3.1.6" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p3.1.7" class="ltx_text"></span>, their attackers do not directly alter the shared model, but rather manipulate it through model updates (users through gradients, and the server by modifying the aggregated model update).
Recent work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite>, have shown that manipulating the shared model allows a malicious server to extract user data perfectly from the model gradients.
In contrast to all these attacks that consider vanilla FL, our work attacks FL with additional extension for dedicated privacy protection through DDP and SA.
As we discussed in <a href="#S4.SS3" title="4.3 Reconstructing Data ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>, the extraction attacks for vanilla FL can be integrated into our attack flow for improved data extraction.
We show this using <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>’s trap weights.</p>
</div>
<div id="S7.p4" class="ltx_para ltx_noindent">
<p id="S7.p4.1" class="ltx_p"><span id="S7.p4.1.1" class="ltx_text ltx_font_bold">Attacks against Hardened FL.</span>
To our knowledge, the only prior work in this vein is due to Pasquini <em id="S7.p4.1.2" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p4.1.3" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.
This work noticed that in SA the server has the ability to dispatch inconsistent models to different users, and used this to circumvent SA entirely.
As we note in <a href="#S4.SS1" title="4.1 Circumventing SA ‣ 4 Attacking FL under SA and DDP ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>, their attack against SA is not able to circumvent DDP since even when all users’s models but the target user’s model produce zero gradients, benign users would still add their share of noise.
Thereby, privacy guarantees through DDP can still be achieved.
Finally, while Pasquini <em id="S7.p4.1.4" class="ltx_emph ltx_font_italic">et al</em>.<span id="S7.p4.1.5" class="ltx_text"></span> focuses on leveraging a specific capability of the server to circumvent a specific mechanism, we systematically study the server’s capabilities arising from FL’s pervasive centralization, and consequently offer a rich breadth of contributions over this work.
We suggest a simpler and more powerful attack that relies on sybils introduced by the server to circumvent SA, compose this attack with other attacks relying on other capabilities to extract individual data points (the attack by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> only extracts updates, not individual user data points) and attack a variant that also includes DDP.</p>
</div>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span><span id="S8.1.1" class="ltx_text ltx_font_smallcaps">Discussion: Privacy-Preserving FL</span>
</h2>

<div id="S8.p1" class="ltx_para">
<p id="S8.p1.1" class="ltx_p">In this section, we discuss the following three main questions:</p>
<ul id="S8.I1" class="ltx_itemize">
<li id="S8.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i1.p1" class="ltx_para">
<p id="S8.I1.i1.p1.1" class="ltx_p"><span id="S8.I1.i1.p1.1.1" class="ltx_text ltx_font_italic">Q1: What is the core reason behind FL’s vulnerability to privacy attacks as the one of this work?</span></p>
</div>
</li>
<li id="S8.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i2.p1" class="ltx_para">
<p id="S8.I1.i2.p1.1" class="ltx_p"><span id="S8.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">Q2: How can the vulnerability be fixed?</span></p>
</div>
</li>
<li id="S8.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S8.I1.i3.p1" class="ltx_para">
<p id="S8.I1.i3.p1.1" class="ltx_p"><span id="S8.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">Q3: What privacy guarantees can, as of now, be provided to FL users?</span></p>
</div>
</li>
</ul>
</div>
<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span><span id="S8.SS1.1.1" class="ltx_text ltx_font_italic">Q1: What is the Root Cause of Vulnerability?</span>
</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">We posit that <em id="S8.SS1.p1.1.1" class="ltx_emph ltx_font_italic">the root cause of FL’s vulnerability to attacks, like the one introduced in this work, is the <span id="S8.SS1.p1.1.1.1" class="ltx_text ltx_font_bold">power imbalance</span> in the centralized design of FL</em>.
At its core, FL is a highly centralized protocol where the server makes the final decisions.
For example, DDP+SA is commonly regarded as a privacy-enhancing solution in modern FL, adding local noise and decentralizing the aggregation step of the original design, however, the server can circumvent the defense by controlling users and manipulating the shared model.
In this paper, we demonstrate that even though the protocols behind DDP+SA and their fundamental cryptographic primitives are correct, the underlying assumptions are not met when the server is malicious.</p>
</div>
<div id="S8.SS1.p2" class="ltx_para">
<p id="S8.SS1.p2.1" class="ltx_p">We detail the factors enabling the different aspects of the attack presented in this work:</p>
<ol id="S8.I2" class="ltx_enumerate">
<li id="S8.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S8.I2.i1.p1" class="ltx_para">
<p id="S8.I2.i1.p1.1" class="ltx_p">The server is able to control a fraction of users.</p>
</div>
</li>
<li id="S8.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S8.I2.i2.p1" class="ltx_para">
<p id="S8.I2.i2.p1.1" class="ltx_p">The server provisions users for participation in each protocol-round.</p>
</div>
</li>
<li id="S8.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S8.I2.i3.p1" class="ltx_para">
<p id="S8.I2.i3.p1.1" class="ltx_p">The server holds the power to manipulate the shared model.</p>
</div>
</li>
<li id="S8.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S8.I2.i4.p1" class="ltx_para">
<p id="S8.I2.i4.p1.1" class="ltx_p">The users have no inherent way of verifying each other’s correctness.</p>
</div>
</li>
<li id="S8.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S8.I2.i5.p1" class="ltx_para">
<p id="S8.I2.i5.p1.1" class="ltx_p">The users cannot meaningfully validate model updates.</p>
</div>
</li>
</ol>
</div>
<div id="S8.SS1.p3" class="ltx_para">
<p id="S8.SS1.p3.1" class="ltx_p">Decentralization can indeed help balance the power disparity, but it should be introduced very carefully and consider the system as a whole <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>. Next, we elaborate more on decentralization and other methods that can address the vulnerability.</p>
</div>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span><span id="S8.SS2.1.1" class="ltx_text ltx_font_italic">Q2: How to Fix the Vulnerability?</span>
</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">We analyze how to fix the vulnerability by considering the following three approaches: (1) decentralization, (2) verification on the user-side to decrease trust assumptions made about the server, and (3) application of external hardware and (cryptographic) protocols that implement guarantees under the presence of an untrusted server.</p>
</div>
<div id="S8.SS2.p2" class="ltx_para ltx_noindent">
<p id="S8.SS2.p2.1" class="ltx_p"><span id="S8.SS2.p2.1.1" class="ltx_text ltx_font_bold">Decentralizing FL.</span>
Several approaches were introduced to decentralize parts of FL, or the entire protocol.
We survey them and discuss their practical applicability.</p>
</div>
<div id="S8.SS2.p3" class="ltx_para">
<p id="S8.SS2.p3.1" class="ltx_p">Decentralized methods for selecting a protocol round’s participants are available <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>.
For example, mechanisms where the users perform a self-sampling, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, have been put forward.
These allow users to decide about their participation and, thereby, reduce the power of the server.
Anarchic FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>]</cite> goes even further and lets users choose an arbitrary point in time to update the shared model with fully-individualized training parameterization; the server observes user updates directly, but each user can individually determine their required privacy level and act accordingly (for example, train on a lot of data, or add a lot of noise).
This does not only mitigate the attack vector where the server samples a target user along with sybil devices.
If the users implement enough protection locally, the server cannot extract their private data.
Yet, such mechanisms come with a significant increase in operational complexity and are faced with the major challenge of motivating users to provide truthful private data, compromising overall system utility.</p>
</div>
<div id="S8.SS2.p4" class="ltx_para">
<p id="S8.SS2.p4.1" class="ltx_p">An interesting decentralized-FL attempt, Biscotti <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>, addresses the issue of sybil attacks by offering a block-chain based protocol that selects users in a decentralized fashion based on past behavior that appears to demonstrate honesty. This is called Proof-of-Federation (PoF), and honesty is indicated by contributing updates to the model that appear close to many other updates (and are thus assumed to be of high quality).
However, users can try to appear honest and yet act maliciously in ways that will not affect the update-quality metric, for example, by not performing their role when they are supposed to noise or verify updates (roles for round participants in the Biscotti protocol which have no performance quality metric). Biscotti would have to be extensively audited for vulnerabilities to this and other attacks before it can be safely and widely deployed. Ultimately, the designs of these decentralized FL systems are very different from FL’s original design, and usually from each other’s designs. At the time of writing, we are not aware of any prominent real-world FL deployments that adopt such decentralization.</p>
</div>
<div id="S8.SS2.p5" class="ltx_para ltx_noindent">
<p id="S8.SS2.p5.1" class="ltx_p"><span id="S8.SS2.p5.1.1" class="ltx_text ltx_font_bold">User-Side Verification.</span>
Alternatively to decentralization, or in addition to it, we can try to reduce the trust that users have to place in the server by performing verification on the user side.</p>
</div>
<div id="S8.SS2.p6" class="ltx_para">
<p id="S8.SS2.p6.1" class="ltx_p">First, we discuss the verification of the shared model.
Giving users the ability to verify the shared model’s integrity can prevent malicious manipulations against it, such as the ones of the trap weights that we integrated into our attack flow.
Unfortunately, without any changes to FL, there is no full-proof way to distinguish weight manipulation from model weights resulting from legitimate previous training.
Users have no insights on the data of other users, updates are affected by stochasticity including sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, non-deterministic hardware elements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and large-scale FL applications with control flow mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> that do not sample every user in every round <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
Ultimately, this makes it impossible to track how the model evolves over time since a user gets only intermittent views of the shared model.
Given these difficulties, we might consider solutions that modify FL to make manipulation-detection easier.</p>
</div>
<div id="S8.SS2.p7" class="ltx_para">
<p id="S8.SS2.p7.1" class="ltx_p">Second, we discuss the option of users verifying other users.
Privacy guarantees in DDP result from all users adding their share of noise to protect the aggregate of all gradients.
If only one user does not add their share of noise, the claimed theoretical overall privacy guarantees cannot be reached.
Our attack exploits this effect and exposes the gradients of a target user by omitting noise addition of all other users sampled in the protocol round.</p>
</div>
<div id="S8.SS2.p8" class="ltx_para">
<p id="S8.SS2.p8.1" class="ltx_p">To prevent this attack vector, users have to verify that other users calculate their gradients correctly and add the correct amount of noise.
However, due to the centralization in standard FL protocols, users do not have direct communication channels with each other.
The central party acts as an intermediary in their interactions.
As an alternative, FL could be deployed within a Public Key Infrastructure (PKI) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
However, a server that behaves semi-honestly is required during the key collection phase of PKI.
Hence, users rely again on their trust in the server while nothing in the protocol prevents this server from acting maliciously and registering their introduced sybil devices to the PKI prior to training.</p>
</div>
<div id="S8.SS2.p9" class="ltx_para">
<p id="S8.SS2.p9.1" class="ltx_p">In fact, protocols like SA rely on the assumption that users participating in the protocol are actual users and no sybil devices controlled by the server <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
Yet, nothing in the integration of the protocol into FL verifies or enforces this assumption.
As a consequence, users who do not trust the server will have to verify that other users participating with them in a protocol round follow the protocol, correctly calculate their gradients, and add their share of noise.
Verifying correct gradient calculations without users having to reveal their private data to each other, can, in principle, be implemented through zero knowledge proofs (ZKPs) where users commit to their private data and prove correct gradient calculation.
However, in our attack, the sybil devices are controlled by the server.
As a consequence, even when they compute correct gradients, the server will be able to subtract these from the aggregate gradient to obtain the target user’s gradients and conduct extraction as described in this work.
The same argument can be applied to correct noise addition.</p>
</div>
<div id="S8.SS2.p10" class="ltx_para">
<p id="S8.SS2.p10.1" class="ltx_p">This motivates the need for users to verify that other users are no sybil devices.
Prior sybil device detection in FL relies on analyzing gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> under the assumption of a non-malicious server.
This approach fails to prevent our attack where sybils are controlled by the server and can, as mentioned above contribute meaningful gradients, as long as the server can subtract these from the aggregate.
Yet, <span id="S8.SS2.p10.1.1" class="ltx_text ltx_font_italic">if</span> users in FL have a way to confidently determine if other users are sybil devices, and <span id="S8.SS2.p10.1.2" class="ltx_text ltx_font_italic">if</span> users have a chance to refrain from contributing to the protocol under their presence, our attack can be mitigated.</p>
</div>
<div id="S8.SS2.p11" class="ltx_para">
<p id="S8.SS2.p11.1" class="ltx_p">Finally, it is desirable to enable users to verify the whole FL application and its local execution.
This allows them to make sure that their local client handles their data correctly, adds enough local noise, and is not manipulated by the server.
However, in modern FL ecosystems the FL client applications are proprietary software encapsulated in dedicated partitions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> largely inaccessible to users.
Additionally, the developers of verification software are usually the same entity acting as the server, bestowing it with even more power. Therefore, we recommend that <em id="S8.SS2.p11.1.1" class="ltx_emph ltx_font_italic">applications of purportedly privacy-preserving protocols should be open-source</em> so that they are available for audits and verification by the community.</p>
</div>
<div id="S8.SS2.p12" class="ltx_para ltx_noindent">
<p id="S8.SS2.p12.1" class="ltx_p"><span id="S8.SS2.p12.1.1" class="ltx_text ltx_font_bold">Hardware and Protocol Support.</span>
As the last approach to fixing the vulnerability of FL, we discuss the support of dedicated hardware and (cryptographic) protocols to implement guarantees for the users.</p>
</div>
<div id="S8.SS2.p13" class="ltx_para">
<p id="S8.SS2.p13.1" class="ltx_p">By relying on protocols that are based on trusted execution environments (TEE), <em id="S8.SS2.p13.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S8.SS2.p13.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> the server can be prevented from manipulating the shared model.
This reduces the success of data extraction as the one underlying our attack.
Additionally, to make sure that users always receive a well-controlled and no a manipulated model, we suggest releasing the shared model publicly, for example, in a block-chain.
This makes it impossible to manipulate and change a shared model after release.</p>
</div>
<div id="S8.SS2.p14" class="ltx_para">
<p id="S8.SS2.p14.1" class="ltx_p">A drawback of this solution is that it offers outside attackers access to several intermediate model states.
We argue that this is not too restricting, though, since the shared model is also sent out to a few hundreds or thousands of users during each round.
Hence, there exists the possibility of the internal model states being leaked, anyways.
Yet, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> has shown that even non-manipulated ML models, under certain conditions, leak their private training data.
Moreover, when it comes to TEEs, they are prone to side-channel attacks, <em id="S8.SS2.p14.1.1" class="ltx_emph ltx_font_italic">e.g</em>.<span id="S8.SS2.p14.1.2" class="ltx_text"></span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
Hence, we conclude that even under such hardware protection some privacy risk for users remains.</p>
</div>
<div id="S8.SS2.p15" class="ltx_para">
<p id="S8.SS2.p15.1" class="ltx_p">Homomorphic encryption (HE) is a candidate solution to protect user gradients against leakage to a malicious server.
However, existing instantiations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> do not prevent our attack since, for efficiency in training, users decrypt the aggregated gradients and apply them to their (unencrypted) local model.
In our attack, the server controls sybil devices, hence, it obtains access to the decrypted aggregate and can extract the target user’s gradient by subtracting the sybil devices’ contributions.</p>
</div>
<div id="S8.SS2.p16" class="ltx_para">
<p id="S8.SS2.p16.1" class="ltx_p">Finally, a cryptographic protocol that always adds enough noise to user gradients in an aggregation step would mitigate the privacy leakage of the attack presented in this work.
The protocol should offer DP, but without making any questionable trust assumptions on other users.
We envision a secure multiparty computation (SMPC) protocol that performs update aggregation, much like SA, but also ensures that the output is added with a sufficient amount of noise to implement DP before it is dispatched to the server. As long as within this protocol users do not learn about each others’ inputs, and the server only learns the aggregated (and noised) output, the protocol may be able to offer a comparatively favorable privacy-utility trade-off.
To the best of our knowledge, so far, no protocol for jointly adding sufficient amounts of noise to user gradients in SMPC exists and given the gradients’ high dimensionality, the costs of any such approach will most likely not be practical, yet.</p>
</div>
</section>
<section id="S8.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3 </span><span id="S8.SS3.1.1" class="ltx_text ltx_font_italic">Q3: What Users Can Do Today?</span>
</h3>

<div id="S8.SS3.p1" class="ltx_para">
<p id="S8.SS3.p1.1" class="ltx_p">Let us assume that a user wishes to attain DP guarantees, and with good reason, as DP is the most viable and protective form of privacy guarantee in use today.
Our study of FL <span id="S8.SS3.p1.1.1" class="ltx_text" style="color:#000000;">under SA and DDP</span> demonstrates that the mere inclusion of a DP mechanism does not necessarily provide protection if this mechanism makes assumptions that are inaccurate in the context of the given system.</p>
</div>
<div id="S8.SS3.p2" class="ltx_para">
<p id="S8.SS3.p2.1" class="ltx_p">We currently see two promising directions for users to attain strong privacy guarantees.</p>
</div>
<div id="S8.SS3.p3" class="ltx_para ltx_noindent">
<p id="S8.SS3.p3.1" class="ltx_p"><span id="S8.SS3.p3.1.1" class="ltx_text ltx_font_bold">Local Differential Privacy.</span> The first one consists of implementing their full privacy protection locally, without trusting any other participant of the protocol, neither the server nor other users to contribute to their protection.
One way to implement such protection is LDP with conservative parameterization, and while ensuring that data point reuse is accounted for and prevented when needed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite>.
While LDP comes at cost of the utility of the shared models, approaches to successfully improve the trade-offs exist for FL deployments with large numbers of participants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite>.
We think that improving LDP to desirable privacy-utility trade-offs is a promising direction.</p>
</div>
<div id="S8.SS3.p4" class="ltx_para ltx_noindent">
<p id="S8.SS3.p4.1" class="ltx_p"><span id="S8.SS3.p4.1.1" class="ltx_text ltx_font_bold">FL Protocols with Trusted Servers.</span>
The second option for users to obtain privacy guarantees is to opt-out of FL altogether, if they do not trust the server, potentially while trying to hide this, for example, by providing randomized “garbage” updates to the server.
However, this approach comes with a corresponding utility cost and undermines the purpose of collaborative learning to produce a performant model that fits many diverse individuals.</p>
</div>
<div id="S8.SS3.p5" class="ltx_para">
<p id="S8.SS3.p5.1" class="ltx_p">Finally, these options are only available if users can control the local FL application software, which is usually not the case (unless an opt-out option is provided).</p>
</div>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span><span id="S9.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S9.p1" class="ltx_para">
<p id="S9.p1.1" class="ltx_p">Truly privacy-preserving ML must defend itself from attackers that are malicious and hence do not follow protocol.
In this work, we presented a highly efficient data reconstruction attack against FL in a strongly protected deployment, namely with SA and DDP.
<span id="S9.p1.1.1" class="ltx_text" style="color:#000000;">
Our attack adds sybil devices to the pool of users who are provisioned and sampled by the server.
Such sybils return model updates that are known to the server, thus can be easily subtracted from the total aggregate to expose model updates provided by targeted individual users.
By including trap weights into our attack, we are able to reconstruct individual users’ data points of high quality.
</span>
Based on the attack’s success, we analyzed the question of the minimum trust model that is required to obtain meaningful privacy guarantees for the users.
We showed that FL can provide privacy guarantees if the users trust the server, or if they rely on adequate cryptographic protocols, and if relevant additional protection methods are in place.
Most of the methods for protection aim at shifting power from the server to the conglomerate of users and come with significant costs or overhead.
Therefore, such systems are <span id="S9.p1.1.2" class="ltx_text" style="color:#000000;">often</span> not yet practically in place.
As a consequence, we recommend that, <span id="S9.p1.1.3" class="ltx_text" style="color:#000000;">currently</span>, users only participate in FL protocols that are orchestrated by a trusted server.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgments</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">We would like to acknowledge our sponsors, who support our research with financial and in-kind contributions: Amazon, Apple, CIFAR
through the Canada CIFAR AI Chair, DARPA through the GARD project, Intel, Meta, NFRF through an Exploration
grant, NSERC through the COHESA Strategic Alliance, the Ontario Early Researcher Award, and the Sloan Foundation. Resources used in preparing this research were provided,
in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector
Institute.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Deep learning with differential privacy.

</span>
<span class="ltx_bibblock">In Edgar Weippl, editor, <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security</span>. ACM, 2016.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Naman Agarwal, Peter Kairouz, and Ziyu Liu.

</span>
<span class="ltx_bibblock">The skellam mechanism for differentially private federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 34, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Tiago A Almeida, José María G Hidalgo, and Akebo Yamakami.

</span>
<span class="ltx_bibblock">Contributions to the study of sms spam filtering: new collection and
results.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of the 11th ACM symposium on Document
engineering</span>, pages 259–262, 2011.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Giuseppe Ateniese, Luigi V Mancini, Angelo Spognardi, Antonio Villani, Domenico
Vitali, and Giovanni Felici.

</span>
<span class="ltx_bibblock">Hacking smart machines with smarter ones: How to extract meaningful
data from machine learning classifiers.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">International Journal of Security and Networks</span>, 10(3):137–150,
2015.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Borja Balle, Peter Kairouz, Brendan McMahan, Om Thakkar, and Abhradeep
Guha Thakurta.

</span>
<span class="ltx_bibblock">Privacy amplification via random check-ins.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
33:4623–4634, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
James Henry Bell, Kallista A Bonawitz, Adrià Gascón, Tancrède
Lepoint, and Mariana Raykova.

</span>
<span class="ltx_bibblock">Secure single-server aggregation with (poly) logarithmic overhead.

</span>
<span class="ltx_bibblock">In <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2020 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 1253–1269, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Andrea Bittau, Úlfar Erlingsson, Petros Maniatis, Ilya Mironov, Ananth
Raghunathan, David Lie, Mitch Rudominer, Ushasree Kode, Julien Tinnes, and
Bernhard Seefeld.

</span>
<span class="ltx_bibblock">Prochlo: Strong privacy for analytics in the crowd.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th Symposium on Operating Systems
Principles</span>, pages 441–459, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia
Shumailov, and Nicolas Papernot.

</span>
<span class="ltx_bibblock">When the curious abandon honesty: Federated learning is not private.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2112.02918</span>, 2021.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex
Ingerman, Vladimir Ivanov, Chloe Kiddon, Jakub Konečnỳ, Stefano
Mazzocchi, Brendan McMahan, et al.

</span>
<span class="ltx_bibblock">Towards federated learning at scale: System design.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of Machine Learning and Systems</span>, 1:374–388, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan
McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth.

</span>
<span class="ltx_bibblock">Practical secure aggregation for privacy-preserving machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security</span>, pages 1175–1191, 2017.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Lukas Burkhalter, Hidde Lycklama, Alexander Viand, Nicolas Küchler, and
Anwar Hithnawi.

</span>
<span class="ltx_bibblock">Rofl: Attestable robustness for secure federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2107.03311</span>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Wei-Ning Chen, Christopher A Choquette-Choo, Peter Kairouz, and Ananda Theertha
Suresh.

</span>
<span class="ltx_bibblock">The fundamental price of secure aggregation in differentially private
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2203.03761</span>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">2009 IEEE conference on computer vision and pattern
recognition</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
C Dwork.

</span>
<span class="ltx_bibblock">Differential privacy in the 40th international colloquium on
automata.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Languages and Programming</span>, 2006.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Úlfar Erlingsson, Vitaly Feldman, Ilya Mironov, Ananth Raghunathan, Kunal
Talwar, and Abhradeep Thakurta.

</span>
<span class="ltx_bibblock">Amplification by shuffling: From local to central differential
privacy via anonymity.

</span>
<span class="ltx_bibblock">In <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
Discrete Algorithms</span>, pages 2468–2479. SIAM, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Liam H Fowl, Jonas Geiping, Wojciech Czaja, Micah Goldblum, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Robbing the fed: Directly obtaining private data in federated
learning with modified models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2021.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Clement Fung, Chris JM Yoon, and Ivan Beschastnikh.

</span>
<span class="ltx_bibblock">The limitations of federated learning in sybil settings.

</span>
<span class="ltx_bibblock">In <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">23rd International Symposium on Research in Attacks,
Intrusions and Defenses (RAID 2020)</span>, pages 301–316, 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Inverting Gradients – How easy is it to break privacy in
federated learning?</span>

</span>
<span class="ltx_bibblock">2020.

</span>
<span class="ltx_bibblock">23 pages, 20 figures. The first three authors contributed equally.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Antonious Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and
Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Shuffled model of differential privacy in federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</span>, pages 2521–2529. PMLR, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Antonious M Girgis, Deepesh Data, and Suhas Diggavi.

</span>
<span class="ltx_bibblock">Differentially private federated learning with shuffling and client
self-sampling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">2021 IEEE International Symposium on Information Theory
(ISIT)</span>, pages 338–343. IEEE, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Raja Giryes, Guillermo Sapiro, and Alex M Bronstein.

</span>
<span class="ltx_bibblock">Deep neural networks with random gaussian weights: A universal
classification strategy?

</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Signal Processing</span>, 64(13):3444–3457,
2016.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Deep sparse rectifier neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of the fourteenth international conference on
artificial intelligence and statistics</span>, pages 315–323. JMLR Workshop and
Conference Proceedings, 2011.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial nets.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings of the Advances in Neural Information Processing
Systems (NeurIPS)</span>, Montreal, Canada, December 2014.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Xiaojie Guo, Zheli Liu, Jin Li, Jiqiang Gao, Boyu Hou, Changyu Dong, and Thar
Baker.

</span>
<span class="ltx_bibblock">Verifl: Communication-efficient and fast verifiable aggregation for
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
16:1736–1751, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.

</span>
<span class="ltx_bibblock">Delving deep into rectifiers: Surpassing human-level performance on
imagenet classification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">2015 IEEE International Conference on Computer Vision
(ICCV)</span>. IEEE, 2015.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz.

</span>
<span class="ltx_bibblock">Deep models under the gan: information leakage from collaborative
deep learning.

</span>
<span class="ltx_bibblock">2017.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Patrick Jauernig, Ahmad-Reza Sadeghi, and Emmanuel Stapf.

</span>
<span class="ltx_bibblock">Trusted execution environments: properties, applications, and
challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">IEEE Security &amp; Privacy</span>, 18(2):56–60, 2020.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Hengrui Jia, Mohammad Yaghini, Christopher A Choquette-Choo, Natalie Dullerud,
Anvith Thudi, Varun Chandrasekaran, and Nicolas Papernot.

</span>
<span class="ltx_bibblock">Proof-of-learning: Definitions and practice.

</span>
<span class="ltx_bibblock">In <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">2021 IEEE Symposium on Security and Privacy (SP)</span>, pages
1039–1056. IEEE, 2021.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Peter Kairouz, Ziyu Liu, and Thomas Steinke.

</span>
<span class="ltx_bibblock">The distributed discrete gaussian mechanism for federated learning
with secure aggregation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
5201–5212. PMLR, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta,
and Zheng Xu.

</span>
<span class="ltx_bibblock">Practical and private (deep) learning without sampling or shuffling.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
5213–5225. PMLR, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Geoffrey Hinton, et al.

</span>
<span class="ltx_bibblock">Learning multiple layers of features from tiny images.

</span>
<span class="ltx_bibblock">2009.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yann LeCun, Corinna Cortes, and C. J. Burges.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">MNIST handwritten digit database</span>.

</span>
<span class="ltx_bibblock">2010.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
Aguera y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">pages 1273–1282, 2017.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Brendan McMahan and Abhradeep Thakurta.

</span>
<span class="ltx_bibblock">Federated learning with formal differential privacy guarantees.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://ai.googleblog.com/2022/02/federated-learning-with-formal.html?m=1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://ai.googleblog.com/2022/02/federated-learning-with-formal.html?m=1</a>,
2022.

</span>
<span class="ltx_bibblock">Last Acccessed: March 25, 2022.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang.

</span>
<span class="ltx_bibblock">Learning differentially private recurrent language models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">International Conference on Learning Representations</span>, 2018.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Luca Melis, Congzheng Song, Emiliano de Cristofaro, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Exploiting unintended feature leakage in collaborative learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">2019 IEEE Symposium on Security and Privacy (SP)</span>, pages
691–706. IEEE, 19/05/2019 - 23/05/2019.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Vangelis Metsis, Ion Androutsopoulos, and Georgios Paliouras.

</span>
<span class="ltx_bibblock">Spam filtering with naive bayes-which naive bayes?

</span>
<span class="ltx_bibblock">In <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">CEAS</span>, volume 17, pages 28–69. Mountain View, CA, 2006.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard Marin, Diego Perino, and
Nicolas Kourtellis.

</span>
<span class="ltx_bibblock">Ppfl: privacy-preserving federated learning with trusted execution
environments.

</span>
<span class="ltx_bibblock">In <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Proceedings of the 19th Annual International Conference on
Mobile Systems, Applications, and Services</span>, pages 94–108, 2021.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Milad Nasr, Reza Shokri, and Amir Houmansadr.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning: Passive and active
white-box inference attacks against centralized and federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">2019 IEEE symposium on security and privacy (SP)</span>, pages
739–753. IEEE, 2019.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Nicolas Papernot, Martín Abadi, Úlfar Erlingsson, Ian Goodfellow, and Kunal
Talwar.

</span>
<span class="ltx_bibblock">Semi-supervised knowledge transfer for deep learning from private
training data, 18/10/2016.

</span>
<span class="ltx_bibblock">Accepted to ICLR 17 as an oral.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Dario Pasquini, Danilo Francati, and Giuseppe Ateniese.

</span>
<span class="ltx_bibblock">Eluding secure aggregation in federated learning via model
inconsistency.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2111.07380</span>, 2021.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Dario Pasquini, Mathilde Raynal, and Carmela Troncoso.

</span>
<span class="ltx_bibblock">On the privacy of decentralized machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2205.08443</span>, 2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Le Trieu Phong, Yoshinori Aono, Takuya Hayashi, Lihua Wang, and Shiho Moriai.

</span>
<span class="ltx_bibblock">Privacy-preserving deep learning: Revisited and enhanced.

</span>
<span class="ltx_bibblock">In <span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">International Conference on Applications and Techniques in
Information Security</span>, pages 100–110. Springer, 2017.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Rachel Potvin and Josh Levenberg.

</span>
<span class="ltx_bibblock">Why google stores billions of lines of code in a single repository.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 59(7):78–87, 2016.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Swaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan,
and Françoise Beaufays.

</span>
<span class="ltx_bibblock">Training production language models without memorizing user data.

</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.10031</span>, 2020.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Caitlin Sadowski, Emma Söderberg, Luke Church, Michal Sipko, and Alberto
Bacchelli.

</span>
<span class="ltx_bibblock">Modern code review: a case study at google.

</span>
<span class="ltx_bibblock">In <span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Proceedings of the 40th international conference on software
engineering: Software engineering in practice</span>, pages 181–190, 2018.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Muhammad Shayan, Clement Fung, Chris J. M. Yoon, and Ivan Beschastnikh.

</span>
<span class="ltx_bibblock">Biscotti: A blockchain system for private and secure federated
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</span>,
32(7):1513–1525, 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov.

</span>
<span class="ltx_bibblock">Membership inference attacks against machine learning models.

</span>
<span class="ltx_bibblock">In IEEE Symposium on Security and Privacy, editor, <span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">2017 IEEE
Symposium on Security and Privacy - SP 2017</span>. IEEE Symposium on Security and
Privacy and SP and Symposium on Security &amp; Privacy and S &amp; P and
IEEE S &amp; P, IEEE, 2017.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas Papernot,
Murat A. Erdogdu, and Ross Anderson.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">Manipulating SGD with Data Ordering Attacks</span>.

</span>
<span class="ltx_bibblock">2021.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Lichao Sun, Jianwei Qian, and Xun Chen.

</span>
<span class="ltx_bibblock">Ldp-fl: Practical private aggregation in federated learning with
local differential privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.15789</span>, 2020.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Anvith Thudi, Ilia Shumailov, Franziska Boenisch, and Nicolas Papernot.

</span>
<span class="ltx_bibblock">Bounding membership inference, 2022.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Stacey Truex, Nathalie Baracaldo, Ali Anwar, Thomas Steinke, Heiko Ludwig, Rui
Zhang, and Yi Zhou.

</span>
<span class="ltx_bibblock">A hybrid approach to privacy-preserving federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proceedings of the 12th ACM workshop on artificial
intelligence and security</span>, pages 1–11, 2019.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Stacey Truex, Ling Liu, Ka-Ho Chow, Mehmet Emre Gursoy, and Wenqi Wei.

</span>
<span class="ltx_bibblock">Ldp-fed: Federated learning with local differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Proceedings of the Third ACM International Workshop on Edge
Systems, Analytics and Networking</span>, pages 61–66, 2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Jack Wallen.

</span>
<span class="ltx_bibblock">Android 12 adds ai and machine learning with private compute core but
keeps your data secure.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.techrepublic.com/article/android-12-adds-ai-and-machine-learning-with-private-computecore-but-keeps-your-data-secure/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.techrepublic.com/article/android-12-adds-ai-and-machine-learning-with-private-computecore-but-keeps-your-data-secure/</a>,
2021.

</span>
<span class="ltx_bibblock">Last Acccessed: October 17, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Zhibo Wang, Mengkai Song, Zhifei Zhang, Yang Song, Qian Wang, and Hairong Qi.

</span>
<span class="ltx_bibblock">Beyond inferring class representatives: User-level privacy leakage
from federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib55.1.1" class="ltx_text ltx_font_italic">IEEE INFOCOM 2019 - IEEE Conference on Computer
Communications</span>. IEEE, 2019.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin,
Tony QS Quek, and H Vincent Poor.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy: Algorithms and
performance analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
15:3454–3469, 2020.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Yuxin Wen, Jonas A. Geiping, Liam Fowl, Micah Goldblum, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Fishing for user data in large-batch federated learning via gradient
magnification.

</span>
<span class="ltx_bibblock">In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari,
Gang Niu, and Sivan Sabato, editors, <span id="bib.bib57.1.1" class="ltx_text ltx_font_italic">Proceedings of the 39th
International Conference on Machine Learning</span>, volume 162 of <span id="bib.bib57.2.2" class="ltx_text ltx_font_italic">Proceedings
of Machine Learning Research</span>, pages 23668–23684. PMLR, 17–23 Jul 2022.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Guowen Xu, Hongwei Li, Sen Liu, Kan Yang, and Xiaodong Lin.

</span>
<span class="ltx_bibblock">Verifynet: Secure and verifiable federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
15:911–926, 2019.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Haibo Yang, Xin Zhang, Prashant Khanduri, and Jia Liu.

</span>
<span class="ltx_bibblock">Anarchic federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2108.09875</span>, 2021.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Hongxu Yin, Arun Mallya, Arash Vahdat, Jose M Alvarez, Jan Kautz, and Pavlo
Molchanov.

</span>
<span class="ltx_bibblock">See through gradients: Image batch recovery via gradinversion.

</span>
<span class="ltx_bibblock">In <span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 16337–16346, 2021.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu.

</span>
<span class="ltx_bibblock"><math id="bib.bib61.1.m1.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib61.1.m1.1a"><mo stretchy="false" id="bib.bib61.1.m1.1.1" xref="bib.bib61.1.m1.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib61.1.m1.1b"><ci id="bib.bib61.1.m1.1.1.cmml" xref="bib.bib61.1.m1.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib61.1.m1.1c">\{</annotation></semantics></math>BatchCrypt<math id="bib.bib61.2.m2.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib61.2.m2.1a"><mo stretchy="false" id="bib.bib61.2.m2.1.1" xref="bib.bib61.2.m2.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib61.2.m2.1b"><ci id="bib.bib61.2.m2.1.1.cmml" xref="bib.bib61.2.m2.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib61.2.m2.1c">\}</annotation></semantics></math>: Efficient homomorphic encryption for
<math id="bib.bib61.3.m3.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="bib.bib61.3.m3.1a"><mo stretchy="false" id="bib.bib61.3.m3.1.1" xref="bib.bib61.3.m3.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="bib.bib61.3.m3.1b"><ci id="bib.bib61.3.m3.1.1.cmml" xref="bib.bib61.3.m3.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib61.3.m3.1c">\{</annotation></semantics></math>Cross-Silo<math id="bib.bib61.4.m4.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="bib.bib61.4.m4.1a"><mo stretchy="false" id="bib.bib61.4.m4.1.1" xref="bib.bib61.4.m4.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="bib.bib61.4.m4.1b"><ci id="bib.bib61.4.m4.1.1.cmml" xref="bib.bib61.4.m4.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib61.4.m4.1c">\}</annotation></semantics></math> federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.5.1" class="ltx_text ltx_font_italic">2020 USENIX annual technical conference (USENIX ATC 20)</span>,
pages 493–506, 2020.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen.

</span>
<span class="ltx_bibblock">idlg: Improved deep leakage from gradients.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2001.02610</span>, 2020.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Ligeng Zhu, Zhijian Liu, and Song Han.

</span>
<span class="ltx_bibblock">Deep leakage from gradients.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 32, 2019.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Differential Privacy and DPSGD</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">DP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> formalizes the idea that no data point should significantly influence the results of an analysis conducted on a whole dataset.
Therefore, the analysis should yield roughly the same results whether or not any particular dataset is included in the dataset or not.
Formally, this is expressed by the following definition.</p>
</div>
<div id="Thmdf1" class="ltx_theorem ltx_theorem_df">
<h6 class="ltx_title ltx_runin ltx_title_theorem">
<span class="ltx_tag ltx_tag_theorem"><span id="Thmdf1.2.1.1" class="ltx_text ltx_font_bold">Definition 1</span></span><span id="Thmdf1.3.2" class="ltx_text ltx_font_bold"> </span>(<math id="Thmdf1.1.m1.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="Thmdf1.1.m1.2b"><mrow id="Thmdf1.1.m1.2.3.2" xref="Thmdf1.1.m1.2.3.1.cmml"><mo stretchy="false" id="Thmdf1.1.m1.2.3.2.1" xref="Thmdf1.1.m1.2.3.1.cmml">(</mo><mi id="Thmdf1.1.m1.1.1" xref="Thmdf1.1.m1.1.1.cmml">ε</mi><mo id="Thmdf1.1.m1.2.3.2.2" xref="Thmdf1.1.m1.2.3.1.cmml">,</mo><mi id="Thmdf1.1.m1.2.2" xref="Thmdf1.1.m1.2.2.cmml">δ</mi><mo stretchy="false" id="Thmdf1.1.m1.2.3.2.3" xref="Thmdf1.1.m1.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="Thmdf1.1.m1.2c"><interval closure="open" id="Thmdf1.1.m1.2.3.1.cmml" xref="Thmdf1.1.m1.2.3.2"><ci id="Thmdf1.1.m1.1.1.cmml" xref="Thmdf1.1.m1.1.1">𝜀</ci><ci id="Thmdf1.1.m1.2.2.cmml" xref="Thmdf1.1.m1.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="Thmdf1.1.m1.2d">(\varepsilon,\delta)</annotation></semantics></math>-Differential Privacy)<span id="Thmdf1.4.3" class="ltx_text ltx_font_bold">.</span>
</h6>
<div id="Thmdf1.p1" class="ltx_para">
<p id="Thmdf1.p1.7" class="ltx_p">Let <math id="Thmdf1.p1.1.m1.1" class="ltx_Math" alttext="A\colon\mathcal{D}^{*}\rightarrow\mathcal{R}" display="inline"><semantics id="Thmdf1.p1.1.m1.1a"><mrow id="Thmdf1.p1.1.m1.1.1" xref="Thmdf1.p1.1.m1.1.1.cmml"><mi id="Thmdf1.p1.1.m1.1.1.2" xref="Thmdf1.p1.1.m1.1.1.2.cmml">A</mi><mo lspace="0.278em" rspace="0.278em" id="Thmdf1.p1.1.m1.1.1.1" xref="Thmdf1.p1.1.m1.1.1.1.cmml">:</mo><mrow id="Thmdf1.p1.1.m1.1.1.3" xref="Thmdf1.p1.1.m1.1.1.3.cmml"><msup id="Thmdf1.p1.1.m1.1.1.3.2" xref="Thmdf1.p1.1.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="Thmdf1.p1.1.m1.1.1.3.2.2" xref="Thmdf1.p1.1.m1.1.1.3.2.2.cmml">𝒟</mi><mo id="Thmdf1.p1.1.m1.1.1.3.2.3" xref="Thmdf1.p1.1.m1.1.1.3.2.3.cmml">∗</mo></msup><mo stretchy="false" id="Thmdf1.p1.1.m1.1.1.3.1" xref="Thmdf1.p1.1.m1.1.1.3.1.cmml">→</mo><mi class="ltx_font_mathcaligraphic" id="Thmdf1.p1.1.m1.1.1.3.3" xref="Thmdf1.p1.1.m1.1.1.3.3.cmml">ℛ</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdf1.p1.1.m1.1b"><apply id="Thmdf1.p1.1.m1.1.1.cmml" xref="Thmdf1.p1.1.m1.1.1"><ci id="Thmdf1.p1.1.m1.1.1.1.cmml" xref="Thmdf1.p1.1.m1.1.1.1">:</ci><ci id="Thmdf1.p1.1.m1.1.1.2.cmml" xref="Thmdf1.p1.1.m1.1.1.2">𝐴</ci><apply id="Thmdf1.p1.1.m1.1.1.3.cmml" xref="Thmdf1.p1.1.m1.1.1.3"><ci id="Thmdf1.p1.1.m1.1.1.3.1.cmml" xref="Thmdf1.p1.1.m1.1.1.3.1">→</ci><apply id="Thmdf1.p1.1.m1.1.1.3.2.cmml" xref="Thmdf1.p1.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="Thmdf1.p1.1.m1.1.1.3.2.1.cmml" xref="Thmdf1.p1.1.m1.1.1.3.2">superscript</csymbol><ci id="Thmdf1.p1.1.m1.1.1.3.2.2.cmml" xref="Thmdf1.p1.1.m1.1.1.3.2.2">𝒟</ci><times id="Thmdf1.p1.1.m1.1.1.3.2.3.cmml" xref="Thmdf1.p1.1.m1.1.1.3.2.3"></times></apply><ci id="Thmdf1.p1.1.m1.1.1.3.3.cmml" xref="Thmdf1.p1.1.m1.1.1.3.3">ℛ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdf1.p1.1.m1.1c">A\colon\mathcal{D}^{*}\rightarrow\mathcal{R}</annotation></semantics></math> a randomized algorithm. <math id="Thmdf1.p1.2.m2.1" class="ltx_Math" alttext="A" display="inline"><semantics id="Thmdf1.p1.2.m2.1a"><mi id="Thmdf1.p1.2.m2.1.1" xref="Thmdf1.p1.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="Thmdf1.p1.2.m2.1b"><ci id="Thmdf1.p1.2.m2.1.1.cmml" xref="Thmdf1.p1.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="Thmdf1.p1.2.m2.1c">A</annotation></semantics></math> satisfies <math id="Thmdf1.p1.3.m3.2" class="ltx_Math" alttext="(\varepsilon,\delta)" display="inline"><semantics id="Thmdf1.p1.3.m3.2a"><mrow id="Thmdf1.p1.3.m3.2.3.2" xref="Thmdf1.p1.3.m3.2.3.1.cmml"><mo stretchy="false" id="Thmdf1.p1.3.m3.2.3.2.1" xref="Thmdf1.p1.3.m3.2.3.1.cmml">(</mo><mi id="Thmdf1.p1.3.m3.1.1" xref="Thmdf1.p1.3.m3.1.1.cmml">ε</mi><mo id="Thmdf1.p1.3.m3.2.3.2.2" xref="Thmdf1.p1.3.m3.2.3.1.cmml">,</mo><mi id="Thmdf1.p1.3.m3.2.2" xref="Thmdf1.p1.3.m3.2.2.cmml">δ</mi><mo stretchy="false" id="Thmdf1.p1.3.m3.2.3.2.3" xref="Thmdf1.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="Thmdf1.p1.3.m3.2b"><interval closure="open" id="Thmdf1.p1.3.m3.2.3.1.cmml" xref="Thmdf1.p1.3.m3.2.3.2"><ci id="Thmdf1.p1.3.m3.1.1.cmml" xref="Thmdf1.p1.3.m3.1.1">𝜀</ci><ci id="Thmdf1.p1.3.m3.2.2.cmml" xref="Thmdf1.p1.3.m3.2.2">𝛿</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="Thmdf1.p1.3.m3.2c">(\varepsilon,\delta)</annotation></semantics></math>-DP with <math id="Thmdf1.p1.4.m4.1" class="ltx_Math" alttext="\varepsilon\in\mathbb{R}_{+}" display="inline"><semantics id="Thmdf1.p1.4.m4.1a"><mrow id="Thmdf1.p1.4.m4.1.1" xref="Thmdf1.p1.4.m4.1.1.cmml"><mi id="Thmdf1.p1.4.m4.1.1.2" xref="Thmdf1.p1.4.m4.1.1.2.cmml">ε</mi><mo id="Thmdf1.p1.4.m4.1.1.1" xref="Thmdf1.p1.4.m4.1.1.1.cmml">∈</mo><msub id="Thmdf1.p1.4.m4.1.1.3" xref="Thmdf1.p1.4.m4.1.1.3.cmml"><mi id="Thmdf1.p1.4.m4.1.1.3.2" xref="Thmdf1.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mo id="Thmdf1.p1.4.m4.1.1.3.3" xref="Thmdf1.p1.4.m4.1.1.3.3.cmml">+</mo></msub></mrow><annotation-xml encoding="MathML-Content" id="Thmdf1.p1.4.m4.1b"><apply id="Thmdf1.p1.4.m4.1.1.cmml" xref="Thmdf1.p1.4.m4.1.1"><in id="Thmdf1.p1.4.m4.1.1.1.cmml" xref="Thmdf1.p1.4.m4.1.1.1"></in><ci id="Thmdf1.p1.4.m4.1.1.2.cmml" xref="Thmdf1.p1.4.m4.1.1.2">𝜀</ci><apply id="Thmdf1.p1.4.m4.1.1.3.cmml" xref="Thmdf1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="Thmdf1.p1.4.m4.1.1.3.1.cmml" xref="Thmdf1.p1.4.m4.1.1.3">subscript</csymbol><ci id="Thmdf1.p1.4.m4.1.1.3.2.cmml" xref="Thmdf1.p1.4.m4.1.1.3.2">ℝ</ci><plus id="Thmdf1.p1.4.m4.1.1.3.3.cmml" xref="Thmdf1.p1.4.m4.1.1.3.3"></plus></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdf1.p1.4.m4.1c">\varepsilon\in\mathbb{R}_{+}</annotation></semantics></math> and <math id="Thmdf1.p1.5.m5.2" class="ltx_Math" alttext="\delta\in[0,1]" display="inline"><semantics id="Thmdf1.p1.5.m5.2a"><mrow id="Thmdf1.p1.5.m5.2.3" xref="Thmdf1.p1.5.m5.2.3.cmml"><mi id="Thmdf1.p1.5.m5.2.3.2" xref="Thmdf1.p1.5.m5.2.3.2.cmml">δ</mi><mo id="Thmdf1.p1.5.m5.2.3.1" xref="Thmdf1.p1.5.m5.2.3.1.cmml">∈</mo><mrow id="Thmdf1.p1.5.m5.2.3.3.2" xref="Thmdf1.p1.5.m5.2.3.3.1.cmml"><mo stretchy="false" id="Thmdf1.p1.5.m5.2.3.3.2.1" xref="Thmdf1.p1.5.m5.2.3.3.1.cmml">[</mo><mn id="Thmdf1.p1.5.m5.1.1" xref="Thmdf1.p1.5.m5.1.1.cmml">0</mn><mo id="Thmdf1.p1.5.m5.2.3.3.2.2" xref="Thmdf1.p1.5.m5.2.3.3.1.cmml">,</mo><mn id="Thmdf1.p1.5.m5.2.2" xref="Thmdf1.p1.5.m5.2.2.cmml">1</mn><mo stretchy="false" id="Thmdf1.p1.5.m5.2.3.3.2.3" xref="Thmdf1.p1.5.m5.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="Thmdf1.p1.5.m5.2b"><apply id="Thmdf1.p1.5.m5.2.3.cmml" xref="Thmdf1.p1.5.m5.2.3"><in id="Thmdf1.p1.5.m5.2.3.1.cmml" xref="Thmdf1.p1.5.m5.2.3.1"></in><ci id="Thmdf1.p1.5.m5.2.3.2.cmml" xref="Thmdf1.p1.5.m5.2.3.2">𝛿</ci><interval closure="closed" id="Thmdf1.p1.5.m5.2.3.3.1.cmml" xref="Thmdf1.p1.5.m5.2.3.3.2"><cn type="integer" id="Thmdf1.p1.5.m5.1.1.cmml" xref="Thmdf1.p1.5.m5.1.1">0</cn><cn type="integer" id="Thmdf1.p1.5.m5.2.2.cmml" xref="Thmdf1.p1.5.m5.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdf1.p1.5.m5.2c">\delta\in[0,1]</annotation></semantics></math> if for all neighboring datasets <math id="Thmdf1.p1.6.m6.1" class="ltx_Math" alttext="D\sim D^{\prime}" display="inline"><semantics id="Thmdf1.p1.6.m6.1a"><mrow id="Thmdf1.p1.6.m6.1.1" xref="Thmdf1.p1.6.m6.1.1.cmml"><mi id="Thmdf1.p1.6.m6.1.1.2" xref="Thmdf1.p1.6.m6.1.1.2.cmml">D</mi><mo id="Thmdf1.p1.6.m6.1.1.1" xref="Thmdf1.p1.6.m6.1.1.1.cmml">∼</mo><msup id="Thmdf1.p1.6.m6.1.1.3" xref="Thmdf1.p1.6.m6.1.1.3.cmml"><mi id="Thmdf1.p1.6.m6.1.1.3.2" xref="Thmdf1.p1.6.m6.1.1.3.2.cmml">D</mi><mo id="Thmdf1.p1.6.m6.1.1.3.3" xref="Thmdf1.p1.6.m6.1.1.3.3.cmml">′</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="Thmdf1.p1.6.m6.1b"><apply id="Thmdf1.p1.6.m6.1.1.cmml" xref="Thmdf1.p1.6.m6.1.1"><csymbol cd="latexml" id="Thmdf1.p1.6.m6.1.1.1.cmml" xref="Thmdf1.p1.6.m6.1.1.1">similar-to</csymbol><ci id="Thmdf1.p1.6.m6.1.1.2.cmml" xref="Thmdf1.p1.6.m6.1.1.2">𝐷</ci><apply id="Thmdf1.p1.6.m6.1.1.3.cmml" xref="Thmdf1.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="Thmdf1.p1.6.m6.1.1.3.1.cmml" xref="Thmdf1.p1.6.m6.1.1.3">superscript</csymbol><ci id="Thmdf1.p1.6.m6.1.1.3.2.cmml" xref="Thmdf1.p1.6.m6.1.1.3.2">𝐷</ci><ci id="Thmdf1.p1.6.m6.1.1.3.3.cmml" xref="Thmdf1.p1.6.m6.1.1.3.3">′</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdf1.p1.6.m6.1c">D\sim D^{\prime}</annotation></semantics></math>, i.e. datasets that differ on only one element, and for all possible subsets <math id="Thmdf1.p1.7.m7.1" class="ltx_Math" alttext="R\subseteq\mathcal{R}" display="inline"><semantics id="Thmdf1.p1.7.m7.1a"><mrow id="Thmdf1.p1.7.m7.1.1" xref="Thmdf1.p1.7.m7.1.1.cmml"><mi id="Thmdf1.p1.7.m7.1.1.2" xref="Thmdf1.p1.7.m7.1.1.2.cmml">R</mi><mo id="Thmdf1.p1.7.m7.1.1.1" xref="Thmdf1.p1.7.m7.1.1.1.cmml">⊆</mo><mi class="ltx_font_mathcaligraphic" id="Thmdf1.p1.7.m7.1.1.3" xref="Thmdf1.p1.7.m7.1.1.3.cmml">ℛ</mi></mrow><annotation-xml encoding="MathML-Content" id="Thmdf1.p1.7.m7.1b"><apply id="Thmdf1.p1.7.m7.1.1.cmml" xref="Thmdf1.p1.7.m7.1.1"><subset id="Thmdf1.p1.7.m7.1.1.1.cmml" xref="Thmdf1.p1.7.m7.1.1.1"></subset><ci id="Thmdf1.p1.7.m7.1.1.2.cmml" xref="Thmdf1.p1.7.m7.1.1.2">𝑅</ci><ci id="Thmdf1.p1.7.m7.1.1.3.cmml" xref="Thmdf1.p1.7.m7.1.1.3">ℛ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="Thmdf1.p1.7.m7.1c">R\subseteq\mathcal{R}</annotation></semantics></math></p>
<table id="A2.EGx3" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="A1.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="A1.E3.m1.2" class="ltx_Math" alttext="\displaystyle\mathbb{P}\left[A(D)\in R\right]\leq e^{\varepsilon}\cdot\mathbb{P}\left[M(D^{\prime})\in R\right]+\delta\,." display="inline"><semantics id="A1.E3.m1.2a"><mrow id="A1.E3.m1.2.2.1" xref="A1.E3.m1.2.2.1.1.cmml"><mrow id="A1.E3.m1.2.2.1.1" xref="A1.E3.m1.2.2.1.1.cmml"><mrow id="A1.E3.m1.2.2.1.1.1" xref="A1.E3.m1.2.2.1.1.1.cmml"><mi id="A1.E3.m1.2.2.1.1.1.3" xref="A1.E3.m1.2.2.1.1.1.3.cmml">ℙ</mi><mo lspace="0em" rspace="0em" id="A1.E3.m1.2.2.1.1.1.2" xref="A1.E3.m1.2.2.1.1.1.2.cmml">​</mo><mrow id="A1.E3.m1.2.2.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.1.1.2.cmml"><mo id="A1.E3.m1.2.2.1.1.1.1.1.2" xref="A1.E3.m1.2.2.1.1.1.1.2.1.cmml">[</mo><mrow id="A1.E3.m1.2.2.1.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.1.1.1.1.cmml"><mrow id="A1.E3.m1.2.2.1.1.1.1.1.1.2" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mi id="A1.E3.m1.2.2.1.1.1.1.1.1.2.2" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="A1.E3.m1.2.2.1.1.1.1.1.1.2.1" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml">​</mo><mrow id="A1.E3.m1.2.2.1.1.1.1.1.1.2.3.2" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="A1.E3.m1.2.2.1.1.1.1.1.1.2.3.2.1" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2.cmml">(</mo><mi id="A1.E3.m1.1.1" xref="A1.E3.m1.1.1.cmml">D</mi><mo stretchy="false" id="A1.E3.m1.2.2.1.1.1.1.1.1.2.3.2.2" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="A1.E3.m1.2.2.1.1.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1.cmml">∈</mo><mi id="A1.E3.m1.2.2.1.1.1.1.1.1.3" xref="A1.E3.m1.2.2.1.1.1.1.1.1.3.cmml">R</mi></mrow><mo id="A1.E3.m1.2.2.1.1.1.1.1.3" xref="A1.E3.m1.2.2.1.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="A1.E3.m1.2.2.1.1.3" xref="A1.E3.m1.2.2.1.1.3.cmml">≤</mo><mrow id="A1.E3.m1.2.2.1.1.2" xref="A1.E3.m1.2.2.1.1.2.cmml"><mrow id="A1.E3.m1.2.2.1.1.2.1" xref="A1.E3.m1.2.2.1.1.2.1.cmml"><mrow id="A1.E3.m1.2.2.1.1.2.1.3" xref="A1.E3.m1.2.2.1.1.2.1.3.cmml"><msup id="A1.E3.m1.2.2.1.1.2.1.3.2" xref="A1.E3.m1.2.2.1.1.2.1.3.2.cmml"><mi id="A1.E3.m1.2.2.1.1.2.1.3.2.2" xref="A1.E3.m1.2.2.1.1.2.1.3.2.2.cmml">e</mi><mi id="A1.E3.m1.2.2.1.1.2.1.3.2.3" xref="A1.E3.m1.2.2.1.1.2.1.3.2.3.cmml">ε</mi></msup><mo lspace="0.222em" rspace="0.222em" id="A1.E3.m1.2.2.1.1.2.1.3.1" xref="A1.E3.m1.2.2.1.1.2.1.3.1.cmml">⋅</mo><mi id="A1.E3.m1.2.2.1.1.2.1.3.3" xref="A1.E3.m1.2.2.1.1.2.1.3.3.cmml">ℙ</mi></mrow><mo lspace="0em" rspace="0em" id="A1.E3.m1.2.2.1.1.2.1.2" xref="A1.E3.m1.2.2.1.1.2.1.2.cmml">​</mo><mrow id="A1.E3.m1.2.2.1.1.2.1.1.1" xref="A1.E3.m1.2.2.1.1.2.1.1.2.cmml"><mo id="A1.E3.m1.2.2.1.1.2.1.1.1.2" xref="A1.E3.m1.2.2.1.1.2.1.1.2.1.cmml">[</mo><mrow id="A1.E3.m1.2.2.1.1.2.1.1.1.1" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.cmml"><mrow id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.cmml"><mi id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.3" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.3.cmml">M</mi><mo lspace="0em" rspace="0em" id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.2" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.2.cmml">​</mo><mrow id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.2" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml"><mi id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.2" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.2.cmml">D</mi><mo id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.3" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.3" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A1.E3.m1.2.2.1.1.2.1.1.1.1.2" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.2.cmml">∈</mo><mi id="A1.E3.m1.2.2.1.1.2.1.1.1.1.3" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.3.cmml">R</mi></mrow><mo id="A1.E3.m1.2.2.1.1.2.1.1.1.3" xref="A1.E3.m1.2.2.1.1.2.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="A1.E3.m1.2.2.1.1.2.2" xref="A1.E3.m1.2.2.1.1.2.2.cmml">+</mo><mi id="A1.E3.m1.2.2.1.1.2.3" xref="A1.E3.m1.2.2.1.1.2.3.cmml">δ</mi></mrow></mrow><mo lspace="0.170em" id="A1.E3.m1.2.2.1.2" xref="A1.E3.m1.2.2.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.E3.m1.2b"><apply id="A1.E3.m1.2.2.1.1.cmml" xref="A1.E3.m1.2.2.1"><leq id="A1.E3.m1.2.2.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.3"></leq><apply id="A1.E3.m1.2.2.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.1"><times id="A1.E3.m1.2.2.1.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.1.2"></times><ci id="A1.E3.m1.2.2.1.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.1.3">ℙ</ci><apply id="A1.E3.m1.2.2.1.1.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1"><csymbol cd="latexml" id="A1.E3.m1.2.2.1.1.1.1.2.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.2">delimited-[]</csymbol><apply id="A1.E3.m1.2.2.1.1.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1"><in id="A1.E3.m1.2.2.1.1.1.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1.1"></in><apply id="A1.E3.m1.2.2.1.1.1.1.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2"><times id="A1.E3.m1.2.2.1.1.1.1.1.1.2.1.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2.1"></times><ci id="A1.E3.m1.2.2.1.1.1.1.1.1.2.2.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1.2.2">𝐴</ci><ci id="A1.E3.m1.1.1.cmml" xref="A1.E3.m1.1.1">𝐷</ci></apply><ci id="A1.E3.m1.2.2.1.1.1.1.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.1.1.1.1.3">𝑅</ci></apply></apply></apply><apply id="A1.E3.m1.2.2.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.2"><plus id="A1.E3.m1.2.2.1.1.2.2.cmml" xref="A1.E3.m1.2.2.1.1.2.2"></plus><apply id="A1.E3.m1.2.2.1.1.2.1.cmml" xref="A1.E3.m1.2.2.1.1.2.1"><times id="A1.E3.m1.2.2.1.1.2.1.2.cmml" xref="A1.E3.m1.2.2.1.1.2.1.2"></times><apply id="A1.E3.m1.2.2.1.1.2.1.3.cmml" xref="A1.E3.m1.2.2.1.1.2.1.3"><ci id="A1.E3.m1.2.2.1.1.2.1.3.1.cmml" xref="A1.E3.m1.2.2.1.1.2.1.3.1">⋅</ci><apply id="A1.E3.m1.2.2.1.1.2.1.3.2.cmml" xref="A1.E3.m1.2.2.1.1.2.1.3.2"><csymbol cd="ambiguous" id="A1.E3.m1.2.2.1.1.2.1.3.2.1.cmml" xref="A1.E3.m1.2.2.1.1.2.1.3.2">superscript</csymbol><ci id="A1.E3.m1.2.2.1.1.2.1.3.2.2.cmml" xref="A1.E3.m1.2.2.1.1.2.1.3.2.2">𝑒</ci><ci id="A1.E3.m1.2.2.1.1.2.1.3.2.3.cmml" xref="A1.E3.m1.2.2.1.1.2.1.3.2.3">𝜀</ci></apply><ci id="A1.E3.m1.2.2.1.1.2.1.3.3.cmml" xref="A1.E3.m1.2.2.1.1.2.1.3.3">ℙ</ci></apply><apply id="A1.E3.m1.2.2.1.1.2.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1"><csymbol cd="latexml" id="A1.E3.m1.2.2.1.1.2.1.1.2.1.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.2">delimited-[]</csymbol><apply id="A1.E3.m1.2.2.1.1.2.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1"><in id="A1.E3.m1.2.2.1.1.2.1.1.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.2"></in><apply id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1"><times id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.2"></times><ci id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.3">𝑀</ci><apply id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.1.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1">superscript</csymbol><ci id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.2.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.2">𝐷</ci><ci id="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.1.1.1.1.3">′</ci></apply></apply><ci id="A1.E3.m1.2.2.1.1.2.1.1.1.1.3.cmml" xref="A1.E3.m1.2.2.1.1.2.1.1.1.1.3">𝑅</ci></apply></apply></apply><ci id="A1.E3.m1.2.2.1.1.2.3.cmml" xref="A1.E3.m1.2.2.1.1.2.3">𝛿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.E3.m1.2c">\displaystyle\mathbb{P}\left[A(D)\in R\right]\leq e^{\varepsilon}\cdot\mathbb{P}\left[M(D^{\prime})\in R\right]+\delta\,.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</div>
<div id="A1.p2" class="ltx_para">
<p id="A1.p2.12" class="ltx_p">In ML, DP formalizes the idea that any possible training data point in a training dataset cannot significantly impact the resulting ML model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>.
One notable approach to achieve this is Differentially Private Stochastic Gradient Descent (DPSGD) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
DPSGD alters the training process to introduce DP guarantees for weight update operations, and thereby protects underlying individual data points.
Particularly, the gradient computed for each data point or a mini-batch of data points is first clipped in their <math id="A1.p2.1.m1.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="A1.p2.1.m1.1a"><msub id="A1.p2.1.m1.1.1" xref="A1.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="A1.p2.1.m1.1.1.2" xref="A1.p2.1.m1.1.1.2.cmml">ℓ</mi><mn id="A1.p2.1.m1.1.1.3" xref="A1.p2.1.m1.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.p2.1.m1.1b"><apply id="A1.p2.1.m1.1.1.cmml" xref="A1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A1.p2.1.m1.1.1.1.cmml" xref="A1.p2.1.m1.1.1">subscript</csymbol><ci id="A1.p2.1.m1.1.1.2.cmml" xref="A1.p2.1.m1.1.1.2">ℓ</ci><cn type="integer" id="A1.p2.1.m1.1.1.3.cmml" xref="A1.p2.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.1.m1.1c">\ell_{2}</annotation></semantics></math>-norm to bound influence.
Clipping of the gradient <math id="A1.p2.2.m2.1" class="ltx_Math" alttext="g(\{x_{i}\}_{b})" display="inline"><semantics id="A1.p2.2.m2.1a"><mrow id="A1.p2.2.m2.1.1" xref="A1.p2.2.m2.1.1.cmml"><mi id="A1.p2.2.m2.1.1.3" xref="A1.p2.2.m2.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="A1.p2.2.m2.1.1.2" xref="A1.p2.2.m2.1.1.2.cmml">​</mo><mrow id="A1.p2.2.m2.1.1.1.1" xref="A1.p2.2.m2.1.1.1.1.1.cmml"><mo stretchy="false" id="A1.p2.2.m2.1.1.1.1.2" xref="A1.p2.2.m2.1.1.1.1.1.cmml">(</mo><msub id="A1.p2.2.m2.1.1.1.1.1" xref="A1.p2.2.m2.1.1.1.1.1.cmml"><mrow id="A1.p2.2.m2.1.1.1.1.1.1.1" xref="A1.p2.2.m2.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="A1.p2.2.m2.1.1.1.1.1.1.1.2" xref="A1.p2.2.m2.1.1.1.1.1.1.2.cmml">{</mo><msub id="A1.p2.2.m2.1.1.1.1.1.1.1.1" xref="A1.p2.2.m2.1.1.1.1.1.1.1.1.cmml"><mi id="A1.p2.2.m2.1.1.1.1.1.1.1.1.2" xref="A1.p2.2.m2.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="A1.p2.2.m2.1.1.1.1.1.1.1.1.3" xref="A1.p2.2.m2.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="A1.p2.2.m2.1.1.1.1.1.1.1.3" xref="A1.p2.2.m2.1.1.1.1.1.1.2.cmml">}</mo></mrow><mi id="A1.p2.2.m2.1.1.1.1.1.3" xref="A1.p2.2.m2.1.1.1.1.1.3.cmml">b</mi></msub><mo stretchy="false" id="A1.p2.2.m2.1.1.1.1.3" xref="A1.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.2.m2.1b"><apply id="A1.p2.2.m2.1.1.cmml" xref="A1.p2.2.m2.1.1"><times id="A1.p2.2.m2.1.1.2.cmml" xref="A1.p2.2.m2.1.1.2"></times><ci id="A1.p2.2.m2.1.1.3.cmml" xref="A1.p2.2.m2.1.1.3">𝑔</ci><apply id="A1.p2.2.m2.1.1.1.1.1.cmml" xref="A1.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.2.m2.1.1.1.1.1.2.cmml" xref="A1.p2.2.m2.1.1.1.1">subscript</csymbol><set id="A1.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="A1.p2.2.m2.1.1.1.1.1.1.1"><apply id="A1.p2.2.m2.1.1.1.1.1.1.1.1.cmml" xref="A1.p2.2.m2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="A1.p2.2.m2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="A1.p2.2.m2.1.1.1.1.1.1.1.1.2.cmml" xref="A1.p2.2.m2.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="A1.p2.2.m2.1.1.1.1.1.1.1.1.3.cmml" xref="A1.p2.2.m2.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><ci id="A1.p2.2.m2.1.1.1.1.1.3.cmml" xref="A1.p2.2.m2.1.1.1.1.1.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.2.m2.1c">g(\{x_{i}\}_{b})</annotation></semantics></math> for mini-batch <math id="A1.p2.3.m3.1" class="ltx_Math" alttext="b" display="inline"><semantics id="A1.p2.3.m3.1a"><mi id="A1.p2.3.m3.1.1" xref="A1.p2.3.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="A1.p2.3.m3.1b"><ci id="A1.p2.3.m3.1.1.cmml" xref="A1.p2.3.m3.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.3.m3.1c">b</annotation></semantics></math> of data <math id="A1.p2.4.m4.1" class="ltx_Math" alttext="\{x_{i}\}_{b}" display="inline"><semantics id="A1.p2.4.m4.1a"><msub id="A1.p2.4.m4.1.1" xref="A1.p2.4.m4.1.1.cmml"><mrow id="A1.p2.4.m4.1.1.1.1" xref="A1.p2.4.m4.1.1.1.2.cmml"><mo stretchy="false" id="A1.p2.4.m4.1.1.1.1.2" xref="A1.p2.4.m4.1.1.1.2.cmml">{</mo><msub id="A1.p2.4.m4.1.1.1.1.1" xref="A1.p2.4.m4.1.1.1.1.1.cmml"><mi id="A1.p2.4.m4.1.1.1.1.1.2" xref="A1.p2.4.m4.1.1.1.1.1.2.cmml">x</mi><mi id="A1.p2.4.m4.1.1.1.1.1.3" xref="A1.p2.4.m4.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="A1.p2.4.m4.1.1.1.1.3" xref="A1.p2.4.m4.1.1.1.2.cmml">}</mo></mrow><mi id="A1.p2.4.m4.1.1.3" xref="A1.p2.4.m4.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="A1.p2.4.m4.1b"><apply id="A1.p2.4.m4.1.1.cmml" xref="A1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="A1.p2.4.m4.1.1.2.cmml" xref="A1.p2.4.m4.1.1">subscript</csymbol><set id="A1.p2.4.m4.1.1.1.2.cmml" xref="A1.p2.4.m4.1.1.1.1"><apply id="A1.p2.4.m4.1.1.1.1.1.cmml" xref="A1.p2.4.m4.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.4.m4.1.1.1.1.1.1.cmml" xref="A1.p2.4.m4.1.1.1.1.1">subscript</csymbol><ci id="A1.p2.4.m4.1.1.1.1.1.2.cmml" xref="A1.p2.4.m4.1.1.1.1.1.2">𝑥</ci><ci id="A1.p2.4.m4.1.1.1.1.1.3.cmml" xref="A1.p2.4.m4.1.1.1.1.1.3">𝑖</ci></apply></set><ci id="A1.p2.4.m4.1.1.3.cmml" xref="A1.p2.4.m4.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.4.m4.1c">\{x_{i}\}_{b}</annotation></semantics></math> is performed according to a clipping parameter <math id="A1.p2.5.m5.1" class="ltx_Math" alttext="c" display="inline"><semantics id="A1.p2.5.m5.1a"><mi id="A1.p2.5.m5.1.1" xref="A1.p2.5.m5.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A1.p2.5.m5.1b"><ci id="A1.p2.5.m5.1.1.cmml" xref="A1.p2.5.m5.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.5.m5.1c">c</annotation></semantics></math>, by replacing <math id="A1.p2.6.m6.1" class="ltx_Math" alttext="g(\{x_{i}\}_{b})" display="inline"><semantics id="A1.p2.6.m6.1a"><mrow id="A1.p2.6.m6.1.1" xref="A1.p2.6.m6.1.1.cmml"><mi id="A1.p2.6.m6.1.1.3" xref="A1.p2.6.m6.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="A1.p2.6.m6.1.1.2" xref="A1.p2.6.m6.1.1.2.cmml">​</mo><mrow id="A1.p2.6.m6.1.1.1.1" xref="A1.p2.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="A1.p2.6.m6.1.1.1.1.2" xref="A1.p2.6.m6.1.1.1.1.1.cmml">(</mo><msub id="A1.p2.6.m6.1.1.1.1.1" xref="A1.p2.6.m6.1.1.1.1.1.cmml"><mrow id="A1.p2.6.m6.1.1.1.1.1.1.1" xref="A1.p2.6.m6.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="A1.p2.6.m6.1.1.1.1.1.1.1.2" xref="A1.p2.6.m6.1.1.1.1.1.1.2.cmml">{</mo><msub id="A1.p2.6.m6.1.1.1.1.1.1.1.1" xref="A1.p2.6.m6.1.1.1.1.1.1.1.1.cmml"><mi id="A1.p2.6.m6.1.1.1.1.1.1.1.1.2" xref="A1.p2.6.m6.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="A1.p2.6.m6.1.1.1.1.1.1.1.1.3" xref="A1.p2.6.m6.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="A1.p2.6.m6.1.1.1.1.1.1.1.3" xref="A1.p2.6.m6.1.1.1.1.1.1.2.cmml">}</mo></mrow><mi id="A1.p2.6.m6.1.1.1.1.1.3" xref="A1.p2.6.m6.1.1.1.1.1.3.cmml">b</mi></msub><mo stretchy="false" id="A1.p2.6.m6.1.1.1.1.3" xref="A1.p2.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.6.m6.1b"><apply id="A1.p2.6.m6.1.1.cmml" xref="A1.p2.6.m6.1.1"><times id="A1.p2.6.m6.1.1.2.cmml" xref="A1.p2.6.m6.1.1.2"></times><ci id="A1.p2.6.m6.1.1.3.cmml" xref="A1.p2.6.m6.1.1.3">𝑔</ci><apply id="A1.p2.6.m6.1.1.1.1.1.cmml" xref="A1.p2.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.6.m6.1.1.1.1.1.2.cmml" xref="A1.p2.6.m6.1.1.1.1">subscript</csymbol><set id="A1.p2.6.m6.1.1.1.1.1.1.2.cmml" xref="A1.p2.6.m6.1.1.1.1.1.1.1"><apply id="A1.p2.6.m6.1.1.1.1.1.1.1.1.cmml" xref="A1.p2.6.m6.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.6.m6.1.1.1.1.1.1.1.1.1.cmml" xref="A1.p2.6.m6.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="A1.p2.6.m6.1.1.1.1.1.1.1.1.2.cmml" xref="A1.p2.6.m6.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="A1.p2.6.m6.1.1.1.1.1.1.1.1.3.cmml" xref="A1.p2.6.m6.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><ci id="A1.p2.6.m6.1.1.1.1.1.3.cmml" xref="A1.p2.6.m6.1.1.1.1.1.3">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.6.m6.1c">g(\{x_{i}\}_{b})</annotation></semantics></math> with <math id="A1.p2.7.m7.4" class="ltx_Math" alttext="g(\{x_{i}\}_{b})/\max(1,\frac{||g(\{x_{i}\}_{b})||_{2}}{c})" display="inline"><semantics id="A1.p2.7.m7.4a"><mrow id="A1.p2.7.m7.4.4" xref="A1.p2.7.m7.4.4.cmml"><mrow id="A1.p2.7.m7.4.4.1" xref="A1.p2.7.m7.4.4.1.cmml"><mi id="A1.p2.7.m7.4.4.1.3" xref="A1.p2.7.m7.4.4.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="A1.p2.7.m7.4.4.1.2" xref="A1.p2.7.m7.4.4.1.2.cmml">​</mo><mrow id="A1.p2.7.m7.4.4.1.1.1" xref="A1.p2.7.m7.4.4.1.1.1.1.cmml"><mo stretchy="false" id="A1.p2.7.m7.4.4.1.1.1.2" xref="A1.p2.7.m7.4.4.1.1.1.1.cmml">(</mo><msub id="A1.p2.7.m7.4.4.1.1.1.1" xref="A1.p2.7.m7.4.4.1.1.1.1.cmml"><mrow id="A1.p2.7.m7.4.4.1.1.1.1.1.1" xref="A1.p2.7.m7.4.4.1.1.1.1.1.2.cmml"><mo stretchy="false" id="A1.p2.7.m7.4.4.1.1.1.1.1.1.2" xref="A1.p2.7.m7.4.4.1.1.1.1.1.2.cmml">{</mo><msub id="A1.p2.7.m7.4.4.1.1.1.1.1.1.1" xref="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.cmml"><mi id="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.2" xref="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.3" xref="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="A1.p2.7.m7.4.4.1.1.1.1.1.1.3" xref="A1.p2.7.m7.4.4.1.1.1.1.1.2.cmml">}</mo></mrow><mi id="A1.p2.7.m7.4.4.1.1.1.1.3" xref="A1.p2.7.m7.4.4.1.1.1.1.3.cmml">b</mi></msub><mo stretchy="false" id="A1.p2.7.m7.4.4.1.1.1.3" xref="A1.p2.7.m7.4.4.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A1.p2.7.m7.4.4.2" xref="A1.p2.7.m7.4.4.2.cmml">/</mo><mrow id="A1.p2.7.m7.4.4.3.2" xref="A1.p2.7.m7.4.4.3.1.cmml"><mi id="A1.p2.7.m7.2.2" xref="A1.p2.7.m7.2.2.cmml">max</mi><mo id="A1.p2.7.m7.4.4.3.2a" xref="A1.p2.7.m7.4.4.3.1.cmml">⁡</mo><mrow id="A1.p2.7.m7.4.4.3.2.1" xref="A1.p2.7.m7.4.4.3.1.cmml"><mo stretchy="false" id="A1.p2.7.m7.4.4.3.2.1.1" xref="A1.p2.7.m7.4.4.3.1.cmml">(</mo><mn id="A1.p2.7.m7.3.3" xref="A1.p2.7.m7.3.3.cmml">1</mn><mo id="A1.p2.7.m7.4.4.3.2.1.2" xref="A1.p2.7.m7.4.4.3.1.cmml">,</mo><mfrac id="A1.p2.7.m7.1.1" xref="A1.p2.7.m7.1.1.cmml"><msub id="A1.p2.7.m7.1.1.1" xref="A1.p2.7.m7.1.1.1.cmml"><mrow id="A1.p2.7.m7.1.1.1.1.1" xref="A1.p2.7.m7.1.1.1.1.2.cmml"><mo maxsize="142%" minsize="142%" id="A1.p2.7.m7.1.1.1.1.1.2" xref="A1.p2.7.m7.1.1.1.1.2.1.cmml">‖</mo><mrow id="A1.p2.7.m7.1.1.1.1.1.1" xref="A1.p2.7.m7.1.1.1.1.1.1.cmml"><mi id="A1.p2.7.m7.1.1.1.1.1.1.3" xref="A1.p2.7.m7.1.1.1.1.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="A1.p2.7.m7.1.1.1.1.1.1.2" xref="A1.p2.7.m7.1.1.1.1.1.1.2.cmml">​</mo><mrow id="A1.p2.7.m7.1.1.1.1.1.1.1.1" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="A1.p2.7.m7.1.1.1.1.1.1.1.1.2" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml"><mrow id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.2" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.2.cmml">{</mo><msub id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.3" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mi id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.3" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.3.cmml">b</mi></msub><mo stretchy="false" id="A1.p2.7.m7.1.1.1.1.1.1.1.1.3" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo maxsize="142%" minsize="142%" id="A1.p2.7.m7.1.1.1.1.1.3" xref="A1.p2.7.m7.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="A1.p2.7.m7.1.1.1.3" xref="A1.p2.7.m7.1.1.1.3.cmml">2</mn></msub><mi id="A1.p2.7.m7.1.1.3" xref="A1.p2.7.m7.1.1.3.cmml">c</mi></mfrac><mo stretchy="false" id="A1.p2.7.m7.4.4.3.2.1.3" xref="A1.p2.7.m7.4.4.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.7.m7.4b"><apply id="A1.p2.7.m7.4.4.cmml" xref="A1.p2.7.m7.4.4"><divide id="A1.p2.7.m7.4.4.2.cmml" xref="A1.p2.7.m7.4.4.2"></divide><apply id="A1.p2.7.m7.4.4.1.cmml" xref="A1.p2.7.m7.4.4.1"><times id="A1.p2.7.m7.4.4.1.2.cmml" xref="A1.p2.7.m7.4.4.1.2"></times><ci id="A1.p2.7.m7.4.4.1.3.cmml" xref="A1.p2.7.m7.4.4.1.3">𝑔</ci><apply id="A1.p2.7.m7.4.4.1.1.1.1.cmml" xref="A1.p2.7.m7.4.4.1.1.1"><csymbol cd="ambiguous" id="A1.p2.7.m7.4.4.1.1.1.1.2.cmml" xref="A1.p2.7.m7.4.4.1.1.1">subscript</csymbol><set id="A1.p2.7.m7.4.4.1.1.1.1.1.2.cmml" xref="A1.p2.7.m7.4.4.1.1.1.1.1.1"><apply id="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.cmml" xref="A1.p2.7.m7.4.4.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.1.cmml" xref="A1.p2.7.m7.4.4.1.1.1.1.1.1.1">subscript</csymbol><ci id="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.2.cmml" xref="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.2">𝑥</ci><ci id="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.3.cmml" xref="A1.p2.7.m7.4.4.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><ci id="A1.p2.7.m7.4.4.1.1.1.1.3.cmml" xref="A1.p2.7.m7.4.4.1.1.1.1.3">𝑏</ci></apply></apply><apply id="A1.p2.7.m7.4.4.3.1.cmml" xref="A1.p2.7.m7.4.4.3.2"><max id="A1.p2.7.m7.2.2.cmml" xref="A1.p2.7.m7.2.2"></max><cn type="integer" id="A1.p2.7.m7.3.3.cmml" xref="A1.p2.7.m7.3.3">1</cn><apply id="A1.p2.7.m7.1.1.cmml" xref="A1.p2.7.m7.1.1"><divide id="A1.p2.7.m7.1.1.2.cmml" xref="A1.p2.7.m7.1.1"></divide><apply id="A1.p2.7.m7.1.1.1.cmml" xref="A1.p2.7.m7.1.1.1"><csymbol cd="ambiguous" id="A1.p2.7.m7.1.1.1.2.cmml" xref="A1.p2.7.m7.1.1.1">subscript</csymbol><apply id="A1.p2.7.m7.1.1.1.1.2.cmml" xref="A1.p2.7.m7.1.1.1.1.1"><csymbol cd="latexml" id="A1.p2.7.m7.1.1.1.1.2.1.cmml" xref="A1.p2.7.m7.1.1.1.1.1.2">norm</csymbol><apply id="A1.p2.7.m7.1.1.1.1.1.1.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1"><times id="A1.p2.7.m7.1.1.1.1.1.1.2.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.2"></times><ci id="A1.p2.7.m7.1.1.1.1.1.1.3.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.3">𝑔</ci><apply id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1">subscript</csymbol><set id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1"><apply id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><ci id="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.3.cmml" xref="A1.p2.7.m7.1.1.1.1.1.1.1.1.1.3">𝑏</ci></apply></apply></apply><cn type="integer" id="A1.p2.7.m7.1.1.1.3.cmml" xref="A1.p2.7.m7.1.1.1.3">2</cn></apply><ci id="A1.p2.7.m7.1.1.3.cmml" xref="A1.p2.7.m7.1.1.3">𝑐</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.7.m7.4c">g(\{x_{i}\}_{b})/\max(1,\frac{||g(\{x_{i}\}_{b})||_{2}}{c})</annotation></semantics></math>. This ensures that if the <math id="A1.p2.8.m8.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="A1.p2.8.m8.1a"><msub id="A1.p2.8.m8.1.1" xref="A1.p2.8.m8.1.1.cmml"><mi mathvariant="normal" id="A1.p2.8.m8.1.1.2" xref="A1.p2.8.m8.1.1.2.cmml">ℓ</mi><mn id="A1.p2.8.m8.1.1.3" xref="A1.p2.8.m8.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="A1.p2.8.m8.1b"><apply id="A1.p2.8.m8.1.1.cmml" xref="A1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="A1.p2.8.m8.1.1.1.cmml" xref="A1.p2.8.m8.1.1">subscript</csymbol><ci id="A1.p2.8.m8.1.1.2.cmml" xref="A1.p2.8.m8.1.1.2">ℓ</ci><cn type="integer" id="A1.p2.8.m8.1.1.3.cmml" xref="A1.p2.8.m8.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.8.m8.1c">\ell_{2}</annotation></semantics></math>-norm of the gradients is <math id="A1.p2.9.m9.1" class="ltx_Math" alttext="\leq c" display="inline"><semantics id="A1.p2.9.m9.1a"><mrow id="A1.p2.9.m9.1.1" xref="A1.p2.9.m9.1.1.cmml"><mi id="A1.p2.9.m9.1.1.2" xref="A1.p2.9.m9.1.1.2.cmml"></mi><mo id="A1.p2.9.m9.1.1.1" xref="A1.p2.9.m9.1.1.1.cmml">≤</mo><mi id="A1.p2.9.m9.1.1.3" xref="A1.p2.9.m9.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.9.m9.1b"><apply id="A1.p2.9.m9.1.1.cmml" xref="A1.p2.9.m9.1.1"><leq id="A1.p2.9.m9.1.1.1.cmml" xref="A1.p2.9.m9.1.1.1"></leq><csymbol cd="latexml" id="A1.p2.9.m9.1.1.2.cmml" xref="A1.p2.9.m9.1.1.2">absent</csymbol><ci id="A1.p2.9.m9.1.1.3.cmml" xref="A1.p2.9.m9.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.9.m9.1c">\leq c</annotation></semantics></math>, the gradients stays unaltered, and if the norm is <math id="A1.p2.10.m10.1" class="ltx_Math" alttext="&gt;c" display="inline"><semantics id="A1.p2.10.m10.1a"><mrow id="A1.p2.10.m10.1.1" xref="A1.p2.10.m10.1.1.cmml"><mi id="A1.p2.10.m10.1.1.2" xref="A1.p2.10.m10.1.1.2.cmml"></mi><mo id="A1.p2.10.m10.1.1.1" xref="A1.p2.10.m10.1.1.1.cmml">&gt;</mo><mi id="A1.p2.10.m10.1.1.3" xref="A1.p2.10.m10.1.1.3.cmml">c</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.p2.10.m10.1b"><apply id="A1.p2.10.m10.1.1.cmml" xref="A1.p2.10.m10.1.1"><gt id="A1.p2.10.m10.1.1.1.cmml" xref="A1.p2.10.m10.1.1.1"></gt><csymbol cd="latexml" id="A1.p2.10.m10.1.1.2.cmml" xref="A1.p2.10.m10.1.1.2">absent</csymbol><ci id="A1.p2.10.m10.1.1.3.cmml" xref="A1.p2.10.m10.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.10.m10.1c">&gt;c</annotation></semantics></math>, the gradient get scaled down to be in norm of <math id="A1.p2.11.m11.1" class="ltx_Math" alttext="c" display="inline"><semantics id="A1.p2.11.m11.1a"><mi id="A1.p2.11.m11.1.1" xref="A1.p2.11.m11.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="A1.p2.11.m11.1b"><ci id="A1.p2.11.m11.1.1.cmml" xref="A1.p2.11.m11.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.11.m11.1c">c</annotation></semantics></math>.
After the clipping, Gaussian noise with scale <math id="A1.p2.12.m12.1" class="ltx_Math" alttext="\sigma" display="inline"><semantics id="A1.p2.12.m12.1a"><mi id="A1.p2.12.m12.1.1" xref="A1.p2.12.m12.1.1.cmml">σ</mi><annotation-xml encoding="MathML-Content" id="A1.p2.12.m12.1b"><ci id="A1.p2.12.m12.1.1.cmml" xref="A1.p2.12.m12.1.1">𝜎</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p2.12.m12.1c">\sigma</annotation></semantics></math> is applied to the gradients of each mini-batch before performing the model updates.

<span id="A1.p2.12.1" class="ltx_text" style="color:#000000;"></span></p>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="color:#000000;">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Extended Experimental Evaluation</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p"><span id="A2.p1.1.1" class="ltx_text" style="color:#000000;">We present an extended experimental evaluation on two additional image and two textual datasets for spam classification.</span></p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">B.1 </span><span id="A2.SS1.1.1" class="ltx_text ltx_font_italic">Datasets</span>
</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p"><span id="A2.SS1.p1.1.1" class="ltx_text" style="color:#000000;">We describe the additional datasets used for our extended experimentation.</span></p>
</div>
<div id="A2.SS1.p2" class="ltx_para ltx_noindent">
<p id="A2.SS1.p2.1" class="ltx_p"><span id="A2.SS1.p2.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">MNIST</span><span id="A2.SS1.p2.1.2" class="ltx_text" style="color:#000000;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.SS1.p2.1.3.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="A2.SS1.p2.1.4.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="A2.SS1.p2.1.5" class="ltx_text" style="color:#000000;"> is a vision dataset consisting of 70,000 gray-scale images and corresponding labels for ten classes.
The images are of size 28x28 pixels and depict the hand-written digits zero to nine.
The dataset is divided into a training set consisting of 60,000 images and a test set consisting of 10,000 images.</span></p>
</div>
<div id="A2.SS1.p3" class="ltx_para ltx_noindent">
<p id="A2.SS1.p3.1" class="ltx_p"><span id="A2.SS1.p3.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">ImageNet</span><span id="A2.SS1.p3.1.2" class="ltx_text" style="color:#000000;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.SS1.p3.1.3.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="A2.SS1.p3.1.4.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="A2.SS1.p3.1.5" class="ltx_text" style="color:#000000;"> is a large and complex vision dataset containing color images with 224x224x3 pixels that belong to 1,000 different classes.
The dataset contains 1,281,167 training, 50,000 validation, and 100,000 test images.</span></p>
</div>
<div id="A2.SS1.p4" class="ltx_para ltx_noindent">
<p id="A2.SS1.p4.1" class="ltx_p"><span id="A2.SS1.p4.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">Spam Mails Dataset.</span><span id="A2.SS1.p4.1.2" class="ltx_text" style="color:#000000;">
The Enron-Spam datasets </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.SS1.p4.1.3.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="A2.SS1.p4.1.4.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="A2.SS1.p4.1.5" class="ltx_text" style="color:#000000;"> contains 5,170 emails in English language which are tagged as legitimate or spam.
We used the the dataset from Kaggle</span><span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://www.kaggle.com/datasets/venky73/spam-mails-dataset</span></span></span><span id="A2.SS1.p4.1.6" class="ltx_text" style="color:#000000;">.</span></p>
</div>
<div id="A2.SS1.p5" class="ltx_para ltx_noindent">
<p id="A2.SS1.p5.1" class="ltx_p"><span id="A2.SS1.p5.1.1" class="ltx_text ltx_font_bold" style="color:#000000;">SMS Spam Collection Dataset.</span><span id="A2.SS1.p5.1.2" class="ltx_text" style="color:#000000;">
We used the SMS Spam Collection dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.SS1.p5.1.3.1" class="ltx_text" style="color:#000000;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="A2.SS1.p5.1.4.2" class="ltx_text" style="color:#000000;">]</span></cite><span id="A2.SS1.p5.1.5" class="ltx_text" style="color:#000000;"> from Kaggle</span><span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset</span></span></span><span id="A2.SS1.p5.1.6" class="ltx_text" style="color:#000000;"> which consists of a set of SMS messages in English language. In total, it holds 5,574 SMS messages, tagged as legitimate or spam.</span></p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">B.2 </span><span id="A2.SS2.1.1" class="ltx_text ltx_font_italic">Experimental Results on Image Data</span>
</h3>

<figure id="A2.F14" class="ltx_figure"><img src="/html/2301.04017/assets/x20.png" id="A2.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="A2.F14.14.4.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="A2.F14.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">MNIST: Directly Extracted Data under DDP.<span id="A2.F14.6.3.3" class="ltx_text ltx_font_medium"> Rescaled clipped and noised gradients from a mini-batch with 20 data points from CIFAR10 dataset.
DDP setup: <math id="A2.F14.4.1.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="A2.F14.4.1.1.m1.1b"><mrow id="A2.F14.4.1.1.m1.1.1" xref="A2.F14.4.1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="A2.F14.4.1.1.m1.1.1.2" xref="A2.F14.4.1.1.m1.1.1.2.cmml">c</mi><mo mathcolor="#000000" id="A2.F14.4.1.1.m1.1.1.1" xref="A2.F14.4.1.1.m1.1.1.1.cmml">=</mo><mn mathcolor="#000000" id="A2.F14.4.1.1.m1.1.1.3" xref="A2.F14.4.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.F14.4.1.1.m1.1c"><apply id="A2.F14.4.1.1.m1.1.1.cmml" xref="A2.F14.4.1.1.m1.1.1"><eq id="A2.F14.4.1.1.m1.1.1.1.cmml" xref="A2.F14.4.1.1.m1.1.1.1"></eq><ci id="A2.F14.4.1.1.m1.1.1.2.cmml" xref="A2.F14.4.1.1.m1.1.1.2">𝑐</ci><cn type="integer" id="A2.F14.4.1.1.m1.1.1.3.cmml" xref="A2.F14.4.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F14.4.1.1.m1.1d">c=1</annotation></semantics></math>, <math id="A2.F14.5.2.2.m2.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="A2.F14.5.2.2.m2.1b"><mrow id="A2.F14.5.2.2.m2.1.1" xref="A2.F14.5.2.2.m2.1.1.cmml"><mi mathcolor="#000000" id="A2.F14.5.2.2.m2.1.1.2" xref="A2.F14.5.2.2.m2.1.1.2.cmml">σ</mi><mo mathcolor="#000000" id="A2.F14.5.2.2.m2.1.1.1" xref="A2.F14.5.2.2.m2.1.1.1.cmml">=</mo><mn mathcolor="#000000" id="A2.F14.5.2.2.m2.1.1.3" xref="A2.F14.5.2.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.F14.5.2.2.m2.1c"><apply id="A2.F14.5.2.2.m2.1.1.cmml" xref="A2.F14.5.2.2.m2.1.1"><eq id="A2.F14.5.2.2.m2.1.1.1.cmml" xref="A2.F14.5.2.2.m2.1.1.1"></eq><ci id="A2.F14.5.2.2.m2.1.1.2.cmml" xref="A2.F14.5.2.2.m2.1.1.2">𝜎</ci><cn type="float" id="A2.F14.5.2.2.m2.1.1.3.cmml" xref="A2.F14.5.2.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F14.5.2.2.m2.1d">\sigma=0.1</annotation></semantics></math>, and <math id="A2.F14.6.3.3.m3.1" class="ltx_Math" alttext="\mathsf{M}=100" display="inline"><semantics id="A2.F14.6.3.3.m3.1b"><mrow id="A2.F14.6.3.3.m3.1.1" xref="A2.F14.6.3.3.m3.1.1.cmml"><mi mathcolor="#000000" id="A2.F14.6.3.3.m3.1.1.2" xref="A2.F14.6.3.3.m3.1.1.2.cmml">𝖬</mi><mo mathcolor="#000000" id="A2.F14.6.3.3.m3.1.1.1" xref="A2.F14.6.3.3.m3.1.1.1.cmml">=</mo><mn mathcolor="#000000" id="A2.F14.6.3.3.m3.1.1.3" xref="A2.F14.6.3.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.F14.6.3.3.m3.1c"><apply id="A2.F14.6.3.3.m3.1.1.cmml" xref="A2.F14.6.3.3.m3.1.1"><eq id="A2.F14.6.3.3.m3.1.1.1.cmml" xref="A2.F14.6.3.3.m3.1.1.1"></eq><ci id="A2.F14.6.3.3.m3.1.1.2.cmml" xref="A2.F14.6.3.3.m3.1.1.2">𝖬</ci><cn type="integer" id="A2.F14.6.3.3.m3.1.1.3.cmml" xref="A2.F14.6.3.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F14.6.3.3.m3.1d">\mathsf{M}=100</annotation></semantics></math>.</span></span></figcaption>
</figure>
<figure id="A2.F15" class="ltx_figure"><img src="/html/2301.04017/assets/x21.png" id="A2.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="106" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="A2.F15.14.4.1" class="ltx_text" style="font-size:90%;">Figure 15</span>: </span><span id="A2.F15.6.3" class="ltx_text ltx_font_bold" style="font-size:90%;">ImageNet: Directly Extracted Data under DDP.<span id="A2.F15.6.3.3" class="ltx_text ltx_font_medium"> Rescaled clipped and noised gradients from a mini-batch with 20 data points from CIFAR10 dataset.
DDP setup: <math id="A2.F15.4.1.1.m1.1" class="ltx_Math" alttext="c=1" display="inline"><semantics id="A2.F15.4.1.1.m1.1b"><mrow id="A2.F15.4.1.1.m1.1.1" xref="A2.F15.4.1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="A2.F15.4.1.1.m1.1.1.2" xref="A2.F15.4.1.1.m1.1.1.2.cmml">c</mi><mo mathcolor="#000000" id="A2.F15.4.1.1.m1.1.1.1" xref="A2.F15.4.1.1.m1.1.1.1.cmml">=</mo><mn mathcolor="#000000" id="A2.F15.4.1.1.m1.1.1.3" xref="A2.F15.4.1.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.F15.4.1.1.m1.1c"><apply id="A2.F15.4.1.1.m1.1.1.cmml" xref="A2.F15.4.1.1.m1.1.1"><eq id="A2.F15.4.1.1.m1.1.1.1.cmml" xref="A2.F15.4.1.1.m1.1.1.1"></eq><ci id="A2.F15.4.1.1.m1.1.1.2.cmml" xref="A2.F15.4.1.1.m1.1.1.2">𝑐</ci><cn type="integer" id="A2.F15.4.1.1.m1.1.1.3.cmml" xref="A2.F15.4.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F15.4.1.1.m1.1d">c=1</annotation></semantics></math>, <math id="A2.F15.5.2.2.m2.1" class="ltx_Math" alttext="\sigma=0.1" display="inline"><semantics id="A2.F15.5.2.2.m2.1b"><mrow id="A2.F15.5.2.2.m2.1.1" xref="A2.F15.5.2.2.m2.1.1.cmml"><mi mathcolor="#000000" id="A2.F15.5.2.2.m2.1.1.2" xref="A2.F15.5.2.2.m2.1.1.2.cmml">σ</mi><mo mathcolor="#000000" id="A2.F15.5.2.2.m2.1.1.1" xref="A2.F15.5.2.2.m2.1.1.1.cmml">=</mo><mn mathcolor="#000000" id="A2.F15.5.2.2.m2.1.1.3" xref="A2.F15.5.2.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.F15.5.2.2.m2.1c"><apply id="A2.F15.5.2.2.m2.1.1.cmml" xref="A2.F15.5.2.2.m2.1.1"><eq id="A2.F15.5.2.2.m2.1.1.1.cmml" xref="A2.F15.5.2.2.m2.1.1.1"></eq><ci id="A2.F15.5.2.2.m2.1.1.2.cmml" xref="A2.F15.5.2.2.m2.1.1.2">𝜎</ci><cn type="float" id="A2.F15.5.2.2.m2.1.1.3.cmml" xref="A2.F15.5.2.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F15.5.2.2.m2.1d">\sigma=0.1</annotation></semantics></math>, and <math id="A2.F15.6.3.3.m3.1" class="ltx_Math" alttext="\mathsf{M}=100" display="inline"><semantics id="A2.F15.6.3.3.m3.1b"><mrow id="A2.F15.6.3.3.m3.1.1" xref="A2.F15.6.3.3.m3.1.1.cmml"><mi mathcolor="#000000" id="A2.F15.6.3.3.m3.1.1.2" xref="A2.F15.6.3.3.m3.1.1.2.cmml">𝖬</mi><mo mathcolor="#000000" id="A2.F15.6.3.3.m3.1.1.1" xref="A2.F15.6.3.3.m3.1.1.1.cmml">=</mo><mn mathcolor="#000000" id="A2.F15.6.3.3.m3.1.1.3" xref="A2.F15.6.3.3.m3.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.F15.6.3.3.m3.1c"><apply id="A2.F15.6.3.3.m3.1.1.cmml" xref="A2.F15.6.3.3.m3.1.1"><eq id="A2.F15.6.3.3.m3.1.1.1.cmml" xref="A2.F15.6.3.3.m3.1.1.1"></eq><ci id="A2.F15.6.3.3.m3.1.1.2.cmml" xref="A2.F15.6.3.3.m3.1.1.2">𝖬</ci><cn type="integer" id="A2.F15.6.3.3.m3.1.1.3.cmml" xref="A2.F15.6.3.3.m3.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.F15.6.3.3.m3.1d">\mathsf{M}=100</annotation></semantics></math>.</span></span></figcaption>
</figure>
<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p"><span id="A2.SS2.p1.1.1" class="ltx_text" style="color:#000000;">We first analyze the fraction of noisy data points that can be extracted from the gradients for MNIST and ImageNet.
With a mini-batch size of 20, for MNIST roughly  95% and for ImageNet roughly  59% of noisy data points can be extracted.
We depict the rescaled extracted data points from gradients for MNIST in </span><a href="#A2.F14" title="In B.2 Experimental Results on Image Data ‣ Appendix B Extended Experimental Evaluation ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a><span id="A2.SS2.p1.1.2" class="ltx_text" style="color:#000000;"> and for ImageNet in </span><a href="#A2.F15" title="In B.2 Experimental Results on Image Data ‣ Appendix B Extended Experimental Evaluation ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">15</span></a><span id="A2.SS2.p1.1.3" class="ltx_text" style="color:#000000;">.</span></p>
</div>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="color:#000000;">
<span class="ltx_tag ltx_tag_subsection">B.3 </span><span id="A2.SS3.1.1" class="ltx_text ltx_font_italic">Experimental Results on Textual Data</span>
</h3>

<figure id="A2.F16" class="ltx_figure"><img src="/html/2301.04017/assets/figures/other_datasets/language_eps_email_spam.png" id="A2.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="327" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="A2.F16.5.1.1" class="ltx_text" style="font-size:90%;">Figure 16</span>: </span><span id="A2.F16.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">Email Data Extraction under Noise.<span id="A2.F16.6.2.1" class="ltx_text ltx_font_medium"> Extraction performance under noise for DDP from language model on the Enron-Spam dataset. Extraction remains successful, even in presence of noise.</span></span></figcaption>
</figure>
<figure id="A2.F17" class="ltx_figure"><img src="/html/2301.04017/assets/figures/other_datasets/language_eps_sms_spam.png" id="A2.F17.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="327" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="color:#000000;"><span class="ltx_tag ltx_tag_figure"><span id="A2.F17.5.1.1" class="ltx_text" style="font-size:90%;">Figure 17</span>: </span><span id="A2.F17.6.2" class="ltx_text ltx_font_bold" style="font-size:90%;">SMS Data Extraction under Noise.<span id="A2.F17.6.2.1" class="ltx_text ltx_font_medium"> Extraction performance under noise for DDP from language model on the SMS Spam Collection dataset. Extraction remains successful, even in presence of noise.</span></span></figcaption>
</figure>
<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p"><span id="A2.SS3.p1.1.1" class="ltx_text" style="color:#000000;">We present the results of extraction under noise for the Enron-Spam dataset and the SMS Spam Collection dataset in </span><a href="#A2.F16" title="In B.3 Experimental Results on Textual Data ‣ Appendix B Extended Experimental Evaluation ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">16</span></a><span id="A2.SS3.p1.1.2" class="ltx_text" style="color:#000000;"> and </span><a href="#A2.F17" title="In B.3 Experimental Results on Textual Data ‣ Appendix B Extended Experimental Evaluation ‣ Reconstructing Individual Data Points in Federated Learning Hardened with Differential Privacy and Secure Aggregation⋄" class="ltx_ref" style="color:#000000;"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">17</span></a><span id="A2.SS3.p1.1.3" class="ltx_text" style="color:#000000;">, respectively.
For both datasets we observe a degrease in the fraction of recovered tokens with increased noise scale.
The performance in the SMS Spam Collection dataset drops faster than in the email-based Enron-Spam dataset.
This effect is most likely to the limited number of token in SMS messages in comparison to emails or the reviews in the IMDB dataset.</span><span id="A2.SS3.p1.1.4" class="ltx_text"></span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2301.04016" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2301.04017" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2301.04017">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2301.04017" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2301.04018" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 03:50:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
