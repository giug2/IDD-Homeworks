<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.02499] mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding</title><meta property="og:description" content="Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLU…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.02499">

<!--Generated on Wed Feb 28 18:28:18 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">mPLUG-DocOwl <span id="id2.id1" class="ltx_ERROR undefined">\scalerel</span>*<img src="/html/2307.02499/assets/figs/docowl_logo.png" id="id1.g1" class="ltx_graphics ltx_img_square" width="414" height="414" alt="[Uncaptioned image]">○: Modularized Multimodal Large Language Model for Document Understanding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jiabo Ye  , Anwen Hu<span id="footnotex1" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>, Haiyang Xu, Qinghao Ye, Ming Yan<span id="footnotex2" class="ltx_note ltx_role_footnotemark"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">2</span></span></span></span>, Yuhao Dan, 
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_bold">Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang
<br class="ltx_break"></span>DAMO Academy, Alibaba Group 
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{yejiabo.yjb, huanwen.haw, shuofeng.xhy, yeqinghao.yqh, ym119608}@alibaba-inc.com</span>
</span><span class="ltx_author_notes">Equal contributionCorresponding author</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id5.id1" class="ltx_p">Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified instruction tuning strategy. We also build an OCR-free document instruction understanding evaluation set LLMDoc to better compare models’ capabilities on instruct compliance and document understanding. Experimental results show that our model outperforms existing multi-modal models, demonstrating its strong ability of document understanding. Besides, without specific fine-tuning, mPLUG-DocOwl generalizes well on various downstream tasks. Our code, models, training data and evaluation set are available at https://github.com/X-PLUG/mPLUG-DocOwl.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Large language models (LLMs) like ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>, BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a href="#bib.bib19" title="" class="ltx_ref">2022</a>)</cite>, and LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>)</cite> have undergone rapid development to enable the realization of general artificial intelligence, boasting impressive zero-shot capabilities across diverse linguistic applications. With the LLM as the language decoder, Multimodal large language models (MLLMs) such as MiniGPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>, LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib12" title="" class="ltx_ref">2023a</a>)</cite>, and mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> have demonstrated remarkable zero-shot performance in various open-ended vision-and-language tasks.
These models are trained to align text and images during the pre-training phase, and then to promote diverse abilities during the instruction tuning phase.
Interestingly, these MLLMs exhibit superficial OCR-free text recognition abilities without explicit training on visual text understanding datasets <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>; Liu et al., <a href="#bib.bib13" title="" class="ltx_ref">2023b</a>)</cite>. Nevertheless, due to lacking specific training, these models still face the challenge of comprehending intricate relationships between visual text and objects in diverse types of images, such as charts, documents and webpages.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">By performing unified instruction tuning for Document Understanding upon the mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, we further propose a modularized MLLM <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a href="#bib.bib11" title="" class="ltx_ref">2022</a>; Xu et al., <a href="#bib.bib32" title="" class="ltx_ref">2023b</a>)</cite>, namely mPLUG-DocOwl.
Our approach utilizes a modularized framework similar to mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, which incorporates a visual abstractor module to link a pre-trained LLM with a visual knowledge module, achieving the alignment of text and images. To enhance diverse document understanding capabilities, we reorganize various downstream document understanding tasks in the same form of instructions. To maintain general uni/multi-modal abilities, we also include language-only and general vision-and-language instruction datasets used by mPLUG-Owl to train the mPLUG-DocOwl. During training, both the visual knowledge module and LLM decoder are frozen, only the visual abstractor and the Low-Rank Adaption (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite> in LLM are fine-tuned.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">mPLUG-DocOwl achieves ocr-free state-of-the-art performance on multiple commonly used document understanding datasets. Furthermore, our experiments on a carefully-built document instruction understanding evaluation set LLMDoc shows that mPLUG-DocOwl achieves significantly better visual text understanding performance on various domains than existing MLMMs.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our main contributions can be highlighted as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose a modularized MLLM, <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">mPLUG-DocOwl</span>, which is the first one to balance language-only, general vision-and-language, and document understanding based on unified instruction tuning.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We carefully construct an instruction understanding test set with human evaluation, dubbed <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">LLMDoc</span>, to assess diverse document understanding capabilities.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Empirical results demonstrate that our mPLUG-DocOwl surpasses existing methods on ocr-free document understanding, including multiple standard benchmarks and LLMDoc.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual Text Understanding</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">There are two types of models for understanding images that contain rich textual information. The first kind of approaches <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Huang et al., <a href="#bib.bib8" title="" class="ltx_ref">2022</a>; Hu et al., <a href="#bib.bib6" title="" class="ltx_ref">2021</a>; Tang et al., <a href="#bib.bib25" title="" class="ltx_ref">2023</a>; Yang et al., <a href="#bib.bib34" title="" class="ltx_ref">2021</a>)</cite> utilize off-the-shelf OCR models or APIs to recognize text from images, and then design pretraining tasks to facilitate cross-modality alignment between visual and textual inputs. On the other hand, end-to-end approaches <cite class="ltx_cite ltx_citemacro_citep">(Davis et al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>; Kim et al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>; Lee et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite> utilize a high-resolution image encoder to learn text recognition during the pretraining stage. Both two types of models rely on specific finetuning on different downstream datasets and can’t achieve open-domain instruction understanding performance like Multimodal Large Language Models.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Multimodal Large Language Model</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Large Language Models (LLMs) have demonstrated impressive zero-shot abilities across various open-ended tasks. Recent research has also explored the application of LLMs for multi-modal generation, utilizing two different paradigms: systematic collaboration and end-to-end trained models. Systematic collaboration approaches, such as Visual ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib30" title="" class="ltx_ref">2023</a>)</cite> and MM-REACT <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>, leverage various vision experts or tools to express visual information with text descriptions. Subsequently, LLMs, such as ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>, can act as agents and select appropriate experts and tools for visual understanding. Finally, LLMs would summarize the output of these experts to answer user queries. On the other hand, some approaches, such as MiniGPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>, LLaVA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib12" title="" class="ltx_ref">2023a</a>)</cite>, and mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, leverage LLMs to build unified models for multi-modality with limited connected parameters. These methods show superficial OCR-free text recognition abilities under the zero-shot setting. However, for complicated document understanding, due to lacking in-domain training, they encounter challenges in handling diverse image types, recognizing rich texts and comprehending relationships between visual semantic and text information. In this work, through unified instruction tuning, mPLUG-DocOwl achieves much better document understanding performance and maintains general uni/multi-modal abilities.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>mPLUG-DocOwl</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/2307.02499/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="293" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">The summary of the instruction tuning paradigm of our mPLUG-DocOwl.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The architecture of mPLUG-DocOwl is based on a popular multi-modal language model, mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, which comprises a pre-trained visual foundation model, a visual abstractor, and a language foundation model. The visual foundation model is responsible for extracting visual features from the input images, and the visual abstractor distills these features using a set of learnable tokens. The resulting visual features are then concatenated with the word embeddings of the input sentence and fed into the language model to generate the response. This powerful architecture allows for accurate and efficient multi-modal language processing.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> exhibits superficial OCR ability when presented with images containing salient text. Inspired by this, we propose to further fine-tune the model with document instruction tuning data for better document understanding performance, covering document, table, chart and natural image and webpage. During fine-tuning, we freeze the visual encoder and the language model and train the visual abstractor. We also adopt the low-rank adaptation approach (LoRA) <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite> to enhance the language model’s ability.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Instruction Tuning Data</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This section introduces the composition of our instruction tuning data in detail. To ensure the versatility of mPLUG-DocOwl, we collect diverse document understanding datasets with different task formats, including Visual Question Answering (VQA) <cite class="ltx_cite ltx_citemacro_citep">(Antol et al., <a href="#bib.bib1" title="" class="ltx_ref">2015</a>)</cite>, Information Extraction (IE), Natural Language Inference (NLI) <cite class="ltx_cite ltx_citemacro_citep">(Bowman et al., <a href="#bib.bib3" title="" class="ltx_ref">2015</a>)</cite>, and Image Captioning (IC). mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> performs instruction tuning with a unified format as "&lt;image&gt;Human:{question} AI:{answer}". In this work, we convert different document understanding tasks to the same format as mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> by replacing the {question} and {answer} placeholders as follows.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Visual Question Answering</span> We simply use the raw question and answer as the {question} and {answer} placeholders. We collect VQA datasets on diverse domains, including ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, DocVQA <cite class="ltx_cite ltx_citemacro_citep">(Mathew et al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>, InfographicsVQA (InfoVQA) <cite class="ltx_cite ltx_citemacro_citep">(Mathew et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>)</cite>, WikiTableQuestions (WTQ) <cite class="ltx_cite ltx_citemacro_citep">(Pasupat and Liang, <a href="#bib.bib18" title="" class="ltx_ref">2015</a>)</cite>, TextVQA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite> and VisualMRC <cite class="ltx_cite ltx_citemacro_citep">(Tanaka et al., <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Information Extraction</span> requires the model to extract key-value pairs from the input image. The ‘keys’ (or ‘categories’) are always a stationary set. To convert this task to the instruction tuning format, we treat the value as the {answer} and construct the {question} as ‘What is the value for the {key}?’. When the key does not exist in the image, the {answer} is set to ‘None’. We collect Information Extraction data from DeepForm <cite class="ltx_cite ltx_citemacro_citep">(Svetlichnaya, <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>, and Kleister Charity (KLC) <cite class="ltx_cite ltx_citemacro_citep">(Stanislawek et al., <a href="#bib.bib22" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para ltx_noindent">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Natural Language Inference</span> is a binary classification task with labels ‘Entailed’ and ‘Refuted’. Given a statement, we construct the {question} as ‘{statement}, Yes or No?’. The {answer} is ‘Yes’ or ‘No’ and refers to ‘Entailed’ or ‘Refuted’, respectively. TabFact <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, a natural language inference dataset about tables, is chosen for instruction tuning.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para ltx_noindent">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Image Captioning</span> aims to briefly describe an image with fluent language. We treat the caption as the {answer} and randomly choose a prompt as the {question} like LLaVa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib12" title="" class="ltx_ref">2023a</a>)</cite>. TextCaps <cite class="ltx_cite ltx_citemacro_citep">(Sidorov et al., <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite> is an appropriate captioning dataset on natural images with texts.</p>
</div>
<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Language-only and General Vision-and-language Instruction Tuning.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.1" class="ltx_p">To enhance the model’s ability of language comprehension and multi-modal open-ended conversation, we follow mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> to introduce language-only and general vision-and-language instruction tuning data <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a href="#bib.bib26" title="" class="ltx_ref">2023</a>; Vicuna, <a href="#bib.bib28" title="" class="ltx_ref">2023</a>; Xu et al., <a href="#bib.bib31" title="" class="ltx_ref">2023a</a>; Liu et al., <a href="#bib.bib12" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2307.02499/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="145" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S3.F2.3.2" class="ltx_text" style="font-size:90%;">Different types of datasets used to train mPLUG-DocOwl.</span></figcaption>
</figure>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p2.1" class="ltx_p"><a href="#S3.F2" title="In Language-only and General Vision-and-language Instruction Tuning. ‣ 3.2 Instruction Tuning Data ‣ 3 mPLUG-DocOwl ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows the composition of our instruction tuning data grouped by the dataset type. We use training sets of these datasets as instruction tuning data and evaluate models on test sets.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Training Details</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We adopt a two-stage training paradigm, where the Vision Transformer and Language model are kept frozen. In the first stage, both the visual abstractor and LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>)</cite> in the language model are fine-tuned. The first stage only uses the document understanding data and takes 10 epochs. In the second stage, we further freeze the visual abstractor and only train the LoRA. Besides document understanding data, the language-only and general vision-and-language instruction tuning data are further introduced at this stage and up-sampled 6 times. The second stage takes 3 epochs. Other training hyper-parameters are the same as mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>LLMDoc</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Existing benchmarks are hard to evaluate the open-ended instruction understanding results given by MLMMs. For better compare the instruction understanding performance in the document domain, we further construct a test set with human evaluation, namely LLMDoc.</p>
</div>
<section id="S4.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Collection</h4>

<div id="S4.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px1.p1.1" class="ltx_p">To comprehensively evaluate the model’s abilities, we consider five scenarios to construct our evaluation dataset, including table (TabFact <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>), chart (ChartQA <cite class="ltx_cite ltx_citemacro_citep">(Masry et al., <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>), document (DocVQA <cite class="ltx_cite ltx_citemacro_citep">(Mathew et al., <a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>), natural image (TextVQA <cite class="ltx_cite ltx_citemacro_citep">(Singh et al., <a href="#bib.bib21" title="" class="ltx_ref">2019</a>)</cite>) and webpage (VisualMRC <cite class="ltx_cite ltx_citemacro_citep">(Tanaka et al., <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>). Specifically, for each dataset, we sample 20 images from the test split. For 10 of these images, we adopt a raw question as the instruction. While for the other 10, we ask annotators to write instructions requiring stronger
capabilities like summarization, inference, and calculation. In total, we obtain 100 test samples.</p>
</div>
</section>
<section id="S4.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Human Evaluation</h4>

<div id="S4.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p1.1" class="ltx_p">Following the rating criteria proposed in Self-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib29" title="" class="ltx_ref">2022</a>)</cite>, we perform the human evaluation to score the model’s responses, where A &gt; B &gt; C &gt; D and A represents ‘correct and satisfying response’, B means ‘acceptable response with minor imperfections’, C refers to ‘response to the instruction but has significant errors’ and D means ‘irrelevant or invalid response’.</p>
</div>
<figure id="S4.F3" class="ltx_figure ltx_align_floatright"><img src="/html/2307.02499/assets/figs/llm_comp.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="611" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.3.2" class="ltx_text" style="font-size:90%;">Human evaluation of mPLUG-DocOwl, mPLUG-Owl and MiniGPT-4 on LLMDoc.</span></figcaption>
</figure>
<div id="S4.SS1.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS1.SSS0.Px2.p2.1" class="ltx_p">We compare mPLUG-DocOwl with other popular mult-modal large language models, including mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite> and Mini-GPT4 <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite>, on LLMDoc. As shown in <a href="#S4.F3" title="In Human Evaluation ‣ 4.1 LLMDoc ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>, mPLUG-DocOwl achieves significantly better performance, with 37 responses being scored as “A”, demonstrating the stronger understanding ability of mPLUG-DocOwl in diverse document scenarios. Besides, it’s worth noting that all models have some responses scored as “C” or “D”, showing that instruction understanding performance in the document domain is still far from promising and needs more endeavor.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Benchmark Evaluation</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Besides human evaluation, we also compare our mPLUG-DocOwl with ocr-free
state-of-the-art document understanding models on public datasets. <a href="#S4.T1" title="In 4.2 Benchmark Evaluation ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows the comparison with Dessurt <cite class="ltx_cite ltx_citemacro_citep">(Davis et al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>, Donut <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> and Pix2Struct <cite class="ltx_cite ltx_citemacro_citep">(Lee et al., <a href="#bib.bib10" title="" class="ltx_ref">2022</a>)</cite> on DUE-Benchmark <cite class="ltx_cite ltx_citemacro_citep">(Borchmann et al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>)</cite>, which mainly requires the text recognition and layout understanding abilities on documents and tables. Besides, <a href="#S4.T2" title="In 4.2 Benchmark Evaluation ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a> presents the evaluation on the chart, natural image and webpage datasets, which ask stronger ability to relate visual semantics and text information. Without finetuning on each dataset, our mPLUG-DocOwl achieves comparable or even better performance.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison with ocr-free methods on DUE-Benchmark.</span></figcaption>
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T1.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></th>
<th id="S4.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DocVQA</span></th>
<th id="S4.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">InfoVQA</span></th>
<th id="S4.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">DeepForm</span></th>
<th id="S4.T1.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">KLC</span></th>
<th id="S4.T1.1.2.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">WTQ</span></th>
<th id="S4.T1.1.2.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T1.1.2.1.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TabFact</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.3.1" class="ltx_tr">
<th id="S4.T1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T1.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Dessurt</span></th>
<td id="S4.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.2.1" class="ltx_text" style="font-size:80%;">63.2</span></td>
<td id="S4.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.3.1.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T1.1.4.2" class="ltx_tr">
<th id="S4.T1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S4.T1.1.4.2.1.1" class="ltx_text" style="font-size:80%;">Donut</span></th>
<td id="S4.T1.1.4.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.2.1" class="ltx_text" style="font-size:80%;">67.5</span></td>
<td id="S4.T1.1.4.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.3.1" class="ltx_text" style="font-size:80%;">11.6</span></td>
<td id="S4.T1.1.4.2.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">61.6</span></td>
<td id="S4.T1.1.4.2.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.5.1" class="ltx_text" style="font-size:80%;">30.0</span></td>
<td id="S4.T1.1.4.2.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.6.1" class="ltx_text" style="font-size:80%;">18.8</span></td>
<td id="S4.T1.1.4.2.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.4.2.7.1" class="ltx_text" style="font-size:80%;">54.6</span></td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T1.1.1.1.1" class="ltx_text" style="font-size:80%;">Pix2Struct</span><sub id="S4.T1.1.1.1.2" class="ltx_sub"><span id="S4.T1.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">base</span></sub>
</th>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.2.1" class="ltx_text" style="font-size:80%;">72.1</span></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.3.1" class="ltx_text" style="font-size:80%;">38.2</span></td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.4.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.1.1.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.6.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T1.1.1.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.7.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T1.1.5.3" class="ltx_tr">
<th id="S4.T1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T1.1.5.3.1.1" class="ltx_text" style="font-size:80%;">mPLUG-DocOwl</span></th>
<td id="S4.T1.1.5.3.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.1.5.3.2.1" class="ltx_text" style="font-size:80%;">62.2</span></td>
<td id="S4.T1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.1.5.3.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">38.2</span></td>
<td id="S4.T1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.1.5.3.4.1" class="ltx_text" style="font-size:80%;">42.6</span></td>
<td id="S4.T1.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.1.5.3.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">30.3</span></td>
<td id="S4.T1.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.1.5.3.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">26.9</span></td>
<td id="S4.T1.1.5.3.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T1.1.5.3.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">60.2</span></td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.4.2" class="ltx_text" style="font-size:90%;">Comparison with ocr-free methods on chart, natural image and webpage understanding.</span></figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt"><span id="S4.T2.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Model</span></th>
<th id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">ChartQA</span></th>
<th id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TextVQA</span></th>
<th id="S4.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TextCaps</span></th>
<th id="S4.T2.1.2.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S4.T2.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">VisualMRC</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.3.1" class="ltx_tr">
<th id="S4.T2.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S4.T2.1.3.1.1.1" class="ltx_text" style="font-size:80%;">Donut</span></th>
<td id="S4.T2.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.1.2.1" class="ltx_text" style="font-size:80%;">41.8</span></td>
<td id="S4.T2.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.1.3.1" class="ltx_text" style="font-size:80%;">43.5</span></td>
<td id="S4.T2.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.1.4.1" class="ltx_text" style="font-size:80%;">74.4</span></td>
<td id="S4.T2.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T2.1.3.1.5.1" class="ltx_text" style="font-size:80%;">93.91</span></td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<span id="S4.T2.1.1.1.1" class="ltx_text" style="font-size:80%;">Pix2Struct</span><sub id="S4.T2.1.1.1.2" class="ltx_sub"><span id="S4.T2.1.1.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">base</span></sub>
</th>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.2.1" class="ltx_text" style="font-size:80%;">56.0</span></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.3.1" class="ltx_text" style="font-size:80%;">-</span></td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.4.1" class="ltx_text" style="font-size:80%;">88.0</span></td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center"><span id="S4.T2.1.1.5.1" class="ltx_text" style="font-size:80%;">-</span></td>
</tr>
<tr id="S4.T2.1.4.2" class="ltx_tr">
<th id="S4.T2.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><span id="S4.T2.1.4.2.1.1" class="ltx_text" style="font-size:80%;">mPLUG-DocOwl</span></th>
<td id="S4.T2.1.4.2.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.4.2.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">57.4</span></td>
<td id="S4.T2.1.4.2.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.4.2.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">52.6</span></td>
<td id="S4.T2.1.4.2.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.4.2.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">111.9</span></td>
<td id="S4.T2.1.4.2.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.1.4.2.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">188.8</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Qualitative Analysis</h3>

<figure id="S4.F4" class="ltx_figure"><img src="/html/2307.02499/assets/x3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="549" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">Qualitative results of mPLUG-DocOwl. The crucial regions and corresponding words are annotated with the same colors for clearer visualization. Wrong answers are colored <span id="S4.F4.4.2.1" class="ltx_text" style="color:#FF0000;">red</span>.</span></figcaption>
</figure>
<section id="S4.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Benchmark Results.</h4>

<div id="S4.SS3.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px1.p1.1" class="ltx_p">Qualitative results on different types of images are shown in <a href="#S4.F4" title="In 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>. Crucial regions and corresponding responses are annotated with the same colors.
Case (a) shows that mPLUG-DocOwl can accurately find the answer from a webpage screenshot with complex contents. Case (b) shows that mPLUG-DocOwl is even able to understand hand-drawn tables and correctly recognize handwritten fonts. In case (c), mPLUG-DocOwl can summarize key points from a chart. It successfully understands that the table is about internet usage and infers that “Never” means “Never used internet”. However, it also generates illusory outputs, such as "in the United States". The question in case (d) requires the model to understand the “Result” column, compare the points and return the date with the best results. Case (e) demonstrates that our model is capable of processing scanned documents and distinguishing company and person names. Case (f) shows that mPLUG-DocOwl can not only recognize small and blurry text but also perform simple calculations following the user intent.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2307.02499/assets/x4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="440" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Part one.</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2307.02499/assets/x5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="542" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">Qualitative comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. Part two.</span></figcaption>
</figure>
</section>
<section id="S4.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">LLMDoc Results</h4>

<div id="S4.SS3.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p1.1" class="ltx_p"><a href="#S4.F5" title="In Benchmark Results. ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> and <a href="#S4.F6" title="In Benchmark Results. ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> present the comparison between mPLUG-DocOwl and Mini-GPT4 on LLMDoc. <a href="#S4.F5" title="In Benchmark Results. ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> (a) requires models to convert a table into JSON format. Our mPLUG-DocOwl correctly understands the instruction and return a string in JSON format, but misses the last row. Mini-GPT4 fails to comprehend the instruction and doesn’t understand the content within the table.
In <a href="#S4.F5" title="In Benchmark Results. ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> (b), both mPLUG-DocOwl and Mini-GPT4 correctly recognize the name of the shop.
However, Mini-GPT4 overlooks a smaller sign indicating clothes in this shop are medical uniforms.
As for chart understanding in <a href="#S4.F6" title="In Benchmark Results. ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> (c), Mini-GPT4 gives a wrong answer and redundant response, while our mPLUG-DocOwl gives a concise and correct response. In <a href="#S4.F6" title="In Benchmark Results. ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> (d), Bernadette’s actual purpose is to confirm with Suzy if she would like to have the copy sent overnight. This not only requires the model to accurately recognize the text, but also to understand the relationships between involved persons. mPLUG-DocOwl recognizes the phrase "request a copy of chapter," but misunderstands the subject and object. Mini-GPT4 only comprehends that this image is a mail scenario and provides a vague and hallucinatory response. In <a href="#S4.F6" title="In Benchmark Results. ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> (e), mPLUG-DocOwl gives a correct summary of the two latest news but Mini-GPT4 generates news irrelevant to the webpage screenshot.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2307.02499/assets/x6.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="417" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S4.F7.3.2" class="ltx_text" style="font-size:90%;">Failure cases on LLMDoc. Part one.</span></figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2307.02499/assets/x7.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="412" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S4.F8.3.2" class="ltx_text" style="font-size:90%;">Failure cases on LLMDoc. Part two.</span></figcaption>
</figure>
<div id="S4.SS3.SSS0.Px2.p2" class="ltx_para">
<p id="S4.SS3.SSS0.Px2.p2.1" class="ltx_p">The LLMDoc contains many challenging instruction understanding cases in the document domain. <a href="#S4.F7" title="In LLMDoc Results ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a> and <a href="#S4.F8" title="In LLMDoc Results ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> show some wrong responses given by mPLUG-DocOwl. In <a href="#S4.F7" title="In LLMDoc Results ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a> (a), mPLUG-DocOwl only takes note of the three names in the picture, but ignores the fact that the user itself is also a speaker. In <a href="#S4.F7" title="In LLMDoc Results ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a> (b), mPLUG-DocOwl fails to perform multi-step calculations on multiple elements in the image. In <a href="#S4.F8" title="In LLMDoc Results ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> (c), the model can understand the scene and the text in it, but fantasizes about non-existent characters. In <a href="#S4.F8" title="In LLMDoc Results ‣ 4.3 Qualitative Analysis ‣ 4 Experiment ‣ mPLUG-DocOwl \scalerel*○: Modularized Multimodal Large Language Model for Document Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> (d), mPLUG-DocOwl fails to understand the instruction for writing news and only read the texts in the tablet.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work, we infuse diverse ocr-free document understanding capabilities into mPLUG-Owl by incorporating document understanding data into instruction finetuning. Experiment results demonstrate that our mPLUG-DocOwl achieves comparable or even better performance than existing OCR-free methods. Besides, benefiting from language-only and general vision-and-language instruction tuning, mPLUG-DocOwl can better comprehend user instructions and intentions, enabling more complex interactions. Moreover, human evaluation on LLMDoc reveals that mPLUG-DocOwl still struggles with document-related commonsense reasoning, mathematical calculations, and creative generation. This provides valuable insights about developing stronger document understanding abilities with the LLM in the future.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antol et al. [2015]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh.

</span>
<span class="ltx_bibblock">Vqa: Visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em>, pages 2425–2433, 2015.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Borchmann et al. [2021]</span>
<span class="ltx_bibblock">
L. Borchmann, M. Pietruszka, T. Stanislawek, D. Jurkiewicz, M. Turski,
K. Szyndler, and F. Gralinski.

</span>
<span class="ltx_bibblock">DUE: end-to-end document understanding benchmark.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">NeurIPS Datasets and Benchmarks</em>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bowman et al. [2015]</span>
<span class="ltx_bibblock">
S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning.

</span>
<span class="ltx_bibblock">A large annotated corpus for learning natural language inference.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1508.05326</em>, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2020]</span>
<span class="ltx_bibblock">
W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang.

</span>
<span class="ltx_bibblock">Tabfact : A large-scale dataset for table-based fact verification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>, Addis Ababa, Ethiopia, April 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davis et al. [2022]</span>
<span class="ltx_bibblock">
B. L. Davis, B. S. Morse, B. L. Price, C. Tensmeyer, C. Wigington, and V. I.
Morariu.

</span>
<span class="ltx_bibblock">End-to-end document recognition and understanding with dessurt.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">ECCV Workshops (4)</em>, volume 13804 of <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">Lecture Notes
in Computer Science</em>, pages 280–296. Springer, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2021]</span>
<span class="ltx_bibblock">
A. Hu, S. Chen, and Q. Jin.

</span>
<span class="ltx_bibblock">Question-controlled text-aware image captioning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ACM Multimedia</em>, pages 3097–3105. ACM, 2021.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2022]</span>
<span class="ltx_bibblock">
E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
W. Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">The Tenth International Conference on Learning
Representations, ICLR 2022, Virtual Event, April 25-29, 2022</em>.
OpenReview.net, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=nZeVKeeFYf9</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2022]</span>
<span class="ltx_bibblock">
Y. Huang, T. Lv, L. Cui, Y. Lu, and F. Wei.

</span>
<span class="ltx_bibblock">Layoutlmv3: Pre-training for document AI with unified text and
image masking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ACM Multimedia</em>, pages 4083–4091. ACM, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. [2022]</span>
<span class="ltx_bibblock">
G. Kim, T. Hong, M. Yim, J. Nam, J. Park, J. Yim, W. Hwang, S. Yun, D. Han, and
S. Park.

</span>
<span class="ltx_bibblock">Ocr-free document understanding transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">ECCV (28)</em>, volume 13688 of <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in
Computer Science</em>, pages 498–517. Springer, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2022]</span>
<span class="ltx_bibblock">
K. Lee, M. Joshi, I. Turc, H. Hu, F. Liu, J. Eisenschlos, U. Khandelwal,
P. Shaw, M. Chang, and K. Toutanova.

</span>
<span class="ltx_bibblock">Pix2struct: Screenshot parsing as pretraining for visual language
understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2210.03347, 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2022]</span>
<span class="ltx_bibblock">
C. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao,
J. Zhang, S. Huang, F. Huang, J. Zhou, and L. Si.

</span>
<span class="ltx_bibblock">mplug: Effective and efficient vision-language learning by
cross-modal skip-connections.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">EMNLP</em>, pages 7241–7259. Association for Computational
Linguistics, 2022.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023a]</span>
<span class="ltx_bibblock">
H. Liu, C. Li, Q. Wu, and Y. J. Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.08485, 2023a.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2023b]</span>
<span class="ltx_bibblock">
Y. Liu, Z. Li, H. Li, W. Yu, M. Huang, D. Peng, M. Liu, M. Chen, C. Li, L. Jin,
et al.

</span>
<span class="ltx_bibblock">On the hidden mystery of ocr in large multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.07895</em>, 2023b.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Masry et al. [2022]</span>
<span class="ltx_bibblock">
A. Masry, D. X. Long, J. Q. Tan, S. R. Joty, and E. Hoque.

</span>
<span class="ltx_bibblock">Chartqa: A benchmark for question answering about charts with
visual and logical reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">ACL (Findings)</em>, pages 2263–2279. Association for
Computational Linguistics, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. [2021]</span>
<span class="ltx_bibblock">
M. Mathew, D. Karatzas, and C. V. Jawahar.

</span>
<span class="ltx_bibblock">Docvqa: A dataset for VQA on document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">WACV</em>, pages 2199–2208. IEEE, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mathew et al. [2022]</span>
<span class="ltx_bibblock">
M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. V. Jawahar.

</span>
<span class="ltx_bibblock">Infographicvqa.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">WACV</em>, pages 2582–2591. IEEE, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2022]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>, 2022.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat and Liang [2015]</span>
<span class="ltx_bibblock">
P. Pasupat and P. Liang.

</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ACL (1)</em>, pages 1470–1480. The Association for Computer
Linguistics, 2015.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et al. [2022]</span>
<span class="ltx_bibblock">
T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilic, D. Hesslow,
R. Castagné, A. S. Luccioni, F. Yvon, M. Gallé, J. Tow, A. M.
Rush, S. Biderman, A. Webson, P. S. Ammanamanchi, T. Wang, B. Sagot,
N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman,
A. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O.
Suarez, V. Sanh, H. Laurençon, Y. Jernite, J. Launay, M. Mitchell,
C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy, A. Rogers,
A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien,
D. I. Adelani, and et al.

</span>
<span class="ltx_bibblock">BLOOM: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2211.05100, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sidorov et al. [2020]</span>
<span class="ltx_bibblock">
O. Sidorov, R. Hu, M. Rohrbach, and A. Singh.

</span>
<span class="ltx_bibblock">Textcaps: A dataset for image captioning with reading
comprehension.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">ECCV (2)</em>, volume 12347 of <em id="bib.bib20.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in
Computer Science</em>, pages 742–758. Springer, 2020.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. [2019]</span>
<span class="ltx_bibblock">
A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and
M. Rohrbach.

</span>
<span class="ltx_bibblock">Towards VQA models that can read.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 8317–8326. Computer Vision Foundation /
IEEE, 2019.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stanislawek et al. [2021]</span>
<span class="ltx_bibblock">
T. Stanislawek, F. Gralinski, A. Wróblewska, D. Lipinski, A. Kaliska,
P. Rosalska, B. Topolski, and P. Biecek.

</span>
<span class="ltx_bibblock">Kleister: Key information extraction datasets involving long
documents with complex layouts.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICDAR (1)</em>, volume 12821 of <em id="bib.bib22.2.2" class="ltx_emph ltx_font_italic">Lecture Notes in
Computer Science</em>, pages 564–579. Springer, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Svetlichnaya [2020]</span>
<span class="ltx_bibblock">
S. Svetlichnaya.

</span>
<span class="ltx_bibblock">Deepform: Understand structured documents at scale, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanaka et al. [2021]</span>
<span class="ltx_bibblock">
R. Tanaka, K. Nishida, and S. Yoshida.

</span>
<span class="ltx_bibblock">Visualmrc: Machine reading comprehension on document images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, pages 13878–13888. AAAI Press, 2021.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. [2023]</span>
<span class="ltx_bibblock">
Z. Tang, Z. Yang, G. Wang, Y. Fang, Y. Liu, C. Zhu, M. Zeng, C. Zhang, and
M. Bansal.

</span>
<span class="ltx_bibblock">Unifying vision, text, and layout for universal document processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 19254–19264, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. [2023]</span>
<span class="ltx_bibblock">
R. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and
T. B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux, T. Lacroix,
B. Rozière, N. Goyal, E. Hambro, F. Azhar, A. Rodriguez, A. Joulin,
E. Grave, and G. Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.13971, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vicuna [2023]</span>
<span class="ltx_bibblock">
Vicuna.

</span>
<span class="ltx_bibblock">Vicuna: An open chatbot impressing gpt-4.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/lm-sys/FastChat" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/lm-sys/FastChat</a>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2022]</span>
<span class="ltx_bibblock">
Y. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and
H. Hajishirzi.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language model with self generated
instructions.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2212.10560, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/arXiv.2212.10560</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.48550/arXiv.2212.10560" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.48550/arXiv.2212.10560</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023]</span>
<span class="ltx_bibblock">
C. Wu, S. Yin, W. Qi, X. Wang, Z. Tang, and N. Duan.

</span>
<span class="ltx_bibblock">Visual chatgpt: Talking, drawing and editing with visual foundation
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.04671, 2023.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023a]</span>
<span class="ltx_bibblock">
C. Xu, D. Guo, N. Duan, and J. J. McAuley.

</span>
<span class="ltx_bibblock">Baize: An open-source chat model with parameter-efficient tuning on
self-chat data.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.01196, 2023a.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2023b]</span>
<span class="ltx_bibblock">
H. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang,
G. Xu, J. Zhang, S. Huang, F. Huang, and J. Zhou.

</span>
<span class="ltx_bibblock">mplug-2: A modularized multi-modal foundation model across text,
image and video.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2302.00402, 2023b.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. [2020]</span>
<span class="ltx_bibblock">
Y. Xu, M. Li, L. Cui, S. Huang, F. Wei, and M. Zhou.

</span>
<span class="ltx_bibblock">Layoutlm: Pre-training of text and layout for document image
understanding.

</span>
<span class="ltx_bibblock">In R. Gupta, Y. Liu, J. Tang, and B. A. Prakash, editors, <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">KDD
’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data
Mining, Virtual Event, CA, USA, August 23-27, 2020</em>, pages 1192–1200. ACM,
2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/3394486.3403172</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1145/3394486.3403172" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1145/3394486.3403172</a>.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2021]</span>
<span class="ltx_bibblock">
Z. Yang, Y. Lu, J. Wang, X. Yin, D. Florêncio, L. Wang, C. Zhang,
L. Zhang, and J. Luo.

</span>
<span class="ltx_bibblock">TAP: text-aware pre-training for text-vqa and text-caption.

</span>
<span class="ltx_bibblock">In <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, pages 8751–8761. Computer Vision Foundation /
IEEE, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2023]</span>
<span class="ltx_bibblock">
Z. Yang, L. Li, J. Wang, K. Lin, E. Azarnasab, F. Ahmed, Z. Liu, C. Liu,
M. Zeng, and L. Wang.

</span>
<span class="ltx_bibblock">MM-REACT: prompting chatgpt for multimodal reasoning and action.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2303.11381, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. [2023]</span>
<span class="ltx_bibblock">
Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi,
C. Li, Y. Xu, H. Chen, J. Tian, Q. Qi, J. Zhang, and F. Huang.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with
multimodality.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/2304.14178, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023]</span>
<span class="ltx_bibblock">
D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced
large language models, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.02498" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.02499" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.02499">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.02499" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.02500" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 18:28:18 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
