<article class="ltx_document">
 <h1 class="ltx_title ltx_title_document">
  PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Yiduo Guo
    <sup class="ltx_sup" id="id8.8.id1">
     <span class="ltx_text ltx_font_italic" id="id8.8.id1.1">
      1
     </span>
    </sup>
    <span class="ltx_note ltx_role_footnote" id="footnotex1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       Equal contribution
      </span>
     </span>
    </span>
    ,  Zekai Zhang
    <sup class="ltx_sup" id="id9.9.id2">
     <span class="ltx_text ltx_font_italic" id="id9.9.id2.1">
      1
     </span>
    </sup>
    <span class="ltx_note ltx_role_footnote" id="footnotex2">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
       Equal contribution
      </span>
     </span>
    </span>
    ,  Yaobo Liang
    <sup class="ltx_sup" id="id10.10.id3">
     <span class="ltx_text ltx_font_italic" id="id10.10.id3.1">
      2
     </span>
    </sup>
    ,  Dongyan Zhao
    <sup class="ltx_sup" id="id11.11.id4">
     <span class="ltx_text ltx_font_italic" id="id11.11.id4.1">
      1
     </span>
    </sup>
    ,  Nan Duan
    <sup class="ltx_sup" id="id12.12.id5">
     <span class="ltx_text ltx_font_italic" id="id12.12.id5.1">
      2
     </span>
    </sup>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id13.13.id6">
     1
    </sup>
    Peking University
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id14.14.id7">
     2
    </sup>
    Microsoft Research Asia
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id15.15.id8">
     yiduo@stu.pku.edu.cn,justinzzk@stu.pku.edu.cn,yaobo.liang@microsoft.com
     <br class="ltx_break"/>
     zhaody@pku.edu.cn,nanduan@microsoft.com
     <br class="ltx_break"/>
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id16.id1">
   Recent evaluations of Large Language Models (LLMs) have centered around testing their zero-shot/few-shot capabilities for basic natural language tasks and their ability to translate instructions into tool APIs. However, the evaluation of LLMs utilizing complex tools to finish multi-turn, multi-modal instructions in a complex multi-modal environment has not been investigated. To address this gap, we introduce the PowerPoint Task Completion (PPTC) benchmark to assess LLMs’ ability to create and edit PPT files based on user instructions. It contains 279 multi-turn sessions covering diverse topics and hundreds of instructions involving multi-modal operations. We also propose the PPTX-Match Evaluation System that evaluates if LLMs finish the instruction based on the prediction file rather than the label API sequence, thus it supports various LLM-generated API sequences. We measure 3 closed LLMs and 6 open-source LLMs. The results show that GPT-4 outperforms other LLMs with 75.1% accuracy in single-turn dialogue testing but faces challenges in completing entire sessions, achieving just 6% session accuracy. We find three main error causes in our benchmark: error accumulation in the multi-turn session,
long PPT template processing, and multi-modality perception. These pose great challenges for future LLM and agent systems. We release the data, code, and evaluation system of PPTC at
   <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/gydpku/PPTC" target="_blank" title="">
    https://github.com/gydpku/PPTC
   </a>
   .
   <span class="ltx_figure" id="S0.F1">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="314" id="S0.F1.g1" src="/html/2311.01767/assets/session.png" width="290"/>
    <span class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 1:
     </span>
     Within our benchmark, we simulate this multi-turn dialogue scenario between humans and LLMs to evaluate LLMs’ PPT task completion performance.
    </span>
   </span>
  </p>
 </div>
 <div class="ltx_para ltx_noindent" id="p1">
  <div class="ltx_block ltx_align_bottom" id="p1.7">
   <p class="ltx_p" id="p1.7.8">
    <span class="ltx_text ltx_font_bold" id="p1.7.8.1">
     PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
   <p class="ltx_p ltx_align_center" id="p1.7.7" style="width:433.6pt;">
    <span class="ltx_text ltx_inline-block" id="p1.7.7.7" style="width:0.0pt;">
     <span class="ltx_tabular ltx_align_top" id="p1.7.7.7.7">
      <span class="ltx_tbody">
       <span class="ltx_tr" id="p1.5.5.5.5.5">
        <span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.5">
         <span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.5.5">
          Yiduo Guo
          <sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.1">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.1.1">
            1
           </span>
          </sup>
          <span class="ltx_note ltx_role_footnote" id="footnotex3">
           <sup class="ltx_note_mark">
            1
           </sup>
           <span class="ltx_note_outer">
            <span class="ltx_note_content">
             <sup class="ltx_note_mark">
              1
             </sup>
             <span class="ltx_tag ltx_tag_note">
              <span class="ltx_text ltx_font_medium" id="footnotex3.1.1.1">
               1
              </span>
             </span>
             Equal contribution
            </span>
           </span>
          </span>
          ,  Zekai Zhang
          <sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.2">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.2.1">
            1
           </span>
          </sup>
          <span class="ltx_note ltx_role_footnote" id="footnotex4">
           <sup class="ltx_note_mark">
            1
           </sup>
           <span class="ltx_note_outer">
            <span class="ltx_note_content">
             <sup class="ltx_note_mark">
              1
             </sup>
             <span class="ltx_tag ltx_tag_note">
              <span class="ltx_text ltx_font_medium" id="footnotex4.1.1.1">
               1
              </span>
             </span>
             Equal contribution
            </span>
           </span>
          </span>
          ,  Yaobo Liang
          <sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.3">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.3.1">
            2
           </span>
          </sup>
          ,  Dongyan Zhao
          <sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.4">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.4.1">
            1
           </span>
          </sup>
          ,  Nan Duan
          <sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.5">
           <span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.5.1">
            2
           </span>
          </sup>
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.6.6.6.6.6">
        <span class="ltx_td ltx_align_center" id="p1.6.6.6.6.6.1">
         <sup class="ltx_sup" id="p1.6.6.6.6.6.1.1">
          1
         </sup>
         Peking University
        </span>
       </span>
       <span class="ltx_tr" id="p1.7.7.7.7.7">
        <span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.1">
         <sup class="ltx_sup" id="p1.7.7.7.7.7.1.1">
          2
         </sup>
         Microsoft Research Asia
        </span>
       </span>
       <span class="ltx_tr" id="p1.7.7.7.7.8.1">
        <span class="ltx_td ltx_align_center" id="p1.7.7.7.7.8.1.1">
         <span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.8.1.1.1">
          yiduo@stu.pku.edu.cn,justinzzk@stu.pku.edu.cn,yaobo.liang@microsoft.com
         </span>
        </span>
       </span>
       <span class="ltx_tr" id="p1.7.7.7.7.9.2">
        <span class="ltx_td ltx_align_center" id="p1.7.7.7.7.9.2.1">
         <span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.9.2.1.1">
          zhaody@pku.edu.cn,nanduan@microsoft.com
         </span>
        </span>
       </span>
      </span>
     </span>
    </span>
   </p>
   <br class="ltx_break ltx_centering"/>
  </div>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <figure class="ltx_figure" id="S1.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="419" id="S1.F2.g1" src="/html/2311.01767/assets/completion2.png" width="598"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    We illustrate how LLMs complete one turn in a session. (A) To prompt the LLM, we provide it with the current instruction, previous instructions (dialogue history), PPT file content, and the API reference file. ’PPT reader’ is a function that transforms the PPT file into the text-based format as the PPT file content. (B) The LLM then generates the API sequence and executes it to obtain the prediction PPT file. (C) We evaluate attributes and position relations in the prediction file.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Recent evaluation works for Large Language Models (e.g. ChatGPT and GPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     OpenAI (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    ) focus on their zero-shot/few-shot abilities on basic natural language tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     Jiao et al. (
     <a class="ltx_ref" href="#bib.bib10" title="">
      2023
     </a>
     ); Zhong et al. (
     <a class="ltx_ref" href="#bib.bib40" title="">
      2023
     </a>
     ); Wang et al. (
     <a class="ltx_ref" href="#bib.bib32" title="">
      2023b
     </a>
     ); Qin et al. (
     <a class="ltx_ref" href="#bib.bib21" title="">
      2023a
     </a>
     )
    </cite>
    and their tool-use ability to generate APIs for solving user instructions, such as basic APIs like a calculator in tool transformer
    <cite class="ltx_cite ltx_citemacro_cite">
     Schick et al. (
     <a class="ltx_ref" href="#bib.bib25" title="">
      2023
     </a>
     )
    </cite>
    , RapidAPIs in ToolLLM
    <cite class="ltx_cite ltx_citemacro_cite">
     Qin et al. (
     <a class="ltx_ref" href="#bib.bib23" title="">
      2023c
     </a>
     )
    </cite>
    , and hugggingface APIs in Gorilla
    <cite class="ltx_cite ltx_citemacro_cite">
     Patil et al. (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     )
    </cite>
    . However, these tool-use works emphasize the translation of natural language instructions into APIs and ignore the challenge of using APIs in the observation of complex multi-modal environments to finish user instructions.
Also, their evaluation approach focuses on comparing the generated APIs with the label API sequence, assuming there’s only one unique solution. This approach becomes impracticable in situations with multiple/unlimited correct solutions. To address these challenges, we introduce
    <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">
     P
    </span>
    ower-
    <span class="ltx_text ltx_font_bold" id="S1.p1.1.2">
     P
    </span>
    oint
    <span class="ltx_text ltx_font_bold" id="S1.p1.1.3">
     T
    </span>
    ask
    <span class="ltx_text ltx_font_bold" id="S1.p1.1.4">
     C
    </span>
    ompletion (PPTC), a benchmark that measures LLMs’ performance in creating and editing PPT file tasks based on user instructions. We choose PowerPoint as it includes various elements like textbox, table, and image and supports a wider range of APIs than Word and Excel.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Our benchmark has three distinctive features from other task completion benchmarks: (1)
    <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">
     Multi-turn dialogue with varying difficulty
    </span>
    . Our PPTC benchmark simulates the multi-turn dialogue session between the user and the LLM (see Figure
    <a class="ltx_ref" href="#S0.F1" title="Figure 1 ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    ) and contains 279 multi-turn sessions. Each multi-turn session in our benchmark includes 2 to 17 turns. Each turn consists of a user instruction that describes the user’s needs, a feasible solution that provides the correct solution, and the resulting label output file. Some turns can be easily addressed using a single API, while over half of the instructions require multiple APIs for completion. We provide the LLM with a reference API file that contains all feasible APIs for selection. (2)
    <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">
     Multi-modality
    </span>
    . Finishing the instruction of our benchmark requires understanding the multi-modal PPT file content and using multi-modal API operations (e.g., PPTC has 268 image operation-related instructions). (3)
    <span class="ltx_text ltx_font_bold" id="S1.p2.1.3">
     Evaluation based on the final status
    </span>
    : We propose the PPTX-Match Evaluation system to evaluate the LLM’s outcome. To identify if the LLM completes the instruction, it checks the PPT file produced by executing the LLM-generated APIs rather than the LLM-generated APIs, thus all API sequences that lead to the correct final status are acceptable.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    To finish the instruction, we use the current instruction, past turns’ instructions (dialogue history), the PPT file content (specific environment information), and the reference API file as the input to prompt the LLM to generate an API sequence as the solution (See Figure
    <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    (A)). Then we use the API executor to execute the API sequence and return the user the resulting PPT file (See Figure
    <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    (B)). We name the resulting PPT file as the prediction file. In the evaluation step (See Figure
    <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    (C)), the PPTX-Match Evaluation system first uses the Python-PPTX library to extract all attributes from the prediction PPT file and the label output file. Then it uses the position relation checker to check if objects’ positions conform to the label relation and the attribute content checker to check if the attribute’s content is matched with the corresponding label attribute’s content. The LLM correctly completes the current turn’s instruction if all attributes of the file pass these tests. Evaluation metrics include turn-based accuracy which is the ratio of correctly completed turns to the total number of turns and session-based accuracy which is the ratio of correctly completed sessions to the overall session count.
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    We measure the performance of three closed-source LLMs (GPT-4, ChatGPT, and Davince-003) and six open-source LLMs (e.g., LLaMa-2) in our benchmark. We further test planning (e.g., CoT
    <cite class="ltx_cite ltx_citemacro_cite">
     Wei et al. (
     <a class="ltx_ref" href="#bib.bib33" title="">
      2022
     </a>
     )
    </cite>
    ) and content selection algorithms’ performance based on GPT-4.
Experiment results show that GPT-4 is the strongest LLM among all LLMs but still encounters challenges when completing entire multi-turn sessions. For example, although GPT-4 achieves 75.1% turn-based accuracy in the creating new PPT file task, it only achieves 22.7% session-based accuracy as errors made in previous turns. GPT-4 and other LLMs also struggle to process long PPT templates (complex file environment). For example, GPT-4 only achieves 38.1% turn-based accuracy in the editing task. We further find that GPT-4 struggles to finish instructions involving non-text modality operations, especially for position-related operations, such as ’Put object A on the top of the slide’. It only achieves 24% accuracy in these instructions.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    In summary, this paper has the following contributions:
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    (1) We propose the PowerPoint Task Completion benchmark to measure LLM’s task completion performance within the PowerPoint official software. This benchmark contains 279 multi-turn sessions with hundreds of multi-modal instructions in the complex multi-modal environment.
   </p>
  </div>
  <div class="ltx_para" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    (2) We propose the PPTX-evaluation system to automatically measure LLMs’ performance in our benchmark. We test 3 closed-source LLMs and 6 open-source LLMs and find that GPT-4 is the strongest LLM among all LLMs.
   </p>
  </div>
  <div class="ltx_para" id="S1.p8">
   <p class="ltx_p" id="S1.p8.1">
    (3) We further analyze LLMs in our benchmarks and find three key error factors: error accumulation in the session, long PPT template processing, and multi-modality perception. These findings pose significant challenges for future LLMs and LLM-based systems.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   PPTC Benchmark
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    In this section, we introduce our Power-Point Task Completion (PPTC) benchmark, including the overview of our benchmark, its collection and validation process, and the PPTX-Match Evaluation System for evaluation. We further analyze the statistics information of our benchmark.
   </p>
  </div>
  <section class="ltx_subsection" id="S2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.1
    </span>
    Benchmark Overview
   </h3>
   <div class="ltx_para" id="S2.SS1.p1">
    <p class="ltx_p" id="S2.SS1.p1.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">
      Benchmark components
     </span>
     Our benchmark focuses on two basic tasks within PowerPoint: creating the new PPT file and editing the existing long PPT template for measuring long PPT Content understanding. We have gathered 229 multi-turn dialogue sessions for creating the new PPT file and 50 sessions for editing existing templates. Each multi-turn session includes 2 to 17 turns. Each turn comprises three parts: (1) the user instruction (2) the label output file as the ground truth (3) one feasible API sequence for finishing the instruction. Our benchmark also contains an API reference file that includes 49 feasible APIs for various operations and can complete all instructions in our benchmark. For each API, we describe its functionality and arguments and provide usage guidelines. For complex APIs, we also offer example cases. We list the details of all APIs in Appendix
     <a class="ltx_ref" href="#A1.F7" title="Figure 7 ‣ Appendix A The API Reference File ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p2">
    <p class="ltx_p" id="S2.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.1">
      Task description
     </span>
     To complete the instruction in one turn, in general, the AI assistant must comprehend the user’s current and prior instructions for context. It should also analyze the content of the PPT file to identify relevant objects mentioned in the instruction. Additionally, it needs to select appropriate APIs from a reference API file to achieve the user’s goals. So we use these as the input of the AI assistant and it should output an API sequence as the solution. Then, it executes this API sequence and provides the user with the resulting PPT file as its response (See the whole process in Figure
     <a class="ltx_ref" href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     ).
    </p>
   </div>
   <div class="ltx_para" id="S2.SS1.p3">
    <p class="ltx_p" id="S2.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS1.p3.1.1">
      Addressing LLM limitations in our benchmark
     </span>
     Compared to the general AI assistant, LLMs still have two limitations for completing the task in our benchmarks: (1) LLMs can not directly process the PPT file. So we provide a PPT reader function that extracts all shapes and their information from the PPT file and transforms them into the text format as the PPT file content. Then LLMs can understand and process the PPT file content. (2) LLMs cannot directly use PPT software through a keyboard and mouse. Therefore, we have defined PPT APIs based on the operational logic within the PPT software. and provide an implementation for these APIs in Python that can swiftly generate PPT files. In future work, it may be possible to explore the use of large multimodal models to understand on-screen content and implement APIs using a keyboard and mouse.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.2
    </span>
    Benchmark Collection
   </h3>
   <div class="ltx_para" id="S2.SS2.p1">
    <p class="ltx_p" id="S2.SS2.p1.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p1.1.1">
      Design Principles
     </span>
     We follow these principles to design our benchmark:
(1) Multi-turn instructions: One session in our benchmark should contain multi-turn instructions to finish the user’s complex need.
(2) Instructions of varying difficulty: Some instructions can be achieved with a single API, while others necessitate a sequence of APIs for successful completion.
(3) Diverse multimodal operations: User instructions should cover a wide range of operations on PPT, such as text-related, image-related, and position-related APIs.
     <span class="ltx_text" id="S2.SS2.p1.1.2" style="color:#000000;">
      (4) Topic Consistency: The dialogue in a session should center around the session topic. Each user instruction in a session aligns closely with the previous instructions (the context), ensuring a coherent and contextually relevant dialogue flow.
(5) Practicability First: The session topic and specific instructions should simulate the user’s need in real world
     </span>
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p2">
    <p class="ltx_p" id="S2.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1">
      Benchmark Collection and Validation
     </span>
     To collect user instructions, we engage 6 skilled crowd workers who craft instructions in accordance with the principles we’ve outlined. Our crowd workers comprise professional data science engineers well-versed in PowerPoint. To achieve practicability first, we request crowd workers to write instructions based on their actual PowerPoint experience. On each session, the workers are asked to first find and list a practicable session topic. For the editing PPT template task, the topic must based on the template file background and is practicable to the template
     <span class="ltx_note ltx_role_footnote" id="footnote1">
      <sup class="ltx_note_mark">
       *
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         *
        </sup>
        <span class="ltx_tag ltx_tag_note">
         *
        </span>
        We collect 50 PPT templates from the SlidesCarnival website (
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.slidescarnival.com/" target="_blank" title="">
         https://www.slidescarnival.com/
        </a>
        ). SlidesCarnival is a free and open-source PPT template website. Each session in the editing task has a unique template. We encourage topic diversity in our templates. We remove templates that are too short (2
        <math alttext="\sim" class="ltx_Math" display="inline" id="footnote1.m1.1">
         <semantics id="footnote1.m1.1b">
          <mo id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">
           ∼
          </mo>
          <annotation-xml encoding="MathML-Content" id="footnote1.m1.1c">
           <csymbol cd="latexml" id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">
            similar-to
           </csymbol>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="footnote1.m1.1d">
           \sim
          </annotation>
         </semantics>
        </math>
        5 slides) and have repeated topics.
       </span>
      </span>
     </span>
     . To achieve multi-instructions and topic consistency, the workers write instructions step by step and make them consistent with the topic. To achieve diverse multi-operations, we ask them not to write session that only involves a single modality operation. Each worker takes on a specific role in the instructional writing work and is encouraged to write instructions in his/her own words. Workers were asked to spend at least 20 minutes on every session. We delete repeated sessions and short sessions that have no more than 50 tokens.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p3">
    <p class="ltx_p" id="S2.SS2.p3.1">
     Then we ask the seventh worker to write the feasible API sequence with minimal API usage for each instruction. Next, the workers create the PPT label file by using the provided API sequence.
During the whole process, the principal engineer reviews and refines the instructions and API sequences written by the above 7 workers for initial quality assurance.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p4">
    <p class="ltx_p" id="S2.SS2.p4.1">
     To ensure the data quality of this benchmark, the three authors of this paper further undertake the following validation steps:
(1) Assessing Instruction Clarity and Relevance: They examine whether the instructions are clear, contextually related to the session topic, and align with the ongoing conversation.
(2) API Sequence Execution: The authors execute the provided API sequences to identify and rectify coding errors.
(3) Goal Achievement Check: They verify if the instructions’ intended goals are successfully completed in the label files.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS2.p5">
    <p class="ltx_p" id="S2.SS2.p5.1">
     In the event that errors are identified during this validation process, the authors promptly report them to the respective workers for revision. The three authors are computer science senior students and researchers.
    </p>
   </div>
   <figure class="ltx_figure" id="S2.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="195" id="S2.F3.g1" src="/html/2311.01767/assets/statistics.png" width="604"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     Statistics for PPTC. a) Session turn number distribution. b) Instruction API number distribution (tokens). c) Distribution of instructions involving Chart, Table, Picture, and Position. Instructions involving ’Position’ need the system to conduct position-related operations based on the understanding of spatial information. Note that one instruction may involve multiple different modalities.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.3
    </span>
    PPTX-Match Evaluation System
   </h3>
   <div class="ltx_para" id="S2.SS3.p1">
    <p class="ltx_p" id="S2.SS3.p1.1">
     We design the PPTX-Match Evaluation System to evaluate LLMs’ performance on the PPTC benchmark. Specifically, our PPTX-Match Evaluation System first uses a Python-PPTX Content Reader Module to iterate over all shapes in the prediction PPT file produced with the LLM and the label output file. A shape in the PPTX library typically refers to an individual object, such as a text box or table. Then our system extracts attributes like text, style, and position of the shapes using the PPTX library.
Next, we check all attributes from the prediction PPT file. For non-position attributes (e.g., text content), we first convert it and the corresponding attribute in the label PPT file into two strings, and then we use the Exact Match method to examine if the two strings are the same. If they are different or we do not find the corresponding attribute in the label file, then we find an incorrect match. For the position attribute (e.g., location information), we focus on checking if the objects in the prediction PPT file satisfy the correct position relation &lt;A, B, REL&gt;, where A and B are objects that should satisfy the relation REL. In the benchmark collection process, we ask crowd workers to label the position relation that objects should satisfy to finish the instruction. During the evaluation phase, we extract the position attributes of objects A and B and use predefined functions to verify if the objects’ position attributes satisfy the position relation.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS3.p2">
    <p class="ltx_p" id="S2.SS3.p2.1">
     If there are no incorrect matches for all non-position attributes and no rule violations for all position-related attributes, we consider the LLM has successfully completed the user instruction.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S2.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     2.4
    </span>
    Benchmark Statistics Analysis
   </h3>
   <div class="ltx_para" id="S2.SS4.p1">
    <p class="ltx_p" id="S2.SS4.p1.1">
     To understand the properties of PPTC, we analyze
the instructions and APIs in the benchmark.
Specifically, we explore (i) the number of turns in a session, (ii) the difficulty of the instruction in terms of the number
of APIs required to finish it, and (iii) the
number of multi-modality instructions.
We report statistics about the PPTC benchmark in Figure
     <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ 2.2 Benchmark Collection ‣ 2 PPTC Benchmark ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     .
    </p>
   </div>
   <div class="ltx_para" id="S2.SS4.p2">
    <p class="ltx_p" id="S2.SS4.p2.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS4.p2.1.1">
      The number of turns in a session
     </span>
     The session turn number distribution (Figure
     <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ 2.2 Benchmark Collection ‣ 2 PPTC Benchmark ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     (a)), measured
as the number of turns in a session, shows that all sessions in our benchmark have at least two turns and almost all sessions have at least 3 turns (between 3
and 13 turns for the 5th to 95th percentile, respectively). The longest session has 17 turns, which is very challenging as the errors made in previous turns can influence the completion of the current instruction.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS4.p3">
    <p class="ltx_p" id="S2.SS4.p3.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS4.p3.1.1">
      Diffculty varies in APIs number
     </span>
     The number of APIs in a sequence falls between 1 and
5 for the 5th to 95th percentile (Figure
     <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ 2.2 Benchmark Collection ‣ 2 PPTC Benchmark ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     (b)), respectively, shows that our instructions’ difficulty varies from a simple sentence that can be finished by one API to a complex instruction that requires the LLM to generate multiple APIs. The longest API sequence consists of 29 APIs. Generating long API sequences is very challenging as the LLM needs to understand sub-goals in the complex instruction, select appropriate APIs from the file, and generate APIs in a reliable order.
    </p>
   </div>
   <div class="ltx_para" id="S2.SS4.p4">
    <p class="ltx_p" id="S2.SS4.p4.1">
     <span class="ltx_text ltx_font_bold" id="S2.SS4.p4.1.1">
      Rich multi-modal instructions
     </span>
     Our benchmark has hundreds of instructions that involve multi-modalities content (Figure
     <a class="ltx_ref" href="#S2.F3" title="Figure 3 ‣ 2.2 Benchmark Collection ‣ 2 PPTC Benchmark ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     (c)). The "chart" modality has the fewest instructions, with 120, while the "position" modality has the most, with 292 instructions. To finish these instructions, LLMs need to employ related-modal APIs based on the understanding of multi-modal file content.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Algorithms
  </h2>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    In this section, we introduce the algorithms we considered to enhance the LLM’s performance in our benchmark.
These algorithms can be categorized into two approaches: planning algorithms that help the LLM in decomposing the user instruction and solving it step by step and selection algorithms that assist the LLM in choosing important environmental information or APIs.
   </p>
  </div>
  <section class="ltx_subsection" id="S3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.1
    </span>
    Planning Algorithms
   </h3>
   <div class="ltx_para" id="S3.SS1.p1">
    <p class="ltx_p" id="S3.SS1.p1.1">
     Complex user instructions often require multiple intermediate steps to complete. We mainly consider two planning algorithms:
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p2">
    <p class="ltx_p" id="S3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">
      Zero-shot-CoT
      <cite class="ltx_cite ltx_citemacro_cite">
       Kojima et al.
       <span class="ltx_text ltx_font_medium" id="S3.SS1.p2.1.1.1.1.1.1">
        (
       </span>
       <a class="ltx_ref" href="#bib.bib13" title="">
        2022
       </a>
       <span class="ltx_text ltx_font_medium" id="S3.SS1.p2.1.1.2.2.2.1">
        )
       </span>
      </cite>
     </span>
     enables LLMs to autonomously generate intermediate reasoning processes for complex
instruction by prompting LLMs to "Let’s think step by step".
    </p>
   </div>
   <div class="ltx_para" id="S3.SS1.p3">
    <p class="ltx_p" id="S3.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">
      Tree of Thoughts (ToT)
      <cite class="ltx_cite ltx_citemacro_cite">
       Yao et al.
       <span class="ltx_text ltx_font_medium" id="S3.SS1.p3.1.1.1.1.1.1">
        (
       </span>
       <a class="ltx_ref" href="#bib.bib39" title="">
        2023
       </a>
       <span class="ltx_text ltx_font_medium" id="S3.SS1.p3.1.1.2.2.2.1">
        )
       </span>
      </cite>
     </span>
     enables LLMs to follow tree-like reasoning paths, where each tree node represents a thinking state. It leverages LLMs to generate evaluations or votes on different thoughts.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     3.2
    </span>
    Selection Algorithms
   </h3>
   <div class="ltx_para" id="S3.SS2.p1">
    <p class="ltx_p" id="S3.SS2.p1.1">
     Combining the whole PPT file and the whole API file into the LLM’s input can result in an overwhelming amount of redundant information, such as irrelevant file content and unhelpful APIs. Filtering the redundant information would improve the efficiency of the LLM.
In this context, we primarily focus on two algorithms for selecting the PPT file content and APIs, respectively:
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p2">
    <p class="ltx_p" id="S3.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">
      Content Selection algorithm
     </span>
     Firstly, we extract all shapes of the PPT file by Python-PPTX. Next, we prompt the LLM to select the shapes for completing the user’s instruction. We show the prompt in Figure
     <a class="ltx_ref" href="#A2.F8" title="Figure 8 ‣ Appendix B The Prompt for Content Selection Algorithm ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     , in which we add three demonstration examples to guide the LLM to do selection. In this algorithm, we replace the whole PPT file with the selected shapes when prompting the LLM to generate the API sequence.
    </p>
   </div>
   <div class="ltx_para" id="S3.SS2.p3">
    <p class="ltx_p" id="S3.SS2.p3.1">
     <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">
      API Selection algorithm
     </span>
     The API selection algorithm is based on the embedding similarity to select the most relevant APIs for user instructions. Specifically, we use the text embedding API to get the embeddings of all API descriptions and the current user instruction. Next, we compute the cosine similarity between the instruction embedding and each API description’s embedding and rank them based on the similarity score. In this algorithm, we replace the whole reference API file with the top
     <math alttext="k" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1">
      <semantics id="S3.SS2.p3.1.m1.1a">
       <mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">
        k
       </mi>
       <annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b">
        <ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">
         𝑘
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">
        k
       </annotation>
      </semantics>
     </math>
     APIs when prompting the LLM to generate the API sequence.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Experiments
  </h2>
  <section class="ltx_subsection" id="S4.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.1
    </span>
    Large Language Models Selected for Evaluation
   </h3>
   <div class="ltx_para" id="S4.SS1.p1">
    <p class="ltx_p" id="S4.SS1.p1.1">
     Here, we assess different cutting-edge large language models using our benchmark. These chosen models showcase a wide array of capabilities and are highly regarded in the field. The evaluated large language models include 3 closed-source LLMs and 6 open-source LLMs:
    </p>
   </div>
   <div class="ltx_para" id="S4.SS1.p2">
    <ul class="ltx_itemize" id="S4.I1">
     <li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i1.p1">
       <p class="ltx_p" id="S4.I1.i1.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i1.p1.1.1">
         GPT-4
        </span>
        <cite class="ltx_cite ltx_citemacro_cite">
         OpenAI (
         <a class="ltx_ref" href="#bib.bib18" title="">
          2023
         </a>
         )
        </cite>
        : The latest LLM in the GPT series. GPT-4 is a cutting-edge, large-scale generative pre-trained transformer model. It offers improved performance and a wider knowledge base compared to its predecessors. It showcases human-level proficiency in several scenarios.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i2.p1">
       <p class="ltx_p" id="S4.I1.i2.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i2.p1.1.1">
         ChatGPT
        </span>
        : ChatGPT is a conversational AI model crafted for dynamic interactions. It’s learned from extensive instruction data and fine-tuned through reinforcement learning with human feedback (RLHF). This empowers it to deliver responses that align with human expectations, maintaining context and coherence in conversations.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i3" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i3.p1">
       <p class="ltx_p" id="S4.I1.i3.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i3.p1.1.1">
         Text-Davinci-003
        </span>
        <cite class="ltx_cite ltx_citemacro_cite">
         Brown et al. (
         <a class="ltx_ref" href="#bib.bib2" title="">
          2020
         </a>
         )
        </cite>
        : GPT-3.5 sits between GPT-3 and GPT-4, enhancing performance via additional instruction tuning. It acts as a link between these models, facilitating comparison. We’ve chosen the
        <span class="ltx_text ltx_font_bold" id="S4.I1.i3.p1.1.2">
         Text-Davinci-003
        </span>
        variant from the GPT-3.5 series for our evaluation.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i4" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i4.p1">
       <p class="ltx_p" id="S4.I1.i4.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i4.p1.1.1">
         LLaMa-2-Chat
        </span>
        <cite class="ltx_cite ltx_citemacro_cite">
         Touvron et al. (
         <a class="ltx_ref" href="#bib.bib29" title="">
          2023
         </a>
         )
        </cite>
        : LLaMa 2, an auto-regressive open-source language model, employs an optimized transformer design. Chat versions utilize supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to match human preferences for helpfulness and safety.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i5" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i5.p1">
       <p class="ltx_p" id="S4.I1.i5.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i5.p1.1.1">
         Baichuan-Chat
        </span>
        : It is a transformer model trained on approximately 1.2 trillion tokens. It supports both Chinese and English, with a context window length of 4096.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i6" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i6.p1">
       <p class="ltx_p" id="S4.I1.i6.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i6.p1.1.1">
         Baichuan-2-Chat
        </span>
        <cite class="ltx_cite ltx_citemacro_cite">
         Yang et al. (
         <a class="ltx_ref" href="#bib.bib37" title="">
          2023
         </a>
         )
        </cite>
        : It is a large-scale multilingual language model trained from scratch, on 2.6 trillion tokens. The chat version uses Supervised Fine-Tuning
(SFT) and Reinforcement Learning from Human Feedback (RLHF) to align with humans.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i7" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i7.p1">
       <p class="ltx_p" id="S4.I1.i7.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i7.p1.1.1">
         WizardLM v1.2
        </span>
        <cite class="ltx_cite ltx_citemacro_cite">
         Xu et al. (
         <a class="ltx_ref" href="#bib.bib35" title="">
          2023a
         </a>
         )
        </cite>
        : WizardLM v1.2 is finetuned from LLaMa 2 using supervised instruction fine-tuning, where instructions are created by rewriting the initial instructions step by step.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i8" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i8.p1">
       <p class="ltx_p" id="S4.I1.i8.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i8.p1.1.1">
         Vicuna v1.5 (16k)
        </span>
        <cite class="ltx_cite ltx_citemacro_cite">
         Chiang et al. (
         <a class="ltx_ref" href="#bib.bib4" title="">
          2023
         </a>
         )
        </cite>
        : Vicuna v1.5 (16k) is finetuned from LLaMa 2 using supervised instruction fine-tuning and linear RoPE scaling. It’s trained on about 125K conversations sourced from ShareGPT.com.
       </p>
      </div>
     </li>
     <li class="ltx_item" id="S4.I1.i9" style="list-style-type:none;">
      <span class="ltx_tag ltx_tag_item">
       •
      </span>
      <div class="ltx_para" id="S4.I1.i9.p1">
       <p class="ltx_p" id="S4.I1.i9.p1.1">
        <span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.I1.i9.p1.1.1">
         Code-LLaMa-instruct
        </span>
        <cite class="ltx_cite ltx_citemacro_cite">
         Chiang et al. (
         <a class="ltx_ref" href="#bib.bib4" title="">
          2023
         </a>
         )
        </cite>
        : Code-LLaMa is a LLaMa-based LLM designed for general code completion and understanding. Its instruction version further supports the chat function with users. Code LLaMa models feature a multitask training objective consisting of both autoregressive and causal infilling prediction (predicting the missing part of a program given a surrounding context).
       </p>
      </div>
     </li>
    </ul>
   </div>
   <figure class="ltx_figure ltx_figure_panel" id="S4.F4">
    <svg class="ltx_picture ltx_figure_panel" height="705.19" id="S4.SS1.1.pic1" overflow="visible" version="1.1" width="600">
     <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,705.19) matrix(1 0 0 -1 0 0)">
      <g fill="#00A600" fill-opacity="1.0">
       <path d="M 0 5.91 L 0 699.28 C 0 702.55 2.64 705.19 5.91 705.19 L 594.09 705.19 C 597.36 705.19 600 702.55 600 699.28 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill="#FFFFFF" fill-opacity="1.0">
       <path d="M 1.97 5.91 L 1.97 681.08 L 598.03 681.08 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
       </path>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 686.99)">
       <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.1.pic1.5.5.5.1.1" style="width:402.3pt;">
         <span class="ltx_p" id="S4.SS1.1.pic1.5.5.5.1.1.1">
          <span class="ltx_text ltx_font_bold" id="S4.SS1.1.pic1.5.5.5.1.1.1.1">
           Inference prompt in PPTC
          </span>
         </span>
        </span>
       </foreignobject>
      </g>
      <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
       <foreignobject color="#000000" height="655.49" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
        <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4" style="width:402.3pt;">
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.5">
          (
          <span class="ltx_text ltx_font_bold" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.5.1">
           Task instruction
          </span>
          ) You are an AI assistant to help the user to operate PowerPoint and edit the contents.
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.6">
          Give you the user instruction:&lt;Current user instruction&gt;, you can complete it based on the following APIs and PPT file content. Current you are at page &lt;Page id&gt;. Please finish the user instruction with the functions you have.
Don’t generate instructions beyond what the user has instructed.
Don’t guess what the user may instruct in the next step and generete API for them.
Don’t use python loop to call API. You can only call API once in one line.
If the user does not specify the page to be modified, you can directly start using the APIs without having to navigate to other pages.
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.7">
          You need to generate code which can finish the user instruction. The multiple lines of code should be surrounded by &lt;code&gt; and &lt;/code&gt; such as:
&lt;code&gt;
API();
API();
&lt;/code&gt;
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.8">
          For example, if the user instruction is "create a slide", then the answer should be:
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.9">
          &lt;code&gt;
create_slide();
&lt;/code&gt;
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.10">
          (
          <span class="ltx_text ltx_font_bold" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.10.1">
           API file
          </span>
          ) Now, you have access to a list of PowerPoint APIs with the following functions: &lt;APIs and their descriptions&gt;
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
          (e.g.,API(name="set
          <math alttext="\_" class="ltx_Math" display="inline" id="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1">
           <semantics id="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a">
            <mi id="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">
             _
            </mi>
            <annotation-xml encoding="MathML-Content" id="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b">
             <ci id="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">
              _
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S4.SS1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">
             \_
            </annotation>
           </semantics>
          </math>
          width", parameters="(width)",
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.11">
          description="This API sets the width of the selected object.",
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2">
          parameter
          <math alttext="\_" class="ltx_Math" display="inline" id="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1">
           <semantics id="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1a">
            <mi id="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1" mathvariant="normal" xref="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml">
             _
            </mi>
            <annotation-xml encoding="MathML-Content" id="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1b">
             <ci id="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1.cmml" xref="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1.1">
              _
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S4.SS1.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m1.1c">
             \_
            </annotation>
           </semantics>
          </math>
          description="It takes one parameter ’width’, the width of an object in centimeters as float.",
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3">
          composition
          <math alttext="\_" class="ltx_Math" display="inline" id="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1">
           <semantics id="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1a">
            <mi id="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1" mathvariant="normal" xref="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml">
             _
            </mi>
            <annotation-xml encoding="MathML-Content" id="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1b">
             <ci id="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1">
              _
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S4.SS1.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1c">
             \_
            </annotation>
           </semantics>
          </math>
          instruction="You should first choose an object before you can change the width of it.",
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4">
          api
          <math alttext="\_" class="ltx_Math" display="inline" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1">
           <semantics id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1a">
            <mi id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1" mathvariant="normal" xref="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.cmml">
             _
            </mi>
            <annotation-xml encoding="MathML-Content" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1b">
             <ci id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1.cmml" xref="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1.1">
              _
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m1.1c">
             \_
            </annotation>
           </semantics>
          </math>
          desc="width of picture and shapes")
)
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.12">
          (
          <span class="ltx_text ltx_font_bold" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.12.1">
           PPT file content
          </span>
          ) All the PPT contents are:
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.13">
          &lt;Begin of PPT&gt;
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.14">
          <span class="ltx_text ltx_font_italic" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.14.1">
           Turn-based: &lt;Parsed PPT file content of the label PPT file of the previous turns&gt;
           <br class="ltx_break"/>
           Session-based: &lt;Parsed PPT file content of the LLM prediction file of the previous turns&gt;
           <br class="ltx_break"/>
          </span>
          &lt;End of PPT&gt;
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.15">
          (
          <span class="ltx_text ltx_font_bold" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.15.1">
           Dialogue history
          </span>
          )
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.16">
          ¬User¬:
Hello!
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.17">
          ¬AI¬:
Hi there! How can I help you?
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.18">
          ¬User¬:
&lt;the first instruction&gt;
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.19">
          ¬AI¬:
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.20">
          <span class="ltx_text ltx_font_italic" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.20.1">
           Turn-based: &lt;the correct feasible API sequence&gt;,
           <br class="ltx_break"/>
           Session-based: &lt;the LLM-generated API sequence&gt;
          </span>
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.21">
          …
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.22">
          ¬User¬:
&lt;Current user instruction&gt;. Surrounding your answer with &lt;code&gt; and &lt;/code&gt;.
         </span>
         <span class="ltx_p" id="S4.SS1.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.23">
          ¬AI¬:
         </span>
        </span>
       </foreignobject>
      </g>
     </g>
    </svg>
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     The inference prompt we used in both turn-based and session-based evaluation settings. In the turn-based evaluation, we assess the LLM’s performance for the current turn and assume the LLM has correctly finished previous turns. We then use feasible API sequences of previous turns as the AI response in the dialogue history and parse the label file of previous turns as the PPT file content. In the session-based evaluation, we evaluate the completion of the entire session and do not assume the LLM has correctly finished previous turns. We use the LLM’s generated API sequences as the response and parsed the LLM prediction file as the PPT file content.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="S4.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.2
    </span>
    Experimental Setup
   </h3>
   <div class="ltx_para" id="S4.SS2.p1">
    <p class="ltx_p" id="S4.SS2.p1.1">
     In this section, we provide an overview of the experimental setup utilized to assess the performance of LLMs on our PPTC benchmark.
    </p>
   </div>
   <section class="ltx_subsubsection" id="S4.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      4.2.1
     </span>
     Turn-Based and Session-Based Evaluations
    </h4>
    <div class="ltx_para" id="S4.SS2.SSS1.p1">
     <p class="ltx_p" id="S4.SS2.SSS1.p1.1">
      We consider two performance evaluation approaches in our benchmark: turn-based and session-based evaluations. For the turn-based evaluation, we measure the LLM’s ability to finish a single turn. Specifically, in this evaluation, we assume that the previous turns have been correctly finished, and we prompt the LLM to generate the API sequence to finish the current turn’s user instruction. The prompt consists of the task instruction for finishing the current user instruction, the API file containing feasible APIs, the parsed PPT file content from the PPT file, and dialogue history consisting of instructions of previous turns with their feasible API sequences (see the left of Figure
      <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.1 Large Language Models Selected for Evaluation ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      ).
For the session-based evaluation, we measure the LLM’s ability to finish a session containing multiple turns. For all turns in a session, we prompt the LLM to finish them sequentially. The prompt in this evaluation has two differences: the API solutions for previous turns in dialogue history are the outputs of the LLM instead of the correct API sequences. (2) The PPT content is parsed from the PPT file obtained by executing the previous outputs of the LLM (see the right of Figure
      <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.1 Large Language Models Selected for Evaluation ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
       <span class="ltx_text ltx_ref_tag">
        4
       </span>
      </a>
      ). That means the error made by LLMs in previous turns would influence subsequent turns.
     </p>
    </div>
    <div class="ltx_para" id="S4.SS2.SSS1.p2">
     <p class="ltx_p" id="S4.SS2.SSS1.p2.1">
      <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">
       Metrics
      </span>
      For turn-based evaluation, we report the turn-based accuracy as the ratio of the number of successfully finished turns to the total number of turns. We also report the average token number of the input of one turn and the average API number for finishing one turn as the cost measurement. For session-based evaluation, we report the session-based accuracy as the ratio of the number of successfully finished sessions to the total number of sessions. We also report the average value of the token number of all inputs in one session and the average API number required to complete one session as the cost measurement.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="S4.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.3
    </span>
    Implementation Details
   </h3>
   <div class="ltx_para" id="S4.SS3.p1">
    <p class="ltx_p" id="S4.SS3.p1.1">
     All experiments were conducted using the respective language models’ API provided by Azure OpenAI Service
     <span class="ltx_note ltx_role_footnote" id="footnote2">
      <sup class="ltx_note_mark">
       †
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         †
        </sup>
        <span class="ltx_tag ltx_tag_note">
         †
        </span>
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://azure.microsoft.com/en-us/products/cognitive-services/openai-service" target="_blank" title="">
         https://azure.microsoft.com/en-us/products/cognitive-services/openai-service
        </a>
       </span>
      </span>
     </span>
     . Azure OpenAI services offer two API types: completion and chat completion. Completion API generates text from prompts, while chat completion API responds based on conversation history and new input. We use the completion API for Text-Davinci-003 and the chat completion API for ChatGPT and GPT-4. We set a temperature of zero for deterministic output and a max token limit of 2048. The frequency penalty and top p are kept at their default values of zero and 1, respectively. We use the text-embedding-ada-002 API as the embedding API in the API selection algorithm and set
     <math alttext="k" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1">
      <semantics id="S4.SS3.p1.1.m1.1a">
       <mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">
        k
       </mi>
       <annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b">
        <ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">
         𝑘
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">
        k
       </annotation>
      </semantics>
     </math>
     as 15. For open-source LLMs, we choose the chat version of LLaMa-2, the v1.2 version of WizardLM, and the chat version of Baichuan as our open-source LLMs. We choose the 13 billion parameters model of the three LLMs.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p2">
    <p class="ltx_p" id="S4.SS3.p2.1">
     For the zero-shot CoT method, we add the sentence ’Let’s think step by step’ after the dialogue history of the prompt.
For the ToT method, we follow the official code to run it
     <span class="ltx_note ltx_role_footnote" id="footnote3">
      <sup class="ltx_note_mark">
       ‡
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         ‡
        </sup>
        <span class="ltx_tag ltx_tag_note">
         ‡
        </span>
        ToT:
        <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/princeton-nlp/tree-of-thought-llm" target="_blank" title="">
         https://github.com/princeton-nlp/tree-of-thought-llm
        </a>
       </span>
      </span>
     </span>
     . We run the four algorithm methods based on the GPT-4 model.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS3.p3">
    <p class="ltx_p" id="S4.SS3.p3.1">
     If the token number of the input prompt is beyond the token limit, we cut the PPT file content to reduce the token number of the prompt.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.4
    </span>
    Main results
   </h3>
   <figure class="ltx_table" id="S4.T1">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:449.7pt;height:130.2pt;vertical-align:-0.6pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-149.9pt,43.2pt) scale(0.6,0.6) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S4.T1.1.1.1.1">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1.1" rowspan="3">
          <span class="ltx_text" id="S4.T1.1.1.1.1.1.1">
           Models and Methods
          </span>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="S4.T1.1.1.1.1.2">
          Creating new PPT
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="S4.T1.1.1.1.1.3">
          Editing PPT template
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.2.2">
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T1.1.1.2.2.1">
          Turn-based
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T1.1.1.2.2.2">
          Session-based
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T1.1.1.2.2.3">
          Turn-based
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T1.1.1.2.2.4">
          Session-based
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.3.3">
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.1">
          Accuracy
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.2">
          Avg token
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3.3.3">
          Avg API
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.4">
          Accuracy
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.5">
          Avg token
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3.3.6">
          Avg API
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.7">
          Accuracy
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.8">
          Avg token
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.3.3.9">
          Avg API
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.10">
          Accuracy
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.11">
          Avg token
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.3.12">
          Avg API
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.4.4">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.1">
          TD-003
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.2">
          72.6
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.3">
          2.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.4">
          3.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.5">
          12.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.6">
          20.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.7">
          23.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.8">
          24.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.9">
          2.9k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.4.4.10">
          8.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.11">
          4.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.12">
          13.2k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.4.4.13">
          26.6
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.5.5">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.1">
          ChatGPT
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.2">
          70.6
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.3">
          2.9k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.4">
          3.2
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.5">
          12.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.6">
          20.0k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.7">
          23.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.8">
          26.3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.9">
          4.1k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.5.5.10">
          7.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.11">
          2.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.12">
          9.2k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.5.5.13">
          22.9
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.6.6">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.1">
          GPT-4
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.2">
          75.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.3">
          2.9k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.4">
          2.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.5">
          22.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.6">
          20.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.7">
          22.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.8">
          38.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.9">
          7.5k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.6.6.10">
          7.8
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.11">
          6.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.12">
          24.1k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.6.6.13">
          24.7
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.7.7">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.1.1.7.7.1">
          LLaMa-2
         </th>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.2">
          16.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.3">
          2.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.1.7.7.4">
          3.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.5">
          3.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.6">
          21.6k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.1.7.7.7">
          24.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.8">
          8.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.9">
          2.2k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.1.1.7.7.10">
          7.2
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.11">
          0.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.12">
          9.5k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.1.1.7.7.13">
          15.6
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.8.8">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.1">
          Code-LLaMa
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.2">
          36.8
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.3">
          2.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.4">
          3.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.5">
          0.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.6">
          20.7k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.7">
          32.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.8">
          18.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.9">
          3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.8.8.10">
          7.3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.11">
          2.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.12">
          9.6k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.8.8.13">
          22.6
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.9.9">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.9.9.1">
          WizardLM
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.2">
          23.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.3">
          1.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.9.9.4">
          3.3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.5">
          4.3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.6">
          12.5k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.9.9.7">
          22.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.8">
          10.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.9">
          1.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.9.9.10">
          5.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.11">
          0.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.12">
          4.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.9.9.13">
          16.5
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.10.10">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.10.10.1">
          Vicuna-v1.5
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.2">
          24.3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.3">
          1.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.10.10.4">
          3.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.5">
          2.2
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.6">
          11.0k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.10.10.7">
          33.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.8">
          6.8
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.9">
          1.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.10.10.10">
          6.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.11">
          0.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.12">
          4.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.10.10.13">
          22.7
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.11.11">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.1.1.11.11.1">
          Baichuan
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.2">
          15.5
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.3">
          1.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.11.11.4">
          9.8
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.5">
          0.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.6">
          10.9k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.11.11.7">
          44.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.8">
          4.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.9">
          1.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.11.11.10">
          9.6
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.11">
          0.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.12">
          4.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.11.11.13">
          24.3
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T1.1.1.12.12">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.1.12.12.1">
          Baichuan-2
         </th>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.2">
          16.3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.3">
          1.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.1.12.12.4">
          9.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.5">
          3.6
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.6">
          11.6k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.1.12.12.7">
          48.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.8">
          8.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.9">
          1.3k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.1.12.12.10">
          9.2
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.11">
          0.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.12">
          4.2k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.1.1.12.12.13">
          22.3
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     We report the results of LLMs in this table.’ TD-003’ is the Text-Davinci-003 model. We directly use the prompts in Figure
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.1 Large Language Models Selected for Evaluation ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     to prompt LLMs to generate the API sequence.
    </figcaption>
   </figure>
   <figure class="ltx_table" id="S4.T2">
    <div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:447.4pt;height:84.1pt;vertical-align:-0.6pt;">
     <span class="ltx_transformed_inner" style="transform:translate(-162.0pt,30.2pt) scale(0.58,0.58) ;">
      <table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
       <tbody class="ltx_tbody">
        <tr class="ltx_tr" id="S4.T2.1.1.1.1">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.1.1.1" rowspan="3">
          <span class="ltx_text" id="S4.T2.1.1.1.1.1.1">
           Models and Methods
          </span>
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="S4.T2.1.1.1.1.2">
          Creating new PPT file
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="S4.T2.1.1.1.1.3">
          Editing PPT template
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.2.2">
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T2.1.1.2.2.1">
          Turn-based
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T2.1.1.2.2.2">
          Session-based
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3" id="S4.T2.1.1.2.2.3">
          Turn-based
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" colspan="3" id="S4.T2.1.1.2.2.4">
          Session-based
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.3.3">
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.1">
          Accuracy
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.2">
          Avg token
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3.3.3">
          Avg API
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.4">
          Accuracy
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.5">
          Avg token
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3.3.6">
          Avg API
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.7">
          Accuracy
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.8">
          Avg token
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3.3.9">
          Avg API
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.10">
          Accuracy
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.11">
          Avg token
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.3.12">
          Avg API
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.4.4">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.4.4.1">
          GPT-4
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.2">
          75.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.3">
          2.9k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4.4.4">
          2.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.5">
          22.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.6">
          20.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4.4.7">
          22.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.8">
          38.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.9">
          7.5k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4.4.10">
          7.8
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.11">
          6.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.12">
          24.1k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.4.13">
          24.7
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.5.5">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.5.5.1">
          GPT-4+CoT
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.2">
          77.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.3">
          2.9k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5.5.4">
          3.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.5">
          23.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.6">
          20.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5.5.7">
          22.7
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.8">
          40.6
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.9">
          7.5k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5.5.10">
          8.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.11">
          6.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.12">
          24.1k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.5.13">
          25.2
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.6.6">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.6.6.1">
          GPT-4+ToT
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.2">
          76.5
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.3">
          20.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.6.6.4">
          3.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.5">
          21.8
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.6">
          146.4k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.6.6.7">
          22.6
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.8">
          40.6
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.9">
          81k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.6.6.10">
          7.6
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.11">
          4.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.12">
          256.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.6.6.13">
          24.0
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.7.7">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T2.1.1.7.7.1">
          GPT-4+Content selection
         </th>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.2">
          77.5
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.3">
          3.4k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.7.7.4">
          3.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.5">
          21.8
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.6">
          24.5k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.7.7.7">
          22.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.8">
          43.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.9">
          5.8k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.7.7.10">
          8.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.11">
          4.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.12">
          18.7k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.7.7.13">
          25.2
         </td>
        </tr>
        <tr class="ltx_tr" id="S4.T2.1.1.8.8">
         <th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.1.8.8.1">
          GPT-4+API selection
         </th>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.2">
          76.4
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.3">
          1.5k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.1.8.8.4">
          2.9
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.5">
          18.8
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.6">
          10.6k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.1.8.8.7">
          21.3
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.8">
          38.1
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.9">
          7k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.1.8.8.10">
          8.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.11">
          10.0
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.12">
          22.4k
         </td>
         <td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.8.8.13">
          25.8
         </td>
        </tr>
       </tbody>
      </table>
     </span>
    </div>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     We report the results of GPT-4 and algorithms based on the GPT-4 model. ’CoT’ and ’ToT’ are the chain of thought and tree of thought algorithms.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS4.p1">
    <p class="ltx_p" id="S4.SS4.p1.1">
     We report the results of LLMs in both turn-based and session-based evaluations in Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.4 Main results ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     and
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.4 Main results ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
From the results, we highlight the following key findings.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS4.p2">
    <p class="ltx_p" id="S4.SS4.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">
      (1) Superior Performance of GPT-4:
     </span>
     GPT-4 consistently outperforms other closed-source and open-source LLMs in both two tasks. Impressively, GPT-4 achieves 75.1% turn-based accuracy in the creating new PPT file task, demonstrating its strong capability to finish one turn of the user instruction. GPT-4 also has a lower API cost compared to other closed-source LLMs since its precise API usage. GPT-4 incurs the highest token expense when editing PPT templates. That is because its higher token limit than other LLMs allows us to input more PPT template content.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS4.p3">
    <p class="ltx_p" id="S4.SS4.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS4.p3.1.1">
      (2) Code continual pre-training and further instruction finetuning can boost open-source LLMs’ performance.
     </span>
     : Based on Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.4 Main results ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , it’s evident that current open-source LLMs struggle to match the performance of closed-source LLMs. For example, LLaMa-2-chat only achieves 16.2% turn-based accuracy in the creating new PPT file task, which is far from the performance achieved by closed-source LLMs.
We further find that code continual pretraining (Code-LLaMa) and instruction fine-tuning based on LLaMa-2 (WizardLM and Vicuna) can further improve LLaMa-2 performance obviously. For example, Code-LLaMa improves LLaMa-2’s turn-based accuracy in the creating new PPT file task by 20.4 %. This observation suggests that there’s untapped potential in open-source LLMs when it comes to our benchmark, and this potential can be unlocked further by code pre-training and enhancing instruction following ability.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS4.p4">
    <p class="ltx_p" id="S4.SS4.p4.3">
     <span class="ltx_text ltx_font_bold" id="S4.SS4.p4.3.1">
      (3) Planning and selection algorithms can improve LLMs’ turn-based performance
     </span>
     From Table
     <a class="ltx_ref" href="#S4.T2" title="Table 2 ‣ 4.4 Main results ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     , we observe that the planning algorithms (CoT and ToT) can further improve the turn-based performance of GPT-4 by 1
     <math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS4.p4.1.m1.1">
      <semantics id="S4.SS4.p4.1.m1.1a">
       <mo id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b">
        <csymbol cd="latexml" id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     2 percent. However, we surprisingly find that the more complex ToT algorithm does not outperform the zero-shot CoT algorithm with a 5
     <math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS4.p4.2.m2.1">
      <semantics id="S4.SS4.p4.2.m2.1a">
       <mo id="S4.SS4.p4.2.m2.1.1" xref="S4.SS4.p4.2.m2.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.SS4.p4.2.m2.1b">
        <csymbol cd="latexml" id="S4.SS4.p4.2.m2.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS4.p4.2.m2.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     10 times token cost. Content and API selection algorithms can further improve the turn-based performance of GPT-4 by 1
     <math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS4.p4.3.m3.1">
      <semantics id="S4.SS4.p4.3.m3.1a">
       <mo id="S4.SS4.p4.3.m3.1.1" xref="S4.SS4.p4.3.m3.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.SS4.p4.3.m3.1b">
        <csymbol cd="latexml" id="S4.SS4.p4.3.m3.1.1.cmml" xref="S4.SS4.p4.3.m3.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS4.p4.3.m3.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     5 percent. That is because they reduce the task difficulty by filtering irrelevant PPT content/APIs in the input prompt. The API selection algorithm also reduces the average token cost by reducing the number of APIs listed in the prompt. However, for the challenging session-based evaluation, these algorithms can not improve GPT-4’s performance or improve it slightly.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S4.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     4.5
    </span>
    Three challenges in our PPTC benchmark
   </h3>
   <div class="ltx_para" id="S4.SS5.p1">
    <p class="ltx_p" id="S4.SS5.p1.1">
     From the result Table
     <a class="ltx_ref" href="#S4.T1" title="Table 1 ‣ 4.4 Main results ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     and Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.5 Three challenges in our PPTC benchmark ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     . we highlight the following three key challenges.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS5.p2">
    <p class="ltx_p" id="S4.SS5.p2.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS5.p2.1.1">
      (1) Error accumulation makes LLMs performance poor in finishing the entire multi-turn session.
     </span>
     : The performance of all LLMs in handling sessions consisting of multiple turns is notably poor. Even GPT-4, which performs well in turn-based evaluation, achieves only a 22.7% session-based accuracy for the "creating new PPT file" task and a mere 6.0% session-based accuracy for the "editing PPT template" task. Current planning algorithms usually fail to improve session-based accuracy. In some cases, they can even make the performance worse. The session-based evaluation is challenging since errors made in previous turns make the LLM fail to finish the session and also influence the completion of the current turn. Also, we need more advanced planning algorithms to complete the multi-turn session.
    </p>
   </div>
   <figure class="ltx_figure" id="S4.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="189" id="S4.F5.g1" src="/html/2311.01767/assets/analysis_2.png" width="595"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     We illustrate the analysis results of the creating new PPT file task (task 1) and the editing PPT template task (task 2). In sub-figure (a), we report the average turn-based accuracy for instructions involving chart, table, picture, position, and pure text. We don’t draw the accuracy of task 2 as no chart instruction in this task. In sub-figure (b), we report the ratio of four common errors made by GPT-4. In sub-figure (c), we report the accuracy with the model size. We don’t plot the session-based accuracy of task 2 as it is zero.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S4.SS5.p3">
    <p class="ltx_p" id="S4.SS5.p3.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS5.p3.1.1">
      (2) LLMs perform badly in processing long PPT template:
     </span>
     Current LLMs’ performance in the editing PPT temples task is pretty poor. For example, the strongest GPT-4 only achieves 38.1% turn-based accuracy and 6.0% session-based accuracy in this task. Other LLMs’ performance is even poorer. The content selection algorithm can partially solve this challenge by filtering out irrelevant file content, but GPT-4 with it still only achieves 43.1% turn-based accuracy. That means current LLMs (e.g., GPT-4) still struggle to handle complex and lengthy PPT templates. For open-source LLMs, there’s a risk of information loss due to token limitations (typically 2
     <math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS5.p3.1.m1.1">
      <semantics id="S4.SS5.p3.1.m1.1a">
       <mo id="S4.SS5.p3.1.m1.1.1" xref="S4.SS5.p3.1.m1.1.1.cmml">
        ∼
       </mo>
       <annotation-xml encoding="MathML-Content" id="S4.SS5.p3.1.m1.1b">
        <csymbol cd="latexml" id="S4.SS5.p3.1.m1.1.1.cmml" xref="S4.SS5.p3.1.m1.1.1">
         similar-to
        </csymbol>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S4.SS5.p3.1.m1.1c">
        \sim
       </annotation>
      </semantics>
     </math>
     4K tokens limit), which often require truncating lengthy PPT content. When it comes to session-based performance, the accuracy remains nearly zero. This implies that current LLMs are still far from being ideal PPT agents capable of effectively assisting users in editing PPT templates during a multi-turn dialogue session.
    </p>
   </div>
   <div class="ltx_para" id="S4.SS5.p4">
    <p class="ltx_p" id="S4.SS5.p4.1">
     <span class="ltx_text ltx_font_bold" id="S4.SS5.p4.1.1">
      (3) Multi-modal instructions increase the LLM’s failure rate significantly.
     </span>
     To assess LLMs’ task completion performance for instructions involving multi-modal operations (Table, Chart, Picture, Position, and text), we calculate the average accuracy of GPT-4 for instructions involving each modality, respectively. This is done by dividing the number of correctly completed instructions within each modality by the total number of instructions involving that modality’s operation. The results are presented in Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.5 Three challenges in our PPTC benchmark ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     (a). From the figure, we observe that GPT-4 performs exceptionally well in the text modality, achieving an accuracy of 85.6%. Its performance becomes poorer when processing structured data (Chart and Table), with 12.4% and 16.2% lower accuracy. Instructions involving picture-related operation pose an even greater challenge for GPT-4, as it achieves a 56.8% turn-based accuracy in this modality. GPT-4 exhibits its weakest performance in instructions involving position-related (spatial) operations, with only 24% accuracy. This underscores GPT-4’s limitations in spatial perception ability.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Analysis
  </h2>
  <div class="ltx_para" id="S5.p1">
   <p class="ltx_p" id="S5.p1.1">
    In this section, we analyze the reasons for GPT-4’s errors. We further analyze the influence of model size and dialogue history.
   </p>
  </div>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS1.1.1">
     Error Analysis of GPT-4 in our benchmark
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     To analyze the error made by GPT-4, in our benchmark, we gather 50 wrong samples for each of the two tasks in our benchmark in the turn-based evaluation.
We find that these wrong samples fall into four error types and visualize the distribution of these four main error types in Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.5 Three challenges in our PPTC benchmark ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     (b): (1) Position errors: These occur when GPT-4 struggles with instructions involving position adjustments. For example, when asked to move the shape to the bottom of the slide, GPT-4 wrongly calls the "set
     <math alttext="\_" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1">
      <semantics id="S5.SS1.p1.1.m1.1a">
       <mi id="S5.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S5.SS1.p1.1.m1.1.1.cmml">
        _
       </mi>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b">
        <ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">
         _
        </ci>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">
        \_
       </annotation>
      </semantics>
     </math>
     top" API. position error is the main error in the creating new PPT file task as this task contains many instructions involving position operation. (2) Calling unavailable APIs: GPT-4 sometimes generates APIs that don’t actually exist in the reference API file, resulting in what we call the "API hallucination problem." (3) Misunderstanding PPT file content: GPT-4’s comprehension of the PPT content can be flawed, leading to the generation of incorrect API sequences. For example, when instructed to make the font size of the current slide’s title consistent with previous slides, GPT-4 set a font size that is different from what was used in previous slides’ titles. In the editing template task, misunderstanding the PPT content becomes the main error since this task needs to understand the complex PPT template. (4) Unfollowing Powerpoint task rules: Completing Powerpoint tasks demands a deep understanding of various task rules. For instance, writing a new slide title requires first deleting the original text and then inserting the new title into the text box. However, GPT-4 may directly insert the new content. For the session-based evaluation, we also collect 50 wrong examples. We find that the main reason for the poor session-based performance is the LLM fails to finish the session once it makes an error in one turn of the session. The reasons for errors made in a single turn are similar to those in the turn-based evaluation. One unique phenomenon in this evaluation is that the LLM would repeat previous errors (e.g., employing infeasible APIs) in subsequent turns.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS2.1.1">
     Does bigger LLM work better on PPTC?
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     To investigate how the model size impacts the LLM’s performance in our benchmark, we conduct tests using LLaMa-2-chat LLM with 7, 13, and 70 billion parameters and plot the results in Figure
     <a class="ltx_ref" href="#S4.F5" title="Figure 5 ‣ 4.5 Three challenges in our PPTC benchmark ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     (c). We observe that larger LLM consistently achieve higher turn-based accuracy for both the creating new PPT and editing PPT template tasks. For example, in the creating new PPT file task, we find that the turn-based accuracy increases from 13.2 (7B) to 30.1 (70B). However, we do not observe a clear positive correlation between model size and session-based performance. One possible explanation is that although the 70B LLM can correctly finish more intermediate steps, it still falls short of completing the entire session. To improve the session-based performance, a larger LLM may be necessary.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    <span class="ltx_text ltx_font_italic" id="S5.SS3.1.1">
     Does dialogue history help LLMs to generate the API sequence?
    </span>
   </h3>
   <div class="ltx_para" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     To investigate the influence of dialogue history in our prompt (see Figure
     <a class="ltx_ref" href="#S4.F4" title="Figure 4 ‣ 4.1 Large Language Models Selected for Evaluation ‣ 4 Experiments ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     ), we make an ablation experiment for the dialogue history component of our turn-based evaluation prompt
     <span class="ltx_note ltx_role_footnote" id="footnote4">
      <sup class="ltx_note_mark">
       §
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         §
        </sup>
        <span class="ltx_tag ltx_tag_note">
         §
        </span>
        The task instruction, current user instruction, API file, PPT content in the prompt are necessary parts for generating the API sequence. So we don’t conduct ablation studies on them.
       </span>
      </span>
     </span>
     . In this evaluation, the dialogue history contains previous turns along with their feasible API sequences. When we removed the dialogue history from the prompt, we observed a decline in GPT-4’s performance.
Specifically, GPT-4 drops its performance from 75.1 % to 73.1 % in the creating new PPT file task and decreases its performance by 6.2 % in the editing template task. This experiment shows the positive effect of the dialogue history, as it helps the LLM to both understand the dialogue background and instruct the LLM to correctly use the APIs, similar to few-shot demonstration examples.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Related Works
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    <span class="ltx_text ltx_font_bold" id="S6.p1.1.1">
     Large Language Models
    </span>
    like ChatGPT, GPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     Bubeck et al. (
     <a class="ltx_ref" href="#bib.bib3" title="">
      2023
     </a>
     ); OpenAI (
     <a class="ltx_ref" href="#bib.bib18" title="">
      2023
     </a>
     )
    </cite>
    , and Bard have billions of parameters and have been trained on the Internet corpus with trillions of tokens. They can write code
    <cite class="ltx_cite ltx_citemacro_cite">
     Liu et al. (
     <a class="ltx_ref" href="#bib.bib15" title="">
      2023a
     </a>
     )
    </cite>
    , prove mathematical theorems
    <cite class="ltx_cite ltx_citemacro_cite">
     Jiang et al. (
     <a class="ltx_ref" href="#bib.bib9" title="">
      2022
     </a>
     )
    </cite>
    , pass the professional exam
    <cite class="ltx_cite ltx_citemacro_cite">
     Zhong et al. (
     <a class="ltx_ref" href="#bib.bib40" title="">
      2023
     </a>
     ); Gilson et al. (
     <a class="ltx_ref" href="#bib.bib7" title="">
      2023
     </a>
     ); Katz et al. (
     <a class="ltx_ref" href="#bib.bib11" title="">
      2023
     </a>
     )
    </cite>
    , and also perform well on other basic natural language tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     Kim et al. (
     <a class="ltx_ref" href="#bib.bib12" title="">
      2023
     </a>
     ); Jiao et al. (
     <a class="ltx_ref" href="#bib.bib10" title="">
      2023
     </a>
     ); Zhong et al. (
     <a class="ltx_ref" href="#bib.bib40" title="">
      2023
     </a>
     ); Wang et al. (
     <a class="ltx_ref" href="#bib.bib32" title="">
      2023b
     </a>
     )
    </cite>
    . That raises the hope of achieving artificial general intelligence (AGI).
   </p>
  </div>
  <div class="ltx_para" id="S6.p2">
   <p class="ltx_p" id="S6.p2.1">
    To further boost LLM’s performance on the specific task, one approach involves prompting engineerings, such as the chain of thought prompting
    <cite class="ltx_cite ltx_citemacro_cite">
     Wei et al. (
     <a class="ltx_ref" href="#bib.bib33" title="">
      2022
     </a>
     ); Shi et al. (
     <a class="ltx_ref" href="#bib.bib27" title="">
      2022
     </a>
     ); Yao et al. (
     <a class="ltx_ref" href="#bib.bib39" title="">
      2023
     </a>
     )
    </cite>
    , self-consistency
    <cite class="ltx_cite ltx_citemacro_cite">
     Wang et al. (
     <a class="ltx_ref" href="#bib.bib31" title="">
      2022
     </a>
     )
    </cite>
    and the least to most prompting
    <cite class="ltx_cite ltx_citemacro_cite">
     Zhou et al. (
     <a class="ltx_ref" href="#bib.bib41" title="">
      2022
     </a>
     )
    </cite>
    . Another approach aims to use feedback to improve performance. The self-refine method
    <cite class="ltx_cite ltx_citemacro_cite">
     Madaan et al. (
     <a class="ltx_ref" href="#bib.bib17" title="">
      2023
     </a>
     )
    </cite>
    refines the output through iterative feedback and refinement Provided by LLM itself. The Reflexion
    <cite class="ltx_cite ltx_citemacro_cite">
     Shinn et al. (
     <a class="ltx_ref" href="#bib.bib28" title="">
      2023
     </a>
     )
    </cite>
    method generates and stores the reflection based on the sparse reward signal and then uses the reflection to induce better decisions in subsequent trials. The learning to program method
    <cite class="ltx_cite ltx_citemacro_cite">
     Guo et al. (
     <a class="ltx_ref" href="#bib.bib8" title="">
      2023
     </a>
     )
    </cite>
    learns the task program by inducing the general solutions from the errors (feedback) iteratively and uses the program to guide the inference.
   </p>
  </div>
  <div class="ltx_para" id="S6.p3">
   <p class="ltx_p" id="S6.p3.1">
    <span class="ltx_text ltx_font_bold" id="S6.p3.1.1">
     Task completion benchmarks for measuring large language models
    </span>
    . To measure LLM’s task completion performance,
Saycan
    <cite class="ltx_cite ltx_citemacro_cite">
     Brohan et al. (
     <a class="ltx_ref" href="#bib.bib1" title="">
      2023
     </a>
     )
    </cite>
    and VirtualHome
    <cite class="ltx_cite ltx_citemacro_cite">
     Puig et al. (
     <a class="ltx_ref" href="#bib.bib20" title="">
      2018
     </a>
     )
    </cite>
    benchmarks ask LLM to generate the correct action sequence for controlling the robot to finish user instruction. WebShop
    <cite class="ltx_cite ltx_citemacro_cite">
     Yao et al. (
     <a class="ltx_ref" href="#bib.bib38" title="">
      2022
     </a>
     )
    </cite>
    and Android in the wild
    <cite class="ltx_cite ltx_citemacro_cite">
     Rawles et al. (
     <a class="ltx_ref" href="#bib.bib24" title="">
      2023
     </a>
     )
    </cite>
    ask LLM to navigate websites and conduct actions to meet the user requirement. APIBench
    <cite class="ltx_cite ltx_citemacro_cite">
     Patil et al. (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     )
    </cite>
    and ToolBench
    <cite class="ltx_cite ltx_citemacro_cite">
     Xu et al. (
     <a class="ltx_ref" href="#bib.bib36" title="">
      2023b
     </a>
     ); Qin et al. (
     <a class="ltx_ref" href="#bib.bib22" title="">
      2023b
     </a>
     )
    </cite>
    involve selecting and using APIs to complete the task instruction. Agentbench
    <cite class="ltx_cite ltx_citemacro_cite">
     Liu et al. (
     <a class="ltx_ref" href="#bib.bib16" title="">
      2023b
     </a>
     )
    </cite>
    assesses LLM as autonomous agents in 8 environments and WebArena
    <cite class="ltx_cite ltx_citemacro_cite">
     Zhou et al. (
     <a class="ltx_ref" href="#bib.bib42" title="">
      2023
     </a>
     )
    </cite>
    considers task completion in web-based interactions.
   </p>
  </div>
  <div class="ltx_para" id="S6.p4">
   <p class="ltx_p" id="S6.p4.1">
    <span class="ltx_text ltx_font_bold" id="S6.p4.1.1">
     AI assistant system for complex task completion
    </span>
    For more complex tasks that involve using tools and utilizing environmental information, there are many strong AI systems (e.g., TaskMatrix
    <cite class="ltx_cite ltx_citemacro_cite">
     Liang et al. (
     <a class="ltx_ref" href="#bib.bib14" title="">
      2023
     </a>
     )
    </cite>
    ) that help the user finish the complex task. One approach involves connecting massive models and tools with the Large Language Model for task completion. Examples include Visual ChatGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Wu et al. (
     <a class="ltx_ref" href="#bib.bib34" title="">
      2023
     </a>
     )
    </cite>
    and HuggingGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     Shen et al. (
     <a class="ltx_ref" href="#bib.bib26" title="">
      2023
     </a>
     )
    </cite>
    which use LLM to deploy task-specific models to finish the user instruction based on the observation of task information (e.g., visual information), Voyager
    <cite class="ltx_cite ltx_citemacro_cite">
     Wang et al. (
     <a class="ltx_ref" href="#bib.bib30" title="">
      2023a
     </a>
     )
    </cite>
    that uses the fixed LLM to continually learn skills (tools) based on the observation of the Minecraft environment. Another approach involves training an end-to-end LLM to finish the user instruction. Examples include Gorilla
    <cite class="ltx_cite ltx_citemacro_cite">
     Patil et al. (
     <a class="ltx_ref" href="#bib.bib19" title="">
      2023
     </a>
     )
    </cite>
    for generating API calls to finish the user query by using the API bench to fine-tune the pre-trained LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     Touvron et al. (
     <a class="ltx_ref" href="#bib.bib29" title="">
      2023
     </a>
     )
    </cite>
    model. PaLM-E
    <cite class="ltx_cite ltx_citemacro_cite">
     Driess et al. (
     <a class="ltx_ref" href="#bib.bib6" title="">
      2023
     </a>
     )
    </cite>
    for various robot reasoning tasks by fine-tuning the pretrained PaLM
    <cite class="ltx_cite ltx_citemacro_cite">
     Chowdhery et al. (
     <a class="ltx_ref" href="#bib.bib5" title="">
      2022
     </a>
     )
    </cite>
    model using features from sensor modalities. Different from the above systems, we focus on the research topic of the AI assistant system in office software.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    We introduce the PowerPoint Task Completion benchmark to measure LLMs’ ability to complete user instructions within the context of the PowerPoint software. It contains hundreds of multi-turn sessions with different topics and thousands of instructions with varying levels of difficulty. We further propose the PPTX-evaluation system to access and compare the performance of different LLMs. Results show that GPT-4 is the strongest LLM but still performs poorly in finishing entire sessions. We further analyze the behavior of LLMs and find three main error factors that limit their performance. Our benchmark and findings can help the research community design better AI task completion assistants.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S8">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    8
   </span>
   Limitations
  </h2>
  <div class="ltx_para" id="S8.p1">
   <p class="ltx_p" id="S8.p1.1">
    Our benchmark does not consider instructions that involve subjective evaluation. For example, the user may want to make the slide more beautiful. However, it’s hard to automatically evaluate if the generated file (the model output) is more beautiful. Another limitation is that we do not consider the instructions that need non-API operations. For example, the user may want to draw a cat on the slide. That instruction needs the AI-assistant system to draw the cat by dragging the mouse and is still infeasible for LLMs and LLM-based systems. We only consider instructions that can be completed by directly executing the API sequence.
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brohan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Do as i can, not as i say: Grounding language in robotic affordances.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">
      Conference on Robot Learning
     </em>
     , pages 287–318. PMLR.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Brown et al. (2020)
    </span>
    <span class="ltx_bibblock">
     Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.
    </span>
    <span class="ltx_bibblock">
     Language models are few-shot learners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">
      Advances in neural information processing systems
     </em>
     , 33:1877–1901.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Bubeck et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Sparks of artificial general intelligence: Early experiments with gpt-4.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">
      arXiv preprint arXiv:2303.12712
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chiang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">
      See https://vicuna. lmsys. org (accessed 14 April 2023)
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Chowdhery et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Palm: Scaling language modeling with pathways.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2204.02311
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Driess et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Palm-e: An embodied multimodal language model.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:2303.03378
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Gilson et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     How does chatgpt perform on the united states medical licensing examination? the implications of large language models for medical education and knowledge assessment.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">
      JMIR Medical Education
     </em>
     , 9(1):e45312.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Guo et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yiduo Guo, Yaobo Liang, Chenfei Wu, Wenshan Wu, Dongyan Zhao, and Nan Duan. 2023.
    </span>
    <span class="ltx_bibblock">
     Learning to program with natural language.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">
      arXiv preprint arXiv:2304.10464
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix, Yuhuai Wu, and Guillaume Lample. 2022.
    </span>
    <span class="ltx_bibblock">
     Draft, sketch, and prove: Guiding formal theorem provers with informal proofs.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">
      arXiv preprint arXiv:2210.12283
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Jiao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing Wang, and Zhaopeng Tu. 2023.
    </span>
    <span class="ltx_bibblock">
     Is chatgpt a good translator? a preliminary study.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">
      arXiv preprint arXiv:2301.08745
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Katz et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2023.
    </span>
    <span class="ltx_bibblock">
     Gpt-4 passes the bar exam.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">
      Available at SSRN 4389233
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kim et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023.
    </span>
    <span class="ltx_bibblock">
     Language models can solve computer tasks.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">
      arXiv preprint arXiv:2303.17491
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Kojima et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022.
    </span>
    <span class="ltx_bibblock">
     Large language models are zero-shot reasoners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">
      Advances in neural information processing systems
     </em>
     , 35:22199–22213.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">
      arXiv preprint arXiv:2303.16434
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023a.
    </span>
    <span class="ltx_bibblock">
     Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">
      arXiv preprint arXiv:2305.01210
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Liu et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023b.
    </span>
    <span class="ltx_bibblock">
     Agentbench: Evaluating llms as agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">
      arXiv preprint arXiv:2308.03688
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Madaan et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Self-refine: Iterative refinement with self-feedback.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">
      arXiv preprint arXiv:2303.17651
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     OpenAI (2023)
    </span>
    <span class="ltx_bibblock">
     OpenAI. 2023.
    </span>
    <span class="ltx_bibblock">
     Gpt-4 technical report.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">
      arXiv preprint arXiv:2303.08774
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Patil et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023.
    </span>
    <span class="ltx_bibblock">
     Gorilla: Large language model connected with massive apis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">
      arXiv preprint arXiv:2305.15334
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Puig et al. (2018)
    </span>
    <span class="ltx_bibblock">
     Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018.
    </span>
    <span class="ltx_bibblock">
     Virtualhome: Simulating household activities via programs.
    </span>
    <span class="ltx_bibblock">
     In
     <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">
      Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
     </em>
     , pages 8494–8502.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023a.
    </span>
    <span class="ltx_bibblock">
     Is chatgpt a general-purpose natural language processing task solver?
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">
      arXiv preprint arXiv:2302.06476
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023b.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.08354" target="_blank" title="">
      Tool learning with foundation models
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Qin et al. (2023c)
    </span>
    <span class="ltx_bibblock">
     Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. 2023c.
    </span>
    <span class="ltx_bibblock">
     Toolllm: Facilitating large language models to master 16000+ real-world apis.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2307.16789
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Rawles et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2023.
    </span>
    <span class="ltx_bibblock">
     Android in the wild: A large-scale dataset for android device control.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">
      arXiv preprint arXiv:2307.10088
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Schick et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.
    </span>
    <span class="ltx_bibblock">
     Toolformer: Language models can teach themselves to use tools.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">
      arXiv preprint arXiv:2302.04761
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shen et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">
      arXiv preprint arXiv:2303.17580
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shi et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.
    </span>
    <span class="ltx_bibblock">
     Language models are multilingual chain-of-thought reasoners.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">
      arXiv preprint arXiv:2210.03057
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Shinn et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Noah Shinn, Beck Labash, and Ashwin Gopinath. 2023.
    </span>
    <span class="ltx_bibblock">
     Reflexion: an autonomous agent with dynamic memory and self-reflection.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:2303.11366
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Touvron et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">
      arXiv preprint arXiv:2302.13971
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a.
    </span>
    <span class="ltx_bibblock">
     Voyager: An open-ended embodied agent with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">
      arXiv preprint arXiv:2305.16291
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.
    </span>
    <span class="ltx_bibblock">
     Self-consistency improves chain of thought reasoning in language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">
      arXiv preprint arXiv:2203.11171
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wang et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui Xia. 2023b.
    </span>
    <span class="ltx_bibblock">
     Is chatgpt a good sentiment analyzer? a preliminary study.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">
      arXiv preprint arXiv:2304.04339
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wei et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
    </span>
    <span class="ltx_bibblock">
     Chain of thought prompting elicits reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">
      arXiv preprint arXiv:2201.11903
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Wu et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. 2023.
    </span>
    <span class="ltx_bibblock">
     Visual chatgpt: Talking, drawing and editing with visual foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">
      arXiv preprint arXiv:2303.04671
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2023a)
    </span>
    <span class="ltx_bibblock">
     Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023a.
    </span>
    <span class="ltx_bibblock">
     Wizardlm: Empowering large language models to follow complex instructions.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">
      arXiv preprint arXiv:2304.12244
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Xu et al. (2023b)
    </span>
    <span class="ltx_bibblock">
     Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023b.
    </span>
    <span class="ltx_bibblock">
     On the tool manipulation capability of open-source large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">
      arXiv preprint arXiv:2305.16504
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yang et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, Fan Yang, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Baichuan 2: Open large-scale language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">
      arXiv preprint arXiv:2309.10305
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022.
    </span>
    <span class="ltx_bibblock">
     Webshop: Towards scalable real-world web interaction with grounded language agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">
      Advances in Neural Information Processing Systems
     </em>
     , 35:20744–20757.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Yao et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. 2023.
    </span>
    <span class="ltx_bibblock">
     Tree of thoughts: Deliberate problem solving with large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">
      arXiv preprint arXiv:2305.10601
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhong et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023.
    </span>
    <span class="ltx_bibblock">
     Agieval: A human-centric benchmark for evaluating foundation models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">
      arXiv preprint arXiv:2304.06364
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhou et al. (2022)
    </span>
    <span class="ltx_bibblock">
     Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. 2022.
    </span>
    <span class="ltx_bibblock">
     Least-to-most prompting enables complex reasoning in large language models.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">
      arXiv preprint arXiv:2205.10625
     </em>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">
     Zhou et al. (2023)
    </span>
    <span class="ltx_bibblock">
     Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. 2023.
    </span>
    <span class="ltx_bibblock">
     Webarena: A realistic web environment for building autonomous agents.
    </span>
    <span class="ltx_bibblock">
     <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">
      arXiv preprint arXiv:2307.13854
     </em>
     .
    </span>
   </li>
  </ul>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   The API Reference File
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.1">
    We list all APIs and their descriptions in Figures
    <a class="ltx_ref" href="#A1.F6" title="Figure 6 ‣ Appendix A The API Reference File ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
     <span class="ltx_text ltx_ref_tag">
      6
     </span>
    </a>
    and
    <a class="ltx_ref" href="#A1.F7" title="Figure 7 ‣ Appendix A The API Reference File ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
     <span class="ltx_text ltx_ref_tag">
      7
     </span>
    </a>
    . We provide 49 feasible APIs.
   </p>
  </div>
  <figure class="ltx_figure ltx_figure_panel" id="A1.F6">
   <svg class="ltx_picture ltx_figure_panel" height="806.51" id="A1.1.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,806.51) matrix(1 0 0 -1 0 0)">
     <g fill="#00A600" fill-opacity="1.0">
      <path d="M 0 5.91 L 0 800.6 C 0 803.86 2.64 806.51 5.91 806.51 L 594.09 806.51 C 597.36 806.51 600 803.86 600 800.6 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FFFFFF" fill-opacity="1.0">
      <path d="M 1.97 5.91 L 1.97 785.09 L 598.03 785.09 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 790.99)">
      <foreignobject color="#FFFFFF" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.1.pic1.1.1.1.1.1" style="width:402.3pt;">
        <span class="ltx_p" id="A1.1.pic1.1.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A1.1.pic1.1.1.1.1.1.1.1">
          API reference file
         </span>
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
      <foreignobject color="#000000" height="759.5" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.1.pic1.2.2.2.1.1" style="width:402.3pt;">
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A1.1.pic1.2.2.2.1.1.1.1">
          Slide-related APIs
         </span>
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.2">
         API: create slide(): This API creates a new slide.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.3">
         API: move to previous slide(): This API moves to the previous slide.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.4">
         API: move to next slide(): This API moves to the next slide.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.5">
         API: move to slide(slide id): This API moves to the slide with given slide id.It takes one parameter ’slide id’, the ID of the slide to move to as a integer.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.6">
         <span class="ltx_text ltx_font_bold" id="A1.1.pic1.2.2.2.1.1.6.1">
          Choose-related APIs
         </span>
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.7">
         API: choose title(): This API selects the title on the slide. You should first call choose title() before inserting text to or changing font attributes of the title.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.8">
         API: choose content(): This API select the content on the slide. You should first call choose content() before inserting text to or changing font attributes of the content.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.9">
         API: choose textbox(idx): This API selects the textbox element on the slide. It takes one parameter, the index of textbox as integer. idx is set to 0 by default, meaning the first textbox. You should first call choose textbox() before inserting text to or changing font attributes of the textbox element.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.10">
         API: choose picture(idx): This API selects the picture element on the slide. It takes one parameter, the index of textbox as integer. idx is set to 0 by default, meaning the first textbox. You should first call choose picture() before changing height, width, rotation of the picture element. You should not call choose picture() before inserting picture element.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.11">
         API: choose chart(): This API selects the chart element on the slide. You should first call choose chart() before changing the chart. You should not call choose chart() before inserting chart element.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.12">
         API: choose shape(shape name): This API selects a specific shape by shape name on the slide. It takes one parameter ’shape name’, the name of the shape to select as a string. shape name can be chosen from [’rectangle’,’right arrow’,’rounded rectangle’,’triangle’,’callout’,’cloud’,’star’,’circle’] You should first call choose shape(shape name) before you can do operations on the shape. You should not call choose shape(shape name) before inserting shape element.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.13">
         API: choose table(): This API selects the table element on the slide. You should first call choose table() before changing the table. You should not call choose table() before inserting table element.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.14">
         API: choose table cell(row id, column id): This API selects a specific cell in the table by giving row id and column id. It takes two parameters, the row id and column id of the cell to select as integers (id starts from 0). Remember the first parameter is row id, the second parameter is column id. You should first call choose table cell(row id, column id) before inserting text.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.15">
         <span class="ltx_text ltx_font_bold" id="A1.1.pic1.2.2.2.1.1.15.1">
          Basic APIs
         </span>
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.16">
         API: set background color(color): This API sets the background color of the slide. It takes one parameter ’color’, the color name to set as a string, such as ’red’, ’purple’.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.17">
         API: set width(width): This API sets the width of the selected object. It takes one parameter ’width’, the width of an object in centimeters as float. You should first choose an object before you can change the width of it.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.18">
         API: set height(height): This API sets the height of the selected object. It takes one parameter ’height’, the height of an object in centimeters as float. You should first choose an object before you can change the height of it
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.19">
         API: rotate element(angle): This API rotates the selected element by the specified angle. It takes one parameter ’angle’, the angle to rotate clockwise as integer. You should first choose an object before you can rotate it.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.20">
         API: set fill color(color): This API sets the fill color of the selected object after the object is chosen. It takes one parameter ’color’, the color name to set as a string, such as ’red’, ’purple’. You can set the fill color of content, title or textbox.
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.21">
         API: set left(left): This API moves and changes the object’s position. It sets the x position of the selected object’s leftmost point. It takes one parameter, the x position to set. You should first choose an object before you can change the left of it
        </span>
        <span class="ltx_p" id="A1.1.pic1.2.2.2.1.1.22">
         API: set top(top): This API moves and changes the object’s position. It sets the y position of the selected object’s upmost point. It takes one parameter, the y position to set. You should first choose an object before you can change the top of it.
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     Figure 6:
    </span>
    The reference API file: part 1.
   </figcaption>
  </figure>
  <figure class="ltx_figure ltx_figure_panel" id="A1.F7">
   <svg class="ltx_picture ltx_figure_panel" height="719.83" id="A1.2.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,719.83) matrix(1 0 0 -1 0 0)">
     <g fill="#00A600" fill-opacity="1.0">
      <path d="M 0 5.91 L 0 713.93 C 0 717.19 2.64 719.83 5.91 719.83 L 594.09 719.83 C 597.36 719.83 600 717.19 600 713.93 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FFFFFF" fill-opacity="1.0">
      <path d="M 1.97 5.91 L 1.97 698.41 L 598.03 698.41 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 704.32)">
      <foreignobject color="#FFFFFF" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.2.pic1.1.1.1.1.1" style="width:402.3pt;">
        <span class="ltx_p" id="A1.2.pic1.1.1.1.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A1.2.pic1.1.1.1.1.1.1.1">
          API reference file
         </span>
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
      <foreignobject color="#000000" height="672.82" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.2.pic1.2.2.2.1.1" style="width:402.3pt;">
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A1.2.pic1.2.2.2.1.1.1.1">
          Text-related APIs
         </span>
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.2">
         API: insert text(text): This API inserts text into a text frame (textbox, title, content, table).
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.3">
         API: insert bullet point(text): This API inserts a bullet point into the content. It takes one parameter, the text of the bullet point to insert as a string.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.4">
         API: insert note(text): This API inserts a note onto the slide. It takes one parameter, the note text to insert as a string.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.5">
         API: insert textbox(): This API inserts a textbox onto the slide. When you need to add a caption or text under/above/left to/right to an object, you can call insert textbox().
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.6">
         API: delete text(): This API delete the text part of an object. You should first choose content or title before you can call delete text()
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.7">
         API: set font size(font size): This API sets the size of the font It can take one argument ’font size’, the font size to set as an integer.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.8">
         API: set font color(color): This API sets the color of the font. It takes one parameter ’color’, the color name to set as a string, such as ’red’, ’purple’.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.9">
         API: set font bold(): This API sets the font to be bold.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.10">
         API: set font italic(): This API sets the font to be italic.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.11">
         API: set font underline(): This API sets the font to be underlined.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.12">
         API: set font style(font name): This API sets the font style of the selected text. It can take one argument ’font style’, the font name as a string.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.13">
         API: set line space(line space level): This API sets the line spacing of the selected text. It can take one argument ’line space level’, as an integer, default 0.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.14">
         API: text align left(): This API aligns the text to left.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.15">
         API: text align center(): This API aligns the text to center.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.16">
         API: text align right(): This API aligns the text to right.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.17">
         <span class="ltx_text ltx_font_bold" id="A1.2.pic1.2.2.2.1.1.17.1">
          Image and shape-related APIs
         </span>
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.18">
         API: insert picture(picture name): This API inserts a picture onto the slide. It takes one parameter ’picture name’, the name or description of picture as a string
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.19">
         API: insert rectangle(): This API inserts a rectangle or square shape onto the slide.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.20">
         API: insert right arrow(): This API inserts an arrow shape onto the slide.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.21">
         API: insert rounded rectangle(): This API inserts a rounded rectangle shape onto the slide.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.22">
         API: insert triangle(): This API inserts a triangle shape onto the slide.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.23">
         API: insert callout(): This API inserts a callout shape onto the slide.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.24">
         API: insert cloud(): This API inserts a cloud shape onto the slide.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.25">
         API: insert star(): This API inserts a star shape onto the current slide.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.26">
         API: insert circle(): This API inserts a circle or oval shape into the current slide.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.27">
         <span class="ltx_text ltx_font_bold" id="A1.2.pic1.2.2.2.1.1.27.1">
          Table-related APIs
         </span>
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.28">
         API: insert table(row num, col num): This API inserts a table of row num rows and col num columns onto the current slide. It takes two argument, the row number and the column number of the inserted table as integer. Remember the first parameter is row number and the second parameter is column number.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.29">
         API: insert table row(row data): This API inserts a row (list) of data into the table. It takes one argument, the data to insert as a list of numbers or strings. You should first call choose table() before you can call insert table row(). The parameter ’row data’ should be a list of strings.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.30">
         <span class="ltx_text ltx_font_bold" id="A1.2.pic1.2.2.2.1.1.30.1">
          Chart-related APIs
         </span>
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.31">
         API: insert line chart(data, series): This API inserts a line chart onto the slide. It takes two argument, ’data’ is a list of numbers and ’series’ is a list of strings.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.32">
         API: insert bar chart(data, series): This API inserts a bar chart onto the slide. It takes two argument, ’data’ is a list of numbers and ’series’ is a list of strings.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.33">
         API: insert pie chart(data, series): This API inserts a pie chart onto the slide. It takes two argument, ’data’ is a list of numbers and ’series’ is a list of strings.
        </span>
        <span class="ltx_p" id="A1.2.pic1.2.2.2.1.1.34">
         API: set chart title(title): This API sets the title of a previously inserted chart. It takes one argument ’title’, the title to be set as a string.
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     Figure 7:
    </span>
    The reference API file: part 2.
   </figcaption>
  </figure>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   The Prompt for Content Selection Algorithm
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    We put the prompt of content selection algorithm in Figure
    <a class="ltx_ref" href="#A2.F8" title="Figure 8 ‣ Appendix B The Prompt for Content Selection Algorithm ‣ PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion">
     <span class="ltx_text ltx_ref_tag">
      8
     </span>
    </a>
    .
   </p>
  </div>
  <figure class="ltx_figure ltx_figure_panel" id="A2.F8">
   <svg class="ltx_picture ltx_figure_panel" height="557.83" id="A2.1.pic1" overflow="visible" version="1.1" width="600">
    <g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,557.83) matrix(1 0 0 -1 0 0)">
     <g fill="#00A600" fill-opacity="1.0">
      <path d="M 0 5.91 L 0 551.92 C 0 555.19 2.64 557.83 5.91 557.83 L 594.09 557.83 C 597.36 557.83 600 555.19 600 551.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill="#FFFFFF" fill-opacity="1.0">
      <path d="M 1.97 5.91 L 1.97 533.72 L 598.03 533.72 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none">
      </path>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 539.62)">
      <foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.1.pic1.26.26.26.1.1" style="width:402.3pt;">
        <span class="ltx_p" id="A2.1.pic1.26.26.26.1.1.1">
         <span class="ltx_text ltx_font_bold" id="A2.1.pic1.26.26.26.1.1.1.1">
          Content Selection prompt
         </span>
        </span>
       </span>
      </foreignobject>
     </g>
     <g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)">
      <foreignobject color="#000000" height="508.13" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
       <span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25" style="width:402.3pt;">
        <span class="ltx_p" id="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">
         You are an AI assistant for PowerPoint. Your task is to determine what kind of content is necessary to fulfill the user’s instruction.
You have an API to extract the content, please call the get
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1">
          <semantics id="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a">
           <mi id="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" mathvariant="normal" xref="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b">
            <ci id="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         content api with correct parameters to fulfill the user’s instruction.
You need to extract the minimum necessary information to fulfill user’s instruction.
        </span>
        <span class="ltx_p" id="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4">
         <span class="ltx_text ltx_font_bold" id="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1">
          Get
          <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.m1.1">
           <semantics id="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.m1.1a">
            <mi id="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.m1.1.1" mathvariant="normal" xref="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.m1.1.1.cmml">
             _
            </mi>
            <annotation-xml encoding="MathML-Content" id="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.m1.1b">
             <ci id="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.m1.1.1.cmml" xref="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.m1.1.1">
              _
             </ci>
            </annotation-xml>
            <annotation encoding="application/x-tex" id="A2.1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.1.m1.1c">
             \_
            </annotation>
           </semantics>
          </math>
          content API:
         </span>
         get
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1">
          <semantics id="A2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1a">
           <mi id="A2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1" mathvariant="normal" xref="A2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1b">
            <ci id="A2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1.cmml" xref="A2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.3.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         content(need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1">
          <semantics id="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1a">
           <mi id="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1.1" mathvariant="normal" xref="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1b">
            <ci id="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1.1.cmml" xref="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.4.m2.1c">
            \_
           </annotation>
          </semantics>
         </math>
         text: Indicates whether text information is required. The text information encompasses text in title, content, textbox, table, chart, and shape. This parameter is particularly useful when inserting or modifying text of title, content, textbox, table, chart, and shape, or when information about these objects is essential.
        </span>
        <span class="ltx_p" id="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1">
          <semantics id="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1a">
           <mi id="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1" mathvariant="normal" xref="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1b">
            <ci id="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1.cmml" xref="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.5.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         style: Indicates whether style information is required. Style information includes attributes like font type, font size, color, background color, line space, bold, undeline, italic and other visual aspects of objects like rotation. This is useful when changing the appearance of text or objects or when information about an object’s appearance is essential.
        </span>
        <span class="ltx_p" id="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1">
          <semantics id="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1a">
           <mi id="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1" mathvariant="normal" xref="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1b">
            <ci id="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1.cmml" xref="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.6.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         position: Indicates whether position information is required. The position details encompass an object’s height, width, and its left and top positions. This is crucial when moving objects or altering an object’s size.
        </span>
        <span class="ltx_p" id="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1">
          <semantics id="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1a">
           <mi id="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1" mathvariant="normal" xref="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1b">
            <ci id="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1.cmml" xref="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.7.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         title: Determines if information related to the title is required.
        </span>
        <span class="ltx_p" id="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1">
          <semantics id="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1a">
           <mi id="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1" mathvariant="normal" xref="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1b">
            <ci id="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1.cmml" xref="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.8.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         content: Determines if information related to the content is required.
        </span>
        <span class="ltx_p" id="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1">
          <semantics id="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1a">
           <mi id="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1" mathvariant="normal" xref="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1b">
            <ci id="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1.cmml" xref="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.9.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         picture: Determines if information related to the picture is required.
        </span>
        <span class="ltx_p" id="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1">
          <semantics id="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1a">
           <mi id="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1" mathvariant="normal" xref="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1b">
            <ci id="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1.cmml" xref="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.10.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         table: Determines if information related to the table is required.
        </span>
        <span class="ltx_p" id="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1">
          <semantics id="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1a">
           <mi id="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1.1" mathvariant="normal" xref="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1b">
            <ci id="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1.1.cmml" xref="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.11.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         chart: Determines if information related to the chart is required.
        </span>
        <span class="ltx_p" id="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m1.1">
          <semantics id="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m1.1a">
           <mi id="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m1.1.1" mathvariant="normal" xref="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m1.1b">
            <ci id="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m1.1.1.cmml" xref="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.12.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         textbox: Determines if information related to the textbox is required.
        </span>
        <span class="ltx_p" id="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1">
          <semantics id="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1a">
           <mi id="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1.1" mathvariant="normal" xref="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1b">
            <ci id="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1.1.cmml" xref="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.13.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         shape: Determines if information related to the shapes (rectangle, right arrow, rounded rectangle, triangle, callout, cloud, star, circle) is required.
)
        </span>
        <span class="ltx_p" id="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14">
         Where the parameters are either 1 (needed) or 0 (not needed).
You should only answer with calling get
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m1.1">
          <semantics id="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m1.1a">
           <mi id="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m1.1.1" mathvariant="normal" xref="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m1.1b">
            <ci id="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m1.1.1.cmml" xref="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.14.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         content() with the right parameters.
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.26">
         <span class="ltx_text ltx_font_bold" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.26.1">
          For examples:
         </span>
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.27">
         Instruction:
Increase the font size of the content to 20.
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.28">
         Explanation:
For information, style information (font size) is needed.
For objects, content is needed.
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.29">
         Answer:
        </span>
        <span class="ltx_p" id="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18">
         get
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.m1.1">
          <semantics id="A2.1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.m1.1a">
           <mi id="A2.1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.m1.1.1" mathvariant="normal" xref="A2.1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.m1.1b">
            <ci id="A2.1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.m1.1.1.cmml" xref="A2.1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.15.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         content(need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.m2.1">
          <semantics id="A2.1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.m2.1a">
           <mi id="A2.1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.m2.1.1" mathvariant="normal" xref="A2.1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.m2.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.m2.1b">
            <ci id="A2.1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.m2.1.1.cmml" xref="A2.1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.m2.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.16.m2.1c">
            \_
           </annotation>
          </semantics>
         </math>
         text=1,need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.m3.1">
          <semantics id="A2.1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.m3.1a">
           <mi id="A2.1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.m3.1.1" mathvariant="normal" xref="A2.1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.m3.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.m3.1b">
            <ci id="A2.1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.m3.1.1.cmml" xref="A2.1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.m3.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.17.m3.1c">
            \_
           </annotation>
          </semantics>
         </math>
         style=1,need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.m4.1">
          <semantics id="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.m4.1a">
           <mi id="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.m4.1.1" mathvariant="normal" xref="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.m4.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.m4.1b">
            <ci id="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.m4.1.1.cmml" xref="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.m4.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.18.m4.1c">
            \_
           </annotation>
          </semantics>
         </math>
         position=0,
        </span>
        <span class="ltx_p" id="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22">
         need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.m1.1">
          <semantics id="A2.1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.m1.1a">
           <mi id="A2.1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.m1.1.1" mathvariant="normal" xref="A2.1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.m1.1b">
            <ci id="A2.1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.m1.1.1.cmml" xref="A2.1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.19.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         title=0,need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.m2.1">
          <semantics id="A2.1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.m2.1a">
           <mi id="A2.1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.m2.1.1" mathvariant="normal" xref="A2.1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.m2.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.m2.1b">
            <ci id="A2.1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.m2.1.1.cmml" xref="A2.1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.m2.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.20.m2.1c">
            \_
           </annotation>
          </semantics>
         </math>
         content=1,need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.m3.1">
          <semantics id="A2.1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.m3.1a">
           <mi id="A2.1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.m3.1.1" mathvariant="normal" xref="A2.1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.m3.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.m3.1b">
            <ci id="A2.1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.m3.1.1.cmml" xref="A2.1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.m3.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.21.m3.1c">
            \_
           </annotation>
          </semantics>
         </math>
         picture=0,need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.m4.1">
          <semantics id="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.m4.1a">
           <mi id="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.m4.1.1" mathvariant="normal" xref="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.m4.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.m4.1b">
            <ci id="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.m4.1.1.cmml" xref="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.m4.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.22.m4.1c">
            \_
           </annotation>
          </semantics>
         </math>
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25">
         table=0,need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.m1.1">
          <semantics id="A2.1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.m1.1a">
           <mi id="A2.1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.m1.1.1" mathvariant="normal" xref="A2.1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.m1.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.m1.1b">
            <ci id="A2.1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.m1.1.1.cmml" xref="A2.1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.m1.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.23.m1.1c">
            \_
           </annotation>
          </semantics>
         </math>
         chart=0,need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.m2.1">
          <semantics id="A2.1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.m2.1a">
           <mi id="A2.1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.m2.1.1" mathvariant="normal" xref="A2.1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.m2.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.m2.1b">
            <ci id="A2.1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.m2.1.1.cmml" xref="A2.1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.m2.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.24.m2.1c">
            \_
           </annotation>
          </semantics>
         </math>
         textbox=0,need
         <math alttext="\_" class="ltx_Math" display="inline" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.m3.1">
          <semantics id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.m3.1a">
           <mi id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.m3.1.1" mathvariant="normal" xref="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.m3.1.1.cmml">
            _
           </mi>
           <annotation-xml encoding="MathML-Content" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.m3.1b">
            <ci id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.m3.1.1.cmml" xref="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.m3.1.1">
             _
            </ci>
           </annotation-xml>
           <annotation encoding="application/x-tex" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.m3.1c">
            \_
           </annotation>
          </semantics>
         </math>
         shape=0)
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.30">
         <span class="ltx_text ltx_font_bold" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.30.1">
          …
         </span>
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.31">
         <span class="ltx_text ltx_font_bold" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.31.1">
          Given the instruction, output the Answer without Explanation:
         </span>
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.32">
         Instruction:
&lt;Current user instruction&gt;
        </span>
        <span class="ltx_p" id="A2.1.pic1.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.25.33">
         Answer:
        </span>
       </span>
      </foreignobject>
     </g>
    </g>
   </svg>
   <figcaption class="ltx_caption">
    <span class="ltx_tag ltx_tag_figure">
     Figure 8:
    </span>
    The prompt of the content selection algorithm.
   </figcaption>
  </figure>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
</article>
