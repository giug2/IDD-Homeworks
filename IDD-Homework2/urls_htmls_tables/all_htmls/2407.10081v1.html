<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era</title>
<!--Generated on Sun Jul 14 04:52:08 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Recommender Systems,  Large Language Models" lang="en" name="keywords"/>
<base href="/html/2407.10081v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S1" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S2" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Conventional List-wise Recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS1" title="In 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Stages in list-wise recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS1.SSS1" title="In 3.1. Stages in list-wise recommendation ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Recall Stage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS1.SSS2" title="In 3.1. Stages in list-wise recommendation ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Ranking Stage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS1.SSS3" title="In 3.1. Stages in list-wise recommendation ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Reranking Stage</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS2" title="In 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Major challenges in list-wise recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS2.SSS1" title="In 3.2. Major challenges in list-wise recommendation ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Feature Interaction in Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS2.SSS2" title="In 3.2. Major challenges in list-wise recommendation ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>User Behavior Modeling in Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS2.SSS3" title="In 3.2. Major challenges in list-wise recommendation ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Multi-task and Multi-scenario learning in Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS3" title="In 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>LLM-enhanced Recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4.SS1" title="In 4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>LLMs for feature engineering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4.SS1.SSS1" title="In 4.1. LLMs for feature engineering ‣ 4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>LLMs for feature generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4.SS1.SSS2" title="In 4.1. LLMs for feature engineering ‣ 4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>LLMs for feature encoder</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4.SS2" title="In 4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>LLMs for ranking</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4.SS2.SSS1" title="In 4.2. LLMs for ranking ‣ 4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Scoring-based LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4.SS2.SSS2" title="In 4.2. LLMs for ranking ‣ 4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Generation-based LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4.SS3" title="In 4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conversational Recommender Systems Before LLM Era</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.SS1" title="In 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Questions Asking</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.SS1.SSS1" title="In 5.1. Questions Asking ‣ 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Item-level</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.SS1.SSS2" title="In 5.1. Questions Asking ‣ 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Attribute-level</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.SS2" title="In 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Maintain Conversations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.SS2.SSS1" title="In 5.2. Maintain Conversations ‣ 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>When to ask &amp; recommend</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.SS2.SSS2" title="In 5.2. Maintain Conversations ‣ 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Leading the conversation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.SS3" title="In 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Evaluation of CRS</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.SS4" title="In 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conversational Recommender Systems in LLM Era</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS1" title="In 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Prompt Engineering Specialized for CRS</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS1.SSS1" title="In 6.1. Prompt Engineering Specialized for CRS ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Prompting-based LLM-CRS Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS1.SSS2" title="In 6.1. Prompt Engineering Specialized for CRS ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Prompting-based LLM-CRS Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS2" title="In 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Fine-tuning Specialized for CRS</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS2.SSS1" title="In 6.2. Fine-tuning Specialized for CRS ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Data Acquisition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS2.SSS2" title="In 6.2. Fine-tuning Specialized for CRS ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Model Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS2.SSS3" title="In 6.2. Fine-tuning Specialized for CRS ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.3 </span>Fine-tuning Strategy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS3" title="In 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS3.SSS1" title="In 6.3. Discussion ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.1 </span>Advantages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.SS3.SSS2" title="In 6.3. Discussion ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3.2 </span>Limitations</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>LLM-powered Recommendation Agent</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7.SS1" title="In 7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Architecture of LLM-Powered Recommendation Agent</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7.SS2" title="In 7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Advances in LLM-powered Recommendation Agents</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7.SS2.SSS0.Px1" title="In 7.2. Advances in LLM-powered Recommendation Agents ‣ 7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title">Single-Agent for Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7.SS2.SSS0.Px2" title="In 7.2. Advances in LLM-powered Recommendation Agents ‣ 7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title">Multi-Agents for Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7.SS3" title="In 7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Development Levels</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7.SS4" title="In 7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S8" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Open problems and Future directions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S9" title="In All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bo Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id5.2.id2">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chenbo116@huawei.com">chenbo116@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinyi Dai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id7.2.id2">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:daixinyi3@huawei.com">daixinyi3@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Huifeng Guo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id8.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id9.2.id2">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:huifeng.guo@huawei.com">huifeng.guo@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Guo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id11.2.id2">Singapore</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:guowei67@huawei.com">guowei67@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weiwen Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id12.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id13.2.id2">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:liuweiwen8@huawei.com">liuweiwen8@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yong Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id14.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id15.2.id2">Singapore</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:liu.yong6@huawei.com">liu.yong6@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiarui Qin
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id17.2.id2">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:qinjiarui3@huawei.com">qinjiarui3@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruiming Tang<sup class="ltx_sup" id="id18.2.id1">†</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.3.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id20.4.id2">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tangruiming@huawei.com">tangruiming@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yichao Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id21.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id22.2.id2">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wangyichao5@huawei.com">wangyichao5@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chuhan Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id23.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id24.2.id2">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wuchuhan1@huawei.com">wuchuhan1@huawei.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yaxiong Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id25.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id26.2.id2">Singapore</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:yx.wu@huawei.com">yx.wu@huawei.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id27.1.id1">Noah’s Ark Lab, Huawei</span><span class="ltx_text ltx_affiliation_country" id="id28.2.id2">Singapore</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhang.hao3@huawei.com">zhang.hao3@huawei.com</a>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id29.id1">Recommender systems (RS) are vital for managing information overload and delivering personalized content, responding to users’ diverse information needs.
The emergence of large language models (LLMs) offers a new horizon for redefining recommender systems with vast general knowledge and reasoning capabilities.
Standing across this LLM era, we aim to integrate recommender systems into a broader picture, and pave the way for more comprehensive solutions for future research.
Therefore, we first offer a comprehensive overview of the technical progression of recommender systems, particularly focusing on language foundation models and their applications in recommendation.
We identify two evolution paths of modern recommender systems—via list-wise recommendation and conversational recommendation.
These two paths finally converge at LLM agents with superior capabilities of long-term memory, reflection, and tool intelligence.
Along these two paths, we point out that the information effectiveness of the recommendation is increased, while the user’s acquisition cost is decreased.
Technical features, research methodologies, and inherent challenges for each milestone along the path are carefully investigated—from traditional list-wise recommendation to LLM-enhanced recommendation to recommendation with LLM agents.
Finally, we highlight several unresolved challenges crucial for the development of future personalization technologies and interfaces and discuss the future prospects.</p>
</div>
<div class="ltx_keywords">Recommender Systems, Large Language Models
</div>
<div class="ltx_keywords">Recommender Systems, Large Language Models
</div>
<div class="ltx_acknowledgements">
<sup class="ltx_sup" id="id30.id1">∗</sup>Authors are listed in alphabetical order.
</div>
<div class="ltx_acknowledgements">
<sup class="ltx_sup" id="id31.id1">†</sup>Ruiming Tang is the corresponding author.
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Information systems Recommender systems</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">We are facing an unprecedented era of information explosion, that profoundly affects our perception, cognition, and actions in both virtual and physical worlds <cite class="ltx_cite ltx_citemacro_citep">(Bawden and Robinson, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib6" title="">2020</a>)</cite>.
Thus, picking helpful and reliable information with acceptable effort is important for humans and the society <cite class="ltx_cite ltx_citemacro_citep">(Bobadilla et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib7" title="">2013</a>)</cite>.
Over the past decades, researchers and practitioners have made tremendous efforts in building information filtering systems (a.k.a, recommender systems) to accommodate individual’s needs by choosing relevant information pieces from a potentially overwhelming number of candidates <cite class="ltx_cite ltx_citemacro_citep">(Koren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib61" title="">2009</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib37" title="">2017</a>)</cite>.
Accompanied by the rise or decline of the World Wide Web, the development of recommender systems empowers a number of great products and companies, such as YouTube, Facebook, and TikTok <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib21" title="">2016</a>; Shapira et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib116" title="">2013</a>; Wang, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib139" title="">2022</a>)</cite>.
As an underlying technique, modern recommender systems are de facto shaping and leading our views and decisions through the penetration of these personalized applications <cite class="ltx_cite ltx_citemacro_citep">(Ko et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib60" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">An ultimate goal of recommender systems is fully understanding users and satisfying their needs by providing accurate and effective information under minimal user effort in the loop of human-system interactions <cite class="ltx_cite ltx_citemacro_citep">(Ko et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib60" title="">2022</a>)</cite>.
A common form of recommendation services is displaying candidate items such as products and news on lists or streaming layouts for information delivery and feedback collection.
To support this paradigm of applications, recommender systems generally need to generate a ranked list of items based on their relevance to user preference inferred from various genres of user feedback, such as clicks, comments, shares, and consumptions.
Different from conventional search engines, recommender systems free users from writing search queries to explicitly explain their intent and can discover their potential interest by analyzing the latent patterns behind their behaviors <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib142" title="">2021</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib37" title="">2017</a>; Covington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib21" title="">2016</a>)</cite>.
Within minor efforts of the clicks made by our fingers, recommender systems can roughly understand our preferences and mitigate our burden in information seeking.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Nonetheless, simple user feedback like clicks is far from reflecting the complexity of human intention <cite class="ltx_cite ltx_citemacro_citep">(Jannach et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib50" title="">2021</a>)</cite>.
In fact, language is a more natural and powerful information carrier to express our requirements, opinions, feelings, and experiences <cite class="ltx_cite ltx_citemacro_citep">(Sun and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib126" title="">2018</a>)</cite>.
Thus, conversational recommender systems that can receive and understand natural language instructions have the potential to capture user interest signals that cannot be evoked by other user feedback <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib31" title="">2021</a>)</cite>.
During interactive and engaging conversations to guide the recommender systems, they are expected to find items that exactly match user intention and even provide complementary information through natural language explanations <cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib20" title="">2016</a>)</cite>.
However, writing textual messages in multi-round conversations significantly increases the effort of users in seeking desired information, though the final results may be more accurate and effective than the guess of non-conversational recommender systems.
In addition, how to fully understand and generate natural languages during interactions poses huge challenges to recommender systems.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Fortunately, the task of understanding and applying natural languages is not so daunting in recent years, owing to the emergence of large language models (LLMs).
By self-supervised learning on the huge unlabeled corpus, LLMs can obtain rich general knowledge about the physical world and human society through the lens of human languages.
Equipped by such knowledge, LLMs bring new opportunities to various fields in recommendation research, including user modeling, item understanding, result explanation, conversation generation, and even pipeline coordination <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib80" title="">2023a</a>)</cite>.
These application forms of LLMs can empower recommender systems in different aspects, aiming to optimize the accuracy of information delivery and explore new forms of interactions between users and systems <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib33" title="">2023</a>)</cite>.
Due to the human-like thought mode of LLMs that can quickly understand user motivation through world knowledge and commonsense reasoning, LLM-empowered conversational recommender systems have the potential to assist users without verbose and rigid chats <cite class="ltx_cite ltx_citemacro_citep">(Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib30" title="">2023</a>)</cite>.
In the flooding era of LLMs, how to introduce, embrace, and enjoy LLMs has triggered an emerging revolution of fundamental technical and business paradigms of personalized recommendation <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib26" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Standing at the crossroads of this technical reform, there have been several surveys that retrospect some methodologies and taxonomies of LLM-empowered recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib85" title="">2023f</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib152" title="">2023b</a>; Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib26" title="">2023</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib80" title="">2023a</a>)</cite>.
However, most of them focus on utilizing LLMs to enhance or replace the modules in conventional recommender systems in list- or stream-based information delivery, while the crucial trend of conversational recommendation led by LLMs is overlooked.
Moreover, there lacks a big picture of the technical position of LLMs in recommender systems and the paradigm shift of recommendation in the dawn of artificial general intelligence.
Thus, a timely overview and outlook of the connections between LLMs and recommender systems can bring useful insights to the design and implementation of next-generation recommendation applications.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this article, we present a panoramagram of the technical evolution of recommender systems in the context of language foundation models and their applications.
We discover two paths in the paradigm evolution of modern recommender systems in improving the effectiveness of information acquirement, and their trajectories converge at the same point that can maximally elicit and exploit the potential of LLMs (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S2.F1" title="Figure 1 ‣ 2. Overview ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">1</span></a>).
We systematically review the milestones on both paths in the aspects of their technical characteristics, research practices, and scientific limitations.
Furthermore, we analyze the possible forms of LLM applications in recommender systems and raise several open problems in this area that are core to the design of future personalization products and interaction interfaces.
Our survey is expected to stimulate discussions on the future technical and commercial shapes of recommender systems in the coming era of super artificial intelligence.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The rest of this paper is organized as follows. In Section 2, we will give a brief overview of the development path of recommender systems and introduce five different types of recommendation techniques, including list-wise recommendation in the deep learning age, LLM-enhanced list-wise recommendation, conversational recommendation before the LLM era, conversational recommendation in LLM era, and LLM-powered recommendation agents. In Sections 3-7, we will elaborate on each technique in detail. We then propose several open problems and raise potential future directions in Section 8. Finally, we conclude the survey in Section 9.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Overview</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="392" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Development trajectory of the modern recommender systems.
The <math alttext="x" class="ltx_Math" display="inline" id="S2.F1.3.m1.1"><semantics id="S2.F1.3.m1.1b"><mi id="S2.F1.3.m1.1.1" xref="S2.F1.3.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.F1.3.m1.1c"><ci id="S2.F1.3.m1.1.1.cmml" xref="S2.F1.3.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.3.m1.1d">x</annotation><annotation encoding="application/x-llamapun" id="S2.F1.3.m1.1e">italic_x</annotation></semantics></math>-axis denotes key technical milestones in artificial intelligence, while the <math alttext="y" class="ltx_Math" display="inline" id="S2.F1.4.m2.1"><semantics id="S2.F1.4.m2.1b"><mi id="S2.F1.4.m2.1.1" xref="S2.F1.4.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.F1.4.m2.1c"><ci id="S2.F1.4.m2.1.1.cmml" xref="S2.F1.4.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.m2.1d">y</annotation><annotation encoding="application/x-llamapun" id="S2.F1.4.m2.1e">italic_y</annotation></semantics></math>-axis represents various interaction types, leading to five different recommendation paradigms (marked by different colors).
Arrows present the general paradigm shift: recommender systems evolve from the lower left corner to the upper right corner, with a decrease in interactive cost and an increase in effective information.
</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Throughout the history of the Internet, we have experienced distinct feelings when facing information explosion at different ages, i.e., from panic and overwhelming to enjoying and immersing.
Accompanied by the ever-deepening connection between the virtual and physical worlds, acquiring and digesting proper information becomes a fundamental requirement and right of people in social life.
Thanks to the rapid development of information-seeking and filtering technologies such as recommender systems, we can easily touch and affect the information we are concerned about, no longer drowning in the flood of information.
By living in the world through the assistance of personalized recommendations, we are characterizing the web and also characterized by the web unconsciously.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Enlightened by the spark of general intelligence typified by GPT-4 and Sora in this age <cite class="ltx_cite ltx_citemacro_citep">(Bubeck et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib9" title="">2023</a>)</cite>, new technical and business orders have been brought to the world.
We have unprecedented opportunities to turn massive information into new knowledge and even wisdom by exploiting the universal and human-like abilities of large foundation models.
Different from deterministic recommender systems in the early times, these powerful models not only perceive the world but also create new ideas and content.
Thus, the collision between two technique forms is igniting new sparks that can draw forth novel discipline paradigms and business models in the personalization, media, and information management areas.
However, both academia and industry are still struggling to figure out the correct direction of this revolution.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_italic" id="S2.p3.1.1">“The farther back we can look, the farther forward we are likely to see.”</span>
By carefully reviewing the passing history of recommender systems in this article, we analyze the underlying inherent impetus behind their technical development trajectory.
From thirty years ago to now, the general goal of recommender systems, i.e., finding and providing useful and relevant information to users, still remains unchanged.
Given limited time and space, recommender systems are pursuing increasing the amount of effective information that can really attract and help users by optimizing data, algorithms, and interaction interfaces.
From the view of users, we are hoping to be satisfied with minimal effort due to the nature of humans, though we may tolerate the cost if the information is sufficiently valuable.
This naturally forms a dilemma between information effectiveness and acquisition cost: recommender systems need more feedback from users to calibrate and improve their mechanisms, but users often mean to provide feedback and educate the systems if their requirements are not accommodated <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib39" title="">2016</a>)</cite>.
This dilemma actually implies two essential directions to optimizing recommender systems, i.e., <span class="ltx_text ltx_font_bold" id="S2.p3.1.2">improving the amount of effective information</span> by making more accurate and helpful recommendations, and <span class="ltx_text ltx_font_bold" id="S2.p3.1.3">decreasing the user interaction cost</span> by freeing users from frequent and time-consuming operations.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">In fact, we find the evolution of recommendation techniques tacitly follows the two philosophies, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S2.F1" title="Figure 1 ‣ 2. Overview ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">1</span></a>.
Transferring from the lower left to the upper right, there are two paths that change the technical forms of recommender systems in different aspects.
The upper path mainly enhances effective information by introducing additional feedback such as natural languages, but sacrifices the interaction efficiency due to the low convenience of text inputs compared to simple clicks.
The lower path focuses on raising effective information by exploiting the potential of large language models to facilitate and automate different modules and stages in recommendation.
Both paths converge at the same point, i.e., converting recommender systems into personalized agents.
Driven by the perception and reasoning abilities provided by large language models, intelligent recommendation agents can fully understand users and actively interact with the world to acquire and aggregate rich knowledge without extensive user instructions and guidance.
The roads to this ideal form are difficult and roundabout due to the temporary sacrifice of user experience (upper path) and the huge paradigm leap between conventional list-oriented recommender systems and conversation-dominated recommendation agents (lower path).
This long journey is remarked by a series of milestones and breakthroughs, tackling the numerous challenges in approaching and breaking the limitation of personalized recommendation.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Over the past decade, recommender systems have greatly enjoyed the bloom of deep learning and achieved huge business success <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib174" title="">2021</a>)</cite>.
The universal approximation property of neural networks makes it possible to train neural models to mine complex interest patterns from user behaviors and model their relevance to candidate items <cite class="ltx_cite ltx_citemacro_citep">(Cybenko, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib22" title="">1989</a>)</cite>.
Numerous neural recommendation methods have been developed to address diverse recommendation tasks and have found extensive application in industrial products.
As shown in area A of Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S2.F1" title="Figure 1 ‣ 2. Overview ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">1</span></a>, notable contributions such as DCN <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib142" title="">2021</a>)</cite>, DeepFM <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib37" title="">2017</a>)</cite>, and DIN <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib184" title="">2018</a>)</cite> facilitate the success of many online customer-centric enterprises.
These models usually aim to capture the collaborative signals encoded by user feedback like clicks or ratings, which can be sparse and noisy.
Without further feedback and external knowledge, it is difficult for recommender systems to understand the exact user intents in many scenarios.
Thus, the intrinsic limitations of neural recommenders in this generation stimulate the paradigm shift toward two directions.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">An intuitive way to improve the effectiveness of recommender systems is incorporating richer-information feedback such as natural languages (area B).
By parsing and processing user instructions in a multi-turn conversation, recommender systems can better understand the real requirements of users, and make accurate recommendations by filling dialog templates <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib181" title="">2013</a>; Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib20" title="">2016</a>)</cite> or generating language responses <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib113" title="">2023</a>)</cite>.
Moreover, users can actively guide the system to adjust the recommendation results through iterative queries and feedback until they are satisfied <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib31" title="">2021</a>)</cite>.
Conversational recommendation enables users to interact with machines like human-human chatting, thereby providing a novel and engaging experience for its customers.
However, the input of textual messages significantly increases the effort of users to complete a round of information acquisition, which is against the product design principles of many online applications on swift and convenient interaction.</p>
</div>
<div class="ltx_para" id="S2.p7">
<p class="ltx_p" id="S2.p7.1">The emergence of large language models (LLMs) opens another direction to improve recommender systems by boosting effective information (area C).
By exploiting the universal language understanding and generation capabilities of LLMs, many major modules and stages in practical recommender systems can be benefited and even replaced.
For example, LLM can enhance the user/item representations and generate auxiliary textual features <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib154" title="">2023b</a>; Torbati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib131" title="">2023</a>)</cite>.
In addition, the rich world knowledge condensed by LLMs is complementary to conventional recommender systems in comprehending users and items, and unifying their knowledge can collaboratively their capabilities in making recommendations <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib69" title="">2023a</a>)</cite>.
Since the interaction interface of LLM-assisted recommenders usually remains unchanged, users can enjoy more targeted and accurate recommendation services without any additional effort.
However, the limited information disclosed by simple user feedback is still a barrier that hinders machines from truly understanding user interest.</p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1">With the human alignment process of LLMs, they are usually specialized in generating natural language conversations.
Different from traditional dialogue expert systems and small generative models, LLMs not only master strong language skills but also better comprehend complicated contexts and capture user interest based on their commonsense knowledge.
Therefore, they naturally become the chatting engine in conversational recommendations to aggregate and explain the results.
Furthermore, equipped with external tools such as databases, search engines, and recommendation models, LLMs can touch and fuse heterogeneous knowledge and incorporate it into recommendations.
By intelligently understanding user intent, digesting multi-source information, and providing convincing explanations, LLMs can not only improve recommendation accuracy and informativeness but also accommodate user requirements with fewer inquiries to reduce the interaction cost <cite class="ltx_cite ltx_citemacro_citep">(Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib30" title="">2023</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib91" title="">2023a</a>)</cite>.
Nonetheless, the accuracy of many LLM-centric conversational recommenders is restricted by the capabilities of available tools, such as the accuracy of integrated recommendation models.
In addition, many high-level skills including complex planning, reasoning, and action execution are difficult to incorporate, which are essential to many genres of information acquisition and processing tasks in personalized recommendations.</p>
</div>
<div class="ltx_para" id="S2.p9">
<p class="ltx_p" id="S2.p9.1">In the two paradigms mentioned above, recommender systems evolve in two distinct ways to provide more effective information in user-machine interactions.
In LLM-centric conversational recommendations, LLM acts as a coordinator to arrange the execution of recommendation tasks and command various external tools including conventional recommendation models.
By contrast, in LLM-empowered systems for list-based recommendation, LLMs play auxiliary roles and are utilized in certain processes such as feature engineering and item ranking <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib154" title="">2023b</a>; Torbati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib131" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib65" title="">2023c</a>)</cite>, which enables partial automation of recommendation processes to lessen the need for human intervention.
Both paradigms come to the same destination that converts recommender systems into recommender agents.
With the ultra-strong capabilities of LLM agents in terms of long-term memory, reflection, and tool intelligence, personalized assistants can become indispensable intimates that provide users with thoughtful and tailored services.
Without wordy conversations and mechanical interactions, humans and personal agents can reach an unvoiced pact and finally build harmony and solid trust.
By scrutinizing the recent history of recommender systems, the outline of their future shape is becoming clear.</p>
</div>
<div class="ltx_para" id="S2.p10">
<p class="ltx_p" id="S2.p10.1">In the following sections, we will walk along the “all roads lead to Rome” and draw lessons from the revolutionary history of personalized recommendation in detail.
Starting from the conventional list-wise recommendation in the deep learning age in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3" title="3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">3</span></a>, we then introduce LLM-enhanced list-wise recommendation in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4" title="4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">4</span></a>, followed by the conversational recommendation before and in the LLM era in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5" title="5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">5</span></a> and Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6" title="6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">6</span></a>, respectively.
Finally, the LLM-powered recommendation agents are described in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7" title="7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Conventional List-wise Recommendation</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="645" id="S3.F2.g1" src="x2.png" width="956"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Example of a conventional list-wise recommendation system.
</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Recommendation systems in the past decades benefited from the bloom of deep learning and usually presented the recommendation results to users with a ranked list.
The representative commercial products include YouTube, Facebook, TikTok, etc.
These recommendation systems generate a ranked list of items based on their users’ preferences.
The preferences are inferred with well-trained deep-learning models exploiting the user feedback, such as clicks and downloads, from the interaction logs generated over these products.
For example, Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.F2" title="Figure 2 ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">2</span></a> presents a typical process of conventional list-wise music recommender systems.
Recommendation lists for singers or songs are generated according to the user features (e.g., the user profile), the item feature (e.g., song title, singer), and the user’s past behaviors (e.g., clicked songs, liked songs).</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In recommender systems, the number of candidate items can be millions or even billions.
To balance the effectiveness and efficiency, <span class="ltx_text ltx_font_bold" id="S3.p2.1.1">a multi-stage cascade ranking system</span> is usually conducted.
Specifically, the ranking system is divided into mainly three stages including <span class="ltx_text ltx_font_italic" id="S3.p2.1.2">recall, ranking, and reranking</span>, taking into consideration efficiency and diversity requirements <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib106" title="">2022</a>)</cite>.
Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS1" title="3.1. Stages in list-wise recommendation ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">3.1</span></a> introduces related studies for different recommendation stages.
Moreover, to exploit the complex and heterogeneous user feedback, the rankers in each stage encounter many challenges such as <span class="ltx_text ltx_font_italic" id="S3.p2.1.3">feature interaction, user behavior modeling, multi-task and multi-scenario learning</span>. Recent work focusing on these challenges will be discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3.SS2" title="3.2. Major challenges in list-wise recommendation ‣ 3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Stages in list-wise recommendation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To build a large-scale real-world recommender system capable of finding a list of items out of millions to billions of available options, a multi-stage system is deployed for ranking items efficiently, which usually contains recall, ranking, and reranking.
The recall stage seeks to retrieve a small set of candidate items users may be interested in from millions or even billions of available items.
After the recall stage, candidate items from multiple channels are fused and sorted by the ranking model according to the estimated scores to select dozens of items.
To meet certain requirements and further improve the recommendation results, a reranking model is applied to adjust the ranking of the item list according to specific metrics such as diversity and fairness.
Through these cascaded stages, the number of candidates is reduced to tens, which are finally exposed to users.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Recall Stage</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">To support the multiple interests of users and the diverse content of the platform, the traditional recall structure generally contains multiple sources, which can be classified into non-personalized recall and personalized recall.
Non-personalized recall algorithms don’t exploit the information about a specific user, but utilize general information like item popularity, operating policy, city, and request time for recommendation.
Typical non-personalized recall algorithms include <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.1">hot item recall</span> based on the item popularity, <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.2">new item recall</span> based on the operating policy, and <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS1.p1.1.3">high accuracy recall</span> based on the click rate.
Without considering the diverse characteristics of different users, these non-personalized methods can hardly capture the users’ unique needs and real-time preferences.
To pursue customized recall results based on users’ different profiles and historical behaviors, many personalized recall algorithms like collaborative-based methods and embedding-based methods are proposed.
YoutubeDNN <cite class="ltx_cite ltx_citemacro_citep">(Covington et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib21" title="">2016</a>)</cite> uses deep neural networks with categorical features and average pooled user behavior sequential features to learn informative user representation for relevant item recall.
Bert4Rec <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib125" title="">2019</a>)</cite> improves YoutubeDNN by adopting a transformer layer to learn the complicit item dependencies and interest evolution.
As a powerful tool for learning user-item relations, GNN-based methods are widely adopted to the recall stage.
Pinsage <cite class="ltx_cite ltx_citemacro_citep">(Ying et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib166" title="">2018</a>)</cite> constructs a pin-board bipartite graph, then uses the graph convolutional networks to mine the collaborative signals and learn high-quality representations.
RippleNet <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib136" title="">2018b</a>)</cite> introduces an auxiliary knowledge graph to complement additional relations to improve the recommendation.
To achieve faster computational efficiency, contextual features and cross features are often discarded, which greatly limits the effect of the recall model.
As a result, we usually design a complex ranking model with numerous hand-crafted features to enhance the final result.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Ranking Stage</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">The recall stage provides a preliminary filtering of items, which are delivered to the ranking models for a fine-grained selection based on users’ preferences.
Since there are usually dozens of items to be ranked, the algorithms in this stage are typically much more complicated combined with rich features for precise user interest mining and item relevance prediction.
To achieve this goal, click-through rate (CTR) prediction and post-click conversion rate (CVR) prediction are the two most critical tasks for personalized ranking systems, which estimate the probability of a user click or a conversion over a given item.
Then these items are sorted by the combination of CTR, CVR, and item bidding price, where the combination strategies are often different in different platforms.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">The ranking models are typically trained using historical exposure logs, following an <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS2.p2.1.1">Embedding &amp; Representation learning &amp; Prediction paradigm</span>.
They firstly transform the input features (including categorical features, sequential features, numerical features, and combinational features) into embedding vectors, then use an advanced network structure to learn high-level feature representation via modeling feature interaction or mining user behavior, finally fed into fully connected layers to get the prediction score.
Since a single model is usually applied in this stage, many ingenious network structures are proposed, and different companies may have different base models with customized optimization to their different application scenarios.
Representative works include DeepFM <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib37" title="">2017</a>)</cite> proposed by Huawei for CTR prediction, DCN <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib141" title="">2017</a>)</cite> proposed by Google for CTR prediction, DIN<cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib184" title="">2018</a>)</cite> proposed by Alibaba for CTR prediction, and ESMM<cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib97" title="">2018a</a>)</cite> proposed by Alibaba for CVR prediction.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Reranking Stage</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">The reranking stage plays a crucial role as the last controller of the recommendation list, it amends the final recommendation results by considering the item relationships and list-wise context <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib90" title="">2022</a>)</cite>.
Early works usually optimize the single accuracy objective, where many deep neural network (DNN) based models have been proposed, such as multi-layer perceptron (MLP) based <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib55" title="">2018</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib87" title="">2021</a>)</cite>, recurrent neural
networks (RNN) based <cite class="ltx_cite ltx_citemacro_citep">(Ai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib3" title="">2018</a>; Zhuang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib189" title="">2018</a>)</cite>, multi-head self-attention (MHSA) based <cite class="ltx_cite ltx_citemacro_citep">(Pei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib103" title="">2019</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib45" title="">2020</a>)</cite>, and graph neural network (GNN) based <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib88" title="">2020a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p2">
<p class="ltx_p" id="S3.SS1.SSS3.p2.1">Later, <cite class="ltx_cite ltx_citemacro_citet">Xi et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib153" title="">2021</a>)</cite> points out that the feedback used in earlier reranking models depends heavily on the displayed ranking list, while different permutations of the ranking list may yield different observation results.
Hence, some recent works seek to provide unobserved signals with an extra evaluator assuming the counterfactual permutations.
Generally, there is a generator to produce possible permutations followed by an evaluator to assess the utility of the corresponding permutation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib135" title="">2019</a>; Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib122" title="">2020</a>; Huzhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib48" title="">2021</a>)</cite>.
Moreover, multi-objective optimization is proposed to improve the single accuracy objective by providing more supervision signals.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p3">
<p class="ltx_p" id="S3.SS1.SSS3.p3.1">Though most existing models focus on accuracy metrics, several other metrics like diversity and fairness are attracting more and more attention due to the long-term benefits and regulatory requirements.
To increase the diversity, some models propose to decrease the similarity among items <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib28" title="">2018</a>; Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib161" title="">2021</a>)</cite>, while some other models choose to promote the coverage over some specific categories <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib107" title="">2020</a>; Abdool et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib2" title="">2020</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib89" title="">2023d</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS3.p4">
<p class="ltx_p" id="S3.SS1.SSS3.p4.1">Besides the above-mentioned works about different loss functions and supervision signals, there are also some new directions—mixed reranking tries to reorder the list of mixed sources of items <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib157" title="">2021</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib182" title="">2020</a>)</cite>; edge reranking considers the reranking with the collaboration of cloud and edge <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib35" title="">2020</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Major challenges in list-wise recommendation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">With the rapid development and great success of deep learning for recommendation, there emerges a large variety of research problems, including feature interaction modeling, user behavior modeling, and multi-task and multi-scenario learning.
In this section, we pick some representative models to have a brief view of the development of deep learning models for recommendation.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Feature Interaction in Recommendation</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Early deep learning based models focused on the adequate modeling of feature interactions, which can be divided into three categories according to the different operators: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p1.1.1">product based, convolutional based and
attention based</span> <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib174" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.1">Product based models</span> use different product operators like the inner product, outer product, or Hadamard product to capture relevance between vectors for effective feature interaction modeling.
DeepFM <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib37" title="">2017</a>)</cite> proposes to utilize an FM layer in parallel with an additional DNN to model feature interactions.
Deep &amp; Cross Network (DCN)  <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib141" title="">2017</a>)</cite> recursively applies vector-wise Hadamard product layer by layer, to capture feature interactions of different orders.
DCN V2 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib142" title="">2021</a>)</cite> enhances DCN by introducing a matrix to replace the feature crossing vector to improve its modeling capability.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p3.1.1">The convolutional based models</span> explore the convolutional neural networks
(CNN) and graph convolutional networks (GCN) for feature interaction modeling.
FGCNN <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib82" title="">2019</a>)</cite> exploits the fine-grained local patterns mining capacity of the CNN layer to generate high-quality features, then feeds the combination of these new features with existing features into IPNN  <cite class="ltx_cite ltx_citemacro_citep">(Qu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib108" title="">2016</a>)</cite> for final prediction.
FiGNN <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib75" title="">2019</a>)</cite> considers different features as graph nodes and designs a fully connected graph with all node pairs fully linked, then conducts the graph propagation for feature interaction modeling.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p4">
<p class="ltx_p" id="S3.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p4.1.1">The attention based models</span> exploit the different attention architectures for feature interaction modeling.
AutoInt <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib123" title="">2019</a>)</cite> proposes to utilize the self-attention layer to learn the relevance between the central feature and all other features, thus learning a contextualized representation for better prediction.
FibiNet <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib46" title="">2019</a>)</cite> applies the SENET <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib44" title="">2018</a>)</cite> to the input features to learn dynamical feature weights and alleviate the noise inside feature interactions.
Though many deep models have been proposed for feature interaction modeling in the last 10 years, the performance difference is indistinguishable and some new models obtain even worse results than previous SOTAs <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib187" title="">2021</a>)</cite>.
One reason is that the recommendation system research field lacks unified datasets and baselines.
Another possible reason is that seldom work pays attention to the basic theory of these models, which makes the conclusions solely depend on the experimental results, influenced by many factors, such as dataset splits, dataset size, hyper-parameters tuning, etc.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>User Behavior Modeling in Recommendation</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">In addition to the modeling of feature interaction, user behavior features that record the user’s historical interaction in a given time range are also critical for recommendation.
To mine the various and crucial interest patterns of users, existing user behavior modeling methods can be summarized into three research trends according to the different characteristics of input user behavior features: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p1.1.1">traditional sequential modeling, long-sequence modeling, and multi-behavior modeling</span> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib40" title="">2023a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.1">Traditional sequential modeling</span> seeks to extract item co-occurrence and sequentially dependency from a single short behavior sequence.
DIN <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib184" title="">2018</a>)</cite> proposes a target-aware attention network to learn different weights for different items, thus obtaining the user’s real interests and removing the noise.
CAN <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib183" title="">2020a</a>)</cite> designs a co-action network with dynamic network parameter generation to capture the interaction between the target item and the historical behaviors.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p3.1.1">Long-sequence modeling</span> attempts to use a longer sequence to model users’ lifelong behaviors and mine more accurate user interests.
SIM <cite class="ltx_cite ltx_citemacro_citep">(Pi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib105" title="">2020</a>)</cite> proposes to search a subset of behaviors most similar to the target item at first, then uses target-aware attention to extract user interests.
ETA <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib16" title="">2021</a>)</cite> leverages locality-sensitive hashing (LSH) and Hamming distance to retrieve relevant items with an end-to-end architecture.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p4">
<p class="ltx_p" id="S3.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p4.1.1">Multi-behavior modeling</span> aims to explicitly consider the different behavior types and rich behavior attributes for more accurate prediction.
MBSTR <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib167" title="">2022</a>)</cite> regards multi-type behaviors as a unified sequence, then proposes a new heterogeneous transformer layer to capture multi-behavior dependencies and behavior-specific semantics.
SC-CNN <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib176" title="">2022c</a>)</cite> concatenates side information with the item identifications as a 3D cube,
then uses a CNN to capture the relations among side features and item identification.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p5">
<p class="ltx_p" id="S3.SS2.SSS2.p5.1">User behavior modeling is similar to the token sequence modeling in the NLP field with the common sequential characteristic.
Inspired by the great success of large language models, it’s very promising to exploit the scaling law of user behavior modeling for training large recommendation models.
HSTU <cite class="ltx_cite ltx_citemacro_citep">(Zhai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib170" title="">2024</a>)</cite> proposed by META regards all the interacted items and behavior types as a unified sequence, then applies a modified transformer architecture to scale the model to a maximum of 1.5 trillion parameters.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>Multi-task and Multi-scenario learning in Recommendation</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">As a pivotal tool to provide online information and services, there are usually multiple optimization tasks (e.g., click, like, purchase, etc.) to reflect the various actions of users.
Besides, there are also a large number of scenarios (e.g., video, music, reading, etc.) to satisfy the diversified requirements of users.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">To jointly optimize multiple tasks, the <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p2.1.1">multi-task learning</span> paradigm  <cite class="ltx_cite ltx_citemacro_citep">(Caruana, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib10" title="">1997</a>)</cite> which uses a unified structure to optimize multiple targets is widely used.
Representative models are Shared Bottom  <cite class="ltx_cite ltx_citemacro_citep">(Caruana, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib10" title="">1997</a>)</cite>, MMOE <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib96" title="">2018b</a>)</cite>, and PLE <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib128" title="">2020</a>)</cite>.
Shared Bottom feeds a shared bottom layer to multiple prediction layers for multi-task learning.
MMOE <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib96" title="">2018b</a>)</cite> learns different gating networks to assign different weights over different experts for more flexible prediction.
PLE <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib128" title="">2020</a>)</cite> further proposes to use shared and specific expert networks to mitigate information conflicts among different tasks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">Considering the various recommendation scenarios, <span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p3.1.1">multi-scenario learning</span> seeks to learn a unified model for different scenarios to avoid the sparsity of cold scenarios and reduce the cost of training multiple models.
STAR <cite class="ltx_cite ltx_citemacro_citep">(Sheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib117" title="">2021</a>)</cite> proposes a star topology structure, which contains the shared centered parameters and scenario-specific parameters to handle the problem of scenario distribution discrepancy.
SASS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib179" title="">2022b</a>)</cite> utilizes a multi-layer domain adaptive transfer module with a two-stage training process to facilitate the multi-domain information transfer.
APG <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib160" title="">2022</a>)</cite> designs a new learning paradigm where input features are used to generate model parameters to achieve domain adaptation.
M2M <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib172" title="">2022a</a>)</cite> introduces the meta unit to incorporate scenario knowledge to learn inter-scenario correlations.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p4">
<p class="ltx_p" id="S3.SS2.SSS3.p4.1">There are also works that try to use a single framework to handle multi-task learning and multi-scenario learning simultaneously.
HiNet <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib185" title="">2023</a>)</cite> proposes a domain-aware attentive network on top with a hierarchical information extraction network to optimize multiple objectives in multiple domains.
The seesaw phenomenon (i.e., models improve the performance of some tasks or scenarios, but harm the others) widely exists.
Though many deep models have been proposed, the deep understanding of how and when the negative transfer happens is still under-explored.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Discussion</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The list-wise recommendation system is a mature recommendation paradigm, it can provide abundant information to users at one time, making it convenient for users to quickly obtain the information they want.
However, the conventional list-wise recommendation systems do have some problems.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Implicit signals can be vague and noisy.</span>
The user feedback on a recommended list (e.g., browse, click) is implicit, which can be vague and noisy to learn user preference.
For example, song completion in a music recommendation system can represent a positive signal because the user did not change the song.
But it can also be a negative signal cause the user might not actually pay attention to the song.
Moreover, the position of an item in a list can influence the user’s attention.
Specifically, items appearing in the first position are more likely to be seen and clicked by users even if the users might prefer the item in the middle.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Lack semantic knowledge</span>
Merely based on the ID-based collaborative signals, the recommendation models can hardly know about the user and item thoroughly.
A music recommendation system may contain millions of songs and lyrics, but they do not include the stories behind the songs and the social connections of the singers. This information is in fact informative for a better representation understanding. What’s more, most of the list-wise recommendation systems currently do not take into consideration the explanation for the recommendation results, which makes the system ambiguous to the users and the users accordingly lose control of the personalized recommendation system.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Fortunately, two emerging recommendation paradigms have been proposed to address the problems mentioned above. The first one is the conversational recommendation system (CRS), in which users can engage in multiple turns of interactions with the system, and the system will in return guide the users to clarify their needs and preferences gradually and finally provide the suitable items. In most cases, the CRS will provide a corresponding explanation according to the user history or previous conversations.
The other is the LLM-enhanced recommendation systems, which can utilize the open-world knowledge acquired from the pre-training stage to help the recommendation models better understand the item and user portrait. Besides, the logical reasoning ability and tool-calling ability can be further exploited to optimize the user behavior understanding and recommendation procedure.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>LLM-enhanced Recommendation</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Thanks to the development of deep learning, list-wise recommender systems have achieved remarkable progress over the past decades as depicted in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S3" title="3. Conventional List-wise Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">3</span></a>.
However, their recommendation performance is still limited, hampered by several major drawbacks as follows:
1) The list-wise recommendation models based on deep learning generally capture the user-item occurrence relationship with discrete ID features by feature interaction and user behaviors modeling <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib37" title="">2017</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib184" title="">2018</a>)</cite>, thus ignoring their original semantic information.
2) The scope of involved knowledge for recommendation modeling is limited within the domain. The lack of open-domain world knowledge obstructs the ability to obtain the latest information and understand the relationship between items and users.
3) These recommendation models might lack recommendation explainability.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Recently, with the emergence of pretrained language models (PLM), especially large language models (LLMs) such as GPT-3 <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib8" title="">2020</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib132" title="">2023</a>)</cite>, ChatGLM <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib25" title="">2022</a>)</cite>, these drawbacks are expected to be solved. LLMs have showcased powerful energy in general intelligence, such as memorizing vast amounts of open-world factual knowledge, excellent understanding of content and context, as well as logical and commonsense reasoning ability, which have been applied in various tasks.
By involving LLMs in recommender systems, the effective information can be boosted remarkably.
Firstly, the semantic content information (e.g., item descriptions) can be well understood and the associated open-world knowledge beyond the training data can also be injected.
Besides, with the blessing of planning and reasoning ability, users’ latent intentions and preferences can be inferred accurately under semantic space.
Moreover, the traditional recommendation paradigm based on collaborative signals may also be upgraded to semantic signal dominance.
Based on the roles of LLMs in recommender systems, we divide these LLM-empowered recommender systems into two categories: <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">LLMs for feature engineering</span> and <span class="ltx_text ltx_font_italic" id="S4.p2.1.2">LLMs for ranking</span>, shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4.F3" title="Figure 3 ‣ 4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">3</span></a></p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="377" id="S4.F3.g1" src="x3.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>The illustrative dissection of LLM-empowered recommendation. LLM-empowered recommender systems can be divided into two categories: LLMs for feature engineering and LLMs for ranking.
</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>LLMs for feature engineering</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">In this category, the role of LLMs is subservient, which is used to excavate beneficial semantic features and representations, and the traditional DL-based recommendation models still dominate the final ranking decision process.
Based on the subordinate adaptation of LLMs during the recommendation pipeline, the existing works can be mainly divided into two kinds: 1) feature generation, and 2) feature encoder.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>LLMs for feature generation</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Feature generation is one of the most important stages for the industrial recommendation, especially before the era of deep learning, which aims to excavate beneficial features for better depicting the user-item relevance.
Benefiting from the excellent content generation capabilities and open-world knowledge, LLMs can be involved into the feature engineering stage for generating auxiliary textual features.
By designing appropriate prompting strategies with several original features (e.g., item descriptions, user profiles, and behaviors), LLMs can produce informative textual features, thus enriching semantic information and alleviating the problem of feature scarcity in cold-start scenarios.
According to the type of augmented features, these works can be further classified into two groups: 1) user/item-level feature augmentation, and 2) instance-level feature augmentation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1">User/item-level feature augmentation.</span>
Benefiting from the external open-world knowledge, LLMs can involve rich and even up-to-date information in the closed recommendation space, thus contributing to the feature augmentation, especially for the item features.
Besides, the powerful summarizing and reasoning ability of LLMs makes user preference modeling <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib24" title="">2024</a>)</cite> and item content understanding <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib154" title="">2023b</a>)</cite> more accurate and efficient.
Served as a feature augmentation plug-in module for recommendation models, KAR <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib154" title="">2023b</a>)</cite> leverages LLMs to generate user-level preference knowledge and item-level factual knowledge.
Besides, SAGCN <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib84" title="">2023c</a>)</cite> designs a chain-based prompting approach to extract semantic aspect-aware user-item interaction features in a fine-grained manner, which are fed into the GCN for further modeling users’ preferences.
In contrast to employing a frozen and general base LLM for feature generation, LLaMA-E<cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib118" title="">2023</a>)</cite> and EcomGPT <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib74" title="">2023d</a>)</cite> engage in targeted fine-tuning of the base LLMs with intra-field knowledge before feature augmentation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.1">Instance-level feature augmentation.</span>
Besides enhancing individual user/item-level features, LLMs are also employed to create synthetic samples thus contributing to a semantic-richer training dataset, which belongs to the instance-level feature augmentation.
GENRE framework <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib86" title="">2023b</a>)</cite> employs custom-designed prompts to extract not only user/news-level features via LLM-generated user profiles and news titles, but also instance-level features with LLM-generated synthetic news pieces.
Consequently, this LLM-augmented approach contributes to improving the prediction of news recommendations.
To efficiently compress and condensate the training data for recommendation, TF-DCon <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib151" title="">2023a</a>)</cite> leverages LLMs to empower the generation of compact users and content compression, reducing the dataset by 95% with similar performance impressively.
Besides, LLMs play a crucial role in enhancing textual inputs for LLM-based recommenders <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib83" title="">2023e</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib127" title="">2023</a>)</cite> and rewriting user queries <cite class="ltx_cite ltx_citemacro_citep">(Peng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib104" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib71" title="">2023e</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>LLMs for feature encoder</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">For conventional ID-based recommender systems <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib142" title="">2021</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib184" title="">2018</a>)</cite>, the structured features are first converted into one-hot features via one-hot encodings with semantic information loss, and following, a feature embedding layer parameterized as an embedding look-up table is deployed to map into dense embeddings, which acts as the default feature encoder in ID-based recommendation.
With the development of multi-modal content understanding technologies, the textual and visual features are no longer simply transformed into discrete one-hot features, but are further encoded by various models such as LLMs to excavate semantic information thus enriching the item/user representations.
Furthermore, benefiting from the bridge role of natural language, the adoption of LLMs as feature encoders also facilitates transfer learning and cross-domain recommendation <cite class="ltx_cite ltx_citemacro_citep">(Tian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib130" title="">2023</a>; Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib34" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1">For item representation enhancement, news recommendation <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib86" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib73" title="">2023i</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib72" title="">h</a>; Runfeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib114" title="">2023</a>)</cite> is the most widely used scenario due to abundant textual features (e.g., news titles, entities, keywords, and abstract).
<cite class="ltx_cite ltx_citemacro_citet">Li et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib72" title="">2023h</a>)</cite> harnessed LLMs as news encoders within traditional recommendation models to generate news representations for personalized news recommendations.
Besides, LKPNR <cite class="ltx_cite ltx_citemacro_citep">(Runfeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib114" title="">2023</a>)</cite> presents a novel framework that combines LLMs and Knowledge Graphs (KG) into news semantic representations.
In addition to news recommendations, LLMs are also served as item encoder applied to software purchase <cite class="ltx_cite ltx_citemacro_citep">(John et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib57" title="">2024</a>)</cite>, social networking <cite class="ltx_cite ltx_citemacro_citep">(Jiang and Ferrara, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib53" title="">2023</a>)</cite>, tour itinerary recommendation <cite class="ltx_cite ltx_citemacro_citep">(Ho and Lim, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib42" title="">2023</a>)</cite>, and other general recommendation scenarios <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib17" title="">2023</a>; Harte et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib38" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">For user representation enhancement, the strength of LLMs lies in understanding the diverse interests and dynamic evolving preferences of users.
To better assist aspect-based recommendations, LLM4ARec <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib67" title="">2023f</a>)</cite> utilizes GPT2 for extracting personalized aspect terms and user representations from user profiles and reviews.
In recommendation, users’ interests can be adequately reflected by the historical interactive item IDs.
To overcome the semantic ID index problem, TIGER <cite class="ltx_cite ltx_citemacro_citep">(Rajput et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib111" title="">2023</a>)</cite> and LMIndexer <cite class="ltx_cite ltx_citemacro_citep">(Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib56" title="">2023</a>)</cite> propose vector quantization (VQ) based methods by compressing each item into a tuple of discrete semantic tokens, thus facilitating the sequential modeling of LLMs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>LLMs for ranking</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In this category, the role of the leader gradually shifts from traditional DL-based models to LLMs, and the recommendation paradigm shifts from collaborative signals to semantic signals.
To further improve the recommendation performance, plenty of works focus on injecting collaborative knowledge from traditional ID-based recommendation models into LLMs <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib178" title="">2023a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib70" title="">2023b</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib188" title="">2023</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib58" title="">2023</a>)</cite>.
Based on the different tasks of LLM, the existing works can be divided into two groups: 1) scoring-based LLMs, and 2) generation-based LLMs.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Scoring-based LLMs</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">For item-scoring tasks, the LLMs act as a pointwise function to estimate the prediction relevance between user and candidate items.
In this task, LLM-empowered recommendation models mainly perform Click-through Rate (CTR) prediction, which are widely-used in the <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p1.1.1">ranking stage</span> for large-scale industrial recommender systems.
To obtain the relevance score from LLMs, several approaches are proposed.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1">The first one is single-tower paradigm <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib79" title="">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib178" title="">2023a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib70" title="">2023b</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib188" title="">2023</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib58" title="">2023</a>)</cite> that discards the original language modeling decoder head and deploys an extra projection layer to calculate the score.
CoLLM<cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib178" title="">2023a</a>)</cite> and E4SRec <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib70" title="">2023b</a>)</cite> integrate collaborative information pre-learned from ID-based recommendation models into LLMs and deploy LoRA for effective parameter fine-tuning.
Kang et al. <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib58" title="">2023</a>)</cite> fine-tuned the LLMs with different sizes for rating prediction and performed detailed experiments under zero-shot and few-show settings.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1">The second one is two-tower paradigm <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib129" title="">2023</a>; Torbati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib131" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib162" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib66" title="">2023g</a>)</cite> that also discards the original decoder head and outputs the user and item representations from two heads. Based on this, the relevance score can be obtained via distance metric between the two representations such as cosine similarity.
Recformer <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib66" title="">2023g</a>)</cite> organizes the raw features into text and leverages the language model for extracting item and user sequence representations.
Besides, LLM-Rec <cite class="ltx_cite ltx_citemacro_citep">(Tang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib129" title="">2023</a>)</cite> further explores the multi-domain behavior modeling for LLMs.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">The last one is preserving the decoder head and obtaining the score over limited tokens (e.g., “Yes.” and “No.”) via Softmax function <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib5" title="">2023b</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib81" title="">2023b</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib95" title="">2024</a>)</cite>.
To align the LLMs with recommendations, TALLRec <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib5" title="">2023b</a>)</cite> designs a framework to build LLM-based recommendation models, which enables the effective and efficient finetuning of LLMs with lightweight LoRA. In these settings, LLMs are required to provide feedback of a user to the new item with “Yes./No.”.
Moreover, ReLLa <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib81" title="">2023b</a>)</cite> further extends to user long behavior modeling and mitigates the incomprehension problem of LLMs with a retrieval-enhanced approach.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Generation-based LLMs</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Different from the item scoring tasks that estimate the relevance score between user and candidate items, item generation tasks require the LLMs to directly generate the final ranked list of items.
According to the difference in whether items generated by LLMs are restricted, generation-based LLMs can be further categorized into two groups: 1) open-set generation, and 2) closed-set generation.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p2.1.1">Open-set generation.</span>
In this category, LLMs directly generate a ranked item list without relying on the predefined candidate item set due to the massive scale of items, which is similar to the current <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p2.1.2">recall stage</span> in multi-stage industrial recommender systems. However, due to the smaller number of items generated by LLMs compared to the recall stage, these approaches belong to novel single-stage generative recommendation <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib51" title="">2023</a>; Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib4" title="">2023a</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib54" title="">2023</a>)</cite>.
GenRec <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib51" title="">2023</a>)</cite> prompts the LLMs to generate the target item directly. To make sure that the generated items are in the item pool, several post-processing operations are proposed, including L2 distance mapping in BIGRec <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib4" title="">2023a</a>)</cite> and cosine similarity in LANCER <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib54" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p3.1.1">Closed-set generation.</span>
As for closed-set item generation tasks, a pre-filtered candidate item list (up to 20 usually) is provided by DL-based recommendation models, and the LLMs are expected to select several items that users are most interested in <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib132" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib146" title="">2023c</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib32" title="">2024</a>)</cite>.
This paradigm is similar to the <span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p3.1.2">reranking stage</span> in industrial recommender systems that LLMs rerank the candidate items provided by the previous ranking models.
LlamaRec <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib168" title="">2023</a>)</cite> proposes a two-stage recommendation framework and deploys the LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib132" title="">2023</a>)</cite> for candidate ranking.
DRDT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib146" title="">2023c</a>)</cite> also uses the retriever-reranker two-stage framework and employs iterative multi-round reflection to refine the final ranked list with given candidates gradually.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Discussion</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">LLM-empowered recommendation breaks down the recommendation internal knowledge barriers and empowers the semantic understanding of the entire recommendation pipeline from the feature engineering to model ranking, thus raising effective information remarkably.
Existing LLM-empowered recommendation models attempt different methods to involve in-domain collaborative knowledge with semantic information, such as tuning LLMs <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib5" title="">2023b</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib81" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib66" title="">2023g</a>)</cite> or inferring with collaborative models <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib70" title="">2023b</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib154" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib70" title="">2023b</a>)</cite>, thus achieving better performance compared with the traditional ID-based list-wise recommendation models. The rich world knowledge and excellent reasoning ability take understanding of users and items to new levels <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib154" title="">2023b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib86" title="">2023b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Although the incorporation of LLMs in recommender systems has brought about better ways of understanding content, the interaction interface of LLM-empowered recommenders remains unchanged relying on implicit feedback like clicks, where users’ current intentions or real needs are difficult to accurately detect and the degree of satisfaction with recommendation results are also difficult to reflect.
To this end, the interactive LLMs present a promising alternative, by offering a more active and adaptive form of user interaction in a conversational style.
Besides, this conversation-styled recommendation could engage in real-time interactions with users to obtain more accurate feedback about the recommendation results.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Conversational Recommender Systems Before LLM Era</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we are pivoting our focus from the previous list-wise recommendation architectures to conversational recommender systems (CRS).
We will review the early-day developments of CRS before the LLM era <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib31" title="">2021</a>)</cite>.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="312" id="S5.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>The illustration of conventional list-wise recommendation and conversational recommender systems (CRS).</figcaption>
</figure>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Compared with CRS, the list-wise recommendation platforms have one major shortcoming: the ambiguity in user understanding.
The user satisfaction and intentions are ambiguously expressed in traditional list-wise recommenders.
The user feedback is usually implicit, such as clicking items or simply thumb-scrolling, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.F4" title="Figure 4 ‣ 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">4</span></a>.
The implicit feedback makes it difficult for the system to determine what the user really likes or the exact intention of the user.
The user might not really like the item she clicked (wrong decision/big thumb) or even not really know what she likes before the system shows her all the options.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">To overcome the above-mentioned shortcoming of the static recommendation systems, conversational recommender systems (CRS) are proposed <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib31" title="">2021</a>)</cite>.
The definition of CRS could be a recommender system that has <span class="ltx_text ltx_font_italic" id="S5.p3.1.1">multi-turn interactions</span> with the users, and the recommendation results could be adjusted in real-time according to user’s interactions.
Natural language is the major form of multi-turn interactions, users could express their delicate intentions and preferences by using natural language.
Furthermore, the system could provide more accurate and appropriate responses thus increasing the effective information obtained by the users.
An example of the recommendation process of CRS is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5.F4" title="Figure 4 ‣ 5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">In the following sections, we will introduce the two key aspects of designing a CRS: <span class="ltx_text ltx_font_italic" id="S5.p4.1.1">question asking</span> and <span class="ltx_text ltx_font_italic" id="S5.p4.1.2">maintaining conversations</span>.
We also review the methods of evaluating such systems and discuss the applications and limitations of CRS before the LLM era.
It should be noticed that the CRS models based on the early-day language models <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib23" title="">2018</a>; Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib109" title="">2019</a>)</cite> such as Bert <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib158" title="">2020</a>)</cite> and GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib186" title="">2020b</a>)</cite> are also included in this section.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Questions Asking</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">For a CRS, the most important task is asking the user questions and trying to narrow down the candidate items according to the user’s answers.
The different asking mechanisms could be categorized into item-level and attribute-level.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Item-level</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">The early-day CRS models ask directly if the user likes the item/recommended list or not <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib31" title="">2021</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib140" title="">2018a</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib181" title="">2013</a>; Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib20" title="">2016</a>)</cite>.
The choice-based models give users options and let them choose which one they like best.
The options could be two items <cite class="ltx_cite ltx_citemacro_citep">(Sepliarskaia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib115" title="">2018</a>)</cite>, a list of items <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib52" title="">2014</a>; Graus and Willemsen, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib36" title="">2015</a>)</cite>, or two different lists <cite class="ltx_cite ltx_citemacro_citep">(Loepp et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib93" title="">2014</a>)</cite>.
The recommendation model will be updated according to the user’s choice.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p2">
<p class="ltx_p" id="S5.SS1.SSS1.p2.1">Interactive recommendation is another research branch of item-level CRS.
It is mainly based on reinforcement learning such as multi-arm bandits (MAB) <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib181" title="">2013</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib140" title="">2018a</a>)</cite> (balancing exploration&amp;exploitation) and deep RL methods <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib180" title="">2018</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib12" title="">2019b</a>; Xian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib156" title="">2019</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib14" title="">2019a</a>; Zou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib190" title="">2020</a>)</cite> which could model the dynamic preference and long-term utility.
Other methods such as Bayesian models <cite class="ltx_cite ltx_citemacro_citep">(Chajewska et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib11" title="">1998</a>; Vendrov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib133" title="">2020</a>)</cite> are also proposed for item-level question asking in CRS.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS1.p3">
<p class="ltx_p" id="S5.SS1.SSS1.p3.1">Although many research works are proposed for item-level question asking, they are not very applicable in real systems because users could be bored of being asked many questions about specific items.
It is more suitable to ask coarse-grained questions about the attributes or concepts that the user likes.
Thus, attribute-level question asking methods are proposed.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Attribute-level</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Attribute-level question asking is more efficient than item-level because an attribute corresponds to a set of items which could help the system reduce the candidate set significantly.
The mainstream approaches include fitting historical interactions, critiquing-based methods, RL-based methods, and graph-based methods.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p2">
<p class="ltx_p" id="S5.SS1.SSS2.p2.1">Historical interaction learning methods deem conversation records to be a sequence.
And the training objective is to predict the next attribute to ask/item to recommend.
These methods are usually based on sequential models like GRU or LSTM <cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib19" title="">2018</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib177" title="">2018</a>)</cite>.
However, these methods do not consider how to respond to user’s rejecting feedback.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p3">
<p class="ltx_p" id="S5.SS1.SSS2.p3.1">The critiquing-based method means that users give critical opinions on certain attributes, such as ”don’t like green,” and the system adjusts the recommended results accordingly.
The system usually removes all the candidate items with rejected attributes using static and heuristic rules <cite class="ltx_cite ltx_citemacro_citep">(Chen and Pu, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib13" title="">2012</a>; Smyth et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib121" title="">2004</a>; Viappiani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib134" title="">2007</a>; Smyth and McGinty, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib120" title="">2003</a>)</cite>.
Apart from the rule-based models, the vector-based methods embed the critic signals into the latent representation of items <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib150" title="">2019</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib94" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS2.p4">
<p class="ltx_p" id="S5.SS1.SSS2.p4.1">RL-based models usually use policy networks that select suitable attributes to ask the users <cite class="ltx_cite ltx_citemacro_citep">(Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib62" title="">2020a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib63" title="">b</a>)</cite>.
Graph-based models include path-walking methods according to the conversations <cite class="ltx_cite ltx_citemacro_citep">(Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib63" title="">2020b</a>)</cite> and GCN-enhanced methods <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib15" title="">2019c</a>; Liao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib76" title="">2020</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Maintain Conversations</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">The question-asking methods focus on ”what to ask.” in this section, we discuss the methods of maintaining a good conversation with users (”when to ask”).
A good conversation instead of continuing interrogating is much better for user experience.
We will describe the timing of question asking and how to lead the conversations.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>When to ask &amp; recommend</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">The strategy for determining when to ask and recommend could be rule-based (making recommendations every <math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p1.1.m1.1"><semantics id="S5.SS2.SSS1.p1.1.m1.1a"><mi id="S5.SS2.SSS1.p1.1.m1.1.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.1.m1.1b"><ci id="S5.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p1.1.m1.1d">italic_k</annotation></semantics></math> turn of asking <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib175" title="">2020</a>)</cite>) or random policy <cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib19" title="">2018</a>)</cite>.
A more sophisticated strategy is implemented by utilizing reinforcement learning.
The RL-based models always utilize a deep policy network with multiple actions. The actions include facets to ask and yielding a recommendation <cite class="ltx_cite ltx_citemacro_citep">(Sun and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib126" title="">2018</a>; Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib62" title="">2020a</a>)</cite>.
The policy-based models are trained using a policy gradient.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Leading the conversation</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Users may not be so sure about what they really like.
Thus the CRS should not only interrogate users but lead the conversation and make recommendations naturally without affecting users’ minds.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">There are some research works about multi-topic learning.
The system could proactively lead a conversation and naturally switch from a non-recommendation dialog to a recommendation dialog <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib92" title="">2020b</a>)</cite>.
These models define the recommendation task as the major goal and topic transitions as short-term goals <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib186" title="">2020b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p3">
<p class="ltx_p" id="S5.SS2.SSS2.p3.1">A good CRS should lead the conversation with special abilities such as suggesting, negotiating, and persuading <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib31" title="">2021</a>)</cite>.
With the development of large language models (LLMs), it could be easier for CRS to deal with the conversations more naturally.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Evaluation of CRS</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The evaluation of CRS is not a trivial task.
The lack of good benchmark datasets is an important issue.
The existing datasets are usually generated under specific constraints or rules.
The dataset scale is not large enough compared to real-world scenarios, resulting in insufficient evaluation <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib31" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">There are two levels of CRS evaluation: <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.1">turn-level</span> and <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">conversation-level</span>.
The turn-level evaluation focuses on a single turn of conversation.
It evaluates conversations from both language generation and recommendation accuracy.
For language generation evaluation, CRS mainly uses metrics such as BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib102" title="">2002</a>)</cite> and Rouge <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib77" title="">2004</a>)</cite>.
For recommendation accuracy, rating-based (RMSE) or ranking-based (NDCG, MAP, MRR, etc) metrics are utilized.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">For conversation-level evaluation, the most accurate method is to conduct an online test.
The metrics include average turns (AT) <cite class="ltx_cite ltx_citemacro_citep">(Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib63" title="">2020b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib68" title="">2021</a>)</cite>, recommendation success rate (SR), and cumulative performance of each turn.
Other conversation-level evaluation methods that do not require online tests include counterfactual evaluations <cite class="ltx_cite ltx_citemacro_citep">(Jagerman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib49" title="">2019</a>; McInerney et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib98" title="">2020</a>)</cite> and user simulation <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Balog, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib173" title="">2020</a>; Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib20" title="">2016</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Discussion</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Although conversational recommender systems offer users personalized item recommendations through natural language interfaces, they have not emerged as the dominant product type.
One of the core limitations of CRS could be the extra user efforts of using natural language to interact with the system.
The early-day CRS models tend to ask the users a lot of questions, which may be wordy and rigid <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib31" title="">2021</a>)</cite>.
It is not convenient for users to type all the words to tell the system what they want in the recommendation scenario.
Because the recommendation scenario corresponds to the ”hanging out” behaviors on the platforms instead of explicit information needs.
The potential improvements of the recommendation accuracy by CRS may be not enough to cover the extra cost of user efforts introduced by the conversations.
Apart from the accuracy of a CRS, we should also be aware of the extra cost of user efforts, which will largely influence user experiences, which is less discussed in early-day CRS research.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conversational Recommender Systems in LLM Era</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The LLM-enhanced recommenders (detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4" title="4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">4</span></a>), akin to traditional recommenders, are a single-turn interaction process that faces challenges in precisely identifying users’ current interests or actual needs and accurately reflecting satisfaction levels with recommendation outcomes. In contrast, CRS has the natural advantage of real-time understanding of user intents and the ability to adapt recommendations by interacting with users in a timely manner. To enhance comprehension of user preferences and needs, the dialogue module of CRS necessitates advanced language understanding and reasoning abilities. Yet, pre-LLM era CRS approaches (discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S5" title="5. Conversational Recommender Systems Before LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">5</span></a>) predominantly utilized pre-trained language models like GPT-2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib109" title="">2019</a>)</cite>, BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib23" title="">2018</a>)</cite>, BART <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib64" title="">2020</a>)</cite>, and T5 <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib110" title="">2020</a>)</cite>. Owing to their limited training corpora and model capacities, these models demonstrate insufficient abilities for effective conversational recommendation.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Through pre-training on extensive world knowledge and alignment with high-quality instruction data, LLMs like GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib101" title="">2023</a>)</cite>, PaLM <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib18" title="">2023</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib132" title="">2023</a>)</cite> and ChatGLM <cite class="ltx_cite ltx_citemacro_citep">(Du et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib25" title="">2022</a>; Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib169" title="">2022</a>)</cite> excel in knowledge retention, language understanding, content generation, and instruction following. These advancements offer innovative avenues for developing sophisticated conversational recommender systems. However, LLMs are generally trained by publicly available sources on the internet, which lack visibility into the data that resides within private platforms, resulting in the sub-optimal understanding capabilities of such data. Thus, LLM-based CRS focuses on integrating the LLMs with off-the-shelf recommenders to equip them with domain-specific knowledge. Based on the methods of applying LLM, LLM-based CRS can be roughly categorized into prompt engineering (prompting-based LLM-CRS, illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.F5.sf1" title="In Figure 5 ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">5(a)</span></a>) and fine-tuning (Fine-tuning-based LLM-CRS, depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6.F5.sf2" title="In Figure 5 ‣ 6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">5(b)</span></a>) approaches.</p>
</div>
<figure class="ltx_figure" id="S6.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F5.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="230" id="S6.F5.sf1.g1" src="x5.png" width="287"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F5.sf1.2.1.1" style="font-size:80%;">(a)</span> </span><span class="ltx_text" id="S6.F5.sf1.3.2" style="font-size:80%;">Prompting-based LLM-CRS</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S6.F5.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="226" id="S6.F5.sf2.g1" src="x6.png" width="287"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F5.sf2.2.1.1" style="font-size:80%;">(b)</span> </span><span class="ltx_text" id="S6.F5.sf2.3.2" style="font-size:80%;">Fine-tuning-based LLM-CRS</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>A overview of the LLM-based conversational recommendation system (CRS): (a) The prompting-based LLM-CRS showcases an organized composition of interconnected modules, each serving a distinct purpose. The LLM acts as a controller, interacting with off-the-shelf recommenders to manage information about items, user profiles, dialogues, user behaviors and etc., through prompts, as well as enhancing performance via reasoning and reflection. (b) The fine-tuning-based LLM-CRS can be divided into two structural approaches: one aims to jointly optimize LLM and recommender, while the other formulates CRS as a generative recommendation task, tackled primarily through the LLM. Besides, the LLM is also utilized to generate CRS data, addressing data scarcity issues.</figcaption>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Prompt Engineering Specialized for CRS</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Recent studies show that prompt serves as a critical interface for human-language model interactions in various tasks. A well crafted prompt strategy, such as CoT <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib147" title="">2022</a>)</cite>, ToT <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib164" title="">2023a</a>)</cite>, ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib165" title="">2023b</a>)</cite>, can efficiently harness the advanced capabilities of LLMs. Thus, some work proposes to integrate recommendations and LLM through prompt engineering, enabling LLM to seamlessly interact with recommendation systems and leverage LLM’s linguistic capabilities to achieve conversational recommendations <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib159" title="">2024</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Prompting-based LLM-CRS Architecture</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">When prompting LLMs as recommender systems, despite variations in specific prompt designs across studies <cite class="ltx_cite ltx_citemacro_citep">(Murakhovs’ka et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib99" title="">2023</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib33" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib143" title="">2023b</a>; Lin and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib78" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib145" title="">2023a</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib41" title="">2023b</a>; Spurlock et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib124" title="">2024</a>)</cite>, the format remains substantially similar with only slight differences. Since existing LLMs lack comprehensive conversational recommendation capabilities, prompting solutions dissect CRS into interconnected sub-modules, each aligned with a specific CRS function, including intent recognition, dialogue management, state tracking, response generation, recommendation calls, etc. Then, specific prompts are crafted for each sub-module to activate the reasoning abilities of LLMs. The conversational recommendation is facilitated through the collaborative interaction among sub-modules. Chat-REC <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib33" title="">2023</a>)</cite> augments LLMs by converting user proﬁles and historical interactions into prompts, and connecting users and products through in-context learning. Through linking traditional recommender systems with LLMs, Chat-REC achieves efficient and effective outcomes, even in cold-start scenarios. GeneRec <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib143" title="">2023b</a>)</cite> focuses on personalized AIGC, which adopts an AI generator to personalize content generation and leverages user instructions to acquire users’ information needs for recommendation. RecMind <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib145" title="">2023a</a>)</cite> introduces the planning mechanism, termed self-inspiring, to build an LLM-powered autonomous recommender agent, which can dynamically explore historical states to improve the recommendation. He <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS1.p1.1.1">et. al.</span> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib41" title="">2023b</a>)</cite> directly treat LLMs as zero-shot CRS and use hand-crafted task description template, format requirement and conversational context to prompt LLM. Spurlock <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS1.p1.1.2">et. al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Spurlock et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib124" title="">2024</a>)</cite> further build a full pipeline, from to prompt creation and content analysis to final recommendation and feedback, around LLMs to simulate how a user might realistically probe the model for recommendations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Prompting-based LLM-CRS Evaluation</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">Liu <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS2.p1.1.1">et. al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin and Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib78" title="">2023</a>)</cite> investigate the feasibility of developing artificial general recommender (AGR) via LLMs. Specifically, they propose ten fundamental principles, such as inconsistency detection, behavioral analysis, etc., that AGR should adhere to. Then they proceed to assess whether LLM can comply with the proposed principles by engaging in recommendation-oriented dialogues with the model while observing its behavior. They demonstrate the potential for LLM to serve as AGR, but with several limitations to be addressed. iEvaLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib144" title="">2023e</a>)</cite> reveals the inadequacy of the existing CRS evaluation protocol, which overemphasizes the matching with ground-truth items annotated by humans while neglecting the interactive nature of CRS. iEvaLM utilizes LLM to build user simulators, which can simulate various system-user interaction scenarios and provide an in-depth evaluation of CRS.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Fine-tuning Specialized for CRS</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">LLM-based CRS via prompt engineering has shown improvements in effectiveness, informativeness, and user experience. However, a notable disparity exists between textual information and user-item collaborative signals, posing challenges for LLMs to capture key information accurately in recommendation scenarios. To address this, approaches focus on finer problem decomposition and advanced prompt techniques to ensure effectiveness. Nonetheless, this leads to complex execution processes and low efficiency. Some endeavors <cite class="ltx_cite ltx_citemacro_citep">(Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib30" title="">2023</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib91" title="">2023a</a>; Mysore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib100" title="">2023</a>; Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib188" title="">2023</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib27" title="">2023</a>; Ravaut et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib112" title="">2024</a>)</cite> involve fine-tuning LLM to enhance CRS effectiveness by injecting recommendation capabilities.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Data Acquisition</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">Fine-tuning involves tuning LLM with massive high-quality domain-specific conversational recommendation data. However, existing datasets for conversational recommendation are typically limited in scale and quality. Meanwhile, the data in most recommendation scenarios primarily consists of explicit or implicit user-item interactions, but lacks conversation context. To alleviate this issue, recent work, such as RecLLM <cite class="ltx_cite ltx_citemacro_citep">(Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib30" title="">2023</a>)</cite> and iEvaLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib144" title="">2023e</a>)</cite>, proposes to construct a user simulator using LLM for conversational data generation. Specifically, RecLLM <cite class="ltx_cite ltx_citemacro_citep">(Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib30" title="">2023</a>)</cite> builds a controllable LLM-based user simulator to generate synthetic conversations through fine-grained controls, while iEvaLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib144" title="">2023e</a>)</cite> simulates various system-user interaction scenarios via carefully designed task instructions. Liu <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.1.1">et. al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib91" title="">2023a</a>)</cite> directly collects a real-world E-commerce pre-sales dialogue dataset. MINT <cite class="ltx_cite ltx_citemacro_citep">(Mysore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib100" title="">2023</a>)</cite> uses LLM to re-purpose the publicly available user-item interaction datasets to form the synthetic narrative-driven recommendation data. CLLM4Rec <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib188" title="">2023</a>)</cite> extends the vocabulary of LLM to cover the user/item IDs and then fuse them into LLM, thereby existing datasets can be easily adapted for model training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>Model Architecture</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">In consideration of the domain-specific nature of recommendation data and models, it is imperative for Large Language Models (LLMs) to effectively harness these resources to generate and convey recommendation outcomes to users. Liu <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS2.p1.1.1">et. al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib91" title="">2023a</a>)</cite> have implemented a collaborative framework wherein the CRS model and LLMs are synergistically integrated to augment each other’s functionality. Additional research <cite class="ltx_cite ltx_citemacro_citep">(Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib30" title="">2023</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib27" title="">2023</a>)</cite> has segmented CRS into various sub-components or tasks, assigning roles such as dialogue management, execution, explanation, and ranking to LLMs, thereby facilitating interaction with users and recommendation systems. MINT <cite class="ltx_cite ltx_citemacro_citep">(Mysore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib100" title="">2023</a>)</cite> conceptualizes CRS as a retrieval problem, subsequently refining LLMs for use as narrative-based retrieval agents. Similarly, CLLM4Rec <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib188" title="">2023</a>)</cite> formalizes CRS as a generative recommendation, proposing the resolution of CRS through the direct application of fine-tuned LLMs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3. </span>Fine-tuning Strategy</h4>
<div class="ltx_para" id="S6.SS2.SSS3.p1">
<p class="ltx_p" id="S6.SS2.SSS3.p1.1">The fine-tuning technique is critical in determining the ultimate quality of LLM-based CRS, with a well-designed and effective strategy significantly enhancing model performance and capabilities. Liu <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS3.p1.1.1">et. al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib91" title="">2023a</a>)</cite> propose a sequential tuning paradigm for the collaboration between LLM and CRS in two ways, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS3.p1.1.2">i.e.</span>, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS3.p1.1.3">LLM assisting CRS</span> and <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS3.p1.1.4">CRS assisting LLM</span>. CLLM4Rec <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib188" title="">2023</a>)</cite> regards CRS as a generative recommendation task and also adopts the sequential tuning strategy, where a pre-training stage to capture user/item collaborative information via language modeling objective and a fine-tuning stage to enhance recommendation generation. RecLLM <cite class="ltx_cite ltx_citemacro_citep">(Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib30" title="">2023</a>)</cite> devises an integrated architecture with multiple modules powered by LLMs, where each module is optimized individually by the synthetic data from the user simulator. MINT <cite class="ltx_cite ltx_citemacro_citep">(Mysore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib100" title="">2023</a>)</cite> formalizes CRS as a retrieval task and directly tunes the LLM via constructed instruction data. LLMCRS <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib27" title="">2023</a>)</cite> relies on the ability of LLM to handle sub-tasks, and it optimizes the overall performance by jointly adapting the model through reinforcement learning for CRS performance feedback.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Discussion</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">In general, LLM-based CRS via prompt engineering facilitates the conversational recommendation through refined task decomposition and eliciting the reasoning abilities of LLMs with in-context learning, without the need for training and fine-tuning the LLMs. This allows for rapid adjustment and deployment across various conversational recommendation scenarios. Moreover, compared to conventional CRS, LLM possesses stronger content understanding and response generation abilities. Thus, LLM-based CRS could achieve better user interest capture and guidance, thereby elevating interaction efficiency and user experience. Meanwhile, Fine-tuning LLMs on conversational recommendation data enhances their ability to comprehend user-specific needs and preferences, thereby improving the accuracy and personalization of recommendations through better domain-specific language understanding. Moreover, the fine-tuned LLMs exhibit improved conversational fluency and naturalness. They demonstrate better intent understanding and response capabilities, streamlining prompt engineering processes and boosting efficiency. Additionally, they can effectively leverage historical interaction data for refined personalization, ensuring higher-quality interactions.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>Advantages</h4>
<div class="ltx_para" id="S6.SS3.SSS1.p1">
<p class="ltx_p" id="S6.SS3.SSS1.p1.1">Thanks to the extensive world knowledge, LLMs demonstrate enhanced proficiency in discerning user intent, comprehending content, and engaging in dialogue. Compared to conventional CRS, LLM-based CRS exhibits superior accuracy in intention recognition and swiftly identifies users’ interests, thereby diminishing interaction rounds and associated costs. Moreover, leveraging LLMs’ robust expressive capabilities, these methods markedly advance in response informativeness, offering elaborate item descriptions and credible explanations, while effectively navigating users through dialogues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span>Limitations</h4>
<div class="ltx_para" id="S6.SS3.SSS2.p1">
<p class="ltx_p" id="S6.SS3.SSS2.p1.1">Despite the promising performance achieved, existing LLM-based CRS methods still rely on the traditional conversational recommendation paradigms, primarily incorporating and aligning LLMs with conventional CRS methodologies via either prompt engineering or fine-tuning techniques. For instance, prompt engineering solutions focus on task decomposition for specific CRS scenarios, employing LLM as controller and sub-task handlers to deal with them separately, such as item ranking, dialogue management, response generation, and reflection, and interacting with recommendation engines to enable conversational recommendation. While fine-tuning solutions involve instruction tuning the LLM with the domain dataset to enable it to recommend and interact with CRS tasks. In summary, existing LLM-based CRSs are deficient in systemic innovation. Firstly, task decomposition and fine-tuning strategies based on specific needs cannot be generalized across more scenarios, resulting in inferior transferability of the existing solutions. Furthermore, these solutions lack advanced adaptive decision-making and process control capabilities, unable to adapt to users’ rapidly changing and diverse needs.

</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>LLM-powered Recommendation Agent</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The LLM-enhanced list-wise recommender systems (as shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S4" title="4. LLM-enhanced Recommendation ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">4</span></a>) are facing the challenge of precisely identifying the users’ current needs by modeling with users’ noisy interaction history (such as clicks, skips, ratings, and reviews).
Meanwhile, the LLM-based conversational recommender systems (as shown in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6" title="6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">6</span></a>) are deficient in adapting to users’ rapidly changing and diverse needs in various information-seeking scenarios by integrating LLMs with off-the-shelf recommenders to equip them with domain-specific knowledge.
In addition, due to the existence of scaling laws for LLMs <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib59" title="">2020</a>)</cite>, we can anticipate further enhancement of LLM-based recommender systems in feature engineering, ranking performance, and conversational skills.
Driven by the powerful perception and reasoning abilities provided by LLMs, the previously mentioned two paths of LLM-based recommender systems (i.e. list-wise recommendation and conversational recommendation) converge at a point, called LLM-powered recommendation agents, that can maximally elicit the potential of LLMs under the scaling law.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">An LLM-powered agent generally refers to an advanced AI system capable of perceiving its environment and taking autonomous actions based on the perceived information to achieve specific goals <cite class="ltx_cite ltx_citemacro_citep">(Franklin and Graesser, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib29" title="">1996</a>)</cite>.
Such an agent usually consists of several key components: perception, planning, memory, tool use, and actions <cite class="ltx_cite ltx_citemacro_citep">(Weng, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib148" title="">2023</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib155" title="">2023a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib137" title="">2023d</a>)</cite>.
To this end, an LLM-powered recommendation agent is capable of perceiving and identifying the users’ rapidly changing needs, autonomously planning the information-seeking tasks by decomposing and reflecting the tasks, retrieving the users’ profiles and preferences from memory, invoking tools for searching and recommending, and taking actions by providing a ranking list of items, giving replies, or asking clarification questions.
An LLM-powered recommendation agent can avoid issues of inaccurate intent recognition by leveraging the LLM-enhanced list-wise recommender systems as a part of tools or memory. At the same time, it can also address the limitations of the LLM-based conversational recommender systems in generalizability across different scenarios and adaptability to diverse users’ information needs.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">In this Section, we first illustrate the architecture of an LLM-powered recommendation agent.
Then, we summarise the recent advances in LLM-based agents for recommendation.
We also define the future developmental stages of LLM-powered recommendation agents and analyze their capabilities at each stage.
We finally discuss some possible directions for LLM-powered recommendation agents.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Architecture of LLM-Powered Recommendation Agent</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">LLM-powered recommendation agents have been recently adapted from generic LLM-powered autonomous agents that frame an LLM as a powerful general problem solver, complemented by several key components, such as perception, planning, memory, and tool use, and actions <cite class="ltx_cite ltx_citemacro_citep">(Weng, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib148" title="">2023</a>; Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib155" title="">2023a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib137" title="">2023d</a>)</cite>.
There exist several proof-of-concept projects to construct LLM-powered agents, such as AutoGPT<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Significant-Gravitas/AutoGPT" title="">https://github.com/Significant-Gravitas/AutoGPT</a></span></span></span>, BabyAGI<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/yoheinakajima/babyagi" title="">https://github.com/yoheinakajima/babyagi</a></span></span></span>, MetaGPT<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/geekan/MetaGPT" title="">https://github.com/geekan/MetaGPT</a></span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Hong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib43" title="">2023</a>)</cite>, XAgent<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenBMB/XAgent" title="">https://github.com/OpenBMB/XAgent</a></span></span></span>, and GPTs<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/introducing-gpts" title="">https://openai.com/blog/introducing-gpts</a></span></span></span>.
These LLM-powered agents have been proven to be powerful in achieving various real-world tasks, such as shopping online <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib163" title="">2022</a>)</cite>, developing software <cite class="ltx_cite ltx_citemacro_citep">(Hong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib43" title="">2023</a>)</cite>, and video creation<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/RayVentura/ShortGPT" title="">https://github.com/RayVentura/ShortGPT</a></span></span></span>.
LLM-powered agents provide a generic solution for addressing users’ complex information needs regarding recommendations.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7.F6" title="Figure 6 ‣ 7.1. Architecture of LLM-Powered Recommendation Agent ‣ 7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">6</span></a> depicts an overview of the LLM-powered recommendation agents.
In particular, an LLM-powered recommendation agent can engage in natural-language or multi-modal interactions, decompose the information-seeking task, retrieve knowledge, recall from memory, make informed decisions, and adapt to unfamiliar scenarios with its inherent generalization and adaptability <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib155" title="">2023a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib137" title="">2023d</a>)</cite>.
In addition, LLM-powered agents can collaborate and divide tasks among each other, forming a multi-agent system <cite class="ltx_cite ltx_citemacro_citep">(Wooldridge, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib149" title="">2009</a>)</cite> to accomplish tasks jointly.
A single-agent recommender system leverages a single LLM as a controller to control the recommendation process, while a multi-agent recommender system leverages multiple LLMs as various roles (such as planning, criticizing, and reflection) in the recommendation process.
To this end, we define “<span class="ltx_text ltx_font_italic" id="S7.SS1.p2.1.1">an LLM-powered recommendation agent as an advanced information-seeking system that is capable of perceiving its environment (including users and context), autonomously planning the information-seeking tasks (such as identifying users’ intention, decomposing the tasks, and reflection), retrieving memory (including profiles, interactions, and conversations), invoking tool use (such as search engines and recommenders), and taking actions (such as providing a ranking list of items, giving replies, or asking clarification questions) to effectively satisfy users’ information needs with minimal user effort</span>”.</p>
</div>
<figure class="ltx_figure" id="S7.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="339" id="S7.F6.g1" src="x7.png" width="714"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>An overview of the LLM-powered recommendation agent.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Advances in LLM-powered Recommendation Agents</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Recent research on LLM-powered recommendation agents mainly focuses on the design of agent architectures.
In particular, LLM-powered recommendation agents can be roughly categorized into single-agent and multi-agents for recommendations.
Note that we mainly concentrate on recommender systems with LLM-powered agents in this survey, rather than LLM-powered user agents (such as RecAgent <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib138" title="">2023f</a>)</cite> and Agent4Rec <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib171" title="">2023b</a>)</cite>) for user simulation.</p>
</div>
<section class="ltx_paragraph" id="S7.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Single-Agent for Recommendation</h5>
<div class="ltx_para" id="S7.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S7.SS2.SSS0.Px1.p1.1">Single-agent recommender systems resemble the LLM-based conversational recommender systems (detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S6" title="6. Conversational Recommender Systems in LLM Era ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">6</span></a>) that consider off-the-shelf recommenders as a source of domain-specific knowledge.
Different from the LLM-based conversational recommender systems that heavily rely on prompt engineering with well-crafted prompt strategies, such as CoT <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib147" title="">2022</a>)</cite>, ToT <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib164" title="">2023a</a>)</cite>, ReAct <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib165" title="">2023b</a>)</cite>, single-agent recommender systems emphasize the rational coordination of various components, such as perception, planning, memory, tools, and actions.
For instance, InteRecAgent <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib47" title="">2023</a>)</cite> (<span class="ltx_text ltx_font_bold" id="S7.SS2.SSS0.Px1.p1.1.1">Inte</span>ractive <span class="ltx_text ltx_font_bold" id="S7.SS2.SSS0.Px1.p1.1.2">Rec</span>ommender <span class="ltx_text ltx_font_bold" id="S7.SS2.SSS0.Px1.p1.1.3">Agent</span>) proposed an efficient workflow for information-seeking task execution, incorporating key components such as a candidate memory bus, dynamic demonstration-augmented task planning, and reflection.
In particular, InteRecAgent introduced an efficient framework that employs LLMs as the brain and contains a minimal set of essential tools (an information query module to address the users’ inquiries, an item retrieval tool to propose a list of item candidates and an item ranking tool to tailor personalized content for users) required to transform LLMs into agents.</p>
</div>
</section>
<section class="ltx_paragraph" id="S7.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Multi-Agents for Recommendation</h5>
<div class="ltx_para" id="S7.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S7.SS2.SSS0.Px2.p1.1">Multi-agent recommender systems are keen on designing different agent roles and facilitating their collaboration to accomplish a complex information-seeking task together.
Multi-agent systems have proven to be effective in reducing the difficulty of the task.
For instance, RAH <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#bib.bib119" title="">2023</a>)</cite> (Recommender system, Assistant, and Human) designed a framework with several LLM-powered agents, such as Perceive, Learn, Act, Critic, and Reflect, utilizing the so-called Learn-Act-Critic loop and a reflection mechanism for improving the alignment with user personalities.
Within this framework, the assistant facilitates two key workflows: (1)
<span class="ltx_text ltx_font_bold" id="S7.SS2.SSS0.Px2.p1.1.1">RecSys→Assistant→Human</span>:
This workflow focuses on the assistant filtering personalized recommendations for the end user.
(2) <span class="ltx_text ltx_font_bold" id="S7.SS2.SSS0.Px2.p1.1.2">Human→Assistant→RecSys</span>: This workflow enables the assistant to learn from user feedback and accordingly tune recommender systems.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Development Levels</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">The LLM-powered recommendation agent is a burgeoning new field that is still in rapid development.
Despite the above-mentioned PoC (Proof of Concept) attempts in LLM-powered recommendation agents, there are still many shortcomings.
For instance, perception modalities are singular, manually designing planning processes is inefficient, memory functions are underutilized, tools are not abundant, and actions are decoupled.
To better promote and guide the development of LLM-powered recommendation agents, Table <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S7.T1" title="Table 1 ‣ 7.3. Development Levels ‣ 7. LLM-powered Recommendation Agent ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the four development levels of LLM-powered recommendation agents and the corresponding capabilities.
Current agents for recommendation mostly fall in L1 and L2, only a few studies cover the capabilities of L3 or L4.
This highlights a need for further research to explore related directions in this domain.</p>
</div>
<figure class="ltx_table" id="S7.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Different levels of intelligence for LLM-powered recommendation agents with CAPS.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S7.T1.20" style="width:424.9pt;height:433pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-33.4pt,34.1pt) scale(0.864057454647852,0.864057454647852) ;">
<table class="ltx_tabular ltx_align_middle" id="S7.T1.20.20">
<tr class="ltx_tr" id="S7.T1.20.20.21">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T1.20.20.21.1">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span>
Level</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S7.T1.20.20.21.2">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.20.20.21.2.1">
<span class="ltx_p" id="S7.T1.20.20.21.2.1.1" style="width:113.8pt;">Key Characteristics</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T1.20.20.21.3">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.20.20.21.3.1">
<span class="ltx_p" id="S7.T1.20.20.21.3.1.1" style="width:256.1pt;">Capabilities</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T1.5.5.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T1.5.5.5.6">
<span class="ltx_rule" style="width:100%;height:0.5pt;background:black;display:inline-block;"> </span>
L1-Compliance</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S7.T1.5.5.5.7">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.5.5.5.7.1">
<span class="ltx_p" id="S7.T1.5.5.5.7.1.1" style="width:113.8pt;">LLM-powered recommendation agents complete the recommendation task by following the instructions pre-defined by the users or the developers.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T1.5.5.5.5">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.5.5.5.5.5">
<span class="ltx_p" id="S7.T1.5.5.5.5.5.5" style="width:256.1pt;"><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.1.1.1.1.1.1.m1.1"><semantics id="S7.T1.1.1.1.1.1.1.m1.1a"><mo id="S7.T1.1.1.1.1.1.1.m1.1.1" xref="S7.T1.1.1.1.1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.1.1.1.1.1.1.m1.1b"><ci id="S7.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S7.T1.1.1.1.1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.1.1.1.1.1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.1.1.1.1.1.1.m1.1d">∙</annotation></semantics></math> Uni-modal perception, primarily text-based. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.2.2.2.2.2.2.m2.1"><semantics id="S7.T1.2.2.2.2.2.2.m2.1a"><mo id="S7.T1.2.2.2.2.2.2.m2.1.1" xref="S7.T1.2.2.2.2.2.2.m2.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.2.2.2.2.2.2.m2.1b"><ci id="S7.T1.2.2.2.2.2.2.m2.1.1.cmml" xref="S7.T1.2.2.2.2.2.2.m2.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.2.2.2.2.2.2.m2.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.2.2.2.2.2.2.m2.1d">∙</annotation></semantics></math> Crafted planning procedure. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.3.3.3.3.3.3.m3.1"><semantics id="S7.T1.3.3.3.3.3.3.m3.1a"><mo id="S7.T1.3.3.3.3.3.3.m3.1.1" xref="S7.T1.3.3.3.3.3.3.m3.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.3.3.3.3.3.3.m3.1b"><ci id="S7.T1.3.3.3.3.3.3.m3.1.1.cmml" xref="S7.T1.3.3.3.3.3.3.m3.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.3.3.3.3.3.3.m3.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.3.3.3.3.3.3.m3.1d">∙</annotation></semantics></math> Short-term memory only within current conversational session. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.4.4.4.4.4.4.m4.1"><semantics id="S7.T1.4.4.4.4.4.4.m4.1a"><mo id="S7.T1.4.4.4.4.4.4.m4.1.1" xref="S7.T1.4.4.4.4.4.4.m4.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.4.4.4.4.4.4.m4.1b"><ci id="S7.T1.4.4.4.4.4.4.m4.1.1.cmml" xref="S7.T1.4.4.4.4.4.4.m4.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.4.4.4.4.4.4.m4.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.4.4.4.4.4.4.m4.1d">∙</annotation></semantics></math> Limited tools, using only recommendation models and search engines. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.5.5.5.5.5.5.m5.1"><semantics id="S7.T1.5.5.5.5.5.5.m5.1a"><mo id="S7.T1.5.5.5.5.5.5.m5.1.1" xref="S7.T1.5.5.5.5.5.5.m5.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.5.5.5.5.5.5.m5.1b"><ci id="S7.T1.5.5.5.5.5.5.m5.1.1.cmml" xref="S7.T1.5.5.5.5.5.5.m5.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.5.5.5.5.5.5.m5.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.5.5.5.5.5.5.m5.1d">∙</annotation></semantics></math> Decoupled actions, either ranking lists or textual responses.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T1.10.10.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T1.10.10.10.6">
<span class="ltx_rule" style="width:100%;height:0.5pt;background:black;display:inline-block;"> </span>
L2-Autonomy</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S7.T1.10.10.10.7">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.10.10.10.7.1">
<span class="ltx_p" id="S7.T1.10.10.10.7.1.1" style="width:113.8pt;">LLM-powered recommendation agents autonomously plan the recommendation task using memory &amp; various tools, and iterate the plan based on both internal and external feedback until completion.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T1.10.10.10.5">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.10.10.10.5.5">
<span class="ltx_p" id="S7.T1.10.10.10.5.5.5" style="width:256.1pt;"><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.6.6.6.1.1.1.m1.1"><semantics id="S7.T1.6.6.6.1.1.1.m1.1a"><mo id="S7.T1.6.6.6.1.1.1.m1.1.1" xref="S7.T1.6.6.6.1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.6.6.6.1.1.1.m1.1b"><ci id="S7.T1.6.6.6.1.1.1.m1.1.1.cmml" xref="S7.T1.6.6.6.1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.6.6.6.1.1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.6.6.6.1.1.1.m1.1d">∙</annotation></semantics></math> Uni-modal perception, primarily text-based. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.7.7.7.2.2.2.m2.1"><semantics id="S7.T1.7.7.7.2.2.2.m2.1a"><mo id="S7.T1.7.7.7.2.2.2.m2.1.1" xref="S7.T1.7.7.7.2.2.2.m2.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.7.7.7.2.2.2.m2.1b"><ci id="S7.T1.7.7.7.2.2.2.m2.1.1.cmml" xref="S7.T1.7.7.7.2.2.2.m2.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.7.7.7.2.2.2.m2.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.7.7.7.2.2.2.m2.1d">∙</annotation></semantics></math> Autonomous planning process with task decomposition and reflection. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.8.8.8.3.3.3.m3.1"><semantics id="S7.T1.8.8.8.3.3.3.m3.1a"><mo id="S7.T1.8.8.8.3.3.3.m3.1.1" xref="S7.T1.8.8.8.3.3.3.m3.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.8.8.8.3.3.3.m3.1b"><ci id="S7.T1.8.8.8.3.3.3.m3.1.1.cmml" xref="S7.T1.8.8.8.3.3.3.m3.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.8.8.8.3.3.3.m3.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.8.8.8.3.3.3.m3.1d">∙</annotation></semantics></math> Short/long-term memory across conversational sessions. Extracted personalized memory from raw conversation history. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.9.9.9.4.4.4.m4.1"><semantics id="S7.T1.9.9.9.4.4.4.m4.1a"><mo id="S7.T1.9.9.9.4.4.4.m4.1.1" xref="S7.T1.9.9.9.4.4.4.m4.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.9.9.9.4.4.4.m4.1b"><ci id="S7.T1.9.9.9.4.4.4.m4.1.1.cmml" xref="S7.T1.9.9.9.4.4.4.m4.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.9.9.9.4.4.4.m4.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.9.9.9.4.4.4.m4.1d">∙</annotation></semantics></math> A set of tools for a certain domain, including recommendation models, search engines, and online APIs. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.10.10.10.5.5.5.m5.1"><semantics id="S7.T1.10.10.10.5.5.5.m5.1a"><mo id="S7.T1.10.10.10.5.5.5.m5.1.1" xref="S7.T1.10.10.10.5.5.5.m5.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.10.10.10.5.5.5.m5.1b"><ci id="S7.T1.10.10.10.5.5.5.m5.1.1.cmml" xref="S7.T1.10.10.10.5.5.5.m5.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.10.10.10.5.5.5.m5.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.10.10.10.5.5.5.m5.1d">∙</annotation></semantics></math> Explainable actions, such as ranking lists with explanations or informative textual responses.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T1.15.15.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T1.15.15.15.6">
<span class="ltx_rule" style="width:100%;height:0.5pt;background:black;display:inline-block;"> </span>
L3-Personification</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S7.T1.15.15.15.7">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.15.15.15.7.1">
<span class="ltx_p" id="S7.T1.15.15.15.7.1.1" style="width:113.8pt;">LLM-powered recommendation agents are equipped with professional knowledge and skills (such as salespersons, tourist guides), and can proactively provide personalized recommendation services at appropriate times in various domains.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T1.15.15.15.5">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.15.15.15.5.5">
<span class="ltx_p" id="S7.T1.15.15.15.5.5.5" style="width:256.1pt;"><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.11.11.11.1.1.1.m1.1"><semantics id="S7.T1.11.11.11.1.1.1.m1.1a"><mo id="S7.T1.11.11.11.1.1.1.m1.1.1" xref="S7.T1.11.11.11.1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.11.11.11.1.1.1.m1.1b"><ci id="S7.T1.11.11.11.1.1.1.m1.1.1.cmml" xref="S7.T1.11.11.11.1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.11.11.11.1.1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.11.11.11.1.1.1.m1.1d">∙</annotation></semantics></math> Multi-modal perception, including text, image, video, and audio. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.12.12.12.2.2.2.m2.1"><semantics id="S7.T1.12.12.12.2.2.2.m2.1a"><mo id="S7.T1.12.12.12.2.2.2.m2.1.1" xref="S7.T1.12.12.12.2.2.2.m2.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.12.12.12.2.2.2.m2.1b"><ci id="S7.T1.12.12.12.2.2.2.m2.1.1.cmml" xref="S7.T1.12.12.12.2.2.2.m2.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.12.12.12.2.2.2.m2.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.12.12.12.2.2.2.m2.1d">∙</annotation></semantics></math> Adaptive autonomous planning procedure leveraging techniques such as graphs. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.13.13.13.3.3.3.m3.1"><semantics id="S7.T1.13.13.13.3.3.3.m3.1a"><mo id="S7.T1.13.13.13.3.3.3.m3.1.1" xref="S7.T1.13.13.13.3.3.3.m3.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.13.13.13.3.3.3.m3.1b"><ci id="S7.T1.13.13.13.3.3.3.m3.1.1.cmml" xref="S7.T1.13.13.13.3.3.3.m3.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.13.13.13.3.3.3.m3.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.13.13.13.3.3.3.m3.1d">∙</annotation></semantics></math> Comprehensive memory, including profiles, interactions, conversations, and extracted personalized memory. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.14.14.14.4.4.4.m4.1"><semantics id="S7.T1.14.14.14.4.4.4.m4.1a"><mo id="S7.T1.14.14.14.4.4.4.m4.1.1" xref="S7.T1.14.14.14.4.4.4.m4.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.14.14.14.4.4.4.m4.1b"><ci id="S7.T1.14.14.14.4.4.4.m4.1.1.cmml" xref="S7.T1.14.14.14.4.4.4.m4.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.14.14.14.4.4.4.m4.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.14.14.14.4.4.4.m4.1d">∙</annotation></semantics></math> Various professional tools for different domains. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.15.15.15.5.5.5.m5.1"><semantics id="S7.T1.15.15.15.5.5.5.m5.1a"><mo id="S7.T1.15.15.15.5.5.5.m5.1.1" xref="S7.T1.15.15.15.5.5.5.m5.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.15.15.15.5.5.5.m5.1b"><ci id="S7.T1.15.15.15.5.5.5.m5.1.1.cmml" xref="S7.T1.15.15.15.5.5.5.m5.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.15.15.15.5.5.5.m5.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.15.15.15.5.5.5.m5.1d">∙</annotation></semantics></math> Reactive &amp; proactive actions, including ranking lists with informative explanations, informative textual responses, and clarification questions.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T1.20.20.20">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T1.20.20.20.6">
<span class="ltx_rule" style="width:100%;height:0.5pt;background:black;display:inline-block;"> </span>
L4-Self-Evolution</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S7.T1.20.20.20.7">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.20.20.20.7.1">
<span class="ltx_p" id="S7.T1.20.20.20.7.1.1" style="width:113.8pt;">LLM-powered recommendation agents are able to autonomously improve, adapt, or transform themselves over time, driven by internal feedback mechanisms or external stimuli.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T1.20.20.20.5">
<span class="ltx_inline-block ltx_align_top" id="S7.T1.20.20.20.5.5">
<span class="ltx_p" id="S7.T1.20.20.20.5.5.5" style="width:256.1pt;"><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.16.16.16.1.1.1.m1.1"><semantics id="S7.T1.16.16.16.1.1.1.m1.1a"><mo id="S7.T1.16.16.16.1.1.1.m1.1.1" xref="S7.T1.16.16.16.1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.16.16.16.1.1.1.m1.1b"><ci id="S7.T1.16.16.16.1.1.1.m1.1.1.cmml" xref="S7.T1.16.16.16.1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.16.16.16.1.1.1.m1.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.16.16.16.1.1.1.m1.1d">∙</annotation></semantics></math> Mixture of multi-modal perception, including text, image, video, and audio. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.17.17.17.2.2.2.m2.1"><semantics id="S7.T1.17.17.17.2.2.2.m2.1a"><mo id="S7.T1.17.17.17.2.2.2.m2.1.1" xref="S7.T1.17.17.17.2.2.2.m2.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.17.17.17.2.2.2.m2.1b"><ci id="S7.T1.17.17.17.2.2.2.m2.1.1.cmml" xref="S7.T1.17.17.17.2.2.2.m2.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.17.17.17.2.2.2.m2.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.17.17.17.2.2.2.m2.1d">∙</annotation></semantics></math> Adaptive autonomous planning procedure with task graph optimization. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.18.18.18.3.3.3.m3.1"><semantics id="S7.T1.18.18.18.3.3.3.m3.1a"><mo id="S7.T1.18.18.18.3.3.3.m3.1.1" xref="S7.T1.18.18.18.3.3.3.m3.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.18.18.18.3.3.3.m3.1b"><ci id="S7.T1.18.18.18.3.3.3.m3.1.1.cmml" xref="S7.T1.18.18.18.3.3.3.m3.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.18.18.18.3.3.3.m3.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.18.18.18.3.3.3.m3.1d">∙</annotation></semantics></math> Comprehensive memory, including profiles, interactions, conversations, and extracted personalized memory. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.19.19.19.4.4.4.m4.1"><semantics id="S7.T1.19.19.19.4.4.4.m4.1a"><mo id="S7.T1.19.19.19.4.4.4.m4.1.1" xref="S7.T1.19.19.19.4.4.4.m4.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.19.19.19.4.4.4.m4.1b"><ci id="S7.T1.19.19.19.4.4.4.m4.1.1.cmml" xref="S7.T1.19.19.19.4.4.4.m4.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.19.19.19.4.4.4.m4.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.19.19.19.4.4.4.m4.1d">∙</annotation></semantics></math> All available tools. 
<br class="ltx_break"/><math alttext="\bullet" class="ltx_Math" display="inline" id="S7.T1.20.20.20.5.5.5.m5.1"><semantics id="S7.T1.20.20.20.5.5.5.m5.1a"><mo id="S7.T1.20.20.20.5.5.5.m5.1.1" xref="S7.T1.20.20.20.5.5.5.m5.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S7.T1.20.20.20.5.5.5.m5.1b"><ci id="S7.T1.20.20.20.5.5.5.m5.1.1.cmml" xref="S7.T1.20.20.20.5.5.5.m5.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S7.T1.20.20.20.5.5.5.m5.1c">\bullet</annotation><annotation encoding="application/x-llamapun" id="S7.T1.20.20.20.5.5.5.m5.1d">∙</annotation></semantics></math> Mixture of reactive &amp; proactive actions.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S7.T1.20.20.22">
<td class="ltx_td ltx_align_left ltx_border_r" id="S7.T1.20.20.22.1"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T1.20.20.22.2"></td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T1.20.20.22.3"></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4. </span>Discussion</h3>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1">Despite the above-mentioned initial attempts in the recommendation domain, there is still significant exploration needed to fully develop recommendation systems utilizing LLM-based agents.
This includes (1) establishing reasonable evaluation mechanisms for recommendation agents, (2) devising rational optimization mechanisms for recommendation agents, such as prompt engineering, optimizing agent structure/workflow, and fine-tuning language models (SFT, RLHF/RLAIF) to achieve parameter optimization, and (3) modeling more complex user behaviors within recommendation agents, including dialogue behaviors and click behaviors.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Open problems and Future directions</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The remarkable progress made by the recommendation community over the past decades has facilitated the success of various great products.
Currently, we are on a brand new path that leads to the next generation of recommender systems.
Based on the lessons learned from history, we raise several problems and directions that may have a critical impact on the technical evolution along this path.</p>
</div>
<div class="ltx_para" id="S8.p2">
<ul class="ltx_itemize" id="S8.I1">
<li class="ltx_item" id="S8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i1.p1">
<p class="ltx_p" id="S8.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i1.p1.1.1">From Browsing to Experience</span>. Current recommender systems have primarily focused on presenting users with items they might like, while future recommender systems can transform this passive browsing into an immersive journey.
Instead of an insipid information stream, an organic synthesis of information like a table d’hôte is much more charming and attractive.
Future research could explore how to collect, exploit, and aggregate information from diverse resources and modalities to facilitate dynamic, context-aware, and personalized experiences that anticipate and adapt to user needs in both real-time and long term.
This shall involve innovative interfaces and interaction designs that provide not just recommendations but engagement opportunities that resonate with users’ evolving preferences and situational context.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i2.p1">
<p class="ltx_p" id="S8.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i2.p1.1.1">From Virtual to Reality</span> Recommender systems are predominantly operated in the digital domain, yet there lies immense potential in bridging this virtual interaction with the physical world.
Future developments could enhance real-world experiences through augmented reality (AR) and virtual reality (VR), offering contextual recommendations that seamlessly blend digital content with physical environments.
Moreover, the connection between recommender systems and physical assets through the Internet of Things (IoT) or smart environments would also pose great opportunities for embodied recommendation, which brings completely different user experience.
While this convergence promises significant advancements, it also presents challenges in ensuring seamless synchronization between virtual recommendations and real-world dynamics, which future research needs to address.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i3.p1">
<p class="ltx_p" id="S8.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i3.p1.1.1">From Perception to Cognition</span> Current recommender systems operate largely on perceptual data, recognizing patterns and correlations among user behaviors and preferences.
The next leap involves transitioning towards cognitive models that understand and predict user motivations, desires, and cognitive states.
With the capacity of LLMs for nuanced comprehension and generation, we can develop more sophisticated cognitive frameworks within recommender systems.
These systems would not only recommend based on surface-level behavior but also infer deeper, more abstract user intentions and emotions by understanding and reasoning on long-term and even life-long user data.
Research in this direction may also integrate cognitive science principles and user psychology into the design and functionality of recommender systems, moving towards a more profound understanding of user cognition and experience.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i4.p1">
<p class="ltx_p" id="S8.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i4.p1.1.1">From Tool to Assistant</span> The role of recommender systems is gradually evolving from being static tools that suggest options to becoming lively assistants that build friendship and trust with users.
Leveraging the powerful audio and video generation abilities of multimodal large foundation models, future recommender systems can be designed to engage with users like intimates rather than machines.
Researchers may focus on unifying recommender systems with digital assistants, which can provide users not just reactive but proactive and personalized digital companions.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i5.p1">
<p class="ltx_p" id="S8.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i5.p1.1.1">From Usability to Responsibility</span>. As recommender systems become deeply integrated into users’ lives, ensuring responsible and ethical usage becomes indispensable.
Future research must address usability not only from a functional perspective but also from ethical, privacy, and bias mitigation standpoints.
The broad applications of large foundation models have posed great risks related to data privacy, algorithmic bias, transparency, and accountability.
Researchers need to develop frameworks that ensure fair and unbiased recommendations, protect user privacy, and provide transparency in how recommendation results are generated, exploited, and disseminated.
In this framework, AI-generated content should be verified and moderated, and proper watermarking techniques are necessary.
The misuse of future recommender systems may lead to inconspicuous distortion of public perception, which should be carefully prevented and traced.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i6.p1">
<p class="ltx_p" id="S8.I1.i6.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i6.p1.1.1">From Product to Ecosystem</span>. A future destination of recommender systems lies in their transition from standalone products to integral components of a broader digital ecosystem.
In leveraging the general intelligence of foundation model capabilities, recommender agents may coordinate an interconnected network that shares insights and collaborates across different domains and platforms.
This involves not only interoperable standards and protocols that facilitate efficient and secure data exchange but also incentive mechanisms that ensure fair and inclusive profit sharing.
This trend may ultimately evolve recommender systems into a unified digital ecosystem, which relies on responsible computing mechanisms that link numerous platforms and individuals in various channels.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusion</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">Recommender systems play a crucial role as a bridge between humans and massive information, helping users obtain more useful information with less effort. As LLMs revolutionize the way humans engage with the online world, they also bring about new opportunities for exploring and improving recommender systems. At this pivotal point, this paper systematically reviews recommender systems’ two main development paths: traditional list-wise recommendation and conversational recommendation (depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.10081v1#S2.F1" title="Figure 1 ‣ 2. Overview ‣ All Roads Lead to Rome: Unveiling the Trajectory of Recommender Systems Across the LLM Era"><span class="ltx_text ltx_ref_tag">1</span></a>), each representing a different approach to better human-computer interaction. After thorough analysis, we believe that “all roads lead to Rome”, that is LLM-powered recommendation agents with more effective information and less interactive cost.
Finally, we point out several potential opportunities across the LLM era for future directions.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdool et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Mustafa Abdool, Malay Haldar, Prashant Ramanathan, Tyler Sax, Lanbo Zhang, Aamir Manaswala, Lynn Yang, Bradley Turnbull, Qing Zhang, and Thomas Legrand. 2020.

</span>
<span class="ltx_bibblock">Managing diversity in airbnb search. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 2952–2960.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ai et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Qingyao Ai, Keping Bi, Jiafeng Guo, and W Bruce Croft. 2018.

</span>
<span class="ltx_bibblock">Learning a deep listwise context model for ranking refinement. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">The 41st international ACM SIGIR conference on research &amp; development in information retrieval</em>. 135–144.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnaan He, and Qi Tian. 2023a.

</span>
<span class="ltx_bibblock">A bi-step grounding paradigm for large language models in recommendation systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">arXiv preprint arXiv:2308.08434</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023b.

</span>
<span class="ltx_bibblock">TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv preprint arXiv:2305.00447</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden and Robinson (2020)</span>
<span class="ltx_bibblock">
David Bawden and Lyn Robinson. 2020.

</span>
<span class="ltx_bibblock">Information overload: An overview.

</span>
<span class="ltx_bibblock">(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bobadilla et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Jesús Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Gutiérrez. 2013.

</span>
<span class="ltx_bibblock">Recommender systems survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Knowledge-based systems</em> 46 (2013), 109–132.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al<span class="ltx_text" id="bib.bib8.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.4.1">Advances in neural information processing systems</em> 33 (2020), 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al<span class="ltx_text" id="bib.bib9.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.4.1">arXiv preprint arXiv:2303.12712</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caruana (1997)</span>
<span class="ltx_bibblock">
Rich Caruana. 1997.

</span>
<span class="ltx_bibblock">Multitask learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Machine learning</em> 28 (1997), 41–75.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chajewska et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (1998)</span>
<span class="ltx_bibblock">
Urszula Chajewska, Lise Getoor, Joseph Norman, and Yuval Shahar. 1998.

</span>
<span class="ltx_bibblock">Utility Elicitation as a Classification Problem.. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">UAI</em>. 79–88.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang, and Yong Yu. 2019b.

</span>
<span class="ltx_bibblock">Large-scale interactive recommendation with tree-structured policy gradient. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the AAAI conference on artificial intelligence</em>, Vol. 33. 3312–3320.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Pu (2012)</span>
<span class="ltx_bibblock">
Li Chen and Pearl Pu. 2012.

</span>
<span class="ltx_bibblock">Critiquing-based recommenders: survey and emerging trends.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">User Modeling and User-Adapted Interaction</em> 22 (2012), 125–150.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H Chi. 2019a.

</span>
<span class="ltx_bibblock">Top-k off-policy correction for a REINFORCE recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em>. 456–464.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2019c)</span>
<span class="ltx_bibblock">
Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia Yang, and Jie Tang. 2019c.

</span>
<span class="ltx_bibblock">Towards knowledge-based recommender dialog system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:1908.05391</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwu Ou. 2021.

</span>
<span class="ltx_bibblock">End-to-end user behavior retrieval in click-through rateprediction model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2108.04468</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shuwei Chen, Xiang Li, Jian Dong, Jin Zhang, Yongkang Wang, and Xingxing Wang. 2023.

</span>
<span class="ltx_bibblock">TBIN: Modeling Long Textual Behavior Data for CTR Prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">arXiv preprint arXiv:2308.08483</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al<span class="ltx_text" id="bib.bib18.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.4.1">Journal of Machine Learning Research</em> 24, 240 (2023), 1–113.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Konstantina Christakopoulou, Alex Beutel, Rui Li, Sagar Jain, and Ed H Chi. 2018.

</span>
<span class="ltx_bibblock">Q&amp;R: A two-stage approach toward interactive recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 139–148.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016.

</span>
<span class="ltx_bibblock">Towards conversational recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>. 815–824.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Covington et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Paul Covington, Jay Adams, and Emre Sargin. 2016.

</span>
<span class="ltx_bibblock">Deep neural networks for youtube recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 10th ACM conference on recommender systems</em>. 191–198.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cybenko (1989)</span>
<span class="ltx_bibblock">
George Cybenko. 1989.

</span>
<span class="ltx_bibblock">Approximation by superpositions of a sigmoidal function.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Mathematics of control, signals and systems</em> 2, 4 (1989), 303–314.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv preprint arXiv:1810.04805</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Kounianhua Du, Jizheng Chen, Jianghao Lin, Yunjia Xi, Hangyu Wang, Xinyi Dai, Bo Chen, Ruiming Tang, and Weinan Zhang. 2024.

</span>
<span class="ltx_bibblock">DisCo: Towards Harmonious Disentanglement and Collaboration between Tabular and Semantic Space for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">arXiv preprint arXiv:2406.00011</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.

</span>
<span class="ltx_bibblock">GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 320–335.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Zhen Wen, Fei Wang, Xiangyu Zhao, Jiliang Tang, and Qing Li. 2023.

</span>
<span class="ltx_bibblock">Recommender Systems in the Era of Large Language Models (LLMs).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2307.02046 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang, Kun Gai, and Fei Sun. 2023.

</span>
<span class="ltx_bibblock">A Large Language Model Enhanced Conversational Recommender System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">ArXiv</em> abs/2308.06212 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yue Feng, Jun Xu, Yanyan Lan, Jiafeng Guo, Wei Zeng, and Xueqi Cheng. 2018.

</span>
<span class="ltx_bibblock">From greedy selection to exploratory decision-making: Diverse ranking with policy-value networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">The 41st international ACM SIGIR conference on research &amp; development in information retrieval</em>. 125–134.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Franklin and Graesser (1996)</span>
<span class="ltx_bibblock">
Stan Franklin and Art Graesser. 1996.

</span>
<span class="ltx_bibblock">Is it an Agent, or just a Program?: A Taxonomy for Autonomous Agents. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">International workshop on agent theories, architectures, and languages</em>. Springer, 21–35.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friedman et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Luke Friedman, Sameer Ahuja, David Allen, Zhenning Tan, Hakim Sidahmed, Changbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, Brian Chu, Zexiang Chen, and Manoj Tiwari. 2023.

</span>
<span class="ltx_bibblock">Leveraging Large Language Models in Conversational Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">ArXiv</em> abs/2305.07961 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chongming Gao, Wenqiang Lei, Xiangnan He, Maarten de Rijke, and Tat-Seng Chua. 2021.

</span>
<span class="ltx_bibblock">Advances and challenges in conversational recommender systems: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">AI Open</em> 2 (2021), 100–126.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jingtong Gao, Bo Chen, Xiangyu Zhao, Weiwen Liu, Xiangyang Li, Yichao Wang, Zijian Zhang, Wanyu Wang, Yuyang Ye, Shanru Lin, et al<span class="ltx_text" id="bib.bib32.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">LLM-enhanced Reranking in Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.4.1">arXiv preprint arXiv:2406.12433</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong, Haofen Wang, and Jiawei Zhang. 2023.

</span>
<span class="ltx_bibblock">Chat-REC: Towards Interactive and Explainable LLMs-Augmented Recommender System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">ArXiv</em> abs/2303.14524 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yuqi Gong, Xichen Ding, Yehui Su, Kaiming Shen, Zhongyi Liu, and Guannan Zhang. 2023.

</span>
<span class="ltx_bibblock">An Unified Search and Recommendation Foundation Model for Cold-Start Scenario. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>. 4595–4601.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yu Gong, Ziwen Jiang, Yufei Feng, Binbin Hu, Kaiqi Zhao, Qingwen Liu, and Wenwu Ou. 2020.

</span>
<span class="ltx_bibblock">EdgeRec: recommender system on edge in Mobile Taobao. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>. 2477–2484.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graus and Willemsen (2015)</span>
<span class="ltx_bibblock">
Mark P Graus and Martijn C Willemsen. 2015.

</span>
<span class="ltx_bibblock">Improving the user experience during cold start through choice-based preference elicitation. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 9th ACM Conference on Recommender Systems</em>. 273–276.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.

</span>
<span class="ltx_bibblock">DeepFM: a factorization-machine based neural network for CTR prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">arXiv preprint arXiv:1703.04247</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harte et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023.

</span>
<span class="ltx_bibblock">Leveraging large language models for sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 1096–1102.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Chen He, Denis Parra, and Katrien Verbert. 2016.

</span>
<span class="ltx_bibblock">Interactive recommender systems: A survey of the state of the art and future research challenges and opportunities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Expert Systems with Applications</em> 56 (2016), 9–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zhicheng He, Weiwen Liu, Wei Guo, Jiarui Qin, Yingxue Zhang, Yaochen Hu, and Ruiming Tang. 2023a.

</span>
<span class="ltx_bibblock">A Survey on User Behavior Modeling in Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:2302.11087</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023b.

</span>
<span class="ltx_bibblock">Large language models as zero-shot conversational recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the 32nd ACM international conference on information and knowledge management</em>. 720–730.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho and Lim (2023)</span>
<span class="ltx_bibblock">
Ngai Lam Ho and Kwan Hui Lim. 2023.

</span>
<span class="ltx_bibblock">Utilizing Language Models for Tour Itinerary Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2311.12355</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al<span class="ltx_text" id="bib.bib43.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Metagpt: Meta programming for multi-agent collaborative framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.4.1">arXiv preprint arXiv:2308.00352</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Jie Hu, Li Shen, and Gang Sun. 2018.

</span>
<span class="ltx_bibblock">Squeeze-and-excitation networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 7132–7141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jinhong Huang, Yang Li, Shan Sun, Bufeng Zhang, and Jin Huang. 2020.

</span>
<span class="ltx_bibblock">Personalized flight itinerary ranking at fliggy. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>. 2541–2548.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tongwen Huang, Zhiqi Zhang, and Junlin Zhang. 2019.

</span>
<span class="ltx_bibblock">FiBiNET: combining feature importance and bilinear feature interaction for click-through rate prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Proceedings of the 13th ACM conference on recommender systems</em>. 169–177.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, and Xing Xie. 2023.

</span>
<span class="ltx_bibblock">Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">ArXiv</em> abs/2308.16505 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huzhang et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Guangda Huzhang, Zhen-Jia Pang, Yongqing Gao, Yawen Liu, Weijie Shen, Wen-Ji Zhou, Qianying Lin, Qing Da, An-Xiang Zeng, Han Yu, et al<span class="ltx_text" id="bib.bib48.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">AliExpress Learning-To-Rank: Maximizing online model performance without going online.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.4.1">IEEE Transactions on Knowledge and Data Engineering</em> 35, 2 (2021), 1214–1226.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jagerman et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Rolf Jagerman, Ilya Markov, and Maarten de Rijke. 2019.

</span>
<span class="ltx_bibblock">When people change their mind: Off-policy evaluation in non-stationary recommendation environments. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em>. 447–455.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jannach et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li Chen. 2021.

</span>
<span class="ltx_bibblock">A survey on conversational recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">ACM Computing Surveys (CSUR)</em> 54, 5 (2021), 1–36.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianchao Ji, Zelong Li, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Juntao Tan, and Yongfeng Zhang. 2023.

</span>
<span class="ltx_bibblock">Text based Large Language Model for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">arXiv preprint arXiv:2307.00457</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Hai Jiang, Xin Qi, and He Sun. 2014.

</span>
<span class="ltx_bibblock">Choice-based recommender systems: a unified approach to achieving relevancy and diversity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">Operations Research</em> 62, 5 (2014), 973–993.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang and Ferrara (2023)</span>
<span class="ltx_bibblock">
Julie Jiang and Emilio Ferrara. 2023.

</span>
<span class="ltx_bibblock">Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2401.00893</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Junzhe Jiang, Shang Qu, Mingyue Cheng, and Qi Liu. 2023.

</span>
<span class="ltx_bibblock">Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">arXiv preprint arXiv:2309.10435</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Ray Jiang, Sven Gowal, Timothy A Mann, and Danilo J Rezende. 2018.

</span>
<span class="ltx_bibblock">Beyond greedy ranking: Slate optimization via list-CVAE.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">arXiv preprint arXiv:1803.01682</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Bowen Jin, Hansi Zeng, Guoyin Wang, Xiusi Chen, Tianxin Wei, Ruirui Li, Zhengyang Wang, Zheng Li, Yang Li, Hanqing Lu, et al<span class="ltx_text" id="bib.bib56.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Language Models As Semantic Indexers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.4.1">arXiv preprint arXiv:2310.07815</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">John et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Angela John, Theophilus Aidoo, Hamayoon Behmanush, Irem B Gunduz, Hewan Shrestha, Maxx Richard Rahman, and Wolfgang Maaß. 2024.

</span>
<span class="ltx_bibblock">LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">arXiv preprint arXiv:2401.06676</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023.

</span>
<span class="ltx_bibblock">Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">arXiv preprint arXiv:2305.06474</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Scaling laws for neural language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">arXiv preprint arXiv:2001.08361</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hyeyoung Ko, Suyeon Lee, Yoonseo Park, and Anna Choi. 2022.

</span>
<span class="ltx_bibblock">A survey of recommendation systems: recommendation models, techniques, and application fields.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Electronics</em> 11, 1 (2022), 141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koren et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Yehuda Koren, Robert Bell, and Chris Volinsky. 2009.

</span>
<span class="ltx_bibblock">Matrix factorization techniques for recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Computer</em> 42, 8 (2009), 30–37.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, and Tat-Seng Chua. 2020a.

</span>
<span class="ltx_bibblock">Estimation-action-reflection: Towards deep interaction between conversational and recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the 13th International Conference on Web Search and Data Mining</em>. 304–312.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang Chen, and Tat-Seng Chua. 2020b.

</span>
<span class="ltx_bibblock">Interactive path reasoning on graph for conversational recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 2073–2083.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020.

</span>
<span class="ltx_bibblock">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. Association for Computational Linguistics, 7871–7880.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, and Ying Shan. 2023c.

</span>
<span class="ltx_bibblock">TagGPT: Large Language Models are Zero-shot Multimodal Taggers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">arXiv preprint arXiv:2304.03022</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2023g)</span>
<span class="ltx_bibblock">
Jiacheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023g.

</span>
<span class="ltx_bibblock">Text Is All You Need: Learning Language Representations for Sequential Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">arXiv preprint arXiv:2305.13731</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2023f)</span>
<span class="ltx_bibblock">
Pan Li, Yuyan Wang, Ed H Chi, and Minmin Chen. 2023f.

</span>
<span class="ltx_bibblock">Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">arXiv preprint arXiv:2306.01475</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shijun Li, Wenqiang Lei, Qingyun Wu, Xiangnan He, Peng Jiang, and Tat-Seng Chua. 2021.

</span>
<span class="ltx_bibblock">Seamlessly unifying attributes and items: Conversational recommendation for cold-start users.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">ACM Transactions on Information Systems (TOIS)</em> 39, 4 (2021), 1–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xiangyang Li, Bo Chen, Lu Hou, and Ruiming Tang. 2023a.

</span>
<span class="ltx_bibblock">CTRL: Connect Tabular and Language Model for CTR Prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">arXiv preprint arXiv:2306.02841</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, and Chunxiao Xing. 2023b.

</span>
<span class="ltx_bibblock">E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">arXiv preprint arXiv:2312.02443</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Xiaopeng Li, Lixin Su, Pengyue Jia, Xiangyu Zhao, Suqi Cheng, Junfeng Wang, and Dawei Yin. 2023e.

</span>
<span class="ltx_bibblock">Agent4Ranking: Semantic Robust Ranking via Personalized Query Rewriting Using Multi-agent LLM.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">arXiv preprint arXiv:2312.15450</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2023h)</span>
<span class="ltx_bibblock">
Xinyi Li, Yongfeng Zhang, and Edward C Malthouse. 2023h.

</span>
<span class="ltx_bibblock">Exploring Fine-tuning ChatGPT for News Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">arXiv preprint arXiv:2311.05850</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2023i)</span>
<span class="ltx_bibblock">
Xinyi Li, Yongfeng Zhang, and Edward C Malthouse. 2023i.

</span>
<span class="ltx_bibblock">PBNR: Prompt-based News Recommender System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">arXiv preprint arXiv:2304.07862</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang, Chengyue Jiang, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang. 2023d.

</span>
<span class="ltx_bibblock">EcomGPT: Instruction-tuning Large Language Model with Chain-of-Task Tasks for E-commerce.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">arXiv preprint arXiv:2308.06966</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Zekun Li, Zeyu Cui, Shu Wu, Xiaoyu Zhang, and Liang Wang. 2019.

</span>
<span class="ltx_bibblock">Fi-gnn: Modeling feature interactions via graph neural networks for ctr prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">Proceedings of the 28th ACM international conference on information and knowledge management</em>. 539–548.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liao et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Lizi Liao, Ryuichi Takanobu, Yunshan Ma, Xun Yang, Minlie Huang, and Tat-Seng Chua. 2020.

</span>
<span class="ltx_bibblock">Topic-guided relational conversational recommender in multi-domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">IEEE Transactions on Knowledge and Data Engineering</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Text summarization branches out</em>. 74–81.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Zhang (2023)</span>
<span class="ltx_bibblock">
Guo Lin and Yongfeng Zhang. 2023.

</span>
<span class="ltx_bibblock">Sparks of Artificial General Recommender (AGR): Experiments with ChatGPT.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Algorithms</em> 16 (2023), 432.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jianghao Lin, Bo Chen, Hangyu Wang, Yunjia Xi, Yanru Qu, Xinyi Dai, Kangning Zhang, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024.

</span>
<span class="ltx_bibblock">ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Proceedings of the ACM on Web Conference 2024</em>. 3319–3330.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Xiangyang Li, Chenxu Zhu, Huifeng Guo, Yong Yu, Ruiming Tang, et al<span class="ltx_text" id="bib.bib80.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">How Can Recommender Systems Benefit from Large Language Models: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.4.1">arXiv preprint arXiv:2306.05817</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2023b.

</span>
<span class="ltx_bibblock">ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">arXiv preprint arXiv:2308.11131</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Bin Liu, Ruiming Tang, Yingzhi Chen, Jinkai Yu, Huifeng Guo, and Yuzhou Zhang. 2019.

</span>
<span class="ltx_bibblock">Feature generation by convolutional neural network for click-through rate prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">The World Wide Web Conference</em>. 1119–1129.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Dairui Liu, Boming Yang, Honghui Du, Derek Greene, Aonghus Lawlor, Ruihai Dong, and Irene Li. 2023e.

</span>
<span class="ltx_bibblock">RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">arXiv preprint arXiv:2312.10463</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Fan Liu, Yaqi Liu, Zhiyong Cheng, Liqiang Nie, and Mohan Kankanhalli. 2023c.

</span>
<span class="ltx_bibblock">Understanding Before Recommendation: Semantic Aspect-Aware Review Exploitation via Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.3.1">arXiv preprint arXiv:2312.16275</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2023f)</span>
<span class="ltx_bibblock">
Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023f.

</span>
<span class="ltx_bibblock">Pre-train, prompt and recommendation: A comprehensive survey of language modelling paradigm adaptations in recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">arXiv preprint arXiv:2302.03735</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2023b.

</span>
<span class="ltx_bibblock">A First Look at LLM-Powered Generative News Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">arXiv preprint arXiv:2305.06566</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shuchang Liu, Fei Sun, Yingqiang Ge, Changhua Pei, and Yongfeng Zhang. 2021.

</span>
<span class="ltx_bibblock">Variation control and evaluation for generative slate recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">Proceedings of the Web Conference 2021</em>. 436–448.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Weiwen Liu, Qing Liu, Ruiming Tang, Junyang Chen, Xiuqiang He, and Pheng Ann Heng. 2020a.

</span>
<span class="ltx_bibblock">Personalized Re-ranking with Item Relationships for E-commerce. In <em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>. 925–934.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Weiwen Liu, Yunjia Xi, Jiarui Qin, Xinyi Dai, Ruiming Tang, Shuai Li, Weinan Zhang, and Rui Zhang. 2023d.

</span>
<span class="ltx_bibblock">Personalized Diversification for Neural Re-ranking in Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">2023 IEEE 39th International Conference on Data Engineering (ICDE)</em>. IEEE, 802–815.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Weiwen Liu, Yunjia Xi, Jiarui Qin, Fei Sun, Bo Chen, Weinan Zhang, Rui Zhang, and Ruiming Tang. 2022.

</span>
<span class="ltx_bibblock">Neural re-ranking in multi-stage recommender systems: A review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">arXiv preprint arXiv:2202.06602</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yuanxing Liu, Weinan , Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan Feng, Hengbin Cui, Yongbin Li, and Wanxiang Che. 2023a.

</span>
<span class="ltx_bibblock">Conversational Recommender System and Large Language Model Are Made for Each Other in E-commerce Pre-sales Dialogue. In <em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 9587–9605.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Zeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, and Ting Liu. 2020b.

</span>
<span class="ltx_bibblock">Towards conversational recommendation over multi-type dialogs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">arXiv preprint arXiv:2005.03954</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loepp et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Benedikt Loepp, Tim Hussein, and Jüergen Ziegler. 2014.

</span>
<span class="ltx_bibblock">Choice-based preference elicitation for collaborative filtering recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em>. 3085–3094.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kai Luo, Hojin Yang, Ga Wu, and Scott Sanner. 2020.

</span>
<span class="ltx_bibblock">Deep critiquing for VAE-based recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</em>. 1269–1278.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sichun Luo, Yuxuan Yao, Bowei He, Yinya Huang, Aojun Zhou, Xinyi Zhang, Yuanzhang Xiao, Mingjie Zhan, and Linqi Song. 2024.

</span>
<span class="ltx_bibblock">Integrating Large Language Models into Recommendation via Mutual Augmentation and Adaptive Aggregation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.13870 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib96.2.2.1">.</span> (2018b)</span>
<span class="ltx_bibblock">
Jiaqi Ma, Zhe Zhao, Xinyang Yi, Jilin Chen, Lichan Hong, and Ed H Chi. 2018b.

</span>
<span class="ltx_bibblock">Modeling task relationships in multi-task learning with multi-gate mixture-of-experts. In <em class="ltx_emph ltx_font_italic" id="bib.bib96.3.1">SIGKDD</em>. 1930–1939.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2018a)</span>
<span class="ltx_bibblock">
Xiao Ma, Liqin Zhao, Guan Huang, Zhi Wang, Zelin Hu, Xiaoqiang Zhu, and Kun Gai. 2018a.

</span>
<span class="ltx_bibblock">Entire space multi-task model: An effective approach for estimating post-click conversion rate. In <em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval</em>. 1137–1140.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McInerney et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
James McInerney, Brian Brost, Praveen Chandar, Rishabh Mehrotra, and Benjamin Carterette. 2020.

</span>
<span class="ltx_bibblock">Counterfactual evaluation of slate recommendations with sequential reward interactions. In <em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 1779–1788.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Murakhovs’ka et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lidiya Murakhovs’ka, Philippe , Tian Xie, Caiming , and Chien-Sheng Wu. 2023.

</span>
<span class="ltx_bibblock">Salespeople vs SalesBot: Exploring the Role of Educational Value in Conversational Recommender Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. 9823–9838.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mysore et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sheshera Mysore, Andrew Mccallum, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Large Language Model Augmented Narrative Driven Recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib100.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em> <em class="ltx_emph ltx_font_italic" id="bib.bib100.4.2">(RecSys ’23)</em>. 777–783.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">ArXiv</em> abs/2303.08774 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation. In <em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>. 311–318.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pei et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Changhua Pei, Yi Zhang, Yongfeng Zhang, Fei Sun, Xiao Lin, Hanxiao Sun, Jian Wu, Peng Jiang, Junfeng Ge, Wenwu Ou, et al<span class="ltx_text" id="bib.bib103.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Personalized re-ranking for recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib103.4.1">Proceedings of the 13th ACM conference on recommender systems</em>. 3–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peng et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenjun Peng, Guiyang Li, Yue Jiang, Zilong Wang, Dan Ou, Xiaoyi Zeng, Enhong Chen, et al<span class="ltx_text" id="bib.bib104.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Large Language Model based Long-tail Query Rewriting in Taobao Search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.4.1">arXiv preprint arXiv:2311.03758</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pi et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020.

</span>
<span class="ltx_bibblock">Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>. 2685–2692.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui Zhang, Yong Yu, and Weinan Zhang. 2022.

</span>
<span class="ltx_bibblock">Rankflow: Joint optimization of multi-stage cascade ranking systems as flows. In <em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 814–824.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xubo Qin, Zhicheng Dou, and Ji-Rong Wen. 2020.

</span>
<span class="ltx_bibblock">Diversifying search results using self-attention network. In <em class="ltx_emph ltx_font_italic" id="bib.bib107.3.1">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</em>. 1265–1274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016.

</span>
<span class="ltx_bibblock">Product-based neural networks for user response prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">2016 IEEE 16th international conference on data mining (ICDM)</em>. IEEE, 1149–1154.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al<span class="ltx_text" id="bib.bib109.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.4.1">OpenAI blog</em> 1, 8 (2019), 9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">J. Mach. Learn. Res.</em> 21 (jan 2020), 67 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajput et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shashank Rajput, Nikhil Mehta, Anima Singh, Raghunandan H Keshavan, Trung Vu, Lukasz Heldt, Lichan Hong, Yi Tay, Vinh Q Tran, Jonah Samost, et al<span class="ltx_text" id="bib.bib111.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Recommender Systems with Generative Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.4.1">arXiv preprint arXiv:2305.05065</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravaut et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mathieu Ravaut, Hao Zhang, Lu Xu, Aixin Sun, and Yong Liu. 2024.

</span>
<span class="ltx_bibblock">Parameter-Efficient Conversational Recommender System as a Language Processing Task. In <em class="ltx_emph ltx_font_italic" id="bib.bib112.3.1">Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. Association for Computational Linguistics, St. Julian’s, Malta, 152–165.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xuhui Ren, Tong Chen, Quoc Viet Hung Nguyen, Li zhen Cui, Zi-Liang Huang, and Hongzhi Yin. 2023.

</span>
<span class="ltx_bibblock">Explicit Knowledge Graph Reasoning for Conversational Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.3.1">ArXiv</em> abs/2305.00783 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Runfeng et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai, et al<span class="ltx_text" id="bib.bib114.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Lkpnr: Llm and kg for personalized news recommendation framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.4.1">arXiv preprint arXiv:2308.12028</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sepliarskaia et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Anna Sepliarskaia, Julia Kiseleva, Filip Radlinski, and Maarten de Rijke. 2018.

</span>
<span class="ltx_bibblock">Preference elicitation as an optimization problem. In <em class="ltx_emph ltx_font_italic" id="bib.bib115.3.1">Proceedings of the 12th ACM Conference on Recommender Systems</em>. 172–180.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shapira et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Bracha Shapira, Lior Rokach, and Shirley Freilikhman. 2013.

</span>
<span class="ltx_bibblock">Facebook single and cross domain data for recommendation systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">User Modeling and User-Adapted Interaction</em> 23 (2013), 211–247.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sheng et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Xiang-Rong Sheng, Liqin Zhao, Guorui Zhou, Xinyao Ding, Binding Dai, Qiang Luo, Siran Yang, Jingshan Lv, Chi Zhang, Hongbo Deng, et al<span class="ltx_text" id="bib.bib117.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">One model to serve all: Star topology adaptive recommender for multi-domain ctr prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib117.4.1">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</em>. 4104–4113.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kaize Shi, Xueyao Sun, Dingxian Wang, Yinlin Fu, Guandong Xu, and Qing Li. 2023.

</span>
<span class="ltx_bibblock">LLaMA-E: Empowering E-commerce Authoring with Multi-Aspect Instruction Following.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.3.1">arXiv preprint arXiv:2308.04913</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yubo Shu, Haonan Zhang, Hansu Gu, Peng Zhang, Tun Lu, Dongsheng Li, and Ning Gu. 2023.

</span>
<span class="ltx_bibblock">RAH! RecSys-Assistant-Human: A Human-Centered Recommendation Framework with LLM Agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">ArXiv</em> abs/2308.09904 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smyth and McGinty (2003)</span>
<span class="ltx_bibblock">
Barry Smyth and Lorraine McGinty. 2003.

</span>
<span class="ltx_bibblock">An analysis of feedback strategies in conversational recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">the Fourteenth Irish Artificial Intelligence and Cognitive Science Conference (AICS 2003)</em>. Citeseer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smyth et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Barry Smyth, Lorraine McGinty, James Reilly, and Kevin McCarthy. 2004.

</span>
<span class="ltx_bibblock">Compound critiques for conversational recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib121.3.1">IEEE/WIC/ACM International Conference on Web Intelligence (WI’04)</em>. IEEE, 145–151.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Junshuai Song, Zhao Li, Chang Zhou, Jinze Bai, Zhenpeng Li, Jian Li, and Jun Gao. 2020.

</span>
<span class="ltx_bibblock">Co-displayed items aware list recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib122.3.1">IEEE Access</em> 8 (2020), 64591–64602.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang, and Jian Tang. 2019.

</span>
<span class="ltx_bibblock">Autoint: Automatic feature interaction learning via self-attentive neural networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib123.3.1">Proceedings of the 28th ACM international conference on information and knowledge management</em>. 1161–1170.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spurlock et al<span class="ltx_text" id="bib.bib124.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Kyle Dylan Spurlock, Cagla Acun, Esin Saka, and Olfa Nasraoui. 2024.

</span>
<span class="ltx_bibblock">ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib124.3.1">ArXiv</em> abs/2401.03605 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib125.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang. 2019.

</span>
<span class="ltx_bibblock">BERT4Rec: Sequential recommendation with bidirectional encoder representations from transformer. In <em class="ltx_emph ltx_font_italic" id="bib.bib125.3.1">Proceedings of the 28th ACM international conference on information and knowledge management</em>. 1441–1450.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun and Zhang (2018)</span>
<span class="ltx_bibblock">
Yueming Sun and Yi Zhang. 2018.

</span>
<span class="ltx_bibblock">Conversational recommender system. In <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">The 41st international acm sigir conference on research &amp; development in information retrieval</em>. 235–244.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib127.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhu Sun, Hongyang Liu, Xinghua Qu, Kaidong Feng, Yan Wang, and Yew-Soon Ong. 2023.

</span>
<span class="ltx_bibblock">Large Language Models for Intent-Driven Session Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib127.3.1">arXiv preprint arXiv:2312.07552</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span class="ltx_text" id="bib.bib128.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Hongyan Tang, Junning Liu, Ming Zhao, and Xudong Gong. 2020.

</span>
<span class="ltx_bibblock">Progressive layered extraction (ple): A novel multi-task learning (mtl) model for personalized recommendations. In <em class="ltx_emph ltx_font_italic" id="bib.bib128.3.1">RecSys</em>. 269–278.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zuoli Tang, Zhaoxin Huan, Zihao Li, Xiaolu Zhang, Jun Hu, Chilin Fu, Jun Zhou, and Chenliang Li. 2023.

</span>
<span class="ltx_bibblock">One Model for All: Large Language Models are Domain-Agnostic Recommendation Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib129.3.1">arXiv preprint arXiv:2310.14304</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tian et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhen Tian, Changwang Zhang, Wayne Xin Zhao, Xin Zhao, Ji-Rong Wen, and Zhao Cao. 2023.

</span>
<span class="ltx_bibblock">UFIN: Universal Feature Interaction Network for Multi-Domain Click-Through Rate Prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">arXiv preprint arXiv:2311.15493</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Torbati et al<span class="ltx_text" id="bib.bib131.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ghazaleh Haratinezhad Torbati, Anna Tigunova, Andrew Yates, and Gerhard Weikum. 2023.

</span>
<span class="ltx_bibblock">Recommendations by Concise User Profiles from Review Text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.3.1">arXiv preprint arXiv:2311.01314</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib132.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al<span class="ltx_text" id="bib.bib132.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.4.1">arXiv preprint arXiv:2302.13971</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vendrov et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Ivan Vendrov, Tyler Lu, Qingqing Huang, and Craig Boutilier. 2020.

</span>
<span class="ltx_bibblock">Gradient-based optimization for bayesian preference elicitation. In <em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 34. 10292–10301.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Viappiani et al<span class="ltx_text" id="bib.bib134.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Paolo Viappiani, Pearl Pu, and Boi Faltings. 2007.

</span>
<span class="ltx_bibblock">Conversational recommenders with adaptive suggestions. In <em class="ltx_emph ltx_font_italic" id="bib.bib134.3.1">Proceedings of the 2007 ACM conference on Recommender systems</em>. 89–96.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Fan Wang, Xiaomin Fang, Lihang Liu, Yaxue Chen, Jiucheng Tao, Zhiming Peng, Cihang Jin, and Hao Tian. 2019.

</span>
<span class="ltx_bibblock">Sequential evaluation and generation framework for combinatorial recommender system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.3.1">arXiv preprint arXiv:1902.00245</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2018b)</span>
<span class="ltx_bibblock">
Hongwei Wang, Fuzheng Zhang, Jialin Wang, Miao Zhao, Wenjie Li, Xing Xie, and Minyi Guo. 2018b.

</span>
<span class="ltx_bibblock">Ripplenet: Propagating user preferences on the knowledge graph for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib136.3.1">Proceedings of the 27th ACM international conference on information and knowledge management</em>. 417–426.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al<span class="ltx_text" id="bib.bib137.3.1">.</span> 2023d.

</span>
<span class="ltx_bibblock">A survey on large language model based autonomous agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib137.4.1">arXiv preprint arXiv:2308.11432</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2023f)</span>
<span class="ltx_bibblock">
Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Jun Xu, Zhicheng Dou, Jun Wang, and Ji-Rong Wen. 2023f.

</span>
<span class="ltx_bibblock">When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">arXiv:2306.02552 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2022)</span>
<span class="ltx_bibblock">
Pengda Wang. 2022.

</span>
<span class="ltx_bibblock">Recommendation algorithm in TikTok: Strengths, dilemmas, and possible directions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.1.1">Int’l J. Soc. Sci. Stud.</em> 10 (2022), 60.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib140.2.2.1">.</span> (2018a)</span>
<span class="ltx_bibblock">
Qing Wang, Chunqiu Zeng, Wubai Zhou, Tao Li, S Sitharama Iyengar, Larisa Shwartz, and Genady Ya Grabarnik. 2018a.

</span>
<span class="ltx_bibblock">Online interactive collaborative filtering using multi-armed bandit with dependent arms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib140.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 31, 8 (2018), 1569–1580.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib141.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017.

</span>
<span class="ltx_bibblock">Deep &amp; cross network for ad click predictions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib141.3.1">Proceedings of the ADKDD’17</em>. 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib142.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong, and Ed Chi. 2021.

</span>
<span class="ltx_bibblock">Dcn v2: Improved deepfm &amp; cross network and practical lessons for web-scale learning to rank systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib142.3.1">Proceedings of the web conference 2021</em>. 1785–1797.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib143.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, and Tat seng Chua. 2023b.

</span>
<span class="ltx_bibblock">Generative Recommendation: Towards Next-generation Recommender Paradigm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib143.3.1">ArXiv</em> abs/2304.03516 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib144.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, and Ji rong Wen. 2023e.

</span>
<span class="ltx_bibblock">Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.3.1">ArXiv</em> abs/2305.13112 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yancheng Wang, Ziyan Jiang, Zheng Chen, Fan Yang, Yingxue Zhou, Eunah Cho, Xing Fan, Xiaojiang Huang, Yanbin Lu, and Yingzhen Yang. 2023a.

</span>
<span class="ltx_bibblock">RecMind: Large Language Model Powered Agent For Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib145.3.1">ArXiv</em> abs/2308.14296 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Yu Wang, Zhiwei Liu, Jianguo Zhang, Weiran Yao, Shelby Heinecke, and Philip S Yu. 2023c.

</span>
<span class="ltx_bibblock">DRDT: Dynamic Reflection with Divergent Thinking for LLM-based Sequential Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">arXiv preprint arXiv:2312.11336</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al<span class="ltx_text" id="bib.bib147.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib147.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weng (2023)</span>
<span class="ltx_bibblock">
Lilian Weng. 2023.

</span>
<span class="ltx_bibblock">LLM-powered Autonomous Agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">lilianweng.github.io</em> (Jun 2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lilianweng.github.io/posts/2023-06-23-agent/" title="">https://lilianweng.github.io/posts/2023-06-23-agent/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wooldridge (2009)</span>
<span class="ltx_bibblock">
Michael Wooldridge. 2009.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">An introduction to multiagent systems</em>.

</span>
<span class="ltx_bibblock">John wiley &amp; sons.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib150.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ga Wu, Kai Luo, Scott Sanner, and Harold Soh. 2019.

</span>
<span class="ltx_bibblock">Deep language-based critiquing for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib150.3.1">Proceedings of the 13th ACM Conference on Recommender Systems</em>. 137–145.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib151.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Jiahao Wu, Qijiong Liu, Hengchang Hu, Wenqi Fan, Shengcai Liu, Qing Li, Xiao-Ming Wu, and Ke Tang. 2023a.

</span>
<span class="ltx_bibblock">Leveraging Large Language Models (LLMs) to Empower Training-Free Dataset Condensation for Content-Based Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib151.3.1">arXiv preprint arXiv:2310.09874</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib152.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al<span class="ltx_text" id="bib.bib152.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">A Survey on Large Language Models for Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib152.4.1">arXiv preprint arXiv:2305.19860</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib153.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yunjia Xi, Weiwen Liu, Xinyi Dai, Ruiming Tang, Weinan Zhang, Qing Liu, Xiuqiang He, and Yong Yu. 2021.

</span>
<span class="ltx_bibblock">Context-aware reranking with utility maximization for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib153.3.1">arXiv preprint arXiv:2110.09059</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib154.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Yunjia Xi, Weiwen Liu, Jianghao Lin, Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, and Yong Yu. 2023b.

</span>
<span class="ltx_bibblock">Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.3.1">arXiv preprint arXiv:2306.10933</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib155.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al<span class="ltx_text" id="bib.bib155.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">The rise and potential of large language model based agents: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib155.4.1">arXiv preprint arXiv:2309.07864</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xian et al<span class="ltx_text" id="bib.bib156.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yikun Xian, Zuohui Fu, Shan Muthukrishnan, Gerard De Melo, and Yongfeng Zhang. 2019.

</span>
<span class="ltx_bibblock">Reinforcement knowledge graph reasoning for explainable recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib156.3.1">Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval</em>. 285–294.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib157.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Ruobing Xie, Shaoliang Zhang, Rui Wang, Feng Xia, and Leyu Lin. 2021.

</span>
<span class="ltx_bibblock">Hierarchical reinforcement learning for integrated recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib157.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 35. 4521–4528.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib158.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Hu Xu, Seungwhan Moon, Honglei Liu, Bing Liu, Pararth Shah, and Philip S Yu. 2020.

</span>
<span class="ltx_bibblock">User memory reasoning for conversational recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.3.1">arXiv preprint arXiv:2006.00184</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib159.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Lanling Xu, Junjie Zhang, Bingqian Li, Jinpeng Wang, Mingchen Cai, Wayne Xin Zhao, and Ji-Rong Wen. 2024.

</span>
<span class="ltx_bibblock">Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib159.3.1">ArXiv</em> abs/2401.04997 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib160.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Bencheng Yan, Pengjie Wang, Kai Zhang, Feng Li, Hongbo Deng, Jian Xu, and Bo Zheng. 2022.

</span>
<span class="ltx_bibblock">Apg: Adaptive parameter generation network for click-through rate prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib160.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 24740–24752.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib161.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Le Yan, Zhen Qin, Rama Kumar Pasumarthi, Xuanhui Wang, and Michael Bendersky. 2021.

</span>
<span class="ltx_bibblock">Diversification-aware learning to rank using distributed representation. In <em class="ltx_emph ltx_font_italic" id="bib.bib161.3.1">Proceedings of the Web Conference 2021</em>. 127–136.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib162.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shenghao Yang, Chenyang Wang, Yankai Liu, Kangping Xu, Weizhi Ma, Yiqun Liu, Min Zhang, Haitao Zeng, Junlan Feng, and Chao Deng. 2023.

</span>
<span class="ltx_bibblock">Collaborative Word-based Pre-trained Item Representation for Transferable Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib162.3.1">arXiv preprint arXiv:2311.10501</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib163.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022.

</span>
<span class="ltx_bibblock">Webshop: Towards scalable real-world web interaction with grounded language agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib163.3.1">Advances in Neural Information Processing Systems</em> 35 (2022), 20744–20757.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib164.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik R Narasimhan. 2023a.

</span>
<span class="ltx_bibblock">Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib164.3.1">Thirty-seventh Conference on Neural Information Processing Systems</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib165.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023b.

</span>
<span class="ltx_bibblock">ReAct: Synergizing Reasoning and Acting in Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib165.3.1">The Eleventh International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ying et al<span class="ltx_text" id="bib.bib166.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec. 2018.

</span>
<span class="ltx_bibblock">Graph convolutional neural networks for web-scale recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib166.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 974–983.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib167.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Enming Yuan, Wei Guo, Zhicheng He, Huifeng Guo, Chengkai Liu, and Ruiming Tang. 2022.

</span>
<span class="ltx_bibblock">Multi-behavior sequential transformer recommender. In <em class="ltx_emph ltx_font_italic" id="bib.bib167.3.1">Proceedings of the 45th international ACM SIGIR conference on research and development in information retrieval</em>. 1642–1652.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib168.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhenrui Yue, Sara Rabhi, Gabriel de Souza Pereira Moreira, Dong Wang, and Even Oldridge. 2023.

</span>
<span class="ltx_bibblock">LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib168.3.1">arXiv preprint arXiv:2311.02089</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib169.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al<span class="ltx_text" id="bib.bib169.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Glm-130b: An open bilingual pre-trained model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib169.4.1">arXiv preprint arXiv:2210.02414</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai et al<span class="ltx_text" id="bib.bib170.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, and Yu Shi. 2024.

</span>
<span class="ltx_bibblock">Actions Speak Louder than Words: Trillion-Parameter Sequential Transducers for Generative Recommendations.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2402.17152 [cs.LG]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib171.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
An Zhang, Leheng Sheng, Yuxin Chen, Hao Li, Yang Deng, Xiang Wang, and Tat-Seng Chua. 2023b.

</span>
<span class="ltx_bibblock">On Generative Agents in Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib171.3.1">arXiv preprint arXiv:2310.10108</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib172.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Qianqian Zhang, Xinru Liao, Quan Liu, Jian Xu, and Bo Zheng. 2022a.

</span>
<span class="ltx_bibblock">Leaving no one behind: A multi-scenario multi-task meta learning approach for advertiser modeling. In <em class="ltx_emph ltx_font_italic" id="bib.bib172.3.1">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</em>. 1368–1376.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Balog (2020)</span>
<span class="ltx_bibblock">
Shuo Zhang and Krisztian Balog. 2020.

</span>
<span class="ltx_bibblock">Evaluating conversational recommender systems via user simulation. In <em class="ltx_emph ltx_font_italic" id="bib.bib173.1.1">Proceedings of the 26th acm sigkdd international conference on knowledge discovery &amp; data mining</em>. 1512–1520.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib174.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Weinan Zhang, Jiarui Qin, Wei Guo, Ruiming Tang, and Xiuqiang He. 2021.

</span>
<span class="ltx_bibblock">Deep learning for click-through rate estimation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib174.3.1">arXiv preprint arXiv:2104.10584</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib175.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiaoying Zhang, Hong Xie, Hang Li, and John CS Lui. 2020.

</span>
<span class="ltx_bibblock">Conversational contextual bandit: Algorithm and application. In <em class="ltx_emph ltx_font_italic" id="bib.bib175.3.1">Proceedings of the web conference 2020</em>. 662–672.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib176.2.2.1">.</span> (2022c)</span>
<span class="ltx_bibblock">
Xuanyu Zhang, Qing Yang, and Dongliang Xu. 2022c.

</span>
<span class="ltx_bibblock">Deepvt: Deep view-temporal interaction network for news recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib176.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 2640–2650.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib177.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Yongfeng Zhang, Xu Chen, Qingyao Ai, Liu Yang, and W Bruce Croft. 2018.

</span>
<span class="ltx_bibblock">Towards conversational search and recommendation: System ask, user respond. In <em class="ltx_emph ltx_font_italic" id="bib.bib177.3.1">Proceedings of the 27th acm international conference on information and knowledge management</em>. 177–186.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib178.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, and Xiangnan He. 2023a.

</span>
<span class="ltx_bibblock">Collm: Integrating collaborative embeddings into large language models for recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib178.3.1">arXiv preprint arXiv:2310.19488</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib179.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yuanliang Zhang, Xiaofeng Wang, Jinxin Hu, Ke Gao, Chenyi Lei, and Fei Fang. 2022b.

</span>
<span class="ltx_bibblock">Scenario-Adaptive and Self-Supervised Model for Multi-Scenario Personalized Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib179.3.1">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</em>. 3674–3683.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib180.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Xiangyu Zhao, Liang Zhang, Zhuoye Ding, Long Xia, Jiliang Tang, and Dawei Yin. 2018.

</span>
<span class="ltx_bibblock">Recommendations with negative feedback via pairwise deep reinforcement learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib180.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 1040–1048.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib181.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Xiaoxue Zhao, Weinan Zhang, and Jun Wang. 2013.

</span>
<span class="ltx_bibblock">Interactive collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib181.3.1">Proceedings of the 22nd ACM international conference on Information &amp; Knowledge Management</em>. 1411–1420.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib182.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xiangyu Zhao, Xudong Zheng, Xiwang Yang, Xiaobing Liu, and Jiliang Tang. 2020.

</span>
<span class="ltx_bibblock">Jointly learning to recommend and advertise. In <em class="ltx_emph ltx_font_italic" id="bib.bib182.3.1">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</em>. 3319–3327.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib183.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Guorui Zhou, Weijie Bian, Kailun Wu, Lejian Ren, Qi Pi, Yujing Zhang, Can Xiao, Xiang-Rong Sheng, Na Mou, Xinchen Luo, et al<span class="ltx_text" id="bib.bib183.3.1">.</span> 2020a.

</span>
<span class="ltx_bibblock">CAN: revisiting feature co-action for click-through rate prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib183.4.1">arXiv preprint arXiv:2011.05625</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib184.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018.

</span>
<span class="ltx_bibblock">Deep interest network for click-through rate prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib184.3.1">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 1059–1068.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib185.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jie Zhou, Xianshuai Cao, Wenhao Li, Lin Bo, Kun Zhang, Chuan Luo, and Qian Yu. 2023.

</span>
<span class="ltx_bibblock">HiNet: Novel Multi-Scenario &amp; Multi-Task Learning with Hierarchical Information Extraction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib185.3.1">arXiv preprint arXiv:2303.06095</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib186.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang, and Ji-Rong Wen. 2020b.

</span>
<span class="ltx_bibblock">Towards topic-guided conversational recommender system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib186.3.1">arXiv preprint arXiv:2010.04125</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib187.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2021.

</span>
<span class="ltx_bibblock">Open benchmarking for click-through rate prediction. In <em class="ltx_emph ltx_font_italic" id="bib.bib187.3.1">Proceedings of the 30th ACM international conference on information &amp; knowledge management</em>. 2759–2769.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib188.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yaochen Zhu, Liang Wu, Qilnli Guo, Liangjie Hong, and Jundong Li. 2023.

</span>
<span class="ltx_bibblock">Collaborative Large Language Model for Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib188.3.1">ArXiv</em> abs/2311.01343 (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al<span class="ltx_text" id="bib.bib189.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Tao Zhuang, Wenwu Ou, and Zhirong Wang. 2018.

</span>
<span class="ltx_bibblock">Globally optimized mutual influence aware ranking in e-commerce search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib189.3.1">arXiv preprint arXiv:1805.08524</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al<span class="ltx_text" id="bib.bib190.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Lixin Zou, Long Xia, Pan Du, Zhuo Zhang, Ting Bai, Weidong Liu, Jian-Yun Nie, and Dawei Yin. 2020.

</span>
<span class="ltx_bibblock">Pseudo Dyna-Q: A reinforcement learning framework for interactive recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib190.3.1">Proceedings of the 13th International Conference on Web Search and Data Mining</em>. 816–824.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Jul 14 04:52:08 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
