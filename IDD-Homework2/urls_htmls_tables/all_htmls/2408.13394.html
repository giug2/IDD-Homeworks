<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.13394] Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline</title><meta property="og:description" content="The increasing adoption of human-robot interaction presents opportunities for technology to positively impact lives, particularly those with visual impairments, through applications such as guide-dog-like assistive rob…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.13394">

<!--Generated on Thu Sep  5 17:46:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_font_bold ltx_title_document" style="font-size:173%;">Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Adam Scicluna, Cedric Le Gentil, Sheila Sutjipto and Gavin Paul
</span><span class="ltx_author_notes">All authors are with the Robotics Institute, Faculty of Engineering and Information Technology, University of Technology Sydney (UTS), Australia. Corresponding author: <span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:80%;">{adam.scicluna@alumni.uts.edu.au}</span>Cedric Le Gentil is supported by the Australian Research Council Discovery Project under Grant DP210101336. Sheila Sutjipto is supported by Australian Government Research Training Program Scholarships. The authors would like to acknowledge the support from the ARC Industrial Transformation Training Centre (ITTC) for Collaborative Robotics in Advanced Manufacturing under grant IC200100001.© 2024 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id2.id1" class="ltx_p">The increasing adoption of human-robot interaction presents opportunities for technology to positively impact lives, particularly those with visual impairments, through applications such as guide-dog-like assistive robotics. We present a pipeline exploring the perception and “intelligent disobedience” required by such a system. A dataset of two people moving in and out of view has been prepared to compare RGB-based and event-based multi-modal dynamic object detection using LiDAR data for 3D position localisation. Our analysis highlights challenges in accurate 3D localisation using 2D image-LiDAR fusion, indicating the need for further refinement. Compared to the performance of the frame-based detection algorithm utilised (YOLOv4), current cutting-edge event-based detection models appear limited to contextual scenarios, such as for automotive platforms. This is highlighted by weak precision and recall over varying confidence and Intersection over Union (IoU) thresholds when using frame-based detections as a ground truth. Therefore, we have publicly released this dataset to the community, containing RGB, event, point cloud and Inertial Measurement Unit (IMU) data along with ground truth poses for the two people in the scene to fill a gap in the current landscape of publicly available datasets and provide a means to assist in the development of safer and more robust algorithms in the future: <a target="_blank" href="https://uts-ri.github.io/revel/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://uts-ri.github.io/revel/</a>.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Training guide dogs requires significant time and financial resources <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, with only about half of the dogs successfully completing the programs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. This creates a gap between the availability of guide dogs and the needs of visually impaired individuals. Thus, there is growing interest in cost-effective robotic alternatives <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. These robotic substitutes must replicate the “intelligent disobedience” of guide dogs, where the robot refuses unsafe commands based on its understanding of an environment. Therefore, reliable perception and decision-making algorithms are necessary to ensure user safety.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><svg id="S1.F1.pic1" class="ltx_picture ltx_centering" height="391.97" overflow="visible" version="1.1" width="598"><g transform="translate(0,391.97) matrix(1 0 0 -1 0 0) translate(0,14.97)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 0 0)" fill="#000000" stroke="#000000"><foreignObject width="598" height="377" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/figures/teaser_cropped.jpg" id="S1.F1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="377" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 70.65 -10.35)" fill="#000000" stroke="#000000"><foreignObject width="110.7" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S1.F1.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:80.0pt;">
<span id="S1.F1.pic1.2.2.2.1.1.1" class="ltx_p"></span>
<span id="S1.F1.pic1.2.2.2.1.1.2" class="ltx_p"><span id="S1.F1.pic1.2.2.2.1.1.2.1" class="ltx_text" style="font-size:70%;">(a) Sensor suite</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 376.65 103.82)" fill="#000000" stroke="#000000"><foreignObject width="110.7" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S1.F1.pic1.3.3.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:80.0pt;">
<span id="S1.F1.pic1.3.3.3.1.1.1" class="ltx_p"></span>
<span id="S1.F1.pic1.3.3.3.1.1.2" class="ltx_p"><span id="S1.F1.pic1.3.3.3.1.1.2.1" class="ltx_text" style="font-size:70%;">(b) Vision data sample</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 376.65 -10.35)" fill="#000000" stroke="#000000"><foreignObject width="110.7" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S1.F1.pic1.4.4.4.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:80.0pt;">
<span id="S1.F1.pic1.4.4.4.1.1.1" class="ltx_p"></span>
<span id="S1.F1.pic1.4.4.4.1.1.2" class="ltx_p"><span id="S1.F1.pic1.4.4.4.1.1.2.1" class="ltx_text" style="font-size:70%;">(c) Geometric data sample</span></span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>(a) Sensor suite featuring a DAVIS 346 frame-event camera and a Cube1 LiDAR for dataset collection. (b) DAVIS camera data sample: events (polarity-coloured in red or blue) overlaid on the RGB frame. (c) LiDAR scan sample with object motion-captured ground truth poses (frames).</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Perception and scene understanding are essential capabilities for robotic systems to be integrated into daily life.
Such systems must robustly comprehend their surroundings in real-time for subsequent decision-making and actions toward safe operations.
In the context of guide-dog-like assistive robotics, this translates into the accurate detection and identification of both static and dynamic objects such as cars, bicycles, pedestrians, etc, and the ability to estimate their 3D pose and dynamics. This would enable the robot to navigate and avoid hazards, enhancing safety and usability.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Recent machine learning advances and GPU availability have enabled efficient context and scene recognition from RGB images using neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
Yet, controlling movement or manipulation demands 3D pose knowledge. Although RGBD and stereo cameras provide accurate depth information, their low dynamic range and susceptibility to motion blur, along with RGBD cameras’ poor outdoor performance due to sunlight’s infrared radiation, pose challenges in safety-critical, dynamic or brightly lit environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. To address this, LiDAR-camera multi-modal sensor suites are used, though LiDAR’s sparsity, noise, and slow acquisition rate complicate detection.
Thus, RGB-LiDAR systems still struggle with standard cameras’ limitations in scenarios under high dynamic range or low light conditions.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Event-based cameras <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> offer a solution to the limitations of traditional frame-based vision by capturing pixel-level changes in illumination independently. However, object detection methods for event cameras are still in their early stages compared to those for RGB data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. To fully exploit the benefits of event cameras, developing new algorithms tailored to their unique data output is crucial.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Early RGB-based object detection used handcrafted features and classic machine learning but struggled with variability and extensive parameter tuning. CNNs revolutionised the field with superior performance in managing data variation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, greatly aided by extensively labelled public datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, where millions of images across thousands of categories provide a crucial benchmark for performance evaluation. Tailored datasets like KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for autonomous driving have also been pivotal in advancing algorithm development. Prominent approaches include R-CNN and its iterations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, where selective searches generate regions classified by a CNN. Recent efforts have focused on improving efficiency evident with YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> and single shot detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.
Works have also combined object detection with semantic segmentation to enhance scene awareness and object analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Frame-based algorithms do not translate well for use with event camera data. Consequently, efforts have been made to reconstruct dense greyscale images from sparse event data to feed to a CNN, introducing an intermediate step that increases the computational burden and latency. Alternatively, CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, spiking neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and graph neural networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> have demonstrated object detection in the event space. Recently, transformers like RVTs have achieved state-of-the-art performance on the Gen1 and 1-Mpx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. While promising, these methods lack the accessibility of algorithms like YOLO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Furthermore, event camera-based techniques struggle to generalise to other scenes due to the limited variety in existing datasets.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Leveraging the complementary nature of LiDAR point clouds and RGB images enables a more comprehensive scene understanding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. For object detection, strategies include using RGB-trained networks on image-like data generated from LiDAR data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, constructing pseudo-LiDAR data from RGB images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, using 2D detectors to propose 3D search spaces <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, and employing RVTs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>. Fusing LiDAR and camera data has proven effective for semantic segmentation, using methods like mapping LiDAR points to the output of an image-based semantic segmentation network and inputting the data into a LiDAR detector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, and addressing sparsity with cylindrical partitioning and asymmetrical 3D CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">To our knowledge, no publicly available dataset contains data from an event camera, an RGB camera, a LiDAR, and an IMU, while providing ground truth poses of the sensor suite and dynamic objects.
In this paper, we introduce a labelled dataset and propose a multi-modal perception pipeline for 3D object detection and spatial pose estimation that combines a 2D detection step (based on RGB or event vision) and a depth estimation step using LiDAR data.
Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows our sensor and some data samples.
We evaluate the performance using the event and RGB camera, highlighting the potential and challenges for future robotic guide-dog systems.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Dataset</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Sensor suite and data collection</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The dataset introduced in this paper is collected indoors with a handheld sensor suite moving in the field of view of a <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">Vicon</span> motion-capture system.
Two people, also tracked by the motion-capture system, are moving in and out of the sensor suite field of view.
The sensor suite consists of:</p>
<ul id="S2.I1" class="ltx_itemize">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.4" class="ltx_p"><span id="S2.I1.i1.p1.4.1" class="ltx_text ltx_font_bold">Inivation DAVIS346</span> event camera:
<span id="S2.I1.i1.p1.4.2" class="ltx_text ltx_font_italic">Stream of event</span> data (up to 1MHz) with each event being a tuple of <math id="S2.I1.i1.p1.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="S2.I1.i1.p1.1.m1.1a"><mi id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><ci id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">x</annotation></semantics></math> and <math id="S2.I1.i1.p1.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="S2.I1.i1.p1.2.m2.1a"><mi id="S2.I1.i1.p1.2.m2.1.1" xref="S2.I1.i1.p1.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.1b"><ci id="S2.I1.i1.p1.2.m2.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.1c">y</annotation></semantics></math> positions in the image space, <math id="S2.I1.i1.p1.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.I1.i1.p1.3.m3.1a"><mi id="S2.I1.i1.p1.3.m3.1.1" xref="S2.I1.i1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.3.m3.1b"><ci id="S2.I1.i1.p1.3.m3.1.1.cmml" xref="S2.I1.i1.p1.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.3.m3.1c">t</annotation></semantics></math> the timestamp, and <math id="S2.I1.i1.p1.4.m4.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S2.I1.i1.p1.4.m4.1a"><mi id="S2.I1.i1.p1.4.m4.1.1" xref="S2.I1.i1.p1.4.m4.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.4.m4.1b"><ci id="S2.I1.i1.p1.4.m4.1.1.cmml" xref="S2.I1.i1.p1.4.m4.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.4.m4.1c">p</annotation></semantics></math> the polarity of the corresponding illumination change;
<span id="S2.I1.i1.p1.4.3" class="ltx_text ltx_font_italic">RGB images</span> at 23Hz;
<span id="S2.I1.i1.p1.4.4" class="ltx_text ltx_font_italic">6-DoF IMU</span> at 1kHz (3-axis gyroscope and 3-axis accelerometer).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Blickfeld Cube1</span> LiDAR: <span id="S2.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">3D point clouds</span> at 7.9Hz with point-wise timestamps.</p>
</div>
</li>
</ul>
<p id="S2.SS1.p1.2" class="ltx_p">All the sensor’s measurements and the output of the motion-capture system are recorded with ROS.
We use the <span id="S2.SS1.p1.2.1" class="ltx_text ltx_font_italic">rpg_dvs_ros</span> driver for the DVS camera and the Blickfeld ROS driver for the LiDAR.
As illustrated in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the sensor suite is equipped with a set of reflective markers tracked by the <span id="S2.SS1.p1.2.2" class="ltx_text ltx_font_italic">Vicon</span> system.
Similarly, the people moving in the surroundings wear helmets with reflective markers.
Subsequently, the <span id="S2.SS1.p1.2.3" class="ltx_text ltx_font_italic">Vicon</span> system provides the 6-DoF pose of the 2 persons and sensor suite in an arbitrarily fixed reference frame.
Overall, the dataset spans 14 minutes over four ROSBags, containing approximately 774 million events<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The DVS driver cuts the event stream into variable-length event-array messages; thus, the dataset contains 25000 event-array messages.</span></span></span>, 22000 RGB images, 6700 point clouds, and 70000 ground truth poses each for two persons in the scene. For the experimentation performed, the ROSBag entitled “dynamic.bag” was used. For convenience and utility, the dataset is labelled with the class identifier corresponding to the colour helmet worn by the person.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Calibration</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.7" class="ltx_p">To use our dataset effectively, we must first perform the intrinsic calibration of the camera and extrinsic calibration between the various sensors and the set of reflective markers.
Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Calibration ‣ II Dataset ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the set of geometric transformations <math id="S2.SS2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{a}^{b}" display="inline"><semantics id="S2.SS2.p1.1.m1.1a"><msubsup id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml"><mi id="S2.SS2.p1.1.m1.1.1.2.2" xref="S2.SS2.p1.1.m1.1.1.2.2.cmml">𝐓</mi><mi id="S2.SS2.p1.1.m1.1.1.2.3" xref="S2.SS2.p1.1.m1.1.1.2.3.cmml">a</mi><mi id="S2.SS2.p1.1.m1.1.1.3" xref="S2.SS2.p1.1.m1.1.1.3.cmml">b</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><apply id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">superscript</csymbol><apply id="S2.SS2.p1.1.m1.1.1.2.cmml" xref="S2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.1.m1.1.1.2.1.cmml" xref="S2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p1.1.m1.1.1.2.2.cmml" xref="S2.SS2.p1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.SS2.p1.1.m1.1.1.2.3.cmml" xref="S2.SS2.p1.1.m1.1.1.2.3">𝑎</ci></apply><ci id="S2.SS2.p1.1.m1.1.1.3.cmml" xref="S2.SS2.p1.1.m1.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\mathbf{T}_{a}^{b}</annotation></semantics></math> estimated during calibration (<math id="S2.SS2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{L}" display="inline"><semantics id="S2.SS2.p1.2.m2.1a"><msubsup id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml"><mi id="S2.SS2.p1.2.m2.1.1.2.2" xref="S2.SS2.p1.2.m2.1.1.2.2.cmml">𝐓</mi><mi id="S2.SS2.p1.2.m2.1.1.2.3" xref="S2.SS2.p1.2.m2.1.1.2.3.cmml">C</mi><mi id="S2.SS2.p1.2.m2.1.1.3" xref="S2.SS2.p1.2.m2.1.1.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><apply id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">superscript</csymbol><apply id="S2.SS2.p1.2.m2.1.1.2.cmml" xref="S2.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.2.m2.1.1.2.1.cmml" xref="S2.SS2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.p1.2.m2.1.1.2.2.cmml" xref="S2.SS2.p1.2.m2.1.1.2.2">𝐓</ci><ci id="S2.SS2.p1.2.m2.1.1.2.3.cmml" xref="S2.SS2.p1.2.m2.1.1.2.3">𝐶</ci></apply><ci id="S2.SS2.p1.2.m2.1.1.3.cmml" xref="S2.SS2.p1.2.m2.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\mathbf{T}_{C}^{L}</annotation></semantics></math>, <math id="S2.SS2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{M_{S}}" display="inline"><semantics id="S2.SS2.p1.3.m3.1a"><msubsup id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml"><mi id="S2.SS2.p1.3.m3.1.1.2.2" xref="S2.SS2.p1.3.m3.1.1.2.2.cmml">𝐓</mi><mi id="S2.SS2.p1.3.m3.1.1.2.3" xref="S2.SS2.p1.3.m3.1.1.2.3.cmml">C</mi><msub id="S2.SS2.p1.3.m3.1.1.3" xref="S2.SS2.p1.3.m3.1.1.3.cmml"><mi id="S2.SS2.p1.3.m3.1.1.3.2" xref="S2.SS2.p1.3.m3.1.1.3.2.cmml">M</mi><mi id="S2.SS2.p1.3.m3.1.1.3.3" xref="S2.SS2.p1.3.m3.1.1.3.3.cmml">S</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><apply id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">superscript</csymbol><apply id="S2.SS2.p1.3.m3.1.1.2.cmml" xref="S2.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.2.1.cmml" xref="S2.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.2.2.cmml" xref="S2.SS2.p1.3.m3.1.1.2.2">𝐓</ci><ci id="S2.SS2.p1.3.m3.1.1.2.3.cmml" xref="S2.SS2.p1.3.m3.1.1.2.3">𝐶</ci></apply><apply id="S2.SS2.p1.3.m3.1.1.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.3.m3.1.1.3.1.cmml" xref="S2.SS2.p1.3.m3.1.1.3">subscript</csymbol><ci id="S2.SS2.p1.3.m3.1.1.3.2.cmml" xref="S2.SS2.p1.3.m3.1.1.3.2">𝑀</ci><ci id="S2.SS2.p1.3.m3.1.1.3.3.cmml" xref="S2.SS2.p1.3.m3.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\mathbf{T}_{C}^{M_{S}}</annotation></semantics></math>, and <math id="S2.SS2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{I}" display="inline"><semantics id="S2.SS2.p1.4.m4.1a"><msubsup id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.p1.4.m4.1.1.2.2" xref="S2.SS2.p1.4.m4.1.1.2.2.cmml">𝐓</mi><mi id="S2.SS2.p1.4.m4.1.1.2.3" xref="S2.SS2.p1.4.m4.1.1.2.3.cmml">C</mi><mi id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">I</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">superscript</csymbol><apply id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.2.1.cmml" xref="S2.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2.2">𝐓</ci><ci id="S2.SS2.p1.4.m4.1.1.2.3.cmml" xref="S2.SS2.p1.4.m4.1.1.2.3">𝐶</ci></apply><ci id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">\mathbf{T}_{C}^{I}</annotation></semantics></math>) or given by the Vicon system (<math id="S2.SS2.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{T}_{W}^{M_{S}}" display="inline"><semantics id="S2.SS2.p1.5.m5.1a"><msubsup id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2.2" xref="S2.SS2.p1.5.m5.1.1.2.2.cmml">𝐓</mi><mi id="S2.SS2.p1.5.m5.1.1.2.3" xref="S2.SS2.p1.5.m5.1.1.2.3.cmml">W</mi><msub id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml"><mi id="S2.SS2.p1.5.m5.1.1.3.2" xref="S2.SS2.p1.5.m5.1.1.3.2.cmml">M</mi><mi id="S2.SS2.p1.5.m5.1.1.3.3" xref="S2.SS2.p1.5.m5.1.1.3.3.cmml">S</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">superscript</csymbol><apply id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.2.1.cmml" xref="S2.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.2.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2.2">𝐓</ci><ci id="S2.SS2.p1.5.m5.1.1.2.3.cmml" xref="S2.SS2.p1.5.m5.1.1.2.3">𝑊</ci></apply><apply id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.3.1.cmml" xref="S2.SS2.p1.5.m5.1.1.3">subscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.3.2.cmml" xref="S2.SS2.p1.5.m5.1.1.3.2">𝑀</ci><ci id="S2.SS2.p1.5.m5.1.1.3.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">\mathbf{T}_{W}^{M_{S}}</annotation></semantics></math>, <math id="S2.SS2.p1.6.m6.1" class="ltx_Math" alttext="\mathbf{T}_{W}^{M_{1}}" display="inline"><semantics id="S2.SS2.p1.6.m6.1a"><msubsup id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml"><mi id="S2.SS2.p1.6.m6.1.1.2.2" xref="S2.SS2.p1.6.m6.1.1.2.2.cmml">𝐓</mi><mi id="S2.SS2.p1.6.m6.1.1.2.3" xref="S2.SS2.p1.6.m6.1.1.2.3.cmml">W</mi><msub id="S2.SS2.p1.6.m6.1.1.3" xref="S2.SS2.p1.6.m6.1.1.3.cmml"><mi id="S2.SS2.p1.6.m6.1.1.3.2" xref="S2.SS2.p1.6.m6.1.1.3.2.cmml">M</mi><mn id="S2.SS2.p1.6.m6.1.1.3.3" xref="S2.SS2.p1.6.m6.1.1.3.3.cmml">1</mn></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><apply id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">superscript</csymbol><apply id="S2.SS2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.6.m6.1.1.2.1.cmml" xref="S2.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p1.6.m6.1.1.2.2.cmml" xref="S2.SS2.p1.6.m6.1.1.2.2">𝐓</ci><ci id="S2.SS2.p1.6.m6.1.1.2.3.cmml" xref="S2.SS2.p1.6.m6.1.1.2.3">𝑊</ci></apply><apply id="S2.SS2.p1.6.m6.1.1.3.cmml" xref="S2.SS2.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.6.m6.1.1.3.1.cmml" xref="S2.SS2.p1.6.m6.1.1.3">subscript</csymbol><ci id="S2.SS2.p1.6.m6.1.1.3.2.cmml" xref="S2.SS2.p1.6.m6.1.1.3.2">𝑀</ci><cn type="integer" id="S2.SS2.p1.6.m6.1.1.3.3.cmml" xref="S2.SS2.p1.6.m6.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">\mathbf{T}_{W}^{M_{1}}</annotation></semantics></math>, and <math id="S2.SS2.p1.7.m7.1" class="ltx_Math" alttext="\mathbf{T}_{W}^{M_{2}}" display="inline"><semantics id="S2.SS2.p1.7.m7.1a"><msubsup id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml"><mi id="S2.SS2.p1.7.m7.1.1.2.2" xref="S2.SS2.p1.7.m7.1.1.2.2.cmml">𝐓</mi><mi id="S2.SS2.p1.7.m7.1.1.2.3" xref="S2.SS2.p1.7.m7.1.1.2.3.cmml">W</mi><msub id="S2.SS2.p1.7.m7.1.1.3" xref="S2.SS2.p1.7.m7.1.1.3.cmml"><mi id="S2.SS2.p1.7.m7.1.1.3.2" xref="S2.SS2.p1.7.m7.1.1.3.2.cmml">M</mi><mn id="S2.SS2.p1.7.m7.1.1.3.3" xref="S2.SS2.p1.7.m7.1.1.3.3.cmml">2</mn></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><apply id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.7.m7.1.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">superscript</csymbol><apply id="S2.SS2.p1.7.m7.1.1.2.cmml" xref="S2.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.7.m7.1.1.2.1.cmml" xref="S2.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.p1.7.m7.1.1.2.2.cmml" xref="S2.SS2.p1.7.m7.1.1.2.2">𝐓</ci><ci id="S2.SS2.p1.7.m7.1.1.2.3.cmml" xref="S2.SS2.p1.7.m7.1.1.2.3">𝑊</ci></apply><apply id="S2.SS2.p1.7.m7.1.1.3.cmml" xref="S2.SS2.p1.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p1.7.m7.1.1.3.1.cmml" xref="S2.SS2.p1.7.m7.1.1.3">subscript</csymbol><ci id="S2.SS2.p1.7.m7.1.1.3.2.cmml" xref="S2.SS2.p1.7.m7.1.1.3.2">𝑀</ci><cn type="integer" id="S2.SS2.p1.7.m7.1.1.3.3.cmml" xref="S2.SS2.p1.7.m7.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">\mathbf{T}_{W}^{M_{2}}</annotation></semantics></math>).
Note that as the RGB and event data are being collected by the same cells in the <span id="S2.SS2.p1.7.1" class="ltx_text ltx_font_italic">DAVIS346</span>, the reference frame of the event and RGB camera are collocated (labelled “Camera” in Fig. <a href="#S2.F2" title="Figure 2 ‣ II-B Calibration ‣ II Dataset ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
Thus, the intrinsic calibration parameters obtained with the RGB camera apply to the event camera as both data types share the same optical path.
Calibration sequences and parameter estimates are included with the main dataset.
Table <a href="#S2.T1" title="TABLE I ‣ II-B Calibration ‣ II Dataset ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> details the estimation process for each transformation <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The camera’s intrinsics are obtained with Matlab’s calibration toolbox https://au.mathworks.com/help/vision/ref/cameracalibrator-app.html</span></span></span>.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><svg id="S2.F2.pic1" class="ltx_picture ltx_centering" height="212.22" overflow="visible" version="1.1" width="411.93"><g transform="translate(0,212.22) matrix(1 0 0 -1 0 0) translate(184.11,0) translate(0,106.11)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -179.5 -101.5)" fill="#000000" stroke="#000000"><foreignObject width="359" height="203" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x1.png" id="S2.F2.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="276" height="156" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -52.54 4.7)" fill="#000000" stroke="#000000"><foreignObject width="10.59" height="8.47" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{C}" display="inline"><semantics id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><mi mathsize="70%" id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><ci id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{C}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -52.25 -37.82)" fill="#000000" stroke="#000000"><foreignObject width="10.01" height="8.47" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{L}" display="inline"><semantics id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><mi mathsize="70%" id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><ci id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{L}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -87.24 -4.74)" fill="#000000" stroke="#000000"><foreignObject width="9.13" height="8.47" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{I}" display="inline"><semantics id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><mi mathsize="70%" id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><ci id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{I}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -37.84 43.03)" fill="#000000" stroke="#000000"><foreignObject width="14.26" height="9.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{M_{S}}" display="inline"><semantics id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><msub id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mi mathsize="70%" id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">S</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><apply id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{M_{S}}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 23.85 43)" fill="#000000" stroke="#000000"><foreignObject width="13.73" height="9.47" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{M_{1}}" display="inline"><semantics id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><msub id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mn mathsize="70%" id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><apply id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><cn type="integer" id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{M_{1}}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 59.28 21.74)" fill="#000000" stroke="#000000"><foreignObject width="13.73" height="9.47" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{M_{2}}" display="inline"><semantics id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><msub id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mn mathsize="70%" id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><apply id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><cn type="integer" id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.7.7.7.7.7.7.7.7.7.7.7.7.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{M_{2}}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 60.04 -49.63)" fill="#000000" stroke="#000000"><foreignObject width="12.2" height="8.47" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{W}" display="inline"><semantics id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><mi mathsize="70%" id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">W</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><ci id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.8.8.8.8.8.8.8.8.8.8.8.8.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{W}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -100.12 25.04)" fill="#000000" stroke="#000000"><foreignObject width="11.26" height="10.33" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{I}" display="inline"><semantics id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1a"><msubsup id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.2" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.3" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml">C</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">I</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.2.3">𝐶</ci></apply><ci id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.9.9.9.9.9.9.9.9.9.9.9.9.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{T}_{C}^{I}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -45.79 -15.12)" fill="#000000" stroke="#000000"><foreignObject width="11.26" height="10.33" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{L}" display="inline"><semantics id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1a"><msubsup id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.2" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.3" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml">C</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.2.3">𝐶</ci></apply><ci id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.10.10.10.10.10.10.10.10.10.10.10.10.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{T}_{C}^{L}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -85.94 36.85)" fill="#000000" stroke="#000000"><foreignObject width="11.26" height="10.33" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{M_{S}}" display="inline"><semantics id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1a"><msubsup id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.2" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.3" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml">C</mi><msub id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">S</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.2.3">𝐶</ci></apply><apply id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.11.11.11.11.11.11.11.11.11.11.11.11.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{T}_{C}^{M_{S}}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -1.71 1.41)" fill="#000000" stroke="#000000"><foreignObject width="12.87" height="10.33" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{W}^{M_{S}}" display="inline"><semantics id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1a"><msubsup id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.2" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml">W</mi><msub id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">S</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.2.3">𝑊</ci></apply><apply id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.12.12.12.12.12.12.12.12.12.12.12.12.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{T}_{W}^{M_{S}}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 19.55 -10.4)" fill="#000000" stroke="#000000"><foreignObject width="12.87" height="10.33" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{W}^{M_{1}}" display="inline"><semantics id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1a"><msubsup id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.2" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.3" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml">W</mi><msub id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mn mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">1</mn></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.2.3">𝑊</ci></apply><apply id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><cn type="integer" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.13.13.13.13.13.13.13.13.13.13.13.13.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{T}_{W}^{M_{1}}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 76.24 -36.38)" fill="#000000" stroke="#000000"><foreignObject width="12.87" height="10.33" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><math id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{W}^{M_{2}}" display="inline"><semantics id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1a"><msubsup id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.2" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.3" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml">W</mi><msub id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mn mathcolor="#808080" mathsize="70%" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">2</mn></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.2.3">𝑊</ci></apply><apply id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><cn type="integer" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.14.14.14.14.14.14.14.14.14.14.14.14.1.1.1.1.1.1.1.1.1.m1.1c">\mathbf{T}_{W}^{M_{2}}</annotation></semantics></math></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 91.75 40.43)" fill="#000000" stroke="#000000"><foreignObject width="131.45" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F2.pic1.22.22.22.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:95.0pt;">
<span id="S2.F2.pic1.22.22.22.1.1.1" class="ltx_p"></span>
<span id="S2.F2.pic1.22.22.22.1.1.2" class="ltx_p"><span id="S2.F2.pic1.22.22.22.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Frames</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -65.73 -14.41)" fill="#000000" stroke="#000000"><foreignObject width="131.45" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:95.0pt;">
<span id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.2" class="ltx_p"></span>
<span id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><math id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{C}" display="inline"><semantics id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><mi mathsize="70%" id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">C</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><ci id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{C}</annotation></semantics></math><span id="S2.F2.pic1.15.15.15.15.15.15.15.15.15.15.15.15.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">: Camera</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -65.73 -14.41)" fill="#000000" stroke="#000000"><foreignObject width="131.45" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:95.0pt;">
<span id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.2" class="ltx_p"></span>
<span id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><math id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{L}" display="inline"><semantics id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><mi mathsize="70%" id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">L</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><ci id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{L}</annotation></semantics></math><span id="S2.F2.pic1.16.16.16.16.16.16.16.16.16.16.16.16.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">: LiDAR</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -65.73 -14.41)" fill="#000000" stroke="#000000"><foreignObject width="131.45" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:95.0pt;">
<span id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.2" class="ltx_p"></span>
<span id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><math id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{I}" display="inline"><semantics id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><mi mathsize="70%" id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><ci id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{I}</annotation></semantics></math><span id="S2.F2.pic1.17.17.17.17.17.17.17.17.17.17.17.17.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">: IMU</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -65.73 -14.41)" fill="#000000" stroke="#000000"><foreignObject width="131.45" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:95.0pt;">
<span id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.2" class="ltx_p"></span>
<span id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><math id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{W}" display="inline"><semantics id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><mi mathsize="70%" id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">W</mi></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><ci id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">𝑊</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{W}</annotation></semantics></math><span id="S2.F2.pic1.18.18.18.18.18.18.18.18.18.18.18.18.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">: World</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -65.73 -14.41)" fill="#000000" stroke="#000000"><foreignObject width="131.45" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:95.0pt;">
<span id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.2" class="ltx_p"></span>
<span id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><math id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{M_{S}}" display="inline"><semantics id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><msub id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mi mathsize="70%" id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">S</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><apply id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{M_{S}}</annotation></semantics></math><span id="S2.F2.pic1.19.19.19.19.19.19.19.19.19.19.19.19.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">: Sensor markers</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -65.73 -14.41)" fill="#000000" stroke="#000000"><foreignObject width="131.45" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:95.0pt;">
<span id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.2" class="ltx_p"></span>
<span id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><math id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{M_{1}}" display="inline"><semantics id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><msub id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mn mathsize="70%" id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">1</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><apply id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><cn type="integer" id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{M_{1}}</annotation></semantics></math><span id="S2.F2.pic1.20.20.20.20.20.20.20.20.20.20.20.20.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">: Helmet 1 markers</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -65.73 -14.41)" fill="#000000" stroke="#000000"><foreignObject width="131.45" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:95.0pt;">
<span id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.2" class="ltx_p"></span>
<span id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1" class="ltx_p"><math id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{M_{2}}" display="inline"><semantics id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1a"><msub id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="70%" id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.2" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">ℱ</mi><msub id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi mathsize="70%" id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mn mathsize="70%" id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml">2</mn></msub></msub><annotation-xml encoding="MathML-Content" id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.2">ℱ</ci><apply id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><cn type="integer" id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1.1.3.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.m1.1c">\mathcal{F}_{M_{2}}</annotation></semantics></math><span id="S2.F2.pic1.21.21.21.21.21.21.21.21.21.21.21.21.1.1.1.1.1.1.1.1.1.1.1" class="ltx_text" style="font-size:70%;">: Helmet 2 markers</span></span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Frames and geometric transformations in the dataset.</figcaption>
</figure>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Insight into the extrinsic calibration procedure of the sensor suite used to collect the proposed dataset.</figcaption>
<table id="S2.T1.4" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.4.5.1" class="ltx_tr">
<th id="S2.T1.4.5.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:42.7pt;">
<span id="S2.T1.4.5.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.1.1.1.1" class="ltx_p"><span id="S2.T1.4.5.1.1.1.1.1" class="ltx_text ltx_font_bold">Trans.</span></span>
</span>
</th>
<th id="S2.T1.4.5.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S2.T1.4.5.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.5.1.2.1.1" class="ltx_p"><span id="S2.T1.4.5.1.2.1.1.1" class="ltx_text ltx_font_bold">Details</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2" class="ltx_tr">
<td id="S2.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S2.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.1.1.1.1.1" class="ltx_p"><math id="S2.T1.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{M_{S}}" display="inline"><semantics id="S2.T1.1.1.1.1.1.m1.1a"><msubsup id="S2.T1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.m1.1.1.cmml"><mi id="S2.T1.1.1.1.1.1.m1.1.1.2.2" xref="S2.T1.1.1.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi id="S2.T1.1.1.1.1.1.m1.1.1.2.3" xref="S2.T1.1.1.1.1.1.m1.1.1.2.3.cmml">C</mi><msub id="S2.T1.1.1.1.1.1.m1.1.1.3" xref="S2.T1.1.1.1.1.1.m1.1.1.3.cmml"><mi id="S2.T1.1.1.1.1.1.m1.1.1.3.2" xref="S2.T1.1.1.1.1.1.m1.1.1.3.2.cmml">M</mi><mi id="S2.T1.1.1.1.1.1.m1.1.1.3.3" xref="S2.T1.1.1.1.1.1.m1.1.1.3.3.cmml">S</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.T1.1.1.1.1.1.m1.1.1.2.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.1.1.1.1.1.m1.1.1.2.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.T1.1.1.1.1.1.m1.1.1.2.2.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.T1.1.1.1.1.1.m1.1.1.2.3.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1.2.3">𝐶</ci></apply><apply id="S2.T1.1.1.1.1.1.m1.1.1.3.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.T1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.T1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S2.T1.1.1.1.1.1.m1.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.m1.1c">\mathbf{T}_{C}^{M_{S}}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T1.2.2.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.2.2.1.1" class="ltx_p"><span id="S2.T1.2.2.2.1.1.1" class="ltx_text" style="font-size:70%;">Camera position from checkerboard detection, then eye-in-hand calibration with Vicon poses of <math id="S2.T1.2.2.2.1.1.1.m1.1" class="ltx_Math" alttext="\mathcal{F}_{M_{S}}" display="inline"><semantics id="S2.T1.2.2.2.1.1.1.m1.1a"><msub id="S2.T1.2.2.2.1.1.1.m1.1.1" xref="S2.T1.2.2.2.1.1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.T1.2.2.2.1.1.1.m1.1.1.2" xref="S2.T1.2.2.2.1.1.1.m1.1.1.2.cmml">ℱ</mi><msub id="S2.T1.2.2.2.1.1.1.m1.1.1.3" xref="S2.T1.2.2.2.1.1.1.m1.1.1.3.cmml"><mi id="S2.T1.2.2.2.1.1.1.m1.1.1.3.2" xref="S2.T1.2.2.2.1.1.1.m1.1.1.3.2.cmml">M</mi><mi id="S2.T1.2.2.2.1.1.1.m1.1.1.3.3" xref="S2.T1.2.2.2.1.1.1.m1.1.1.3.3.cmml">S</mi></msub></msub><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.1.1.m1.1b"><apply id="S2.T1.2.2.2.1.1.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.2.2.2.1.1.1.m1.1.1.1.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.T1.2.2.2.1.1.1.m1.1.1.2.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.2">ℱ</ci><apply id="S2.T1.2.2.2.1.1.1.m1.1.1.3.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.T1.2.2.2.1.1.1.m1.1.1.3.1.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S2.T1.2.2.2.1.1.1.m1.1.1.3.2.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.3.2">𝑀</ci><ci id="S2.T1.2.2.2.1.1.1.m1.1.1.3.3.cmml" xref="S2.T1.2.2.2.1.1.1.m1.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.1.1.m1.1c">\mathcal{F}_{M_{S}}</annotation></semantics></math></span></span>
</span>
</td>
</tr>
<tr id="S2.T1.3.3" class="ltx_tr">
<td id="S2.T1.3.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:42.7pt;">
<span id="S2.T1.3.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.3.1.1.1" class="ltx_p"><math id="S2.T1.3.3.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{L}" display="inline"><semantics id="S2.T1.3.3.1.1.1.m1.1a"><msubsup id="S2.T1.3.3.1.1.1.m1.1.1" xref="S2.T1.3.3.1.1.1.m1.1.1.cmml"><mi id="S2.T1.3.3.1.1.1.m1.1.1.2.2" xref="S2.T1.3.3.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi id="S2.T1.3.3.1.1.1.m1.1.1.2.3" xref="S2.T1.3.3.1.1.1.m1.1.1.2.3.cmml">C</mi><mi id="S2.T1.3.3.1.1.1.m1.1.1.3" xref="S2.T1.3.3.1.1.1.m1.1.1.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.1.1.m1.1b"><apply id="S2.T1.3.3.1.1.1.m1.1.1.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.3.3.1.1.1.m1.1.1.1.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.T1.3.3.1.1.1.m1.1.1.2.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.3.3.1.1.1.m1.1.1.2.1.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.T1.3.3.1.1.1.m1.1.1.2.2.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.T1.3.3.1.1.1.m1.1.1.2.3.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1.2.3">𝐶</ci></apply><ci id="S2.T1.3.3.1.1.1.m1.1.1.3.cmml" xref="S2.T1.3.3.1.1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.1.1.1.m1.1c">\mathbf{T}_{C}^{L}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T1.3.3.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S2.T1.3.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.3.3.2.1.1" class="ltx_p"><span id="S2.T1.3.3.2.1.1.1" class="ltx_text" style="font-size:70%;">Checkerboard plane equation from camera and point-to-plane minimisation with LiDAR points on the checkerboard</span></span>
</span>
</td>
</tr>
<tr id="S2.T1.4.4" class="ltx_tr">
<td id="S2.T1.4.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" style="width:42.7pt;">
<span id="S2.T1.4.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.1.1.1" class="ltx_p"><math id="S2.T1.4.4.1.1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{I}" display="inline"><semantics id="S2.T1.4.4.1.1.1.m1.1a"><msubsup id="S2.T1.4.4.1.1.1.m1.1.1" xref="S2.T1.4.4.1.1.1.m1.1.1.cmml"><mi id="S2.T1.4.4.1.1.1.m1.1.1.2.2" xref="S2.T1.4.4.1.1.1.m1.1.1.2.2.cmml">𝐓</mi><mi id="S2.T1.4.4.1.1.1.m1.1.1.2.3" xref="S2.T1.4.4.1.1.1.m1.1.1.2.3.cmml">C</mi><mi id="S2.T1.4.4.1.1.1.m1.1.1.3" xref="S2.T1.4.4.1.1.1.m1.1.1.3.cmml">I</mi></msubsup><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.1.1.m1.1b"><apply id="S2.T1.4.4.1.1.1.m1.1.1.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.4.4.1.1.1.m1.1.1.1.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1">superscript</csymbol><apply id="S2.T1.4.4.1.1.1.m1.1.1.2.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.T1.4.4.1.1.1.m1.1.1.2.1.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1">subscript</csymbol><ci id="S2.T1.4.4.1.1.1.m1.1.1.2.2.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1.2.2">𝐓</ci><ci id="S2.T1.4.4.1.1.1.m1.1.1.2.3.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1.2.3">𝐶</ci></apply><ci id="S2.T1.4.4.1.1.1.m1.1.1.3.cmml" xref="S2.T1.4.4.1.1.1.m1.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.1.1.1.m1.1c">\mathbf{T}_{C}^{I}</annotation></semantics></math></span>
</span>
</td>
<td id="S2.T1.4.4.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S2.T1.4.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.4.4.2.1.1" class="ltx_p"><span id="S2.T1.4.4.2.1.1.1" class="ltx_text" style="font-size:70%;">Kalibr<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span id="footnote3.1.1.1" class="ltx_text" style="font-size:143%;">3</span></span><span id="footnote3.5" class="ltx_text" style="font-size:143%;">https://github.com/ethz-asl/kalibr</span></span></span></span>: Checkerboard for camera position and continuous-time batch state estimation</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Multi-modal scene understanding</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To enable downstream applications such as guide-dog-like assistive robots, we explore fusing vision and LiDAR data for the 3D localisation of pedestrians and vehicles. The methodology can also be applied to static objects such as trees, buildings and roads. While a complete system should include these necessities, the motivation of this work is to focus on dynamic objects due to the further requirement of tracking relative motion.
Fig. <a href="#S3.F3" title="Figure 3 ‣ III Multi-modal scene understanding ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents an overview of the proposed pipeline.
The main steps are, first, the vision-based detection of objects in the image space (2D), followed by the tracking of the resulting bounding boxes with the Simple Online and Real-time Tracking (SORT) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>.
Then, the bounding boxes are used to crop and filter the LiDAR scans before performing state estimation in the 3D space using a Constant-Velocity Kalman Filter (CVKF).
Note that this pipeline can be used with an event camera or a standard RGB camera if the detection algorithm provides bounding boxes around the detected objects.
The rest of this section provides details about the components of the proposed pipeline.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><svg id="S3.F3.pic1" class="ltx_picture ltx_centering" height="76.61" overflow="visible" version="1.1" width="263.18"><g transform="translate(0,76.61) matrix(1 0 0 -1 0 0) translate(134.03,0) translate(0,58.58)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#FFFFFF" stroke="#FF0000"><path d="M -42.66 -17.76 h 85.33 v 35.51 h -85.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -38.05 3.46)" fill="#000000" stroke="#000000"><foreignObject width="76.1" height="26.29" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.1.1.1.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:55.0pt;">
<span id="S3.F3.pic1.1.1.1.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.1.1.1.1.1.2" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Camera<span id="S3.F3.pic1.1.1.1.1.1.2.1.1" class="ltx_text ltx_font_medium"></span></span></span>
<span id="S3.F3.pic1.1.1.1.1.1.3" class="ltx_p"><span id="S3.F3.pic1.1.1.1.1.1.3.1" class="ltx_text" style="font-size:70%;">(RGB or event)</span></span>
</span></foreignObject></g><g fill="#FFFFFF"><path d="M -125.92 -48.43 h 251.83 v 27.67 h -251.83 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -124.53 -39.44)" fill="#000000" stroke="#000000"><foreignObject width="249.07" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.2.2.2.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:180.0pt;">
<span id="S3.F3.pic1.2.2.2.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.2.2.2.1.1.2" class="ltx_p"><span id="S3.F3.pic1.2.2.2.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Object detection</span></span>
<span id="S3.F3.pic1.2.2.2.1.1.3" class="ltx_p"><span id="S3.F3.pic1.2.2.2.1.1.3.1" class="ltx_text" style="font-size:70%;">RGB: YOLOv4 / Event: RVT</span></span>
</span></foreignObject></g><g fill="#FFFFFF"><path d="M -125.92 -49.81 h 251.83 v 29.06 h -251.83 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -124.53 -31.83)" fill="#000000" stroke="#000000"><foreignObject width="249.07" height="26.29" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.3.3.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:180.0pt;">
<span id="S3.F3.pic1.3.3.3.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.3.3.3.1.1.2" class="ltx_p"><span id="S3.F3.pic1.3.3.3.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Bounding box tracking</span></span>
<span id="S3.F3.pic1.3.3.3.1.1.3" class="ltx_p"><span id="S3.F3.pic1.3.3.3.1.1.3.1" class="ltx_text" style="font-size:70%;">Simple Online Real-time Tracking (SORT)</span></span>
</span></foreignObject></g><g fill="#FFFFFF"><path d="M -125.92 -48.43 h 251.83 v 27.67 h -251.83 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -124.53 -39.44)" fill="#000000" stroke="#000000"><foreignObject width="249.07" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.4.4.4.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:180.0pt;">
<span id="S3.F3.pic1.4.4.4.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.4.4.4.1.1.2" class="ltx_p"><span id="S3.F3.pic1.4.4.4.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">LiDAR fusion</span></span>
<span id="S3.F3.pic1.4.4.4.1.1.3" class="ltx_p"><span id="S3.F3.pic1.4.4.4.1.1.3.1" class="ltx_text" style="font-size:70%;">Bounding box cropping and filtering</span></span>
</span></foreignObject></g><g fill="#FFFFFF"><path d="M -125.92 -48.43 h 251.83 v 27.67 h -251.83 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -124.53 -39.44)" fill="#000000" stroke="#000000"><foreignObject width="249.07" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.5.5.5.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:180.0pt;">
<span id="S3.F3.pic1.5.5.5.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.5.5.5.1.1.2" class="ltx_p"><span id="S3.F3.pic1.5.5.5.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">3D tracking</span></span>
<span id="S3.F3.pic1.5.5.5.1.1.3" class="ltx_p"><span id="S3.F3.pic1.5.5.5.1.1.3.1" class="ltx_text" style="font-size:70%;">Constant Velocity Kalman Filter (CVKF)</span></span>
</span></foreignObject></g><g fill="#FFFFFF" stroke="#FF0000"><path d="M 14.11 -16.6 h 85.33 v 33.21 h -85.33 Z"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 18.73 -4.84)" fill="#000000" stroke="#000000"><foreignObject width="76.1" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.6.6.6.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:55.0pt;">
<span id="S3.F3.pic1.6.6.6.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.6.6.6.1.1.2" class="ltx_p"><span id="S3.F3.pic1.6.6.6.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">3D LiDAR</span></span>
</span></foreignObject></g><g fill="#FFFFFF"><path d="M -129.15 -39.94 h 258.29 v 18.91 h -258.29 Z" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -124.53 -35.33)" fill="#0F75FF" stroke="#0F75FF"><foreignObject width="249.07" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#0F75FF">
<span id="S3.F3.pic1.7.7.7.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:180.0pt;">
<span id="S3.F3.pic1.7.7.7.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.7.7.7.1.1.2" class="ltx_p"><span id="S3.F3.pic1.7.7.7.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">3D object position and velocity</span></span>
</span></foreignObject></g><path d="M 0 -18.03 L 0 -15.77" style="fill:none"></path><g transform="matrix(0.0 -1.0 1.0 0.0 0 -15.77)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -129.42 -15.94)" fill="#000000" stroke="#000000"><foreignObject width="124.53" height="26.29" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.8.8.8.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:90.0pt;">
<span id="S3.F3.pic1.8.8.8.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.8.8.8.1.1.2" class="ltx_p ltx_align_right"><span id="S3.F3.pic1.8.8.8.1.1.2.1" class="ltx_text" style="font-size:70%;">Raw vision data (RGB images or event stream)</span></span>
</span></foreignObject></g><path d="M 0 -20.76 L 4.98 -20.76" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -4.98 -20.76)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -129.42 -25.6)" fill="#000000" stroke="#000000"><foreignObject width="124.53" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.9.9.9.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:90.0pt;">
<span id="S3.F3.pic1.9.9.9.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.9.9.9.1.1.2" class="ltx_p ltx_align_right"><span id="S3.F3.pic1.9.9.9.1.1.2.1" class="ltx_text" style="font-size:70%;">Bounding boxes</span></span>
</span></foreignObject></g><path d="M 0 -49.81 L 0 -53.41" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 0 -53.41)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -129.42 -53.96)" fill="#000000" stroke="#000000"><foreignObject width="124.53" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.10.10.10.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:90.0pt;">
<span id="S3.F3.pic1.10.10.10.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.10.10.10.1.1.2" class="ltx_p ltx_align_right"><span id="S3.F3.pic1.10.10.10.1.1.2.1" class="ltx_text" style="font-size:70%;">Tracked bounding boxes</span></span>
</span></foreignObject></g><path d="M 0 -34.59 L 4.98 -34.59" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 -4.98 -34.59)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -129.42 -39.44)" fill="#000000" stroke="#000000"><foreignObject width="124.53" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.11.11.11.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:90.0pt;">
<span id="S3.F3.pic1.11.11.11.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.11.11.11.1.1.2" class="ltx_p ltx_align_right"><span id="S3.F3.pic1.11.11.11.1.1.2.1" class="ltx_text" style="font-size:70%;">3D points</span></span>
</span></foreignObject></g><path d="M 99.72 0 L 13.84 0 L 13.84 -34.59 L 120.94 -34.59" style="fill:none"></path><g transform="matrix(1.0 0.0 0.0 1.0 120.94 -34.59)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -18.73 -18.62)" fill="#000000" stroke="#000000"><foreignObject width="27.67" height="20.77" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F3.pic1.12.12.12.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:20.0pt;">
<span id="S3.F3.pic1.12.12.12.1.1.1" class="ltx_p"></span>
<span id="S3.F3.pic1.12.12.12.1.1.2" class="ltx_p ltx_align_right"><span id="S3.F3.pic1.12.12.12.1.1.2.1" class="ltx_text" style="font-size:70%;">3D scans</span></span>
</span></foreignObject></g><path d="M 0 -48.43 L 0 -45.2" style="fill:none"></path><g transform="matrix(0.0 1.0 -1.0 0.0 0 -45.2)"><path d="M 4.98 0 C 3.51 0.28 1.11 1.11 -0.55 2.08 L -0.55 -2.08 C 1.11 -1.11 3.51 -0.28 4.98 0" style="stroke:none"></path></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Block diagram overview of the proposed vision-LiDAR object detection and tracking.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">2D object detection and tracking</span>
</h3>

<section id="S3.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS1.5.1.1" class="ltx_text">III-A</span>1 </span>RGB-based detection</h4>

<div id="S3.SS1.SSS1.p1" class="ltx_para">
<p id="S3.SS1.SSS1.p1.1" class="ltx_p">When the proposed framework is used with an RGB camera, we use YOLOv4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> for the task of object detection.
YOLOv4 is a CNN-based algorithm renowned for its real-time capabilities and accuracy.
It is trained on the MS-COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and can classify 80 different types of objects, including pedestrians and vehicles.
Its one-shot detection approach surpasses traditional two-shot detectors like Faster R-CNN in terms of inference speed.
The RGB images from the DAVIS346 are undistorted using the camera’s intrinsic parameters before being passed to YOLOv4.
The output consists of 2D bounding boxes.</p>
</div>
</section>
<section id="S3.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS2.5.1.1" class="ltx_text">III-A</span>2 </span>Event-based detection</h4>

<div id="S3.SS1.SSS2.p1" class="ltx_para">
<p id="S3.SS1.SSS2.p1.5" class="ltx_p">For event-based vision, our pipeline relies on RVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
The choice of RVT is motivated by its proficiency in detecting both vehicle and pedestrian data, coupled with its fast inference time relative to alternative event-based models.
RVT relies on recurrent transformers to leverage the spatiotemporal nature of event data.
Accordingly, the stream of events is preprocessed into a succession of 4-dimensional tensors of size (<math id="S3.SS1.SSS2.p1.1.m1.4" class="ltx_Math" alttext="2,T,h,w" display="inline"><semantics id="S3.SS1.SSS2.p1.1.m1.4a"><mrow id="S3.SS1.SSS2.p1.1.m1.4.5.2" xref="S3.SS1.SSS2.p1.1.m1.4.5.1.cmml"><mn id="S3.SS1.SSS2.p1.1.m1.1.1" xref="S3.SS1.SSS2.p1.1.m1.1.1.cmml">2</mn><mo id="S3.SS1.SSS2.p1.1.m1.4.5.2.1" xref="S3.SS1.SSS2.p1.1.m1.4.5.1.cmml">,</mo><mi id="S3.SS1.SSS2.p1.1.m1.2.2" xref="S3.SS1.SSS2.p1.1.m1.2.2.cmml">T</mi><mo id="S3.SS1.SSS2.p1.1.m1.4.5.2.2" xref="S3.SS1.SSS2.p1.1.m1.4.5.1.cmml">,</mo><mi id="S3.SS1.SSS2.p1.1.m1.3.3" xref="S3.SS1.SSS2.p1.1.m1.3.3.cmml">h</mi><mo id="S3.SS1.SSS2.p1.1.m1.4.5.2.3" xref="S3.SS1.SSS2.p1.1.m1.4.5.1.cmml">,</mo><mi id="S3.SS1.SSS2.p1.1.m1.4.4" xref="S3.SS1.SSS2.p1.1.m1.4.4.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.1.m1.4b"><list id="S3.SS1.SSS2.p1.1.m1.4.5.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.5.2"><cn type="integer" id="S3.SS1.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS2.p1.1.m1.1.1">2</cn><ci id="S3.SS1.SSS2.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS2.p1.1.m1.2.2">𝑇</ci><ci id="S3.SS1.SSS2.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS2.p1.1.m1.3.3">ℎ</ci><ci id="S3.SS1.SSS2.p1.1.m1.4.4.cmml" xref="S3.SS1.SSS2.p1.1.m1.4.4">𝑤</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.1.m1.4c">2,T,h,w</annotation></semantics></math>), with <math id="S3.SS1.SSS2.p1.2.m2.1" class="ltx_Math" alttext="h" display="inline"><semantics id="S3.SS1.SSS2.p1.2.m2.1a"><mi id="S3.SS1.SSS2.p1.2.m2.1.1" xref="S3.SS1.SSS2.p1.2.m2.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.2.m2.1b"><ci id="S3.SS1.SSS2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS2.p1.2.m2.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.2.m2.1c">h</annotation></semantics></math> and <math id="S3.SS1.SSS2.p1.3.m3.1" class="ltx_Math" alttext="w" display="inline"><semantics id="S3.SS1.SSS2.p1.3.m3.1a"><mi id="S3.SS1.SSS2.p1.3.m3.1.1" xref="S3.SS1.SSS2.p1.3.m3.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.3.m3.1b"><ci id="S3.SS1.SSS2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS2.p1.3.m3.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.3.m3.1c">w</annotation></semantics></math> the resolution of the camera, by binning the events into <math id="S3.SS1.SSS2.p1.4.m4.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S3.SS1.SSS2.p1.4.m4.1a"><mi id="S3.SS1.SSS2.p1.4.m4.1.1" xref="S3.SS1.SSS2.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.4.m4.1b"><ci id="S3.SS1.SSS2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS2.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.4.m4.1c">T</annotation></semantics></math> temporal slices (10 slices within a 50 ms window in the publicly available model).
The first dimension of the tensor represents the two polarities of the events, thus storing the events triggered by positive and negative changes separately.
The authors of RVT have released pre-trained models <span id="S3.SS1.SSS2.p1.5.1" class="ltx_text ltx_font_italic">Gen1</span> and <span id="S3.SS1.SSS2.p1.5.2" class="ltx_text ltx_font_italic">1-Mpx</span> that are trained with the Gen1 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and 1-Mpx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> automotive datasets, respectively.
To infer objects’ bounding boxes using the DAVIS346 data, we crop or pad the event tensors to fit the required input size of the models (<math id="S3.SS1.SSS2.p1.5.m5.2" class="ltx_Math" alttext="h,w" display="inline"><semantics id="S3.SS1.SSS2.p1.5.m5.2a"><mrow id="S3.SS1.SSS2.p1.5.m5.2.3.2" xref="S3.SS1.SSS2.p1.5.m5.2.3.1.cmml"><mi id="S3.SS1.SSS2.p1.5.m5.1.1" xref="S3.SS1.SSS2.p1.5.m5.1.1.cmml">h</mi><mo id="S3.SS1.SSS2.p1.5.m5.2.3.2.1" xref="S3.SS1.SSS2.p1.5.m5.2.3.1.cmml">,</mo><mi id="S3.SS1.SSS2.p1.5.m5.2.2" xref="S3.SS1.SSS2.p1.5.m5.2.2.cmml">w</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS2.p1.5.m5.2b"><list id="S3.SS1.SSS2.p1.5.m5.2.3.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.2.3.2"><ci id="S3.SS1.SSS2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS2.p1.5.m5.1.1">ℎ</ci><ci id="S3.SS1.SSS2.p1.5.m5.2.2.cmml" xref="S3.SS1.SSS2.p1.5.m5.2.2">𝑤</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS2.p1.5.m5.2c">h,w</annotation></semantics></math>).</p>
</div>
</section>
<section id="S3.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS1.SSS3.5.1.1" class="ltx_text">III-A</span>3 </span>2D tracking</h4>

<div id="S3.SS1.SSS3.p1" class="ltx_para">
<p id="S3.SS1.SSS3.p1.8" class="ltx_p">The 2D bounding boxes in the image from the event and RGB object detectors are quite noisy in regards to the position and amount of misdetection.
The proposed pipeline leverages the SORT algorithm for multi-frame object association and position estimation to address this issue.
SORT employs the Hungarian Method in conjunction with a Linear CVKF to track objects across frames independently of other objects and camera motion.
The state vector in the CVKF is <math id="S3.SS1.SSS3.p1.1.m1.7" class="ltx_Math" alttext="x=[u,v,s,r,\dot{u},\dot{v},\dot{s}]^{\top}" display="inline"><semantics id="S3.SS1.SSS3.p1.1.m1.7a"><mrow id="S3.SS1.SSS3.p1.1.m1.7.8" xref="S3.SS1.SSS3.p1.1.m1.7.8.cmml"><mi id="S3.SS1.SSS3.p1.1.m1.7.8.2" xref="S3.SS1.SSS3.p1.1.m1.7.8.2.cmml">x</mi><mo id="S3.SS1.SSS3.p1.1.m1.7.8.1" xref="S3.SS1.SSS3.p1.1.m1.7.8.1.cmml">=</mo><msup id="S3.SS1.SSS3.p1.1.m1.7.8.3" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.cmml"><mrow id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml"><mo stretchy="false" id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2.1" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml">[</mo><mi id="S3.SS1.SSS3.p1.1.m1.1.1" xref="S3.SS1.SSS3.p1.1.m1.1.1.cmml">u</mi><mo id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2.2" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml">,</mo><mi id="S3.SS1.SSS3.p1.1.m1.2.2" xref="S3.SS1.SSS3.p1.1.m1.2.2.cmml">v</mi><mo id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2.3" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml">,</mo><mi id="S3.SS1.SSS3.p1.1.m1.3.3" xref="S3.SS1.SSS3.p1.1.m1.3.3.cmml">s</mi><mo id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2.4" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml">,</mo><mi id="S3.SS1.SSS3.p1.1.m1.4.4" xref="S3.SS1.SSS3.p1.1.m1.4.4.cmml">r</mi><mo id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2.5" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml">,</mo><mover accent="true" id="S3.SS1.SSS3.p1.1.m1.5.5" xref="S3.SS1.SSS3.p1.1.m1.5.5.cmml"><mi id="S3.SS1.SSS3.p1.1.m1.5.5.2" xref="S3.SS1.SSS3.p1.1.m1.5.5.2.cmml">u</mi><mo id="S3.SS1.SSS3.p1.1.m1.5.5.1" xref="S3.SS1.SSS3.p1.1.m1.5.5.1.cmml">˙</mo></mover><mo id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2.6" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml">,</mo><mover accent="true" id="S3.SS1.SSS3.p1.1.m1.6.6" xref="S3.SS1.SSS3.p1.1.m1.6.6.cmml"><mi id="S3.SS1.SSS3.p1.1.m1.6.6.2" xref="S3.SS1.SSS3.p1.1.m1.6.6.2.cmml">v</mi><mo id="S3.SS1.SSS3.p1.1.m1.6.6.1" xref="S3.SS1.SSS3.p1.1.m1.6.6.1.cmml">˙</mo></mover><mo id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2.7" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml">,</mo><mover accent="true" id="S3.SS1.SSS3.p1.1.m1.7.7" xref="S3.SS1.SSS3.p1.1.m1.7.7.cmml"><mi id="S3.SS1.SSS3.p1.1.m1.7.7.2" xref="S3.SS1.SSS3.p1.1.m1.7.7.2.cmml">s</mi><mo id="S3.SS1.SSS3.p1.1.m1.7.7.1" xref="S3.SS1.SSS3.p1.1.m1.7.7.1.cmml">˙</mo></mover><mo stretchy="false" id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2.8" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml">]</mo></mrow><mo id="S3.SS1.SSS3.p1.1.m1.7.8.3.3" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.3.cmml">⊤</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.1.m1.7b"><apply id="S3.SS1.SSS3.p1.1.m1.7.8.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.8"><eq id="S3.SS1.SSS3.p1.1.m1.7.8.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.8.1"></eq><ci id="S3.SS1.SSS3.p1.1.m1.7.8.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.8.2">𝑥</ci><apply id="S3.SS1.SSS3.p1.1.m1.7.8.3.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.8.3"><csymbol cd="ambiguous" id="S3.SS1.SSS3.p1.1.m1.7.8.3.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.8.3">superscript</csymbol><list id="S3.SS1.SSS3.p1.1.m1.7.8.3.2.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.2.2"><ci id="S3.SS1.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.1.1">𝑢</ci><ci id="S3.SS1.SSS3.p1.1.m1.2.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.2.2">𝑣</ci><ci id="S3.SS1.SSS3.p1.1.m1.3.3.cmml" xref="S3.SS1.SSS3.p1.1.m1.3.3">𝑠</ci><ci id="S3.SS1.SSS3.p1.1.m1.4.4.cmml" xref="S3.SS1.SSS3.p1.1.m1.4.4">𝑟</ci><apply id="S3.SS1.SSS3.p1.1.m1.5.5.cmml" xref="S3.SS1.SSS3.p1.1.m1.5.5"><ci id="S3.SS1.SSS3.p1.1.m1.5.5.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.5.5.1">˙</ci><ci id="S3.SS1.SSS3.p1.1.m1.5.5.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.5.5.2">𝑢</ci></apply><apply id="S3.SS1.SSS3.p1.1.m1.6.6.cmml" xref="S3.SS1.SSS3.p1.1.m1.6.6"><ci id="S3.SS1.SSS3.p1.1.m1.6.6.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.6.6.1">˙</ci><ci id="S3.SS1.SSS3.p1.1.m1.6.6.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.6.6.2">𝑣</ci></apply><apply id="S3.SS1.SSS3.p1.1.m1.7.7.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.7"><ci id="S3.SS1.SSS3.p1.1.m1.7.7.1.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.7.1">˙</ci><ci id="S3.SS1.SSS3.p1.1.m1.7.7.2.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.7.2">𝑠</ci></apply></list><csymbol cd="latexml" id="S3.SS1.SSS3.p1.1.m1.7.8.3.3.cmml" xref="S3.SS1.SSS3.p1.1.m1.7.8.3.3">top</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.1.m1.7c">x=[u,v,s,r,\dot{u},\dot{v},\dot{s}]^{\top}</annotation></semantics></math>,
where <math id="S3.SS1.SSS3.p1.2.m2.1" class="ltx_Math" alttext="u" display="inline"><semantics id="S3.SS1.SSS3.p1.2.m2.1a"><mi id="S3.SS1.SSS3.p1.2.m2.1.1" xref="S3.SS1.SSS3.p1.2.m2.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.2.m2.1b"><ci id="S3.SS1.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS3.p1.2.m2.1.1">𝑢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.2.m2.1c">u</annotation></semantics></math>, <math id="S3.SS1.SSS3.p1.3.m3.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S3.SS1.SSS3.p1.3.m3.1a"><mi id="S3.SS1.SSS3.p1.3.m3.1.1" xref="S3.SS1.SSS3.p1.3.m3.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.3.m3.1b"><ci id="S3.SS1.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS3.p1.3.m3.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.3.m3.1c">v</annotation></semantics></math>, <math id="S3.SS1.SSS3.p1.4.m4.1" class="ltx_Math" alttext="\dot{u}" display="inline"><semantics id="S3.SS1.SSS3.p1.4.m4.1a"><mover accent="true" id="S3.SS1.SSS3.p1.4.m4.1.1" xref="S3.SS1.SSS3.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS3.p1.4.m4.1.1.2" xref="S3.SS1.SSS3.p1.4.m4.1.1.2.cmml">u</mi><mo id="S3.SS1.SSS3.p1.4.m4.1.1.1" xref="S3.SS1.SSS3.p1.4.m4.1.1.1.cmml">˙</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.4.m4.1b"><apply id="S3.SS1.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS3.p1.4.m4.1.1"><ci id="S3.SS1.SSS3.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS3.p1.4.m4.1.1.1">˙</ci><ci id="S3.SS1.SSS3.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS3.p1.4.m4.1.1.2">𝑢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.4.m4.1c">\dot{u}</annotation></semantics></math> and <math id="S3.SS1.SSS3.p1.5.m5.1" class="ltx_Math" alttext="\dot{v}" display="inline"><semantics id="S3.SS1.SSS3.p1.5.m5.1a"><mover accent="true" id="S3.SS1.SSS3.p1.5.m5.1.1" xref="S3.SS1.SSS3.p1.5.m5.1.1.cmml"><mi id="S3.SS1.SSS3.p1.5.m5.1.1.2" xref="S3.SS1.SSS3.p1.5.m5.1.1.2.cmml">v</mi><mo id="S3.SS1.SSS3.p1.5.m5.1.1.1" xref="S3.SS1.SSS3.p1.5.m5.1.1.1.cmml">˙</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.5.m5.1b"><apply id="S3.SS1.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS3.p1.5.m5.1.1"><ci id="S3.SS1.SSS3.p1.5.m5.1.1.1.cmml" xref="S3.SS1.SSS3.p1.5.m5.1.1.1">˙</ci><ci id="S3.SS1.SSS3.p1.5.m5.1.1.2.cmml" xref="S3.SS1.SSS3.p1.5.m5.1.1.2">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.5.m5.1c">\dot{v}</annotation></semantics></math> represent the centre coordinates and velocity in pixels/frame of the bounding box, <math id="S3.SS1.SSS3.p1.6.m6.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S3.SS1.SSS3.p1.6.m6.1a"><mi id="S3.SS1.SSS3.p1.6.m6.1.1" xref="S3.SS1.SSS3.p1.6.m6.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.6.m6.1b"><ci id="S3.SS1.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS3.p1.6.m6.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.6.m6.1c">s</annotation></semantics></math> and <math id="S3.SS1.SSS3.p1.7.m7.1" class="ltx_Math" alttext="\dot{s}" display="inline"><semantics id="S3.SS1.SSS3.p1.7.m7.1a"><mover accent="true" id="S3.SS1.SSS3.p1.7.m7.1.1" xref="S3.SS1.SSS3.p1.7.m7.1.1.cmml"><mi id="S3.SS1.SSS3.p1.7.m7.1.1.2" xref="S3.SS1.SSS3.p1.7.m7.1.1.2.cmml">s</mi><mo id="S3.SS1.SSS3.p1.7.m7.1.1.1" xref="S3.SS1.SSS3.p1.7.m7.1.1.1.cmml">˙</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.7.m7.1b"><apply id="S3.SS1.SSS3.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS3.p1.7.m7.1.1"><ci id="S3.SS1.SSS3.p1.7.m7.1.1.1.cmml" xref="S3.SS1.SSS3.p1.7.m7.1.1.1">˙</ci><ci id="S3.SS1.SSS3.p1.7.m7.1.1.2.cmml" xref="S3.SS1.SSS3.p1.7.m7.1.1.2">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.7.m7.1c">\dot{s}</annotation></semantics></math> represent the bounding box area and change in area respectively, and <math id="S3.SS1.SSS3.p1.8.m8.1" class="ltx_Math" alttext="r" display="inline"><semantics id="S3.SS1.SSS3.p1.8.m8.1a"><mi id="S3.SS1.SSS3.p1.8.m8.1.1" xref="S3.SS1.SSS3.p1.8.m8.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS3.p1.8.m8.1b"><ci id="S3.SS1.SSS3.p1.8.m8.1.1.cmml" xref="S3.SS1.SSS3.p1.8.m8.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS3.p1.8.m8.1c">r</annotation></semantics></math> represents the aspect ratio of the bounding box, which is assumed to be constant. We apply small changes to parameters outlined in Table <a href="#S4.T2" title="TABLE II ‣ IV-A Implementation ‣ IV Experiments ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> to better handle occlusions and instances of missed subsequent associations to a tracker, which are dangers for a guide-dog-like aid.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">3D fusion</span>
</h3>

<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS1.5.1.1" class="ltx_text">III-B</span>1 </span>LiDAR scan filtering</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">Given stable 2D bounding boxes from the aforementioned vision-based detector-and-tracking step, we first select the LiDAR points that fall into a bounding box by projecting each point of a LiDAR scan into the image using <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{L}" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><msubsup id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.2.cmml">𝐓</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.2.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3.cmml">C</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml">L</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.2">𝐓</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.3">𝐶</ci></apply><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">𝐿</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">\mathbf{T}_{C}^{L}</annotation></semantics></math> and the camera intrinsics as illustrated in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B2 3D tracking ‣ III-B 3D fusion ‣ III Multi-modal scene understanding ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>
Unfortunately, the points associated with a bounding box do not only correspond to the detected object but also to the foreground and background.
Accordingly, we propose a simple filtering method to only extract points belonging to the detected object.
Based on the assumption that the centre of the detected object is roughly aligned with the centre of the bounding box, only points present in a square around the bounding box centre are considered.
The ratio of the square’s area to the bounding box’s area is scaled linearly with the ratio of the bounding box’s area to the image resolution. Therefore, as the bounding box gets smaller, the ratio of the square to bounding box area increases, and vice-versa.
For the rest of the pipeline, the object position is represented with a single 3D point.
Accordingly, we use the median of the points inside the square to feed the tracker presented in the following subsection.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S3.SS2.SSS2.5.1.1" class="ltx_text">III-B</span>2 </span>3D tracking</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Provided with the point representation from the LiDAR scan filtering and the bounding box tracking ID from Section <a href="#S3.SS1.SSS3" title="III-A3 2D tracking ‣ III-A 2D object detection and tracking ‣ III Multi-modal scene understanding ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>3</span></a>, the proposed pipeline initialises and maintains independent CVKF for each object track.
Inspired by the work in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>,
the CVKF state vector consists of the object position and velocity: <math id="S3.SS2.SSS2.p1.1.m1.6" class="ltx_Math" alttext="x_{3D}=[x,y,z,\dot{x},\dot{y},\dot{z}]^{\top}" display="inline"><semantics id="S3.SS2.SSS2.p1.1.m1.6a"><mrow id="S3.SS2.SSS2.p1.1.m1.6.7" xref="S3.SS2.SSS2.p1.1.m1.6.7.cmml"><msub id="S3.SS2.SSS2.p1.1.m1.6.7.2" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.6.7.2.2" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.2.cmml">x</mi><mrow id="S3.SS2.SSS2.p1.1.m1.6.7.2.3" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.3.cmml"><mn id="S3.SS2.SSS2.p1.1.m1.6.7.2.3.2" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.3.2.cmml">3</mn><mo lspace="0em" rspace="0em" id="S3.SS2.SSS2.p1.1.m1.6.7.2.3.1" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.3.1.cmml">​</mo><mi id="S3.SS2.SSS2.p1.1.m1.6.7.2.3.3" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.3.3.cmml">D</mi></mrow></msub><mo id="S3.SS2.SSS2.p1.1.m1.6.7.1" xref="S3.SS2.SSS2.p1.1.m1.6.7.1.cmml">=</mo><msup id="S3.SS2.SSS2.p1.1.m1.6.7.3" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.cmml"><mrow id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml"><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2.1" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml">[</mo><mi id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2.2" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml">,</mo><mi id="S3.SS2.SSS2.p1.1.m1.2.2" xref="S3.SS2.SSS2.p1.1.m1.2.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2.3" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml">,</mo><mi id="S3.SS2.SSS2.p1.1.m1.3.3" xref="S3.SS2.SSS2.p1.1.m1.3.3.cmml">z</mi><mo id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2.4" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml">,</mo><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.4.4" xref="S3.SS2.SSS2.p1.1.m1.4.4.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.4.4.2" xref="S3.SS2.SSS2.p1.1.m1.4.4.2.cmml">x</mi><mo id="S3.SS2.SSS2.p1.1.m1.4.4.1" xref="S3.SS2.SSS2.p1.1.m1.4.4.1.cmml">˙</mo></mover><mo id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2.5" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml">,</mo><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.5.5" xref="S3.SS2.SSS2.p1.1.m1.5.5.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.5.5.2" xref="S3.SS2.SSS2.p1.1.m1.5.5.2.cmml">y</mi><mo id="S3.SS2.SSS2.p1.1.m1.5.5.1" xref="S3.SS2.SSS2.p1.1.m1.5.5.1.cmml">˙</mo></mover><mo id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2.6" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml">,</mo><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.6.6" xref="S3.SS2.SSS2.p1.1.m1.6.6.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.6.6.2" xref="S3.SS2.SSS2.p1.1.m1.6.6.2.cmml">z</mi><mo id="S3.SS2.SSS2.p1.1.m1.6.6.1" xref="S3.SS2.SSS2.p1.1.m1.6.6.1.cmml">˙</mo></mover><mo stretchy="false" id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2.7" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml">]</mo></mrow><mo id="S3.SS2.SSS2.p1.1.m1.6.7.3.3" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.3.cmml">⊤</mo></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.6b"><apply id="S3.SS2.SSS2.p1.1.m1.6.7.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7"><eq id="S3.SS2.SSS2.p1.1.m1.6.7.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.1"></eq><apply id="S3.SS2.SSS2.p1.1.m1.6.7.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.2"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.6.7.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.2">subscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.6.7.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.2">𝑥</ci><apply id="S3.SS2.SSS2.p1.1.m1.6.7.2.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.3"><times id="S3.SS2.SSS2.p1.1.m1.6.7.2.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.3.1"></times><cn type="integer" id="S3.SS2.SSS2.p1.1.m1.6.7.2.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.3.2">3</cn><ci id="S3.SS2.SSS2.p1.1.m1.6.7.2.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.2.3.3">𝐷</ci></apply></apply><apply id="S3.SS2.SSS2.p1.1.m1.6.7.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.6.7.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.3">superscript</csymbol><list id="S3.SS2.SSS2.p1.1.m1.6.7.3.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.2.2"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1">𝑥</ci><ci id="S3.SS2.SSS2.p1.1.m1.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.2.2">𝑦</ci><ci id="S3.SS2.SSS2.p1.1.m1.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.3.3">𝑧</ci><apply id="S3.SS2.SSS2.p1.1.m1.4.4.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4"><ci id="S3.SS2.SSS2.p1.1.m1.4.4.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.1">˙</ci><ci id="S3.SS2.SSS2.p1.1.m1.4.4.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.4.4.2">𝑥</ci></apply><apply id="S3.SS2.SSS2.p1.1.m1.5.5.cmml" xref="S3.SS2.SSS2.p1.1.m1.5.5"><ci id="S3.SS2.SSS2.p1.1.m1.5.5.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.5.5.1">˙</ci><ci id="S3.SS2.SSS2.p1.1.m1.5.5.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.5.5.2">𝑦</ci></apply><apply id="S3.SS2.SSS2.p1.1.m1.6.6.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.6"><ci id="S3.SS2.SSS2.p1.1.m1.6.6.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.6.1">˙</ci><ci id="S3.SS2.SSS2.p1.1.m1.6.6.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.6.2">𝑧</ci></apply></list><csymbol cd="latexml" id="S3.SS2.SSS2.p1.1.m1.6.7.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.6.7.3.3">top</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.6c">x_{3D}=[x,y,z,\dot{x},\dot{y},\dot{z}]^{\top}</annotation></semantics></math>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><svg id="S3.F4.pic1" class="ltx_picture ltx_centering" height="164.3" overflow="visible" version="1.1" width="166.18"><g transform="translate(0,164.3) matrix(1 0 0 -1 0 0) translate(56.61,0) translate(0,116.19)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -52 -43.5)" fill="#000000" stroke="#000000"><foreignObject width="104" height="87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x2.jpg" id="S3.F4.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="115" height="96" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 0.95 -43.5)" fill="#000000" stroke="#000000"><foreignObject width="104" height="87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x3.jpg" id="S3.F4.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="115" height="96" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 0.95 -43.5)" fill="#000000" stroke="#000000"><foreignObject width="104" height="87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x4.jpg" id="S3.F4.pic1.3.3.3.3.3.3.3.3.3.3.3.3.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="115" height="96" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -52 -111.57)" fill="#000000" stroke="#000000"><foreignObject width="104" height="87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x5.jpg" id="S3.F4.pic1.4.4.4.4.4.4.4.4.4.4.4.4.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="115" height="96" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 0.95 -43.5)" fill="#000000" stroke="#000000"><foreignObject width="104" height="87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x6.jpg" id="S3.F4.pic1.5.5.5.5.5.5.5.5.5.5.5.5.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="115" height="96" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 0.95 -43.5)" fill="#000000" stroke="#000000"><foreignObject width="104" height="87" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x7.jpg" id="S3.F4.pic1.6.6.6.6.6.6.6.6.6.6.6.6.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="115" height="96" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -44.97 -10.64)" fill="#000000" stroke="#000000"><foreignObject width="89.94" height="20.77" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F4.pic1.7.7.7.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S3.F4.pic1.7.7.7.1.1.1" class="ltx_p"></span>
<span id="S3.F4.pic1.7.7.7.1.1.2" class="ltx_p"><span id="S3.F4.pic1.7.7.7.1.1.2.1" class="ltx_text" style="font-size:70%;">(a) RGB bounding box</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -44.97 -10.64)" fill="#000000" stroke="#000000"><foreignObject width="89.94" height="22.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F4.pic1.8.8.8.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S3.F4.pic1.8.8.8.1.1.1" class="ltx_p"></span>
<span id="S3.F4.pic1.8.8.8.1.1.2" class="ltx_p"><span id="S3.F4.pic1.8.8.8.1.1.2.1" class="ltx_text" style="font-size:70%;">(b) RGB-LiDAR points</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -44.97 -10.64)" fill="#000000" stroke="#000000"><foreignObject width="89.94" height="25.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F4.pic1.9.9.9.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S3.F4.pic1.9.9.9.1.1.1" class="ltx_p"></span>
<span id="S3.F4.pic1.9.9.9.1.1.2" class="ltx_p"><span id="S3.F4.pic1.9.9.9.1.1.2.1" class="ltx_text" style="font-size:70%;">(c) RGB-LiDAR filtered points</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -44.97 -10.64)" fill="#000000" stroke="#000000"><foreignObject width="89.94" height="20.77" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F4.pic1.10.10.10.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S3.F4.pic1.10.10.10.1.1.1" class="ltx_p"></span>
<span id="S3.F4.pic1.10.10.10.1.1.2" class="ltx_p"><span id="S3.F4.pic1.10.10.10.1.1.2.1" class="ltx_text" style="font-size:70%;">(d) Event bounding box</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -44.97 -10.64)" fill="#000000" stroke="#000000"><foreignObject width="89.94" height="22.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F4.pic1.11.11.11.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S3.F4.pic1.11.11.11.1.1.1" class="ltx_p"></span>
<span id="S3.F4.pic1.11.11.11.1.1.2" class="ltx_p"><span id="S3.F4.pic1.11.11.11.1.1.2.1" class="ltx_text" style="font-size:70%;">(e) Event-LiDAR points</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -44.97 -10.64)" fill="#000000" stroke="#000000"><foreignObject width="89.94" height="25.21" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S3.F4.pic1.12.12.12.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S3.F4.pic1.12.12.12.1.1.1" class="ltx_p"></span>
<span id="S3.F4.pic1.12.12.12.1.1.2" class="ltx_p"><span id="S3.F4.pic1.12.12.12.1.1.2.1" class="ltx_text" style="font-size:70%;">(f) Event-LiDAR filtered points</span></span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Bounding box point cloud segmentation and filtering examples via vision-based (RGB and event) object detection.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Implementation</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">The quantitative results are obtained using the proposed dataset collected with an <span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">Inivation DAVIS346</span> camera and a <span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">Blickfeld Cube1</span> LiDAR.
For the event-based detector, we empirically chose the 1-Mpx model of RVT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> after testing Gen1 and 1-Mpx using our dataset.
No significant performance difference was found.
Both RGB-based and event-based object detectors were used without any retraining or fine-tuning of the networks’ weights.
Table <a href="#S4.T2" title="TABLE II ‣ IV-A Implementation ‣ IV Experiments ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> shows the parameters of the SORT algorithm for vision-based tracking (Section <a href="#S3.SS1.SSS3" title="III-A3 2D tracking ‣ III-A 2D object detection and tracking ‣ III Multi-modal scene understanding ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span>3</span></a>).
The RGB and event detectors have different noise characteristics, so SORT tracker parameters differ slightly.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>The SORT algorithm parameters for image space object tracking.</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" style="width:170.7pt;">
<span id="S4.T2.1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.1.1.1" class="ltx_p"><span id="S4.T2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Parameter</span></span>
</span>
</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.1.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.2.1.1" class="ltx_p"><span id="S4.T2.1.1.1.2.1.1.1" class="ltx_text ltx_font_bold">RGB</span></span>
</span>
</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t">
<span id="S4.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.1.1.3.1.1" class="ltx_p"><span id="S4.T2.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Event</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S4.T2.1.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.1.1.1" class="ltx_p">Maximum age of unmatched tracker [no. of frames]</span>
</span>
</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.1.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.2.1.1" class="ltx_p">10</span>
</span>
</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.1.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.2.1.3.1.1" class="ltx_p">10</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S4.T2.1.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.1.1.1" class="ltx_p">Maximum unmatched predictions [no. frames]</span>
</span>
</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.1.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.2.1.1" class="ltx_p">5</span>
</span>
</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.1.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.3.2.3.1.1" class="ltx_p">3</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S4.T2.1.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.1.1.1" class="ltx_p">Min. number of associated detections for tracking</span>
</span>
</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.1.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.2.1.1" class="ltx_p">3</span>
</span>
</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.1.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.4.3.3.1.1" class="ltx_p">1</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" style="width:170.7pt;">
<span id="S4.T2.1.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.4.1.1.1" class="ltx_p">Min. number of previous associations for prediction</span>
</span>
</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.1.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.4.2.1.1" class="ltx_p">10</span>
</span>
</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_justify ltx_border_t">
<span id="S4.T2.1.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.5.4.3.1.1" class="ltx_p">1</span>
</span>
</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<td id="S4.T2.1.6.5.1" class="ltx_td ltx_align_justify ltx_align_middle ltx_border_b ltx_border_t" style="width:170.7pt;">
<span id="S4.T2.1.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.5.1.1.1" class="ltx_p">IoU threshold for association</span>
</span>
</td>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S4.T2.1.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.5.2.1.1" class="ltx_p">0.3</span>
</span>
</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_justify ltx_border_b ltx_border_t">
<span id="S4.T2.1.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S4.T2.1.6.5.3.1.1" class="ltx_p">0.3</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Experiments were conducted on a low-performance laptop with Ubuntu 20.04.6 LTS, an NVIDIA GTX GeForce 1650 GPU, an AMD Ryzen 7 5700U CPU, and 16GB of RAM.
The proposed pipeline runs close to real-time with both RGB-based and event-based detection.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Vision-based object detection</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To evaluate the performance of the vision-based object detectors in our pipeline for assistive robotics in dynamic settings, we performed object detection with both YOLOv4 and RVT using the proposed dataset. We only consider YOLOv4 detections above a confidence score of 0.5 while varying the RVT confidence threshold across evaluations.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">Each RGB frame is associated with a 50 ms event tensor/sequence required for RVT’s prediction.
Table <a href="#S4.T3" title="TABLE III ‣ IV-B Vision-based object detection ‣ IV Experiments ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> displays RVT’s precision and recall (confidence threshold of 0.3), with and without the tracking, using YOLOv4 as the ground truth due to its proven accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
The definitions of true/false positive/negative for precision and recall are based on IoU thresholds between the bounding boxes of both methods.
Table <a href="#S4.T3" title="TABLE III ‣ IV-B Vision-based object detection ‣ IV Experiments ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> displays results for varying IoU thresholds, while Table <a href="#S4.T4" title="TABLE IV ‣ IV-B Vision-based object detection ‣ IV Experiments ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a> shows results for a fixed IoU threshold with varying RVT confidence thresholds.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">The results show that YOLOv4 outperforms the RVT model. The precision scores suggest a higher number of misclassifications with erroneous class attribution.
Interestingly, using SORT increased the recall but decreased the precision. This suggests that when true positive detections occur in one sequence of events but not in the ensuing sequences, the SORT algorithm improves the detection rate due to its ability to predict the subsequent positions of an object, reducing the number of false negatives.
However, when the tracker incorrectly estimates the object dynamics or false positive detections occur, the precision score decreases.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Evaluation of event-based detection vs. YOLOv4: varying IoU thresholds @ confidence threshold = 0.3.</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td" colspan="2" rowspan="2"></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left" colspan="9"><span id="S4.T3.1.1.1.2.1" class="ltx_text ltx_font_bold">Event-Based Detection: IoU thresholds @ Confidence threshold = 0.3</span></td>
</tr>
<tr id="S4.T3.1.2.2" class="ltx_tr">
<td id="S4.T3.1.2.2.1" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.1.1" class="ltx_text ltx_font_bold">0.5</span></td>
<td id="S4.T3.1.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.2.1" class="ltx_text ltx_font_bold">0.55</span></td>
<td id="S4.T3.1.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.3.1" class="ltx_text ltx_font_bold">0.6</span></td>
<td id="S4.T3.1.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.4.1" class="ltx_text ltx_font_bold">0.65</span></td>
<td id="S4.T3.1.2.2.5" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.5.1" class="ltx_text ltx_font_bold">0.7</span></td>
<td id="S4.T3.1.2.2.6" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.6.1" class="ltx_text ltx_font_bold">0.75</span></td>
<td id="S4.T3.1.2.2.7" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.7.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T3.1.2.2.8" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.8.1" class="ltx_text ltx_font_bold">0.85</span></td>
<td id="S4.T3.1.2.2.9" class="ltx_td ltx_align_center"><span id="S4.T3.1.2.2.9.1" class="ltx_text ltx_font_bold">0.9</span></td>
</tr>
<tr id="S4.T3.1.3.3" class="ltx_tr">
<td id="S4.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T3.1.3.3.1.1" class="ltx_text ltx_font_bold">Pure Detection</span></td>
<td id="S4.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.3.3.2.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S4.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.625</td>
<td id="S4.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.573</td>
<td id="S4.T3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.516</td>
<td id="S4.T3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.447</td>
<td id="S4.T3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">0.357</td>
<td id="S4.T3.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">0.256</td>
<td id="S4.T3.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">0.168</td>
<td id="S4.T3.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">0.082</td>
<td id="S4.T3.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">0.026</td>
</tr>
<tr id="S4.T3.1.4.4" class="ltx_tr">
<td id="S4.T3.1.4.4.1" class="ltx_td ltx_align_center"><span id="S4.T3.1.4.4.1.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S4.T3.1.4.4.2" class="ltx_td ltx_align_center">0.423</td>
<td id="S4.T3.1.4.4.3" class="ltx_td ltx_align_center">0.388</td>
<td id="S4.T3.1.4.4.4" class="ltx_td ltx_align_center">0.349</td>
<td id="S4.T3.1.4.4.5" class="ltx_td ltx_align_center">0.303</td>
<td id="S4.T3.1.4.4.6" class="ltx_td ltx_align_center">0.242</td>
<td id="S4.T3.1.4.4.7" class="ltx_td ltx_align_center">0.174</td>
<td id="S4.T3.1.4.4.8" class="ltx_td ltx_align_center">0.114</td>
<td id="S4.T3.1.4.4.9" class="ltx_td ltx_align_center">0.055</td>
<td id="S4.T3.1.4.4.10" class="ltx_td ltx_align_center">0.017</td>
</tr>
<tr id="S4.T3.1.5.5" class="ltx_tr">
<td id="S4.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="2"><span id="S4.T3.1.5.5.1.1" class="ltx_text ltx_font_bold">Tracked w/ SORT</span></td>
<td id="S4.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.5.5.2.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S4.T3.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.577</td>
<td id="S4.T3.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.533</td>
<td id="S4.T3.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">0.478</td>
<td id="S4.T3.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">0.409</td>
<td id="S4.T3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">0.328</td>
<td id="S4.T3.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">0.236</td>
<td id="S4.T3.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t">0.146</td>
<td id="S4.T3.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t">0.068</td>
<td id="S4.T3.1.5.5.11" class="ltx_td ltx_align_center ltx_border_t">0.02</td>
</tr>
<tr id="S4.T3.1.6.6" class="ltx_tr">
<td id="S4.T3.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.1.6.6.1.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S4.T3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b">0.463</td>
<td id="S4.T3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b">0.427</td>
<td id="S4.T3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b">0.383</td>
<td id="S4.T3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b">0.328</td>
<td id="S4.T3.1.6.6.6" class="ltx_td ltx_align_center ltx_border_b">0.263</td>
<td id="S4.T3.1.6.6.7" class="ltx_td ltx_align_center ltx_border_b">0.189</td>
<td id="S4.T3.1.6.6.8" class="ltx_td ltx_align_center ltx_border_b">0.117</td>
<td id="S4.T3.1.6.6.9" class="ltx_td ltx_align_center ltx_border_b">0.055</td>
<td id="S4.T3.1.6.6.10" class="ltx_td ltx_align_center ltx_border_b">0.016</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Evaluation of event-based detection vs. YOLOv4: varying confidence thresholds @ IoU threshold = 0.5.</figcaption>
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<td id="S4.T4.1.1.1.1" class="ltx_td" colspan="2" rowspan="2"></td>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center" colspan="13"><span id="S4.T4.1.1.1.2.1" class="ltx_text ltx_font_bold">Event-Based Detection: Confidence thresholds @ IoU threshold = 0.5</span></td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<td id="S4.T4.1.2.2.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.1.1" class="ltx_text ltx_font_bold">0.3</span></td>
<td id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.2.1" class="ltx_text ltx_font_bold">0.35</span></td>
<td id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.3.1" class="ltx_text ltx_font_bold">0.4</span></td>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.4.1" class="ltx_text ltx_font_bold">0.45</span></td>
<td id="S4.T4.1.2.2.5" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.5.1" class="ltx_text ltx_font_bold">0.5</span></td>
<td id="S4.T4.1.2.2.6" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.6.1" class="ltx_text ltx_font_bold">0.55</span></td>
<td id="S4.T4.1.2.2.7" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.7.1" class="ltx_text ltx_font_bold">0.6</span></td>
<td id="S4.T4.1.2.2.8" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.8.1" class="ltx_text ltx_font_bold">0.65</span></td>
<td id="S4.T4.1.2.2.9" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.9.1" class="ltx_text ltx_font_bold">0.7</span></td>
<td id="S4.T4.1.2.2.10" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.10.1" class="ltx_text ltx_font_bold">0.75</span></td>
<td id="S4.T4.1.2.2.11" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.11.1" class="ltx_text ltx_font_bold">0.8</span></td>
<td id="S4.T4.1.2.2.12" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.12.1" class="ltx_text ltx_font_bold">0.85</span></td>
<td id="S4.T4.1.2.2.13" class="ltx_td ltx_align_center"><span id="S4.T4.1.2.2.13.1" class="ltx_text ltx_font_bold">0.9</span></td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<td id="S4.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="2"><span id="S4.T4.1.3.3.1.1" class="ltx_text ltx_font_bold">Pure Detection</span></td>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.3.3.2.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.577</td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.658</td>
<td id="S4.T4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.688</td>
<td id="S4.T4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.72</td>
<td id="S4.T4.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">0.756</td>
<td id="S4.T4.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">0.787</td>
<td id="S4.T4.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">0.818</td>
<td id="S4.T4.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">0.849</td>
<td id="S4.T4.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">0.885</td>
<td id="S4.T4.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t">0.916</td>
<td id="S4.T4.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t">0.956</td>
<td id="S4.T4.1.3.3.14" class="ltx_td ltx_align_center ltx_border_t">0.996</td>
<td id="S4.T4.1.3.3.15" class="ltx_td ltx_align_center ltx_border_t">1.0</td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<td id="S4.T4.1.4.4.1" class="ltx_td ltx_align_center"><span id="S4.T4.1.4.4.1.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S4.T4.1.4.4.2" class="ltx_td ltx_align_center">0.463</td>
<td id="S4.T4.1.4.4.3" class="ltx_td ltx_align_center">0.414</td>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_center">0.404</td>
<td id="S4.T4.1.4.4.5" class="ltx_td ltx_align_center">0.394</td>
<td id="S4.T4.1.4.4.6" class="ltx_td ltx_align_center">0.383</td>
<td id="S4.T4.1.4.4.7" class="ltx_td ltx_align_center">0.37</td>
<td id="S4.T4.1.4.4.8" class="ltx_td ltx_align_center">0.352</td>
<td id="S4.T4.1.4.4.9" class="ltx_td ltx_align_center">0.327</td>
<td id="S4.T4.1.4.4.10" class="ltx_td ltx_align_center">0.294</td>
<td id="S4.T4.1.4.4.11" class="ltx_td ltx_align_center">0.239</td>
<td id="S4.T4.1.4.4.12" class="ltx_td ltx_align_center">0.156</td>
<td id="S4.T4.1.4.4.13" class="ltx_td ltx_align_center">0.044</td>
<td id="S4.T4.1.4.4.14" class="ltx_td ltx_align_center">0.001</td>
</tr>
<tr id="S4.T4.1.5.5" class="ltx_tr">
<td id="S4.T4.1.5.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" rowspan="2"><span id="S4.T4.1.5.5.1.1" class="ltx_text ltx_font_bold">Tracked w/ SORT</span></td>
<td id="S4.T4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T4.1.5.5.2.1" class="ltx_text ltx_font_bold">Precision</span></td>
<td id="S4.T4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_t">0.625</td>
<td id="S4.T4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_t">0.611</td>
<td id="S4.T4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">0.643</td>
<td id="S4.T4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">0.681</td>
<td id="S4.T4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_t">0.718</td>
<td id="S4.T4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_t">0.747</td>
<td id="S4.T4.1.5.5.9" class="ltx_td ltx_align_center ltx_border_t">0.784</td>
<td id="S4.T4.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t">0.817</td>
<td id="S4.T4.1.5.5.11" class="ltx_td ltx_align_center ltx_border_t">0.844</td>
<td id="S4.T4.1.5.5.12" class="ltx_td ltx_align_center ltx_border_t">0.857</td>
<td id="S4.T4.1.5.5.13" class="ltx_td ltx_align_center ltx_border_t">0.878</td>
<td id="S4.T4.1.5.5.14" class="ltx_td ltx_align_center ltx_border_t">0.88</td>
<td id="S4.T4.1.5.5.15" class="ltx_td ltx_align_center ltx_border_t">1.0</td>
</tr>
<tr id="S4.T4.1.6.6" class="ltx_tr">
<td id="S4.T4.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T4.1.6.6.1.1" class="ltx_text ltx_font_bold">Recall</span></td>
<td id="S4.T4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b">0.423</td>
<td id="S4.T4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b">0.45</td>
<td id="S4.T4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b">0.441</td>
<td id="S4.T4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b">0.432</td>
<td id="S4.T4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_b">0.423</td>
<td id="S4.T4.1.6.6.7" class="ltx_td ltx_align_center ltx_border_b">0.408</td>
<td id="S4.T4.1.6.6.8" class="ltx_td ltx_align_center ltx_border_b">0.391</td>
<td id="S4.T4.1.6.6.9" class="ltx_td ltx_align_center ltx_border_b">0.368</td>
<td id="S4.T4.1.6.6.10" class="ltx_td ltx_align_center ltx_border_b">0.33</td>
<td id="S4.T4.1.6.6.11" class="ltx_td ltx_align_center ltx_border_b">0.274</td>
<td id="S4.T4.1.6.6.12" class="ltx_td ltx_align_center ltx_border_b">0.19</td>
<td id="S4.T4.1.6.6.13" class="ltx_td ltx_align_center ltx_border_b">0.055</td>
<td id="S4.T4.1.6.6.14" class="ltx_td ltx_align_center ltx_border_b">0.001</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">3D object tracking</span>
</h3>

<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS1.5.1.1" class="ltx_text">IV-C</span>1 </span>Quantitative</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.3" class="ltx_p">Using the proposed detection pipeline and dataset, we evaluate the overall accuracy using the Mean Absolute Error (MAE) and the Root Mean Square Error (RMSE) between the predicted object 3D position in the camera reference frame and the ground truth value from the motion-capture system <math id="S4.SS3.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{T}_{W}^{M_{\bullet}}" display="inline"><semantics id="S4.SS3.SSS1.p1.1.m1.1a"><msubsup id="S4.SS3.SSS1.p1.1.m1.1.1" xref="S4.SS3.SSS1.p1.1.m1.1.1.cmml"><mi id="S4.SS3.SSS1.p1.1.m1.1.1.2.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.2.cmml">𝐓</mi><mi id="S4.SS3.SSS1.p1.1.m1.1.1.2.3" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.3.cmml">W</mi><msub id="S4.SS3.SSS1.p1.1.m1.1.1.3" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml"><mi id="S4.SS3.SSS1.p1.1.m1.1.1.3.2" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.2.cmml">M</mi><mo id="S4.SS3.SSS1.p1.1.m1.1.1.3.3" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.3.cmml">∙</mo></msub></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.1.m1.1b"><apply id="S4.SS3.SSS1.p1.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.1.m1.1.1.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">superscript</csymbol><apply id="S4.SS3.SSS1.p1.1.m1.1.1.2.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.2">𝐓</ci><ci id="S4.SS3.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.2.3">𝑊</ci></apply><apply id="S4.SS3.SSS1.p1.1.m1.1.1.3.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.3">subscript</csymbol><ci id="S4.SS3.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.2">𝑀</ci><ci id="S4.SS3.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S4.SS3.SSS1.p1.1.m1.1.1.3.3">∙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.1.m1.1c">\mathbf{T}_{W}^{M_{\bullet}}</annotation></semantics></math>, <math id="S4.SS3.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{T}_{W}^{M_{S}}" display="inline"><semantics id="S4.SS3.SSS1.p1.2.m2.1a"><msubsup id="S4.SS3.SSS1.p1.2.m2.1.1" xref="S4.SS3.SSS1.p1.2.m2.1.1.cmml"><mi id="S4.SS3.SSS1.p1.2.m2.1.1.2.2" xref="S4.SS3.SSS1.p1.2.m2.1.1.2.2.cmml">𝐓</mi><mi id="S4.SS3.SSS1.p1.2.m2.1.1.2.3" xref="S4.SS3.SSS1.p1.2.m2.1.1.2.3.cmml">W</mi><msub id="S4.SS3.SSS1.p1.2.m2.1.1.3" xref="S4.SS3.SSS1.p1.2.m2.1.1.3.cmml"><mi id="S4.SS3.SSS1.p1.2.m2.1.1.3.2" xref="S4.SS3.SSS1.p1.2.m2.1.1.3.2.cmml">M</mi><mi id="S4.SS3.SSS1.p1.2.m2.1.1.3.3" xref="S4.SS3.SSS1.p1.2.m2.1.1.3.3.cmml">S</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.2.m2.1b"><apply id="S4.SS3.SSS1.p1.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.2.m2.1.1.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">superscript</csymbol><apply id="S4.SS3.SSS1.p1.2.m2.1.1.2.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.2.m2.1.1.2.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.2.m2.1.1.2.2.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.2.2">𝐓</ci><ci id="S4.SS3.SSS1.p1.2.m2.1.1.2.3.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.2.3">𝑊</ci></apply><apply id="S4.SS3.SSS1.p1.2.m2.1.1.3.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS3.SSS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.3.2">𝑀</ci><ci id="S4.SS3.SSS1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS3.SSS1.p1.2.m2.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.2.m2.1c">\mathbf{T}_{W}^{M_{S}}</annotation></semantics></math>, and the calibration <math id="S4.SS3.SSS1.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{T}_{C}^{M_{S}}" display="inline"><semantics id="S4.SS3.SSS1.p1.3.m3.1a"><msubsup id="S4.SS3.SSS1.p1.3.m3.1.1" xref="S4.SS3.SSS1.p1.3.m3.1.1.cmml"><mi id="S4.SS3.SSS1.p1.3.m3.1.1.2.2" xref="S4.SS3.SSS1.p1.3.m3.1.1.2.2.cmml">𝐓</mi><mi id="S4.SS3.SSS1.p1.3.m3.1.1.2.3" xref="S4.SS3.SSS1.p1.3.m3.1.1.2.3.cmml">C</mi><msub id="S4.SS3.SSS1.p1.3.m3.1.1.3" xref="S4.SS3.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S4.SS3.SSS1.p1.3.m3.1.1.3.2" xref="S4.SS3.SSS1.p1.3.m3.1.1.3.2.cmml">M</mi><mi id="S4.SS3.SSS1.p1.3.m3.1.1.3.3" xref="S4.SS3.SSS1.p1.3.m3.1.1.3.3.cmml">S</mi></msub></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p1.3.m3.1b"><apply id="S4.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.3.m3.1.1.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1">superscript</csymbol><apply id="S4.SS3.SSS1.p1.3.m3.1.1.2.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.3.m3.1.1.2.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.SSS1.p1.3.m3.1.1.2.2.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.2.2">𝐓</ci><ci id="S4.SS3.SSS1.p1.3.m3.1.1.2.3.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.2.3">𝐶</ci></apply><apply id="S4.SS3.SSS1.p1.3.m3.1.1.3.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS3.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.3.2">𝑀</ci><ci id="S4.SS3.SSS1.p1.3.m3.1.1.3.3.cmml" xref="S4.SS3.SSS1.p1.3.m3.1.1.3.3">𝑆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p1.3.m3.1c">\mathbf{T}_{C}^{M_{S}}</annotation></semantics></math>.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE V: </span>Accuracy analysis of 3D object position estimation.</figcaption>
<table id="S4.T5.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1" class="ltx_tr">
<td id="S4.T5.1.1.2" class="ltx_td"></td>
<td id="S4.T5.1.1.3" class="ltx_td"></td>
<td id="S4.T5.1.1.4" class="ltx_td ltx_align_center" colspan="2">
<span id="S4.T5.1.1.4.1" class="ltx_text ltx_font_bold">RGB</span> detection</td>
<td id="S4.T5.1.1.5" class="ltx_td"></td>
<td id="S4.T5.1.1.1" class="ltx_td ltx_align_center" colspan="2">
<span id="S4.T5.1.1.1.1" class="ltx_text ltx_font_bold">Event</span> detection<sup id="S4.T5.1.1.1.2" class="ltx_sup">∗</sup>
</td>
</tr>
<tr id="S4.T5.2.3.1" class="ltx_tr">
<td id="S4.T5.2.3.1.1" class="ltx_td"></td>
<td id="S4.T5.2.3.1.2" class="ltx_td"></td>
<td id="S4.T5.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.3.1.3.1" class="ltx_text ltx_font_bold">MAE</span></td>
<td id="S4.T5.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.3.1.4.1" class="ltx_text ltx_font_bold">RMSE</span></td>
<td id="S4.T5.2.3.1.5" class="ltx_td"></td>
<td id="S4.T5.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.3.1.6.1" class="ltx_text ltx_font_bold">MAE</span></td>
<td id="S4.T5.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.3.1.7.1" class="ltx_text ltx_font_bold">RMSE</span></td>
</tr>
<tr id="S4.T5.2.4.2" class="ltx_tr">
<td id="S4.T5.2.4.2.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T5.2.4.2.1.1" class="ltx_text">
<span id="S4.T5.2.4.2.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.2.4.2.1.1.1.1" class="ltx_tr">
<span id="S4.T5.2.4.2.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.2.4.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Filtering</span></span></span>
<span id="S4.T5.2.4.2.1.1.1.2" class="ltx_tr">
<span id="S4.T5.2.4.2.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.2.4.2.1.1.1.2.1.1" class="ltx_text ltx_font_bold">only</span></span></span>
</span></span></td>
<td id="S4.T5.2.4.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.4.2.2.1" class="ltx_text ltx_font_bold">X</span></td>
<td id="S4.T5.2.4.2.3" class="ltx_td ltx_align_center ltx_border_t">0.283</td>
<td id="S4.T5.2.4.2.4" class="ltx_td ltx_align_center ltx_border_t">0.513</td>
<td id="S4.T5.2.4.2.5" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.2.4.2.6" class="ltx_td ltx_align_center ltx_border_t">0.232</td>
<td id="S4.T5.2.4.2.7" class="ltx_td ltx_align_center ltx_border_t">0.41</td>
</tr>
<tr id="S4.T5.2.5.3" class="ltx_tr">
<td id="S4.T5.2.5.3.1" class="ltx_td ltx_align_center"><span id="S4.T5.2.5.3.1.1" class="ltx_text ltx_font_bold">Y</span></td>
<td id="S4.T5.2.5.3.2" class="ltx_td ltx_align_center">0.765</td>
<td id="S4.T5.2.5.3.3" class="ltx_td ltx_align_center">0.79</td>
<td id="S4.T5.2.5.3.4" class="ltx_td"></td>
<td id="S4.T5.2.5.3.5" class="ltx_td ltx_align_center">0.86</td>
<td id="S4.T5.2.5.3.6" class="ltx_td ltx_align_center">0.888</td>
</tr>
<tr id="S4.T5.2.6.4" class="ltx_tr">
<td id="S4.T5.2.6.4.1" class="ltx_td ltx_align_center"><span id="S4.T5.2.6.4.1.1" class="ltx_text ltx_font_bold">Z</span></td>
<td id="S4.T5.2.6.4.2" class="ltx_td ltx_align_center">0.724</td>
<td id="S4.T5.2.6.4.3" class="ltx_td ltx_align_center">1.397</td>
<td id="S4.T5.2.6.4.4" class="ltx_td"></td>
<td id="S4.T5.2.6.4.5" class="ltx_td ltx_align_center">0.929</td>
<td id="S4.T5.2.6.4.6" class="ltx_td ltx_align_center">1.613</td>
</tr>
<tr id="S4.T5.2.7.5" class="ltx_tr">
<td id="S4.T5.2.7.5.1" class="ltx_td ltx_align_center"><span id="S4.T5.2.7.5.1.1" class="ltx_text ltx_font_bold">XZ</span></td>
<td id="S4.T5.2.7.5.2" class="ltx_td ltx_align_center">0.828</td>
<td id="S4.T5.2.7.5.3" class="ltx_td ltx_align_center">1.488</td>
<td id="S4.T5.2.7.5.4" class="ltx_td"></td>
<td id="S4.T5.2.7.5.5" class="ltx_td ltx_align_center">0.983</td>
<td id="S4.T5.2.7.5.6" class="ltx_td ltx_align_center">1.665</td>
</tr>
<tr id="S4.T5.2.8.6" class="ltx_tr">
<td id="S4.T5.2.8.6.1" class="ltx_td ltx_align_center ltx_border_t" rowspan="4"><span id="S4.T5.2.8.6.1.1" class="ltx_text">
<span id="S4.T5.2.8.6.1.1.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T5.2.8.6.1.1.1.1" class="ltx_tr">
<span id="S4.T5.2.8.6.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.2.8.6.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Filtering</span></span></span>
<span id="S4.T5.2.8.6.1.1.1.2" class="ltx_tr">
<span id="S4.T5.2.8.6.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.2.8.6.1.1.1.2.1.1" class="ltx_text ltx_font_bold">and</span></span></span>
<span id="S4.T5.2.8.6.1.1.1.3" class="ltx_tr">
<span id="S4.T5.2.8.6.1.1.1.3.1" class="ltx_td ltx_nopad_r ltx_align_center"><span id="S4.T5.2.8.6.1.1.1.3.1.1" class="ltx_text ltx_font_bold">CVKF</span></span></span>
</span></span></td>
<td id="S4.T5.2.8.6.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T5.2.8.6.2.1" class="ltx_text ltx_font_bold">X</span></td>
<td id="S4.T5.2.8.6.3" class="ltx_td ltx_align_center ltx_border_t">0.281</td>
<td id="S4.T5.2.8.6.4" class="ltx_td ltx_align_center ltx_border_t">0.484</td>
<td id="S4.T5.2.8.6.5" class="ltx_td ltx_border_t"></td>
<td id="S4.T5.2.8.6.6" class="ltx_td ltx_align_center ltx_border_t">0.243</td>
<td id="S4.T5.2.8.6.7" class="ltx_td ltx_align_center ltx_border_t">0.434</td>
</tr>
<tr id="S4.T5.2.9.7" class="ltx_tr">
<td id="S4.T5.2.9.7.1" class="ltx_td ltx_align_center"><span id="S4.T5.2.9.7.1.1" class="ltx_text ltx_font_bold">Y</span></td>
<td id="S4.T5.2.9.7.2" class="ltx_td ltx_align_center">0.763</td>
<td id="S4.T5.2.9.7.3" class="ltx_td ltx_align_center">0.788</td>
<td id="S4.T5.2.9.7.4" class="ltx_td"></td>
<td id="S4.T5.2.9.7.5" class="ltx_td ltx_align_center">0.866</td>
<td id="S4.T5.2.9.7.6" class="ltx_td ltx_align_center">0.894</td>
</tr>
<tr id="S4.T5.2.10.8" class="ltx_tr">
<td id="S4.T5.2.10.8.1" class="ltx_td ltx_align_center"><span id="S4.T5.2.10.8.1.1" class="ltx_text ltx_font_bold">Z</span></td>
<td id="S4.T5.2.10.8.2" class="ltx_td ltx_align_center">0.727</td>
<td id="S4.T5.2.10.8.3" class="ltx_td ltx_align_center">1.712</td>
<td id="S4.T5.2.10.8.4" class="ltx_td"></td>
<td id="S4.T5.2.10.8.5" class="ltx_td ltx_align_center">0.98</td>
<td id="S4.T5.2.10.8.6" class="ltx_td ltx_align_center">1.663</td>
</tr>
<tr id="S4.T5.2.11.9" class="ltx_tr">
<td id="S4.T5.2.11.9.1" class="ltx_td ltx_align_center"><span id="S4.T5.2.11.9.1.1" class="ltx_text ltx_font_bold">XZ</span></td>
<td id="S4.T5.2.11.9.2" class="ltx_td ltx_align_center">0.831</td>
<td id="S4.T5.2.11.9.3" class="ltx_td ltx_align_center">1.779</td>
<td id="S4.T5.2.11.9.4" class="ltx_td"></td>
<td id="S4.T5.2.11.9.5" class="ltx_td ltx_align_center">1.033</td>
<td id="S4.T5.2.11.9.6" class="ltx_td ltx_align_center">1.719</td>
</tr>
<tr id="S4.T5.2.2" class="ltx_tr">
<td id="S4.T5.2.2.1" class="ltx_td ltx_align_left ltx_border_t" colspan="7">
<sup id="S4.T5.2.2.1.1" class="ltx_sup"><span id="S4.T5.2.2.1.1.1" class="ltx_text" style="font-size:70%;">∗</span></sup><span id="S4.T5.2.2.1.2" class="ltx_text" style="font-size:70%;"> The event evaluation uses only one minute of the dataset</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">Table <a href="#S4.T5" title="TABLE V ‣ IV-C1 Quantitative ‣ IV-C 3D object tracking ‣ IV Experiments ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a> shows the results from both detection methods.
To achieve an unbiased evaluation of the 3D estimation framework, the event-based prediction uses a manually selected one-minute portion of the “dynamic.bag” ROSBag where the RVT detector performs well, while the RGB-based pipeline uses the full ROSBag.
The ground truth for people’s positions is at head level, and the LiDAR filtering focuses on the hip level.
Metrics are separated by the individual axis and the XZ plane.
The Y-axis (gravity-aligned axis) error of just under a metre reflects head-vs-hip tracking.
Overall, both modalities result in a range of approximately 0.8 to 1 m MAE in the XZ plane, validating the proposed detection/tracking pipeline.
The larger Z-axis error (depth) compared to the X-axis indicates that scan filtering does not fully isolate the object from the foreground and background, as seen in Figure <a href="#S3.F4" title="Figure 4 ‣ III-B2 3D tracking ‣ III-B 3D fusion ‣ III Multi-modal scene understanding ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> where the spread of 3D point cloud data contained inside a 2D bounding box is broader along the depth axis.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">Given that the RMSE weights heavily outlier errors, the difference between MAE and RMSE suggests that a few tracking results are highly inaccurate, while a majority are accurate.
Curiously, the CVKF does not enhance but rather worsens the final estimates.
Thanks to the vision-based SORT tracking, the output of the LiDAR filtering step is already smooth.
Thus, the inherent delay of the final CVKF’s estimate with respect to the true state value can only result in lesser accuracy.
Additionally, occlusions and missed associated detections handled by the 2D SORT algorithm lead to the wrong selection of points in the LiDAR scans - leading to high geometric errors.
This correlates with the disparity between MAE and RMSE.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS3.SSS2.5.1.1" class="ltx_text">IV-C</span>2 </span>Qualitative</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">To test and demonstrate the ability of the proposed pipeline to provide spatial awareness for assistive robots such as guide-dog-like aid, a sensor suite consisting of an Intel RealSense camera and Velodyne VLP-16 LiDAR was utilised in an urban environment.
This data was collected to inspect the effectiveness of the RGB version of our pipeline on longer-range detections and the ability to estimate vehicle dynamics.
While no ground truth is available for quantitative evaluation, Fig.<a href="#S4.F5" title="Figure 5 ‣ IV-C2 Qualitative ‣ IV-C 3D object tracking ‣ IV Experiments ‣ Towards Robust Perception for Assistive Robotics: An RGB-Event-LiDAR Dataset and Multi-Modal Detection Pipeline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows multiple detections of pedestrians and vehicles and their estimated velocity.
These correspond to the expected velocities of the difference agents.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><svg id="S4.F5.pic1" class="ltx_picture ltx_centering" height="108.22" overflow="visible" version="1.1" width="256.18"><g transform="translate(0,108.22) matrix(1 0 0 -1 0 0) translate(85.61,0) translate(0,54.11)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 -81 -49.5)" fill="#000000" stroke="#000000"><foreignObject width="162" height="99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x8.jpg" id="S4.F5.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="173" height="109" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 0.95 -49.5)" fill="#000000" stroke="#000000"><foreignObject width="165" height="99" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible"><img src="/html/2408.13394/assets/x9.jpg" id="S4.F5.pic1.2.2.2.2.2.2.2.2.2.2.2.2.1.1.1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="182" height="109" alt="Refer to caption"></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -44.97 -10.64)" fill="#000000" stroke="#000000"><foreignObject width="89.94" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S4.F5.pic1.3.3.3.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S4.F5.pic1.3.3.3.1.1.1" class="ltx_p"></span>
<span id="S4.F5.pic1.3.3.3.1.1.2" class="ltx_p"><span id="S4.F5.pic1.3.3.3.1.1.2.1" class="ltx_text" style="font-size:70%;">(a) Moving bus</span></span>
</span></foreignObject></g><g transform="matrix(1.0 0.0 0.0 1.0 -44.97 -10.64)" fill="#000000" stroke="#000000"><foreignObject width="89.94" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">
<span id="S4.F5.pic1.4.4.4.1.1" class="ltx_inline-block ltx_minipage ltx_align_top" style="width:65.0pt;">
<span id="S4.F5.pic1.4.4.4.1.1.1" class="ltx_p"></span>
<span id="S4.F5.pic1.4.4.4.1.1.2" class="ltx_p"><span id="S4.F5.pic1.4.4.4.1.1.2.1" class="ltx_text" style="font-size:70%;">(b) Parked cars</span></span>
</span></foreignObject></g></g></svg>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Detection samples and estimated dynamics in an urban environment.</figcaption>
</figure>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Discussion</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In the LiDAR filtering step, assuming the “central square” cropping of 3D points aligns the object’s centre with the bounding box centre is arbitrary and often untrue.
The central part of the bounding box may correspond to background information.
For instance, when a person extends an arm, the bounding box expands, which shifts the centre away from the torso, leading to an incorrect point selection.
Similarly, for vehicles, the bounding box centre may align with the windshield, causing the LiDAR to observe the background.
Future work will explore using efficient per-pixel semantic labels to better handle partial occlusions.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Our pipeline also assumes constant-velocity models in different trackers.
While effective for wheel-based systems like autonomous vehicles, these models fall short for handheld devices, such as in our dataset, and platforms with jerky motion, such as bipedal and quadrupedal robots.
In scenarios mimicking guide dogs, inaccurate vehicle tracking can have severe consequences.
Investigating varied motion models and incorporating the robot’s movement commands might enhance system robustness.
Furthermore, employing trackers like DeepSORT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, which utilise metrics other than IoU, can strengthen frame-to-frame association and tracking.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Expanding into 3D detection improves reliability by merging and cross-checking outputs from multiple modalities, beyond using LiDAR for depth.
However, LiDAR-only detection faces limitations like vertical sparseness and motion distortion.
Sensor fusion is crucial for robust robotic autonomy. Future research should refine detection synchronisation, moving beyond timestamp-based LiDAR scan matching.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">Our findings indicate that event-based object detectors lack adaptability and generalisation, while frame-based detectors are ready for use without retraining, thanks to large, diverse publicly available training datasets. The lack of diverse event camera training data hinders adaptability, as event cameras are more affected by camera motion.
Our dataset will enable the robotics community to investigate these issues, paving the way for safer and more robust algorithms.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">This paper has presented a dataset for comparing event-based and RGB-based multi-modal 3D object detection and tracking with LiDAR data.
The dataset includes RGB, event, LiDAR, and inertial data, along with human ground-truth positions determined by a motion-capture system, addressing a gap in publicly available datasets for applications such as guide-dog-like assistive robots.
We proposed a pipeline for dynamic object detection and tracking that performs vision-based object detection followed by LiDAR-based 3D position estimation.
Our experiments show that frame-based detection algorithms generalise well to various scenes, while the current state-of-the-art event models are limited to smaller, automotive-oriented scenarios.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">Future work will enrich our dataset with data from various mobile platforms (wheeled, bipedal, and quadrupedal).
This is important due to the spatiotemporal nature of the event data: regular movements lead to recurrent patterns in the event stream.
For the proposed pipeline, our efforts will focus on refining 3D localisation and tracking to better adapt to rapid dynamic changes and employing advanced machine learning techniques for more accurate object isolation.
Ultimately, we will integrate the proposed perception framework into an advanced assistive robot to help vision-impaired users navigate challenging environments safely.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Guide Dogs Victoria, “Facts + stats,”
<a target="_blank" href="https://vic.guidedogs.com.au/about-gdv/fact-sheet/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://vic.guidedogs.com.au/about-gdv/fact-sheet/</a>, 2023, accessed:
2023-11-18.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
L. M. Tomkins, P. C. Thomson, and P. D. McGreevy, “Associations between motor,
sensory and structural lateralisation and guide dog success,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">The
Veterinary Journal</em>, vol. 192, no. 3, pp. 359–367, 2012.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
B. Hong, Z. Lin, X. Chen, J. Hou, S. Lv, and Z. Gao, “Development and
application of key technologies for guide dog robot: A systematic literature
review,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Robotics and Autonomous Systems</em>, vol. 154, p. 104104, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with
deep convolutional neural networks,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information
processing systems</em>, vol. 25, 2012.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
object detection with region proposal networks,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on
Pattern Analysis and Machine Intelligence</em>, vol. 39, no. 6, pp. 1137–1149,
2017.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
G. Paul, L. Liu, and D. Liu, “A novel approach to steel rivet detection in
poorly illuminated steel structural environments,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2016 14th
International Conference on Control, Automation, Robotics and Vision
(ICARCV)</em>, 2016, pp. 1–7.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P. Lichtsteiner, C. Posch, and T. Delbruck, “A 128<math id="bib.bib7.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="bib.bib7.1.m1.1a"><mo id="bib.bib7.1.m1.1.1" xref="bib.bib7.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="bib.bib7.1.m1.1b"><times id="bib.bib7.1.m1.1.1.cmml" xref="bib.bib7.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.1.m1.1c">\times</annotation></semantics></math> 128 120 db 15
<math id="bib.bib7.2.m2.1" class="ltx_Math" alttext="\mu" display="inline"><semantics id="bib.bib7.2.m2.1a"><mi id="bib.bib7.2.m2.1.1" xref="bib.bib7.2.m2.1.1.cmml">μ</mi><annotation-xml encoding="MathML-Content" id="bib.bib7.2.m2.1b"><ci id="bib.bib7.2.m2.1.1.cmml" xref="bib.bib7.2.m2.1.1">𝜇</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib7.2.m2.1c">\mu</annotation></semantics></math>s latency asynchronous temporal contrast vision sensor,” <em id="bib.bib7.3.1" class="ltx_emph ltx_font_italic">IEEE
Journal of Solid-State Circuits</em>, vol. 43, no. 2, pp. 566–576, 2008.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
Unified, real-time object detection,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conf.
on computer vision and pattern recognition</em>, 2016, pp. 779–788.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A
large-scale hierarchical image database,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">IEEE Conference on
Computer Vision and Pattern Recognition</em>, 2009, pp. 248–255.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,
P. Dollár, and C. L. Zitnick, “Microsoft coco: Common objects in
context,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV 2014: 13th European Conference,
Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>.   Springer, 2014, pp. 740–755.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset,
S. Kamali, S. Popov, M. Malloci, A. Kolesnikov <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “The open
images dataset v4: Unified image classification, object detection, and visual
relationship detection at scale,” <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">International journal of computer
vision</em>, vol. 128, no. 7, pp. 1956–1981, 2020.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman, “The
pascal visual object classes (voc) challenge,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">International journal
of computer vision</em>, vol. 88, pp. 303–338, 2010.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics: The
kitti dataset,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">The International Journal of Robotics Research</em>,
vol. 32, no. 11, pp. 1231–1237, 2013.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
R. Girshick, “Fast r-cnn,” in <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">2015 IEEE International Conference on
Computer Vision (ICCV)</em>, 2015, pp. 1440–1448.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Bochkovskiy, C.-Y. Wang, and H.-Y. M. Liao, “Yolov4: Optimal speed and
accuracy of object detection,” 2020. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/2004.10934" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2004.10934</a>

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C. Berg,
“Ssd: Single shot multibox detector,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Computer Vision–ECCV
2016</em>.   Springer, 2016, pp. 21–37.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
R. J. Wang, X. Li, and C. X. Ling, “Pelee: A real-time object detection system
on mobile devices,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing
systems</em>, vol. 31, 2018.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Dai, K. He, and J. Sun, “Instance-aware semantic segmentation via
multi-task network cascades,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2016 IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)</em>, 2016, pp. 3150–3158.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2017
IEEE International Conference on Computer Vision (ICCV)</em>, 2017, pp.
2980–2988.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D. Gehrig, A. Loquercio, K. Derpanis, and D. Scaramuzza, “End-to-end learning
of representations for asynchronous event-based data,” in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2019
IEEE/CVF International Conference on Computer Vision (ICCV)</em>, 2019, pp.
5632–5642.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
M. Gehrig, S. B. Shrestha, D. Mouritzen, and D. Scaramuzza, “Event-based
angular velocity regression with spiking networks,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2020 IEEE
International Conference on Robotics and Automation (ICRA)</em>, 2020, pp.
4195–4202.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S. Schaefer, D. Gehrig, and D. Scaramuzza, “Aegnn: Asynchronous event-based
graph neural networks,” in <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)</em>, 2022, pp. 12 361–12 371.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
E. Perot, P. De Tournemire, D. Nitti, J. Masci, and A. Sironi, “Learning to
detect objects with a 1 megapixel event camera,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, vol. 33, pp. 16 639–16 652, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
P. De Tournemire, D. Nitti, E. Perot, D. Migliore, and A. Sironi, “A large
scale event-based detection dataset for automotive,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:2001.08499</em>, 2020.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Gehrig and D. Scaramuzza, “Recurrent vision transformers for object
detection with event cameras,” in <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2023 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR)</em>.   Los Alamitos, CA, USA: IEEE Computer Society, jun 2023, pp.
13 884–13 893.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
L. Zhao, H. Zhou, X. Zhu, X. Song, H. Li, and W. Tao, “Lif-seg: Lidar and
camera image fusion for 3d lidar semantic segmentation,” <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">IEEE
Transactions on Multimedia</em>, vol. 26, pp. 1158–1168, 2024.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
B. Dai, C. Le Gentil, and T. Vidal-Calleja, “Connecting the dots for real-time
lidar-based object detection with yolo,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Australasian Conference on
Robotics and Automation, ACRA</em>, 2018.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
C. Lin, D. Tian, X. Duan, J. Zhou, D. Zhao, and D. Cao, “Cl3d: Camera-lidar 3d
object detection with point feature enhancement and point-guided fusion,”
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 23,
no. 10, pp. 18 040–18 050, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas, “Frustum pointnets for 3d
object detection from rgb-d data,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2018 IEEE/CVF Conf. on Computer
Vision and Pattern Recognition</em>, 2018, pp. 918–927.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
X. Bai, Z. Hu, X. Zhu, Q. Huang, Y. Chen, H. Fu, and C.-L. Tai, “Transfusion:
Robust lidar-camera fusion for 3d object detection with transformers,” in
<em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR)</em>, 2022, pp. 1080–1089.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
S. Vora, A. H. Lang, B. Helou, and O. Beijbom, “Pointpainting: Sequential
fusion for 3d object detection,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR)</em>, 2020, pp. 4603–4611.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
X. Zhu, H. Zhou, T. Wang, F. Hong, W. Li, Y. Ma, H. Li, R. Yang, and D. Lin,
“Cylindrical and asymmetrical 3d convolution networks for lidar-based
perception,” <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine
Intelligence</em>, vol. 44, no. 10, pp. 6807–6822, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online and
realtime tracking,” in <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">2016 IEEE International Conference on Image
Processing (ICIP)</em>, 2016, pp. 3464–3468.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
X. Weng, J. Wang, D. Held, and K. Kitani, “3D Multi-Object Tracking: A
Baseline and New Evaluation Metrics,” <em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">IROS</em>, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
N. Wojke, A. Bewley, and D. Paulus, “Simple online and realtime tracking with
a deep association metric,” 2017. [Online]. Available:
<a target="_blank" href="https://arxiv.org/abs/1703.07402" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/1703.07402</a>

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.13393" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.13394" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.13394">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.13394" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.13395" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 17:46:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
