<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024.</title>
<!--Generated on Fri Jun  7 05:09:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Recommendation system,  computational storage device,  near data processing,  neural network
" lang="en" name="keywords"/>
<base href="/html/2406.14571v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S1" title="In PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2" title="In PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.SS1" title="In II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">End-to-End RecSys Training Pipeline</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.SS2" title="In II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Key Properties of RecSys Data Preprocessing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.SS3" title="In II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Feature Generation/Normalization in Data Preprocessing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.SS4" title="In II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">System Architecture for CPU-centric Data Preprocessing</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3" title="In PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Characterization and Motivation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.SS1" title="In III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Motivation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.SS2" title="In III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Breakdown of End-to-End Data Preprocessing Time</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.SS3" title="In III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Analysis on Performance-Limiting Operations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.SS4" title="In III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Our Goal: Scalable <math alttext="\&amp;" class="ltx_Math" display="inline"><semantics><mo>&amp;</mo><annotation-xml encoding="MathML-Content"><and></and></annotation-xml><annotation encoding="application/x-tex">\&amp;</annotation><annotation encoding="application/x-llamapun">&amp;</annotation></semantics></math> Cost-Effective Preprocessing</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4" title="In PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><math alttext="PreSto" class="ltx_Math" display="inline"><semantics><mrow><mi>P</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mi>o</mi></mrow><annotation-xml encoding="MathML-Content"><apply><times></times><ci>𝑃</ci><ci>𝑟</ci><ci>𝑒</ci><ci>𝑆</ci><ci>𝑡</ci><ci>𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex">PreSto</annotation><annotation encoding="application/x-llamapun">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math><span class="ltx_text ltx_font_smallcaps">: An In-Storage “Pre”processing Architecture for RecSys Training</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.SS1" title="In IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">System Design Considerations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.SS2" title="In IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Proposed Approach: In-Storage Data Preprocessing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.SS3" title="In IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Data Preprocessing Accelerator Microarchitecture</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5" title="In PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Methodology</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5.SS1" title="In V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Benchmarks</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5.SS2" title="In V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Experimental Setup</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5.SS3" title="In V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Evaluation Methods</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6" title="In PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Evaluation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.SS1" title="In VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Performance and Cost-Effectiveness of <math alttext="PreSto" class="ltx_Math" display="inline"><semantics><mrow><mi>P</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mi>o</mi></mrow><annotation-xml encoding="MathML-Content"><apply><times></times><ci>𝑃</ci><ci>𝑟</ci><ci>𝑒</ci><ci>𝑆</ci><ci>𝑡</ci><ci>𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex">PreSto</annotation><annotation encoding="application/x-llamapun">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (PoC)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.SS2" title="In VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><math alttext="PreSto" class="ltx_Math" display="inline"><semantics><mrow><mi>P</mi><mo>⁢</mo><mi>r</mi><mo>⁢</mo><mi>e</mi><mo>⁢</mo><mi>S</mi><mo>⁢</mo><mi>t</mi><mo>⁢</mo><mi>o</mi></mrow><annotation-xml encoding="MathML-Content"><apply><times></times><ci>𝑃</ci><ci>𝑟</ci><ci>𝑒</ci><ci>𝑆</ci><ci>𝑡</ci><ci>𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex">PreSto</annotation><annotation encoding="application/x-llamapun">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math><span class="ltx_text ltx_font_italic">’s Effect on Energy-Efficiency and TCO</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.SS3" title="In VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-C</span> </span><span class="ltx_text ltx_font_italic">PreSto vs. Alternative Accelerated Preprocessing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.SS4" title="In VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-D</span> </span><span class="ltx_text ltx_font_italic">PreSto Sensitivity to the Number of Features to Preprocess</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S7" title="In PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Related work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S8" title="In PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document" style="font-size:207%;">PreSto: An In-Storage Data Preprocessing System
<br class="ltx_break"/>for Training Recommendation Models
<span class="ltx_note ltx_role_thanks" id="id4.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>
2 Co-first authors who contributed equally to this research.
<br class="ltx_break"/><span class="ltx_rule" style="width:113.8pt;height:0.4pt;background:black;display:inline-block;"> </span>
<br class="ltx_break"/>This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024.
</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Yunjae Lee2
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Electrical Engineering
<br class="ltx_break"/>KAIST
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id5.1.id1">yunjae408@kaist.ac.kr</span>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hyeseong Kim2
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Electrical Engineering
<br class="ltx_break"/>KAIST
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id6.1.id1">hyeseong.kim@kaist.ac.kr</span>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minsoo Rhu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">School of Electrical Engineering
<br class="ltx_break"/>KAIST
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.1.id1">mrhu@kaist.ac.kr</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id3.3">Training recommendation systems (RecSys) faces
several challenges as it requires the “data preprocessing” stage to preprocess
an ample amount of raw data and feed them to the GPU for training in a
seamless manner. To sustain high training throughput, state-of-the-art
solutions reserve a large fleet of CPU servers for preprocessing
which incurs substantial deployment cost and power consumption.
Our characterization reveals that prior CPU-centric preprocessing
is bottlenecked on feature generation and feature normalization
operations as it fails to reap out the abundant inter-/intra-feature
parallelism in RecSys preprocessing. PreSto
is a storage-centric preprocessing system leveraging In-Storage
Processing (ISP), which offloads the bottlenecked preprocessing
operations to our ISP units. We show that PreSto outperforms the baseline
CPU-centric system with a <math alttext="9.6\times" class="ltx_math_unparsed" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mrow id="id1.1.m1.1b"><mn id="id1.1.m1.1.1">9.6</mn><mo id="id1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="id1.1.m1.1c">9.6\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">9.6 ×</annotation></semantics></math> speedup in end-to-end
preprocessing time, <math alttext="4.3\times" class="ltx_math_unparsed" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mrow id="id2.2.m2.1b"><mn id="id2.2.m2.1.1">4.3</mn><mo id="id2.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="id2.2.m2.1c">4.3\times</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">4.3 ×</annotation></semantics></math> enhancement in cost-efficiency, and
<math alttext="11.3\times" class="ltx_math_unparsed" display="inline" id="id3.3.m3.1"><semantics id="id3.3.m3.1a"><mrow id="id3.3.m3.1b"><mn id="id3.3.m3.1.1">11.3</mn><mo id="id3.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="id3.3.m3.1c">11.3\times</annotation><annotation encoding="application/x-llamapun" id="id3.3.m3.1d">11.3 ×</annotation></semantics></math> improvement in energy-efficiency on average for
production-scale RecSys preprocessing.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Recommendation system, computational storage device, near data processing, neural network

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Deep neural network (DNN) based machine learning (ML) algorithms have
demonstrated their effectiveness in a wide range of application domains.
Among the successfully deployed ML applications, recommendation systems
(RecSys) have emerged as a highly effective tool for online content
recommendation services.
Such rising demand for recommendation services has rendered hyperscalers to
dedicate significant resources to the development and training of diverse
RecSys models to ensure high-quality inference services. Unlike
latency-optimized ML inference, training algorithms are
throughput-hungry workloads that favor high-performance, throughput-optimized
accelerators like GPUs. However, these power-hungry GPUs
account for a large portion of ML system’s operating expenses, so
maintaining high GPU utilization becomes critical for lowering
TCO (total cost of ownership). Unfortunately,
keeping the RecSys training pipeline busy with minimal GPU idle
time requires the “data preprocessing” stage to
preprocess an ample amount of raw data, so that
the preprocessed, train-ready tensors can be fed into the GPU in a
seamless manner.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Traditionally, the RecSys training pipeline employed <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">offline</em> data preprocessing
where the raw data retrieved from the storage system is transformed into
train-ready tensors in advance and gets archived at a separate storage space for future
usage. However, the proliferation of petabyte-scale
data and the wide variety of RecSys models developed by ML engineers have
rendered offline preprocessing to incur an intractable amount of
overhead <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite>. Specifically, it becomes increasingly difficult to provision the
substantial storage space required to store all the data preprocessed offline, while
also adapting to changes in the newly developed RecSys models.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">These challenges have triggered a shift towards <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">online</em> preprocessing,
which involves preprocessing the raw data “on-the-fly”. Online
preprocessing obviates the need to separately store the preprocessed
data, so it helps better respond to changes in the model
architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite>. Nevertheless, these online preprocessing
approaches introduce several system-level challenges, potentially causing
a throughput mismatch between data preprocessing and ML model training.
Consider a scenario where preprocessing and training jobs are
<em class="ltx_emph ltx_font_italic" id="S1.p3.1.2">co-located</em> within the same GPU training server node, i.e.,
preprocessing is undertaken on the host CPU that also manages the
GPU-side training job. If the CPU does not have a high enough computation
power for preprocessing, it fails in generating sufficient amount of
train-ready tensors that the GPU can consume, leading to significant GPU
underutilization (less than <math alttext="20\%" class="ltx_Math" display="inline" id="S1.p3.1.m1.1"><semantics id="S1.p3.1.m1.1a"><mrow id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mn id="S1.p3.1.m1.1.1.2" xref="S1.p3.1.m1.1.1.2.cmml">20</mn><mo id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><csymbol cd="latexml" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1">percent</csymbol><cn id="S1.p3.1.m1.1.1.2.cmml" type="integer" xref="S1.p3.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S1.p3.1.m1.1d">20 %</annotation></semantics></math> GPU utility, Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.SS1" title="III-A Motivation ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>).
To address these challenges, Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite> and Audibert et
al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib5" title="">5</a>]</cite> suggest a <em class="ltx_emph ltx_font_italic" id="S1.p3.1.3">server disaggregation</em> solution where
a pool of CPU servers is reserved for
preprocessing purposes. Disaggregating CPU servers for preprocessing
allows hundreds to thousands of CPU cores to be allocated
<em class="ltx_emph ltx_font_italic" id="S1.p3.1.4">on-demand</em>, even for a single data preprocessing job, effectively
closing the performance gap between preprocessing and model training
thereby minimizing GPU idle time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib25" title="">25</a>]</cite>. However, this baseline “CPU-centric” disaggregated preprocessing
incurs significant deployment cost and power consumption due to
the large number of pooled CPU servers.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Given this landscape, an important objective of our work is to characterize
baseline CPU-centric disaggregated data preprocessing systems targeting
production-scale RecSys models, root-causing its critical system-level
challenges. A key observation we make is that the majority of data
preprocessing time is spent conducting <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">feature generation</em> and
<em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">feature normalization</em> operations, which inherently contain high
<em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">inter-/intra-feature parallelism</em>. However, the latency-optimized CPU
architectures, the de facto standard in data preprocessing, fail to fully
exploit the abundant inter-/intra-feature parallelism in feature
generation and normalization, leading to sub-optimal performance. This in turn
leads to the feature generation and normalization to account for <math alttext="79\%" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mn id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">79</mn><mo id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><csymbol cd="latexml" id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1">percent</csymbol><cn id="S1.p4.1.m1.1.1.2.cmml" type="integer" xref="S1.p4.1.m1.1.1.2">79</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">79\%</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">79 %</annotation></semantics></math>
of the RecSys data preprocessing time, causing the most significant performance
bottleneck. To make up for the meager preprocessing throughput provided with
CPUs, the data preprocessing stage necessitates a large number of CPU cores (up
to several hundreds) to be allocated so that its aggregate preprocessing
throughput matches the throughput demands of GPU’s model training stage, which
leads to substantial deployment cost.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we propose to employ <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">accelerated</em> computing for RecSys data
preprocessing to fundamentally address its system-level challenges at low cost.
Because the abundant inter-/intra-feature parallelism in data preprocessing is
well-suited for domain-specific acceleration, our first key proposal is to
<em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">offload</em> the time-consuming feature generation and normalization
operations to our accelerator for high-performance data preprocessing. An
important design decision still remains, however, regarding <em class="ltx_emph ltx_font_italic" id="S1.p5.1.3">where</em> our
data preprocessing accelerator should be placed within the overall system
architecture. State-of-the-art data preprocessing solutions
employ disaggregated CPU servers to dynamically allocate the right amount of
CPU cores for data preprocessing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib5" title="">5</a>]</cite>. While using our data
preprocessing accelerator as a drop-in replacement for CPUs within the
disaggregated CPU servers is a practically feasible option, we observe that
such a design point is sub-optimal as it unnecessarily incurs high network
traffic to copy data in (the raw data to be preprocessed) and out (the
preprocessed train-ready tensors) of the disaggregated server for data
preprocessing.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.6">To this end, we present <math alttext="PreSto" class="ltx_Math" display="inline" id="S1.p6.1.m1.1"><semantics id="S1.p6.1.m1.1a"><mrow id="S1.p6.1.m1.1.1" xref="S1.p6.1.m1.1.1.cmml"><mi id="S1.p6.1.m1.1.1.2" xref="S1.p6.1.m1.1.1.2.cmml">P</mi><mo id="S1.p6.1.m1.1.1.1" xref="S1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p6.1.m1.1.1.3" xref="S1.p6.1.m1.1.1.3.cmml">r</mi><mo id="S1.p6.1.m1.1.1.1a" xref="S1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p6.1.m1.1.1.4" xref="S1.p6.1.m1.1.1.4.cmml">e</mi><mo id="S1.p6.1.m1.1.1.1b" xref="S1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p6.1.m1.1.1.5" xref="S1.p6.1.m1.1.1.5.cmml">S</mi><mo id="S1.p6.1.m1.1.1.1c" xref="S1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p6.1.m1.1.1.6" xref="S1.p6.1.m1.1.1.6.cmml">t</mi><mo id="S1.p6.1.m1.1.1.1d" xref="S1.p6.1.m1.1.1.1.cmml">⁢</mo><mi id="S1.p6.1.m1.1.1.7" xref="S1.p6.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.1.m1.1b"><apply id="S1.p6.1.m1.1.1.cmml" xref="S1.p6.1.m1.1.1"><times id="S1.p6.1.m1.1.1.1.cmml" xref="S1.p6.1.m1.1.1.1"></times><ci id="S1.p6.1.m1.1.1.2.cmml" xref="S1.p6.1.m1.1.1.2">𝑃</ci><ci id="S1.p6.1.m1.1.1.3.cmml" xref="S1.p6.1.m1.1.1.3">𝑟</ci><ci id="S1.p6.1.m1.1.1.4.cmml" xref="S1.p6.1.m1.1.1.4">𝑒</ci><ci id="S1.p6.1.m1.1.1.5.cmml" xref="S1.p6.1.m1.1.1.5">𝑆</ci><ci id="S1.p6.1.m1.1.1.6.cmml" xref="S1.p6.1.m1.1.1.6">𝑡</ci><ci id="S1.p6.1.m1.1.1.7.cmml" xref="S1.p6.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S1.p6.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (<span class="ltx_text ltx_font_bold" id="S1.p6.6.1">Pre</span>processing in-<span class="ltx_text ltx_font_bold" id="S1.p6.6.2">Sto</span>rage), which is
an In-Storage Processing (ISP) based data preprocessing system for RecSys
training. In conventional systems, the petabyte-scale raw data that is to be preprocessed are stored in a distributed storage system.
Rather than copying the raw data over the datacenter network and preprocessing
them at a disaggregated, <em class="ltx_emph ltx_font_italic" id="S1.p6.6.3">remote</em> preprocessing server, <math alttext="PreSto" class="ltx_Math" display="inline" id="S1.p6.2.m2.1"><semantics id="S1.p6.2.m2.1a"><mrow id="S1.p6.2.m2.1.1" xref="S1.p6.2.m2.1.1.cmml"><mi id="S1.p6.2.m2.1.1.2" xref="S1.p6.2.m2.1.1.2.cmml">P</mi><mo id="S1.p6.2.m2.1.1.1" xref="S1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S1.p6.2.m2.1.1.3" xref="S1.p6.2.m2.1.1.3.cmml">r</mi><mo id="S1.p6.2.m2.1.1.1a" xref="S1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S1.p6.2.m2.1.1.4" xref="S1.p6.2.m2.1.1.4.cmml">e</mi><mo id="S1.p6.2.m2.1.1.1b" xref="S1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S1.p6.2.m2.1.1.5" xref="S1.p6.2.m2.1.1.5.cmml">S</mi><mo id="S1.p6.2.m2.1.1.1c" xref="S1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S1.p6.2.m2.1.1.6" xref="S1.p6.2.m2.1.1.6.cmml">t</mi><mo id="S1.p6.2.m2.1.1.1d" xref="S1.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S1.p6.2.m2.1.1.7" xref="S1.p6.2.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.2.m2.1b"><apply id="S1.p6.2.m2.1.1.cmml" xref="S1.p6.2.m2.1.1"><times id="S1.p6.2.m2.1.1.1.cmml" xref="S1.p6.2.m2.1.1.1"></times><ci id="S1.p6.2.m2.1.1.2.cmml" xref="S1.p6.2.m2.1.1.2">𝑃</ci><ci id="S1.p6.2.m2.1.1.3.cmml" xref="S1.p6.2.m2.1.1.3">𝑟</ci><ci id="S1.p6.2.m2.1.1.4.cmml" xref="S1.p6.2.m2.1.1.4">𝑒</ci><ci id="S1.p6.2.m2.1.1.5.cmml" xref="S1.p6.2.m2.1.1.5">𝑆</ci><ci id="S1.p6.2.m2.1.1.6.cmml" xref="S1.p6.2.m2.1.1.6">𝑡</ci><ci id="S1.p6.2.m2.1.1.7.cmml" xref="S1.p6.2.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.2.m2.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S1.p6.2.m2.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> conducts
preprocessing <em class="ltx_emph ltx_font_italic" id="S1.p6.6.4">near data</em> using ISP. We demonstrate that such
“storage-centric” data preprocessing can effectively close
the performance gap between preprocessing and model training at a much lower
cost compared to existing solutions.
Overall, <math alttext="PreSto" class="ltx_Math" display="inline" id="S1.p6.3.m3.1"><semantics id="S1.p6.3.m3.1a"><mrow id="S1.p6.3.m3.1.1" xref="S1.p6.3.m3.1.1.cmml"><mi id="S1.p6.3.m3.1.1.2" xref="S1.p6.3.m3.1.1.2.cmml">P</mi><mo id="S1.p6.3.m3.1.1.1" xref="S1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="S1.p6.3.m3.1.1.3" xref="S1.p6.3.m3.1.1.3.cmml">r</mi><mo id="S1.p6.3.m3.1.1.1a" xref="S1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="S1.p6.3.m3.1.1.4" xref="S1.p6.3.m3.1.1.4.cmml">e</mi><mo id="S1.p6.3.m3.1.1.1b" xref="S1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="S1.p6.3.m3.1.1.5" xref="S1.p6.3.m3.1.1.5.cmml">S</mi><mo id="S1.p6.3.m3.1.1.1c" xref="S1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="S1.p6.3.m3.1.1.6" xref="S1.p6.3.m3.1.1.6.cmml">t</mi><mo id="S1.p6.3.m3.1.1.1d" xref="S1.p6.3.m3.1.1.1.cmml">⁢</mo><mi id="S1.p6.3.m3.1.1.7" xref="S1.p6.3.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.p6.3.m3.1b"><apply id="S1.p6.3.m3.1.1.cmml" xref="S1.p6.3.m3.1.1"><times id="S1.p6.3.m3.1.1.1.cmml" xref="S1.p6.3.m3.1.1.1"></times><ci id="S1.p6.3.m3.1.1.2.cmml" xref="S1.p6.3.m3.1.1.2">𝑃</ci><ci id="S1.p6.3.m3.1.1.3.cmml" xref="S1.p6.3.m3.1.1.3">𝑟</ci><ci id="S1.p6.3.m3.1.1.4.cmml" xref="S1.p6.3.m3.1.1.4">𝑒</ci><ci id="S1.p6.3.m3.1.1.5.cmml" xref="S1.p6.3.m3.1.1.5">𝑆</ci><ci id="S1.p6.3.m3.1.1.6.cmml" xref="S1.p6.3.m3.1.1.6">𝑡</ci><ci id="S1.p6.3.m3.1.1.7.cmml" xref="S1.p6.3.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p6.3.m3.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S1.p6.3.m3.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> provides
high speedup on data preprocessing performance (average
<math alttext="9.6\times" class="ltx_math_unparsed" display="inline" id="S1.p6.4.m4.1"><semantics id="S1.p6.4.m4.1a"><mrow id="S1.p6.4.m4.1b"><mn id="S1.p6.4.m4.1.1">9.6</mn><mo id="S1.p6.4.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.p6.4.m4.1c">9.6\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.4.m4.1d">9.6 ×</annotation></semantics></math>) at low cost, significantly reducing the TCO (average <math alttext="4.3\times" class="ltx_math_unparsed" display="inline" id="S1.p6.5.m5.1"><semantics id="S1.p6.5.m5.1a"><mrow id="S1.p6.5.m5.1b"><mn id="S1.p6.5.m5.1.1">4.3</mn><mo id="S1.p6.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.p6.5.m5.1c">4.3\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.5.m5.1d">4.3 ×</annotation></semantics></math>) and energy consumption (average <math alttext="11.3\times" class="ltx_math_unparsed" display="inline" id="S1.p6.6.m6.1"><semantics id="S1.p6.6.m6.1a"><mrow id="S1.p6.6.m6.1b"><mn id="S1.p6.6.m6.1.1">11.3</mn><mo id="S1.p6.6.m6.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S1.p6.6.m6.1c">11.3\times</annotation><annotation encoding="application/x-llamapun" id="S1.p6.6.m6.1d">11.3 ×</annotation></semantics></math>)
vs. the baseline CPU-centric disaggregated preprocessing.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">End-to-End RecSys Training Pipeline</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">DNNs typically require some form of input data preprocessing before
training. For instance, image classification requires
preprocessing operations like image decoding, resizing, cropping, or
augmentation. Additionally, speech recognition preprocessing includes Fourier
transform and normalization of audio data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib57" title="">57</a>]</cite>. Similarly, RecSys also requires
a unique data preprocessing to generate the train-ready tensors consumed
by the model training stage, which we detail below.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Traditionally, the RecSys training pipeline employed <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">offline</em> data
preprocessing where the raw feature data stored in the storage system is
transformed into train-ready tensors well before model training takes
place. However, the proliferation of petabyte-scale data and the frequent
development of diverse RecSys models by ML engineers have made offline
preprocessing impractical because it is difficult to manage the substantial
storage space required to store the offline preprocessed data. This challenge
has triggered a shift towards <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.2">online</em> preprocessing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite>, which involves
preprocessing the raw feature data “on-the-fly” (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>). The train-ready tensors are derived from the
features that are constantly generated online by inference services.
Specifically, inference servers log various end-user’s interactions with the
inference service as distinct features (e.g., news feed a user has clicked,
items a user has purchased) using logging engines, e.g., Meta’s Scribe<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib28" title="">28</a>]</cite>.
Additionally, various streaming and batch engines, such as Spark<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib69" title="">69</a>]</cite>, further label and filter data before storing them
in a centralized data warehouse<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib72" title="">72</a>]</cite>. These raw feature data are categorized into two types: dense and sparse. Dense features represent continuous values
(e.g., the time when a user viewed a video from YouTube), while sparse features
represent sparse categorical values which can be variable-length (e.g., list of
YouTube videos a user has viewed over a one-hour period). The logged features archived within the centralized data warehouse are then fetched into the storage system of each datacenter for future data preprocessing.
The data preprocessing stage, also known as the ETL (<span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.3">E</span>xtract, <span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.4">T</span>ransform, and
<span class="ltx_text ltx_font_bold" id="S2.SS1.p2.1.5">L</span>oad) phase, involves the following series of operations:</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="237" id="S2.F1.g1" src="x1.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>High-level overview of the end-to-end RecSys training pipeline. In this work, we assume our baseline data storage and ingestion pipeline for data preprocessing by referring to the related academic literature published by Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib72" title="">72</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib71" title="">71</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p3">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">(Extract) The logged raw feature data
are first retrieved from the storage system in preparation for the feature-specific
transform operations.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">(Transform) The extracted raw feature data go through feature generation and feature
normalization in order to generate the train-ready tensors.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">(Load) The train-ready tensors are copied over to the GPU’s high-bandwidth memory (HBM) in preparation for the RecSys model training.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.2">Once the train-ready tensors are loaded into GPU’s memory, the actual
model training is undertaken. Specifically, the GPU executes the
embedding layers (embedding lookups, pooling for embedding reductions), feature interactions (batched GEMM), and MLP layers (GEMM).
When it comes to embedding layers,
the embedding look-up operation retrieves embeddings
from an embedding table <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib51" title="">51</a>]</cite>, which utilizes embedding
indices that are generated during the Transform phase of ETL (e.g., the
embedding indices transformed from feature <math alttext="X" class="ltx_Math" display="inline" id="S2.SS1.p4.1.m1.1"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.1.m1.1d">italic_X</annotation></semantics></math>’ and <math alttext="W" class="ltx_Math" display="inline" id="S2.SS1.p4.2.m2.1"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p4.2.m2.1d">italic_W</annotation></semantics></math> in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a> are utilized for
embedding look-up operations). Recent work on various system-level performance optimizations
for RecSys has primarily focused on this “training” stage
of end-to-end training pipeline, rather than data preprocessing. The focus
of this work is on RecSys data preprocessing, so we refer
to these relevant prior studies for more details on the RecSys model architectures and
training/inference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">While GPU-centric systems are popular options for training purposes, it is
worth emphasizing that the entire data preprocessing stage (the ETL phase) is
executed using CPUs in state-of-the-art data preprocessing
systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib32" title="">32</a>]</cite>,
including those for RecSys <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib25" title="">25</a>]</cite>. In the rest of
this paper, we assume such “CPU-centric” RecSys data preprocessing system
for our baseline system.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">Key Properties of RecSys Data Preprocessing</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">While numerous prior work explored preprocessing for vision, speech,
and language processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib8" title="">8</a>]</cite>, data preprocessing for RecSys
is relatively less explored <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib71" title="">71</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib25" title="">25</a>]</cite>. Unlike image or audio
data, RecSys data is represented in a <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">tabular</em> format with
multiple rows and columns. Concretely, each row represents an
individual “user” whereas each column represents a distinct
“feature” related to that user’s past interactions with the RecSys
inference service (shown as the data generation stage in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">In the context of online preprocessing, deciding which features
to utilize for model training depends on the ML engineer’s choice, i.e., it is extremely challenging to predict which
specific features will be utilized. Consequently, the
hardware/software system for online preprocessing exhibits some unique
properties in the Extract and Transform phase:</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.8">(Extract) The raw feature data is first converted and stored in a
<em class="ltx_emph ltx_font_italic" id="S2.I2.i1.p1.8.1">columnar</em> format (shown as the data storage stage in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>). A group of rows are sharded into mutually
exclusive <em class="ltx_emph ltx_font_italic" id="S2.I2.i1.p1.8.2">partitions</em> and different partitions are stored as independent
columnar files into a distributed storage system of datacenter (e.g., the two partitions in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a> are stored as two separate columnar files over two
SSDs). The reason why these tabular data are converted in a columnar
format is to avoid overfetching unwanted features. For example, in the data
storage stage depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>, with a columnar format,
any given feature for all the users can be <em class="ltx_emph ltx_font_italic" id="S2.I2.i1.p1.8.3">selectively</em> extracted
from the storage system without having to retrieve unwanted features,
e.g., it is possible to only fetch features <math alttext="X" class="ltx_Math" display="inline" id="S2.I2.i1.p1.1.m1.1"><semantics id="S2.I2.i1.p1.1.m1.1a"><mi id="S2.I2.i1.p1.1.m1.1.1" xref="S2.I2.i1.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.1.m1.1b"><ci id="S2.I2.i1.p1.1.m1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.1.m1.1d">italic_X</annotation></semantics></math> and <math alttext="W" class="ltx_Math" display="inline" id="S2.I2.i1.p1.2.m2.1"><semantics id="S2.I2.i1.p1.2.m2.1a"><mi id="S2.I2.i1.p1.2.m2.1.1" xref="S2.I2.i1.p1.2.m2.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.2.m2.1b"><ci id="S2.I2.i1.p1.2.m2.1.1.cmml" xref="S2.I2.i1.p1.2.m2.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.2.m2.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.2.m2.1d">italic_W</annotation></semantics></math> without having
to fetch features <math alttext="Y" class="ltx_Math" display="inline" id="S2.I2.i1.p1.3.m3.1"><semantics id="S2.I2.i1.p1.3.m3.1a"><mi id="S2.I2.i1.p1.3.m3.1.1" xref="S2.I2.i1.p1.3.m3.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.3.m3.1b"><ci id="S2.I2.i1.p1.3.m3.1.1.cmml" xref="S2.I2.i1.p1.3.m3.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.3.m3.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.3.m3.1d">italic_Y</annotation></semantics></math> and <math alttext="Z" class="ltx_Math" display="inline" id="S2.I2.i1.p1.4.m4.1"><semantics id="S2.I2.i1.p1.4.m4.1a"><mi id="S2.I2.i1.p1.4.m4.1.1" xref="S2.I2.i1.p1.4.m4.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.4.m4.1b"><ci id="S2.I2.i1.p1.4.m4.1.1.cmml" xref="S2.I2.i1.p1.4.m4.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.4.m4.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.4.m4.1d">italic_Z</annotation></semantics></math>. In contrast, with the original,
row-oriented format, extracting features <math alttext="X" class="ltx_Math" display="inline" id="S2.I2.i1.p1.5.m5.1"><semantics id="S2.I2.i1.p1.5.m5.1a"><mi id="S2.I2.i1.p1.5.m5.1.1" xref="S2.I2.i1.p1.5.m5.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.5.m5.1b"><ci id="S2.I2.i1.p1.5.m5.1.1.cmml" xref="S2.I2.i1.p1.5.m5.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.5.m5.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.5.m5.1d">italic_X</annotation></semantics></math> and <math alttext="W" class="ltx_Math" display="inline" id="S2.I2.i1.p1.6.m6.1"><semantics id="S2.I2.i1.p1.6.m6.1a"><mi id="S2.I2.i1.p1.6.m6.1.1" xref="S2.I2.i1.p1.6.m6.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.6.m6.1b"><ci id="S2.I2.i1.p1.6.m6.1.1.cmml" xref="S2.I2.i1.p1.6.m6.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.6.m6.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.6.m6.1d">italic_W</annotation></semantics></math> for all users
inevitably leads to (unwanted) features <math alttext="Y" class="ltx_Math" display="inline" id="S2.I2.i1.p1.7.m7.1"><semantics id="S2.I2.i1.p1.7.m7.1a"><mi id="S2.I2.i1.p1.7.m7.1.1" xref="S2.I2.i1.p1.7.m7.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.7.m7.1b"><ci id="S2.I2.i1.p1.7.m7.1.1.cmml" xref="S2.I2.i1.p1.7.m7.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.7.m7.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.7.m7.1d">italic_Y</annotation></semantics></math> and <math alttext="Z" class="ltx_Math" display="inline" id="S2.I2.i1.p1.8.m8.1"><semantics id="S2.I2.i1.p1.8.m8.1a"><mi id="S2.I2.i1.p1.8.m8.1.1" xref="S2.I2.i1.p1.8.m8.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.8.m8.1b"><ci id="S2.I2.i1.p1.8.m8.1.1.cmml" xref="S2.I2.i1.p1.8.m8.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.8.m8.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.8.m8.1d">italic_Z</annotation></semantics></math> to be retrieved,
wasting data read bandwidth.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.5">(Transform) The transformations conducted at this phase generate the
mini-batch inputs that are utilized by the GPU for model training. All
transformation operations performed within a mini-batch (detailed in the next
subsection) are executed independently from transformations targeting other
mini-batches. This is because RecSys transformation operations exhibit abundant
<em class="ltx_emph ltx_font_italic" id="S2.I2.i2.p1.5.1">inter-/intra-feature parallelism</em>. Specifically, each element within a
given feature vector (e.g., user <math alttext="A" class="ltx_Math" display="inline" id="S2.I2.i2.p1.1.m1.1"><semantics id="S2.I2.i2.p1.1.m1.1a"><mi id="S2.I2.i2.p1.1.m1.1.1" xref="S2.I2.i2.p1.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.1.m1.1b"><ci id="S2.I2.i2.p1.1.m1.1.1.cmml" xref="S2.I2.i2.p1.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.1.m1.1d">italic_A</annotation></semantics></math> and <math alttext="C" class="ltx_Math" display="inline" id="S2.I2.i2.p1.2.m2.1"><semantics id="S2.I2.i2.p1.2.m2.1a"><mi id="S2.I2.i2.p1.2.m2.1.1" xref="S2.I2.i2.p1.2.m2.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.2.m2.1b"><ci id="S2.I2.i2.p1.2.m2.1.1.cmml" xref="S2.I2.i2.p1.2.m2.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.2.m2.1c">C</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.2.m2.1d">italic_C</annotation></semantics></math>’s feature <math alttext="X" class="ltx_Math" display="inline" id="S2.I2.i2.p1.3.m3.1"><semantics id="S2.I2.i2.p1.3.m3.1a"><mi id="S2.I2.i2.p1.3.m3.1.1" xref="S2.I2.i2.p1.3.m3.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.3.m3.1b"><ci id="S2.I2.i2.p1.3.m3.1.1.cmml" xref="S2.I2.i2.p1.3.m3.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.3.m3.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.3.m3.1d">italic_X</annotation></semantics></math> in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>) represents a given user’s interaction and there
exists no data dependency across different users. Therefore, a transformation
operation can be conducted element-wise by exploiting <em class="ltx_emph ltx_font_italic" id="S2.I2.i2.p1.5.2">intra</em>-feature
parallelism. Similarly, different features (e.g., feature <math alttext="X" class="ltx_Math" display="inline" id="S2.I2.i2.p1.4.m4.1"><semantics id="S2.I2.i2.p1.4.m4.1a"><mi id="S2.I2.i2.p1.4.m4.1.1" xref="S2.I2.i2.p1.4.m4.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.4.m4.1b"><ci id="S2.I2.i2.p1.4.m4.1.1.cmml" xref="S2.I2.i2.p1.4.m4.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.4.m4.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.4.m4.1d">italic_X</annotation></semantics></math> and <math alttext="W" class="ltx_Math" display="inline" id="S2.I2.i2.p1.5.m5.1"><semantics id="S2.I2.i2.p1.5.m5.1a"><mi id="S2.I2.i2.p1.5.m5.1.1" xref="S2.I2.i2.p1.5.m5.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.5.m5.1b"><ci id="S2.I2.i2.p1.5.m5.1.1.cmml" xref="S2.I2.i2.p1.5.m5.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.5.m5.1c">W</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.5.m5.1d">italic_W</annotation></semantics></math> for all
users) are subject to independent transformation operations by leveraging
<em class="ltx_emph ltx_font_italic" id="S2.I2.i2.p1.5.3">inter</em>-feature parallelism.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Feature Generation/Normalization in Data Preprocessing</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.2">RecSys data preprocessing can be divided into
three key steps. First, the <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.2.1">feature generation</em> step generates new features
using the raw feature data extracted from storage (Step ❶ in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>, e.g., a new feature <math alttext="X" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mi id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">italic_X</annotation></semantics></math>’ is generated from the
raw feature <math alttext="X" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.2.m2.1d">italic_X</annotation></semantics></math>). Notably, one of the representative sparse feature
generation operations is the “Bucketize”
operation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib63" title="">63</a>]</cite>, which transforms dense features into
sparse features by sharding features based on predefined bucket boundaries
(Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#alg1" title="Algorithm 1 ‣ II-C Feature Generation/Normalization in Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>This paper focuses on the feature generation
(Bucketize) and feature normalization (SigridHash, Log) operations publicly available
in the open-source TorchArrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib63" title="">63</a>]</cite>. Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#alg1" title="Algorithm 1 ‣ II-C Feature Generation/Normalization in Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>
and Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#alg2" title="Algorithm 2 ‣ II-C Feature Generation/Normalization in Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a> are simplified versions of each algorithm
implemented in TorchArrow. </span></span></span>). Once the desired number of features is
generated, they undergo <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.2.2">feature normalization</em> (Step ❷). Common
feature normalization techniques include “Log” (which normalizes dense
features using a logarithmic function) and
“SigridHash” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib63" title="">63</a>]</cite> (which normalizes sparse features
by computing a hash value that maps those features within the maximum index
of the corresponding embedding table of the RecSys model, see
Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#alg2" title="Algorithm 2 ‣ II-C Feature Generation/Normalization in Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>). Finally, the normalized features are converted
into an input mini-batch (Step ❸), which eventually gets loaded into
GPU memory for training the RecSys model.</p>
</div>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg1.2.1.1">Algorithm 1</span> </span> Bucketize for feature generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib63" title="">63</a>]</cite></figcaption>
<div class="ltx_listing ltx_listing" id="alg1.3">
<div class="ltx_listingline" id="alg1.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l1.1.1.1" style="font-size:80%;">1:</span></span>  Input dense feature <math alttext="a[1\,\ldots\,n]" class="ltx_Math" display="inline" id="alg1.l1.m1.1"><semantics id="alg1.l1.m1.1a"><mrow id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml"><mi id="alg1.l1.m1.1.1.3" xref="alg1.l1.m1.1.1.3.cmml">a</mi><mo id="alg1.l1.m1.1.1.2" xref="alg1.l1.m1.1.1.2.cmml">⁢</mo><mrow id="alg1.l1.m1.1.1.1.1" xref="alg1.l1.m1.1.1.1.2.cmml"><mo id="alg1.l1.m1.1.1.1.1.2" stretchy="false" xref="alg1.l1.m1.1.1.1.2.1.cmml">[</mo><mrow id="alg1.l1.m1.1.1.1.1.1" xref="alg1.l1.m1.1.1.1.1.1.cmml"><mn id="alg1.l1.m1.1.1.1.1.1.2" xref="alg1.l1.m1.1.1.1.1.1.2.cmml">1</mn><mo id="alg1.l1.m1.1.1.1.1.1.1" lspace="0.170em" xref="alg1.l1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m1.1.1.1.1.1.3" mathvariant="normal" xref="alg1.l1.m1.1.1.1.1.1.3.cmml">…</mi><mo id="alg1.l1.m1.1.1.1.1.1.1a" lspace="0.170em" xref="alg1.l1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m1.1.1.1.1.1.4" xref="alg1.l1.m1.1.1.1.1.1.4.cmml">n</mi></mrow><mo id="alg1.l1.m1.1.1.1.1.3" stretchy="false" xref="alg1.l1.m1.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><apply id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1"><times id="alg1.l1.m1.1.1.2.cmml" xref="alg1.l1.m1.1.1.2"></times><ci id="alg1.l1.m1.1.1.3.cmml" xref="alg1.l1.m1.1.1.3">𝑎</ci><apply id="alg1.l1.m1.1.1.1.2.cmml" xref="alg1.l1.m1.1.1.1.1"><csymbol cd="latexml" id="alg1.l1.m1.1.1.1.2.1.cmml" xref="alg1.l1.m1.1.1.1.1.2">delimited-[]</csymbol><apply id="alg1.l1.m1.1.1.1.1.1.cmml" xref="alg1.l1.m1.1.1.1.1.1"><times id="alg1.l1.m1.1.1.1.1.1.1.cmml" xref="alg1.l1.m1.1.1.1.1.1.1"></times><cn id="alg1.l1.m1.1.1.1.1.1.2.cmml" type="integer" xref="alg1.l1.m1.1.1.1.1.1.2">1</cn><ci id="alg1.l1.m1.1.1.1.1.1.3.cmml" xref="alg1.l1.m1.1.1.1.1.1.3">…</ci><ci id="alg1.l1.m1.1.1.1.1.1.4.cmml" xref="alg1.l1.m1.1.1.1.1.1.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">a[1\,\ldots\,n]</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m1.1d">italic_a [ 1 … italic_n ]</annotation></semantics></math>; bucket boundary <math alttext="b[1\,\ldots\,m]" class="ltx_Math" display="inline" id="alg1.l1.m2.1"><semantics id="alg1.l1.m2.1a"><mrow id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3.cmml">b</mi><mo id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">⁢</mo><mrow id="alg1.l1.m2.1.1.1.1" xref="alg1.l1.m2.1.1.1.2.cmml"><mo id="alg1.l1.m2.1.1.1.1.2" stretchy="false" xref="alg1.l1.m2.1.1.1.2.1.cmml">[</mo><mrow id="alg1.l1.m2.1.1.1.1.1" xref="alg1.l1.m2.1.1.1.1.1.cmml"><mn id="alg1.l1.m2.1.1.1.1.1.2" xref="alg1.l1.m2.1.1.1.1.1.2.cmml">1</mn><mo id="alg1.l1.m2.1.1.1.1.1.1" lspace="0.170em" xref="alg1.l1.m2.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m2.1.1.1.1.1.3" mathvariant="normal" xref="alg1.l1.m2.1.1.1.1.1.3.cmml">…</mi><mo id="alg1.l1.m2.1.1.1.1.1.1a" lspace="0.170em" xref="alg1.l1.m2.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m2.1.1.1.1.1.4" xref="alg1.l1.m2.1.1.1.1.1.4.cmml">m</mi></mrow><mo id="alg1.l1.m2.1.1.1.1.3" stretchy="false" xref="alg1.l1.m2.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><times id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2"></times><ci id="alg1.l1.m2.1.1.3.cmml" xref="alg1.l1.m2.1.1.3">𝑏</ci><apply id="alg1.l1.m2.1.1.1.2.cmml" xref="alg1.l1.m2.1.1.1.1"><csymbol cd="latexml" id="alg1.l1.m2.1.1.1.2.1.cmml" xref="alg1.l1.m2.1.1.1.1.2">delimited-[]</csymbol><apply id="alg1.l1.m2.1.1.1.1.1.cmml" xref="alg1.l1.m2.1.1.1.1.1"><times id="alg1.l1.m2.1.1.1.1.1.1.cmml" xref="alg1.l1.m2.1.1.1.1.1.1"></times><cn id="alg1.l1.m2.1.1.1.1.1.2.cmml" type="integer" xref="alg1.l1.m2.1.1.1.1.1.2">1</cn><ci id="alg1.l1.m2.1.1.1.1.1.3.cmml" xref="alg1.l1.m2.1.1.1.1.1.3">…</ci><ci id="alg1.l1.m2.1.1.1.1.1.4.cmml" xref="alg1.l1.m2.1.1.1.1.1.4">𝑚</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">b[1\,\ldots\,m]</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m2.1d">italic_b [ 1 … italic_m ]</annotation></semantics></math>; output <math alttext="c[1\,\ldots\,n]" class="ltx_Math" display="inline" id="alg1.l1.m3.1"><semantics id="alg1.l1.m3.1a"><mrow id="alg1.l1.m3.1.1" xref="alg1.l1.m3.1.1.cmml"><mi id="alg1.l1.m3.1.1.3" xref="alg1.l1.m3.1.1.3.cmml">c</mi><mo id="alg1.l1.m3.1.1.2" xref="alg1.l1.m3.1.1.2.cmml">⁢</mo><mrow id="alg1.l1.m3.1.1.1.1" xref="alg1.l1.m3.1.1.1.2.cmml"><mo id="alg1.l1.m3.1.1.1.1.2" stretchy="false" xref="alg1.l1.m3.1.1.1.2.1.cmml">[</mo><mrow id="alg1.l1.m3.1.1.1.1.1" xref="alg1.l1.m3.1.1.1.1.1.cmml"><mn id="alg1.l1.m3.1.1.1.1.1.2" xref="alg1.l1.m3.1.1.1.1.1.2.cmml">1</mn><mo id="alg1.l1.m3.1.1.1.1.1.1" lspace="0.170em" xref="alg1.l1.m3.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m3.1.1.1.1.1.3" mathvariant="normal" xref="alg1.l1.m3.1.1.1.1.1.3.cmml">…</mi><mo id="alg1.l1.m3.1.1.1.1.1.1a" lspace="0.170em" xref="alg1.l1.m3.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg1.l1.m3.1.1.1.1.1.4" xref="alg1.l1.m3.1.1.1.1.1.4.cmml">n</mi></mrow><mo id="alg1.l1.m3.1.1.1.1.3" stretchy="false" xref="alg1.l1.m3.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m3.1b"><apply id="alg1.l1.m3.1.1.cmml" xref="alg1.l1.m3.1.1"><times id="alg1.l1.m3.1.1.2.cmml" xref="alg1.l1.m3.1.1.2"></times><ci id="alg1.l1.m3.1.1.3.cmml" xref="alg1.l1.m3.1.1.3">𝑐</ci><apply id="alg1.l1.m3.1.1.1.2.cmml" xref="alg1.l1.m3.1.1.1.1"><csymbol cd="latexml" id="alg1.l1.m3.1.1.1.2.1.cmml" xref="alg1.l1.m3.1.1.1.1.2">delimited-[]</csymbol><apply id="alg1.l1.m3.1.1.1.1.1.cmml" xref="alg1.l1.m3.1.1.1.1.1"><times id="alg1.l1.m3.1.1.1.1.1.1.cmml" xref="alg1.l1.m3.1.1.1.1.1.1"></times><cn id="alg1.l1.m3.1.1.1.1.1.2.cmml" type="integer" xref="alg1.l1.m3.1.1.1.1.1.2">1</cn><ci id="alg1.l1.m3.1.1.1.1.1.3.cmml" xref="alg1.l1.m3.1.1.1.1.1.3">…</ci><ci id="alg1.l1.m3.1.1.1.1.1.4.cmml" xref="alg1.l1.m3.1.1.1.1.1.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m3.1c">c[1\,\ldots\,n]</annotation><annotation encoding="application/x-llamapun" id="alg1.l1.m3.1d">italic_c [ 1 … italic_n ]</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l2.1.1.1" style="font-size:80%;">2:</span></span>  /* Digitize input dense features based on bucket */

</div>
<div class="ltx_listingline" id="alg1.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l3.1.1.1" style="font-size:80%;">3:</span></span>  <span class="ltx_text ltx_font_bold" id="alg1.l3.2">for</span> <math alttext="i\leftarrow" class="ltx_Math" display="inline" id="alg1.l3.m1.1"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">i</mi><mo id="alg1.l3.m1.1.1.1" stretchy="false" xref="alg1.l3.m1.1.1.1.cmml">←</mo><mi id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><ci id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1">←</ci><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝑖</ci><csymbol cd="latexml" id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">i\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m1.1d">italic_i ←</annotation></semantics></math> <math alttext="1" class="ltx_Math" display="inline" id="alg1.l3.m2.1"><semantics id="alg1.l3.m2.1a"><mn id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b"><cn id="alg1.l3.m2.1.1.cmml" type="integer" xref="alg1.l3.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m2.1d">1</annotation></semantics></math> to <math alttext="n" class="ltx_Math" display="inline" id="alg1.l3.m3.1"><semantics id="alg1.l3.m3.1a"><mi id="alg1.l3.m3.1.1" xref="alg1.l3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m3.1b"><ci id="alg1.l3.m3.1.1.cmml" xref="alg1.l3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="alg1.l3.m3.1d">italic_n</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg1.l3.3">do</span>
</div>
<div class="ltx_listingline" id="alg1.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l4.1.1.1" style="font-size:80%;">4:</span></span>     /* Find the index of buckets to which the input value belongs using binary search algorithm*/

</div>
<div class="ltx_listingline" id="alg1.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l5.1.1.1" style="font-size:80%;">5:</span></span>     <math alttext="c[i]\leftarrow\texttt{SearchBucketID}(a[i],\,b[1\,\ldots\,m])" class="ltx_Math" display="inline" id="alg1.l5.m1.4"><semantics id="alg1.l5.m1.4a"><mrow id="alg1.l5.m1.4.4" xref="alg1.l5.m1.4.4.cmml"><mrow id="alg1.l5.m1.4.4.4" xref="alg1.l5.m1.4.4.4.cmml"><mi id="alg1.l5.m1.4.4.4.2" xref="alg1.l5.m1.4.4.4.2.cmml">c</mi><mo id="alg1.l5.m1.4.4.4.1" xref="alg1.l5.m1.4.4.4.1.cmml">⁢</mo><mrow id="alg1.l5.m1.4.4.4.3.2" xref="alg1.l5.m1.4.4.4.3.1.cmml"><mo id="alg1.l5.m1.4.4.4.3.2.1" stretchy="false" xref="alg1.l5.m1.4.4.4.3.1.1.cmml">[</mo><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">i</mi><mo id="alg1.l5.m1.4.4.4.3.2.2" stretchy="false" xref="alg1.l5.m1.4.4.4.3.1.1.cmml">]</mo></mrow></mrow><mo id="alg1.l5.m1.4.4.3" stretchy="false" xref="alg1.l5.m1.4.4.3.cmml">←</mo><mrow id="alg1.l5.m1.4.4.2" xref="alg1.l5.m1.4.4.2.cmml"><mtext class="ltx_mathvariant_monospace" id="alg1.l5.m1.4.4.2.4" xref="alg1.l5.m1.4.4.2.4a.cmml">SearchBucketID</mtext><mo id="alg1.l5.m1.4.4.2.3" xref="alg1.l5.m1.4.4.2.3.cmml">⁢</mo><mrow id="alg1.l5.m1.4.4.2.2.2" xref="alg1.l5.m1.4.4.2.2.3.cmml"><mo id="alg1.l5.m1.4.4.2.2.2.3" stretchy="false" xref="alg1.l5.m1.4.4.2.2.3.cmml">(</mo><mrow id="alg1.l5.m1.3.3.1.1.1.1" xref="alg1.l5.m1.3.3.1.1.1.1.cmml"><mi id="alg1.l5.m1.3.3.1.1.1.1.2" xref="alg1.l5.m1.3.3.1.1.1.1.2.cmml">a</mi><mo id="alg1.l5.m1.3.3.1.1.1.1.1" xref="alg1.l5.m1.3.3.1.1.1.1.1.cmml">⁢</mo><mrow id="alg1.l5.m1.3.3.1.1.1.1.3.2" xref="alg1.l5.m1.3.3.1.1.1.1.3.1.cmml"><mo id="alg1.l5.m1.3.3.1.1.1.1.3.2.1" stretchy="false" xref="alg1.l5.m1.3.3.1.1.1.1.3.1.1.cmml">[</mo><mi id="alg1.l5.m1.2.2" xref="alg1.l5.m1.2.2.cmml">i</mi><mo id="alg1.l5.m1.3.3.1.1.1.1.3.2.2" stretchy="false" xref="alg1.l5.m1.3.3.1.1.1.1.3.1.1.cmml">]</mo></mrow></mrow><mo id="alg1.l5.m1.4.4.2.2.2.4" rspace="0.337em" xref="alg1.l5.m1.4.4.2.2.3.cmml">,</mo><mrow id="alg1.l5.m1.4.4.2.2.2.2" xref="alg1.l5.m1.4.4.2.2.2.2.cmml"><mi id="alg1.l5.m1.4.4.2.2.2.2.3" xref="alg1.l5.m1.4.4.2.2.2.2.3.cmml">b</mi><mo id="alg1.l5.m1.4.4.2.2.2.2.2" xref="alg1.l5.m1.4.4.2.2.2.2.2.cmml">⁢</mo><mrow id="alg1.l5.m1.4.4.2.2.2.2.1.1" xref="alg1.l5.m1.4.4.2.2.2.2.1.2.cmml"><mo id="alg1.l5.m1.4.4.2.2.2.2.1.1.2" stretchy="false" xref="alg1.l5.m1.4.4.2.2.2.2.1.2.1.cmml">[</mo><mrow id="alg1.l5.m1.4.4.2.2.2.2.1.1.1" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.cmml"><mn id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.2" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.2.cmml">1</mn><mo id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.1" lspace="0.170em" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.1.cmml">⁢</mo><mi id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.3" mathvariant="normal" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.3.cmml">…</mi><mo id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.1a" lspace="0.170em" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.1.cmml">⁢</mo><mi id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.4" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.4.cmml">m</mi></mrow><mo id="alg1.l5.m1.4.4.2.2.2.2.1.1.3" stretchy="false" xref="alg1.l5.m1.4.4.2.2.2.2.1.2.1.cmml">]</mo></mrow></mrow><mo id="alg1.l5.m1.4.4.2.2.2.5" stretchy="false" xref="alg1.l5.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.4b"><apply id="alg1.l5.m1.4.4.cmml" xref="alg1.l5.m1.4.4"><ci id="alg1.l5.m1.4.4.3.cmml" xref="alg1.l5.m1.4.4.3">←</ci><apply id="alg1.l5.m1.4.4.4.cmml" xref="alg1.l5.m1.4.4.4"><times id="alg1.l5.m1.4.4.4.1.cmml" xref="alg1.l5.m1.4.4.4.1"></times><ci id="alg1.l5.m1.4.4.4.2.cmml" xref="alg1.l5.m1.4.4.4.2">𝑐</ci><apply id="alg1.l5.m1.4.4.4.3.1.cmml" xref="alg1.l5.m1.4.4.4.3.2"><csymbol cd="latexml" id="alg1.l5.m1.4.4.4.3.1.1.cmml" xref="alg1.l5.m1.4.4.4.3.2.1">delimited-[]</csymbol><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">𝑖</ci></apply></apply><apply id="alg1.l5.m1.4.4.2.cmml" xref="alg1.l5.m1.4.4.2"><times id="alg1.l5.m1.4.4.2.3.cmml" xref="alg1.l5.m1.4.4.2.3"></times><ci id="alg1.l5.m1.4.4.2.4a.cmml" xref="alg1.l5.m1.4.4.2.4"><mtext class="ltx_mathvariant_monospace" id="alg1.l5.m1.4.4.2.4.cmml" xref="alg1.l5.m1.4.4.2.4">SearchBucketID</mtext></ci><interval closure="open" id="alg1.l5.m1.4.4.2.2.3.cmml" xref="alg1.l5.m1.4.4.2.2.2"><apply id="alg1.l5.m1.3.3.1.1.1.1.cmml" xref="alg1.l5.m1.3.3.1.1.1.1"><times id="alg1.l5.m1.3.3.1.1.1.1.1.cmml" xref="alg1.l5.m1.3.3.1.1.1.1.1"></times><ci id="alg1.l5.m1.3.3.1.1.1.1.2.cmml" xref="alg1.l5.m1.3.3.1.1.1.1.2">𝑎</ci><apply id="alg1.l5.m1.3.3.1.1.1.1.3.1.cmml" xref="alg1.l5.m1.3.3.1.1.1.1.3.2"><csymbol cd="latexml" id="alg1.l5.m1.3.3.1.1.1.1.3.1.1.cmml" xref="alg1.l5.m1.3.3.1.1.1.1.3.2.1">delimited-[]</csymbol><ci id="alg1.l5.m1.2.2.cmml" xref="alg1.l5.m1.2.2">𝑖</ci></apply></apply><apply id="alg1.l5.m1.4.4.2.2.2.2.cmml" xref="alg1.l5.m1.4.4.2.2.2.2"><times id="alg1.l5.m1.4.4.2.2.2.2.2.cmml" xref="alg1.l5.m1.4.4.2.2.2.2.2"></times><ci id="alg1.l5.m1.4.4.2.2.2.2.3.cmml" xref="alg1.l5.m1.4.4.2.2.2.2.3">𝑏</ci><apply id="alg1.l5.m1.4.4.2.2.2.2.1.2.cmml" xref="alg1.l5.m1.4.4.2.2.2.2.1.1"><csymbol cd="latexml" id="alg1.l5.m1.4.4.2.2.2.2.1.2.1.cmml" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.2">delimited-[]</csymbol><apply id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.cmml" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1"><times id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.1.cmml" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.1"></times><cn id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.2.cmml" type="integer" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.2">1</cn><ci id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.3.cmml" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.3">…</ci><ci id="alg1.l5.m1.4.4.2.2.2.2.1.1.1.4.cmml" xref="alg1.l5.m1.4.4.2.2.2.2.1.1.1.4">𝑚</ci></apply></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.4c">c[i]\leftarrow\texttt{SearchBucketID}(a[i],\,b[1\,\ldots\,m])</annotation><annotation encoding="application/x-llamapun" id="alg1.l5.m1.4d">italic_c [ italic_i ] ← SearchBucketID ( italic_a [ italic_i ] , italic_b [ 1 … italic_m ] )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg1.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg1.l6.1.1.1" style="font-size:80%;">6:</span></span>  <span class="ltx_text ltx_font_bold" id="alg1.l6.2">end</span> <span class="ltx_text ltx_font_bold" id="alg1.l6.3">for</span>
</div>
</div>
</figure>
<figure class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top" id="alg2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span class="ltx_text ltx_font_bold" id="alg2.2.1.1">Algorithm 2</span> </span> SigridHash for feature normalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib63" title="">63</a>]</cite></figcaption>
<div class="ltx_listing ltx_listing" id="alg2.3">
<div class="ltx_listingline" id="alg2.l1">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l1.1.1.1" style="font-size:80%;">1:</span></span>  Input sparse feature <math alttext="a[1\,\ldots\,n]" class="ltx_Math" display="inline" id="alg2.l1.m1.1"><semantics id="alg2.l1.m1.1a"><mrow id="alg2.l1.m1.1.1" xref="alg2.l1.m1.1.1.cmml"><mi id="alg2.l1.m1.1.1.3" xref="alg2.l1.m1.1.1.3.cmml">a</mi><mo id="alg2.l1.m1.1.1.2" xref="alg2.l1.m1.1.1.2.cmml">⁢</mo><mrow id="alg2.l1.m1.1.1.1.1" xref="alg2.l1.m1.1.1.1.2.cmml"><mo id="alg2.l1.m1.1.1.1.1.2" stretchy="false" xref="alg2.l1.m1.1.1.1.2.1.cmml">[</mo><mrow id="alg2.l1.m1.1.1.1.1.1" xref="alg2.l1.m1.1.1.1.1.1.cmml"><mn id="alg2.l1.m1.1.1.1.1.1.2" xref="alg2.l1.m1.1.1.1.1.1.2.cmml">1</mn><mo id="alg2.l1.m1.1.1.1.1.1.1" lspace="0.170em" xref="alg2.l1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg2.l1.m1.1.1.1.1.1.3" mathvariant="normal" xref="alg2.l1.m1.1.1.1.1.1.3.cmml">…</mi><mo id="alg2.l1.m1.1.1.1.1.1.1a" lspace="0.170em" xref="alg2.l1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg2.l1.m1.1.1.1.1.1.4" xref="alg2.l1.m1.1.1.1.1.1.4.cmml">n</mi></mrow><mo id="alg2.l1.m1.1.1.1.1.3" stretchy="false" xref="alg2.l1.m1.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l1.m1.1b"><apply id="alg2.l1.m1.1.1.cmml" xref="alg2.l1.m1.1.1"><times id="alg2.l1.m1.1.1.2.cmml" xref="alg2.l1.m1.1.1.2"></times><ci id="alg2.l1.m1.1.1.3.cmml" xref="alg2.l1.m1.1.1.3">𝑎</ci><apply id="alg2.l1.m1.1.1.1.2.cmml" xref="alg2.l1.m1.1.1.1.1"><csymbol cd="latexml" id="alg2.l1.m1.1.1.1.2.1.cmml" xref="alg2.l1.m1.1.1.1.1.2">delimited-[]</csymbol><apply id="alg2.l1.m1.1.1.1.1.1.cmml" xref="alg2.l1.m1.1.1.1.1.1"><times id="alg2.l1.m1.1.1.1.1.1.1.cmml" xref="alg2.l1.m1.1.1.1.1.1.1"></times><cn id="alg2.l1.m1.1.1.1.1.1.2.cmml" type="integer" xref="alg2.l1.m1.1.1.1.1.1.2">1</cn><ci id="alg2.l1.m1.1.1.1.1.1.3.cmml" xref="alg2.l1.m1.1.1.1.1.1.3">…</ci><ci id="alg2.l1.m1.1.1.1.1.1.4.cmml" xref="alg2.l1.m1.1.1.1.1.1.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m1.1c">a[1\,\ldots\,n]</annotation><annotation encoding="application/x-llamapun" id="alg2.l1.m1.1d">italic_a [ 1 … italic_n ]</annotation></semantics></math>; seed <math alttext="s" class="ltx_Math" display="inline" id="alg2.l1.m2.1"><semantics id="alg2.l1.m2.1a"><mi id="alg2.l1.m2.1.1" xref="alg2.l1.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="alg2.l1.m2.1b"><ci id="alg2.l1.m2.1.1.cmml" xref="alg2.l1.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="alg2.l1.m2.1d">italic_s</annotation></semantics></math>; max value <math alttext="d" class="ltx_Math" display="inline" id="alg2.l1.m3.1"><semantics id="alg2.l1.m3.1a"><mi id="alg2.l1.m3.1.1" xref="alg2.l1.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="alg2.l1.m3.1b"><ci id="alg2.l1.m3.1.1.cmml" xref="alg2.l1.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m3.1c">d</annotation><annotation encoding="application/x-llamapun" id="alg2.l1.m3.1d">italic_d</annotation></semantics></math>; output <math alttext="c[1\,\ldots\,n]" class="ltx_Math" display="inline" id="alg2.l1.m4.1"><semantics id="alg2.l1.m4.1a"><mrow id="alg2.l1.m4.1.1" xref="alg2.l1.m4.1.1.cmml"><mi id="alg2.l1.m4.1.1.3" xref="alg2.l1.m4.1.1.3.cmml">c</mi><mo id="alg2.l1.m4.1.1.2" xref="alg2.l1.m4.1.1.2.cmml">⁢</mo><mrow id="alg2.l1.m4.1.1.1.1" xref="alg2.l1.m4.1.1.1.2.cmml"><mo id="alg2.l1.m4.1.1.1.1.2" stretchy="false" xref="alg2.l1.m4.1.1.1.2.1.cmml">[</mo><mrow id="alg2.l1.m4.1.1.1.1.1" xref="alg2.l1.m4.1.1.1.1.1.cmml"><mn id="alg2.l1.m4.1.1.1.1.1.2" xref="alg2.l1.m4.1.1.1.1.1.2.cmml">1</mn><mo id="alg2.l1.m4.1.1.1.1.1.1" lspace="0.170em" xref="alg2.l1.m4.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg2.l1.m4.1.1.1.1.1.3" mathvariant="normal" xref="alg2.l1.m4.1.1.1.1.1.3.cmml">…</mi><mo id="alg2.l1.m4.1.1.1.1.1.1a" lspace="0.170em" xref="alg2.l1.m4.1.1.1.1.1.1.cmml">⁢</mo><mi id="alg2.l1.m4.1.1.1.1.1.4" xref="alg2.l1.m4.1.1.1.1.1.4.cmml">n</mi></mrow><mo id="alg2.l1.m4.1.1.1.1.3" stretchy="false" xref="alg2.l1.m4.1.1.1.2.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l1.m4.1b"><apply id="alg2.l1.m4.1.1.cmml" xref="alg2.l1.m4.1.1"><times id="alg2.l1.m4.1.1.2.cmml" xref="alg2.l1.m4.1.1.2"></times><ci id="alg2.l1.m4.1.1.3.cmml" xref="alg2.l1.m4.1.1.3">𝑐</ci><apply id="alg2.l1.m4.1.1.1.2.cmml" xref="alg2.l1.m4.1.1.1.1"><csymbol cd="latexml" id="alg2.l1.m4.1.1.1.2.1.cmml" xref="alg2.l1.m4.1.1.1.1.2">delimited-[]</csymbol><apply id="alg2.l1.m4.1.1.1.1.1.cmml" xref="alg2.l1.m4.1.1.1.1.1"><times id="alg2.l1.m4.1.1.1.1.1.1.cmml" xref="alg2.l1.m4.1.1.1.1.1.1"></times><cn id="alg2.l1.m4.1.1.1.1.1.2.cmml" type="integer" xref="alg2.l1.m4.1.1.1.1.1.2">1</cn><ci id="alg2.l1.m4.1.1.1.1.1.3.cmml" xref="alg2.l1.m4.1.1.1.1.1.3">…</ci><ci id="alg2.l1.m4.1.1.1.1.1.4.cmml" xref="alg2.l1.m4.1.1.1.1.1.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l1.m4.1c">c[1\,\ldots\,n]</annotation><annotation encoding="application/x-llamapun" id="alg2.l1.m4.1d">italic_c [ 1 … italic_n ]</annotation></semantics></math>;

</div>
<div class="ltx_listingline" id="alg2.l2">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l2.1.1.1" style="font-size:80%;">2:</span></span>  /* Apply hashing to input sparse features and limit their values */

</div>
<div class="ltx_listingline" id="alg2.l3">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l3.1.1.1" style="font-size:80%;">3:</span></span>  <span class="ltx_text ltx_font_bold" id="alg2.l3.2">for</span> <math alttext="i\leftarrow" class="ltx_Math" display="inline" id="alg2.l3.m1.1"><semantics id="alg2.l3.m1.1a"><mrow id="alg2.l3.m1.1.1" xref="alg2.l3.m1.1.1.cmml"><mi id="alg2.l3.m1.1.1.2" xref="alg2.l3.m1.1.1.2.cmml">i</mi><mo id="alg2.l3.m1.1.1.1" stretchy="false" xref="alg2.l3.m1.1.1.1.cmml">←</mo><mi id="alg2.l3.m1.1.1.3" xref="alg2.l3.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg2.l3.m1.1b"><apply id="alg2.l3.m1.1.1.cmml" xref="alg2.l3.m1.1.1"><ci id="alg2.l3.m1.1.1.1.cmml" xref="alg2.l3.m1.1.1.1">←</ci><ci id="alg2.l3.m1.1.1.2.cmml" xref="alg2.l3.m1.1.1.2">𝑖</ci><csymbol cd="latexml" id="alg2.l3.m1.1.1.3.cmml" xref="alg2.l3.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.m1.1c">i\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg2.l3.m1.1d">italic_i ←</annotation></semantics></math> <math alttext="1" class="ltx_Math" display="inline" id="alg2.l3.m2.1"><semantics id="alg2.l3.m2.1a"><mn id="alg2.l3.m2.1.1" xref="alg2.l3.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="alg2.l3.m2.1b"><cn id="alg2.l3.m2.1.1.cmml" type="integer" xref="alg2.l3.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="alg2.l3.m2.1d">1</annotation></semantics></math> to <math alttext="n" class="ltx_Math" display="inline" id="alg2.l3.m3.1"><semantics id="alg2.l3.m3.1a"><mi id="alg2.l3.m3.1.1" xref="alg2.l3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="alg2.l3.m3.1b"><ci id="alg2.l3.m3.1.1.cmml" xref="alg2.l3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="alg2.l3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="alg2.l3.m3.1d">italic_n</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="alg2.l3.3">do</span>
</div>
<div class="ltx_listingline" id="alg2.l4">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l4.1.1.1" style="font-size:80%;">4:</span></span>     /* Compute seeded hash function */

</div>
<div class="ltx_listingline" id="alg2.l5">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l5.1.1.1" style="font-size:80%;">5:</span></span>     <math alttext="h\leftarrow\texttt{ComputeHash}(a[i],\,s)" class="ltx_Math" display="inline" id="alg2.l5.m1.3"><semantics id="alg2.l5.m1.3a"><mrow id="alg2.l5.m1.3.3" xref="alg2.l5.m1.3.3.cmml"><mi id="alg2.l5.m1.3.3.3" xref="alg2.l5.m1.3.3.3.cmml">h</mi><mo id="alg2.l5.m1.3.3.2" stretchy="false" xref="alg2.l5.m1.3.3.2.cmml">←</mo><mrow id="alg2.l5.m1.3.3.1" xref="alg2.l5.m1.3.3.1.cmml"><mtext class="ltx_mathvariant_monospace" id="alg2.l5.m1.3.3.1.3" xref="alg2.l5.m1.3.3.1.3a.cmml">ComputeHash</mtext><mo id="alg2.l5.m1.3.3.1.2" xref="alg2.l5.m1.3.3.1.2.cmml">⁢</mo><mrow id="alg2.l5.m1.3.3.1.1.1" xref="alg2.l5.m1.3.3.1.1.2.cmml"><mo id="alg2.l5.m1.3.3.1.1.1.2" stretchy="false" xref="alg2.l5.m1.3.3.1.1.2.cmml">(</mo><mrow id="alg2.l5.m1.3.3.1.1.1.1" xref="alg2.l5.m1.3.3.1.1.1.1.cmml"><mi id="alg2.l5.m1.3.3.1.1.1.1.2" xref="alg2.l5.m1.3.3.1.1.1.1.2.cmml">a</mi><mo id="alg2.l5.m1.3.3.1.1.1.1.1" xref="alg2.l5.m1.3.3.1.1.1.1.1.cmml">⁢</mo><mrow id="alg2.l5.m1.3.3.1.1.1.1.3.2" xref="alg2.l5.m1.3.3.1.1.1.1.3.1.cmml"><mo id="alg2.l5.m1.3.3.1.1.1.1.3.2.1" stretchy="false" xref="alg2.l5.m1.3.3.1.1.1.1.3.1.1.cmml">[</mo><mi id="alg2.l5.m1.1.1" xref="alg2.l5.m1.1.1.cmml">i</mi><mo id="alg2.l5.m1.3.3.1.1.1.1.3.2.2" stretchy="false" xref="alg2.l5.m1.3.3.1.1.1.1.3.1.1.cmml">]</mo></mrow></mrow><mo id="alg2.l5.m1.3.3.1.1.1.3" rspace="0.337em" xref="alg2.l5.m1.3.3.1.1.2.cmml">,</mo><mi id="alg2.l5.m1.2.2" xref="alg2.l5.m1.2.2.cmml">s</mi><mo id="alg2.l5.m1.3.3.1.1.1.4" stretchy="false" xref="alg2.l5.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg2.l5.m1.3b"><apply id="alg2.l5.m1.3.3.cmml" xref="alg2.l5.m1.3.3"><ci id="alg2.l5.m1.3.3.2.cmml" xref="alg2.l5.m1.3.3.2">←</ci><ci id="alg2.l5.m1.3.3.3.cmml" xref="alg2.l5.m1.3.3.3">ℎ</ci><apply id="alg2.l5.m1.3.3.1.cmml" xref="alg2.l5.m1.3.3.1"><times id="alg2.l5.m1.3.3.1.2.cmml" xref="alg2.l5.m1.3.3.1.2"></times><ci id="alg2.l5.m1.3.3.1.3a.cmml" xref="alg2.l5.m1.3.3.1.3"><mtext class="ltx_mathvariant_monospace" id="alg2.l5.m1.3.3.1.3.cmml" xref="alg2.l5.m1.3.3.1.3">ComputeHash</mtext></ci><interval closure="open" id="alg2.l5.m1.3.3.1.1.2.cmml" xref="alg2.l5.m1.3.3.1.1.1"><apply id="alg2.l5.m1.3.3.1.1.1.1.cmml" xref="alg2.l5.m1.3.3.1.1.1.1"><times id="alg2.l5.m1.3.3.1.1.1.1.1.cmml" xref="alg2.l5.m1.3.3.1.1.1.1.1"></times><ci id="alg2.l5.m1.3.3.1.1.1.1.2.cmml" xref="alg2.l5.m1.3.3.1.1.1.1.2">𝑎</ci><apply id="alg2.l5.m1.3.3.1.1.1.1.3.1.cmml" xref="alg2.l5.m1.3.3.1.1.1.1.3.2"><csymbol cd="latexml" id="alg2.l5.m1.3.3.1.1.1.1.3.1.1.cmml" xref="alg2.l5.m1.3.3.1.1.1.1.3.2.1">delimited-[]</csymbol><ci id="alg2.l5.m1.1.1.cmml" xref="alg2.l5.m1.1.1">𝑖</ci></apply></apply><ci id="alg2.l5.m1.2.2.cmml" xref="alg2.l5.m1.2.2">𝑠</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l5.m1.3c">h\leftarrow\texttt{ComputeHash}(a[i],\,s)</annotation><annotation encoding="application/x-llamapun" id="alg2.l5.m1.3d">italic_h ← ComputeHash ( italic_a [ italic_i ] , italic_s )</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l6">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l6.1.1.1" style="font-size:80%;">6:</span></span>     <math alttext="c[i]\leftarrow" class="ltx_Math" display="inline" id="alg2.l6.m1.1"><semantics id="alg2.l6.m1.1a"><mrow id="alg2.l6.m1.1.2" xref="alg2.l6.m1.1.2.cmml"><mrow id="alg2.l6.m1.1.2.2" xref="alg2.l6.m1.1.2.2.cmml"><mi id="alg2.l6.m1.1.2.2.2" xref="alg2.l6.m1.1.2.2.2.cmml">c</mi><mo id="alg2.l6.m1.1.2.2.1" xref="alg2.l6.m1.1.2.2.1.cmml">⁢</mo><mrow id="alg2.l6.m1.1.2.2.3.2" xref="alg2.l6.m1.1.2.2.3.1.cmml"><mo id="alg2.l6.m1.1.2.2.3.2.1" stretchy="false" xref="alg2.l6.m1.1.2.2.3.1.1.cmml">[</mo><mi id="alg2.l6.m1.1.1" xref="alg2.l6.m1.1.1.cmml">i</mi><mo id="alg2.l6.m1.1.2.2.3.2.2" stretchy="false" xref="alg2.l6.m1.1.2.2.3.1.1.cmml">]</mo></mrow></mrow><mo id="alg2.l6.m1.1.2.1" stretchy="false" xref="alg2.l6.m1.1.2.1.cmml">←</mo><mi id="alg2.l6.m1.1.2.3" xref="alg2.l6.m1.1.2.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="alg2.l6.m1.1b"><apply id="alg2.l6.m1.1.2.cmml" xref="alg2.l6.m1.1.2"><ci id="alg2.l6.m1.1.2.1.cmml" xref="alg2.l6.m1.1.2.1">←</ci><apply id="alg2.l6.m1.1.2.2.cmml" xref="alg2.l6.m1.1.2.2"><times id="alg2.l6.m1.1.2.2.1.cmml" xref="alg2.l6.m1.1.2.2.1"></times><ci id="alg2.l6.m1.1.2.2.2.cmml" xref="alg2.l6.m1.1.2.2.2">𝑐</ci><apply id="alg2.l6.m1.1.2.2.3.1.cmml" xref="alg2.l6.m1.1.2.2.3.2"><csymbol cd="latexml" id="alg2.l6.m1.1.2.2.3.1.1.cmml" xref="alg2.l6.m1.1.2.2.3.2.1">delimited-[]</csymbol><ci id="alg2.l6.m1.1.1.cmml" xref="alg2.l6.m1.1.1">𝑖</ci></apply></apply><csymbol cd="latexml" id="alg2.l6.m1.1.2.3.cmml" xref="alg2.l6.m1.1.2.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l6.m1.1c">c[i]\leftarrow</annotation><annotation encoding="application/x-llamapun" id="alg2.l6.m1.1d">italic_c [ italic_i ] ←</annotation></semantics></math> <math alttext="h\bmod d" class="ltx_Math" display="inline" id="alg2.l6.m2.1"><semantics id="alg2.l6.m2.1a"><mrow id="alg2.l6.m2.1.1" xref="alg2.l6.m2.1.1.cmml"><mi id="alg2.l6.m2.1.1.2" xref="alg2.l6.m2.1.1.2.cmml">h</mi><mo id="alg2.l6.m2.1.1.1" xref="alg2.l6.m2.1.1.1.cmml">mod</mo><mi id="alg2.l6.m2.1.1.3" xref="alg2.l6.m2.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="alg2.l6.m2.1b"><apply id="alg2.l6.m2.1.1.cmml" xref="alg2.l6.m2.1.1"><csymbol cd="latexml" id="alg2.l6.m2.1.1.1.cmml" xref="alg2.l6.m2.1.1.1">modulo</csymbol><ci id="alg2.l6.m2.1.1.2.cmml" xref="alg2.l6.m2.1.1.2">ℎ</ci><ci id="alg2.l6.m2.1.1.3.cmml" xref="alg2.l6.m2.1.1.3">𝑑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg2.l6.m2.1c">h\bmod d</annotation><annotation encoding="application/x-llamapun" id="alg2.l6.m2.1d">italic_h roman_mod italic_d</annotation></semantics></math>
</div>
<div class="ltx_listingline" id="alg2.l7">
<span class="ltx_tag ltx_tag_listingline"><span class="ltx_text" id="alg2.l7.1.1.1" style="font-size:80%;">7:</span></span>  <span class="ltx_text ltx_font_bold" id="alg2.l7.2">end</span> <span class="ltx_text ltx_font_bold" id="alg2.l7.3">for</span>
</div>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">System Architecture for CPU-centric Data Preprocessing</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS4.p1.1.1">Software architecture.</span> As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>, the RecSys training pipeline employs the
producer-consumer model. GPU training workers load and consume train-ready
tensors (i.e., mini-batches) that the CPU preprocessing workers generate by
transforming raw feature data. Because all transformation operations conducted
within a mini-batch are executed locally without any dependencies to other
mini-batches, state-of-the-art frameworks for end-to-end RecSys training pipeline such as
TorchRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib24" title="">24</a>]</cite> allocate a single worker per each CPU core to handle the generation of
each train-ready tensors that constitute a given mini-batch. By spawning
multiple CPU workers in parallel, multiple input mini-batches are concurrently
generated, enabling scalable improvements in data preprocessing throughput.</p>
</div>
<div class="ltx_para" id="S2.SS4.p2">
<p class="ltx_p" id="S2.SS4.p2.3"><span class="ltx_text ltx_font_bold" id="S2.SS4.p2.3.1">Hardware architecture.</span> While the software architecture of RecSys data
preprocessing provides high scalability, a critical challenge remains
regarding <em class="ltx_emph ltx_font_italic" id="S2.SS4.p2.3.2">how to allocate a sufficient number of CPU cores that provide
high enough data preprocessing throughput that meets the throughput demands
of GPU-side training?</em> Traditionally, model training and data preprocessing
jobs are co-located within the same server node containing multiple GPUs (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F2" title="Figure 2 ‣ II-D System Architecture for CPU-centric Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>(a)) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib49" title="">49</a>]</cite>. As such, the
performance of such co-located training pipeline is limited by how many CPU
cores are available within the same server node (e.g., NVIDIA DGX system
contains <math alttext="8" class="ltx_Math" display="inline" id="S2.SS4.p2.1.m1.1"><semantics id="S2.SS4.p2.1.m1.1a"><mn id="S2.SS4.p2.1.m1.1.1" xref="S2.SS4.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.1.m1.1b"><cn id="S2.SS4.p2.1.m1.1.1.cmml" type="integer" xref="S2.SS4.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.1.m1.1d">8</annotation></semantics></math> A100 GPUs and <math alttext="128" class="ltx_Math" display="inline" id="S2.SS4.p2.2.m2.1"><semantics id="S2.SS4.p2.2.m2.1a"><mn id="S2.SS4.p2.2.m2.1.1" xref="S2.SS4.p2.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.2.m2.1b"><cn id="S2.SS4.p2.2.m2.1.1.cmml" type="integer" xref="S2.SS4.p2.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.2.m2.1c">128</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.2.m2.1d">128</annotation></semantics></math> CPU cores, allowing a single GPU
to utilize <math alttext="16" class="ltx_Math" display="inline" id="S2.SS4.p2.3.m3.1"><semantics id="S2.SS4.p2.3.m3.1a"><mn id="S2.SS4.p2.3.m3.1.1" xref="S2.SS4.p2.3.m3.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S2.SS4.p2.3.m3.1b"><cn id="S2.SS4.p2.3.m3.1.1.cmml" type="integer" xref="S2.SS4.p2.3.m3.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p2.3.m3.1c">16</annotation><annotation encoding="application/x-llamapun" id="S2.SS4.p2.3.m3.1d">16</annotation></semantics></math> CPU cores for data preprocessing), potentially suffering from
significant GPU underutilization when the aggregate CPU-side data
preprocessing throughput underwhelms the GPU-side
training performance (detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.SS1" title="III-A Motivation ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>).</p>
</div>
<div class="ltx_para" id="S2.SS4.p3">
<p class="ltx_p" id="S2.SS4.p3.1">To address these challenges, Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite> and Audibert et
al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib5" title="">5</a>]</cite> proposed server
disaggregation where a pool of CPU servers is reserved
for data preprocessing (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F2" title="Figure 2 ‣ II-D System Architecture for CPU-centric Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>(b)).
Disaggregating CPU servers allows a large pool of CPU cores to be elastically
allocated on-demand for preprocessing, effectively closing the performance
gap between preprocessing and model training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib25" title="">25</a>]</cite>. However, such
design point incurs substantial deployment cost and power consumption due to
the large number of pooled CPU servers.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="126" id="S2.F2.g1" src="x2.png" width="382"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>System architectures for RecSys training. (a) A system that co-locates CPU-based data preprocessing
workers with GPU-based model training workers within the same server node. (b) A system that provisions a pool of disaggregated CPU servers for data preprocessing. </figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Characterization and Motivation</span>
</h2>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<td class="ltx_td ltx_border_tt" id="S3.T1.1.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_border_tt" id="S3.T1.1.1.1.2" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="5" id="S3.T1.1.1.1.3" style="padding-left:3.0pt;padding-right:3.0pt;">Data preprocessing configuration parameters</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S3.T1.1.1.1.4" style="padding-left:3.0pt;padding-right:3.0pt;">RecSys model architecture</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2.2">
<td class="ltx_td ltx_align_center" colspan="2" id="S3.T1.1.2.2.1" style="padding-left:3.0pt;padding-right:3.0pt;">Type</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.2" style="padding-left:3.0pt;padding-right:3.0pt;"># Dense feats.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.3" style="padding-left:3.0pt;padding-right:3.0pt;"># Sparse feats.</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T1.1.2.2.4" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S3.T1.1.2.2.4.1"></span> <span class="ltx_text" id="S3.T1.1.2.2.4.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.2.2.4.2.1">
<span class="ltx_tr" id="S3.T1.1.2.2.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.2.2.4.2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.2.2.4.2.1.1.1.1" style="font-size:80%;">Avg. sparse</span></span></span>
<span class="ltx_tr" id="S3.T1.1.2.2.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.2.2.4.2.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.2.2.4.2.1.2.1.1" style="font-size:80%;">feat. length</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.2.2.4.3"></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T1.1.2.2.5" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S3.T1.1.2.2.5.1"></span> <span class="ltx_text" id="S3.T1.1.2.2.5.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.2.2.5.2.1">
<span class="ltx_tr" id="S3.T1.1.2.2.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.2.2.5.2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.2.2.5.2.1.1.1.1" style="font-size:80%;"># Generated</span></span></span>
<span class="ltx_tr" id="S3.T1.1.2.2.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.2.2.5.2.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.2.2.5.2.1.2.1.1" style="font-size:80%;">sparse feats.</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.2.2.5.3"></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.6" style="padding-left:3.0pt;padding-right:3.0pt;">Bucket size</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.7" style="padding-left:3.0pt;padding-right:3.0pt;">Bottom MLP</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2.8" style="padding-left:3.0pt;padding-right:3.0pt;">Top MLP</td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T1.1.2.2.9" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S3.T1.1.2.2.9.1"></span> <span class="ltx_text" id="S3.T1.1.2.2.9.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.2.2.9.2.1">
<span class="ltx_tr" id="S3.T1.1.2.2.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.2.2.9.2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.2.2.9.2.1.1.1.1" style="font-size:80%;"># Tables</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.2.2.9.3"></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T1.1.2.2.10" style="padding-left:3.0pt;padding-right:3.0pt;">
<span class="ltx_text" id="S3.T1.1.2.2.10.1"></span> <span class="ltx_text" id="S3.T1.1.2.2.10.2">
<span class="ltx_tabular ltx_align_middle" id="S3.T1.1.2.2.10.2.1">
<span class="ltx_tr" id="S3.T1.1.2.2.10.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.2.2.10.2.1.1.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.2.2.10.2.1.1.1.1" style="font-size:80%;">Avg.</span></span></span>
<span class="ltx_tr" id="S3.T1.1.2.2.10.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T1.1.2.2.10.2.1.2.1" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.2.2.10.2.1.2.1.1" style="font-size:80%;"># Embeddings</span></span></span>
</span></span><span class="ltx_text" id="S3.T1.1.2.2.10.3"></span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.1" style="padding-left:3.0pt;padding-right:3.0pt;">Public</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.2" style="padding-left:3.0pt;padding-right:3.0pt;">RM1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.3" style="padding-left:3.0pt;padding-right:3.0pt;">13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.4" style="padding-left:3.0pt;padding-right:3.0pt;">26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.5" style="padding-left:3.0pt;padding-right:3.0pt;">1 (fixed)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.6" style="padding-left:3.0pt;padding-right:3.0pt;">13</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.7" style="padding-left:3.0pt;padding-right:3.0pt;">1024</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.8" style="padding-left:3.0pt;padding-right:3.0pt;">512-256-128</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.9" style="padding-left:3.0pt;padding-right:3.0pt;">1024-1024-512-256-1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.10" style="padding-left:3.0pt;padding-right:3.0pt;">39</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.3.3.11" style="padding-left:3.0pt;padding-right:3.0pt;">500,000</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.4.4.1" rowspan="4" style="padding-left:3.0pt;padding-right:3.0pt;"><span class="ltx_text" id="S3.T1.1.4.4.1.1">Synthetic</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.2" style="padding-left:3.0pt;padding-right:3.0pt;">RM2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.3" style="padding-left:3.0pt;padding-right:3.0pt;">504</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.4" style="padding-left:3.0pt;padding-right:3.0pt;">42</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.5" style="padding-left:3.0pt;padding-right:3.0pt;">20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.6" style="padding-left:3.0pt;padding-right:3.0pt;">21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.7" style="padding-left:3.0pt;padding-right:3.0pt;">1024</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.8" style="padding-left:3.0pt;padding-right:3.0pt;">512-256-128</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.9" style="padding-left:3.0pt;padding-right:3.0pt;">1024-1024-512-256-1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.10" style="padding-left:3.0pt;padding-right:3.0pt;">63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.4.4.11" style="padding-left:3.0pt;padding-right:3.0pt;">500,000</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.5">
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.1" style="padding-left:3.0pt;padding-right:3.0pt;">RM3</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.2" style="padding-left:3.0pt;padding-right:3.0pt;">504</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.3" style="padding-left:3.0pt;padding-right:3.0pt;">42</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.4" style="padding-left:3.0pt;padding-right:3.0pt;">20</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.5" style="padding-left:3.0pt;padding-right:3.0pt;">42</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.6" style="padding-left:3.0pt;padding-right:3.0pt;">1024</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.7" style="padding-left:3.0pt;padding-right:3.0pt;">512-256-128</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.8" style="padding-left:3.0pt;padding-right:3.0pt;">1024-1024-512-256-1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.9" style="padding-left:3.0pt;padding-right:3.0pt;">84</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.5.5.10" style="padding-left:3.0pt;padding-right:3.0pt;">500,000</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.6">
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.1" style="padding-left:3.0pt;padding-right:3.0pt;">RM4</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.2" style="padding-left:3.0pt;padding-right:3.0pt;">504</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.3" style="padding-left:3.0pt;padding-right:3.0pt;">42</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.4" style="padding-left:3.0pt;padding-right:3.0pt;">20</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.5" style="padding-left:3.0pt;padding-right:3.0pt;">42</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.6" style="padding-left:3.0pt;padding-right:3.0pt;">2048</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.7" style="padding-left:3.0pt;padding-right:3.0pt;">512-256-128</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.8" style="padding-left:3.0pt;padding-right:3.0pt;">1024-1024-512-256-1</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.9" style="padding-left:3.0pt;padding-right:3.0pt;">84</td>
<td class="ltx_td ltx_align_center" id="S3.T1.1.6.6.10" style="padding-left:3.0pt;padding-right:3.0pt;">500,000</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.1" style="padding-left:3.0pt;padding-right:3.0pt;">RM5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.2" style="padding-left:3.0pt;padding-right:3.0pt;">504</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.3" style="padding-left:3.0pt;padding-right:3.0pt;">42</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.4" style="padding-left:3.0pt;padding-right:3.0pt;">20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.5" style="padding-left:3.0pt;padding-right:3.0pt;">42</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.6" style="padding-left:3.0pt;padding-right:3.0pt;">4096</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.7" style="padding-left:3.0pt;padding-right:3.0pt;">512-256-128</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.8" style="padding-left:3.0pt;padding-right:3.0pt;">1024-1024-512-256-1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.9" style="padding-left:3.0pt;padding-right:3.0pt;">84</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.7.7.10" style="padding-left:3.0pt;padding-right:3.0pt;">500,000</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>The RecSys training dataset configuration and the target model architecture.
RM1 is based on the public Criteo dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib10" title="">10</a>]</cite> and RM2-5 are synthetically generated models we created by referring to production-grade RecSys dataset’s characteristics released by Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this paper, we utilize the open-source RecSys data preprocessing library
TorchArrow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib63" title="">63</a>]</cite> to conduct a workload characterization
study on state-of-the-art, CPU-centric data preprocessing systems. We note
that there exists a significant disparity between the publicly available RecSys
dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib10" title="">10</a>]</cite> and the characteristics of a production-level
dataset mentioned by a recent work from Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite>. Specifically, compared
to the public dataset, production-level RecSys datasets contain a much larger
number of dense/sparse features and larger average sparse feature length.
Such discrepancy can undermine the primary objective of our study which is to characterize
state-of-the-art RecSys preprocessing. Consequently, we scale up the
open-sourced Criteo dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib10" title="">10</a>]</cite> (referred to as RM1
in this paper) and develop four <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">synthetic</em> RecSys models (RM2-5) based
on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite> to better represent the properties of
production-level RecSys datasets. Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.T1" title="TABLE I ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">I</span></a> summarizes the details
of our public/synthetic datasets and the RecSys models trained.
Our characterization is conducted with a training batch size of 8,192.
Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5" title="V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">V</span></a> further details our evaluation methodology.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="125" id="S3.F3.g1" src="x3.png" width="403"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Effective preprocessing throughput (left axis) and the resulting GPU utilization (right axis) as a function of the number of CPU cores (i.e., number of preprocessing workers) utilized for preprocessing. The dotted line shows the upperbound, maximum training throughput achievable using a single NVIDIA A100 GPU (left axis), which assumes the GPU is seamlessly fed with sufficient amount of train-ready tensors without interruption. To measure GPU’s utilization, we use the CUDA Profiling Tools Interface (CUPTI) library. The experiment is collected over the evaluation platform detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5" title="V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">V</span></a> using our synthetic model RM5.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Motivation</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.9">As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.SS4" title="II-D System Architecture for CPU-centric Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-D</span></span></a>, the aggregate data
preprocessing throughput is strictly determined by how many CPU cores (i.e.,
the number of data preprocessing workers) are utilized for
preprocessing. Consider a co-location based RecSys training system
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F2" title="Figure 2 ‣ II-D System Architecture for CPU-centric Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>(a)) using a state-of-the-art DGX
server <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib52" title="">52</a>]</cite> which contains <math alttext="8" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn id="S3.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">8</annotation></semantics></math> A100 GPUs and <math alttext="128" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn id="S3.SS1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">128</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">128</annotation></semantics></math> CPU cores,
allowing a single GPU to utilize <math alttext="16" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><cn id="S3.SS1.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">16</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">16</annotation></semantics></math> CPU cores for data preprocessing. In
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.F3" title="Figure 3 ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">3</span></a>, we scale up the number of CPU cores
for preprocessing (from <math alttext="1" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mn id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><cn id="S3.SS1.p1.4.m4.1.1.cmml" type="integer" xref="S3.SS1.p1.4.m4.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">1</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">1</annotation></semantics></math> to a maximum of <math alttext="16" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mn id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><cn id="S3.SS1.p1.5.m5.1.1.cmml" type="integer" xref="S3.SS1.p1.5.m5.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">16</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">16</annotation></semantics></math>) and study its effect on
data preprocessing throughput (left axis) and the percentage of execution
time the A100 GPU is actually training the model (right axis). We
make the following two key observations from this experiment. First, the
preprocessing throughput increases almost linearly as a function of the number
of CPU cores, achieving <math alttext="15\times" class="ltx_math_unparsed" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1b"><mn id="S3.SS1.p1.6.m6.1.1">15</mn><mo id="S3.SS1.p1.6.m6.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">15\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">15 ×</annotation></semantics></math> throughput improvement with <math alttext="16" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mn id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><cn id="S3.SS1.p1.7.m7.1.1.cmml" type="integer" xref="S3.SS1.p1.7.m7.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">16</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">16</annotation></semantics></math>
preprocessing workers vs. a single worker. Second, even with <math alttext="16" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><mn id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><cn id="S3.SS1.p1.8.m8.1.1.cmml" type="integer" xref="S3.SS1.p1.8.m8.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">16</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">16</annotation></semantics></math>
preprocessing workers (i.e., the maximum number of CPU cores that can be
allocated in co-located RecSys preprocessing), the GPU spends less than <math alttext="20\%" class="ltx_Math" display="inline" id="S3.SS1.p1.9.m9.1"><semantics id="S3.SS1.p1.9.m9.1a"><mrow id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml"><mn id="S3.SS1.p1.9.m9.1.1.2" xref="S3.SS1.p1.9.m9.1.1.2.cmml">20</mn><mo id="S3.SS1.p1.9.m9.1.1.1" xref="S3.SS1.p1.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><apply id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1"><csymbol cd="latexml" id="S3.SS1.p1.9.m9.1.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1.1">percent</csymbol><cn id="S3.SS1.p1.9.m9.1.1.2.cmml" type="integer" xref="S3.SS1.p1.9.m9.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">20\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.9.m9.1d">20 %</annotation></semantics></math> of its
execution time actually conducting model training as the train-ready tensors
are not being sufficiently supplied to the GPU. Consequently, the end-to-end
training throughput gets bounded by the effective preprocessing
throughput which is well below the maximum training throughput
achievable (dotted line).</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="108" id="S3.F4.g1" src="x4.png" width="365"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The number of CPU cores required for CPU-centric preprocessing to fully utilize a training node containing 8 A100 GPUs.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.3">Server disaggregation for data preprocessing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib25" title="">25</a>]</cite> is an effective
solution to close such wide performance gap, as it enables the allocation of
any number of CPU preprocessing workers as required by the GPU training stage
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F2" title="Figure 2 ‣ II-D System Architecture for CPU-centric Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">2</span></a>(b)). In
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.F4" title="Figure 4 ‣ III-A Motivation ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">4</span></a>, we derive the number of CPU cores required in
the data preprocessing stage to sustain the model
training stage’s high throughput requirement. For production-level synthetic datasets with large number of features and sparse feature
lengths, several hundreds of CPU cores are required (<math alttext="367" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">367</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn id="S3.SS1.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS1.p2.1.m1.1.1">367</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">367</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">367</annotation></semantics></math> cores for RM5)
to sufficiently supply the train-ready tensors for an <math alttext="8" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn id="S3.SS1.p2.2.m2.1.1.cmml" type="integer" xref="S3.SS1.p2.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">8</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">8</annotation></semantics></math> A100 GPU server node.
It is important to note that hundreds to thousands of
such production-level RecSys models are developed by
ML engineers, invoking numerous
concurrent training jobs executed over
several tens of thousands of high-performance GPUs across
the datacenter fleet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib25" title="">25</a>]</cite>.
Such high demand for RecSys model training directly translates into substantial
cost and power consumption in maintaining the disaggregated CPU servers for data
preprocessing, e.g., Meta states that up to <math alttext="60\%" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mn id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">60</mn><mo id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="latexml" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1">percent</csymbol><cn id="S3.SS1.p2.3.m3.1.1.2.cmml" type="integer" xref="S3.SS1.p2.3.m3.1.1.2">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">60\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">60 %</annotation></semantics></math>
of power consumption in RecSys training pipeline is dedicated to
the storage and data ingestion pipeline of online CPU-centric data preprocessing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Given such, a key motivation of this work is to conduct a detailed characterization on
CPU-centric RecSys data preprocessing systems. In the remainder of this section,
we root-cause the system-level bottlenecks of existing solutions in search for
a scalable and cost-effective preprocessing system for RecSys.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Breakdown of End-to-End Data Preprocessing Time</span>
</h3>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="130" id="S3.F5.g1" src="x5.png" width="390"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Latency to preprocess a single mini-batch input using a single preprocessing worker in the
baseline CPU-centric system, broken into key steps of preprocessing. The “Extract” stage (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.SS1" title="II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a>) is further divided into (1) latency to fetch encoded raw feature
data from the remote storage node (denoted as “Extract (Read)”) and (2) latency spent decoding them (denoted as “Extract (Decode)”). As depicted, data preprocessing is bounded by the compute-intensive feature generation and normalization operations, rather than I/O operations (“Extract (Read)”). All results are normalized to RM1.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.9">To identify the key bottlenecks in RecSys data preprocessing,
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.F5" title="Figure 5 ‣ III-B Breakdown of End-to-End Data Preprocessing Time ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">5</span></a> first evaluates end-to-end latency to
preprocess a single training mini-batch during the ETL phase.
Compared to the public RM1, the production-level RM2-5
contain a larger number of dense and sparse features with larger average
sparse feature lengths.
As such, these production-level
models experience a substantial increase in
total preprocessing time where RM5 experiences the largest <math alttext="14\times" class="ltx_math_unparsed" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1b"><mn id="S3.SS2.p1.1.m1.1.1">14</mn><mo id="S3.SS2.p1.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">14\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">14 ×</annotation></semantics></math> increase in latency.
Specifically, with the larger number of features in the RM2-5 models, both dense and sparse
feature normalization time (i.e., Log and SigridHash, respectively) account for up
to <math alttext="55\%" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mrow id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mn id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">55</mn><mo id="S3.SS2.p1.2.m2.1.1.1" xref="S3.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1.1">percent</csymbol><cn id="S3.SS2.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS2.p1.2.m2.1.1.2">55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">55\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">55 %</annotation></semantics></math> of its preprocessing time.
Additionally, these production-grade RM2-5 models
experience a notable increase in feature generation time (i.e., Bucketize)
due to the large number of sparse features to generate and its large bucket size.
While RM3-5 all have the same number of sparse features to generate (at <math alttext="42" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">42</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn id="S3.SS2.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS2.p1.3.m3.1.1">42</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">42</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">42</annotation></semantics></math>), larger
bucket sizes (from <math alttext="1024" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mn id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><cn id="S3.SS2.p1.4.m4.1.1.cmml" type="integer" xref="S3.SS2.p1.4.m4.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">1024</annotation></semantics></math> to <math alttext="4096" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><mn id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">4096</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><cn id="S3.SS2.p1.5.m5.1.1.cmml" type="integer" xref="S3.SS2.p1.5.m5.1.1">4096</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">4096</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">4096</annotation></semantics></math> in RM3 to RM5, <math alttext="m" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><mi id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><ci id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_m</annotation></semantics></math> in Algorithm <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#alg1" title="Algorithm 1 ‣ II-C Feature Generation/Normalization in Data Preprocessing ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>) incur longer feature generation time because the
latency to search the bucket ID of each feature value increases. Conversely, despite RM1-3 all having the same bucket size at <math alttext="1024" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><mn id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">1024</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><cn id="S3.SS2.p1.7.m7.1.1.cmml" type="integer" xref="S3.SS2.p1.7.m7.1.1">1024</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">1024</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.1d">1024</annotation></semantics></math>,
the larger number of sparse features to generate (from <math alttext="13" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1"><semantics id="S3.SS2.p1.8.m8.1a"><mn id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><cn id="S3.SS2.p1.8.m8.1.1.cmml" type="integer" xref="S3.SS2.p1.8.m8.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">13</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m8.1d">13</annotation></semantics></math> to <math alttext="42" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m9.1"><semantics id="S3.SS2.p1.9.m9.1a"><mn id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">42</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><cn id="S3.SS2.p1.9.m9.1.1.cmml" type="integer" xref="S3.SS2.p1.9.m9.1.1">42</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">42</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.9.m9.1d">42</annotation></semantics></math> in RM1 to RM3) leads to larger feature generation time as
well.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Overall, feature generation (Bucketize) and feature normalization (SigridHash and Log) collectively account
for an average <math alttext="79\%" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">79</mn><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1">percent</csymbol><cn id="S3.SS2.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.2">79</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">79\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">79 %</annotation></semantics></math> of the data preprocessing time and become the most
significant performance limiter in RecSys preprocessing.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Analysis on Performance-Limiting Operations</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.3">Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.F6" title="Figure 6 ‣ III-C Analysis on Performance-Limiting Operations ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">6</span></a> shows the CPU and memory bandwidth
utilization and the last-level cache (LLC) hit rate during the execution of
the three key performance-limiting operations (i.e., Bucketize, SigridHash,
Log) in RM1 and RM5, respectively. Our
analysis reveals the following key insights. First thing to note is that all
three operations exhibit high CPU utilization with relatively low memory
bandwidth utilization. RM5 in particular achieves increased memory
bandwidth utilization as it has more features to generate and normalize
compared to RM1. Nonetheless, the memory bandwidth utility of RM5 still
remains under <math alttext="15\%" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mn id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">15</mn><mo id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1">percent</csymbol><cn id="S3.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S3.SS3.p1.1.m1.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">15\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">15 %</annotation></semantics></math> of the maximum <math alttext="281.6" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mn id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">281.6</mn><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><cn id="S3.SS3.p1.2.m2.1.1.cmml" type="float" xref="S3.SS3.p1.2.m2.1.1">281.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">281.6</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">281.6</annotation></semantics></math> GB/sec of memory throughput,
exhibiting a compute-bound behavior. While the feature generation and
normalization operations require large amount of data accesses, the
actual working set it touches upon is relatively small. In RM1 and RM5,
it amounts to as small as several tens of KBs to at most tens of
MBs. For instance, the
Bucketize operation involves sharding features based on the
predefined bucket range whose active working set fits well within
on-chip caches, leading to an LLC hit rate of <math alttext="85\%" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mrow id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml"><mn id="S3.SS3.p1.3.m3.1.1.2" xref="S3.SS3.p1.3.m3.1.1.2.cmml">85</mn><mo id="S3.SS3.p1.3.m3.1.1.1" xref="S3.SS3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><apply id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1.1">percent</csymbol><cn id="S3.SS3.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.SS3.p1.3.m3.1.1.2">85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">85\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">85 %</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="126" id="S3.F6.g1" src="x6.png" width="390"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>CPU and memory bandwidth utilization (left) and LLC hit rate (right) during the execution of feature generation/normalization operations for the public model (RM1) and the production-scale model (RM5). We utilize Linux <span class="ltx_text ltx_font_typewriter" id="S3.F6.2.1">perf</span> for our evaluation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.6.2.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.1.1">Our Goal: Scalable <math alttext="\&amp;" class="ltx_Math" display="inline" id="S3.SS4.1.1.m1.1"><semantics id="S3.SS4.1.1.m1.1b"><mo id="S3.SS4.1.1.m1.1.1" xref="S3.SS4.1.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.1.1.m1.1c"><and id="S3.SS4.1.1.m1.1.1.cmml" xref="S3.SS4.1.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.1.1.m1.1d">\&amp;</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.1.1.m1.1e">&amp;</annotation></semantics></math> Cost-Effective Preprocessing</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Overall, we conclude that current CPU-centric RecSys preprocessing systems
are bounded by the level of computation power available with CPUs,
failing to fully reap out the inter-/intra-feature parallelism inherent in
preprocessing. Although disaggregating a pool of CPUs for
data preprocessing can help meet the computation demands of
preprocessing, it requires high deployment cost and high power consumption.
We argue that the abundant inter-/intra-feature parallelism in data
preprocessing is well-suited for domain-specific acceleration, proposing an
<em class="ltx_emph ltx_font_italic" id="S3.SS4.p1.1.1">accelerated</em> computing system for RecSys data preprocessing which is
scalable and cost-effective.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><math alttext="PreSto" class="ltx_Math" display="inline" id="S4.1.m1.1"><semantics id="S4.1.m1.1b"><mrow id="S4.1.m1.1.1" xref="S4.1.m1.1.1.cmml"><mi id="S4.1.m1.1.1.2" xref="S4.1.m1.1.1.2.cmml">P</mi><mo id="S4.1.m1.1.1.1" xref="S4.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.1.m1.1.1.3" xref="S4.1.m1.1.1.3.cmml">r</mi><mo id="S4.1.m1.1.1.1b" xref="S4.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.1.m1.1.1.4" xref="S4.1.m1.1.1.4.cmml">e</mi><mo id="S4.1.m1.1.1.1c" xref="S4.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.1.m1.1.1.5" xref="S4.1.m1.1.1.5.cmml">S</mi><mo id="S4.1.m1.1.1.1d" xref="S4.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.1.m1.1.1.6" xref="S4.1.m1.1.1.6.cmml">t</mi><mo id="S4.1.m1.1.1.1e" xref="S4.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.1.m1.1.1.7" xref="S4.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.1.m1.1c"><apply id="S4.1.m1.1.1.cmml" xref="S4.1.m1.1.1"><times id="S4.1.m1.1.1.1.cmml" xref="S4.1.m1.1.1.1"></times><ci id="S4.1.m1.1.1.2.cmml" xref="S4.1.m1.1.1.2">𝑃</ci><ci id="S4.1.m1.1.1.3.cmml" xref="S4.1.m1.1.1.3">𝑟</ci><ci id="S4.1.m1.1.1.4.cmml" xref="S4.1.m1.1.1.4">𝑒</ci><ci id="S4.1.m1.1.1.5.cmml" xref="S4.1.m1.1.1.5">𝑆</ci><ci id="S4.1.m1.1.1.6.cmml" xref="S4.1.m1.1.1.6">𝑡</ci><ci id="S4.1.m1.1.1.7.cmml" xref="S4.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.1.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S4.1.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math><span class="ltx_text ltx_font_smallcaps" id="S4.2.1">: An In-Storage “Pre”processing Architecture for RecSys Training</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We present <math alttext="PreSto" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">P</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">r</mi><mo id="S4.p1.1.m1.1.1.1a" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.4" xref="S4.p1.1.m1.1.1.4.cmml">e</mi><mo id="S4.p1.1.m1.1.1.1b" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.5" xref="S4.p1.1.m1.1.1.5.cmml">S</mi><mo id="S4.p1.1.m1.1.1.1c" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.6" xref="S4.p1.1.m1.1.1.6.cmml">t</mi><mo id="S4.p1.1.m1.1.1.1d" xref="S4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.p1.1.m1.1.1.7" xref="S4.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><times id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></times><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">𝑟</ci><ci id="S4.p1.1.m1.1.1.4.cmml" xref="S4.p1.1.m1.1.1.4">𝑒</ci><ci id="S4.p1.1.m1.1.1.5.cmml" xref="S4.p1.1.m1.1.1.5">𝑆</ci><ci id="S4.p1.1.m1.1.1.6.cmml" xref="S4.p1.1.m1.1.1.6">𝑡</ci><ci id="S4.p1.1.m1.1.1.7.cmml" xref="S4.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (<span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Pre</span>processing in-<span class="ltx_text ltx_font_bold" id="S4.p1.1.2">Sto</span>rage), our In-Storage Processing (ISP) based data preprocessing system for RecSys training.
Our proposed system offloads the
time-consuming feature generation and normalization operations to an
accelerator that is tightly coupled with the storage system.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="179" id="S4.F7.g1" src="x7.png" width="398"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>(a) Preprocessing accelerators co-located with GPUs and (b) disaggregated pool of preprocessing accelerators.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">System Design Considerations</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Our key proposition is to employ accelerated computing for
data preprocessing, so an important question to be answered is <em class="ltx_emph ltx_font_italic" id="S4.SS1.p1.1.1">where</em> our
accelerator(s) should be deployed within the system architecture. Below we elaborate
on the two possible design points that retrofit our data preprocessing accelerator within
conventional co-located and disaggregated server architectures.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Preprocessing accelerators co-located with GPUs.</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.F7" title="Figure 7 ‣ IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">7</span></a>(a) illustrates our first design point, a scale-“up”
server architecture employing a PCIe switch to co-locate the preprocessing
accelerators with the GPUs. When the number of co-located accelerators is
large enough to meet the GPU’s training demands, this design point can obviate
the need for disaggregated CPU servers dedicated to preprocessing. However, a
critical limitation of such scale-up server is its <em class="ltx_emph ltx_font_italic" id="S4.SS1.p2.1.2">scalability</em> because
the aggregate preprocessing throughput is still bounded by the total number of
accelerators co-located within this scale-up server. As such, for large-scale
RecSys models whose data preprocessing demands exceed the preprocessing
throughput available with co-located accelerators, the GPU training workers will
still suffer from idle periods. Another key concern with this approach is that
both the accelerator-side data preprocessing workers and the GPU-side training
workers all time-share the PCIe bus to communicate with the host CPU.
Because preprocessing workers and training workers concurrently
execute in a training pipeline, the PCIe bus can become a performance
hotspot under such unbalanced system architecture. Given such critical
limitations, we conclude that such scale-up solution is impractical.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">Disaggregated pool of preprocessing accelerators.</span> To address the
scalability issue in our scale-up server design, an alternative solution would
be to utilize our preprocessing accelerator as a drop-in replacement of CPUs in
the disaggregated preprocessing CPU pool. As shown in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.F7" title="Figure 7 ‣ IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">7</span></a>(b), this design point effectively provides a
disaggregated pool of preprocessing accelerators to the GPU training workers. By
decoupling training jobs from preprocessing jobs over distinct server pools, the
optimal number of preprocessing accelerators to allocate that meets a target
training job’s throughput demands can be determined dynamically, providing
high scalability. Furthermore, this design point can better exploit
inter-/intra-feature parallelism using domain-specific acceleration for high
efficiency. However, similar to the baseline CPU-centric disaggregated
preprocessing, server disaggregation still incurs substantial deployment cost
and high TCO.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Proposed Approach: In-Storage Data Preprocessing</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Hardware architecture.</span> Due to the aforementioned challenges of co-located
and disaggregated accelerator systems, our proposed system employs ISP (in-storage
processing) architectures for data preprocessing, i.e., <em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.2">in-storage
“pre”processing</em>. As shown in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.F8" title="Figure 8 ‣ IV-B Proposed Approach: In-Storage Data Preprocessing ‣ IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">8</span></a>, this approach utilizes ISP devices like
SmartSSDs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib59" title="">59</a>]</cite> to directly replace conventional SSD cards. A
SmartSSD tightly couples a normal SSD with a lightweight FPGA device within
the NVMe U.2 form factor. This allows SmartSSDs to become a drop-in
replacement for normal SSDs while still staying within the NVMe’s <math alttext="25" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn id="S4.SS2.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.p1.1.m1.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">25</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">25</annotation></semantics></math> Watts power envelope<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>A
high-end FPGA card like Xilinx U280 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib67" title="">67</a>]</cite> which has a TDP of 225
Watts cannot be utilized for a U.2 SmartSSD card. </span></span></span>. As such, a SmartSSD
can utilize its local FPGA device to implement our preprocessing accelerator
right next to the local SSD where the raw feature data is stored. Such design
point effectively tackles the system challenges of both co-located and
disaggregated preprocessing accelerators as follows.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="146" id="S4.F8.g1" src="x8.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>System architecture of PreSto.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="246" id="S4.F9.g1" src="x9.png" width="332"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Major components of <math alttext="PreSto" class="ltx_Math" display="inline" id="S4.F9.2.m1.1"><semantics id="S4.F9.2.m1.1b"><mrow id="S4.F9.2.m1.1.1" xref="S4.F9.2.m1.1.1.cmml"><mi id="S4.F9.2.m1.1.1.2" xref="S4.F9.2.m1.1.1.2.cmml">P</mi><mo id="S4.F9.2.m1.1.1.1" xref="S4.F9.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F9.2.m1.1.1.3" xref="S4.F9.2.m1.1.1.3.cmml">r</mi><mo id="S4.F9.2.m1.1.1.1b" xref="S4.F9.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F9.2.m1.1.1.4" xref="S4.F9.2.m1.1.1.4.cmml">e</mi><mo id="S4.F9.2.m1.1.1.1c" xref="S4.F9.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F9.2.m1.1.1.5" xref="S4.F9.2.m1.1.1.5.cmml">S</mi><mo id="S4.F9.2.m1.1.1.1d" xref="S4.F9.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F9.2.m1.1.1.6" xref="S4.F9.2.m1.1.1.6.cmml">t</mi><mo id="S4.F9.2.m1.1.1.1e" xref="S4.F9.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F9.2.m1.1.1.7" xref="S4.F9.2.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F9.2.m1.1c"><apply id="S4.F9.2.m1.1.1.cmml" xref="S4.F9.2.m1.1.1"><times id="S4.F9.2.m1.1.1.1.cmml" xref="S4.F9.2.m1.1.1.1"></times><ci id="S4.F9.2.m1.1.1.2.cmml" xref="S4.F9.2.m1.1.1.2">𝑃</ci><ci id="S4.F9.2.m1.1.1.3.cmml" xref="S4.F9.2.m1.1.1.3">𝑟</ci><ci id="S4.F9.2.m1.1.1.4.cmml" xref="S4.F9.2.m1.1.1.4">𝑒</ci><ci id="S4.F9.2.m1.1.1.5.cmml" xref="S4.F9.2.m1.1.1.5">𝑆</ci><ci id="S4.F9.2.m1.1.1.6.cmml" xref="S4.F9.2.m1.1.1.6">𝑡</ci><ci id="S4.F9.2.m1.1.1.7.cmml" xref="S4.F9.2.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F9.2.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S4.F9.2.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> software system and key steps undertaken during end-to-end RecSys training.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<ol class="ltx_enumerate" id="S4.I1">
<li class="ltx_item" id="S4.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I1.i1.p1">
<p class="ltx_p" id="S4.I1.i1.p1.2"><span class="ltx_text ltx_font_bold" id="S4.I1.i1.p1.2.1">Scalability.</span> As discussed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>, the tabular
raw feature data subject for preprocessing is stored as columnar files. A group
of rows within the tabular data is sharded into partitions and different
partitions are stored as independent columnar files in a distributed storage
system (e.g., <math alttext="2" class="ltx_Math" display="inline" id="S4.I1.i1.p1.1.m1.1"><semantics id="S4.I1.i1.p1.1.m1.1a"><mn id="S4.I1.i1.p1.1.m1.1.1" xref="S4.I1.i1.p1.1.m1.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.1.m1.1b"><cn id="S4.I1.i1.p1.1.m1.1.1.cmml" type="integer" xref="S4.I1.i1.p1.1.m1.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.1.m1.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.1.m1.1d">2</annotation></semantics></math> columnar files stored in two SSDs in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S2.F1" title="Figure 1 ‣ II-A End-to-End RecSys Training Pipeline ‣ II Background ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">1</span></a>).
While multiple blocks that constitute a single columnar file can further be distributed across
multiple storage devices, state-of-the-art file systems for RecSys (e.g., Meta’s
Tectonic file system<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib55" title="">55</a>]</cite>) store all the blocks in a given partition
contiguously within a single storage device. This ensures that all preprocessing operations can be conducted locally within a SmartSSD because all transformation
operations conducted within a mini-batch (i.e., partition) are executed locally without any dependencies to other mini-batches (i.e., partitions in other columnar files).
Consequently, in our proposed system, the overall
preprocessing throughput can scale proportionally with the number of SmartSSDs
allocated to a preprocessing worker targeting a given training job.
A training job
with a target preprocessing throughput in mind is first
allocated with the appropriate number of SmartSSDs (detailed
later in this subsection), one that is large enough to
sustain the training stage’s preprocessing need. We then have
each SmartSSD independently extract raw feature data from its local SSD, which
is immediately P2P transferred over to the local FPGA device for on-the-fly
preprocessing. Because multiple SmartSSDs (i.e., multiple preprocessing
workers) concurrently conduct preprocessing and generate mini-batch inputs,
our <math alttext="PreSto" class="ltx_Math" display="inline" id="S4.I1.i1.p1.2.m2.1"><semantics id="S4.I1.i1.p1.2.m2.1a"><mrow id="S4.I1.i1.p1.2.m2.1.1" xref="S4.I1.i1.p1.2.m2.1.1.cmml"><mi id="S4.I1.i1.p1.2.m2.1.1.2" xref="S4.I1.i1.p1.2.m2.1.1.2.cmml">P</mi><mo id="S4.I1.i1.p1.2.m2.1.1.1" xref="S4.I1.i1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.I1.i1.p1.2.m2.1.1.3" xref="S4.I1.i1.p1.2.m2.1.1.3.cmml">r</mi><mo id="S4.I1.i1.p1.2.m2.1.1.1a" xref="S4.I1.i1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.I1.i1.p1.2.m2.1.1.4" xref="S4.I1.i1.p1.2.m2.1.1.4.cmml">e</mi><mo id="S4.I1.i1.p1.2.m2.1.1.1b" xref="S4.I1.i1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.I1.i1.p1.2.m2.1.1.5" xref="S4.I1.i1.p1.2.m2.1.1.5.cmml">S</mi><mo id="S4.I1.i1.p1.2.m2.1.1.1c" xref="S4.I1.i1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.I1.i1.p1.2.m2.1.1.6" xref="S4.I1.i1.p1.2.m2.1.1.6.cmml">t</mi><mo id="S4.I1.i1.p1.2.m2.1.1.1d" xref="S4.I1.i1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.I1.i1.p1.2.m2.1.1.7" xref="S4.I1.i1.p1.2.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i1.p1.2.m2.1b"><apply id="S4.I1.i1.p1.2.m2.1.1.cmml" xref="S4.I1.i1.p1.2.m2.1.1"><times id="S4.I1.i1.p1.2.m2.1.1.1.cmml" xref="S4.I1.i1.p1.2.m2.1.1.1"></times><ci id="S4.I1.i1.p1.2.m2.1.1.2.cmml" xref="S4.I1.i1.p1.2.m2.1.1.2">𝑃</ci><ci id="S4.I1.i1.p1.2.m2.1.1.3.cmml" xref="S4.I1.i1.p1.2.m2.1.1.3">𝑟</ci><ci id="S4.I1.i1.p1.2.m2.1.1.4.cmml" xref="S4.I1.i1.p1.2.m2.1.1.4">𝑒</ci><ci id="S4.I1.i1.p1.2.m2.1.1.5.cmml" xref="S4.I1.i1.p1.2.m2.1.1.5">𝑆</ci><ci id="S4.I1.i1.p1.2.m2.1.1.6.cmml" xref="S4.I1.i1.p1.2.m2.1.1.6">𝑡</ci><ci id="S4.I1.i1.p1.2.m2.1.1.7.cmml" xref="S4.I1.i1.p1.2.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i1.p1.2.m2.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i1.p1.2.m2.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> ISP units provide highly scalable preprocessing service.</p>
</div>
</li>
<li class="ltx_item" id="S4.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I1.i2.p1">
<p class="ltx_p" id="S4.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.I1.i2.p1.1.1">Efficiency.</span> In our proposed system, all preprocessing
operations are conducted <em class="ltx_emph ltx_font_italic" id="S4.I1.i2.p1.1.2">locally</em> within the storage system. This is
because the SmartSSD’s FPGA accelerator can extract the raw feature data
directly from its local SSD using P2P data transfers, obviating the need
for a disaggregated accelerator server pool with high maintenance cost.
Such design decision also helps eliminate the communication overhead
associated with disaggregated accelerator server designs
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.F7" title="Figure 7 ‣ IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">7</span></a>(b)), which requires the raw feature data to be
transferred from the storage system to the remote accelerator pool. It is
also worth pointing out that SmartSSDs can seamlessly be deployed within
the power constraints of the baseline distributed storage system, all
thanks to the use of commodity devices that operate within the NVMe SSD’s
power budget (less than 25 watts per device <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib6" title="">6</a>]</cite>).
Consequently, <math alttext="PreSto" class="ltx_Math" display="inline" id="S4.I1.i2.p1.1.m1.1"><semantics id="S4.I1.i2.p1.1.m1.1a"><mrow id="S4.I1.i2.p1.1.m1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.cmml"><mi id="S4.I1.i2.p1.1.m1.1.1.2" xref="S4.I1.i2.p1.1.m1.1.1.2.cmml">P</mi><mo id="S4.I1.i2.p1.1.m1.1.1.1" xref="S4.I1.i2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.I1.i2.p1.1.m1.1.1.3" xref="S4.I1.i2.p1.1.m1.1.1.3.cmml">r</mi><mo id="S4.I1.i2.p1.1.m1.1.1.1a" xref="S4.I1.i2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.I1.i2.p1.1.m1.1.1.4" xref="S4.I1.i2.p1.1.m1.1.1.4.cmml">e</mi><mo id="S4.I1.i2.p1.1.m1.1.1.1b" xref="S4.I1.i2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.I1.i2.p1.1.m1.1.1.5" xref="S4.I1.i2.p1.1.m1.1.1.5.cmml">S</mi><mo id="S4.I1.i2.p1.1.m1.1.1.1c" xref="S4.I1.i2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.I1.i2.p1.1.m1.1.1.6" xref="S4.I1.i2.p1.1.m1.1.1.6.cmml">t</mi><mo id="S4.I1.i2.p1.1.m1.1.1.1d" xref="S4.I1.i2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.I1.i2.p1.1.m1.1.1.7" xref="S4.I1.i2.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.I1.i2.p1.1.m1.1b"><apply id="S4.I1.i2.p1.1.m1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1"><times id="S4.I1.i2.p1.1.m1.1.1.1.cmml" xref="S4.I1.i2.p1.1.m1.1.1.1"></times><ci id="S4.I1.i2.p1.1.m1.1.1.2.cmml" xref="S4.I1.i2.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.I1.i2.p1.1.m1.1.1.3.cmml" xref="S4.I1.i2.p1.1.m1.1.1.3">𝑟</ci><ci id="S4.I1.i2.p1.1.m1.1.1.4.cmml" xref="S4.I1.i2.p1.1.m1.1.1.4">𝑒</ci><ci id="S4.I1.i2.p1.1.m1.1.1.5.cmml" xref="S4.I1.i2.p1.1.m1.1.1.5">𝑆</ci><ci id="S4.I1.i2.p1.1.m1.1.1.6.cmml" xref="S4.I1.i2.p1.1.m1.1.1.6">𝑡</ci><ci id="S4.I1.i2.p1.1.m1.1.1.7.cmml" xref="S4.I1.i2.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.I1.i2.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S4.I1.i2.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> is minimally intrusive to existing hardware
infrastructure while maintaining power-efficiency via accelerated
computing.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Software architecture.</span> Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.F9" title="Figure 9 ‣ IV-B Proposed Approach: In-Storage Data Preprocessing ‣ IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">9</span></a> provides a high-level
overview of our software architecture. The two key components of our software
system are the train manager and the preprocess manager. The train manager
is implemented as part of the training worker process whose main role is to manage the
end-to-end model training job, from data preprocessing to model training.
That is, it requests the mini-batch inputs (i.e., train-ready tensors) from the
storage system and once the mini-batch inputs are returned, they are forwarded to the GPU for
model training. The preprocess manager is in charge of spawning and managing the
actual preprocessing workers using the SmartSSD devices. Once the
mini-batch inputs are ready, the preprocess manager returns them back
to the train manager. Below we summarize the major steps undertaken during
the end-to-end RecSys training process.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<ol class="ltx_enumerate" id="S4.I2">
<li class="ltx_item" id="S4.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S4.I2.i1.p1">
<p class="ltx_p" id="S4.I2.i1.p1.1">When a training job is launched by TorchRec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib24" title="">24</a>]</cite>, the train
manager receives important information about the target training job (e.g.,
model configuration for training/preprocessing, mini-batch size, and other
metadata) and goes through several boot-strapping procedures in preparation for
model training. These include the
allocation of an input queue to store the mini-batch inputs designated for model
training and a Remote Procedure Call (RPC) initialization (step
❶ in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.F9" title="Figure 9 ‣ IV-B Proposed Approach: In-Storage Data Preprocessing ‣ IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">9</span></a>).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S4.I2.i2.p1">
<p class="ltx_p" id="S4.I2.i2.p1.4">Using the training job information, the train manager measures the
maximum training throughput achievable with GPUs for the given training job.
This is done by supplying the GPUs with dummy mini-batch inputs and
stress-testing their highest sustainable throughput. This process only takes
tens of seconds, the overhead of which is amortized over the several hours/days worth of training
time. The train manager then initializes the preprocess manager, sending
information relevant to the preprocessing jobs (e.g., configuration
parameters of preprocessing). One of the important information that is
forwarded to the preprocess manager is GPU’s maximum training throughput (<math alttext="T" class="ltx_Math" display="inline" id="S4.I2.i2.p1.1.m1.1"><semantics id="S4.I2.i2.p1.1.m1.1a"><mi id="S4.I2.i2.p1.1.m1.1.1" xref="S4.I2.i2.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.1.m1.1b"><ci id="S4.I2.i2.p1.1.m1.1.1.cmml" xref="S4.I2.i2.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.1.m1.1d">italic_T</annotation></semantics></math>),
as it determines the level of preprocessing throughput that must be
sustained in order to fully utilize the GPUs for model training. As
such, the preprocess manager measures offline the maximum preprocessing
throughput delivered with a single SmartSSD device under the given
preprocessing configuration (<math alttext="P" class="ltx_Math" display="inline" id="S4.I2.i2.p1.2.m2.1"><semantics id="S4.I2.i2.p1.2.m2.1a"><mi id="S4.I2.i2.p1.2.m2.1.1" xref="S4.I2.i2.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.2.m2.1b"><ci id="S4.I2.i2.p1.2.m2.1.1.cmml" xref="S4.I2.i2.p1.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.2.m2.1d">italic_P</annotation></semantics></math>). By dividing the maximum training
throughput with the per-SmartSSD preprocessing throughput (<math alttext="T" class="ltx_Math" display="inline" id="S4.I2.i2.p1.3.m3.1"><semantics id="S4.I2.i2.p1.3.m3.1a"><mi id="S4.I2.i2.p1.3.m3.1.1" xref="S4.I2.i2.p1.3.m3.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.3.m3.1b"><ci id="S4.I2.i2.p1.3.m3.1.1.cmml" xref="S4.I2.i2.p1.3.m3.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.3.m3.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.3.m3.1d">italic_T</annotation></semantics></math>/<math alttext="P" class="ltx_Math" display="inline" id="S4.I2.i2.p1.4.m4.1"><semantics id="S4.I2.i2.p1.4.m4.1a"><mi id="S4.I2.i2.p1.4.m4.1.1" xref="S4.I2.i2.p1.4.m4.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i2.p1.4.m4.1b"><ci id="S4.I2.i2.p1.4.m4.1.1.cmml" xref="S4.I2.i2.p1.4.m4.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i2.p1.4.m4.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i2.p1.4.m4.1d">italic_P</annotation></semantics></math>),
the preprocess manager derives the number of SmartSSD devices
that need to be allocated to fully saturate the GPUs (step ❷).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S4.I2.i3.p1">
<p class="ltx_p" id="S4.I2.i3.p1.2">The preprocess manager launches the necessary number of preprocessing
workers based on the derived number of SmartSSD (<math alttext="T" class="ltx_Math" display="inline" id="S4.I2.i3.p1.1.m1.1"><semantics id="S4.I2.i3.p1.1.m1.1a"><mi id="S4.I2.i3.p1.1.m1.1.1" xref="S4.I2.i3.p1.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.1.m1.1b"><ci id="S4.I2.i3.p1.1.m1.1.1.cmml" xref="S4.I2.i3.p1.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.1.m1.1c">T</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.1.m1.1d">italic_T</annotation></semantics></math>/<math alttext="P" class="ltx_Math" display="inline" id="S4.I2.i3.p1.2.m2.1"><semantics id="S4.I2.i3.p1.2.m2.1a"><mi id="S4.I2.i3.p1.2.m2.1.1" xref="S4.I2.i3.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S4.I2.i3.p1.2.m2.1b"><ci id="S4.I2.i3.p1.2.m2.1.1.cmml" xref="S4.I2.i3.p1.2.m2.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.I2.i3.p1.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S4.I2.i3.p1.2.m2.1d">italic_P</annotation></semantics></math>) (step ❸). As each
preprocessing worker independently generates mini-batch inputs locally within the
SmartSSD, each device fetches its share of raw feature data from the local SSD
and transfers them P2P to the FPGA to conduct preprocessing on-the-fly
(step ❹).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S4.I2.i4.p1">
<p class="ltx_p" id="S4.I2.i4.p1.1">Once the mini-batch inputs are ready, they are converted into a train-ready tensor
format, as required by TorchRec, and copied over to the input queue within the
train manager (step ❺).</p>
</div>
</li>
<li class="ltx_item" id="S4.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S4.I2.i5.p1">
<p class="ltx_p" id="S4.I2.i5.p1.1">Finally, the train manager transfers the mini-batch from its input queue to
the GPU and kicks off the model training process (step ❻ and ❼).
Because multiple SmartSSDs are concurrently preprocessing raw features and replenishing
the train manager’s input queue, it ensures that the GPU is constantly supplied with
sufficient amount of mini-batch inputs to consume.</p>
</div>
</li>
</ol>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="S4.F10.g1" src="x10.png" width="340"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span><math alttext="PreSto" class="ltx_Math" display="inline" id="S4.F10.2.m1.1"><semantics id="S4.F10.2.m1.1b"><mrow id="S4.F10.2.m1.1.1" xref="S4.F10.2.m1.1.1.cmml"><mi id="S4.F10.2.m1.1.1.2" xref="S4.F10.2.m1.1.1.2.cmml">P</mi><mo id="S4.F10.2.m1.1.1.1" xref="S4.F10.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F10.2.m1.1.1.3" xref="S4.F10.2.m1.1.1.3.cmml">r</mi><mo id="S4.F10.2.m1.1.1.1b" xref="S4.F10.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F10.2.m1.1.1.4" xref="S4.F10.2.m1.1.1.4.cmml">e</mi><mo id="S4.F10.2.m1.1.1.1c" xref="S4.F10.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F10.2.m1.1.1.5" xref="S4.F10.2.m1.1.1.5.cmml">S</mi><mo id="S4.F10.2.m1.1.1.1d" xref="S4.F10.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F10.2.m1.1.1.6" xref="S4.F10.2.m1.1.1.6.cmml">t</mi><mo id="S4.F10.2.m1.1.1.1e" xref="S4.F10.2.m1.1.1.1.cmml">⁢</mo><mi id="S4.F10.2.m1.1.1.7" xref="S4.F10.2.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.F10.2.m1.1c"><apply id="S4.F10.2.m1.1.1.cmml" xref="S4.F10.2.m1.1.1"><times id="S4.F10.2.m1.1.1.1.cmml" xref="S4.F10.2.m1.1.1.1"></times><ci id="S4.F10.2.m1.1.1.2.cmml" xref="S4.F10.2.m1.1.1.2">𝑃</ci><ci id="S4.F10.2.m1.1.1.3.cmml" xref="S4.F10.2.m1.1.1.3">𝑟</ci><ci id="S4.F10.2.m1.1.1.4.cmml" xref="S4.F10.2.m1.1.1.4">𝑒</ci><ci id="S4.F10.2.m1.1.1.5.cmml" xref="S4.F10.2.m1.1.1.5">𝑆</ci><ci id="S4.F10.2.m1.1.1.6.cmml" xref="S4.F10.2.m1.1.1.6">𝑡</ci><ci id="S4.F10.2.m1.1.1.7.cmml" xref="S4.F10.2.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F10.2.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S4.F10.2.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> accelerator microarchitecture.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Data Preprocessing Accelerator Microarchitecture</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The design objective of our <math alttext="PreSto" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">P</mi><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">r</mi><mo id="S4.SS3.p1.1.m1.1.1.1a" xref="S4.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p1.1.m1.1.1.4" xref="S4.SS3.p1.1.m1.1.1.4.cmml">e</mi><mo id="S4.SS3.p1.1.m1.1.1.1b" xref="S4.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p1.1.m1.1.1.5" xref="S4.SS3.p1.1.m1.1.1.5.cmml">S</mi><mo id="S4.SS3.p1.1.m1.1.1.1c" xref="S4.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p1.1.m1.1.1.6" xref="S4.SS3.p1.1.m1.1.1.6.cmml">t</mi><mo id="S4.SS3.p1.1.m1.1.1.1d" xref="S4.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS3.p1.1.m1.1.1.7" xref="S4.SS3.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><ci id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">𝑃</ci><ci id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">𝑟</ci><ci id="S4.SS3.p1.1.m1.1.1.4.cmml" xref="S4.SS3.p1.1.m1.1.1.4">𝑒</ci><ci id="S4.SS3.p1.1.m1.1.1.5.cmml" xref="S4.SS3.p1.1.m1.1.1.5">𝑆</ci><ci id="S4.SS3.p1.1.m1.1.1.6.cmml" xref="S4.SS3.p1.1.m1.1.1.6">𝑡</ci><ci id="S4.SS3.p1.1.m1.1.1.7.cmml" xref="S4.SS3.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> accelerator is to maximally
exploit inter/intra-feature parallelism inherent in RecSys data preprocessing.
As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.F10" title="Figure 10 ‣ IV-B Proposed Approach: In-Storage Data Preprocessing ‣ IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">10</span></a>, we use the custom logic
within the FPGA to implement a hardwired decoder unit,
feature generation units, and feature normalization units. Each unit is equipped with the essential hardware logic
tailored to the following operations: “Decoder” for columnar file decoding
(our columnar files assume the Apache Parquet file format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib3" title="">3</a>]</cite>), “Bucketize” for feature generation, and
“SigridHash” as well as “Log” for feature normalization.
To maximally
exploit inter-/intra-feature parallelism, we employ the following
design optimizations. First, to exploit inter-feature parallelism, we deploy
multiple processing elements dedicated to each individual feature, directly
connected to the off-chip-memory interface to fully utilize the bandwidth of
global memory (DRAM). Second, to exploit intra-feature parallelism, each
processing element employs double-buffering to overlap the next feature value’s data
fetch operation with the current feature value’s generation and normalization operations. That
is, once a portion of an input feature is fetched on-chip, its
transformation is immediately executed while the next feature value is
concurrently being fetched from the off-chip memory.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Methodology</span>
</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Benchmarks</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The majority of current academic research on RecSys utilizes the Criteo
dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib10" title="">10</a>]</cite>, which is the largest publicly available dataset
for RecSys. The Criteo dataset consists of 13 dense and 26 sparse features,
with a fixed feature length of 1 for each sparse feature. According to
recent work from Meta <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite>, production-level RecSys models can amount
to 504 dense and 42 sparse features with an average sparse feature length of 20,
much larger than the public Criteo dataset. To narrow this gap in our evaluation,
we additionally construct four
synthetic RecSys models in accordance with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite>, expanding the existing
features of the Criteo dataset to better cover the evaluation space of
production-level RecSys models with large number of dense/sparse features.
Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.T1" title="TABLE I ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">I</span></a> details the configuration of the five RecSys models and their training
datasets we evaluate.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Experimental Setup</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.4"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.4.1">Hardware.</span> Exploring <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">P</mi><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">r</mi><mo id="S5.SS2.p1.1.m1.1.1.1a" xref="S5.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.1.m1.1.1.4" xref="S5.SS2.p1.1.m1.1.1.4.cmml">e</mi><mo id="S5.SS2.p1.1.m1.1.1.1b" xref="S5.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.1.m1.1.1.5" xref="S5.SS2.p1.1.m1.1.1.5.cmml">S</mi><mo id="S5.SS2.p1.1.m1.1.1.1c" xref="S5.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.1.m1.1.1.6" xref="S5.SS2.p1.1.m1.1.1.6.cmml">t</mi><mo id="S5.SS2.p1.1.m1.1.1.1d" xref="S5.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.1.m1.1.1.7" xref="S5.SS2.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><times id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></times><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑃</ci><ci id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">𝑟</ci><ci id="S5.SS2.p1.1.m1.1.1.4.cmml" xref="S5.SS2.p1.1.m1.1.1.4">𝑒</ci><ci id="S5.SS2.p1.1.m1.1.1.5.cmml" xref="S5.SS2.p1.1.m1.1.1.5">𝑆</ci><ci id="S5.SS2.p1.1.m1.1.1.6.cmml" xref="S5.SS2.p1.1.m1.1.1.6">𝑡</ci><ci id="S5.SS2.p1.1.m1.1.1.7.cmml" xref="S5.SS2.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> in a production-level training
pipeline that accurately reflects industry’s large-scale disaggregated CPU
servers, multi-node/multi-GPU systems, and a distributed storage array
integrated with SmartSSDs is challenging at the academic research level for
several reasons. Aside from many undisclosed details of hyperscaler’s
production ML infrastructure, the unavailability of SmartSSDs in cloud services
like Amazon AWS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib2" title="">2</a>]</cite> rendered our experiments to employ the following
methodology. We first demonstrate <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">P</mi><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">r</mi><mo id="S5.SS2.p1.2.m2.1.1.1a" xref="S5.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.2.m2.1.1.4" xref="S5.SS2.p1.2.m2.1.1.4.cmml">e</mi><mo id="S5.SS2.p1.2.m2.1.1.1b" xref="S5.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.2.m2.1.1.5" xref="S5.SS2.p1.2.m2.1.1.5.cmml">S</mi><mo id="S5.SS2.p1.2.m2.1.1.1c" xref="S5.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.2.m2.1.1.6" xref="S5.SS2.p1.2.m2.1.1.6.cmml">t</mi><mo id="S5.SS2.p1.2.m2.1.1.1d" xref="S5.SS2.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.2.m2.1.1.7" xref="S5.SS2.p1.2.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><times id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></times><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">𝑃</ci><ci id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">𝑟</ci><ci id="S5.SS2.p1.2.m2.1.1.4.cmml" xref="S5.SS2.p1.2.m2.1.1.4">𝑒</ci><ci id="S5.SS2.p1.2.m2.1.1.5.cmml" xref="S5.SS2.p1.2.m2.1.1.5">𝑆</ci><ci id="S5.SS2.p1.2.m2.1.1.6.cmml" xref="S5.SS2.p1.2.m2.1.1.6">𝑡</ci><ci id="S5.SS2.p1.2.m2.1.1.7.cmml" xref="S5.SS2.p1.2.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s advantages in real systems by
constructing a small-scale, proof-of-concept (PoC) prototype of <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">P</mi><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml">r</mi><mo id="S5.SS2.p1.3.m3.1.1.1a" xref="S5.SS2.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.3.m3.1.1.4" xref="S5.SS2.p1.3.m3.1.1.4.cmml">e</mi><mo id="S5.SS2.p1.3.m3.1.1.1b" xref="S5.SS2.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.3.m3.1.1.5" xref="S5.SS2.p1.3.m3.1.1.5.cmml">S</mi><mo id="S5.SS2.p1.3.m3.1.1.1c" xref="S5.SS2.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.3.m3.1.1.6" xref="S5.SS2.p1.3.m3.1.1.6.cmml">t</mi><mo id="S5.SS2.p1.3.m3.1.1.1d" xref="S5.SS2.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.3.m3.1.1.7" xref="S5.SS2.p1.3.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><times id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1"></times><ci id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">𝑃</ci><ci id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">𝑟</ci><ci id="S5.SS2.p1.3.m3.1.1.4.cmml" xref="S5.SS2.p1.3.m3.1.1.4">𝑒</ci><ci id="S5.SS2.p1.3.m3.1.1.5.cmml" xref="S5.SS2.p1.3.m3.1.1.5">𝑆</ci><ci id="S5.SS2.p1.3.m3.1.1.6.cmml" xref="S5.SS2.p1.3.m3.1.1.6">𝑡</ci><ci id="S5.SS2.p1.3.m3.1.1.7.cmml" xref="S5.SS2.p1.3.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> using
commodity CPU/GPU/SmartSSD devices. We then develop an analytical
model that estimates <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p1.4.m4.1"><semantics id="S5.SS2.p1.4.m4.1a"><mrow id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mi id="S5.SS2.p1.4.m4.1.1.2" xref="S5.SS2.p1.4.m4.1.1.2.cmml">P</mi><mo id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.4.m4.1.1.3" xref="S5.SS2.p1.4.m4.1.1.3.cmml">r</mi><mo id="S5.SS2.p1.4.m4.1.1.1a" xref="S5.SS2.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.4.m4.1.1.4" xref="S5.SS2.p1.4.m4.1.1.4.cmml">e</mi><mo id="S5.SS2.p1.4.m4.1.1.1b" xref="S5.SS2.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.4.m4.1.1.5" xref="S5.SS2.p1.4.m4.1.1.5.cmml">S</mi><mo id="S5.SS2.p1.4.m4.1.1.1c" xref="S5.SS2.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.4.m4.1.1.6" xref="S5.SS2.p1.4.m4.1.1.6.cmml">t</mi><mo id="S5.SS2.p1.4.m4.1.1.1d" xref="S5.SS2.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p1.4.m4.1.1.7" xref="S5.SS2.p1.4.m4.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><times id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1"></times><ci id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2">𝑃</ci><ci id="S5.SS2.p1.4.m4.1.1.3.cmml" xref="S5.SS2.p1.4.m4.1.1.3">𝑟</ci><ci id="S5.SS2.p1.4.m4.1.1.4.cmml" xref="S5.SS2.p1.4.m4.1.1.4">𝑒</ci><ci id="S5.SS2.p1.4.m4.1.1.5.cmml" xref="S5.SS2.p1.4.m4.1.1.5">𝑆</ci><ci id="S5.SS2.p1.4.m4.1.1.6.cmml" xref="S5.SS2.p1.4.m4.1.1.6">𝑡</ci><ci id="S5.SS2.p1.4.m4.1.1.7.cmml" xref="S5.SS2.p1.4.m4.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.4.m4.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s performance at large-scale by utilizing real
measurements from our PoC prototype.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.3">
<tr class="ltx_tr" id="S5.T2.3.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.1" style="padding-left:3.5pt;padding-right:3.5pt;">Unit</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.2" style="padding-left:3.5pt;padding-right:3.5pt;">LUT</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.3" style="padding-left:3.5pt;padding-right:3.5pt;">REG</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.4" style="padding-left:3.5pt;padding-right:3.5pt;">BRAM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.5" style="padding-left:3.5pt;padding-right:3.5pt;">URAM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.3.1.6" style="padding-left:3.5pt;padding-right:3.5pt;">DSP</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.1" style="padding-left:3.5pt;padding-right:3.5pt;">Decode</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.2" style="padding-left:3.5pt;padding-right:3.5pt;">18.84%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.3" style="padding-left:3.5pt;padding-right:3.5pt;">8.49%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.4" style="padding-left:3.5pt;padding-right:3.5pt;">25.08%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.5" style="padding-left:3.5pt;padding-right:3.5pt;">0.00%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.2.6" style="padding-left:3.5pt;padding-right:3.5pt;">0.00%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.3.1" style="padding-left:3.5pt;padding-right:3.5pt;">Bucketize</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.3.2" style="padding-left:3.5pt;padding-right:3.5pt;">7.88%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.3.3" style="padding-left:3.5pt;padding-right:3.5pt;">4.28%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.3.4" style="padding-left:3.5pt;padding-right:3.5pt;">6.19%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.3.5" style="padding-left:3.5pt;padding-right:3.5pt;">27.59%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.3.6" style="padding-left:3.5pt;padding-right:3.5pt;">0.00%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.4">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.4.1" style="padding-left:3.5pt;padding-right:3.5pt;">SigridHash</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.4.2" style="padding-left:3.5pt;padding-right:3.5pt;">23.11%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.4.3" style="padding-left:3.5pt;padding-right:3.5pt;">12.47%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.4.4" style="padding-left:3.5pt;padding-right:3.5pt;">11.89%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.4.5" style="padding-left:3.5pt;padding-right:3.5pt;">0.00%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.4.6" style="padding-left:3.5pt;padding-right:3.5pt;">19.19%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.5">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.5.1" style="padding-left:3.5pt;padding-right:3.5pt;">Log</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.5.2" style="padding-left:3.5pt;padding-right:3.5pt;">4.18%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.5.3" style="padding-left:3.5pt;padding-right:3.5pt;">2.79%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.5.4" style="padding-left:3.5pt;padding-right:3.5pt;">4.89%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.5.5" style="padding-left:3.5pt;padding-right:3.5pt;">0.00%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.5.6" style="padding-left:3.5pt;padding-right:3.5pt;">10.62%</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.6">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.3.6.1" style="padding-left:3.5pt;padding-right:3.5pt;">Total</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.3.6.2" style="padding-left:3.5pt;padding-right:3.5pt;">54.02%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.3.6.3" style="padding-left:3.5pt;padding-right:3.5pt;">28.03%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.3.6.4" style="padding-left:3.5pt;padding-right:3.5pt;">48.05%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.3.6.5" style="padding-left:3.5pt;padding-right:3.5pt;">27.59%</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T2.3.6.6" style="padding-left:3.5pt;padding-right:3.5pt;">29.81%</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>FPGA resource utilization of <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.T2.2.m1.1"><semantics id="S5.T2.2.m1.1b"><mrow id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml"><mi id="S5.T2.2.m1.1.1.2" xref="S5.T2.2.m1.1.1.2.cmml">P</mi><mo id="S5.T2.2.m1.1.1.1" xref="S5.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S5.T2.2.m1.1.1.3" xref="S5.T2.2.m1.1.1.3.cmml">r</mi><mo id="S5.T2.2.m1.1.1.1b" xref="S5.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S5.T2.2.m1.1.1.4" xref="S5.T2.2.m1.1.1.4.cmml">e</mi><mo id="S5.T2.2.m1.1.1.1c" xref="S5.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S5.T2.2.m1.1.1.5" xref="S5.T2.2.m1.1.1.5.cmml">S</mi><mo id="S5.T2.2.m1.1.1.1d" xref="S5.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S5.T2.2.m1.1.1.6" xref="S5.T2.2.m1.1.1.6.cmml">t</mi><mo id="S5.T2.2.m1.1.1.1e" xref="S5.T2.2.m1.1.1.1.cmml">⁢</mo><mi id="S5.T2.2.m1.1.1.7" xref="S5.T2.2.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><apply id="S5.T2.2.m1.1.1.cmml" xref="S5.T2.2.m1.1.1"><times id="S5.T2.2.m1.1.1.1.cmml" xref="S5.T2.2.m1.1.1.1"></times><ci id="S5.T2.2.m1.1.1.2.cmml" xref="S5.T2.2.m1.1.1.2">𝑃</ci><ci id="S5.T2.2.m1.1.1.3.cmml" xref="S5.T2.2.m1.1.1.3">𝑟</ci><ci id="S5.T2.2.m1.1.1.4.cmml" xref="S5.T2.2.m1.1.1.4">𝑒</ci><ci id="S5.T2.2.m1.1.1.5.cmml" xref="S5.T2.2.m1.1.1.5">𝑆</ci><ci id="S5.T2.2.m1.1.1.6.cmml" xref="S5.T2.2.m1.1.1.6">𝑡</ci><ci id="S5.T2.2.m1.1.1.7.cmml" xref="S5.T2.2.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s preprocessing accelerator. Decode, Bucketize, SigridHash, and Log units are synthesized with an operating frequency of 223MHz.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.6">At a high-level, our PoC prototype includes three major components:
(1) a storage node (with and without a SmartSSD to model <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">P</mi><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.1.m1.1.1.3" xref="S5.SS2.p2.1.m1.1.1.3.cmml">r</mi><mo id="S5.SS2.p2.1.m1.1.1.1a" xref="S5.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.1.m1.1.1.4" xref="S5.SS2.p2.1.m1.1.1.4.cmml">e</mi><mo id="S5.SS2.p2.1.m1.1.1.1b" xref="S5.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.1.m1.1.1.5" xref="S5.SS2.p2.1.m1.1.1.5.cmml">S</mi><mo id="S5.SS2.p2.1.m1.1.1.1c" xref="S5.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.1.m1.1.1.6" xref="S5.SS2.p2.1.m1.1.1.6.cmml">t</mi><mo id="S5.SS2.p2.1.m1.1.1.1d" xref="S5.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.1.m1.1.1.7" xref="S5.SS2.p2.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><times id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1"></times><ci id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">𝑃</ci><ci id="S5.SS2.p2.1.m1.1.1.3.cmml" xref="S5.SS2.p2.1.m1.1.1.3">𝑟</ci><ci id="S5.SS2.p2.1.m1.1.1.4.cmml" xref="S5.SS2.p2.1.m1.1.1.4">𝑒</ci><ci id="S5.SS2.p2.1.m1.1.1.5.cmml" xref="S5.SS2.p2.1.m1.1.1.5">𝑆</ci><ci id="S5.SS2.p2.1.m1.1.1.6.cmml" xref="S5.SS2.p2.1.m1.1.1.6">𝑡</ci><ci id="S5.SS2.p2.1.m1.1.1.7.cmml" xref="S5.SS2.p2.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (with
SmartSSD) and baseline storage system (without SmartSSD)), (2)
a GPU training node, and (3) a pool of multiple CPU nodes for
preprocessing (to model baseline disaggregated CPU preprocessing
service), which communicate over a network using <math alttext="10" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mn id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><cn id="S5.SS2.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS2.p2.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">10</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">10</annotation></semantics></math> Gbps
Ethernet. Both the storage node and the pool of CPU nodes for
preprocessing are designed using a total of three two-socket Intel
Xeon Gold 6242 CPU nodes (32 CPU cores per node) where one node is
used as the storage node and the other two nodes are utilized as a
remote pool for data preprocessing (maximum 2<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mo id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><times id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">×</annotation></semantics></math>32<math alttext="=" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mo id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><eq id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">=</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">=</annotation></semantics></math>64 CPU
cores available for data preprocessing). The GPU training node
contains AMD EPYC 7502 CPU connected with a single NVIDIA A100 GPU.
When evaluating <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p2.5.m5.1"><semantics id="S5.SS2.p2.5.m5.1a"><mrow id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml"><mi id="S5.SS2.p2.5.m5.1.1.2" xref="S5.SS2.p2.5.m5.1.1.2.cmml">P</mi><mo id="S5.SS2.p2.5.m5.1.1.1" xref="S5.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.5.m5.1.1.3" xref="S5.SS2.p2.5.m5.1.1.3.cmml">r</mi><mo id="S5.SS2.p2.5.m5.1.1.1a" xref="S5.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.5.m5.1.1.4" xref="S5.SS2.p2.5.m5.1.1.4.cmml">e</mi><mo id="S5.SS2.p2.5.m5.1.1.1b" xref="S5.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.5.m5.1.1.5" xref="S5.SS2.p2.5.m5.1.1.5.cmml">S</mi><mo id="S5.SS2.p2.5.m5.1.1.1c" xref="S5.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.5.m5.1.1.6" xref="S5.SS2.p2.5.m5.1.1.6.cmml">t</mi><mo id="S5.SS2.p2.5.m5.1.1.1d" xref="S5.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.5.m5.1.1.7" xref="S5.SS2.p2.5.m5.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><apply id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1"><times id="S5.SS2.p2.5.m5.1.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1.1"></times><ci id="S5.SS2.p2.5.m5.1.1.2.cmml" xref="S5.SS2.p2.5.m5.1.1.2">𝑃</ci><ci id="S5.SS2.p2.5.m5.1.1.3.cmml" xref="S5.SS2.p2.5.m5.1.1.3">𝑟</ci><ci id="S5.SS2.p2.5.m5.1.1.4.cmml" xref="S5.SS2.p2.5.m5.1.1.4">𝑒</ci><ci id="S5.SS2.p2.5.m5.1.1.5.cmml" xref="S5.SS2.p2.5.m5.1.1.5">𝑆</ci><ci id="S5.SS2.p2.5.m5.1.1.6.cmml" xref="S5.SS2.p2.5.m5.1.1.6">𝑡</ci><ci id="S5.SS2.p2.5.m5.1.1.7.cmml" xref="S5.SS2.p2.5.m5.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.5.m5.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>, we add a single SmartSSD card <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib59" title="">59</a>]</cite> to the
storage node which handles data preprocessing locally within the
storage node. <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p2.6.m6.1"><semantics id="S5.SS2.p2.6.m6.1a"><mrow id="S5.SS2.p2.6.m6.1.1" xref="S5.SS2.p2.6.m6.1.1.cmml"><mi id="S5.SS2.p2.6.m6.1.1.2" xref="S5.SS2.p2.6.m6.1.1.2.cmml">P</mi><mo id="S5.SS2.p2.6.m6.1.1.1" xref="S5.SS2.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.6.m6.1.1.3" xref="S5.SS2.p2.6.m6.1.1.3.cmml">r</mi><mo id="S5.SS2.p2.6.m6.1.1.1a" xref="S5.SS2.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.6.m6.1.1.4" xref="S5.SS2.p2.6.m6.1.1.4.cmml">e</mi><mo id="S5.SS2.p2.6.m6.1.1.1b" xref="S5.SS2.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.6.m6.1.1.5" xref="S5.SS2.p2.6.m6.1.1.5.cmml">S</mi><mo id="S5.SS2.p2.6.m6.1.1.1c" xref="S5.SS2.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.6.m6.1.1.6" xref="S5.SS2.p2.6.m6.1.1.6.cmml">t</mi><mo id="S5.SS2.p2.6.m6.1.1.1d" xref="S5.SS2.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p2.6.m6.1.1.7" xref="S5.SS2.p2.6.m6.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.6.m6.1b"><apply id="S5.SS2.p2.6.m6.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1"><times id="S5.SS2.p2.6.m6.1.1.1.cmml" xref="S5.SS2.p2.6.m6.1.1.1"></times><ci id="S5.SS2.p2.6.m6.1.1.2.cmml" xref="S5.SS2.p2.6.m6.1.1.2">𝑃</ci><ci id="S5.SS2.p2.6.m6.1.1.3.cmml" xref="S5.SS2.p2.6.m6.1.1.3">𝑟</ci><ci id="S5.SS2.p2.6.m6.1.1.4.cmml" xref="S5.SS2.p2.6.m6.1.1.4">𝑒</ci><ci id="S5.SS2.p2.6.m6.1.1.5.cmml" xref="S5.SS2.p2.6.m6.1.1.5">𝑆</ci><ci id="S5.SS2.p2.6.m6.1.1.6.cmml" xref="S5.SS2.p2.6.m6.1.1.6">𝑡</ci><ci id="S5.SS2.p2.6.m6.1.1.7.cmml" xref="S5.SS2.p2.6.m6.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.6.m6.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.6.m6.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s preprocessing accelerator is designed using
Xilinx Vitis HLS 2022.2 whose resource utilization is summarized in
Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5.T2" title="TABLE II ‣ V-B Experimental Setup ‣ V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.2">As for the analytical model we developed for
large-scale performance estimations, we utilize the observations
made from our characterization study in
Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3" title="III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">III</span></a> where data preprocessing
operations are embarrassingly parallel and exhibit high
scalability. Because end-to-end training performance as well as its
data preprocessing performance is throughput-bound rather than latency-bound,
our analytical performance model assumes that the preprocessing throughput
measured from our real PoC prototype (i.e., the preprocessing throughput measured
with a single CPU core (baseline) and a single SmartSSD (<math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p3.1.m1.1"><semantics id="S5.SS2.p3.1.m1.1a"><mrow id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml"><mi id="S5.SS2.p3.1.m1.1.1.2" xref="S5.SS2.p3.1.m1.1.1.2.cmml">P</mi><mo id="S5.SS2.p3.1.m1.1.1.1" xref="S5.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.1.m1.1.1.3" xref="S5.SS2.p3.1.m1.1.1.3.cmml">r</mi><mo id="S5.SS2.p3.1.m1.1.1.1a" xref="S5.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.1.m1.1.1.4" xref="S5.SS2.p3.1.m1.1.1.4.cmml">e</mi><mo id="S5.SS2.p3.1.m1.1.1.1b" xref="S5.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.1.m1.1.1.5" xref="S5.SS2.p3.1.m1.1.1.5.cmml">S</mi><mo id="S5.SS2.p3.1.m1.1.1.1c" xref="S5.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.1.m1.1.1.6" xref="S5.SS2.p3.1.m1.1.1.6.cmml">t</mi><mo id="S5.SS2.p3.1.m1.1.1.1d" xref="S5.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.1.m1.1.1.7" xref="S5.SS2.p3.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><apply id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1"><times id="S5.SS2.p3.1.m1.1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1.1"></times><ci id="S5.SS2.p3.1.m1.1.1.2.cmml" xref="S5.SS2.p3.1.m1.1.1.2">𝑃</ci><ci id="S5.SS2.p3.1.m1.1.1.3.cmml" xref="S5.SS2.p3.1.m1.1.1.3">𝑟</ci><ci id="S5.SS2.p3.1.m1.1.1.4.cmml" xref="S5.SS2.p3.1.m1.1.1.4">𝑒</ci><ci id="S5.SS2.p3.1.m1.1.1.5.cmml" xref="S5.SS2.p3.1.m1.1.1.5">𝑆</ci><ci id="S5.SS2.p3.1.m1.1.1.6.cmml" xref="S5.SS2.p3.1.m1.1.1.6">𝑡</ci><ci id="S5.SS2.p3.1.m1.1.1.7.cmml" xref="S5.SS2.p3.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>)) scales proportionally with the number of
CPU cores allocated (baseline) or the number of SmartSSDs allocated (<math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p3.2.m2.1"><semantics id="S5.SS2.p3.2.m2.1a"><mrow id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml"><mi id="S5.SS2.p3.2.m2.1.1.2" xref="S5.SS2.p3.2.m2.1.1.2.cmml">P</mi><mo id="S5.SS2.p3.2.m2.1.1.1" xref="S5.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.2.m2.1.1.3" xref="S5.SS2.p3.2.m2.1.1.3.cmml">r</mi><mo id="S5.SS2.p3.2.m2.1.1.1a" xref="S5.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.2.m2.1.1.4" xref="S5.SS2.p3.2.m2.1.1.4.cmml">e</mi><mo id="S5.SS2.p3.2.m2.1.1.1b" xref="S5.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.2.m2.1.1.5" xref="S5.SS2.p3.2.m2.1.1.5.cmml">S</mi><mo id="S5.SS2.p3.2.m2.1.1.1c" xref="S5.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.2.m2.1.1.6" xref="S5.SS2.p3.2.m2.1.1.6.cmml">t</mi><mo id="S5.SS2.p3.2.m2.1.1.1d" xref="S5.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p3.2.m2.1.1.7" xref="S5.SS2.p3.2.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><apply id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1"><times id="S5.SS2.p3.2.m2.1.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1.1"></times><ci id="S5.SS2.p3.2.m2.1.1.2.cmml" xref="S5.SS2.p3.2.m2.1.1.2">𝑃</ci><ci id="S5.SS2.p3.2.m2.1.1.3.cmml" xref="S5.SS2.p3.2.m2.1.1.3">𝑟</ci><ci id="S5.SS2.p3.2.m2.1.1.4.cmml" xref="S5.SS2.p3.2.m2.1.1.4">𝑒</ci><ci id="S5.SS2.p3.2.m2.1.1.5.cmml" xref="S5.SS2.p3.2.m2.1.1.5">𝑆</ci><ci id="S5.SS2.p3.2.m2.1.1.6.cmml" xref="S5.SS2.p3.2.m2.1.1.6">𝑡</ci><ci id="S5.SS2.p3.2.m2.1.1.7.cmml" xref="S5.SS2.p3.2.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p3.2.m2.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>)
for data preprocessing.</p>
</div>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.2"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.2.1">Software.</span> Our end-to-end RecSys training pipeline is implemented using
TorchArrow (v0.1.0) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib63" title="">63</a>]</cite> for data preprocessing and TorchRec
(v0.3.2) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib24" title="">24</a>]</cite> for model training, assuming a mini-batch size
of 8,192. We assume the Apache Parquet file format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib3" title="">3</a>]</cite> when the columnar
raw feature data is stored in our storage system. Both baseline
CPU-centric and <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p4.1.m1.1"><semantics id="S5.SS2.p4.1.m1.1a"><mrow id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml"><mi id="S5.SS2.p4.1.m1.1.1.2" xref="S5.SS2.p4.1.m1.1.1.2.cmml">P</mi><mo id="S5.SS2.p4.1.m1.1.1.1" xref="S5.SS2.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.1.m1.1.1.3" xref="S5.SS2.p4.1.m1.1.1.3.cmml">r</mi><mo id="S5.SS2.p4.1.m1.1.1.1a" xref="S5.SS2.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.1.m1.1.1.4" xref="S5.SS2.p4.1.m1.1.1.4.cmml">e</mi><mo id="S5.SS2.p4.1.m1.1.1.1b" xref="S5.SS2.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.1.m1.1.1.5" xref="S5.SS2.p4.1.m1.1.1.5.cmml">S</mi><mo id="S5.SS2.p4.1.m1.1.1.1c" xref="S5.SS2.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.1.m1.1.1.6" xref="S5.SS2.p4.1.m1.1.1.6.cmml">t</mi><mo id="S5.SS2.p4.1.m1.1.1.1d" xref="S5.SS2.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.1.m1.1.1.7" xref="S5.SS2.p4.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><apply id="S5.SS2.p4.1.m1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1"><times id="S5.SS2.p4.1.m1.1.1.1.cmml" xref="S5.SS2.p4.1.m1.1.1.1"></times><ci id="S5.SS2.p4.1.m1.1.1.2.cmml" xref="S5.SS2.p4.1.m1.1.1.2">𝑃</ci><ci id="S5.SS2.p4.1.m1.1.1.3.cmml" xref="S5.SS2.p4.1.m1.1.1.3">𝑟</ci><ci id="S5.SS2.p4.1.m1.1.1.4.cmml" xref="S5.SS2.p4.1.m1.1.1.4">𝑒</ci><ci id="S5.SS2.p4.1.m1.1.1.5.cmml" xref="S5.SS2.p4.1.m1.1.1.5">𝑆</ci><ci id="S5.SS2.p4.1.m1.1.1.6.cmml" xref="S5.SS2.p4.1.m1.1.1.6">𝑡</ci><ci id="S5.SS2.p4.1.m1.1.1.7.cmml" xref="S5.SS2.p4.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> preprocessing system communicate with the GPU training
node using
the PyTorch RPC
API <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib11" title="">11</a>]</cite>. We use
Xilinx Runtime library to manage <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS2.p4.2.m2.1"><semantics id="S5.SS2.p4.2.m2.1a"><mrow id="S5.SS2.p4.2.m2.1.1" xref="S5.SS2.p4.2.m2.1.1.cmml"><mi id="S5.SS2.p4.2.m2.1.1.2" xref="S5.SS2.p4.2.m2.1.1.2.cmml">P</mi><mo id="S5.SS2.p4.2.m2.1.1.1" xref="S5.SS2.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.2.m2.1.1.3" xref="S5.SS2.p4.2.m2.1.1.3.cmml">r</mi><mo id="S5.SS2.p4.2.m2.1.1.1a" xref="S5.SS2.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.2.m2.1.1.4" xref="S5.SS2.p4.2.m2.1.1.4.cmml">e</mi><mo id="S5.SS2.p4.2.m2.1.1.1b" xref="S5.SS2.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.2.m2.1.1.5" xref="S5.SS2.p4.2.m2.1.1.5.cmml">S</mi><mo id="S5.SS2.p4.2.m2.1.1.1c" xref="S5.SS2.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.2.m2.1.1.6" xref="S5.SS2.p4.2.m2.1.1.6.cmml">t</mi><mo id="S5.SS2.p4.2.m2.1.1.1d" xref="S5.SS2.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS2.p4.2.m2.1.1.7" xref="S5.SS2.p4.2.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.2.m2.1b"><apply id="S5.SS2.p4.2.m2.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1"><times id="S5.SS2.p4.2.m2.1.1.1.cmml" xref="S5.SS2.p4.2.m2.1.1.1"></times><ci id="S5.SS2.p4.2.m2.1.1.2.cmml" xref="S5.SS2.p4.2.m2.1.1.2">𝑃</ci><ci id="S5.SS2.p4.2.m2.1.1.3.cmml" xref="S5.SS2.p4.2.m2.1.1.3">𝑟</ci><ci id="S5.SS2.p4.2.m2.1.1.4.cmml" xref="S5.SS2.p4.2.m2.1.1.4">𝑒</ci><ci id="S5.SS2.p4.2.m2.1.1.5.cmml" xref="S5.SS2.p4.2.m2.1.1.5">𝑆</ci><ci id="S5.SS2.p4.2.m2.1.1.6.cmml" xref="S5.SS2.p4.2.m2.1.1.6">𝑡</ci><ci id="S5.SS2.p4.2.m2.1.1.7.cmml" xref="S5.SS2.p4.2.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.2.m2.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.2.m2.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s FPGA device.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Evaluation Methods</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">Power measurement.</span> When measuring the power consumption of the CPU-based
storage node as well as the disaggregated preprocessing nodes, we measure its
system-level power consumption using Intel Performance Counter Monitor
(PCM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib23" title="">23</a>]</cite>. The Xilinx Vivado <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib68" title="">68</a>]</cite> and NVIDIA
System Management Interface (nvidia-smi) are used when measuring the power
of <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">P</mi><mo id="S5.SS3.p1.1.m1.1.1.1" xref="S5.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">r</mi><mo id="S5.SS3.p1.1.m1.1.1.1a" xref="S5.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p1.1.m1.1.1.4" xref="S5.SS3.p1.1.m1.1.1.4.cmml">e</mi><mo id="S5.SS3.p1.1.m1.1.1.1b" xref="S5.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p1.1.m1.1.1.5" xref="S5.SS3.p1.1.m1.1.1.5.cmml">S</mi><mo id="S5.SS3.p1.1.m1.1.1.1c" xref="S5.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p1.1.m1.1.1.6" xref="S5.SS3.p1.1.m1.1.1.6.cmml">t</mi><mo id="S5.SS3.p1.1.m1.1.1.1d" xref="S5.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p1.1.m1.1.1.7" xref="S5.SS3.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><times id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></times><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">𝑃</ci><ci id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3">𝑟</ci><ci id="S5.SS3.p1.1.m1.1.1.4.cmml" xref="S5.SS3.p1.1.m1.1.1.4">𝑒</ci><ci id="S5.SS3.p1.1.m1.1.1.5.cmml" xref="S5.SS3.p1.1.m1.1.1.5">𝑆</ci><ci id="S5.SS3.p1.1.m1.1.1.6.cmml" xref="S5.SS3.p1.1.m1.1.1.6">𝑡</ci><ci id="S5.SS3.p1.1.m1.1.1.7.cmml" xref="S5.SS3.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s FPGA accelerator and the GPU, respectively.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Cost-efficiency.</span>
To quantify the cost-effectiveness of <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">P</mi><mo id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">r</mi><mo id="S5.SS3.p2.1.m1.1.1.1a" xref="S5.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p2.1.m1.1.1.4" xref="S5.SS3.p2.1.m1.1.1.4.cmml">e</mi><mo id="S5.SS3.p2.1.m1.1.1.1b" xref="S5.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p2.1.m1.1.1.5" xref="S5.SS3.p2.1.m1.1.1.5.cmml">S</mi><mo id="S5.SS3.p2.1.m1.1.1.1c" xref="S5.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p2.1.m1.1.1.6" xref="S5.SS3.p2.1.m1.1.1.6.cmml">t</mi><mo id="S5.SS3.p2.1.m1.1.1.1d" xref="S5.SS3.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p2.1.m1.1.1.7" xref="S5.SS3.p2.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><times id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></times><ci id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">𝑃</ci><ci id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">𝑟</ci><ci id="S5.SS3.p2.1.m1.1.1.4.cmml" xref="S5.SS3.p2.1.m1.1.1.4">𝑒</ci><ci id="S5.SS3.p2.1.m1.1.1.5.cmml" xref="S5.SS3.p2.1.m1.1.1.5">𝑆</ci><ci id="S5.SS3.p2.1.m1.1.1.6.cmml" xref="S5.SS3.p2.1.m1.1.1.6">𝑡</ci><ci id="S5.SS3.p2.1.m1.1.1.7.cmml" xref="S5.SS3.p2.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>, we also evaluate cost-efficiency using the evaluation metric
suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib43" title="">43</a>]</cite> as summarized below:</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<table class="ltx_equation ltx_eqn_table" id="S5.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Cost-efficiency}=\frac{Throughput\times Duration}{\text{CapEx}+\text{%
OpEx}}" class="ltx_Math" display="block" id="S5.Ex1.m1.1"><semantics id="S5.Ex1.m1.1a"><mrow id="S5.Ex1.m1.1.1" xref="S5.Ex1.m1.1.1.cmml"><mtext id="S5.Ex1.m1.1.1.2" xref="S5.Ex1.m1.1.1.2a.cmml">Cost-efficiency</mtext><mo id="S5.Ex1.m1.1.1.1" xref="S5.Ex1.m1.1.1.1.cmml">=</mo><mfrac id="S5.Ex1.m1.1.1.3" xref="S5.Ex1.m1.1.1.3.cmml"><mrow id="S5.Ex1.m1.1.1.3.2" xref="S5.Ex1.m1.1.1.3.2.cmml"><mrow id="S5.Ex1.m1.1.1.3.2.2" xref="S5.Ex1.m1.1.1.3.2.2.cmml"><mrow id="S5.Ex1.m1.1.1.3.2.2.2" xref="S5.Ex1.m1.1.1.3.2.2.2.cmml"><mi id="S5.Ex1.m1.1.1.3.2.2.2.2" xref="S5.Ex1.m1.1.1.3.2.2.2.2.cmml">T</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.3" xref="S5.Ex1.m1.1.1.3.2.2.2.3.cmml">h</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1a" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.4" xref="S5.Ex1.m1.1.1.3.2.2.2.4.cmml">r</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1b" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.5" xref="S5.Ex1.m1.1.1.3.2.2.2.5.cmml">o</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1c" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.6" xref="S5.Ex1.m1.1.1.3.2.2.2.6.cmml">u</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1d" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.7" xref="S5.Ex1.m1.1.1.3.2.2.2.7.cmml">g</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1e" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.8" xref="S5.Ex1.m1.1.1.3.2.2.2.8.cmml">h</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1f" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.9" xref="S5.Ex1.m1.1.1.3.2.2.2.9.cmml">p</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1g" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.10" xref="S5.Ex1.m1.1.1.3.2.2.2.10.cmml">u</mi><mo id="S5.Ex1.m1.1.1.3.2.2.2.1h" xref="S5.Ex1.m1.1.1.3.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.2.2.11" xref="S5.Ex1.m1.1.1.3.2.2.2.11.cmml">t</mi></mrow><mo id="S5.Ex1.m1.1.1.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="S5.Ex1.m1.1.1.3.2.2.1.cmml">×</mo><mi id="S5.Ex1.m1.1.1.3.2.2.3" xref="S5.Ex1.m1.1.1.3.2.2.3.cmml">D</mi></mrow><mo id="S5.Ex1.m1.1.1.3.2.1" xref="S5.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.3" xref="S5.Ex1.m1.1.1.3.2.3.cmml">u</mi><mo id="S5.Ex1.m1.1.1.3.2.1a" xref="S5.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.4" xref="S5.Ex1.m1.1.1.3.2.4.cmml">r</mi><mo id="S5.Ex1.m1.1.1.3.2.1b" xref="S5.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.5" xref="S5.Ex1.m1.1.1.3.2.5.cmml">a</mi><mo id="S5.Ex1.m1.1.1.3.2.1c" xref="S5.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.6" xref="S5.Ex1.m1.1.1.3.2.6.cmml">t</mi><mo id="S5.Ex1.m1.1.1.3.2.1d" xref="S5.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.7" xref="S5.Ex1.m1.1.1.3.2.7.cmml">i</mi><mo id="S5.Ex1.m1.1.1.3.2.1e" xref="S5.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.8" xref="S5.Ex1.m1.1.1.3.2.8.cmml">o</mi><mo id="S5.Ex1.m1.1.1.3.2.1f" xref="S5.Ex1.m1.1.1.3.2.1.cmml">⁢</mo><mi id="S5.Ex1.m1.1.1.3.2.9" xref="S5.Ex1.m1.1.1.3.2.9.cmml">n</mi></mrow><mrow id="S5.Ex1.m1.1.1.3.3" xref="S5.Ex1.m1.1.1.3.3.cmml"><mtext id="S5.Ex1.m1.1.1.3.3.2" xref="S5.Ex1.m1.1.1.3.3.2a.cmml">CapEx</mtext><mo id="S5.Ex1.m1.1.1.3.3.1" xref="S5.Ex1.m1.1.1.3.3.1.cmml">+</mo><mtext id="S5.Ex1.m1.1.1.3.3.3" xref="S5.Ex1.m1.1.1.3.3.3a.cmml">OpEx</mtext></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex1.m1.1b"><apply id="S5.Ex1.m1.1.1.cmml" xref="S5.Ex1.m1.1.1"><eq id="S5.Ex1.m1.1.1.1.cmml" xref="S5.Ex1.m1.1.1.1"></eq><ci id="S5.Ex1.m1.1.1.2a.cmml" xref="S5.Ex1.m1.1.1.2"><mtext id="S5.Ex1.m1.1.1.2.cmml" xref="S5.Ex1.m1.1.1.2">Cost-efficiency</mtext></ci><apply id="S5.Ex1.m1.1.1.3.cmml" xref="S5.Ex1.m1.1.1.3"><divide id="S5.Ex1.m1.1.1.3.1.cmml" xref="S5.Ex1.m1.1.1.3"></divide><apply id="S5.Ex1.m1.1.1.3.2.cmml" xref="S5.Ex1.m1.1.1.3.2"><times id="S5.Ex1.m1.1.1.3.2.1.cmml" xref="S5.Ex1.m1.1.1.3.2.1"></times><apply id="S5.Ex1.m1.1.1.3.2.2.cmml" xref="S5.Ex1.m1.1.1.3.2.2"><times id="S5.Ex1.m1.1.1.3.2.2.1.cmml" xref="S5.Ex1.m1.1.1.3.2.2.1"></times><apply id="S5.Ex1.m1.1.1.3.2.2.2.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2"><times id="S5.Ex1.m1.1.1.3.2.2.2.1.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.1"></times><ci id="S5.Ex1.m1.1.1.3.2.2.2.2.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.2">𝑇</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.3.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.3">ℎ</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.4.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.4">𝑟</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.5.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.5">𝑜</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.6.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.6">𝑢</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.7.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.7">𝑔</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.8.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.8">ℎ</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.9.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.9">𝑝</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.10.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.10">𝑢</ci><ci id="S5.Ex1.m1.1.1.3.2.2.2.11.cmml" xref="S5.Ex1.m1.1.1.3.2.2.2.11">𝑡</ci></apply><ci id="S5.Ex1.m1.1.1.3.2.2.3.cmml" xref="S5.Ex1.m1.1.1.3.2.2.3">𝐷</ci></apply><ci id="S5.Ex1.m1.1.1.3.2.3.cmml" xref="S5.Ex1.m1.1.1.3.2.3">𝑢</ci><ci id="S5.Ex1.m1.1.1.3.2.4.cmml" xref="S5.Ex1.m1.1.1.3.2.4">𝑟</ci><ci id="S5.Ex1.m1.1.1.3.2.5.cmml" xref="S5.Ex1.m1.1.1.3.2.5">𝑎</ci><ci id="S5.Ex1.m1.1.1.3.2.6.cmml" xref="S5.Ex1.m1.1.1.3.2.6">𝑡</ci><ci id="S5.Ex1.m1.1.1.3.2.7.cmml" xref="S5.Ex1.m1.1.1.3.2.7">𝑖</ci><ci id="S5.Ex1.m1.1.1.3.2.8.cmml" xref="S5.Ex1.m1.1.1.3.2.8">𝑜</ci><ci id="S5.Ex1.m1.1.1.3.2.9.cmml" xref="S5.Ex1.m1.1.1.3.2.9">𝑛</ci></apply><apply id="S5.Ex1.m1.1.1.3.3.cmml" xref="S5.Ex1.m1.1.1.3.3"><plus id="S5.Ex1.m1.1.1.3.3.1.cmml" xref="S5.Ex1.m1.1.1.3.3.1"></plus><ci id="S5.Ex1.m1.1.1.3.3.2a.cmml" xref="S5.Ex1.m1.1.1.3.3.2"><mtext id="S5.Ex1.m1.1.1.3.3.2.cmml" xref="S5.Ex1.m1.1.1.3.3.2">CapEx</mtext></ci><ci id="S5.Ex1.m1.1.1.3.3.3a.cmml" xref="S5.Ex1.m1.1.1.3.3.3"><mtext id="S5.Ex1.m1.1.1.3.3.3.cmml" xref="S5.Ex1.m1.1.1.3.3.3">OpEx</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex1.m1.1c">\text{Cost-efficiency}=\frac{Throughput\times Duration}{\text{CapEx}+\text{%
OpEx}}</annotation><annotation encoding="application/x-llamapun" id="S5.Ex1.m1.1d">Cost-efficiency = divide start_ARG italic_T italic_h italic_r italic_o italic_u italic_g italic_h italic_p italic_u italic_t × italic_D italic_u italic_r italic_a italic_t italic_i italic_o italic_n end_ARG start_ARG CapEx + OpEx end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table class="ltx_equation ltx_eqn_table" id="S5.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{where OpEx}=\sum(Power\times Duration\times Electricity)" class="ltx_Math" display="block" id="S5.Ex2.m1.1"><semantics id="S5.Ex2.m1.1a"><mrow id="S5.Ex2.m1.1.1" xref="S5.Ex2.m1.1.1.cmml"><mtext id="S5.Ex2.m1.1.1.3" xref="S5.Ex2.m1.1.1.3a.cmml">where OpEx</mtext><mo id="S5.Ex2.m1.1.1.2" rspace="0.111em" xref="S5.Ex2.m1.1.1.2.cmml">=</mo><mrow id="S5.Ex2.m1.1.1.1" xref="S5.Ex2.m1.1.1.1.cmml"><mo id="S5.Ex2.m1.1.1.1.2" movablelimits="false" rspace="0em" xref="S5.Ex2.m1.1.1.1.2.cmml">∑</mo><mrow id="S5.Ex2.m1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml"><mo id="S5.Ex2.m1.1.1.1.1.1.2" stretchy="false" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.Ex2.m1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml"><mrow id="S5.Ex2.m1.1.1.1.1.1.1.2" xref="S5.Ex2.m1.1.1.1.1.1.1.2.cmml"><mrow id="S5.Ex2.m1.1.1.1.1.1.1.2.2" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.cmml"><mrow id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.cmml"><mrow id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2.cmml">P</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.3" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.3.cmml">o</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1a" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.4" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.4.cmml">w</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1b" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.5" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.5.cmml">e</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1c" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.6" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.6.cmml">r</mi></mrow><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.1.cmml">×</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.3" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.3.cmml">D</mi></mrow><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.1" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.3" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.3.cmml">u</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.1a" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.4" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.4.cmml">r</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.1b" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.5" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.5.cmml">a</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.1c" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.6" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.6.cmml">t</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.1d" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.7" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.7.cmml">i</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.1e" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.8" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.8.cmml">o</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.2.1f" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.2.9" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.9.cmml">n</mi></mrow><mo id="S5.Ex2.m1.1.1.1.1.1.1.2.1" lspace="0.222em" rspace="0.222em" xref="S5.Ex2.m1.1.1.1.1.1.1.2.1.cmml">×</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.2.3" xref="S5.Ex2.m1.1.1.1.1.1.1.2.3.cmml">E</mi></mrow><mo id="S5.Ex2.m1.1.1.1.1.1.1.1" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.3" xref="S5.Ex2.m1.1.1.1.1.1.1.3.cmml">l</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1a" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.4" xref="S5.Ex2.m1.1.1.1.1.1.1.4.cmml">e</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1b" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.5" xref="S5.Ex2.m1.1.1.1.1.1.1.5.cmml">c</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1c" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.6" xref="S5.Ex2.m1.1.1.1.1.1.1.6.cmml">t</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1d" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.7" xref="S5.Ex2.m1.1.1.1.1.1.1.7.cmml">r</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1e" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.8" xref="S5.Ex2.m1.1.1.1.1.1.1.8.cmml">i</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1f" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.9" xref="S5.Ex2.m1.1.1.1.1.1.1.9.cmml">c</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1g" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.10" xref="S5.Ex2.m1.1.1.1.1.1.1.10.cmml">i</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1h" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.11" xref="S5.Ex2.m1.1.1.1.1.1.1.11.cmml">t</mi><mo id="S5.Ex2.m1.1.1.1.1.1.1.1i" xref="S5.Ex2.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.Ex2.m1.1.1.1.1.1.1.12" xref="S5.Ex2.m1.1.1.1.1.1.1.12.cmml">y</mi></mrow><mo id="S5.Ex2.m1.1.1.1.1.1.3" stretchy="false" xref="S5.Ex2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.Ex2.m1.1b"><apply id="S5.Ex2.m1.1.1.cmml" xref="S5.Ex2.m1.1.1"><eq id="S5.Ex2.m1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.2"></eq><ci id="S5.Ex2.m1.1.1.3a.cmml" xref="S5.Ex2.m1.1.1.3"><mtext id="S5.Ex2.m1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.3">where OpEx</mtext></ci><apply id="S5.Ex2.m1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1"><sum id="S5.Ex2.m1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.2"></sum><apply id="S5.Ex2.m1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1"><times id="S5.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.1"></times><apply id="S5.Ex2.m1.1.1.1.1.1.1.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2"><times id="S5.Ex2.m1.1.1.1.1.1.1.2.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.1"></times><apply id="S5.Ex2.m1.1.1.1.1.1.1.2.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2"><times id="S5.Ex2.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.1"></times><apply id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2"><times id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.1"></times><apply id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2"><times id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.1"></times><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.2">𝑃</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.3">𝑜</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.4.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.4">𝑤</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.5.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.5">𝑒</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.6.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.2.6">𝑟</ci></apply><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.2.3">𝐷</ci></apply><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.3">𝑢</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.4.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.4">𝑟</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.5.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.5">𝑎</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.6.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.6">𝑡</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.7.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.7">𝑖</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.8.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.8">𝑜</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.2.9.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.2.9">𝑛</ci></apply><ci id="S5.Ex2.m1.1.1.1.1.1.1.2.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.2.3">𝐸</ci></apply><ci id="S5.Ex2.m1.1.1.1.1.1.1.3.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.3">𝑙</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.4.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.4">𝑒</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.5.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.5">𝑐</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.6.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.6">𝑡</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.7.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.7">𝑟</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.8.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.8">𝑖</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.9.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.9">𝑐</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.10.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.10">𝑖</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.11.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.11">𝑡</ci><ci id="S5.Ex2.m1.1.1.1.1.1.1.12.cmml" xref="S5.Ex2.m1.1.1.1.1.1.1.12">𝑦</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.Ex2.m1.1c">\text{where OpEx}=\sum(Power\times Duration\times Electricity)</annotation><annotation encoding="application/x-llamapun" id="S5.Ex2.m1.1d">where OpEx = ∑ ( italic_P italic_o italic_w italic_e italic_r × italic_D italic_u italic_r italic_a italic_t italic_i italic_o italic_n × italic_E italic_l italic_e italic_c italic_t italic_r italic_i italic_c italic_i italic_t italic_y )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.9">CapEx ($) refers to the one-time capital expenditure required to purchase and
establish the hardware platform components. OpEx ($) represents the operating
expenditure of this hardware platform. To determine CapEx, we utilize the cost
information obtained from the respective company
website <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib59" title="">59</a>]</cite>. OpEx is derived using the power consumed by
the hardware components (<math alttext="Power" class="ltx_Math" display="inline" id="S5.SS3.p4.1.m1.1"><semantics id="S5.SS3.p4.1.m1.1a"><mrow id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml"><mi id="S5.SS3.p4.1.m1.1.1.2" xref="S5.SS3.p4.1.m1.1.1.2.cmml">P</mi><mo id="S5.SS3.p4.1.m1.1.1.1" xref="S5.SS3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.1.m1.1.1.3" xref="S5.SS3.p4.1.m1.1.1.3.cmml">o</mi><mo id="S5.SS3.p4.1.m1.1.1.1a" xref="S5.SS3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.1.m1.1.1.4" xref="S5.SS3.p4.1.m1.1.1.4.cmml">w</mi><mo id="S5.SS3.p4.1.m1.1.1.1b" xref="S5.SS3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.1.m1.1.1.5" xref="S5.SS3.p4.1.m1.1.1.5.cmml">e</mi><mo id="S5.SS3.p4.1.m1.1.1.1c" xref="S5.SS3.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.1.m1.1.1.6" xref="S5.SS3.p4.1.m1.1.1.6.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><apply id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"><times id="S5.SS3.p4.1.m1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1.1"></times><ci id="S5.SS3.p4.1.m1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2">𝑃</ci><ci id="S5.SS3.p4.1.m1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.1.1.3">𝑜</ci><ci id="S5.SS3.p4.1.m1.1.1.4.cmml" xref="S5.SS3.p4.1.m1.1.1.4">𝑤</ci><ci id="S5.SS3.p4.1.m1.1.1.5.cmml" xref="S5.SS3.p4.1.m1.1.1.5">𝑒</ci><ci id="S5.SS3.p4.1.m1.1.1.6.cmml" xref="S5.SS3.p4.1.m1.1.1.6">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">Power</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.1.m1.1d">italic_P italic_o italic_w italic_e italic_r</annotation></semantics></math>), the active duration of each hardware
component (<math alttext="Duration" class="ltx_Math" display="inline" id="S5.SS3.p4.2.m2.1"><semantics id="S5.SS3.p4.2.m2.1a"><mrow id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml"><mi id="S5.SS3.p4.2.m2.1.1.2" xref="S5.SS3.p4.2.m2.1.1.2.cmml">D</mi><mo id="S5.SS3.p4.2.m2.1.1.1" xref="S5.SS3.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.2.m2.1.1.3" xref="S5.SS3.p4.2.m2.1.1.3.cmml">u</mi><mo id="S5.SS3.p4.2.m2.1.1.1a" xref="S5.SS3.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.2.m2.1.1.4" xref="S5.SS3.p4.2.m2.1.1.4.cmml">r</mi><mo id="S5.SS3.p4.2.m2.1.1.1b" xref="S5.SS3.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.2.m2.1.1.5" xref="S5.SS3.p4.2.m2.1.1.5.cmml">a</mi><mo id="S5.SS3.p4.2.m2.1.1.1c" xref="S5.SS3.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.2.m2.1.1.6" xref="S5.SS3.p4.2.m2.1.1.6.cmml">t</mi><mo id="S5.SS3.p4.2.m2.1.1.1d" xref="S5.SS3.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.2.m2.1.1.7" xref="S5.SS3.p4.2.m2.1.1.7.cmml">i</mi><mo id="S5.SS3.p4.2.m2.1.1.1e" xref="S5.SS3.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.2.m2.1.1.8" xref="S5.SS3.p4.2.m2.1.1.8.cmml">o</mi><mo id="S5.SS3.p4.2.m2.1.1.1f" xref="S5.SS3.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.2.m2.1.1.9" xref="S5.SS3.p4.2.m2.1.1.9.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><apply id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1"><times id="S5.SS3.p4.2.m2.1.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1.1"></times><ci id="S5.SS3.p4.2.m2.1.1.2.cmml" xref="S5.SS3.p4.2.m2.1.1.2">𝐷</ci><ci id="S5.SS3.p4.2.m2.1.1.3.cmml" xref="S5.SS3.p4.2.m2.1.1.3">𝑢</ci><ci id="S5.SS3.p4.2.m2.1.1.4.cmml" xref="S5.SS3.p4.2.m2.1.1.4">𝑟</ci><ci id="S5.SS3.p4.2.m2.1.1.5.cmml" xref="S5.SS3.p4.2.m2.1.1.5">𝑎</ci><ci id="S5.SS3.p4.2.m2.1.1.6.cmml" xref="S5.SS3.p4.2.m2.1.1.6">𝑡</ci><ci id="S5.SS3.p4.2.m2.1.1.7.cmml" xref="S5.SS3.p4.2.m2.1.1.7">𝑖</ci><ci id="S5.SS3.p4.2.m2.1.1.8.cmml" xref="S5.SS3.p4.2.m2.1.1.8">𝑜</ci><ci id="S5.SS3.p4.2.m2.1.1.9.cmml" xref="S5.SS3.p4.2.m2.1.1.9">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">Duration</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.2.m2.1d">italic_D italic_u italic_r italic_a italic_t italic_i italic_o italic_n</annotation></semantics></math>, a period of 3 years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib7" title="">7</a>]</cite>), and the average price of electricity
(<math alttext="Electricity" class="ltx_Math" display="inline" id="S5.SS3.p4.3.m3.1"><semantics id="S5.SS3.p4.3.m3.1a"><mrow id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml"><mi id="S5.SS3.p4.3.m3.1.1.2" xref="S5.SS3.p4.3.m3.1.1.2.cmml">E</mi><mo id="S5.SS3.p4.3.m3.1.1.1" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.3" xref="S5.SS3.p4.3.m3.1.1.3.cmml">l</mi><mo id="S5.SS3.p4.3.m3.1.1.1a" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.4" xref="S5.SS3.p4.3.m3.1.1.4.cmml">e</mi><mo id="S5.SS3.p4.3.m3.1.1.1b" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.5" xref="S5.SS3.p4.3.m3.1.1.5.cmml">c</mi><mo id="S5.SS3.p4.3.m3.1.1.1c" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.6" xref="S5.SS3.p4.3.m3.1.1.6.cmml">t</mi><mo id="S5.SS3.p4.3.m3.1.1.1d" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.7" xref="S5.SS3.p4.3.m3.1.1.7.cmml">r</mi><mo id="S5.SS3.p4.3.m3.1.1.1e" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.8" xref="S5.SS3.p4.3.m3.1.1.8.cmml">i</mi><mo id="S5.SS3.p4.3.m3.1.1.1f" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.9" xref="S5.SS3.p4.3.m3.1.1.9.cmml">c</mi><mo id="S5.SS3.p4.3.m3.1.1.1g" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.10" xref="S5.SS3.p4.3.m3.1.1.10.cmml">i</mi><mo id="S5.SS3.p4.3.m3.1.1.1h" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.11" xref="S5.SS3.p4.3.m3.1.1.11.cmml">t</mi><mo id="S5.SS3.p4.3.m3.1.1.1i" xref="S5.SS3.p4.3.m3.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.3.m3.1.1.12" xref="S5.SS3.p4.3.m3.1.1.12.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><apply id="S5.SS3.p4.3.m3.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1"><times id="S5.SS3.p4.3.m3.1.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1.1"></times><ci id="S5.SS3.p4.3.m3.1.1.2.cmml" xref="S5.SS3.p4.3.m3.1.1.2">𝐸</ci><ci id="S5.SS3.p4.3.m3.1.1.3.cmml" xref="S5.SS3.p4.3.m3.1.1.3">𝑙</ci><ci id="S5.SS3.p4.3.m3.1.1.4.cmml" xref="S5.SS3.p4.3.m3.1.1.4">𝑒</ci><ci id="S5.SS3.p4.3.m3.1.1.5.cmml" xref="S5.SS3.p4.3.m3.1.1.5">𝑐</ci><ci id="S5.SS3.p4.3.m3.1.1.6.cmml" xref="S5.SS3.p4.3.m3.1.1.6">𝑡</ci><ci id="S5.SS3.p4.3.m3.1.1.7.cmml" xref="S5.SS3.p4.3.m3.1.1.7">𝑟</ci><ci id="S5.SS3.p4.3.m3.1.1.8.cmml" xref="S5.SS3.p4.3.m3.1.1.8">𝑖</ci><ci id="S5.SS3.p4.3.m3.1.1.9.cmml" xref="S5.SS3.p4.3.m3.1.1.9">𝑐</ci><ci id="S5.SS3.p4.3.m3.1.1.10.cmml" xref="S5.SS3.p4.3.m3.1.1.10">𝑖</ci><ci id="S5.SS3.p4.3.m3.1.1.11.cmml" xref="S5.SS3.p4.3.m3.1.1.11">𝑡</ci><ci id="S5.SS3.p4.3.m3.1.1.12.cmml" xref="S5.SS3.p4.3.m3.1.1.12">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">Electricity</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.3.m3.1d">italic_E italic_l italic_e italic_c italic_t italic_r italic_i italic_c italic_i italic_t italic_y</annotation></semantics></math>, $0.0733/kWh <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib43" title="">43</a>]</cite>). It is worth pointing out
that the numerator value to calculate cost-efficiency
(<math alttext="Throughput" class="ltx_Math" display="inline" id="S5.SS3.p4.4.m4.1"><semantics id="S5.SS3.p4.4.m4.1a"><mrow id="S5.SS3.p4.4.m4.1.1" xref="S5.SS3.p4.4.m4.1.1.cmml"><mi id="S5.SS3.p4.4.m4.1.1.2" xref="S5.SS3.p4.4.m4.1.1.2.cmml">T</mi><mo id="S5.SS3.p4.4.m4.1.1.1" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.3" xref="S5.SS3.p4.4.m4.1.1.3.cmml">h</mi><mo id="S5.SS3.p4.4.m4.1.1.1a" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.4" xref="S5.SS3.p4.4.m4.1.1.4.cmml">r</mi><mo id="S5.SS3.p4.4.m4.1.1.1b" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.5" xref="S5.SS3.p4.4.m4.1.1.5.cmml">o</mi><mo id="S5.SS3.p4.4.m4.1.1.1c" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.6" xref="S5.SS3.p4.4.m4.1.1.6.cmml">u</mi><mo id="S5.SS3.p4.4.m4.1.1.1d" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.7" xref="S5.SS3.p4.4.m4.1.1.7.cmml">g</mi><mo id="S5.SS3.p4.4.m4.1.1.1e" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.8" xref="S5.SS3.p4.4.m4.1.1.8.cmml">h</mi><mo id="S5.SS3.p4.4.m4.1.1.1f" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.9" xref="S5.SS3.p4.4.m4.1.1.9.cmml">p</mi><mo id="S5.SS3.p4.4.m4.1.1.1g" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.10" xref="S5.SS3.p4.4.m4.1.1.10.cmml">u</mi><mo id="S5.SS3.p4.4.m4.1.1.1h" xref="S5.SS3.p4.4.m4.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.4.m4.1.1.11" xref="S5.SS3.p4.4.m4.1.1.11.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.4.m4.1b"><apply id="S5.SS3.p4.4.m4.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1"><times id="S5.SS3.p4.4.m4.1.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1.1"></times><ci id="S5.SS3.p4.4.m4.1.1.2.cmml" xref="S5.SS3.p4.4.m4.1.1.2">𝑇</ci><ci id="S5.SS3.p4.4.m4.1.1.3.cmml" xref="S5.SS3.p4.4.m4.1.1.3">ℎ</ci><ci id="S5.SS3.p4.4.m4.1.1.4.cmml" xref="S5.SS3.p4.4.m4.1.1.4">𝑟</ci><ci id="S5.SS3.p4.4.m4.1.1.5.cmml" xref="S5.SS3.p4.4.m4.1.1.5">𝑜</ci><ci id="S5.SS3.p4.4.m4.1.1.6.cmml" xref="S5.SS3.p4.4.m4.1.1.6">𝑢</ci><ci id="S5.SS3.p4.4.m4.1.1.7.cmml" xref="S5.SS3.p4.4.m4.1.1.7">𝑔</ci><ci id="S5.SS3.p4.4.m4.1.1.8.cmml" xref="S5.SS3.p4.4.m4.1.1.8">ℎ</ci><ci id="S5.SS3.p4.4.m4.1.1.9.cmml" xref="S5.SS3.p4.4.m4.1.1.9">𝑝</ci><ci id="S5.SS3.p4.4.m4.1.1.10.cmml" xref="S5.SS3.p4.4.m4.1.1.10">𝑢</ci><ci id="S5.SS3.p4.4.m4.1.1.11.cmml" xref="S5.SS3.p4.4.m4.1.1.11">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.4.m4.1c">Throughput</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.4.m4.1d">italic_T italic_h italic_r italic_o italic_u italic_g italic_h italic_p italic_u italic_t</annotation></semantics></math><math alttext="\times" class="ltx_Math" display="inline" id="S5.SS3.p4.5.m5.1"><semantics id="S5.SS3.p4.5.m5.1a"><mo id="S5.SS3.p4.5.m5.1.1" xref="S5.SS3.p4.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.5.m5.1b"><times id="S5.SS3.p4.5.m5.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.5.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.5.m5.1d">×</annotation></semantics></math><math alttext="Duration" class="ltx_Math" display="inline" id="S5.SS3.p4.6.m6.1"><semantics id="S5.SS3.p4.6.m6.1a"><mrow id="S5.SS3.p4.6.m6.1.1" xref="S5.SS3.p4.6.m6.1.1.cmml"><mi id="S5.SS3.p4.6.m6.1.1.2" xref="S5.SS3.p4.6.m6.1.1.2.cmml">D</mi><mo id="S5.SS3.p4.6.m6.1.1.1" xref="S5.SS3.p4.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.6.m6.1.1.3" xref="S5.SS3.p4.6.m6.1.1.3.cmml">u</mi><mo id="S5.SS3.p4.6.m6.1.1.1a" xref="S5.SS3.p4.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.6.m6.1.1.4" xref="S5.SS3.p4.6.m6.1.1.4.cmml">r</mi><mo id="S5.SS3.p4.6.m6.1.1.1b" xref="S5.SS3.p4.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.6.m6.1.1.5" xref="S5.SS3.p4.6.m6.1.1.5.cmml">a</mi><mo id="S5.SS3.p4.6.m6.1.1.1c" xref="S5.SS3.p4.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.6.m6.1.1.6" xref="S5.SS3.p4.6.m6.1.1.6.cmml">t</mi><mo id="S5.SS3.p4.6.m6.1.1.1d" xref="S5.SS3.p4.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.6.m6.1.1.7" xref="S5.SS3.p4.6.m6.1.1.7.cmml">i</mi><mo id="S5.SS3.p4.6.m6.1.1.1e" xref="S5.SS3.p4.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.6.m6.1.1.8" xref="S5.SS3.p4.6.m6.1.1.8.cmml">o</mi><mo id="S5.SS3.p4.6.m6.1.1.1f" xref="S5.SS3.p4.6.m6.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.6.m6.1.1.9" xref="S5.SS3.p4.6.m6.1.1.9.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.6.m6.1b"><apply id="S5.SS3.p4.6.m6.1.1.cmml" xref="S5.SS3.p4.6.m6.1.1"><times id="S5.SS3.p4.6.m6.1.1.1.cmml" xref="S5.SS3.p4.6.m6.1.1.1"></times><ci id="S5.SS3.p4.6.m6.1.1.2.cmml" xref="S5.SS3.p4.6.m6.1.1.2">𝐷</ci><ci id="S5.SS3.p4.6.m6.1.1.3.cmml" xref="S5.SS3.p4.6.m6.1.1.3">𝑢</ci><ci id="S5.SS3.p4.6.m6.1.1.4.cmml" xref="S5.SS3.p4.6.m6.1.1.4">𝑟</ci><ci id="S5.SS3.p4.6.m6.1.1.5.cmml" xref="S5.SS3.p4.6.m6.1.1.5">𝑎</ci><ci id="S5.SS3.p4.6.m6.1.1.6.cmml" xref="S5.SS3.p4.6.m6.1.1.6">𝑡</ci><ci id="S5.SS3.p4.6.m6.1.1.7.cmml" xref="S5.SS3.p4.6.m6.1.1.7">𝑖</ci><ci id="S5.SS3.p4.6.m6.1.1.8.cmml" xref="S5.SS3.p4.6.m6.1.1.8">𝑜</ci><ci id="S5.SS3.p4.6.m6.1.1.9.cmml" xref="S5.SS3.p4.6.m6.1.1.9">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.6.m6.1c">Duration</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.6.m6.1d">italic_D italic_u italic_r italic_a italic_t italic_i italic_o italic_n</annotation></semantics></math>) is identical for both baseline disaggregated
CPU preprocessing and <math alttext="PreSto" class="ltx_Math" display="inline" id="S5.SS3.p4.7.m7.1"><semantics id="S5.SS3.p4.7.m7.1a"><mrow id="S5.SS3.p4.7.m7.1.1" xref="S5.SS3.p4.7.m7.1.1.cmml"><mi id="S5.SS3.p4.7.m7.1.1.2" xref="S5.SS3.p4.7.m7.1.1.2.cmml">P</mi><mo id="S5.SS3.p4.7.m7.1.1.1" xref="S5.SS3.p4.7.m7.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.7.m7.1.1.3" xref="S5.SS3.p4.7.m7.1.1.3.cmml">r</mi><mo id="S5.SS3.p4.7.m7.1.1.1a" xref="S5.SS3.p4.7.m7.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.7.m7.1.1.4" xref="S5.SS3.p4.7.m7.1.1.4.cmml">e</mi><mo id="S5.SS3.p4.7.m7.1.1.1b" xref="S5.SS3.p4.7.m7.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.7.m7.1.1.5" xref="S5.SS3.p4.7.m7.1.1.5.cmml">S</mi><mo id="S5.SS3.p4.7.m7.1.1.1c" xref="S5.SS3.p4.7.m7.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.7.m7.1.1.6" xref="S5.SS3.p4.7.m7.1.1.6.cmml">t</mi><mo id="S5.SS3.p4.7.m7.1.1.1d" xref="S5.SS3.p4.7.m7.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.7.m7.1.1.7" xref="S5.SS3.p4.7.m7.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.7.m7.1b"><apply id="S5.SS3.p4.7.m7.1.1.cmml" xref="S5.SS3.p4.7.m7.1.1"><times id="S5.SS3.p4.7.m7.1.1.1.cmml" xref="S5.SS3.p4.7.m7.1.1.1"></times><ci id="S5.SS3.p4.7.m7.1.1.2.cmml" xref="S5.SS3.p4.7.m7.1.1.2">𝑃</ci><ci id="S5.SS3.p4.7.m7.1.1.3.cmml" xref="S5.SS3.p4.7.m7.1.1.3">𝑟</ci><ci id="S5.SS3.p4.7.m7.1.1.4.cmml" xref="S5.SS3.p4.7.m7.1.1.4">𝑒</ci><ci id="S5.SS3.p4.7.m7.1.1.5.cmml" xref="S5.SS3.p4.7.m7.1.1.5">𝑆</ci><ci id="S5.SS3.p4.7.m7.1.1.6.cmml" xref="S5.SS3.p4.7.m7.1.1.6">𝑡</ci><ci id="S5.SS3.p4.7.m7.1.1.7.cmml" xref="S5.SS3.p4.7.m7.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.7.m7.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.7.m7.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>: both baseline and our proposal
can sustain the throughput demands of GPU’s training stage, so <math alttext="Throughput" class="ltx_Math" display="inline" id="S5.SS3.p4.8.m8.1"><semantics id="S5.SS3.p4.8.m8.1a"><mrow id="S5.SS3.p4.8.m8.1.1" xref="S5.SS3.p4.8.m8.1.1.cmml"><mi id="S5.SS3.p4.8.m8.1.1.2" xref="S5.SS3.p4.8.m8.1.1.2.cmml">T</mi><mo id="S5.SS3.p4.8.m8.1.1.1" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.3" xref="S5.SS3.p4.8.m8.1.1.3.cmml">h</mi><mo id="S5.SS3.p4.8.m8.1.1.1a" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.4" xref="S5.SS3.p4.8.m8.1.1.4.cmml">r</mi><mo id="S5.SS3.p4.8.m8.1.1.1b" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.5" xref="S5.SS3.p4.8.m8.1.1.5.cmml">o</mi><mo id="S5.SS3.p4.8.m8.1.1.1c" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.6" xref="S5.SS3.p4.8.m8.1.1.6.cmml">u</mi><mo id="S5.SS3.p4.8.m8.1.1.1d" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.7" xref="S5.SS3.p4.8.m8.1.1.7.cmml">g</mi><mo id="S5.SS3.p4.8.m8.1.1.1e" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.8" xref="S5.SS3.p4.8.m8.1.1.8.cmml">h</mi><mo id="S5.SS3.p4.8.m8.1.1.1f" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.9" xref="S5.SS3.p4.8.m8.1.1.9.cmml">p</mi><mo id="S5.SS3.p4.8.m8.1.1.1g" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.10" xref="S5.SS3.p4.8.m8.1.1.10.cmml">u</mi><mo id="S5.SS3.p4.8.m8.1.1.1h" xref="S5.SS3.p4.8.m8.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.8.m8.1.1.11" xref="S5.SS3.p4.8.m8.1.1.11.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.8.m8.1b"><apply id="S5.SS3.p4.8.m8.1.1.cmml" xref="S5.SS3.p4.8.m8.1.1"><times id="S5.SS3.p4.8.m8.1.1.1.cmml" xref="S5.SS3.p4.8.m8.1.1.1"></times><ci id="S5.SS3.p4.8.m8.1.1.2.cmml" xref="S5.SS3.p4.8.m8.1.1.2">𝑇</ci><ci id="S5.SS3.p4.8.m8.1.1.3.cmml" xref="S5.SS3.p4.8.m8.1.1.3">ℎ</ci><ci id="S5.SS3.p4.8.m8.1.1.4.cmml" xref="S5.SS3.p4.8.m8.1.1.4">𝑟</ci><ci id="S5.SS3.p4.8.m8.1.1.5.cmml" xref="S5.SS3.p4.8.m8.1.1.5">𝑜</ci><ci id="S5.SS3.p4.8.m8.1.1.6.cmml" xref="S5.SS3.p4.8.m8.1.1.6">𝑢</ci><ci id="S5.SS3.p4.8.m8.1.1.7.cmml" xref="S5.SS3.p4.8.m8.1.1.7">𝑔</ci><ci id="S5.SS3.p4.8.m8.1.1.8.cmml" xref="S5.SS3.p4.8.m8.1.1.8">ℎ</ci><ci id="S5.SS3.p4.8.m8.1.1.9.cmml" xref="S5.SS3.p4.8.m8.1.1.9">𝑝</ci><ci id="S5.SS3.p4.8.m8.1.1.10.cmml" xref="S5.SS3.p4.8.m8.1.1.10">𝑢</ci><ci id="S5.SS3.p4.8.m8.1.1.11.cmml" xref="S5.SS3.p4.8.m8.1.1.11">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.8.m8.1c">Throughput</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.8.m8.1d">italic_T italic_h italic_r italic_o italic_u italic_g italic_h italic_p italic_u italic_t</annotation></semantics></math>
and <math alttext="Duration" class="ltx_Math" display="inline" id="S5.SS3.p4.9.m9.1"><semantics id="S5.SS3.p4.9.m9.1a"><mrow id="S5.SS3.p4.9.m9.1.1" xref="S5.SS3.p4.9.m9.1.1.cmml"><mi id="S5.SS3.p4.9.m9.1.1.2" xref="S5.SS3.p4.9.m9.1.1.2.cmml">D</mi><mo id="S5.SS3.p4.9.m9.1.1.1" xref="S5.SS3.p4.9.m9.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.9.m9.1.1.3" xref="S5.SS3.p4.9.m9.1.1.3.cmml">u</mi><mo id="S5.SS3.p4.9.m9.1.1.1a" xref="S5.SS3.p4.9.m9.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.9.m9.1.1.4" xref="S5.SS3.p4.9.m9.1.1.4.cmml">r</mi><mo id="S5.SS3.p4.9.m9.1.1.1b" xref="S5.SS3.p4.9.m9.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.9.m9.1.1.5" xref="S5.SS3.p4.9.m9.1.1.5.cmml">a</mi><mo id="S5.SS3.p4.9.m9.1.1.1c" xref="S5.SS3.p4.9.m9.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.9.m9.1.1.6" xref="S5.SS3.p4.9.m9.1.1.6.cmml">t</mi><mo id="S5.SS3.p4.9.m9.1.1.1d" xref="S5.SS3.p4.9.m9.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.9.m9.1.1.7" xref="S5.SS3.p4.9.m9.1.1.7.cmml">i</mi><mo id="S5.SS3.p4.9.m9.1.1.1e" xref="S5.SS3.p4.9.m9.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.9.m9.1.1.8" xref="S5.SS3.p4.9.m9.1.1.8.cmml">o</mi><mo id="S5.SS3.p4.9.m9.1.1.1f" xref="S5.SS3.p4.9.m9.1.1.1.cmml">⁢</mo><mi id="S5.SS3.p4.9.m9.1.1.9" xref="S5.SS3.p4.9.m9.1.1.9.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.9.m9.1b"><apply id="S5.SS3.p4.9.m9.1.1.cmml" xref="S5.SS3.p4.9.m9.1.1"><times id="S5.SS3.p4.9.m9.1.1.1.cmml" xref="S5.SS3.p4.9.m9.1.1.1"></times><ci id="S5.SS3.p4.9.m9.1.1.2.cmml" xref="S5.SS3.p4.9.m9.1.1.2">𝐷</ci><ci id="S5.SS3.p4.9.m9.1.1.3.cmml" xref="S5.SS3.p4.9.m9.1.1.3">𝑢</ci><ci id="S5.SS3.p4.9.m9.1.1.4.cmml" xref="S5.SS3.p4.9.m9.1.1.4">𝑟</ci><ci id="S5.SS3.p4.9.m9.1.1.5.cmml" xref="S5.SS3.p4.9.m9.1.1.5">𝑎</ci><ci id="S5.SS3.p4.9.m9.1.1.6.cmml" xref="S5.SS3.p4.9.m9.1.1.6">𝑡</ci><ci id="S5.SS3.p4.9.m9.1.1.7.cmml" xref="S5.SS3.p4.9.m9.1.1.7">𝑖</ci><ci id="S5.SS3.p4.9.m9.1.1.8.cmml" xref="S5.SS3.p4.9.m9.1.1.8">𝑜</ci><ci id="S5.SS3.p4.9.m9.1.1.9.cmml" xref="S5.SS3.p4.9.m9.1.1.9">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.9.m9.1c">Duration</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.9.m9.1d">italic_D italic_u italic_r italic_a italic_t italic_i italic_o italic_n</annotation></semantics></math> are constant values. Therefore, the difference in
cost-efficiency is determined by (CapEx+OpEx).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Evaluation</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.4">We first demonstrate <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.p1.1.m1.1"><semantics id="S6.p1.1.m1.1a"><mrow id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mi id="S6.p1.1.m1.1.1.2" xref="S6.p1.1.m1.1.1.2.cmml">P</mi><mo id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p1.1.m1.1.1.3" xref="S6.p1.1.m1.1.1.3.cmml">r</mi><mo id="S6.p1.1.m1.1.1.1a" xref="S6.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p1.1.m1.1.1.4" xref="S6.p1.1.m1.1.1.4.cmml">e</mi><mo id="S6.p1.1.m1.1.1.1b" xref="S6.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p1.1.m1.1.1.5" xref="S6.p1.1.m1.1.1.5.cmml">S</mi><mo id="S6.p1.1.m1.1.1.1c" xref="S6.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p1.1.m1.1.1.6" xref="S6.p1.1.m1.1.1.6.cmml">t</mi><mo id="S6.p1.1.m1.1.1.1d" xref="S6.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.p1.1.m1.1.1.7" xref="S6.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><times id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1"></times><ci id="S6.p1.1.m1.1.1.2.cmml" xref="S6.p1.1.m1.1.1.2">𝑃</ci><ci id="S6.p1.1.m1.1.1.3.cmml" xref="S6.p1.1.m1.1.1.3">𝑟</ci><ci id="S6.p1.1.m1.1.1.4.cmml" xref="S6.p1.1.m1.1.1.4">𝑒</ci><ci id="S6.p1.1.m1.1.1.5.cmml" xref="S6.p1.1.m1.1.1.5">𝑆</ci><ci id="S6.p1.1.m1.1.1.6.cmml" xref="S6.p1.1.m1.1.1.6">𝑡</ci><ci id="S6.p1.1.m1.1.1.7.cmml" xref="S6.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s merits using our PoC prototype
(Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.SS1" title="VI-A Performance and Cost-Effectiveness of P⁢r⁢e⁢S⁢t⁢o (PoC) ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-A</span></span></a>). We then utilize our analytical model
(Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5" title="V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">V</span></a>) to estimate <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.p1.2.m2.1"><semantics id="S6.p1.2.m2.1a"><mrow id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mi id="S6.p1.2.m2.1.1.2" xref="S6.p1.2.m2.1.1.2.cmml">P</mi><mo id="S6.p1.2.m2.1.1.1" xref="S6.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.p1.2.m2.1.1.3" xref="S6.p1.2.m2.1.1.3.cmml">r</mi><mo id="S6.p1.2.m2.1.1.1a" xref="S6.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.p1.2.m2.1.1.4" xref="S6.p1.2.m2.1.1.4.cmml">e</mi><mo id="S6.p1.2.m2.1.1.1b" xref="S6.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.p1.2.m2.1.1.5" xref="S6.p1.2.m2.1.1.5.cmml">S</mi><mo id="S6.p1.2.m2.1.1.1c" xref="S6.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.p1.2.m2.1.1.6" xref="S6.p1.2.m2.1.1.6.cmml">t</mi><mo id="S6.p1.2.m2.1.1.1d" xref="S6.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.p1.2.m2.1.1.7" xref="S6.p1.2.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><times id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1.1"></times><ci id="S6.p1.2.m2.1.1.2.cmml" xref="S6.p1.2.m2.1.1.2">𝑃</ci><ci id="S6.p1.2.m2.1.1.3.cmml" xref="S6.p1.2.m2.1.1.3">𝑟</ci><ci id="S6.p1.2.m2.1.1.4.cmml" xref="S6.p1.2.m2.1.1.4">𝑒</ci><ci id="S6.p1.2.m2.1.1.5.cmml" xref="S6.p1.2.m2.1.1.5">𝑆</ci><ci id="S6.p1.2.m2.1.1.6.cmml" xref="S6.p1.2.m2.1.1.6">𝑡</ci><ci id="S6.p1.2.m2.1.1.7.cmml" xref="S6.p1.2.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.p1.2.m2.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s effect on performance,
energy-efficiency, and TCO at larger scale (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.SS2" title="VI-B 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜’s Effect on Energy-Efficiency and TCO ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">VI-B</span></span></a>). In
the rest of this section, the baseline CPU-centric preprocessing system
assumes the disaggregated CPU server design (denoted “<math alttext="Disagg" class="ltx_Math" display="inline" id="S6.p1.3.m3.1"><semantics id="S6.p1.3.m3.1a"><mrow id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml"><mi id="S6.p1.3.m3.1.1.2" xref="S6.p1.3.m3.1.1.2.cmml">D</mi><mo id="S6.p1.3.m3.1.1.1" xref="S6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.p1.3.m3.1.1.3" xref="S6.p1.3.m3.1.1.3.cmml">i</mi><mo id="S6.p1.3.m3.1.1.1a" xref="S6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.p1.3.m3.1.1.4" xref="S6.p1.3.m3.1.1.4.cmml">s</mi><mo id="S6.p1.3.m3.1.1.1b" xref="S6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.p1.3.m3.1.1.5" xref="S6.p1.3.m3.1.1.5.cmml">a</mi><mo id="S6.p1.3.m3.1.1.1c" xref="S6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.p1.3.m3.1.1.6" xref="S6.p1.3.m3.1.1.6.cmml">g</mi><mo id="S6.p1.3.m3.1.1.1d" xref="S6.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.p1.3.m3.1.1.7" xref="S6.p1.3.m3.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><apply id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1"><times id="S6.p1.3.m3.1.1.1.cmml" xref="S6.p1.3.m3.1.1.1"></times><ci id="S6.p1.3.m3.1.1.2.cmml" xref="S6.p1.3.m3.1.1.2">𝐷</ci><ci id="S6.p1.3.m3.1.1.3.cmml" xref="S6.p1.3.m3.1.1.3">𝑖</ci><ci id="S6.p1.3.m3.1.1.4.cmml" xref="S6.p1.3.m3.1.1.4">𝑠</ci><ci id="S6.p1.3.m3.1.1.5.cmml" xref="S6.p1.3.m3.1.1.5">𝑎</ci><ci id="S6.p1.3.m3.1.1.6.cmml" xref="S6.p1.3.m3.1.1.6">𝑔</ci><ci id="S6.p1.3.m3.1.1.7.cmml" xref="S6.p1.3.m3.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.p1.3.m3.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>”), one which
is equipped with a maximum of <math alttext="64" class="ltx_Math" display="inline" id="S6.p1.4.m4.1"><semantics id="S6.p1.4.m4.1a"><mn id="S6.p1.4.m4.1.1" xref="S6.p1.4.m4.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S6.p1.4.m4.1b"><cn id="S6.p1.4.m4.1.1.cmml" type="integer" xref="S6.p1.4.m4.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.4.m4.1c">64</annotation><annotation encoding="application/x-llamapun" id="S6.p1.4.m4.1d">64</annotation></semantics></math> CPU cores in our small-scale PoC
prototype.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.6.2.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.1.1">Performance and Cost-Effectiveness of <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.1.1.m1.1"><semantics id="S6.SS1.1.1.m1.1b"><mrow id="S6.SS1.1.1.m1.1.1" xref="S6.SS1.1.1.m1.1.1.cmml"><mi id="S6.SS1.1.1.m1.1.1.2" xref="S6.SS1.1.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS1.1.1.m1.1.1.1" xref="S6.SS1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.1.1.m1.1.1.3" xref="S6.SS1.1.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS1.1.1.m1.1.1.1b" xref="S6.SS1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.1.1.m1.1.1.4" xref="S6.SS1.1.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS1.1.1.m1.1.1.1c" xref="S6.SS1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.1.1.m1.1.1.5" xref="S6.SS1.1.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS1.1.1.m1.1.1.1d" xref="S6.SS1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.1.1.m1.1.1.6" xref="S6.SS1.1.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS1.1.1.m1.1.1.1e" xref="S6.SS1.1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.1.1.m1.1.1.7" xref="S6.SS1.1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.1.1.m1.1c"><apply id="S6.SS1.1.1.m1.1.1.cmml" xref="S6.SS1.1.1.m1.1.1"><times id="S6.SS1.1.1.m1.1.1.1.cmml" xref="S6.SS1.1.1.m1.1.1.1"></times><ci id="S6.SS1.1.1.m1.1.1.2.cmml" xref="S6.SS1.1.1.m1.1.1.2">𝑃</ci><ci id="S6.SS1.1.1.m1.1.1.3.cmml" xref="S6.SS1.1.1.m1.1.1.3">𝑟</ci><ci id="S6.SS1.1.1.m1.1.1.4.cmml" xref="S6.SS1.1.1.m1.1.1.4">𝑒</ci><ci id="S6.SS1.1.1.m1.1.1.5.cmml" xref="S6.SS1.1.1.m1.1.1.5">𝑆</ci><ci id="S6.SS1.1.1.m1.1.1.6.cmml" xref="S6.SS1.1.1.m1.1.1.6">𝑡</ci><ci id="S6.SS1.1.1.m1.1.1.7.cmml" xref="S6.SS1.1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.1.1.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.1.1.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (PoC)</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.11"><span class="ltx_text ltx_font_bold" id="S6.SS1.p1.11.1">Throughput.</span> Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F11" title="Figure 11 ‣ VI-A Performance and Cost-Effectiveness of P⁢r⁢e⁢S⁢t⁢o (PoC) ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">11</span></a> compares the data
preprocessing throughput of <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p1.1.m1.1"><semantics id="S6.SS1.p1.1.m1.1a"><mrow id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mi id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.1.m1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS1.p1.1.m1.1.1.1a" xref="S6.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.1.m1.1.1.4" xref="S6.SS1.p1.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS1.p1.1.m1.1.1.1b" xref="S6.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.1.m1.1.1.5" xref="S6.SS1.p1.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS1.p1.1.m1.1.1.1c" xref="S6.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.1.m1.1.1.6" xref="S6.SS1.p1.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS1.p1.1.m1.1.1.1d" xref="S6.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.1.m1.1.1.7" xref="S6.SS1.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><times id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1"></times><ci id="S6.SS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.2">𝑃</ci><ci id="S6.SS1.p1.1.m1.1.1.3.cmml" xref="S6.SS1.p1.1.m1.1.1.3">𝑟</ci><ci id="S6.SS1.p1.1.m1.1.1.4.cmml" xref="S6.SS1.p1.1.m1.1.1.4">𝑒</ci><ci id="S6.SS1.p1.1.m1.1.1.5.cmml" xref="S6.SS1.p1.1.m1.1.1.5">𝑆</ci><ci id="S6.SS1.p1.1.m1.1.1.6.cmml" xref="S6.SS1.p1.1.m1.1.1.6">𝑡</ci><ci id="S6.SS1.p1.1.m1.1.1.7.cmml" xref="S6.SS1.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> vs. <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p1.2.m2.1"><semantics id="S6.SS1.p1.2.m2.1a"><mrow id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml"><mi id="S6.SS1.p1.2.m2.1.1.2" xref="S6.SS1.p1.2.m2.1.1.2.cmml">D</mi><mo id="S6.SS1.p1.2.m2.1.1.1" xref="S6.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.2.m2.1.1.3" xref="S6.SS1.p1.2.m2.1.1.3.cmml">i</mi><mo id="S6.SS1.p1.2.m2.1.1.1a" xref="S6.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.2.m2.1.1.4" xref="S6.SS1.p1.2.m2.1.1.4.cmml">s</mi><mo id="S6.SS1.p1.2.m2.1.1.1b" xref="S6.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.2.m2.1.1.5" xref="S6.SS1.p1.2.m2.1.1.5.cmml">a</mi><mo id="S6.SS1.p1.2.m2.1.1.1c" xref="S6.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.2.m2.1.1.6" xref="S6.SS1.p1.2.m2.1.1.6.cmml">g</mi><mo id="S6.SS1.p1.2.m2.1.1.1d" xref="S6.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.2.m2.1.1.7" xref="S6.SS1.p1.2.m2.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><apply id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1"><times id="S6.SS1.p1.2.m2.1.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1.1"></times><ci id="S6.SS1.p1.2.m2.1.1.2.cmml" xref="S6.SS1.p1.2.m2.1.1.2">𝐷</ci><ci id="S6.SS1.p1.2.m2.1.1.3.cmml" xref="S6.SS1.p1.2.m2.1.1.3">𝑖</ci><ci id="S6.SS1.p1.2.m2.1.1.4.cmml" xref="S6.SS1.p1.2.m2.1.1.4">𝑠</ci><ci id="S6.SS1.p1.2.m2.1.1.5.cmml" xref="S6.SS1.p1.2.m2.1.1.5">𝑎</ci><ci id="S6.SS1.p1.2.m2.1.1.6.cmml" xref="S6.SS1.p1.2.m2.1.1.6">𝑔</ci><ci id="S6.SS1.p1.2.m2.1.1.7.cmml" xref="S6.SS1.p1.2.m2.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.2.m2.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>. As depicted,
a single SmartSSD device (<math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p1.3.m3.1"><semantics id="S6.SS1.p1.3.m3.1a"><mrow id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml"><mi id="S6.SS1.p1.3.m3.1.1.2" xref="S6.SS1.p1.3.m3.1.1.2.cmml">P</mi><mo id="S6.SS1.p1.3.m3.1.1.1" xref="S6.SS1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.3.m3.1.1.3" xref="S6.SS1.p1.3.m3.1.1.3.cmml">r</mi><mo id="S6.SS1.p1.3.m3.1.1.1a" xref="S6.SS1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.3.m3.1.1.4" xref="S6.SS1.p1.3.m3.1.1.4.cmml">e</mi><mo id="S6.SS1.p1.3.m3.1.1.1b" xref="S6.SS1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.3.m3.1.1.5" xref="S6.SS1.p1.3.m3.1.1.5.cmml">S</mi><mo id="S6.SS1.p1.3.m3.1.1.1c" xref="S6.SS1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.3.m3.1.1.6" xref="S6.SS1.p1.3.m3.1.1.6.cmml">t</mi><mo id="S6.SS1.p1.3.m3.1.1.1d" xref="S6.SS1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.3.m3.1.1.7" xref="S6.SS1.p1.3.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><apply id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1"><times id="S6.SS1.p1.3.m3.1.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1.1"></times><ci id="S6.SS1.p1.3.m3.1.1.2.cmml" xref="S6.SS1.p1.3.m3.1.1.2">𝑃</ci><ci id="S6.SS1.p1.3.m3.1.1.3.cmml" xref="S6.SS1.p1.3.m3.1.1.3">𝑟</ci><ci id="S6.SS1.p1.3.m3.1.1.4.cmml" xref="S6.SS1.p1.3.m3.1.1.4">𝑒</ci><ci id="S6.SS1.p1.3.m3.1.1.5.cmml" xref="S6.SS1.p1.3.m3.1.1.5">𝑆</ci><ci id="S6.SS1.p1.3.m3.1.1.6.cmml" xref="S6.SS1.p1.3.m3.1.1.6">𝑡</ci><ci id="S6.SS1.p1.3.m3.1.1.7.cmml" xref="S6.SS1.p1.3.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.3.m3.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>) consistently outperforms <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p1.4.m4.1"><semantics id="S6.SS1.p1.4.m4.1a"><mrow id="S6.SS1.p1.4.m4.1.1" xref="S6.SS1.p1.4.m4.1.1.cmml"><mi id="S6.SS1.p1.4.m4.1.1.2" xref="S6.SS1.p1.4.m4.1.1.2.cmml">D</mi><mo id="S6.SS1.p1.4.m4.1.1.1" xref="S6.SS1.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.4.m4.1.1.3" xref="S6.SS1.p1.4.m4.1.1.3.cmml">i</mi><mo id="S6.SS1.p1.4.m4.1.1.1a" xref="S6.SS1.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.4.m4.1.1.4" xref="S6.SS1.p1.4.m4.1.1.4.cmml">s</mi><mo id="S6.SS1.p1.4.m4.1.1.1b" xref="S6.SS1.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.4.m4.1.1.5" xref="S6.SS1.p1.4.m4.1.1.5.cmml">a</mi><mo id="S6.SS1.p1.4.m4.1.1.1c" xref="S6.SS1.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.4.m4.1.1.6" xref="S6.SS1.p1.4.m4.1.1.6.cmml">g</mi><mo id="S6.SS1.p1.4.m4.1.1.1d" xref="S6.SS1.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.4.m4.1.1.7" xref="S6.SS1.p1.4.m4.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.4.m4.1b"><apply id="S6.SS1.p1.4.m4.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1"><times id="S6.SS1.p1.4.m4.1.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1.1"></times><ci id="S6.SS1.p1.4.m4.1.1.2.cmml" xref="S6.SS1.p1.4.m4.1.1.2">𝐷</ci><ci id="S6.SS1.p1.4.m4.1.1.3.cmml" xref="S6.SS1.p1.4.m4.1.1.3">𝑖</ci><ci id="S6.SS1.p1.4.m4.1.1.4.cmml" xref="S6.SS1.p1.4.m4.1.1.4">𝑠</ci><ci id="S6.SS1.p1.4.m4.1.1.5.cmml" xref="S6.SS1.p1.4.m4.1.1.5">𝑎</ci><ci id="S6.SS1.p1.4.m4.1.1.6.cmml" xref="S6.SS1.p1.4.m4.1.1.6">𝑔</ci><ci id="S6.SS1.p1.4.m4.1.1.7.cmml" xref="S6.SS1.p1.4.m4.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.4.m4.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.4.m4.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> even with <math alttext="32" class="ltx_Math" display="inline" id="S6.SS1.p1.5.m5.1"><semantics id="S6.SS1.p1.5.m5.1a"><mn id="S6.SS1.p1.5.m5.1.1" xref="S6.SS1.p1.5.m5.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.5.m5.1b"><cn id="S6.SS1.p1.5.m5.1.1.cmml" type="integer" xref="S6.SS1.p1.5.m5.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.5.m5.1c">32</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.5.m5.1d">32</annotation></semantics></math> CPU
cores (i.e., a single CPU node) and demonstrates the benefits of our ISP solution.
Since <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p1.6.m6.1"><semantics id="S6.SS1.p1.6.m6.1a"><mrow id="S6.SS1.p1.6.m6.1.1" xref="S6.SS1.p1.6.m6.1.1.cmml"><mi id="S6.SS1.p1.6.m6.1.1.2" xref="S6.SS1.p1.6.m6.1.1.2.cmml">D</mi><mo id="S6.SS1.p1.6.m6.1.1.1" xref="S6.SS1.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.6.m6.1.1.3" xref="S6.SS1.p1.6.m6.1.1.3.cmml">i</mi><mo id="S6.SS1.p1.6.m6.1.1.1a" xref="S6.SS1.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.6.m6.1.1.4" xref="S6.SS1.p1.6.m6.1.1.4.cmml">s</mi><mo id="S6.SS1.p1.6.m6.1.1.1b" xref="S6.SS1.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.6.m6.1.1.5" xref="S6.SS1.p1.6.m6.1.1.5.cmml">a</mi><mo id="S6.SS1.p1.6.m6.1.1.1c" xref="S6.SS1.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.6.m6.1.1.6" xref="S6.SS1.p1.6.m6.1.1.6.cmml">g</mi><mo id="S6.SS1.p1.6.m6.1.1.1d" xref="S6.SS1.p1.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.6.m6.1.1.7" xref="S6.SS1.p1.6.m6.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.6.m6.1b"><apply id="S6.SS1.p1.6.m6.1.1.cmml" xref="S6.SS1.p1.6.m6.1.1"><times id="S6.SS1.p1.6.m6.1.1.1.cmml" xref="S6.SS1.p1.6.m6.1.1.1"></times><ci id="S6.SS1.p1.6.m6.1.1.2.cmml" xref="S6.SS1.p1.6.m6.1.1.2">𝐷</ci><ci id="S6.SS1.p1.6.m6.1.1.3.cmml" xref="S6.SS1.p1.6.m6.1.1.3">𝑖</ci><ci id="S6.SS1.p1.6.m6.1.1.4.cmml" xref="S6.SS1.p1.6.m6.1.1.4">𝑠</ci><ci id="S6.SS1.p1.6.m6.1.1.5.cmml" xref="S6.SS1.p1.6.m6.1.1.5">𝑎</ci><ci id="S6.SS1.p1.6.m6.1.1.6.cmml" xref="S6.SS1.p1.6.m6.1.1.6">𝑔</ci><ci id="S6.SS1.p1.6.m6.1.1.7.cmml" xref="S6.SS1.p1.6.m6.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.6.m6.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.6.m6.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>’s performance scales well to the number of CPU cores (workers)
utilized, allocating more CPU cores can still match the throughput provided with <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p1.7.m7.1"><semantics id="S6.SS1.p1.7.m7.1a"><mrow id="S6.SS1.p1.7.m7.1.1" xref="S6.SS1.p1.7.m7.1.1.cmml"><mi id="S6.SS1.p1.7.m7.1.1.2" xref="S6.SS1.p1.7.m7.1.1.2.cmml">P</mi><mo id="S6.SS1.p1.7.m7.1.1.1" xref="S6.SS1.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.7.m7.1.1.3" xref="S6.SS1.p1.7.m7.1.1.3.cmml">r</mi><mo id="S6.SS1.p1.7.m7.1.1.1a" xref="S6.SS1.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.7.m7.1.1.4" xref="S6.SS1.p1.7.m7.1.1.4.cmml">e</mi><mo id="S6.SS1.p1.7.m7.1.1.1b" xref="S6.SS1.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.7.m7.1.1.5" xref="S6.SS1.p1.7.m7.1.1.5.cmml">S</mi><mo id="S6.SS1.p1.7.m7.1.1.1c" xref="S6.SS1.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.7.m7.1.1.6" xref="S6.SS1.p1.7.m7.1.1.6.cmml">t</mi><mo id="S6.SS1.p1.7.m7.1.1.1d" xref="S6.SS1.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.7.m7.1.1.7" xref="S6.SS1.p1.7.m7.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.7.m7.1b"><apply id="S6.SS1.p1.7.m7.1.1.cmml" xref="S6.SS1.p1.7.m7.1.1"><times id="S6.SS1.p1.7.m7.1.1.1.cmml" xref="S6.SS1.p1.7.m7.1.1.1"></times><ci id="S6.SS1.p1.7.m7.1.1.2.cmml" xref="S6.SS1.p1.7.m7.1.1.2">𝑃</ci><ci id="S6.SS1.p1.7.m7.1.1.3.cmml" xref="S6.SS1.p1.7.m7.1.1.3">𝑟</ci><ci id="S6.SS1.p1.7.m7.1.1.4.cmml" xref="S6.SS1.p1.7.m7.1.1.4">𝑒</ci><ci id="S6.SS1.p1.7.m7.1.1.5.cmml" xref="S6.SS1.p1.7.m7.1.1.5">𝑆</ci><ci id="S6.SS1.p1.7.m7.1.1.6.cmml" xref="S6.SS1.p1.7.m7.1.1.6">𝑡</ci><ci id="S6.SS1.p1.7.m7.1.1.7.cmml" xref="S6.SS1.p1.7.m7.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.7.m7.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.7.m7.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> albeit at a proportional increase in cost (e.g, Disagg(64) using two CPU nodes (<math alttext="64" class="ltx_Math" display="inline" id="S6.SS1.p1.8.m8.1"><semantics id="S6.SS1.p1.8.m8.1a"><mn id="S6.SS1.p1.8.m8.1.1" xref="S6.SS1.p1.8.m8.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.8.m8.1b"><cn id="S6.SS1.p1.8.m8.1.1.cmml" type="integer" xref="S6.SS1.p1.8.m8.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.8.m8.1c">64</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.8.m8.1d">64</annotation></semantics></math> cores) is able to slightly outperform <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p1.9.m9.1"><semantics id="S6.SS1.p1.9.m9.1a"><mrow id="S6.SS1.p1.9.m9.1.1" xref="S6.SS1.p1.9.m9.1.1.cmml"><mi id="S6.SS1.p1.9.m9.1.1.2" xref="S6.SS1.p1.9.m9.1.1.2.cmml">P</mi><mo id="S6.SS1.p1.9.m9.1.1.1" xref="S6.SS1.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.9.m9.1.1.3" xref="S6.SS1.p1.9.m9.1.1.3.cmml">r</mi><mo id="S6.SS1.p1.9.m9.1.1.1a" xref="S6.SS1.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.9.m9.1.1.4" xref="S6.SS1.p1.9.m9.1.1.4.cmml">e</mi><mo id="S6.SS1.p1.9.m9.1.1.1b" xref="S6.SS1.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.9.m9.1.1.5" xref="S6.SS1.p1.9.m9.1.1.5.cmml">S</mi><mo id="S6.SS1.p1.9.m9.1.1.1c" xref="S6.SS1.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.9.m9.1.1.6" xref="S6.SS1.p1.9.m9.1.1.6.cmml">t</mi><mo id="S6.SS1.p1.9.m9.1.1.1d" xref="S6.SS1.p1.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p1.9.m9.1.1.7" xref="S6.SS1.p1.9.m9.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.9.m9.1b"><apply id="S6.SS1.p1.9.m9.1.1.cmml" xref="S6.SS1.p1.9.m9.1.1"><times id="S6.SS1.p1.9.m9.1.1.1.cmml" xref="S6.SS1.p1.9.m9.1.1.1"></times><ci id="S6.SS1.p1.9.m9.1.1.2.cmml" xref="S6.SS1.p1.9.m9.1.1.2">𝑃</ci><ci id="S6.SS1.p1.9.m9.1.1.3.cmml" xref="S6.SS1.p1.9.m9.1.1.3">𝑟</ci><ci id="S6.SS1.p1.9.m9.1.1.4.cmml" xref="S6.SS1.p1.9.m9.1.1.4">𝑒</ci><ci id="S6.SS1.p1.9.m9.1.1.5.cmml" xref="S6.SS1.p1.9.m9.1.1.5">𝑆</ci><ci id="S6.SS1.p1.9.m9.1.1.6.cmml" xref="S6.SS1.p1.9.m9.1.1.6">𝑡</ci><ci id="S6.SS1.p1.9.m9.1.1.7.cmml" xref="S6.SS1.p1.9.m9.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.9.m9.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.9.m9.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> by average <math alttext="27\%" class="ltx_Math" display="inline" id="S6.SS1.p1.10.m10.1"><semantics id="S6.SS1.p1.10.m10.1a"><mrow id="S6.SS1.p1.10.m10.1.1" xref="S6.SS1.p1.10.m10.1.1.cmml"><mn id="S6.SS1.p1.10.m10.1.1.2" xref="S6.SS1.p1.10.m10.1.1.2.cmml">27</mn><mo id="S6.SS1.p1.10.m10.1.1.1" xref="S6.SS1.p1.10.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.10.m10.1b"><apply id="S6.SS1.p1.10.m10.1.1.cmml" xref="S6.SS1.p1.10.m10.1.1"><csymbol cd="latexml" id="S6.SS1.p1.10.m10.1.1.1.cmml" xref="S6.SS1.p1.10.m10.1.1.1">percent</csymbol><cn id="S6.SS1.p1.10.m10.1.1.2.cmml" type="integer" xref="S6.SS1.p1.10.m10.1.1.2">27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.10.m10.1c">27\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.10.m10.1d">27 %</annotation></semantics></math> but with <math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S6.SS1.p1.11.m11.1"><semantics id="S6.SS1.p1.11.m11.1a"><mrow id="S6.SS1.p1.11.m11.1b"><mn id="S6.SS1.p1.11.m11.1.1">2</mn><mo id="S6.SS1.p1.11.m11.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p1.11.m11.1c">2\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.11.m11.1d">2 ×</annotation></semantics></math> higher cost).</p>
</div>
<figure class="ltx_figure" id="S6.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="S6.F11.g1" src="x11.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Preprocessing throughput of <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.F11.6.m1.1"><semantics id="S6.F11.6.m1.1b"><mrow id="S6.F11.6.m1.1.1" xref="S6.F11.6.m1.1.1.cmml"><mi id="S6.F11.6.m1.1.1.2" xref="S6.F11.6.m1.1.1.2.cmml">P</mi><mo id="S6.F11.6.m1.1.1.1" xref="S6.F11.6.m1.1.1.1.cmml">⁢</mo><mi id="S6.F11.6.m1.1.1.3" xref="S6.F11.6.m1.1.1.3.cmml">r</mi><mo id="S6.F11.6.m1.1.1.1b" xref="S6.F11.6.m1.1.1.1.cmml">⁢</mo><mi id="S6.F11.6.m1.1.1.4" xref="S6.F11.6.m1.1.1.4.cmml">e</mi><mo id="S6.F11.6.m1.1.1.1c" xref="S6.F11.6.m1.1.1.1.cmml">⁢</mo><mi id="S6.F11.6.m1.1.1.5" xref="S6.F11.6.m1.1.1.5.cmml">S</mi><mo id="S6.F11.6.m1.1.1.1d" xref="S6.F11.6.m1.1.1.1.cmml">⁢</mo><mi id="S6.F11.6.m1.1.1.6" xref="S6.F11.6.m1.1.1.6.cmml">t</mi><mo id="S6.F11.6.m1.1.1.1e" xref="S6.F11.6.m1.1.1.1.cmml">⁢</mo><mi id="S6.F11.6.m1.1.1.7" xref="S6.F11.6.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F11.6.m1.1c"><apply id="S6.F11.6.m1.1.1.cmml" xref="S6.F11.6.m1.1.1"><times id="S6.F11.6.m1.1.1.1.cmml" xref="S6.F11.6.m1.1.1.1"></times><ci id="S6.F11.6.m1.1.1.2.cmml" xref="S6.F11.6.m1.1.1.2">𝑃</ci><ci id="S6.F11.6.m1.1.1.3.cmml" xref="S6.F11.6.m1.1.1.3">𝑟</ci><ci id="S6.F11.6.m1.1.1.4.cmml" xref="S6.F11.6.m1.1.1.4">𝑒</ci><ci id="S6.F11.6.m1.1.1.5.cmml" xref="S6.F11.6.m1.1.1.5">𝑆</ci><ci id="S6.F11.6.m1.1.1.6.cmml" xref="S6.F11.6.m1.1.1.6">𝑡</ci><ci id="S6.F11.6.m1.1.1.7.cmml" xref="S6.F11.6.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F11.6.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.F11.6.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (single SmartSSD) vs. <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.F11.7.m2.1"><semantics id="S6.F11.7.m2.1b"><mrow id="S6.F11.7.m2.1.1" xref="S6.F11.7.m2.1.1.cmml"><mi id="S6.F11.7.m2.1.1.2" xref="S6.F11.7.m2.1.1.2.cmml">D</mi><mo id="S6.F11.7.m2.1.1.1" xref="S6.F11.7.m2.1.1.1.cmml">⁢</mo><mi id="S6.F11.7.m2.1.1.3" xref="S6.F11.7.m2.1.1.3.cmml">i</mi><mo id="S6.F11.7.m2.1.1.1b" xref="S6.F11.7.m2.1.1.1.cmml">⁢</mo><mi id="S6.F11.7.m2.1.1.4" xref="S6.F11.7.m2.1.1.4.cmml">s</mi><mo id="S6.F11.7.m2.1.1.1c" xref="S6.F11.7.m2.1.1.1.cmml">⁢</mo><mi id="S6.F11.7.m2.1.1.5" xref="S6.F11.7.m2.1.1.5.cmml">a</mi><mo id="S6.F11.7.m2.1.1.1d" xref="S6.F11.7.m2.1.1.1.cmml">⁢</mo><mi id="S6.F11.7.m2.1.1.6" xref="S6.F11.7.m2.1.1.6.cmml">g</mi><mo id="S6.F11.7.m2.1.1.1e" xref="S6.F11.7.m2.1.1.1.cmml">⁢</mo><mi id="S6.F11.7.m2.1.1.7" xref="S6.F11.7.m2.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F11.7.m2.1c"><apply id="S6.F11.7.m2.1.1.cmml" xref="S6.F11.7.m2.1.1"><times id="S6.F11.7.m2.1.1.1.cmml" xref="S6.F11.7.m2.1.1.1"></times><ci id="S6.F11.7.m2.1.1.2.cmml" xref="S6.F11.7.m2.1.1.2">𝐷</ci><ci id="S6.F11.7.m2.1.1.3.cmml" xref="S6.F11.7.m2.1.1.3">𝑖</ci><ci id="S6.F11.7.m2.1.1.4.cmml" xref="S6.F11.7.m2.1.1.4">𝑠</ci><ci id="S6.F11.7.m2.1.1.5.cmml" xref="S6.F11.7.m2.1.1.5">𝑎</ci><ci id="S6.F11.7.m2.1.1.6.cmml" xref="S6.F11.7.m2.1.1.6">𝑔</ci><ci id="S6.F11.7.m2.1.1.7.cmml" xref="S6.F11.7.m2.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F11.7.m2.1d">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.F11.7.m2.1e">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>. Disagg(<math alttext="N" class="ltx_Math" display="inline" id="S6.F11.8.m3.1"><semantics id="S6.F11.8.m3.1b"><mi id="S6.F11.8.m3.1.1" xref="S6.F11.8.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.F11.8.m3.1c"><ci id="S6.F11.8.m3.1.1.cmml" xref="S6.F11.8.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F11.8.m3.1d">N</annotation><annotation encoding="application/x-llamapun" id="S6.F11.8.m3.1e">italic_N</annotation></semantics></math>) is a design point executing with <math alttext="N" class="ltx_Math" display="inline" id="S6.F11.9.m4.1"><semantics id="S6.F11.9.m4.1b"><mi id="S6.F11.9.m4.1.1" xref="S6.F11.9.m4.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.F11.9.m4.1c"><ci id="S6.F11.9.m4.1.1.cmml" xref="S6.F11.9.m4.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F11.9.m4.1d">N</annotation><annotation encoding="application/x-llamapun" id="S6.F11.9.m4.1e">italic_N</annotation></semantics></math> preprocessing workers using <math alttext="N" class="ltx_Math" display="inline" id="S6.F11.10.m5.1"><semantics id="S6.F11.10.m5.1b"><mi id="S6.F11.10.m5.1.1" xref="S6.F11.10.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S6.F11.10.m5.1c"><ci id="S6.F11.10.m5.1.1.cmml" xref="S6.F11.10.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F11.10.m5.1d">N</annotation><annotation encoding="application/x-llamapun" id="S6.F11.10.m5.1e">italic_N</annotation></semantics></math> CPU cores. Results are normalized to Disagg(1).</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="136" id="S6.F12.g1" src="x12.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>(Left axis) Data preprocessing time to generate
a single mini-batch using a single preprocessing worker. Latency is broken down into key steps of data preprocessing.
(Right axis) <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.F12.3.m1.1"><semantics id="S6.F12.3.m1.1b"><mrow id="S6.F12.3.m1.1.1" xref="S6.F12.3.m1.1.1.cmml"><mi id="S6.F12.3.m1.1.1.2" xref="S6.F12.3.m1.1.1.2.cmml">P</mi><mo id="S6.F12.3.m1.1.1.1" xref="S6.F12.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F12.3.m1.1.1.3" xref="S6.F12.3.m1.1.1.3.cmml">r</mi><mo id="S6.F12.3.m1.1.1.1b" xref="S6.F12.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F12.3.m1.1.1.4" xref="S6.F12.3.m1.1.1.4.cmml">e</mi><mo id="S6.F12.3.m1.1.1.1c" xref="S6.F12.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F12.3.m1.1.1.5" xref="S6.F12.3.m1.1.1.5.cmml">S</mi><mo id="S6.F12.3.m1.1.1.1d" xref="S6.F12.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F12.3.m1.1.1.6" xref="S6.F12.3.m1.1.1.6.cmml">t</mi><mo id="S6.F12.3.m1.1.1.1e" xref="S6.F12.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F12.3.m1.1.1.7" xref="S6.F12.3.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F12.3.m1.1c"><apply id="S6.F12.3.m1.1.1.cmml" xref="S6.F12.3.m1.1.1"><times id="S6.F12.3.m1.1.1.1.cmml" xref="S6.F12.3.m1.1.1.1"></times><ci id="S6.F12.3.m1.1.1.2.cmml" xref="S6.F12.3.m1.1.1.2">𝑃</ci><ci id="S6.F12.3.m1.1.1.3.cmml" xref="S6.F12.3.m1.1.1.3">𝑟</ci><ci id="S6.F12.3.m1.1.1.4.cmml" xref="S6.F12.3.m1.1.1.4">𝑒</ci><ci id="S6.F12.3.m1.1.1.5.cmml" xref="S6.F12.3.m1.1.1.5">𝑆</ci><ci id="S6.F12.3.m1.1.1.6.cmml" xref="S6.F12.3.m1.1.1.6">𝑡</ci><ci id="S6.F12.3.m1.1.1.7.cmml" xref="S6.F12.3.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F12.3.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.F12.3.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s end-to-end speedup for preprocessing. All results are normalized to <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.F12.4.m2.1"><semantics id="S6.F12.4.m2.1b"><mrow id="S6.F12.4.m2.1.1" xref="S6.F12.4.m2.1.1.cmml"><mi id="S6.F12.4.m2.1.1.2" xref="S6.F12.4.m2.1.1.2.cmml">D</mi><mo id="S6.F12.4.m2.1.1.1" xref="S6.F12.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F12.4.m2.1.1.3" xref="S6.F12.4.m2.1.1.3.cmml">i</mi><mo id="S6.F12.4.m2.1.1.1b" xref="S6.F12.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F12.4.m2.1.1.4" xref="S6.F12.4.m2.1.1.4.cmml">s</mi><mo id="S6.F12.4.m2.1.1.1c" xref="S6.F12.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F12.4.m2.1.1.5" xref="S6.F12.4.m2.1.1.5.cmml">a</mi><mo id="S6.F12.4.m2.1.1.1d" xref="S6.F12.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F12.4.m2.1.1.6" xref="S6.F12.4.m2.1.1.6.cmml">g</mi><mo id="S6.F12.4.m2.1.1.1e" xref="S6.F12.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F12.4.m2.1.1.7" xref="S6.F12.4.m2.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F12.4.m2.1c"><apply id="S6.F12.4.m2.1.1.cmml" xref="S6.F12.4.m2.1.1"><times id="S6.F12.4.m2.1.1.1.cmml" xref="S6.F12.4.m2.1.1.1"></times><ci id="S6.F12.4.m2.1.1.2.cmml" xref="S6.F12.4.m2.1.1.2">𝐷</ci><ci id="S6.F12.4.m2.1.1.3.cmml" xref="S6.F12.4.m2.1.1.3">𝑖</ci><ci id="S6.F12.4.m2.1.1.4.cmml" xref="S6.F12.4.m2.1.1.4">𝑠</ci><ci id="S6.F12.4.m2.1.1.5.cmml" xref="S6.F12.4.m2.1.1.5">𝑎</ci><ci id="S6.F12.4.m2.1.1.6.cmml" xref="S6.F12.4.m2.1.1.6">𝑔</ci><ci id="S6.F12.4.m2.1.1.7.cmml" xref="S6.F12.4.m2.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F12.4.m2.1d">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.F12.4.m2.1e">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>.</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="106" id="S6.F13.g1" src="x13.png" width="357"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Aggregate latency incurred during any RPC calls executed for inter-node communication during the course of data preprocessing.
</figcaption>
</figure>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.10"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.10.1">Latency.</span> To better highlight where <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p2.1.m1.1"><semantics id="S6.SS1.p2.1.m1.1a"><mrow id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml"><mi id="S6.SS1.p2.1.m1.1.1.2" xref="S6.SS1.p2.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS1.p2.1.m1.1.1.1" xref="S6.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.1.m1.1.1.3" xref="S6.SS1.p2.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS1.p2.1.m1.1.1.1a" xref="S6.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.1.m1.1.1.4" xref="S6.SS1.p2.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS1.p2.1.m1.1.1.1b" xref="S6.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.1.m1.1.1.5" xref="S6.SS1.p2.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS1.p2.1.m1.1.1.1c" xref="S6.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.1.m1.1.1.6" xref="S6.SS1.p2.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS1.p2.1.m1.1.1.1d" xref="S6.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.1.m1.1.1.7" xref="S6.SS1.p2.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><apply id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1"><times id="S6.SS1.p2.1.m1.1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1.1"></times><ci id="S6.SS1.p2.1.m1.1.1.2.cmml" xref="S6.SS1.p2.1.m1.1.1.2">𝑃</ci><ci id="S6.SS1.p2.1.m1.1.1.3.cmml" xref="S6.SS1.p2.1.m1.1.1.3">𝑟</ci><ci id="S6.SS1.p2.1.m1.1.1.4.cmml" xref="S6.SS1.p2.1.m1.1.1.4">𝑒</ci><ci id="S6.SS1.p2.1.m1.1.1.5.cmml" xref="S6.SS1.p2.1.m1.1.1.5">𝑆</ci><ci id="S6.SS1.p2.1.m1.1.1.6.cmml" xref="S6.SS1.p2.1.m1.1.1.6">𝑡</ci><ci id="S6.SS1.p2.1.m1.1.1.7.cmml" xref="S6.SS1.p2.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s speedup comes
from, Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F12" title="Figure 12 ‣ VI-A Performance and Cost-Effectiveness of P⁢r⁢e⁢S⁢t⁢o (PoC) ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">12</span></a> compares the latency to
generate a single mini-batch input using a single preprocessing worker using
<math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p2.2.m2.1"><semantics id="S6.SS1.p2.2.m2.1a"><mrow id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml"><mi id="S6.SS1.p2.2.m2.1.1.2" xref="S6.SS1.p2.2.m2.1.1.2.cmml">D</mi><mo id="S6.SS1.p2.2.m2.1.1.1" xref="S6.SS1.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.2.m2.1.1.3" xref="S6.SS1.p2.2.m2.1.1.3.cmml">i</mi><mo id="S6.SS1.p2.2.m2.1.1.1a" xref="S6.SS1.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.2.m2.1.1.4" xref="S6.SS1.p2.2.m2.1.1.4.cmml">s</mi><mo id="S6.SS1.p2.2.m2.1.1.1b" xref="S6.SS1.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.2.m2.1.1.5" xref="S6.SS1.p2.2.m2.1.1.5.cmml">a</mi><mo id="S6.SS1.p2.2.m2.1.1.1c" xref="S6.SS1.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.2.m2.1.1.6" xref="S6.SS1.p2.2.m2.1.1.6.cmml">g</mi><mo id="S6.SS1.p2.2.m2.1.1.1d" xref="S6.SS1.p2.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.2.m2.1.1.7" xref="S6.SS1.p2.2.m2.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><apply id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1"><times id="S6.SS1.p2.2.m2.1.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1.1"></times><ci id="S6.SS1.p2.2.m2.1.1.2.cmml" xref="S6.SS1.p2.2.m2.1.1.2">𝐷</ci><ci id="S6.SS1.p2.2.m2.1.1.3.cmml" xref="S6.SS1.p2.2.m2.1.1.3">𝑖</ci><ci id="S6.SS1.p2.2.m2.1.1.4.cmml" xref="S6.SS1.p2.2.m2.1.1.4">𝑠</ci><ci id="S6.SS1.p2.2.m2.1.1.5.cmml" xref="S6.SS1.p2.2.m2.1.1.5">𝑎</ci><ci id="S6.SS1.p2.2.m2.1.1.6.cmml" xref="S6.SS1.p2.2.m2.1.1.6">𝑔</ci><ci id="S6.SS1.p2.2.m2.1.1.7.cmml" xref="S6.SS1.p2.2.m2.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.2.m2.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> and <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p2.3.m3.1"><semantics id="S6.SS1.p2.3.m3.1a"><mrow id="S6.SS1.p2.3.m3.1.1" xref="S6.SS1.p2.3.m3.1.1.cmml"><mi id="S6.SS1.p2.3.m3.1.1.2" xref="S6.SS1.p2.3.m3.1.1.2.cmml">P</mi><mo id="S6.SS1.p2.3.m3.1.1.1" xref="S6.SS1.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.3.m3.1.1.3" xref="S6.SS1.p2.3.m3.1.1.3.cmml">r</mi><mo id="S6.SS1.p2.3.m3.1.1.1a" xref="S6.SS1.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.3.m3.1.1.4" xref="S6.SS1.p2.3.m3.1.1.4.cmml">e</mi><mo id="S6.SS1.p2.3.m3.1.1.1b" xref="S6.SS1.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.3.m3.1.1.5" xref="S6.SS1.p2.3.m3.1.1.5.cmml">S</mi><mo id="S6.SS1.p2.3.m3.1.1.1c" xref="S6.SS1.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.3.m3.1.1.6" xref="S6.SS1.p2.3.m3.1.1.6.cmml">t</mi><mo id="S6.SS1.p2.3.m3.1.1.1d" xref="S6.SS1.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.3.m3.1.1.7" xref="S6.SS1.p2.3.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.1b"><apply id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1"><times id="S6.SS1.p2.3.m3.1.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1.1"></times><ci id="S6.SS1.p2.3.m3.1.1.2.cmml" xref="S6.SS1.p2.3.m3.1.1.2">𝑃</ci><ci id="S6.SS1.p2.3.m3.1.1.3.cmml" xref="S6.SS1.p2.3.m3.1.1.3">𝑟</ci><ci id="S6.SS1.p2.3.m3.1.1.4.cmml" xref="S6.SS1.p2.3.m3.1.1.4">𝑒</ci><ci id="S6.SS1.p2.3.m3.1.1.5.cmml" xref="S6.SS1.p2.3.m3.1.1.5">𝑆</ci><ci id="S6.SS1.p2.3.m3.1.1.6.cmml" xref="S6.SS1.p2.3.m3.1.1.6">𝑡</ci><ci id="S6.SS1.p2.3.m3.1.1.7.cmml" xref="S6.SS1.p2.3.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.3.m3.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>. The latency breakdown focuses on the key
steps undertaken during preprocessing.
In the baseline <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p2.4.m4.1"><semantics id="S6.SS1.p2.4.m4.1a"><mrow id="S6.SS1.p2.4.m4.1.1" xref="S6.SS1.p2.4.m4.1.1.cmml"><mi id="S6.SS1.p2.4.m4.1.1.2" xref="S6.SS1.p2.4.m4.1.1.2.cmml">D</mi><mo id="S6.SS1.p2.4.m4.1.1.1" xref="S6.SS1.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.4.m4.1.1.3" xref="S6.SS1.p2.4.m4.1.1.3.cmml">i</mi><mo id="S6.SS1.p2.4.m4.1.1.1a" xref="S6.SS1.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.4.m4.1.1.4" xref="S6.SS1.p2.4.m4.1.1.4.cmml">s</mi><mo id="S6.SS1.p2.4.m4.1.1.1b" xref="S6.SS1.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.4.m4.1.1.5" xref="S6.SS1.p2.4.m4.1.1.5.cmml">a</mi><mo id="S6.SS1.p2.4.m4.1.1.1c" xref="S6.SS1.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.4.m4.1.1.6" xref="S6.SS1.p2.4.m4.1.1.6.cmml">g</mi><mo id="S6.SS1.p2.4.m4.1.1.1d" xref="S6.SS1.p2.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.4.m4.1.1.7" xref="S6.SS1.p2.4.m4.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.4.m4.1b"><apply id="S6.SS1.p2.4.m4.1.1.cmml" xref="S6.SS1.p2.4.m4.1.1"><times id="S6.SS1.p2.4.m4.1.1.1.cmml" xref="S6.SS1.p2.4.m4.1.1.1"></times><ci id="S6.SS1.p2.4.m4.1.1.2.cmml" xref="S6.SS1.p2.4.m4.1.1.2">𝐷</ci><ci id="S6.SS1.p2.4.m4.1.1.3.cmml" xref="S6.SS1.p2.4.m4.1.1.3">𝑖</ci><ci id="S6.SS1.p2.4.m4.1.1.4.cmml" xref="S6.SS1.p2.4.m4.1.1.4">𝑠</ci><ci id="S6.SS1.p2.4.m4.1.1.5.cmml" xref="S6.SS1.p2.4.m4.1.1.5">𝑎</ci><ci id="S6.SS1.p2.4.m4.1.1.6.cmml" xref="S6.SS1.p2.4.m4.1.1.6">𝑔</ci><ci id="S6.SS1.p2.4.m4.1.1.7.cmml" xref="S6.SS1.p2.4.m4.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.4.m4.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.4.m4.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>, the “Extract” step includes time to fetch encoded raw
feature data from the remote storage node and decode them. With <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p2.5.m5.1"><semantics id="S6.SS1.p2.5.m5.1a"><mrow id="S6.SS1.p2.5.m5.1.1" xref="S6.SS1.p2.5.m5.1.1.cmml"><mi id="S6.SS1.p2.5.m5.1.1.2" xref="S6.SS1.p2.5.m5.1.1.2.cmml">P</mi><mo id="S6.SS1.p2.5.m5.1.1.1" xref="S6.SS1.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.5.m5.1.1.3" xref="S6.SS1.p2.5.m5.1.1.3.cmml">r</mi><mo id="S6.SS1.p2.5.m5.1.1.1a" xref="S6.SS1.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.5.m5.1.1.4" xref="S6.SS1.p2.5.m5.1.1.4.cmml">e</mi><mo id="S6.SS1.p2.5.m5.1.1.1b" xref="S6.SS1.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.5.m5.1.1.5" xref="S6.SS1.p2.5.m5.1.1.5.cmml">S</mi><mo id="S6.SS1.p2.5.m5.1.1.1c" xref="S6.SS1.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.5.m5.1.1.6" xref="S6.SS1.p2.5.m5.1.1.6.cmml">t</mi><mo id="S6.SS1.p2.5.m5.1.1.1d" xref="S6.SS1.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.5.m5.1.1.7" xref="S6.SS1.p2.5.m5.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.5.m5.1b"><apply id="S6.SS1.p2.5.m5.1.1.cmml" xref="S6.SS1.p2.5.m5.1.1"><times id="S6.SS1.p2.5.m5.1.1.1.cmml" xref="S6.SS1.p2.5.m5.1.1.1"></times><ci id="S6.SS1.p2.5.m5.1.1.2.cmml" xref="S6.SS1.p2.5.m5.1.1.2">𝑃</ci><ci id="S6.SS1.p2.5.m5.1.1.3.cmml" xref="S6.SS1.p2.5.m5.1.1.3">𝑟</ci><ci id="S6.SS1.p2.5.m5.1.1.4.cmml" xref="S6.SS1.p2.5.m5.1.1.4">𝑒</ci><ci id="S6.SS1.p2.5.m5.1.1.5.cmml" xref="S6.SS1.p2.5.m5.1.1.5">𝑆</ci><ci id="S6.SS1.p2.5.m5.1.1.6.cmml" xref="S6.SS1.p2.5.m5.1.1.6">𝑡</ci><ci id="S6.SS1.p2.5.m5.1.1.7.cmml" xref="S6.SS1.p2.5.m5.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.5.m5.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.5.m5.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>,
the “Extract” step includes the P2P data transfer of the encoded raw
feature data from local SSD to the FPGA which is immediately followed by their decoding using our
dedicated decoder unit. Because the decoding algorithm is less
parallelizable than feature generation and normalization operations,
the reduction in the “Extract” step’s execution time is less pronounced, rendering
this step to account for an average 40.8% of the total
preprocessing time of <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p2.6.m6.1"><semantics id="S6.SS1.p2.6.m6.1a"><mrow id="S6.SS1.p2.6.m6.1.1" xref="S6.SS1.p2.6.m6.1.1.cmml"><mi id="S6.SS1.p2.6.m6.1.1.2" xref="S6.SS1.p2.6.m6.1.1.2.cmml">P</mi><mo id="S6.SS1.p2.6.m6.1.1.1" xref="S6.SS1.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.6.m6.1.1.3" xref="S6.SS1.p2.6.m6.1.1.3.cmml">r</mi><mo id="S6.SS1.p2.6.m6.1.1.1a" xref="S6.SS1.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.6.m6.1.1.4" xref="S6.SS1.p2.6.m6.1.1.4.cmml">e</mi><mo id="S6.SS1.p2.6.m6.1.1.1b" xref="S6.SS1.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.6.m6.1.1.5" xref="S6.SS1.p2.6.m6.1.1.5.cmml">S</mi><mo id="S6.SS1.p2.6.m6.1.1.1c" xref="S6.SS1.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.6.m6.1.1.6" xref="S6.SS1.p2.6.m6.1.1.6.cmml">t</mi><mo id="S6.SS1.p2.6.m6.1.1.1d" xref="S6.SS1.p2.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.6.m6.1.1.7" xref="S6.SS1.p2.6.m6.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.6.m6.1b"><apply id="S6.SS1.p2.6.m6.1.1.cmml" xref="S6.SS1.p2.6.m6.1.1"><times id="S6.SS1.p2.6.m6.1.1.1.cmml" xref="S6.SS1.p2.6.m6.1.1.1"></times><ci id="S6.SS1.p2.6.m6.1.1.2.cmml" xref="S6.SS1.p2.6.m6.1.1.2">𝑃</ci><ci id="S6.SS1.p2.6.m6.1.1.3.cmml" xref="S6.SS1.p2.6.m6.1.1.3">𝑟</ci><ci id="S6.SS1.p2.6.m6.1.1.4.cmml" xref="S6.SS1.p2.6.m6.1.1.4">𝑒</ci><ci id="S6.SS1.p2.6.m6.1.1.5.cmml" xref="S6.SS1.p2.6.m6.1.1.5">𝑆</ci><ci id="S6.SS1.p2.6.m6.1.1.6.cmml" xref="S6.SS1.p2.6.m6.1.1.6">𝑡</ci><ci id="S6.SS1.p2.6.m6.1.1.7.cmml" xref="S6.SS1.p2.6.m6.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.6.m6.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.6.m6.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>. Nonetheless, <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p2.7.m7.1"><semantics id="S6.SS1.p2.7.m7.1a"><mrow id="S6.SS1.p2.7.m7.1.1" xref="S6.SS1.p2.7.m7.1.1.cmml"><mi id="S6.SS1.p2.7.m7.1.1.2" xref="S6.SS1.p2.7.m7.1.1.2.cmml">P</mi><mo id="S6.SS1.p2.7.m7.1.1.1" xref="S6.SS1.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.7.m7.1.1.3" xref="S6.SS1.p2.7.m7.1.1.3.cmml">r</mi><mo id="S6.SS1.p2.7.m7.1.1.1a" xref="S6.SS1.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.7.m7.1.1.4" xref="S6.SS1.p2.7.m7.1.1.4.cmml">e</mi><mo id="S6.SS1.p2.7.m7.1.1.1b" xref="S6.SS1.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.7.m7.1.1.5" xref="S6.SS1.p2.7.m7.1.1.5.cmml">S</mi><mo id="S6.SS1.p2.7.m7.1.1.1c" xref="S6.SS1.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.7.m7.1.1.6" xref="S6.SS1.p2.7.m7.1.1.6.cmml">t</mi><mo id="S6.SS1.p2.7.m7.1.1.1d" xref="S6.SS1.p2.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.7.m7.1.1.7" xref="S6.SS1.p2.7.m7.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.7.m7.1b"><apply id="S6.SS1.p2.7.m7.1.1.cmml" xref="S6.SS1.p2.7.m7.1.1"><times id="S6.SS1.p2.7.m7.1.1.1.cmml" xref="S6.SS1.p2.7.m7.1.1.1"></times><ci id="S6.SS1.p2.7.m7.1.1.2.cmml" xref="S6.SS1.p2.7.m7.1.1.2">𝑃</ci><ci id="S6.SS1.p2.7.m7.1.1.3.cmml" xref="S6.SS1.p2.7.m7.1.1.3">𝑟</ci><ci id="S6.SS1.p2.7.m7.1.1.4.cmml" xref="S6.SS1.p2.7.m7.1.1.4">𝑒</ci><ci id="S6.SS1.p2.7.m7.1.1.5.cmml" xref="S6.SS1.p2.7.m7.1.1.5">𝑆</ci><ci id="S6.SS1.p2.7.m7.1.1.6.cmml" xref="S6.SS1.p2.7.m7.1.1.6">𝑡</ci><ci id="S6.SS1.p2.7.m7.1.1.7.cmml" xref="S6.SS1.p2.7.m7.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.7.m7.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.7.m7.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> provides significant
improvements in the performance of feature generation (Bucketize) and
normalization (SigridHash, Log), achieving an average <math alttext="9.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS1.p2.8.m8.1"><semantics id="S6.SS1.p2.8.m8.1a"><mrow id="S6.SS1.p2.8.m8.1b"><mn id="S6.SS1.p2.8.m8.1.1">9.6</mn><mo id="S6.SS1.p2.8.m8.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p2.8.m8.1c">9.6\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.8.m8.1d">9.6 ×</annotation></semantics></math>
(maximum <math alttext="11.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS1.p2.9.m9.1"><semantics id="S6.SS1.p2.9.m9.1a"><mrow id="S6.SS1.p2.9.m9.1b"><mn id="S6.SS1.p2.9.m9.1.1">11.6</mn><mo id="S6.SS1.p2.9.m9.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p2.9.m9.1c">11.6\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.9.m9.1d">11.6 ×</annotation></semantics></math>) reduction in end-to-end preprocessing time.
These results highlight the benefits of <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p2.10.m10.1"><semantics id="S6.SS1.p2.10.m10.1a"><mrow id="S6.SS1.p2.10.m10.1.1" xref="S6.SS1.p2.10.m10.1.1.cmml"><mi id="S6.SS1.p2.10.m10.1.1.2" xref="S6.SS1.p2.10.m10.1.1.2.cmml">P</mi><mo id="S6.SS1.p2.10.m10.1.1.1" xref="S6.SS1.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.10.m10.1.1.3" xref="S6.SS1.p2.10.m10.1.1.3.cmml">r</mi><mo id="S6.SS1.p2.10.m10.1.1.1a" xref="S6.SS1.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.10.m10.1.1.4" xref="S6.SS1.p2.10.m10.1.1.4.cmml">e</mi><mo id="S6.SS1.p2.10.m10.1.1.1b" xref="S6.SS1.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.10.m10.1.1.5" xref="S6.SS1.p2.10.m10.1.1.5.cmml">S</mi><mo id="S6.SS1.p2.10.m10.1.1.1c" xref="S6.SS1.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.10.m10.1.1.6" xref="S6.SS1.p2.10.m10.1.1.6.cmml">t</mi><mo id="S6.SS1.p2.10.m10.1.1.1d" xref="S6.SS1.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p2.10.m10.1.1.7" xref="S6.SS1.p2.10.m10.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.10.m10.1b"><apply id="S6.SS1.p2.10.m10.1.1.cmml" xref="S6.SS1.p2.10.m10.1.1"><times id="S6.SS1.p2.10.m10.1.1.1.cmml" xref="S6.SS1.p2.10.m10.1.1.1"></times><ci id="S6.SS1.p2.10.m10.1.1.2.cmml" xref="S6.SS1.p2.10.m10.1.1.2">𝑃</ci><ci id="S6.SS1.p2.10.m10.1.1.3.cmml" xref="S6.SS1.p2.10.m10.1.1.3">𝑟</ci><ci id="S6.SS1.p2.10.m10.1.1.4.cmml" xref="S6.SS1.p2.10.m10.1.1.4">𝑒</ci><ci id="S6.SS1.p2.10.m10.1.1.5.cmml" xref="S6.SS1.p2.10.m10.1.1.5">𝑆</ci><ci id="S6.SS1.p2.10.m10.1.1.6.cmml" xref="S6.SS1.p2.10.m10.1.1.6">𝑡</ci><ci id="S6.SS1.p2.10.m10.1.1.7.cmml" xref="S6.SS1.p2.10.m10.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.10.m10.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p2.10.m10.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s domain-specific acceleration
using RecSys preprocessing’s inter-/intra-feature parallelism.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.10"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.10.1">Data movements.</span> Another key benefit provided with <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><mrow id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml"><mi id="S6.SS1.p3.1.m1.1.1.2" xref="S6.SS1.p3.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS1.p3.1.m1.1.1.1" xref="S6.SS1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.1.m1.1.1.3" xref="S6.SS1.p3.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS1.p3.1.m1.1.1.1a" xref="S6.SS1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.1.m1.1.1.4" xref="S6.SS1.p3.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS1.p3.1.m1.1.1.1b" xref="S6.SS1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.1.m1.1.1.5" xref="S6.SS1.p3.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS1.p3.1.m1.1.1.1c" xref="S6.SS1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.1.m1.1.1.6" xref="S6.SS1.p3.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS1.p3.1.m1.1.1.1d" xref="S6.SS1.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.1.m1.1.1.7" xref="S6.SS1.p3.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><apply id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1"><times id="S6.SS1.p3.1.m1.1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1.1"></times><ci id="S6.SS1.p3.1.m1.1.1.2.cmml" xref="S6.SS1.p3.1.m1.1.1.2">𝑃</ci><ci id="S6.SS1.p3.1.m1.1.1.3.cmml" xref="S6.SS1.p3.1.m1.1.1.3">𝑟</ci><ci id="S6.SS1.p3.1.m1.1.1.4.cmml" xref="S6.SS1.p3.1.m1.1.1.4">𝑒</ci><ci id="S6.SS1.p3.1.m1.1.1.5.cmml" xref="S6.SS1.p3.1.m1.1.1.5">𝑆</ci><ci id="S6.SS1.p3.1.m1.1.1.6.cmml" xref="S6.SS1.p3.1.m1.1.1.6">𝑡</ci><ci id="S6.SS1.p3.1.m1.1.1.7.cmml" xref="S6.SS1.p3.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> is that all data
preprocessing operations are conducted locally within the storage node, unlike
<math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p3.2.m2.1"><semantics id="S6.SS1.p3.2.m2.1a"><mrow id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml"><mi id="S6.SS1.p3.2.m2.1.1.2" xref="S6.SS1.p3.2.m2.1.1.2.cmml">D</mi><mo id="S6.SS1.p3.2.m2.1.1.1" xref="S6.SS1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.2.m2.1.1.3" xref="S6.SS1.p3.2.m2.1.1.3.cmml">i</mi><mo id="S6.SS1.p3.2.m2.1.1.1a" xref="S6.SS1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.2.m2.1.1.4" xref="S6.SS1.p3.2.m2.1.1.4.cmml">s</mi><mo id="S6.SS1.p3.2.m2.1.1.1b" xref="S6.SS1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.2.m2.1.1.5" xref="S6.SS1.p3.2.m2.1.1.5.cmml">a</mi><mo id="S6.SS1.p3.2.m2.1.1.1c" xref="S6.SS1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.2.m2.1.1.6" xref="S6.SS1.p3.2.m2.1.1.6.cmml">g</mi><mo id="S6.SS1.p3.2.m2.1.1.1d" xref="S6.SS1.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.2.m2.1.1.7" xref="S6.SS1.p3.2.m2.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><apply id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1"><times id="S6.SS1.p3.2.m2.1.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1.1"></times><ci id="S6.SS1.p3.2.m2.1.1.2.cmml" xref="S6.SS1.p3.2.m2.1.1.2">𝐷</ci><ci id="S6.SS1.p3.2.m2.1.1.3.cmml" xref="S6.SS1.p3.2.m2.1.1.3">𝑖</ci><ci id="S6.SS1.p3.2.m2.1.1.4.cmml" xref="S6.SS1.p3.2.m2.1.1.4">𝑠</ci><ci id="S6.SS1.p3.2.m2.1.1.5.cmml" xref="S6.SS1.p3.2.m2.1.1.5">𝑎</ci><ci id="S6.SS1.p3.2.m2.1.1.6.cmml" xref="S6.SS1.p3.2.m2.1.1.6">𝑔</ci><ci id="S6.SS1.p3.2.m2.1.1.7.cmml" xref="S6.SS1.p3.2.m2.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.2.m2.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> which needs to explicitly copy data in (the raw data to be
preprocessed) and out (the train-ready tensors) of the disaggregated CPU
nodes for data preprocessing. Because our small-scale PoC prototype evaluates a
single training job in a highly controlled, isolated setting, <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p3.3.m3.1"><semantics id="S6.SS1.p3.3.m3.1a"><mrow id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml"><mi id="S6.SS1.p3.3.m3.1.1.2" xref="S6.SS1.p3.3.m3.1.1.2.cmml">D</mi><mo id="S6.SS1.p3.3.m3.1.1.1" xref="S6.SS1.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.3.m3.1.1.3" xref="S6.SS1.p3.3.m3.1.1.3.cmml">i</mi><mo id="S6.SS1.p3.3.m3.1.1.1a" xref="S6.SS1.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.3.m3.1.1.4" xref="S6.SS1.p3.3.m3.1.1.4.cmml">s</mi><mo id="S6.SS1.p3.3.m3.1.1.1b" xref="S6.SS1.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.3.m3.1.1.5" xref="S6.SS1.p3.3.m3.1.1.5.cmml">a</mi><mo id="S6.SS1.p3.3.m3.1.1.1c" xref="S6.SS1.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.3.m3.1.1.6" xref="S6.SS1.p3.3.m3.1.1.6.cmml">g</mi><mo id="S6.SS1.p3.3.m3.1.1.1d" xref="S6.SS1.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.3.m3.1.1.7" xref="S6.SS1.p3.3.m3.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><apply id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1"><times id="S6.SS1.p3.3.m3.1.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1.1"></times><ci id="S6.SS1.p3.3.m3.1.1.2.cmml" xref="S6.SS1.p3.3.m3.1.1.2">𝐷</ci><ci id="S6.SS1.p3.3.m3.1.1.3.cmml" xref="S6.SS1.p3.3.m3.1.1.3">𝑖</ci><ci id="S6.SS1.p3.3.m3.1.1.4.cmml" xref="S6.SS1.p3.3.m3.1.1.4">𝑠</ci><ci id="S6.SS1.p3.3.m3.1.1.5.cmml" xref="S6.SS1.p3.3.m3.1.1.5">𝑎</ci><ci id="S6.SS1.p3.3.m3.1.1.6.cmml" xref="S6.SS1.p3.3.m3.1.1.6">𝑔</ci><ci id="S6.SS1.p3.3.m3.1.1.7.cmml" xref="S6.SS1.p3.3.m3.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.3.m3.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>’s RPC
communication time to read out the raw feature data from the remote storage
node and copy into the disaggregated CPU nodes accounts for a relatively small
portion of the end-to-end preprocessing time (but still accounting for
<math alttext="9.1\%" class="ltx_Math" display="inline" id="S6.SS1.p3.4.m4.1"><semantics id="S6.SS1.p3.4.m4.1a"><mrow id="S6.SS1.p3.4.m4.1.1" xref="S6.SS1.p3.4.m4.1.1.cmml"><mn id="S6.SS1.p3.4.m4.1.1.2" xref="S6.SS1.p3.4.m4.1.1.2.cmml">9.1</mn><mo id="S6.SS1.p3.4.m4.1.1.1" xref="S6.SS1.p3.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.4.m4.1b"><apply id="S6.SS1.p3.4.m4.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1"><csymbol cd="latexml" id="S6.SS1.p3.4.m4.1.1.1.cmml" xref="S6.SS1.p3.4.m4.1.1.1">percent</csymbol><cn id="S6.SS1.p3.4.m4.1.1.2.cmml" type="float" xref="S6.SS1.p3.4.m4.1.1.2">9.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.4.m4.1c">9.1\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.4.m4.1d">9.1 %</annotation></semantics></math> in RM2 under <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p3.5.m5.1"><semantics id="S6.SS1.p3.5.m5.1a"><mrow id="S6.SS1.p3.5.m5.1.1" xref="S6.SS1.p3.5.m5.1.1.cmml"><mi id="S6.SS1.p3.5.m5.1.1.2" xref="S6.SS1.p3.5.m5.1.1.2.cmml">D</mi><mo id="S6.SS1.p3.5.m5.1.1.1" xref="S6.SS1.p3.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.5.m5.1.1.3" xref="S6.SS1.p3.5.m5.1.1.3.cmml">i</mi><mo id="S6.SS1.p3.5.m5.1.1.1a" xref="S6.SS1.p3.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.5.m5.1.1.4" xref="S6.SS1.p3.5.m5.1.1.4.cmml">s</mi><mo id="S6.SS1.p3.5.m5.1.1.1b" xref="S6.SS1.p3.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.5.m5.1.1.5" xref="S6.SS1.p3.5.m5.1.1.5.cmml">a</mi><mo id="S6.SS1.p3.5.m5.1.1.1c" xref="S6.SS1.p3.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.5.m5.1.1.6" xref="S6.SS1.p3.5.m5.1.1.6.cmml">g</mi><mo id="S6.SS1.p3.5.m5.1.1.1d" xref="S6.SS1.p3.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.5.m5.1.1.7" xref="S6.SS1.p3.5.m5.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.5.m5.1b"><apply id="S6.SS1.p3.5.m5.1.1.cmml" xref="S6.SS1.p3.5.m5.1.1"><times id="S6.SS1.p3.5.m5.1.1.1.cmml" xref="S6.SS1.p3.5.m5.1.1.1"></times><ci id="S6.SS1.p3.5.m5.1.1.2.cmml" xref="S6.SS1.p3.5.m5.1.1.2">𝐷</ci><ci id="S6.SS1.p3.5.m5.1.1.3.cmml" xref="S6.SS1.p3.5.m5.1.1.3">𝑖</ci><ci id="S6.SS1.p3.5.m5.1.1.4.cmml" xref="S6.SS1.p3.5.m5.1.1.4">𝑠</ci><ci id="S6.SS1.p3.5.m5.1.1.5.cmml" xref="S6.SS1.p3.5.m5.1.1.5">𝑎</ci><ci id="S6.SS1.p3.5.m5.1.1.6.cmml" xref="S6.SS1.p3.5.m5.1.1.6">𝑔</ci><ci id="S6.SS1.p3.5.m5.1.1.7.cmml" xref="S6.SS1.p3.5.m5.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.5.m5.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.5.m5.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> in
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F12" title="Figure 12 ‣ VI-A Performance and Cost-Effectiveness of P⁢r⁢e⁢S⁢t⁢o (PoC) ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">12</span></a>). Since real-world datacenter fleets
concurrently handle a large number of training jobs, all of which time-share
the datacenter network, <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p3.6.m6.1"><semantics id="S6.SS1.p3.6.m6.1a"><mrow id="S6.SS1.p3.6.m6.1.1" xref="S6.SS1.p3.6.m6.1.1.cmml"><mi id="S6.SS1.p3.6.m6.1.1.2" xref="S6.SS1.p3.6.m6.1.1.2.cmml">P</mi><mo id="S6.SS1.p3.6.m6.1.1.1" xref="S6.SS1.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.6.m6.1.1.3" xref="S6.SS1.p3.6.m6.1.1.3.cmml">r</mi><mo id="S6.SS1.p3.6.m6.1.1.1a" xref="S6.SS1.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.6.m6.1.1.4" xref="S6.SS1.p3.6.m6.1.1.4.cmml">e</mi><mo id="S6.SS1.p3.6.m6.1.1.1b" xref="S6.SS1.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.6.m6.1.1.5" xref="S6.SS1.p3.6.m6.1.1.5.cmml">S</mi><mo id="S6.SS1.p3.6.m6.1.1.1c" xref="S6.SS1.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.6.m6.1.1.6" xref="S6.SS1.p3.6.m6.1.1.6.cmml">t</mi><mo id="S6.SS1.p3.6.m6.1.1.1d" xref="S6.SS1.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.6.m6.1.1.7" xref="S6.SS1.p3.6.m6.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.6.m6.1b"><apply id="S6.SS1.p3.6.m6.1.1.cmml" xref="S6.SS1.p3.6.m6.1.1"><times id="S6.SS1.p3.6.m6.1.1.1.cmml" xref="S6.SS1.p3.6.m6.1.1.1"></times><ci id="S6.SS1.p3.6.m6.1.1.2.cmml" xref="S6.SS1.p3.6.m6.1.1.2">𝑃</ci><ci id="S6.SS1.p3.6.m6.1.1.3.cmml" xref="S6.SS1.p3.6.m6.1.1.3">𝑟</ci><ci id="S6.SS1.p3.6.m6.1.1.4.cmml" xref="S6.SS1.p3.6.m6.1.1.4">𝑒</ci><ci id="S6.SS1.p3.6.m6.1.1.5.cmml" xref="S6.SS1.p3.6.m6.1.1.5">𝑆</ci><ci id="S6.SS1.p3.6.m6.1.1.6.cmml" xref="S6.SS1.p3.6.m6.1.1.6">𝑡</ci><ci id="S6.SS1.p3.6.m6.1.1.7.cmml" xref="S6.SS1.p3.6.m6.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.6.m6.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.6.m6.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s ISP capability can be beneficial in alleviating
the preprocessing operation’s pressure on network communications. In
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F13" title="Figure 13 ‣ VI-A Performance and Cost-Effectiveness of P⁢r⁢e⁢S⁢t⁢o (PoC) ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">13</span></a>, we show the aggregate latency incurred during any
RPC calls executed for inter-node data movements during the course of data preprocessing.
Unlike <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS1.p3.7.m7.1"><semantics id="S6.SS1.p3.7.m7.1a"><mrow id="S6.SS1.p3.7.m7.1.1" xref="S6.SS1.p3.7.m7.1.1.cmml"><mi id="S6.SS1.p3.7.m7.1.1.2" xref="S6.SS1.p3.7.m7.1.1.2.cmml">D</mi><mo id="S6.SS1.p3.7.m7.1.1.1" xref="S6.SS1.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.7.m7.1.1.3" xref="S6.SS1.p3.7.m7.1.1.3.cmml">i</mi><mo id="S6.SS1.p3.7.m7.1.1.1a" xref="S6.SS1.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.7.m7.1.1.4" xref="S6.SS1.p3.7.m7.1.1.4.cmml">s</mi><mo id="S6.SS1.p3.7.m7.1.1.1b" xref="S6.SS1.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.7.m7.1.1.5" xref="S6.SS1.p3.7.m7.1.1.5.cmml">a</mi><mo id="S6.SS1.p3.7.m7.1.1.1c" xref="S6.SS1.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.7.m7.1.1.6" xref="S6.SS1.p3.7.m7.1.1.6.cmml">g</mi><mo id="S6.SS1.p3.7.m7.1.1.1d" xref="S6.SS1.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.7.m7.1.1.7" xref="S6.SS1.p3.7.m7.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.7.m7.1b"><apply id="S6.SS1.p3.7.m7.1.1.cmml" xref="S6.SS1.p3.7.m7.1.1"><times id="S6.SS1.p3.7.m7.1.1.1.cmml" xref="S6.SS1.p3.7.m7.1.1.1"></times><ci id="S6.SS1.p3.7.m7.1.1.2.cmml" xref="S6.SS1.p3.7.m7.1.1.2">𝐷</ci><ci id="S6.SS1.p3.7.m7.1.1.3.cmml" xref="S6.SS1.p3.7.m7.1.1.3">𝑖</ci><ci id="S6.SS1.p3.7.m7.1.1.4.cmml" xref="S6.SS1.p3.7.m7.1.1.4">𝑠</ci><ci id="S6.SS1.p3.7.m7.1.1.5.cmml" xref="S6.SS1.p3.7.m7.1.1.5">𝑎</ci><ci id="S6.SS1.p3.7.m7.1.1.6.cmml" xref="S6.SS1.p3.7.m7.1.1.6">𝑔</ci><ci id="S6.SS1.p3.7.m7.1.1.7.cmml" xref="S6.SS1.p3.7.m7.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.7.m7.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.7.m7.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> which incurs additional latency
when the preprocessing worker copies raw feature data from the remote storage
to the disaggregated CPU nodes, our <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p3.8.m8.1"><semantics id="S6.SS1.p3.8.m8.1a"><mrow id="S6.SS1.p3.8.m8.1.1" xref="S6.SS1.p3.8.m8.1.1.cmml"><mi id="S6.SS1.p3.8.m8.1.1.2" xref="S6.SS1.p3.8.m8.1.1.2.cmml">P</mi><mo id="S6.SS1.p3.8.m8.1.1.1" xref="S6.SS1.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.8.m8.1.1.3" xref="S6.SS1.p3.8.m8.1.1.3.cmml">r</mi><mo id="S6.SS1.p3.8.m8.1.1.1a" xref="S6.SS1.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.8.m8.1.1.4" xref="S6.SS1.p3.8.m8.1.1.4.cmml">e</mi><mo id="S6.SS1.p3.8.m8.1.1.1b" xref="S6.SS1.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.8.m8.1.1.5" xref="S6.SS1.p3.8.m8.1.1.5.cmml">S</mi><mo id="S6.SS1.p3.8.m8.1.1.1c" xref="S6.SS1.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.8.m8.1.1.6" xref="S6.SS1.p3.8.m8.1.1.6.cmml">t</mi><mo id="S6.SS1.p3.8.m8.1.1.1d" xref="S6.SS1.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.8.m8.1.1.7" xref="S6.SS1.p3.8.m8.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.8.m8.1b"><apply id="S6.SS1.p3.8.m8.1.1.cmml" xref="S6.SS1.p3.8.m8.1.1"><times id="S6.SS1.p3.8.m8.1.1.1.cmml" xref="S6.SS1.p3.8.m8.1.1.1"></times><ci id="S6.SS1.p3.8.m8.1.1.2.cmml" xref="S6.SS1.p3.8.m8.1.1.2">𝑃</ci><ci id="S6.SS1.p3.8.m8.1.1.3.cmml" xref="S6.SS1.p3.8.m8.1.1.3">𝑟</ci><ci id="S6.SS1.p3.8.m8.1.1.4.cmml" xref="S6.SS1.p3.8.m8.1.1.4">𝑒</ci><ci id="S6.SS1.p3.8.m8.1.1.5.cmml" xref="S6.SS1.p3.8.m8.1.1.5">𝑆</ci><ci id="S6.SS1.p3.8.m8.1.1.6.cmml" xref="S6.SS1.p3.8.m8.1.1.6">𝑡</ci><ci id="S6.SS1.p3.8.m8.1.1.7.cmml" xref="S6.SS1.p3.8.m8.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.8.m8.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.8.m8.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> can completely
eliminate such performance overhead. This leads <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS1.p3.9.m9.1"><semantics id="S6.SS1.p3.9.m9.1a"><mrow id="S6.SS1.p3.9.m9.1.1" xref="S6.SS1.p3.9.m9.1.1.cmml"><mi id="S6.SS1.p3.9.m9.1.1.2" xref="S6.SS1.p3.9.m9.1.1.2.cmml">P</mi><mo id="S6.SS1.p3.9.m9.1.1.1" xref="S6.SS1.p3.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.9.m9.1.1.3" xref="S6.SS1.p3.9.m9.1.1.3.cmml">r</mi><mo id="S6.SS1.p3.9.m9.1.1.1a" xref="S6.SS1.p3.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.9.m9.1.1.4" xref="S6.SS1.p3.9.m9.1.1.4.cmml">e</mi><mo id="S6.SS1.p3.9.m9.1.1.1b" xref="S6.SS1.p3.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.9.m9.1.1.5" xref="S6.SS1.p3.9.m9.1.1.5.cmml">S</mi><mo id="S6.SS1.p3.9.m9.1.1.1c" xref="S6.SS1.p3.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.9.m9.1.1.6" xref="S6.SS1.p3.9.m9.1.1.6.cmml">t</mi><mo id="S6.SS1.p3.9.m9.1.1.1d" xref="S6.SS1.p3.9.m9.1.1.1.cmml">⁢</mo><mi id="S6.SS1.p3.9.m9.1.1.7" xref="S6.SS1.p3.9.m9.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.9.m9.1b"><apply id="S6.SS1.p3.9.m9.1.1.cmml" xref="S6.SS1.p3.9.m9.1.1"><times id="S6.SS1.p3.9.m9.1.1.1.cmml" xref="S6.SS1.p3.9.m9.1.1.1"></times><ci id="S6.SS1.p3.9.m9.1.1.2.cmml" xref="S6.SS1.p3.9.m9.1.1.2">𝑃</ci><ci id="S6.SS1.p3.9.m9.1.1.3.cmml" xref="S6.SS1.p3.9.m9.1.1.3">𝑟</ci><ci id="S6.SS1.p3.9.m9.1.1.4.cmml" xref="S6.SS1.p3.9.m9.1.1.4">𝑒</ci><ci id="S6.SS1.p3.9.m9.1.1.5.cmml" xref="S6.SS1.p3.9.m9.1.1.5">𝑆</ci><ci id="S6.SS1.p3.9.m9.1.1.6.cmml" xref="S6.SS1.p3.9.m9.1.1.6">𝑡</ci><ci id="S6.SS1.p3.9.m9.1.1.7.cmml" xref="S6.SS1.p3.9.m9.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.9.m9.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.9.m9.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> to provide a <math alttext="2.9\times" class="ltx_math_unparsed" display="inline" id="S6.SS1.p3.10.m10.1"><semantics id="S6.SS1.p3.10.m10.1a"><mrow id="S6.SS1.p3.10.m10.1b"><mn id="S6.SS1.p3.10.m10.1.1">2.9</mn><mo id="S6.SS1.p3.10.m10.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS1.p3.10.m10.1c">2.9\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.10.m10.1d">2.9 ×</annotation></semantics></math> reduction in
RPC-invoked inter-node communication time.</p>
</div>
<figure class="ltx_figure" id="S6.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="100" id="S6.F14.g1" src="x14.png" width="340"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>The number of ISP units (left axis) and CPU cores (right axis) required for <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.F14.4.m1.1"><semantics id="S6.F14.4.m1.1b"><mrow id="S6.F14.4.m1.1.1" xref="S6.F14.4.m1.1.1.cmml"><mi id="S6.F14.4.m1.1.1.2" xref="S6.F14.4.m1.1.1.2.cmml">P</mi><mo id="S6.F14.4.m1.1.1.1" xref="S6.F14.4.m1.1.1.1.cmml">⁢</mo><mi id="S6.F14.4.m1.1.1.3" xref="S6.F14.4.m1.1.1.3.cmml">r</mi><mo id="S6.F14.4.m1.1.1.1b" xref="S6.F14.4.m1.1.1.1.cmml">⁢</mo><mi id="S6.F14.4.m1.1.1.4" xref="S6.F14.4.m1.1.1.4.cmml">e</mi><mo id="S6.F14.4.m1.1.1.1c" xref="S6.F14.4.m1.1.1.1.cmml">⁢</mo><mi id="S6.F14.4.m1.1.1.5" xref="S6.F14.4.m1.1.1.5.cmml">S</mi><mo id="S6.F14.4.m1.1.1.1d" xref="S6.F14.4.m1.1.1.1.cmml">⁢</mo><mi id="S6.F14.4.m1.1.1.6" xref="S6.F14.4.m1.1.1.6.cmml">t</mi><mo id="S6.F14.4.m1.1.1.1e" xref="S6.F14.4.m1.1.1.1.cmml">⁢</mo><mi id="S6.F14.4.m1.1.1.7" xref="S6.F14.4.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F14.4.m1.1c"><apply id="S6.F14.4.m1.1.1.cmml" xref="S6.F14.4.m1.1.1"><times id="S6.F14.4.m1.1.1.1.cmml" xref="S6.F14.4.m1.1.1.1"></times><ci id="S6.F14.4.m1.1.1.2.cmml" xref="S6.F14.4.m1.1.1.2">𝑃</ci><ci id="S6.F14.4.m1.1.1.3.cmml" xref="S6.F14.4.m1.1.1.3">𝑟</ci><ci id="S6.F14.4.m1.1.1.4.cmml" xref="S6.F14.4.m1.1.1.4">𝑒</ci><ci id="S6.F14.4.m1.1.1.5.cmml" xref="S6.F14.4.m1.1.1.5">𝑆</ci><ci id="S6.F14.4.m1.1.1.6.cmml" xref="S6.F14.4.m1.1.1.6">𝑡</ci><ci id="S6.F14.4.m1.1.1.7.cmml" xref="S6.F14.4.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F14.4.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.F14.4.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> and <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.F14.5.m2.1"><semantics id="S6.F14.5.m2.1b"><mrow id="S6.F14.5.m2.1.1" xref="S6.F14.5.m2.1.1.cmml"><mi id="S6.F14.5.m2.1.1.2" xref="S6.F14.5.m2.1.1.2.cmml">D</mi><mo id="S6.F14.5.m2.1.1.1" xref="S6.F14.5.m2.1.1.1.cmml">⁢</mo><mi id="S6.F14.5.m2.1.1.3" xref="S6.F14.5.m2.1.1.3.cmml">i</mi><mo id="S6.F14.5.m2.1.1.1b" xref="S6.F14.5.m2.1.1.1.cmml">⁢</mo><mi id="S6.F14.5.m2.1.1.4" xref="S6.F14.5.m2.1.1.4.cmml">s</mi><mo id="S6.F14.5.m2.1.1.1c" xref="S6.F14.5.m2.1.1.1.cmml">⁢</mo><mi id="S6.F14.5.m2.1.1.5" xref="S6.F14.5.m2.1.1.5.cmml">a</mi><mo id="S6.F14.5.m2.1.1.1d" xref="S6.F14.5.m2.1.1.1.cmml">⁢</mo><mi id="S6.F14.5.m2.1.1.6" xref="S6.F14.5.m2.1.1.6.cmml">g</mi><mo id="S6.F14.5.m2.1.1.1e" xref="S6.F14.5.m2.1.1.1.cmml">⁢</mo><mi id="S6.F14.5.m2.1.1.7" xref="S6.F14.5.m2.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F14.5.m2.1c"><apply id="S6.F14.5.m2.1.1.cmml" xref="S6.F14.5.m2.1.1"><times id="S6.F14.5.m2.1.1.1.cmml" xref="S6.F14.5.m2.1.1.1"></times><ci id="S6.F14.5.m2.1.1.2.cmml" xref="S6.F14.5.m2.1.1.2">𝐷</ci><ci id="S6.F14.5.m2.1.1.3.cmml" xref="S6.F14.5.m2.1.1.3">𝑖</ci><ci id="S6.F14.5.m2.1.1.4.cmml" xref="S6.F14.5.m2.1.1.4">𝑠</ci><ci id="S6.F14.5.m2.1.1.5.cmml" xref="S6.F14.5.m2.1.1.5">𝑎</ci><ci id="S6.F14.5.m2.1.1.6.cmml" xref="S6.F14.5.m2.1.1.6">𝑔</ci><ci id="S6.F14.5.m2.1.1.7.cmml" xref="S6.F14.5.m2.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F14.5.m2.1d">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.F14.5.m2.1e">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> to sustain a single multi-GPU server node containing <math alttext="8" class="ltx_Math" display="inline" id="S6.F14.6.m3.1"><semantics id="S6.F14.6.m3.1b"><mn id="S6.F14.6.m3.1.1" xref="S6.F14.6.m3.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S6.F14.6.m3.1c"><cn id="S6.F14.6.m3.1.1.cmml" type="integer" xref="S6.F14.6.m3.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.F14.6.m3.1d">8</annotation><annotation encoding="application/x-llamapun" id="S6.F14.6.m3.1e">8</annotation></semantics></math> A100 GPUs.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.6.1.1">VI-B</span> </span><math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.1.m1.1"><semantics id="S6.SS2.1.m1.1b"><mrow id="S6.SS2.1.m1.1.1" xref="S6.SS2.1.m1.1.1.cmml"><mi id="S6.SS2.1.m1.1.1.2" xref="S6.SS2.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS2.1.m1.1.1.1" xref="S6.SS2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.1.m1.1.1.3" xref="S6.SS2.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS2.1.m1.1.1.1b" xref="S6.SS2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.1.m1.1.1.4" xref="S6.SS2.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS2.1.m1.1.1.1c" xref="S6.SS2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.1.m1.1.1.5" xref="S6.SS2.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS2.1.m1.1.1.1d" xref="S6.SS2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.1.m1.1.1.6" xref="S6.SS2.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS2.1.m1.1.1.1e" xref="S6.SS2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.1.m1.1.1.7" xref="S6.SS2.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.1.m1.1c"><apply id="S6.SS2.1.m1.1.1.cmml" xref="S6.SS2.1.m1.1.1"><times id="S6.SS2.1.m1.1.1.1.cmml" xref="S6.SS2.1.m1.1.1.1"></times><ci id="S6.SS2.1.m1.1.1.2.cmml" xref="S6.SS2.1.m1.1.1.2">𝑃</ci><ci id="S6.SS2.1.m1.1.1.3.cmml" xref="S6.SS2.1.m1.1.1.3">𝑟</ci><ci id="S6.SS2.1.m1.1.1.4.cmml" xref="S6.SS2.1.m1.1.1.4">𝑒</ci><ci id="S6.SS2.1.m1.1.1.5.cmml" xref="S6.SS2.1.m1.1.1.5">𝑆</ci><ci id="S6.SS2.1.m1.1.1.6.cmml" xref="S6.SS2.1.m1.1.1.6">𝑡</ci><ci id="S6.SS2.1.m1.1.1.7.cmml" xref="S6.SS2.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.1.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.1.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="S6.SS2.7.2">’s Effect on Energy-Efficiency and TCO</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We now evaluate <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.p1.1.m1.1"><semantics id="S6.SS2.p1.1.m1.1a"><mrow id="S6.SS2.p1.1.m1.1.1" xref="S6.SS2.p1.1.m1.1.1.cmml"><mi id="S6.SS2.p1.1.m1.1.1.2" xref="S6.SS2.p1.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS2.p1.1.m1.1.1.1" xref="S6.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p1.1.m1.1.1.3" xref="S6.SS2.p1.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS2.p1.1.m1.1.1.1a" xref="S6.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p1.1.m1.1.1.4" xref="S6.SS2.p1.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS2.p1.1.m1.1.1.1b" xref="S6.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p1.1.m1.1.1.5" xref="S6.SS2.p1.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS2.p1.1.m1.1.1.1c" xref="S6.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p1.1.m1.1.1.6" xref="S6.SS2.p1.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS2.p1.1.m1.1.1.1d" xref="S6.SS2.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p1.1.m1.1.1.7" xref="S6.SS2.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p1.1.m1.1b"><apply id="S6.SS2.p1.1.m1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1"><times id="S6.SS2.p1.1.m1.1.1.1.cmml" xref="S6.SS2.p1.1.m1.1.1.1"></times><ci id="S6.SS2.p1.1.m1.1.1.2.cmml" xref="S6.SS2.p1.1.m1.1.1.2">𝑃</ci><ci id="S6.SS2.p1.1.m1.1.1.3.cmml" xref="S6.SS2.p1.1.m1.1.1.3">𝑟</ci><ci id="S6.SS2.p1.1.m1.1.1.4.cmml" xref="S6.SS2.p1.1.m1.1.1.4">𝑒</ci><ci id="S6.SS2.p1.1.m1.1.1.5.cmml" xref="S6.SS2.p1.1.m1.1.1.5">𝑆</ci><ci id="S6.SS2.p1.1.m1.1.1.6.cmml" xref="S6.SS2.p1.1.m1.1.1.6">𝑡</ci><ci id="S6.SS2.p1.1.m1.1.1.7.cmml" xref="S6.SS2.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s effect on performance/Watt (energy-efficiency) and performance/$ (TCO) by utilizing our analytical model for large-scale experiments (Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5.SS2" title="V-B Experimental Setup ‣ V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>).</p>
</div>
<figure class="ltx_figure" id="S6.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="127" id="S6.F15.g1" src="x15.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>(a) Energy-efficiency and (b) cost-efficiency. Power consumption is measured using Intel’s Performance Counter Monitor (PCM) and Xilinx Vivado. Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5.SS3" title="V-C Evaluation Methods ‣ V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a> details our methodology.</figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.15"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.15.1">Energy-efficiency.</span> As discussed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.F4" title="Figure 4 ‣ III-A Motivation ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">4</span></a>,
baseline <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><mrow id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mi id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml">D</mi><mo id="S6.SS2.p2.1.m1.1.1.1" xref="S6.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.1.m1.1.1.3" xref="S6.SS2.p2.1.m1.1.1.3.cmml">i</mi><mo id="S6.SS2.p2.1.m1.1.1.1a" xref="S6.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.1.m1.1.1.4" xref="S6.SS2.p2.1.m1.1.1.4.cmml">s</mi><mo id="S6.SS2.p2.1.m1.1.1.1b" xref="S6.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.1.m1.1.1.5" xref="S6.SS2.p2.1.m1.1.1.5.cmml">a</mi><mo id="S6.SS2.p2.1.m1.1.1.1c" xref="S6.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.1.m1.1.1.6" xref="S6.SS2.p2.1.m1.1.1.6.cmml">g</mi><mo id="S6.SS2.p2.1.m1.1.1.1d" xref="S6.SS2.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.1.m1.1.1.7" xref="S6.SS2.p2.1.m1.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><times id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1.1"></times><ci id="S6.SS2.p2.1.m1.1.1.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2">𝐷</ci><ci id="S6.SS2.p2.1.m1.1.1.3.cmml" xref="S6.SS2.p2.1.m1.1.1.3">𝑖</ci><ci id="S6.SS2.p2.1.m1.1.1.4.cmml" xref="S6.SS2.p2.1.m1.1.1.4">𝑠</ci><ci id="S6.SS2.p2.1.m1.1.1.5.cmml" xref="S6.SS2.p2.1.m1.1.1.5">𝑎</ci><ci id="S6.SS2.p2.1.m1.1.1.6.cmml" xref="S6.SS2.p2.1.m1.1.1.6">𝑔</ci><ci id="S6.SS2.p2.1.m1.1.1.7.cmml" xref="S6.SS2.p2.1.m1.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> requires significant number of CPU cores for data
preprocessing (e.g., <math alttext="367" class="ltx_Math" display="inline" id="S6.SS2.p2.2.m2.1"><semantics id="S6.SS2.p2.2.m2.1a"><mn id="S6.SS2.p2.2.m2.1.1" xref="S6.SS2.p2.2.m2.1.1.cmml">367</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.2.m2.1b"><cn id="S6.SS2.p2.2.m2.1.1.cmml" type="integer" xref="S6.SS2.p2.2.m2.1.1">367</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.2.m2.1c">367</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.2.m2.1d">367</annotation></semantics></math> cores for RM5), which translates into significant
power consumption and high deployment cost. To quantitatively demonstrate
<math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.p2.3.m3.1"><semantics id="S6.SS2.p2.3.m3.1a"><mrow id="S6.SS2.p2.3.m3.1.1" xref="S6.SS2.p2.3.m3.1.1.cmml"><mi id="S6.SS2.p2.3.m3.1.1.2" xref="S6.SS2.p2.3.m3.1.1.2.cmml">P</mi><mo id="S6.SS2.p2.3.m3.1.1.1" xref="S6.SS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.3.m3.1.1.3" xref="S6.SS2.p2.3.m3.1.1.3.cmml">r</mi><mo id="S6.SS2.p2.3.m3.1.1.1a" xref="S6.SS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.3.m3.1.1.4" xref="S6.SS2.p2.3.m3.1.1.4.cmml">e</mi><mo id="S6.SS2.p2.3.m3.1.1.1b" xref="S6.SS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.3.m3.1.1.5" xref="S6.SS2.p2.3.m3.1.1.5.cmml">S</mi><mo id="S6.SS2.p2.3.m3.1.1.1c" xref="S6.SS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.3.m3.1.1.6" xref="S6.SS2.p2.3.m3.1.1.6.cmml">t</mi><mo id="S6.SS2.p2.3.m3.1.1.1d" xref="S6.SS2.p2.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.3.m3.1.1.7" xref="S6.SS2.p2.3.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.3.m3.1b"><apply id="S6.SS2.p2.3.m3.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1"><times id="S6.SS2.p2.3.m3.1.1.1.cmml" xref="S6.SS2.p2.3.m3.1.1.1"></times><ci id="S6.SS2.p2.3.m3.1.1.2.cmml" xref="S6.SS2.p2.3.m3.1.1.2">𝑃</ci><ci id="S6.SS2.p2.3.m3.1.1.3.cmml" xref="S6.SS2.p2.3.m3.1.1.3">𝑟</ci><ci id="S6.SS2.p2.3.m3.1.1.4.cmml" xref="S6.SS2.p2.3.m3.1.1.4">𝑒</ci><ci id="S6.SS2.p2.3.m3.1.1.5.cmml" xref="S6.SS2.p2.3.m3.1.1.5">𝑆</ci><ci id="S6.SS2.p2.3.m3.1.1.6.cmml" xref="S6.SS2.p2.3.m3.1.1.6">𝑡</ci><ci id="S6.SS2.p2.3.m3.1.1.7.cmml" xref="S6.SS2.p2.3.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.3.m3.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.3.m3.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s effectiveness in energy and cost reduction, we evaluate how many ISP
units (i.e., the number of SmartSSD cards) are required to match the
preprocessing demands of a multi-GPU server containing <math alttext="8" class="ltx_Math" display="inline" id="S6.SS2.p2.4.m4.1"><semantics id="S6.SS2.p2.4.m4.1a"><mn id="S6.SS2.p2.4.m4.1.1" xref="S6.SS2.p2.4.m4.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.4.m4.1b"><cn id="S6.SS2.p2.4.m4.1.1.cmml" type="integer" xref="S6.SS2.p2.4.m4.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.4.m4.1c">8</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.4.m4.1d">8</annotation></semantics></math> GPUs
(Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F14" title="Figure 14 ‣ VI-A Performance and Cost-Effectiveness of P⁢r⁢e⁢S⁢t⁢o (PoC) ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">14</span></a>). Remarkably, to match such high
GPU training throughput demand, <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.p2.5.m5.1"><semantics id="S6.SS2.p2.5.m5.1a"><mrow id="S6.SS2.p2.5.m5.1.1" xref="S6.SS2.p2.5.m5.1.1.cmml"><mi id="S6.SS2.p2.5.m5.1.1.2" xref="S6.SS2.p2.5.m5.1.1.2.cmml">P</mi><mo id="S6.SS2.p2.5.m5.1.1.1" xref="S6.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.5.m5.1.1.3" xref="S6.SS2.p2.5.m5.1.1.3.cmml">r</mi><mo id="S6.SS2.p2.5.m5.1.1.1a" xref="S6.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.5.m5.1.1.4" xref="S6.SS2.p2.5.m5.1.1.4.cmml">e</mi><mo id="S6.SS2.p2.5.m5.1.1.1b" xref="S6.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.5.m5.1.1.5" xref="S6.SS2.p2.5.m5.1.1.5.cmml">S</mi><mo id="S6.SS2.p2.5.m5.1.1.1c" xref="S6.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.5.m5.1.1.6" xref="S6.SS2.p2.5.m5.1.1.6.cmml">t</mi><mo id="S6.SS2.p2.5.m5.1.1.1d" xref="S6.SS2.p2.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.5.m5.1.1.7" xref="S6.SS2.p2.5.m5.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.5.m5.1b"><apply id="S6.SS2.p2.5.m5.1.1.cmml" xref="S6.SS2.p2.5.m5.1.1"><times id="S6.SS2.p2.5.m5.1.1.1.cmml" xref="S6.SS2.p2.5.m5.1.1.1"></times><ci id="S6.SS2.p2.5.m5.1.1.2.cmml" xref="S6.SS2.p2.5.m5.1.1.2">𝑃</ci><ci id="S6.SS2.p2.5.m5.1.1.3.cmml" xref="S6.SS2.p2.5.m5.1.1.3">𝑟</ci><ci id="S6.SS2.p2.5.m5.1.1.4.cmml" xref="S6.SS2.p2.5.m5.1.1.4">𝑒</ci><ci id="S6.SS2.p2.5.m5.1.1.5.cmml" xref="S6.SS2.p2.5.m5.1.1.5">𝑆</ci><ci id="S6.SS2.p2.5.m5.1.1.6.cmml" xref="S6.SS2.p2.5.m5.1.1.6">𝑡</ci><ci id="S6.SS2.p2.5.m5.1.1.7.cmml" xref="S6.SS2.p2.5.m5.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.5.m5.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.5.m5.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> only requires a maximum of <math alttext="9" class="ltx_Math" display="inline" id="S6.SS2.p2.6.m6.1"><semantics id="S6.SS2.p2.6.m6.1a"><mn id="S6.SS2.p2.6.m6.1.1" xref="S6.SS2.p2.6.m6.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.6.m6.1b"><cn id="S6.SS2.p2.6.m6.1.1.cmml" type="integer" xref="S6.SS2.p2.6.m6.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.6.m6.1c">9</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.6.m6.1d">9</annotation></semantics></math>
ISP units which incur (9<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS2.p2.7.m7.1"><semantics id="S6.SS2.p2.7.m7.1a"><mo id="S6.SS2.p2.7.m7.1.1" xref="S6.SS2.p2.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.7.m7.1b"><times id="S6.SS2.p2.7.m7.1.1.cmml" xref="S6.SS2.p2.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.7.m7.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.7.m7.1d">×</annotation></semantics></math>25)<math alttext="=" class="ltx_Math" display="inline" id="S6.SS2.p2.8.m8.1"><semantics id="S6.SS2.p2.8.m8.1a"><mo id="S6.SS2.p2.8.m8.1.1" xref="S6.SS2.p2.8.m8.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.8.m8.1b"><eq id="S6.SS2.p2.8.m8.1.1.cmml" xref="S6.SS2.p2.8.m8.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.8.m8.1c">=</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.8.m8.1d">=</annotation></semantics></math>225 Watts of worst-case power
consumption (<math alttext="25" class="ltx_Math" display="inline" id="S6.SS2.p2.9.m9.1"><semantics id="S6.SS2.p2.9.m9.1a"><mn id="S6.SS2.p2.9.m9.1.1" xref="S6.SS2.p2.9.m9.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.9.m9.1b"><cn id="S6.SS2.p2.9.m9.1.1.cmml" type="integer" xref="S6.SS2.p2.9.m9.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.9.m9.1c">25</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.9.m9.1d">25</annotation></semantics></math> Watts TDP per each SmartSSD card). <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS2.p2.10.m10.1"><semantics id="S6.SS2.p2.10.m10.1a"><mrow id="S6.SS2.p2.10.m10.1.1" xref="S6.SS2.p2.10.m10.1.1.cmml"><mi id="S6.SS2.p2.10.m10.1.1.2" xref="S6.SS2.p2.10.m10.1.1.2.cmml">D</mi><mo id="S6.SS2.p2.10.m10.1.1.1" xref="S6.SS2.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.10.m10.1.1.3" xref="S6.SS2.p2.10.m10.1.1.3.cmml">i</mi><mo id="S6.SS2.p2.10.m10.1.1.1a" xref="S6.SS2.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.10.m10.1.1.4" xref="S6.SS2.p2.10.m10.1.1.4.cmml">s</mi><mo id="S6.SS2.p2.10.m10.1.1.1b" xref="S6.SS2.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.10.m10.1.1.5" xref="S6.SS2.p2.10.m10.1.1.5.cmml">a</mi><mo id="S6.SS2.p2.10.m10.1.1.1c" xref="S6.SS2.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.10.m10.1.1.6" xref="S6.SS2.p2.10.m10.1.1.6.cmml">g</mi><mo id="S6.SS2.p2.10.m10.1.1.1d" xref="S6.SS2.p2.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.10.m10.1.1.7" xref="S6.SS2.p2.10.m10.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.10.m10.1b"><apply id="S6.SS2.p2.10.m10.1.1.cmml" xref="S6.SS2.p2.10.m10.1.1"><times id="S6.SS2.p2.10.m10.1.1.1.cmml" xref="S6.SS2.p2.10.m10.1.1.1"></times><ci id="S6.SS2.p2.10.m10.1.1.2.cmml" xref="S6.SS2.p2.10.m10.1.1.2">𝐷</ci><ci id="S6.SS2.p2.10.m10.1.1.3.cmml" xref="S6.SS2.p2.10.m10.1.1.3">𝑖</ci><ci id="S6.SS2.p2.10.m10.1.1.4.cmml" xref="S6.SS2.p2.10.m10.1.1.4">𝑠</ci><ci id="S6.SS2.p2.10.m10.1.1.5.cmml" xref="S6.SS2.p2.10.m10.1.1.5">𝑎</ci><ci id="S6.SS2.p2.10.m10.1.1.6.cmml" xref="S6.SS2.p2.10.m10.1.1.6">𝑔</ci><ci id="S6.SS2.p2.10.m10.1.1.7.cmml" xref="S6.SS2.p2.10.m10.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.10.m10.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.10.m10.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>, on the other
hand, requires up to <math alttext="367" class="ltx_Math" display="inline" id="S6.SS2.p2.11.m11.1"><semantics id="S6.SS2.p2.11.m11.1a"><mn id="S6.SS2.p2.11.m11.1.1" xref="S6.SS2.p2.11.m11.1.1.cmml">367</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.11.m11.1b"><cn id="S6.SS2.p2.11.m11.1.1.cmml" type="integer" xref="S6.SS2.p2.11.m11.1.1">367</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.11.m11.1c">367</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.11.m11.1d">367</annotation></semantics></math> CPU cores (i.e., 12 CPU server nodes) to match
<math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.p2.12.m12.1"><semantics id="S6.SS2.p2.12.m12.1a"><mrow id="S6.SS2.p2.12.m12.1.1" xref="S6.SS2.p2.12.m12.1.1.cmml"><mi id="S6.SS2.p2.12.m12.1.1.2" xref="S6.SS2.p2.12.m12.1.1.2.cmml">P</mi><mo id="S6.SS2.p2.12.m12.1.1.1" xref="S6.SS2.p2.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.12.m12.1.1.3" xref="S6.SS2.p2.12.m12.1.1.3.cmml">r</mi><mo id="S6.SS2.p2.12.m12.1.1.1a" xref="S6.SS2.p2.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.12.m12.1.1.4" xref="S6.SS2.p2.12.m12.1.1.4.cmml">e</mi><mo id="S6.SS2.p2.12.m12.1.1.1b" xref="S6.SS2.p2.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.12.m12.1.1.5" xref="S6.SS2.p2.12.m12.1.1.5.cmml">S</mi><mo id="S6.SS2.p2.12.m12.1.1.1c" xref="S6.SS2.p2.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.12.m12.1.1.6" xref="S6.SS2.p2.12.m12.1.1.6.cmml">t</mi><mo id="S6.SS2.p2.12.m12.1.1.1d" xref="S6.SS2.p2.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.12.m12.1.1.7" xref="S6.SS2.p2.12.m12.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.12.m12.1b"><apply id="S6.SS2.p2.12.m12.1.1.cmml" xref="S6.SS2.p2.12.m12.1.1"><times id="S6.SS2.p2.12.m12.1.1.1.cmml" xref="S6.SS2.p2.12.m12.1.1.1"></times><ci id="S6.SS2.p2.12.m12.1.1.2.cmml" xref="S6.SS2.p2.12.m12.1.1.2">𝑃</ci><ci id="S6.SS2.p2.12.m12.1.1.3.cmml" xref="S6.SS2.p2.12.m12.1.1.3">𝑟</ci><ci id="S6.SS2.p2.12.m12.1.1.4.cmml" xref="S6.SS2.p2.12.m12.1.1.4">𝑒</ci><ci id="S6.SS2.p2.12.m12.1.1.5.cmml" xref="S6.SS2.p2.12.m12.1.1.5">𝑆</ci><ci id="S6.SS2.p2.12.m12.1.1.6.cmml" xref="S6.SS2.p2.12.m12.1.1.6">𝑡</ci><ci id="S6.SS2.p2.12.m12.1.1.7.cmml" xref="S6.SS2.p2.12.m12.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.12.m12.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.12.m12.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s preprocessing performance, incurring much higher power consumption as well as
cost.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F15" title="Figure 15 ‣ VI-B 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜’s Effect on Energy-Efficiency and TCO ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">15</span></a>(a) summarizes how all of this translate into
energy consumption. Overall, <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.p2.13.m13.1"><semantics id="S6.SS2.p2.13.m13.1a"><mrow id="S6.SS2.p2.13.m13.1.1" xref="S6.SS2.p2.13.m13.1.1.cmml"><mi id="S6.SS2.p2.13.m13.1.1.2" xref="S6.SS2.p2.13.m13.1.1.2.cmml">P</mi><mo id="S6.SS2.p2.13.m13.1.1.1" xref="S6.SS2.p2.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.13.m13.1.1.3" xref="S6.SS2.p2.13.m13.1.1.3.cmml">r</mi><mo id="S6.SS2.p2.13.m13.1.1.1a" xref="S6.SS2.p2.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.13.m13.1.1.4" xref="S6.SS2.p2.13.m13.1.1.4.cmml">e</mi><mo id="S6.SS2.p2.13.m13.1.1.1b" xref="S6.SS2.p2.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.13.m13.1.1.5" xref="S6.SS2.p2.13.m13.1.1.5.cmml">S</mi><mo id="S6.SS2.p2.13.m13.1.1.1c" xref="S6.SS2.p2.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.13.m13.1.1.6" xref="S6.SS2.p2.13.m13.1.1.6.cmml">t</mi><mo id="S6.SS2.p2.13.m13.1.1.1d" xref="S6.SS2.p2.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p2.13.m13.1.1.7" xref="S6.SS2.p2.13.m13.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.13.m13.1b"><apply id="S6.SS2.p2.13.m13.1.1.cmml" xref="S6.SS2.p2.13.m13.1.1"><times id="S6.SS2.p2.13.m13.1.1.1.cmml" xref="S6.SS2.p2.13.m13.1.1.1"></times><ci id="S6.SS2.p2.13.m13.1.1.2.cmml" xref="S6.SS2.p2.13.m13.1.1.2">𝑃</ci><ci id="S6.SS2.p2.13.m13.1.1.3.cmml" xref="S6.SS2.p2.13.m13.1.1.3">𝑟</ci><ci id="S6.SS2.p2.13.m13.1.1.4.cmml" xref="S6.SS2.p2.13.m13.1.1.4">𝑒</ci><ci id="S6.SS2.p2.13.m13.1.1.5.cmml" xref="S6.SS2.p2.13.m13.1.1.5">𝑆</ci><ci id="S6.SS2.p2.13.m13.1.1.6.cmml" xref="S6.SS2.p2.13.m13.1.1.6">𝑡</ci><ci id="S6.SS2.p2.13.m13.1.1.7.cmml" xref="S6.SS2.p2.13.m13.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.13.m13.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.13.m13.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> provides an average <math alttext="11.3\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p2.14.m14.1"><semantics id="S6.SS2.p2.14.m14.1a"><mrow id="S6.SS2.p2.14.m14.1b"><mn id="S6.SS2.p2.14.m14.1.1">11.3</mn><mo id="S6.SS2.p2.14.m14.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p2.14.m14.1c">11.3\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.14.m14.1d">11.3 ×</annotation></semantics></math> (maximum
<math alttext="15.1\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p2.15.m15.1"><semantics id="S6.SS2.p2.15.m15.1a"><mrow id="S6.SS2.p2.15.m15.1b"><mn id="S6.SS2.p2.15.m15.1.1">15.1</mn><mo id="S6.SS2.p2.15.m15.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p2.15.m15.1c">15.1\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.15.m15.1d">15.1 ×</annotation></semantics></math>) energy-efficiency improvement, demonstrating its merits.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.8"><span class="ltx_text ltx_font_bold" id="S6.SS2.p3.8.1">Cost-efficiency (TCO).</span>
Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F15" title="Figure 15 ‣ VI-B 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜’s Effect on Energy-Efficiency and TCO ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">15</span></a>(b) compares the cost-efficiency of <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.p3.1.m1.1"><semantics id="S6.SS2.p3.1.m1.1a"><mrow id="S6.SS2.p3.1.m1.1.1" xref="S6.SS2.p3.1.m1.1.1.cmml"><mi id="S6.SS2.p3.1.m1.1.1.2" xref="S6.SS2.p3.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS2.p3.1.m1.1.1.1" xref="S6.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.1.m1.1.1.3" xref="S6.SS2.p3.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS2.p3.1.m1.1.1.1a" xref="S6.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.1.m1.1.1.4" xref="S6.SS2.p3.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS2.p3.1.m1.1.1.1b" xref="S6.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.1.m1.1.1.5" xref="S6.SS2.p3.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS2.p3.1.m1.1.1.1c" xref="S6.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.1.m1.1.1.6" xref="S6.SS2.p3.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS2.p3.1.m1.1.1.1d" xref="S6.SS2.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.1.m1.1.1.7" xref="S6.SS2.p3.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.1.m1.1b"><apply id="S6.SS2.p3.1.m1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1"><times id="S6.SS2.p3.1.m1.1.1.1.cmml" xref="S6.SS2.p3.1.m1.1.1.1"></times><ci id="S6.SS2.p3.1.m1.1.1.2.cmml" xref="S6.SS2.p3.1.m1.1.1.2">𝑃</ci><ci id="S6.SS2.p3.1.m1.1.1.3.cmml" xref="S6.SS2.p3.1.m1.1.1.3">𝑟</ci><ci id="S6.SS2.p3.1.m1.1.1.4.cmml" xref="S6.SS2.p3.1.m1.1.1.4">𝑒</ci><ci id="S6.SS2.p3.1.m1.1.1.5.cmml" xref="S6.SS2.p3.1.m1.1.1.5">𝑆</ci><ci id="S6.SS2.p3.1.m1.1.1.6.cmml" xref="S6.SS2.p3.1.m1.1.1.6">𝑡</ci><ci id="S6.SS2.p3.1.m1.1.1.7.cmml" xref="S6.SS2.p3.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> and <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS2.p3.2.m2.1"><semantics id="S6.SS2.p3.2.m2.1a"><mrow id="S6.SS2.p3.2.m2.1.1" xref="S6.SS2.p3.2.m2.1.1.cmml"><mi id="S6.SS2.p3.2.m2.1.1.2" xref="S6.SS2.p3.2.m2.1.1.2.cmml">D</mi><mo id="S6.SS2.p3.2.m2.1.1.1" xref="S6.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.2.m2.1.1.3" xref="S6.SS2.p3.2.m2.1.1.3.cmml">i</mi><mo id="S6.SS2.p3.2.m2.1.1.1a" xref="S6.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.2.m2.1.1.4" xref="S6.SS2.p3.2.m2.1.1.4.cmml">s</mi><mo id="S6.SS2.p3.2.m2.1.1.1b" xref="S6.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.2.m2.1.1.5" xref="S6.SS2.p3.2.m2.1.1.5.cmml">a</mi><mo id="S6.SS2.p3.2.m2.1.1.1c" xref="S6.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.2.m2.1.1.6" xref="S6.SS2.p3.2.m2.1.1.6.cmml">g</mi><mo id="S6.SS2.p3.2.m2.1.1.1d" xref="S6.SS2.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.2.m2.1.1.7" xref="S6.SS2.p3.2.m2.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.2.m2.1b"><apply id="S6.SS2.p3.2.m2.1.1.cmml" xref="S6.SS2.p3.2.m2.1.1"><times id="S6.SS2.p3.2.m2.1.1.1.cmml" xref="S6.SS2.p3.2.m2.1.1.1"></times><ci id="S6.SS2.p3.2.m2.1.1.2.cmml" xref="S6.SS2.p3.2.m2.1.1.2">𝐷</ci><ci id="S6.SS2.p3.2.m2.1.1.3.cmml" xref="S6.SS2.p3.2.m2.1.1.3">𝑖</ci><ci id="S6.SS2.p3.2.m2.1.1.4.cmml" xref="S6.SS2.p3.2.m2.1.1.4">𝑠</ci><ci id="S6.SS2.p3.2.m2.1.1.5.cmml" xref="S6.SS2.p3.2.m2.1.1.5">𝑎</ci><ci id="S6.SS2.p3.2.m2.1.1.6.cmml" xref="S6.SS2.p3.2.m2.1.1.6">𝑔</ci><ci id="S6.SS2.p3.2.m2.1.1.7.cmml" xref="S6.SS2.p3.2.m2.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.2.m2.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.2.m2.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> for data preprocessing (as defined in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S5.SS3" title="V-C Evaluation Methods ‣ V Methodology ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-C</span></span></a>). Overall, <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.p3.3.m3.1"><semantics id="S6.SS2.p3.3.m3.1a"><mrow id="S6.SS2.p3.3.m3.1.1" xref="S6.SS2.p3.3.m3.1.1.cmml"><mi id="S6.SS2.p3.3.m3.1.1.2" xref="S6.SS2.p3.3.m3.1.1.2.cmml">P</mi><mo id="S6.SS2.p3.3.m3.1.1.1" xref="S6.SS2.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.3.m3.1.1.3" xref="S6.SS2.p3.3.m3.1.1.3.cmml">r</mi><mo id="S6.SS2.p3.3.m3.1.1.1a" xref="S6.SS2.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.3.m3.1.1.4" xref="S6.SS2.p3.3.m3.1.1.4.cmml">e</mi><mo id="S6.SS2.p3.3.m3.1.1.1b" xref="S6.SS2.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.3.m3.1.1.5" xref="S6.SS2.p3.3.m3.1.1.5.cmml">S</mi><mo id="S6.SS2.p3.3.m3.1.1.1c" xref="S6.SS2.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.3.m3.1.1.6" xref="S6.SS2.p3.3.m3.1.1.6.cmml">t</mi><mo id="S6.SS2.p3.3.m3.1.1.1d" xref="S6.SS2.p3.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.3.m3.1.1.7" xref="S6.SS2.p3.3.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.3.m3.1b"><apply id="S6.SS2.p3.3.m3.1.1.cmml" xref="S6.SS2.p3.3.m3.1.1"><times id="S6.SS2.p3.3.m3.1.1.1.cmml" xref="S6.SS2.p3.3.m3.1.1.1"></times><ci id="S6.SS2.p3.3.m3.1.1.2.cmml" xref="S6.SS2.p3.3.m3.1.1.2">𝑃</ci><ci id="S6.SS2.p3.3.m3.1.1.3.cmml" xref="S6.SS2.p3.3.m3.1.1.3">𝑟</ci><ci id="S6.SS2.p3.3.m3.1.1.4.cmml" xref="S6.SS2.p3.3.m3.1.1.4">𝑒</ci><ci id="S6.SS2.p3.3.m3.1.1.5.cmml" xref="S6.SS2.p3.3.m3.1.1.5">𝑆</ci><ci id="S6.SS2.p3.3.m3.1.1.6.cmml" xref="S6.SS2.p3.3.m3.1.1.6">𝑡</ci><ci id="S6.SS2.p3.3.m3.1.1.7.cmml" xref="S6.SS2.p3.3.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.3.m3.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.3.m3.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> provides an average <math alttext="4.3\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p3.4.m4.1"><semantics id="S6.SS2.p3.4.m4.1a"><mrow id="S6.SS2.p3.4.m4.1b"><mn id="S6.SS2.p3.4.m4.1.1">4.3</mn><mo id="S6.SS2.p3.4.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p3.4.m4.1c">4.3\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.4.m4.1d">4.3 ×</annotation></semantics></math>
(maximum <math alttext="5.6\times" class="ltx_math_unparsed" display="inline" id="S6.SS2.p3.5.m5.1"><semantics id="S6.SS2.p3.5.m5.1a"><mrow id="S6.SS2.p3.5.m5.1b"><mn id="S6.SS2.p3.5.m5.1.1">5.6</mn><mo id="S6.SS2.p3.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS2.p3.5.m5.1c">5.6\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.5.m5.1d">5.6 ×</annotation></semantics></math>) improvement in cost-efficiency vs. <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS2.p3.6.m6.1"><semantics id="S6.SS2.p3.6.m6.1a"><mrow id="S6.SS2.p3.6.m6.1.1" xref="S6.SS2.p3.6.m6.1.1.cmml"><mi id="S6.SS2.p3.6.m6.1.1.2" xref="S6.SS2.p3.6.m6.1.1.2.cmml">D</mi><mo id="S6.SS2.p3.6.m6.1.1.1" xref="S6.SS2.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.6.m6.1.1.3" xref="S6.SS2.p3.6.m6.1.1.3.cmml">i</mi><mo id="S6.SS2.p3.6.m6.1.1.1a" xref="S6.SS2.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.6.m6.1.1.4" xref="S6.SS2.p3.6.m6.1.1.4.cmml">s</mi><mo id="S6.SS2.p3.6.m6.1.1.1b" xref="S6.SS2.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.6.m6.1.1.5" xref="S6.SS2.p3.6.m6.1.1.5.cmml">a</mi><mo id="S6.SS2.p3.6.m6.1.1.1c" xref="S6.SS2.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.6.m6.1.1.6" xref="S6.SS2.p3.6.m6.1.1.6.cmml">g</mi><mo id="S6.SS2.p3.6.m6.1.1.1d" xref="S6.SS2.p3.6.m6.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.6.m6.1.1.7" xref="S6.SS2.p3.6.m6.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.6.m6.1b"><apply id="S6.SS2.p3.6.m6.1.1.cmml" xref="S6.SS2.p3.6.m6.1.1"><times id="S6.SS2.p3.6.m6.1.1.1.cmml" xref="S6.SS2.p3.6.m6.1.1.1"></times><ci id="S6.SS2.p3.6.m6.1.1.2.cmml" xref="S6.SS2.p3.6.m6.1.1.2">𝐷</ci><ci id="S6.SS2.p3.6.m6.1.1.3.cmml" xref="S6.SS2.p3.6.m6.1.1.3">𝑖</ci><ci id="S6.SS2.p3.6.m6.1.1.4.cmml" xref="S6.SS2.p3.6.m6.1.1.4">𝑠</ci><ci id="S6.SS2.p3.6.m6.1.1.5.cmml" xref="S6.SS2.p3.6.m6.1.1.5">𝑎</ci><ci id="S6.SS2.p3.6.m6.1.1.6.cmml" xref="S6.SS2.p3.6.m6.1.1.6">𝑔</ci><ci id="S6.SS2.p3.6.m6.1.1.7.cmml" xref="S6.SS2.p3.6.m6.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.6.m6.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.6.m6.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>.
Cost-efficiency is primarily determined by both CapEx and OpEx and our
experiments thus far have demonstrated that <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS2.p3.7.m7.1"><semantics id="S6.SS2.p3.7.m7.1a"><mrow id="S6.SS2.p3.7.m7.1.1" xref="S6.SS2.p3.7.m7.1.1.cmml"><mi id="S6.SS2.p3.7.m7.1.1.2" xref="S6.SS2.p3.7.m7.1.1.2.cmml">P</mi><mo id="S6.SS2.p3.7.m7.1.1.1" xref="S6.SS2.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.7.m7.1.1.3" xref="S6.SS2.p3.7.m7.1.1.3.cmml">r</mi><mo id="S6.SS2.p3.7.m7.1.1.1a" xref="S6.SS2.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.7.m7.1.1.4" xref="S6.SS2.p3.7.m7.1.1.4.cmml">e</mi><mo id="S6.SS2.p3.7.m7.1.1.1b" xref="S6.SS2.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.7.m7.1.1.5" xref="S6.SS2.p3.7.m7.1.1.5.cmml">S</mi><mo id="S6.SS2.p3.7.m7.1.1.1c" xref="S6.SS2.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.7.m7.1.1.6" xref="S6.SS2.p3.7.m7.1.1.6.cmml">t</mi><mo id="S6.SS2.p3.7.m7.1.1.1d" xref="S6.SS2.p3.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.7.m7.1.1.7" xref="S6.SS2.p3.7.m7.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.7.m7.1b"><apply id="S6.SS2.p3.7.m7.1.1.cmml" xref="S6.SS2.p3.7.m7.1.1"><times id="S6.SS2.p3.7.m7.1.1.1.cmml" xref="S6.SS2.p3.7.m7.1.1.1"></times><ci id="S6.SS2.p3.7.m7.1.1.2.cmml" xref="S6.SS2.p3.7.m7.1.1.2">𝑃</ci><ci id="S6.SS2.p3.7.m7.1.1.3.cmml" xref="S6.SS2.p3.7.m7.1.1.3">𝑟</ci><ci id="S6.SS2.p3.7.m7.1.1.4.cmml" xref="S6.SS2.p3.7.m7.1.1.4">𝑒</ci><ci id="S6.SS2.p3.7.m7.1.1.5.cmml" xref="S6.SS2.p3.7.m7.1.1.5">𝑆</ci><ci id="S6.SS2.p3.7.m7.1.1.6.cmml" xref="S6.SS2.p3.7.m7.1.1.6">𝑡</ci><ci id="S6.SS2.p3.7.m7.1.1.7.cmml" xref="S6.SS2.p3.7.m7.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.7.m7.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.7.m7.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> outperforms <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS2.p3.8.m8.1"><semantics id="S6.SS2.p3.8.m8.1a"><mrow id="S6.SS2.p3.8.m8.1.1" xref="S6.SS2.p3.8.m8.1.1.cmml"><mi id="S6.SS2.p3.8.m8.1.1.2" xref="S6.SS2.p3.8.m8.1.1.2.cmml">D</mi><mo id="S6.SS2.p3.8.m8.1.1.1" xref="S6.SS2.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.8.m8.1.1.3" xref="S6.SS2.p3.8.m8.1.1.3.cmml">i</mi><mo id="S6.SS2.p3.8.m8.1.1.1a" xref="S6.SS2.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.8.m8.1.1.4" xref="S6.SS2.p3.8.m8.1.1.4.cmml">s</mi><mo id="S6.SS2.p3.8.m8.1.1.1b" xref="S6.SS2.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.8.m8.1.1.5" xref="S6.SS2.p3.8.m8.1.1.5.cmml">a</mi><mo id="S6.SS2.p3.8.m8.1.1.1c" xref="S6.SS2.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.8.m8.1.1.6" xref="S6.SS2.p3.8.m8.1.1.6.cmml">g</mi><mo id="S6.SS2.p3.8.m8.1.1.1d" xref="S6.SS2.p3.8.m8.1.1.1.cmml">⁢</mo><mi id="S6.SS2.p3.8.m8.1.1.7" xref="S6.SS2.p3.8.m8.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p3.8.m8.1b"><apply id="S6.SS2.p3.8.m8.1.1.cmml" xref="S6.SS2.p3.8.m8.1.1"><times id="S6.SS2.p3.8.m8.1.1.1.cmml" xref="S6.SS2.p3.8.m8.1.1.1"></times><ci id="S6.SS2.p3.8.m8.1.1.2.cmml" xref="S6.SS2.p3.8.m8.1.1.2">𝐷</ci><ci id="S6.SS2.p3.8.m8.1.1.3.cmml" xref="S6.SS2.p3.8.m8.1.1.3">𝑖</ci><ci id="S6.SS2.p3.8.m8.1.1.4.cmml" xref="S6.SS2.p3.8.m8.1.1.4">𝑠</ci><ci id="S6.SS2.p3.8.m8.1.1.5.cmml" xref="S6.SS2.p3.8.m8.1.1.5">𝑎</ci><ci id="S6.SS2.p3.8.m8.1.1.6.cmml" xref="S6.SS2.p3.8.m8.1.1.6">𝑔</ci><ci id="S6.SS2.p3.8.m8.1.1.7.cmml" xref="S6.SS2.p3.8.m8.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p3.8.m8.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p3.8.m8.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> on
both fronts, providing significant reduction in TCO.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS3.5.1.1">VI-C</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS3.6.2">PreSto vs. Alternative Accelerated Preprocessing</span>
</h3>
<figure class="ltx_figure" id="S6.F16"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="167" id="S6.F16.g1" src="x16.png" width="398"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Data preprocessing’s (left axis) performance and (right axis) performance/Watt
over <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.F16.3.m1.1"><semantics id="S6.F16.3.m1.1b"><mrow id="S6.F16.3.m1.1.1" xref="S6.F16.3.m1.1.1.cmml"><mi id="S6.F16.3.m1.1.1.2" xref="S6.F16.3.m1.1.1.2.cmml">P</mi><mo id="S6.F16.3.m1.1.1.1" xref="S6.F16.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F16.3.m1.1.1.3" xref="S6.F16.3.m1.1.1.3.cmml">r</mi><mo id="S6.F16.3.m1.1.1.1b" xref="S6.F16.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F16.3.m1.1.1.4" xref="S6.F16.3.m1.1.1.4.cmml">e</mi><mo id="S6.F16.3.m1.1.1.1c" xref="S6.F16.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F16.3.m1.1.1.5" xref="S6.F16.3.m1.1.1.5.cmml">S</mi><mo id="S6.F16.3.m1.1.1.1d" xref="S6.F16.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F16.3.m1.1.1.6" xref="S6.F16.3.m1.1.1.6.cmml">t</mi><mo id="S6.F16.3.m1.1.1.1e" xref="S6.F16.3.m1.1.1.1.cmml">⁢</mo><mi id="S6.F16.3.m1.1.1.7" xref="S6.F16.3.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F16.3.m1.1c"><apply id="S6.F16.3.m1.1.1.cmml" xref="S6.F16.3.m1.1.1"><times id="S6.F16.3.m1.1.1.1.cmml" xref="S6.F16.3.m1.1.1.1"></times><ci id="S6.F16.3.m1.1.1.2.cmml" xref="S6.F16.3.m1.1.1.2">𝑃</ci><ci id="S6.F16.3.m1.1.1.3.cmml" xref="S6.F16.3.m1.1.1.3">𝑟</ci><ci id="S6.F16.3.m1.1.1.4.cmml" xref="S6.F16.3.m1.1.1.4">𝑒</ci><ci id="S6.F16.3.m1.1.1.5.cmml" xref="S6.F16.3.m1.1.1.5">𝑆</ci><ci id="S6.F16.3.m1.1.1.6.cmml" xref="S6.F16.3.m1.1.1.6">𝑡</ci><ci id="S6.F16.3.m1.1.1.7.cmml" xref="S6.F16.3.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F16.3.m1.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.F16.3.m1.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (single SmartSSD device), <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.F16.4.m2.1"><semantics id="S6.F16.4.m2.1b"><mrow id="S6.F16.4.m2.1.1" xref="S6.F16.4.m2.1.1.cmml"><mi id="S6.F16.4.m2.1.1.2" xref="S6.F16.4.m2.1.1.2.cmml">P</mi><mo id="S6.F16.4.m2.1.1.1" xref="S6.F16.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F16.4.m2.1.1.3" xref="S6.F16.4.m2.1.1.3.cmml">r</mi><mo id="S6.F16.4.m2.1.1.1b" xref="S6.F16.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F16.4.m2.1.1.4" xref="S6.F16.4.m2.1.1.4.cmml">e</mi><mo id="S6.F16.4.m2.1.1.1c" xref="S6.F16.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F16.4.m2.1.1.5" xref="S6.F16.4.m2.1.1.5.cmml">S</mi><mo id="S6.F16.4.m2.1.1.1d" xref="S6.F16.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F16.4.m2.1.1.6" xref="S6.F16.4.m2.1.1.6.cmml">t</mi><mo id="S6.F16.4.m2.1.1.1e" xref="S6.F16.4.m2.1.1.1.cmml">⁢</mo><mi id="S6.F16.4.m2.1.1.7" xref="S6.F16.4.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F16.4.m2.1c"><apply id="S6.F16.4.m2.1.1.cmml" xref="S6.F16.4.m2.1.1"><times id="S6.F16.4.m2.1.1.1.cmml" xref="S6.F16.4.m2.1.1.1"></times><ci id="S6.F16.4.m2.1.1.2.cmml" xref="S6.F16.4.m2.1.1.2">𝑃</ci><ci id="S6.F16.4.m2.1.1.3.cmml" xref="S6.F16.4.m2.1.1.3">𝑟</ci><ci id="S6.F16.4.m2.1.1.4.cmml" xref="S6.F16.4.m2.1.1.4">𝑒</ci><ci id="S6.F16.4.m2.1.1.5.cmml" xref="S6.F16.4.m2.1.1.5">𝑆</ci><ci id="S6.F16.4.m2.1.1.6.cmml" xref="S6.F16.4.m2.1.1.6">𝑡</ci><ci id="S6.F16.4.m2.1.1.7.cmml" xref="S6.F16.4.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F16.4.m2.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.F16.4.m2.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (single U280 FPGA), a single A100 GPU and a single U280 FPGA.
</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F17"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="140" id="S6.F17.g1" src="x17.png" width="398"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Latency of <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.F17.8.m1.1"><semantics id="S6.F17.8.m1.1b"><mrow id="S6.F17.8.m1.1.1" xref="S6.F17.8.m1.1.1.cmml"><mi id="S6.F17.8.m1.1.1.2" xref="S6.F17.8.m1.1.1.2.cmml">D</mi><mo id="S6.F17.8.m1.1.1.1" xref="S6.F17.8.m1.1.1.1.cmml">⁢</mo><mi id="S6.F17.8.m1.1.1.3" xref="S6.F17.8.m1.1.1.3.cmml">i</mi><mo id="S6.F17.8.m1.1.1.1b" xref="S6.F17.8.m1.1.1.1.cmml">⁢</mo><mi id="S6.F17.8.m1.1.1.4" xref="S6.F17.8.m1.1.1.4.cmml">s</mi><mo id="S6.F17.8.m1.1.1.1c" xref="S6.F17.8.m1.1.1.1.cmml">⁢</mo><mi id="S6.F17.8.m1.1.1.5" xref="S6.F17.8.m1.1.1.5.cmml">a</mi><mo id="S6.F17.8.m1.1.1.1d" xref="S6.F17.8.m1.1.1.1.cmml">⁢</mo><mi id="S6.F17.8.m1.1.1.6" xref="S6.F17.8.m1.1.1.6.cmml">g</mi><mo id="S6.F17.8.m1.1.1.1e" xref="S6.F17.8.m1.1.1.1.cmml">⁢</mo><mi id="S6.F17.8.m1.1.1.7" xref="S6.F17.8.m1.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F17.8.m1.1c"><apply id="S6.F17.8.m1.1.1.cmml" xref="S6.F17.8.m1.1.1"><times id="S6.F17.8.m1.1.1.1.cmml" xref="S6.F17.8.m1.1.1.1"></times><ci id="S6.F17.8.m1.1.1.2.cmml" xref="S6.F17.8.m1.1.1.2">𝐷</ci><ci id="S6.F17.8.m1.1.1.3.cmml" xref="S6.F17.8.m1.1.1.3">𝑖</ci><ci id="S6.F17.8.m1.1.1.4.cmml" xref="S6.F17.8.m1.1.1.4">𝑠</ci><ci id="S6.F17.8.m1.1.1.5.cmml" xref="S6.F17.8.m1.1.1.5">𝑎</ci><ci id="S6.F17.8.m1.1.1.6.cmml" xref="S6.F17.8.m1.1.1.6">𝑔</ci><ci id="S6.F17.8.m1.1.1.7.cmml" xref="S6.F17.8.m1.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F17.8.m1.1d">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.F17.8.m1.1e">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> and <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.F17.9.m2.1"><semantics id="S6.F17.9.m2.1b"><mrow id="S6.F17.9.m2.1.1" xref="S6.F17.9.m2.1.1.cmml"><mi id="S6.F17.9.m2.1.1.2" xref="S6.F17.9.m2.1.1.2.cmml">P</mi><mo id="S6.F17.9.m2.1.1.1" xref="S6.F17.9.m2.1.1.1.cmml">⁢</mo><mi id="S6.F17.9.m2.1.1.3" xref="S6.F17.9.m2.1.1.3.cmml">r</mi><mo id="S6.F17.9.m2.1.1.1b" xref="S6.F17.9.m2.1.1.1.cmml">⁢</mo><mi id="S6.F17.9.m2.1.1.4" xref="S6.F17.9.m2.1.1.4.cmml">e</mi><mo id="S6.F17.9.m2.1.1.1c" xref="S6.F17.9.m2.1.1.1.cmml">⁢</mo><mi id="S6.F17.9.m2.1.1.5" xref="S6.F17.9.m2.1.1.5.cmml">S</mi><mo id="S6.F17.9.m2.1.1.1d" xref="S6.F17.9.m2.1.1.1.cmml">⁢</mo><mi id="S6.F17.9.m2.1.1.6" xref="S6.F17.9.m2.1.1.6.cmml">t</mi><mo id="S6.F17.9.m2.1.1.1e" xref="S6.F17.9.m2.1.1.1.cmml">⁢</mo><mi id="S6.F17.9.m2.1.1.7" xref="S6.F17.9.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F17.9.m2.1c"><apply id="S6.F17.9.m2.1.1.cmml" xref="S6.F17.9.m2.1.1"><times id="S6.F17.9.m2.1.1.1.cmml" xref="S6.F17.9.m2.1.1.1"></times><ci id="S6.F17.9.m2.1.1.2.cmml" xref="S6.F17.9.m2.1.1.2">𝑃</ci><ci id="S6.F17.9.m2.1.1.3.cmml" xref="S6.F17.9.m2.1.1.3">𝑟</ci><ci id="S6.F17.9.m2.1.1.4.cmml" xref="S6.F17.9.m2.1.1.4">𝑒</ci><ci id="S6.F17.9.m2.1.1.5.cmml" xref="S6.F17.9.m2.1.1.5">𝑆</ci><ci id="S6.F17.9.m2.1.1.6.cmml" xref="S6.F17.9.m2.1.1.6">𝑡</ci><ci id="S6.F17.9.m2.1.1.7.cmml" xref="S6.F17.9.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F17.9.m2.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.F17.9.m2.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s feature generation/normalization time (left axis) and <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.F17.10.m3.1"><semantics id="S6.F17.10.m3.1b"><mrow id="S6.F17.10.m3.1.1" xref="S6.F17.10.m3.1.1.cmml"><mi id="S6.F17.10.m3.1.1.2" xref="S6.F17.10.m3.1.1.2.cmml">P</mi><mo id="S6.F17.10.m3.1.1.1" xref="S6.F17.10.m3.1.1.1.cmml">⁢</mo><mi id="S6.F17.10.m3.1.1.3" xref="S6.F17.10.m3.1.1.3.cmml">r</mi><mo id="S6.F17.10.m3.1.1.1b" xref="S6.F17.10.m3.1.1.1.cmml">⁢</mo><mi id="S6.F17.10.m3.1.1.4" xref="S6.F17.10.m3.1.1.4.cmml">e</mi><mo id="S6.F17.10.m3.1.1.1c" xref="S6.F17.10.m3.1.1.1.cmml">⁢</mo><mi id="S6.F17.10.m3.1.1.5" xref="S6.F17.10.m3.1.1.5.cmml">S</mi><mo id="S6.F17.10.m3.1.1.1d" xref="S6.F17.10.m3.1.1.1.cmml">⁢</mo><mi id="S6.F17.10.m3.1.1.6" xref="S6.F17.10.m3.1.1.6.cmml">t</mi><mo id="S6.F17.10.m3.1.1.1e" xref="S6.F17.10.m3.1.1.1.cmml">⁢</mo><mi id="S6.F17.10.m3.1.1.7" xref="S6.F17.10.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F17.10.m3.1c"><apply id="S6.F17.10.m3.1.1.cmml" xref="S6.F17.10.m3.1.1"><times id="S6.F17.10.m3.1.1.1.cmml" xref="S6.F17.10.m3.1.1.1"></times><ci id="S6.F17.10.m3.1.1.2.cmml" xref="S6.F17.10.m3.1.1.2">𝑃</ci><ci id="S6.F17.10.m3.1.1.3.cmml" xref="S6.F17.10.m3.1.1.3">𝑟</ci><ci id="S6.F17.10.m3.1.1.4.cmml" xref="S6.F17.10.m3.1.1.4">𝑒</ci><ci id="S6.F17.10.m3.1.1.5.cmml" xref="S6.F17.10.m3.1.1.5">𝑆</ci><ci id="S6.F17.10.m3.1.1.6.cmml" xref="S6.F17.10.m3.1.1.6">𝑡</ci><ci id="S6.F17.10.m3.1.1.7.cmml" xref="S6.F17.10.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F17.10.m3.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.F17.10.m3.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s speedup (right axis) when the
the number of features for preprocessing is changed. The “<math alttext="1\times" class="ltx_math_unparsed" display="inline" id="S6.F17.11.m4.1"><semantics id="S6.F17.11.m4.1b"><mrow id="S6.F17.11.m4.1c"><mn id="S6.F17.11.m4.1.1">1</mn><mo id="S6.F17.11.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.F17.11.m4.1d">1\times</annotation><annotation encoding="application/x-llamapun" id="S6.F17.11.m4.1e">1 ×</annotation></semantics></math>” data point corresponds to the RM5 configuration in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S3.T1" title="TABLE I ‣ III Characterization and Motivation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">I</span></a>. <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.F17.12.m5.1"><semantics id="S6.F17.12.m5.1b"><mrow id="S6.F17.12.m5.1.1" xref="S6.F17.12.m5.1.1.cmml"><mi id="S6.F17.12.m5.1.1.2" xref="S6.F17.12.m5.1.1.2.cmml">D</mi><mo id="S6.F17.12.m5.1.1.1" xref="S6.F17.12.m5.1.1.1.cmml">⁢</mo><mi id="S6.F17.12.m5.1.1.3" xref="S6.F17.12.m5.1.1.3.cmml">i</mi><mo id="S6.F17.12.m5.1.1.1b" xref="S6.F17.12.m5.1.1.1.cmml">⁢</mo><mi id="S6.F17.12.m5.1.1.4" xref="S6.F17.12.m5.1.1.4.cmml">s</mi><mo id="S6.F17.12.m5.1.1.1c" xref="S6.F17.12.m5.1.1.1.cmml">⁢</mo><mi id="S6.F17.12.m5.1.1.5" xref="S6.F17.12.m5.1.1.5.cmml">a</mi><mo id="S6.F17.12.m5.1.1.1d" xref="S6.F17.12.m5.1.1.1.cmml">⁢</mo><mi id="S6.F17.12.m5.1.1.6" xref="S6.F17.12.m5.1.1.6.cmml">g</mi><mo id="S6.F17.12.m5.1.1.1e" xref="S6.F17.12.m5.1.1.1.cmml">⁢</mo><mi id="S6.F17.12.m5.1.1.7" xref="S6.F17.12.m5.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F17.12.m5.1c"><apply id="S6.F17.12.m5.1.1.cmml" xref="S6.F17.12.m5.1.1"><times id="S6.F17.12.m5.1.1.1.cmml" xref="S6.F17.12.m5.1.1.1"></times><ci id="S6.F17.12.m5.1.1.2.cmml" xref="S6.F17.12.m5.1.1.2">𝐷</ci><ci id="S6.F17.12.m5.1.1.3.cmml" xref="S6.F17.12.m5.1.1.3">𝑖</ci><ci id="S6.F17.12.m5.1.1.4.cmml" xref="S6.F17.12.m5.1.1.4">𝑠</ci><ci id="S6.F17.12.m5.1.1.5.cmml" xref="S6.F17.12.m5.1.1.5">𝑎</ci><ci id="S6.F17.12.m5.1.1.6.cmml" xref="S6.F17.12.m5.1.1.6">𝑔</ci><ci id="S6.F17.12.m5.1.1.7.cmml" xref="S6.F17.12.m5.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F17.12.m5.1d">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.F17.12.m5.1e">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math>’s latency to conduct each operation is normalized to its respective “<math alttext="1\times" class="ltx_math_unparsed" display="inline" id="S6.F17.13.m6.1"><semantics id="S6.F17.13.m6.1b"><mrow id="S6.F17.13.m6.1c"><mn id="S6.F17.13.m6.1.1">1</mn><mo id="S6.F17.13.m6.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.F17.13.m6.1d">1\times</annotation><annotation encoding="application/x-llamapun" id="S6.F17.13.m6.1e">1 ×</annotation></semantics></math>” latency of <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.F17.14.m7.1"><semantics id="S6.F17.14.m7.1b"><mrow id="S6.F17.14.m7.1.1" xref="S6.F17.14.m7.1.1.cmml"><mi id="S6.F17.14.m7.1.1.2" xref="S6.F17.14.m7.1.1.2.cmml">P</mi><mo id="S6.F17.14.m7.1.1.1" xref="S6.F17.14.m7.1.1.1.cmml">⁢</mo><mi id="S6.F17.14.m7.1.1.3" xref="S6.F17.14.m7.1.1.3.cmml">r</mi><mo id="S6.F17.14.m7.1.1.1b" xref="S6.F17.14.m7.1.1.1.cmml">⁢</mo><mi id="S6.F17.14.m7.1.1.4" xref="S6.F17.14.m7.1.1.4.cmml">e</mi><mo id="S6.F17.14.m7.1.1.1c" xref="S6.F17.14.m7.1.1.1.cmml">⁢</mo><mi id="S6.F17.14.m7.1.1.5" xref="S6.F17.14.m7.1.1.5.cmml">S</mi><mo id="S6.F17.14.m7.1.1.1d" xref="S6.F17.14.m7.1.1.1.cmml">⁢</mo><mi id="S6.F17.14.m7.1.1.6" xref="S6.F17.14.m7.1.1.6.cmml">t</mi><mo id="S6.F17.14.m7.1.1.1e" xref="S6.F17.14.m7.1.1.1.cmml">⁢</mo><mi id="S6.F17.14.m7.1.1.7" xref="S6.F17.14.m7.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.F17.14.m7.1c"><apply id="S6.F17.14.m7.1.1.cmml" xref="S6.F17.14.m7.1.1"><times id="S6.F17.14.m7.1.1.1.cmml" xref="S6.F17.14.m7.1.1.1"></times><ci id="S6.F17.14.m7.1.1.2.cmml" xref="S6.F17.14.m7.1.1.2">𝑃</ci><ci id="S6.F17.14.m7.1.1.3.cmml" xref="S6.F17.14.m7.1.1.3">𝑟</ci><ci id="S6.F17.14.m7.1.1.4.cmml" xref="S6.F17.14.m7.1.1.4">𝑒</ci><ci id="S6.F17.14.m7.1.1.5.cmml" xref="S6.F17.14.m7.1.1.5">𝑆</ci><ci id="S6.F17.14.m7.1.1.6.cmml" xref="S6.F17.14.m7.1.1.6">𝑡</ci><ci id="S6.F17.14.m7.1.1.7.cmml" xref="S6.F17.14.m7.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F17.14.m7.1d">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.F17.14.m7.1e">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>.
</figcaption>
</figure>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.16">Our study thus far have focused on comparing data preprocessing over the
baseline disaggregated CPU servers and ISP architectures.
For the completeness of our study, we also evaluate the efficacy of alternative accelerated preprocessing solutions where high-end GPUs/FPGAs
function as preprocessing accelerators. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F16" title="Figure 16 ‣ VI-C PreSto vs. Alternative Accelerated Preprocessing ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">16</span></a> compares the preprocessing performance (left axis)
and performance/Watt (energy-efficiency) with four system design points: (1) a single A100 GPU (denoted “A100”) and (2) a single Xilinx U280<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib67" title="">67</a>]</cite> FPGA (denoted “U280”) employed within a disaggregated accelerator pool (Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S4.F7" title="Figure 7 ‣ IV 𝑃⁢𝑟⁢𝑒⁢𝑆⁢𝑡⁢𝑜: An In-Storage “Pre”processing Architecture for RecSys Training ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">7</span></a>(b)), as well as our proposed FPGA-based accelerator system implemented using (3) a single, discrete U280 FPGA card integrated within the storage node over PCIe (denoted “<math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.1.m1.1"><semantics id="S6.SS3.p1.1.m1.1a"><mrow id="S6.SS3.p1.1.m1.1.1" xref="S6.SS3.p1.1.m1.1.1.cmml"><mi id="S6.SS3.p1.1.m1.1.1.2" xref="S6.SS3.p1.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.1.m1.1.1.1" xref="S6.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.1.m1.1.1.3" xref="S6.SS3.p1.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.1.m1.1.1.1a" xref="S6.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.1.m1.1.1.4" xref="S6.SS3.p1.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.1.m1.1.1.1b" xref="S6.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.1.m1.1.1.5" xref="S6.SS3.p1.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.1.m1.1.1.1c" xref="S6.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.1.m1.1.1.6" xref="S6.SS3.p1.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.1.m1.1.1.1d" xref="S6.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.1.m1.1.1.7" xref="S6.SS3.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.1.m1.1b"><apply id="S6.SS3.p1.1.m1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1"><times id="S6.SS3.p1.1.m1.1.1.1.cmml" xref="S6.SS3.p1.1.m1.1.1.1"></times><ci id="S6.SS3.p1.1.m1.1.1.2.cmml" xref="S6.SS3.p1.1.m1.1.1.2">𝑃</ci><ci id="S6.SS3.p1.1.m1.1.1.3.cmml" xref="S6.SS3.p1.1.m1.1.1.3">𝑟</ci><ci id="S6.SS3.p1.1.m1.1.1.4.cmml" xref="S6.SS3.p1.1.m1.1.1.4">𝑒</ci><ci id="S6.SS3.p1.1.m1.1.1.5.cmml" xref="S6.SS3.p1.1.m1.1.1.5">𝑆</ci><ci id="S6.SS3.p1.1.m1.1.1.6.cmml" xref="S6.SS3.p1.1.m1.1.1.6">𝑡</ci><ci id="S6.SS3.p1.1.m1.1.1.7.cmml" xref="S6.SS3.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (U280)”) and (4) one implemented using
a single SmartSSD device (denoted “<math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.2.m2.1"><semantics id="S6.SS3.p1.2.m2.1a"><mrow id="S6.SS3.p1.2.m2.1.1" xref="S6.SS3.p1.2.m2.1.1.cmml"><mi id="S6.SS3.p1.2.m2.1.1.2" xref="S6.SS3.p1.2.m2.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.2.m2.1.1.1" xref="S6.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.2.m2.1.1.3" xref="S6.SS3.p1.2.m2.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.2.m2.1.1.1a" xref="S6.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.2.m2.1.1.4" xref="S6.SS3.p1.2.m2.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.2.m2.1.1.1b" xref="S6.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.2.m2.1.1.5" xref="S6.SS3.p1.2.m2.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.2.m2.1.1.1c" xref="S6.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.2.m2.1.1.6" xref="S6.SS3.p1.2.m2.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.2.m2.1.1.1d" xref="S6.SS3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.2.m2.1.1.7" xref="S6.SS3.p1.2.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.2.m2.1b"><apply id="S6.SS3.p1.2.m2.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1"><times id="S6.SS3.p1.2.m2.1.1.1.cmml" xref="S6.SS3.p1.2.m2.1.1.1"></times><ci id="S6.SS3.p1.2.m2.1.1.2.cmml" xref="S6.SS3.p1.2.m2.1.1.2">𝑃</ci><ci id="S6.SS3.p1.2.m2.1.1.3.cmml" xref="S6.SS3.p1.2.m2.1.1.3">𝑟</ci><ci id="S6.SS3.p1.2.m2.1.1.4.cmml" xref="S6.SS3.p1.2.m2.1.1.4">𝑒</ci><ci id="S6.SS3.p1.2.m2.1.1.5.cmml" xref="S6.SS3.p1.2.m2.1.1.5">𝑆</ci><ci id="S6.SS3.p1.2.m2.1.1.6.cmml" xref="S6.SS3.p1.2.m2.1.1.6">𝑡</ci><ci id="S6.SS3.p1.2.m2.1.1.7.cmml" xref="S6.SS3.p1.2.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.2.m2.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.2.m2.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (SmartSSD)”).
We utilizes NVIDIA’s NVTabular library <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib54" title="">54</a>]</cite> for GPU-based data preprocessing. The U280-based FPGA accelerator is
synthesized with <math alttext="2\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p1.3.m3.1"><semantics id="S6.SS3.p1.3.m3.1a"><mrow id="S6.SS3.p1.3.m3.1b"><mn id="S6.SS3.p1.3.m3.1.1">2</mn><mo id="S6.SS3.p1.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p1.3.m3.1c">2\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.3.m3.1d">2 ×</annotation></semantics></math> larger number of Decoder, Feature generation, and Feature normalization units that maximally utilize
U280’s larger custom logics. <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.4.m4.1"><semantics id="S6.SS3.p1.4.m4.1a"><mrow id="S6.SS3.p1.4.m4.1.1" xref="S6.SS3.p1.4.m4.1.1.cmml"><mi id="S6.SS3.p1.4.m4.1.1.2" xref="S6.SS3.p1.4.m4.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.4.m4.1.1.1" xref="S6.SS3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.4.m4.1.1.3" xref="S6.SS3.p1.4.m4.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.4.m4.1.1.1a" xref="S6.SS3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.4.m4.1.1.4" xref="S6.SS3.p1.4.m4.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.4.m4.1.1.1b" xref="S6.SS3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.4.m4.1.1.5" xref="S6.SS3.p1.4.m4.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.4.m4.1.1.1c" xref="S6.SS3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.4.m4.1.1.6" xref="S6.SS3.p1.4.m4.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.4.m4.1.1.1d" xref="S6.SS3.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.4.m4.1.1.7" xref="S6.SS3.p1.4.m4.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.4.m4.1b"><apply id="S6.SS3.p1.4.m4.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1"><times id="S6.SS3.p1.4.m4.1.1.1.cmml" xref="S6.SS3.p1.4.m4.1.1.1"></times><ci id="S6.SS3.p1.4.m4.1.1.2.cmml" xref="S6.SS3.p1.4.m4.1.1.2">𝑃</ci><ci id="S6.SS3.p1.4.m4.1.1.3.cmml" xref="S6.SS3.p1.4.m4.1.1.3">𝑟</ci><ci id="S6.SS3.p1.4.m4.1.1.4.cmml" xref="S6.SS3.p1.4.m4.1.1.4">𝑒</ci><ci id="S6.SS3.p1.4.m4.1.1.5.cmml" xref="S6.SS3.p1.4.m4.1.1.5">𝑆</ci><ci id="S6.SS3.p1.4.m4.1.1.6.cmml" xref="S6.SS3.p1.4.m4.1.1.6">𝑡</ci><ci id="S6.SS3.p1.4.m4.1.1.7.cmml" xref="S6.SS3.p1.4.m4.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.4.m4.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.4.m4.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (SmartSSD) provides an average <math alttext="2.5\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p1.5.m5.1"><semantics id="S6.SS3.p1.5.m5.1a"><mrow id="S6.SS3.p1.5.m5.1b"><mn id="S6.SS3.p1.5.m5.1.1">2.5</mn><mo id="S6.SS3.p1.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p1.5.m5.1c">2.5\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.5.m5.1d">2.5 ×</annotation></semantics></math> speedup vs. A100 while experiencing an average <math alttext="5\%" class="ltx_Math" display="inline" id="S6.SS3.p1.6.m6.1"><semantics id="S6.SS3.p1.6.m6.1a"><mrow id="S6.SS3.p1.6.m6.1.1" xref="S6.SS3.p1.6.m6.1.1.cmml"><mn id="S6.SS3.p1.6.m6.1.1.2" xref="S6.SS3.p1.6.m6.1.1.2.cmml">5</mn><mo id="S6.SS3.p1.6.m6.1.1.1" xref="S6.SS3.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.6.m6.1b"><apply id="S6.SS3.p1.6.m6.1.1.cmml" xref="S6.SS3.p1.6.m6.1.1"><csymbol cd="latexml" id="S6.SS3.p1.6.m6.1.1.1.cmml" xref="S6.SS3.p1.6.m6.1.1.1">percent</csymbol><cn id="S6.SS3.p1.6.m6.1.1.2.cmml" type="integer" xref="S6.SS3.p1.6.m6.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.6.m6.1c">5\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.6.m6.1d">5 %</annotation></semantics></math> performance loss vs. U280
FPGA. Note that <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.7.m7.1"><semantics id="S6.SS3.p1.7.m7.1a"><mrow id="S6.SS3.p1.7.m7.1.1" xref="S6.SS3.p1.7.m7.1.1.cmml"><mi id="S6.SS3.p1.7.m7.1.1.2" xref="S6.SS3.p1.7.m7.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.7.m7.1.1.1" xref="S6.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.7.m7.1.1.3" xref="S6.SS3.p1.7.m7.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.7.m7.1.1.1a" xref="S6.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.7.m7.1.1.4" xref="S6.SS3.p1.7.m7.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.7.m7.1.1.1b" xref="S6.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.7.m7.1.1.5" xref="S6.SS3.p1.7.m7.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.7.m7.1.1.1c" xref="S6.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.7.m7.1.1.6" xref="S6.SS3.p1.7.m7.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.7.m7.1.1.1d" xref="S6.SS3.p1.7.m7.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.7.m7.1.1.7" xref="S6.SS3.p1.7.m7.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.7.m7.1b"><apply id="S6.SS3.p1.7.m7.1.1.cmml" xref="S6.SS3.p1.7.m7.1.1"><times id="S6.SS3.p1.7.m7.1.1.1.cmml" xref="S6.SS3.p1.7.m7.1.1.1"></times><ci id="S6.SS3.p1.7.m7.1.1.2.cmml" xref="S6.SS3.p1.7.m7.1.1.2">𝑃</ci><ci id="S6.SS3.p1.7.m7.1.1.3.cmml" xref="S6.SS3.p1.7.m7.1.1.3">𝑟</ci><ci id="S6.SS3.p1.7.m7.1.1.4.cmml" xref="S6.SS3.p1.7.m7.1.1.4">𝑒</ci><ci id="S6.SS3.p1.7.m7.1.1.5.cmml" xref="S6.SS3.p1.7.m7.1.1.5">𝑆</ci><ci id="S6.SS3.p1.7.m7.1.1.6.cmml" xref="S6.SS3.p1.7.m7.1.1.6">𝑡</ci><ci id="S6.SS3.p1.7.m7.1.1.7.cmml" xref="S6.SS3.p1.7.m7.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.7.m7.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.7.m7.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (SmartSSD) is able to achieve such performance despite its
much lower power consumption (TDP of <math alttext="25" class="ltx_Math" display="inline" id="S6.SS3.p1.8.m8.1"><semantics id="S6.SS3.p1.8.m8.1a"><mn id="S6.SS3.p1.8.m8.1.1" xref="S6.SS3.p1.8.m8.1.1.cmml">25</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.8.m8.1b"><cn id="S6.SS3.p1.8.m8.1.1.cmml" type="integer" xref="S6.SS3.p1.8.m8.1.1">25</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.8.m8.1c">25</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.8.m8.1d">25</annotation></semantics></math> Watts (SmartSSD) vs. <math alttext="250" class="ltx_Math" display="inline" id="S6.SS3.p1.9.m9.1"><semantics id="S6.SS3.p1.9.m9.1a"><mn id="S6.SS3.p1.9.m9.1.1" xref="S6.SS3.p1.9.m9.1.1.cmml">250</mn><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.9.m9.1b"><cn id="S6.SS3.p1.9.m9.1.1.cmml" type="integer" xref="S6.SS3.p1.9.m9.1.1">250</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.9.m9.1c">250</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.9.m9.1d">250</annotation></semantics></math> Watts
(A100) and 225 Watts (U280)). In general, GPUs are
throughput-optimized devices with high compute and memory throughput, so they
perform best when the target application requires massive compute and memory
accesses. We observe that the compute and memory operations entailed in
RecSys preprocessing is lightweight vs. training. This makes it
challenging for the GPU to amortize the cost of CUDA kernel launches, each of
which has a small working set with modest compute/memory operations, leading
to significant GPU underutilization. U280 does much better than the GPU
but its end-to-end speedup vs. <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.10.m10.1"><semantics id="S6.SS3.p1.10.m10.1a"><mrow id="S6.SS3.p1.10.m10.1.1" xref="S6.SS3.p1.10.m10.1.1.cmml"><mi id="S6.SS3.p1.10.m10.1.1.2" xref="S6.SS3.p1.10.m10.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.10.m10.1.1.1" xref="S6.SS3.p1.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.10.m10.1.1.3" xref="S6.SS3.p1.10.m10.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.10.m10.1.1.1a" xref="S6.SS3.p1.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.10.m10.1.1.4" xref="S6.SS3.p1.10.m10.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.10.m10.1.1.1b" xref="S6.SS3.p1.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.10.m10.1.1.5" xref="S6.SS3.p1.10.m10.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.10.m10.1.1.1c" xref="S6.SS3.p1.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.10.m10.1.1.6" xref="S6.SS3.p1.10.m10.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.10.m10.1.1.1d" xref="S6.SS3.p1.10.m10.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.10.m10.1.1.7" xref="S6.SS3.p1.10.m10.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.10.m10.1b"><apply id="S6.SS3.p1.10.m10.1.1.cmml" xref="S6.SS3.p1.10.m10.1.1"><times id="S6.SS3.p1.10.m10.1.1.1.cmml" xref="S6.SS3.p1.10.m10.1.1.1"></times><ci id="S6.SS3.p1.10.m10.1.1.2.cmml" xref="S6.SS3.p1.10.m10.1.1.2">𝑃</ci><ci id="S6.SS3.p1.10.m10.1.1.3.cmml" xref="S6.SS3.p1.10.m10.1.1.3">𝑟</ci><ci id="S6.SS3.p1.10.m10.1.1.4.cmml" xref="S6.SS3.p1.10.m10.1.1.4">𝑒</ci><ci id="S6.SS3.p1.10.m10.1.1.5.cmml" xref="S6.SS3.p1.10.m10.1.1.5">𝑆</ci><ci id="S6.SS3.p1.10.m10.1.1.6.cmml" xref="S6.SS3.p1.10.m10.1.1.6">𝑡</ci><ci id="S6.SS3.p1.10.m10.1.1.7.cmml" xref="S6.SS3.p1.10.m10.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.10.m10.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.10.m10.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (SmartSSD) is relatively low, due to its
high latency overhead in copying data in/out of the disaggregated preprocessing
node (which accounts for an average <math alttext="47.6\%" class="ltx_Math" display="inline" id="S6.SS3.p1.11.m11.1"><semantics id="S6.SS3.p1.11.m11.1a"><mrow id="S6.SS3.p1.11.m11.1.1" xref="S6.SS3.p1.11.m11.1.1.cmml"><mn id="S6.SS3.p1.11.m11.1.1.2" xref="S6.SS3.p1.11.m11.1.1.2.cmml">47.6</mn><mo id="S6.SS3.p1.11.m11.1.1.1" xref="S6.SS3.p1.11.m11.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.11.m11.1b"><apply id="S6.SS3.p1.11.m11.1.1.cmml" xref="S6.SS3.p1.11.m11.1.1"><csymbol cd="latexml" id="S6.SS3.p1.11.m11.1.1.1.cmml" xref="S6.SS3.p1.11.m11.1.1.1">percent</csymbol><cn id="S6.SS3.p1.11.m11.1.1.2.cmml" type="float" xref="S6.SS3.p1.11.m11.1.1.2">47.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.11.m11.1c">47.6\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.11.m11.1d">47.6 %</annotation></semantics></math> of its end-to-end preprocessing time).
Even though <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.12.m12.1"><semantics id="S6.SS3.p1.12.m12.1a"><mrow id="S6.SS3.p1.12.m12.1.1" xref="S6.SS3.p1.12.m12.1.1.cmml"><mi id="S6.SS3.p1.12.m12.1.1.2" xref="S6.SS3.p1.12.m12.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.12.m12.1.1.1" xref="S6.SS3.p1.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.12.m12.1.1.3" xref="S6.SS3.p1.12.m12.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.12.m12.1.1.1a" xref="S6.SS3.p1.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.12.m12.1.1.4" xref="S6.SS3.p1.12.m12.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.12.m12.1.1.1b" xref="S6.SS3.p1.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.12.m12.1.1.5" xref="S6.SS3.p1.12.m12.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.12.m12.1.1.1c" xref="S6.SS3.p1.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.12.m12.1.1.6" xref="S6.SS3.p1.12.m12.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.12.m12.1.1.1d" xref="S6.SS3.p1.12.m12.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.12.m12.1.1.7" xref="S6.SS3.p1.12.m12.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.12.m12.1b"><apply id="S6.SS3.p1.12.m12.1.1.cmml" xref="S6.SS3.p1.12.m12.1.1"><times id="S6.SS3.p1.12.m12.1.1.1.cmml" xref="S6.SS3.p1.12.m12.1.1.1"></times><ci id="S6.SS3.p1.12.m12.1.1.2.cmml" xref="S6.SS3.p1.12.m12.1.1.2">𝑃</ci><ci id="S6.SS3.p1.12.m12.1.1.3.cmml" xref="S6.SS3.p1.12.m12.1.1.3">𝑟</ci><ci id="S6.SS3.p1.12.m12.1.1.4.cmml" xref="S6.SS3.p1.12.m12.1.1.4">𝑒</ci><ci id="S6.SS3.p1.12.m12.1.1.5.cmml" xref="S6.SS3.p1.12.m12.1.1.5">𝑆</ci><ci id="S6.SS3.p1.12.m12.1.1.6.cmml" xref="S6.SS3.p1.12.m12.1.1.6">𝑡</ci><ci id="S6.SS3.p1.12.m12.1.1.7.cmml" xref="S6.SS3.p1.12.m12.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.12.m12.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.12.m12.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (U280) minimizes such redundant data movements and achieves a slightly higher preprocessing throughput compared to <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.13.m13.1"><semantics id="S6.SS3.p1.13.m13.1a"><mrow id="S6.SS3.p1.13.m13.1.1" xref="S6.SS3.p1.13.m13.1.1.cmml"><mi id="S6.SS3.p1.13.m13.1.1.2" xref="S6.SS3.p1.13.m13.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.13.m13.1.1.1" xref="S6.SS3.p1.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.13.m13.1.1.3" xref="S6.SS3.p1.13.m13.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.13.m13.1.1.1a" xref="S6.SS3.p1.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.13.m13.1.1.4" xref="S6.SS3.p1.13.m13.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.13.m13.1.1.1b" xref="S6.SS3.p1.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.13.m13.1.1.5" xref="S6.SS3.p1.13.m13.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.13.m13.1.1.1c" xref="S6.SS3.p1.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.13.m13.1.1.6" xref="S6.SS3.p1.13.m13.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.13.m13.1.1.1d" xref="S6.SS3.p1.13.m13.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.13.m13.1.1.7" xref="S6.SS3.p1.13.m13.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.13.m13.1b"><apply id="S6.SS3.p1.13.m13.1.1.cmml" xref="S6.SS3.p1.13.m13.1.1"><times id="S6.SS3.p1.13.m13.1.1.1.cmml" xref="S6.SS3.p1.13.m13.1.1.1"></times><ci id="S6.SS3.p1.13.m13.1.1.2.cmml" xref="S6.SS3.p1.13.m13.1.1.2">𝑃</ci><ci id="S6.SS3.p1.13.m13.1.1.3.cmml" xref="S6.SS3.p1.13.m13.1.1.3">𝑟</ci><ci id="S6.SS3.p1.13.m13.1.1.4.cmml" xref="S6.SS3.p1.13.m13.1.1.4">𝑒</ci><ci id="S6.SS3.p1.13.m13.1.1.5.cmml" xref="S6.SS3.p1.13.m13.1.1.5">𝑆</ci><ci id="S6.SS3.p1.13.m13.1.1.6.cmml" xref="S6.SS3.p1.13.m13.1.1.6">𝑡</ci><ci id="S6.SS3.p1.13.m13.1.1.7.cmml" xref="S6.SS3.p1.13.m13.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.13.m13.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.13.m13.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (SmartSSD),
<math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.14.m14.1"><semantics id="S6.SS3.p1.14.m14.1a"><mrow id="S6.SS3.p1.14.m14.1.1" xref="S6.SS3.p1.14.m14.1.1.cmml"><mi id="S6.SS3.p1.14.m14.1.1.2" xref="S6.SS3.p1.14.m14.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.14.m14.1.1.1" xref="S6.SS3.p1.14.m14.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.14.m14.1.1.3" xref="S6.SS3.p1.14.m14.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.14.m14.1.1.1a" xref="S6.SS3.p1.14.m14.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.14.m14.1.1.4" xref="S6.SS3.p1.14.m14.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.14.m14.1.1.1b" xref="S6.SS3.p1.14.m14.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.14.m14.1.1.5" xref="S6.SS3.p1.14.m14.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.14.m14.1.1.1c" xref="S6.SS3.p1.14.m14.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.14.m14.1.1.6" xref="S6.SS3.p1.14.m14.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.14.m14.1.1.1d" xref="S6.SS3.p1.14.m14.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.14.m14.1.1.7" xref="S6.SS3.p1.14.m14.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.14.m14.1b"><apply id="S6.SS3.p1.14.m14.1.1.cmml" xref="S6.SS3.p1.14.m14.1.1"><times id="S6.SS3.p1.14.m14.1.1.1.cmml" xref="S6.SS3.p1.14.m14.1.1.1"></times><ci id="S6.SS3.p1.14.m14.1.1.2.cmml" xref="S6.SS3.p1.14.m14.1.1.2">𝑃</ci><ci id="S6.SS3.p1.14.m14.1.1.3.cmml" xref="S6.SS3.p1.14.m14.1.1.3">𝑟</ci><ci id="S6.SS3.p1.14.m14.1.1.4.cmml" xref="S6.SS3.p1.14.m14.1.1.4">𝑒</ci><ci id="S6.SS3.p1.14.m14.1.1.5.cmml" xref="S6.SS3.p1.14.m14.1.1.5">𝑆</ci><ci id="S6.SS3.p1.14.m14.1.1.6.cmml" xref="S6.SS3.p1.14.m14.1.1.6">𝑡</ci><ci id="S6.SS3.p1.14.m14.1.1.7.cmml" xref="S6.SS3.p1.14.m14.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.14.m14.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.14.m14.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (SmartSSD) delivers much higher energy-efficiency (an average <math alttext="2.9\times" class="ltx_math_unparsed" display="inline" id="S6.SS3.p1.15.m15.1"><semantics id="S6.SS3.p1.15.m15.1a"><mrow id="S6.SS3.p1.15.m15.1b"><mn id="S6.SS3.p1.15.m15.1.1">2.9</mn><mo id="S6.SS3.p1.15.m15.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S6.SS3.p1.15.m15.1c">2.9\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.15.m15.1d">2.9 ×</annotation></semantics></math>) vs. <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS3.p1.16.m16.1"><semantics id="S6.SS3.p1.16.m16.1a"><mrow id="S6.SS3.p1.16.m16.1.1" xref="S6.SS3.p1.16.m16.1.1.cmml"><mi id="S6.SS3.p1.16.m16.1.1.2" xref="S6.SS3.p1.16.m16.1.1.2.cmml">P</mi><mo id="S6.SS3.p1.16.m16.1.1.1" xref="S6.SS3.p1.16.m16.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.16.m16.1.1.3" xref="S6.SS3.p1.16.m16.1.1.3.cmml">r</mi><mo id="S6.SS3.p1.16.m16.1.1.1a" xref="S6.SS3.p1.16.m16.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.16.m16.1.1.4" xref="S6.SS3.p1.16.m16.1.1.4.cmml">e</mi><mo id="S6.SS3.p1.16.m16.1.1.1b" xref="S6.SS3.p1.16.m16.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.16.m16.1.1.5" xref="S6.SS3.p1.16.m16.1.1.5.cmml">S</mi><mo id="S6.SS3.p1.16.m16.1.1.1c" xref="S6.SS3.p1.16.m16.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.16.m16.1.1.6" xref="S6.SS3.p1.16.m16.1.1.6.cmml">t</mi><mo id="S6.SS3.p1.16.m16.1.1.1d" xref="S6.SS3.p1.16.m16.1.1.1.cmml">⁢</mo><mi id="S6.SS3.p1.16.m16.1.1.7" xref="S6.SS3.p1.16.m16.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS3.p1.16.m16.1b"><apply id="S6.SS3.p1.16.m16.1.1.cmml" xref="S6.SS3.p1.16.m16.1.1"><times id="S6.SS3.p1.16.m16.1.1.1.cmml" xref="S6.SS3.p1.16.m16.1.1.1"></times><ci id="S6.SS3.p1.16.m16.1.1.2.cmml" xref="S6.SS3.p1.16.m16.1.1.2">𝑃</ci><ci id="S6.SS3.p1.16.m16.1.1.3.cmml" xref="S6.SS3.p1.16.m16.1.1.3">𝑟</ci><ci id="S6.SS3.p1.16.m16.1.1.4.cmml" xref="S6.SS3.p1.16.m16.1.1.4">𝑒</ci><ci id="S6.SS3.p1.16.m16.1.1.5.cmml" xref="S6.SS3.p1.16.m16.1.1.5">𝑆</ci><ci id="S6.SS3.p1.16.m16.1.1.6.cmml" xref="S6.SS3.p1.16.m16.1.1.6">𝑡</ci><ci id="S6.SS3.p1.16.m16.1.1.7.cmml" xref="S6.SS3.p1.16.m16.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS3.p1.16.m16.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS3.p1.16.m16.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> (U280) by being custom-designed to right-size its compute units
for data preprocessing under a tighter power budget (25 Watts).</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS4.5.1.1">VI-D</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS4.6.2">PreSto Sensitivity to the Number of Features to Preprocess</span>
</h3>
<div class="ltx_para" id="S6.SS4.p1">
<p class="ltx_p" id="S6.SS4.p1.5">We investigate <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS4.p1.1.m1.1"><semantics id="S6.SS4.p1.1.m1.1a"><mrow id="S6.SS4.p1.1.m1.1.1" xref="S6.SS4.p1.1.m1.1.1.cmml"><mi id="S6.SS4.p1.1.m1.1.1.2" xref="S6.SS4.p1.1.m1.1.1.2.cmml">P</mi><mo id="S6.SS4.p1.1.m1.1.1.1" xref="S6.SS4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.1.m1.1.1.3" xref="S6.SS4.p1.1.m1.1.1.3.cmml">r</mi><mo id="S6.SS4.p1.1.m1.1.1.1a" xref="S6.SS4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.1.m1.1.1.4" xref="S6.SS4.p1.1.m1.1.1.4.cmml">e</mi><mo id="S6.SS4.p1.1.m1.1.1.1b" xref="S6.SS4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.1.m1.1.1.5" xref="S6.SS4.p1.1.m1.1.1.5.cmml">S</mi><mo id="S6.SS4.p1.1.m1.1.1.1c" xref="S6.SS4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.1.m1.1.1.6" xref="S6.SS4.p1.1.m1.1.1.6.cmml">t</mi><mo id="S6.SS4.p1.1.m1.1.1.1d" xref="S6.SS4.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.1.m1.1.1.7" xref="S6.SS4.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.1.m1.1b"><apply id="S6.SS4.p1.1.m1.1.1.cmml" xref="S6.SS4.p1.1.m1.1.1"><times id="S6.SS4.p1.1.m1.1.1.1.cmml" xref="S6.SS4.p1.1.m1.1.1.1"></times><ci id="S6.SS4.p1.1.m1.1.1.2.cmml" xref="S6.SS4.p1.1.m1.1.1.2">𝑃</ci><ci id="S6.SS4.p1.1.m1.1.1.3.cmml" xref="S6.SS4.p1.1.m1.1.1.3">𝑟</ci><ci id="S6.SS4.p1.1.m1.1.1.4.cmml" xref="S6.SS4.p1.1.m1.1.1.4">𝑒</ci><ci id="S6.SS4.p1.1.m1.1.1.5.cmml" xref="S6.SS4.p1.1.m1.1.1.5">𝑆</ci><ci id="S6.SS4.p1.1.m1.1.1.6.cmml" xref="S6.SS4.p1.1.m1.1.1.6">𝑡</ci><ci id="S6.SS4.p1.1.m1.1.1.7.cmml" xref="S6.SS4.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>’s sensitivity to the number of features to preprocess by evaluating the latency incurred in executing the Bucketize, SigridHash, and Log operations
when the number of generated, sparse, and dense features are changed. Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#S6.F17" title="Figure 17 ‣ VI-C PreSto vs. Alternative Accelerated Preprocessing ‣ VI Evaluation ‣ PreSto: An In-Storage Data Preprocessing System for Training Recommendation Models 2 Co-first authors who contributed equally to this research. This is the author preprint version of the work. The authoritative version will appear in the Proceedings of the 51st IEEE/ACM International Symposium on Computer Architecture (ISCA-51), 2024."><span class="ltx_text ltx_ref_tag">17</span></a> illustrates a comparison of the average latency
to execute each operation using <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS4.p1.2.m2.1"><semantics id="S6.SS4.p1.2.m2.1a"><mrow id="S6.SS4.p1.2.m2.1.1" xref="S6.SS4.p1.2.m2.1.1.cmml"><mi id="S6.SS4.p1.2.m2.1.1.2" xref="S6.SS4.p1.2.m2.1.1.2.cmml">D</mi><mo id="S6.SS4.p1.2.m2.1.1.1" xref="S6.SS4.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.2.m2.1.1.3" xref="S6.SS4.p1.2.m2.1.1.3.cmml">i</mi><mo id="S6.SS4.p1.2.m2.1.1.1a" xref="S6.SS4.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.2.m2.1.1.4" xref="S6.SS4.p1.2.m2.1.1.4.cmml">s</mi><mo id="S6.SS4.p1.2.m2.1.1.1b" xref="S6.SS4.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.2.m2.1.1.5" xref="S6.SS4.p1.2.m2.1.1.5.cmml">a</mi><mo id="S6.SS4.p1.2.m2.1.1.1c" xref="S6.SS4.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.2.m2.1.1.6" xref="S6.SS4.p1.2.m2.1.1.6.cmml">g</mi><mo id="S6.SS4.p1.2.m2.1.1.1d" xref="S6.SS4.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.2.m2.1.1.7" xref="S6.SS4.p1.2.m2.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.2.m2.1b"><apply id="S6.SS4.p1.2.m2.1.1.cmml" xref="S6.SS4.p1.2.m2.1.1"><times id="S6.SS4.p1.2.m2.1.1.1.cmml" xref="S6.SS4.p1.2.m2.1.1.1"></times><ci id="S6.SS4.p1.2.m2.1.1.2.cmml" xref="S6.SS4.p1.2.m2.1.1.2">𝐷</ci><ci id="S6.SS4.p1.2.m2.1.1.3.cmml" xref="S6.SS4.p1.2.m2.1.1.3">𝑖</ci><ci id="S6.SS4.p1.2.m2.1.1.4.cmml" xref="S6.SS4.p1.2.m2.1.1.4">𝑠</ci><ci id="S6.SS4.p1.2.m2.1.1.5.cmml" xref="S6.SS4.p1.2.m2.1.1.5">𝑎</ci><ci id="S6.SS4.p1.2.m2.1.1.6.cmml" xref="S6.SS4.p1.2.m2.1.1.6">𝑔</ci><ci id="S6.SS4.p1.2.m2.1.1.7.cmml" xref="S6.SS4.p1.2.m2.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.2.m2.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.2.m2.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> and <math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS4.p1.3.m3.1"><semantics id="S6.SS4.p1.3.m3.1a"><mrow id="S6.SS4.p1.3.m3.1.1" xref="S6.SS4.p1.3.m3.1.1.cmml"><mi id="S6.SS4.p1.3.m3.1.1.2" xref="S6.SS4.p1.3.m3.1.1.2.cmml">P</mi><mo id="S6.SS4.p1.3.m3.1.1.1" xref="S6.SS4.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.3.m3.1.1.3" xref="S6.SS4.p1.3.m3.1.1.3.cmml">r</mi><mo id="S6.SS4.p1.3.m3.1.1.1a" xref="S6.SS4.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.3.m3.1.1.4" xref="S6.SS4.p1.3.m3.1.1.4.cmml">e</mi><mo id="S6.SS4.p1.3.m3.1.1.1b" xref="S6.SS4.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.3.m3.1.1.5" xref="S6.SS4.p1.3.m3.1.1.5.cmml">S</mi><mo id="S6.SS4.p1.3.m3.1.1.1c" xref="S6.SS4.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.3.m3.1.1.6" xref="S6.SS4.p1.3.m3.1.1.6.cmml">t</mi><mo id="S6.SS4.p1.3.m3.1.1.1d" xref="S6.SS4.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.3.m3.1.1.7" xref="S6.SS4.p1.3.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.3.m3.1b"><apply id="S6.SS4.p1.3.m3.1.1.cmml" xref="S6.SS4.p1.3.m3.1.1"><times id="S6.SS4.p1.3.m3.1.1.1.cmml" xref="S6.SS4.p1.3.m3.1.1.1"></times><ci id="S6.SS4.p1.3.m3.1.1.2.cmml" xref="S6.SS4.p1.3.m3.1.1.2">𝑃</ci><ci id="S6.SS4.p1.3.m3.1.1.3.cmml" xref="S6.SS4.p1.3.m3.1.1.3">𝑟</ci><ci id="S6.SS4.p1.3.m3.1.1.4.cmml" xref="S6.SS4.p1.3.m3.1.1.4">𝑒</ci><ci id="S6.SS4.p1.3.m3.1.1.5.cmml" xref="S6.SS4.p1.3.m3.1.1.5">𝑆</ci><ci id="S6.SS4.p1.3.m3.1.1.6.cmml" xref="S6.SS4.p1.3.m3.1.1.6">𝑡</ci><ci id="S6.SS4.p1.3.m3.1.1.7.cmml" xref="S6.SS4.p1.3.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.3.m3.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.3.m3.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math>. While the latency of <math alttext="Disagg" class="ltx_Math" display="inline" id="S6.SS4.p1.4.m4.1"><semantics id="S6.SS4.p1.4.m4.1a"><mrow id="S6.SS4.p1.4.m4.1.1" xref="S6.SS4.p1.4.m4.1.1.cmml"><mi id="S6.SS4.p1.4.m4.1.1.2" xref="S6.SS4.p1.4.m4.1.1.2.cmml">D</mi><mo id="S6.SS4.p1.4.m4.1.1.1" xref="S6.SS4.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.4.m4.1.1.3" xref="S6.SS4.p1.4.m4.1.1.3.cmml">i</mi><mo id="S6.SS4.p1.4.m4.1.1.1a" xref="S6.SS4.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.4.m4.1.1.4" xref="S6.SS4.p1.4.m4.1.1.4.cmml">s</mi><mo id="S6.SS4.p1.4.m4.1.1.1b" xref="S6.SS4.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.4.m4.1.1.5" xref="S6.SS4.p1.4.m4.1.1.5.cmml">a</mi><mo id="S6.SS4.p1.4.m4.1.1.1c" xref="S6.SS4.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.4.m4.1.1.6" xref="S6.SS4.p1.4.m4.1.1.6.cmml">g</mi><mo id="S6.SS4.p1.4.m4.1.1.1d" xref="S6.SS4.p1.4.m4.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.4.m4.1.1.7" xref="S6.SS4.p1.4.m4.1.1.7.cmml">g</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.4.m4.1b"><apply id="S6.SS4.p1.4.m4.1.1.cmml" xref="S6.SS4.p1.4.m4.1.1"><times id="S6.SS4.p1.4.m4.1.1.1.cmml" xref="S6.SS4.p1.4.m4.1.1.1"></times><ci id="S6.SS4.p1.4.m4.1.1.2.cmml" xref="S6.SS4.p1.4.m4.1.1.2">𝐷</ci><ci id="S6.SS4.p1.4.m4.1.1.3.cmml" xref="S6.SS4.p1.4.m4.1.1.3">𝑖</ci><ci id="S6.SS4.p1.4.m4.1.1.4.cmml" xref="S6.SS4.p1.4.m4.1.1.4">𝑠</ci><ci id="S6.SS4.p1.4.m4.1.1.5.cmml" xref="S6.SS4.p1.4.m4.1.1.5">𝑎</ci><ci id="S6.SS4.p1.4.m4.1.1.6.cmml" xref="S6.SS4.p1.4.m4.1.1.6">𝑔</ci><ci id="S6.SS4.p1.4.m4.1.1.7.cmml" xref="S6.SS4.p1.4.m4.1.1.7">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.4.m4.1c">Disagg</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.4.m4.1d">italic_D italic_i italic_s italic_a italic_g italic_g</annotation></semantics></math> increases almost proportionally with the number of features for preprocessing,
<math alttext="PreSto" class="ltx_Math" display="inline" id="S6.SS4.p1.5.m5.1"><semantics id="S6.SS4.p1.5.m5.1a"><mrow id="S6.SS4.p1.5.m5.1.1" xref="S6.SS4.p1.5.m5.1.1.cmml"><mi id="S6.SS4.p1.5.m5.1.1.2" xref="S6.SS4.p1.5.m5.1.1.2.cmml">P</mi><mo id="S6.SS4.p1.5.m5.1.1.1" xref="S6.SS4.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.5.m5.1.1.3" xref="S6.SS4.p1.5.m5.1.1.3.cmml">r</mi><mo id="S6.SS4.p1.5.m5.1.1.1a" xref="S6.SS4.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.5.m5.1.1.4" xref="S6.SS4.p1.5.m5.1.1.4.cmml">e</mi><mo id="S6.SS4.p1.5.m5.1.1.1b" xref="S6.SS4.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.5.m5.1.1.5" xref="S6.SS4.p1.5.m5.1.1.5.cmml">S</mi><mo id="S6.SS4.p1.5.m5.1.1.1c" xref="S6.SS4.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.5.m5.1.1.6" xref="S6.SS4.p1.5.m5.1.1.6.cmml">t</mi><mo id="S6.SS4.p1.5.m5.1.1.1d" xref="S6.SS4.p1.5.m5.1.1.1.cmml">⁢</mo><mi id="S6.SS4.p1.5.m5.1.1.7" xref="S6.SS4.p1.5.m5.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.5.m5.1b"><apply id="S6.SS4.p1.5.m5.1.1.cmml" xref="S6.SS4.p1.5.m5.1.1"><times id="S6.SS4.p1.5.m5.1.1.1.cmml" xref="S6.SS4.p1.5.m5.1.1.1"></times><ci id="S6.SS4.p1.5.m5.1.1.2.cmml" xref="S6.SS4.p1.5.m5.1.1.2">𝑃</ci><ci id="S6.SS4.p1.5.m5.1.1.3.cmml" xref="S6.SS4.p1.5.m5.1.1.3">𝑟</ci><ci id="S6.SS4.p1.5.m5.1.1.4.cmml" xref="S6.SS4.p1.5.m5.1.1.4">𝑒</ci><ci id="S6.SS4.p1.5.m5.1.1.5.cmml" xref="S6.SS4.p1.5.m5.1.1.5">𝑆</ci><ci id="S6.SS4.p1.5.m5.1.1.6.cmml" xref="S6.SS4.p1.5.m5.1.1.6">𝑡</ci><ci id="S6.SS4.p1.5.m5.1.1.7.cmml" xref="S6.SS4.p1.5.m5.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.5.m5.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S6.SS4.p1.5.m5.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> does a much better job leveraging inter-/intra-feature parallelism and consistently achieves significant speedups, demonstrating the robustness of our proposal.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Related work</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">There is a large body of prior
literature exploring DNN preprocessing, RecSys model training/inference, RecSys data storage and preprocessing, and domain-specific/general-purpose ISP designs, which we summarize below.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">DNN preprocessing.</span> There are multiple prior work addressing
the performance gap between DNN model training and data
preprocessing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib13" title="">13</a>]</cite>. Mohan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib47" title="">47</a>]</cite> and Murray et
al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib49" title="">49</a>]</cite> proposed software optimizations to address this problem,
while TrainBox <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib57" title="">57</a>]</cite> and DALI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib53" title="">53</a>]</cite> proposed hardware
accelerated preprocessing tackling computer vision and audio training tasks. Similarly, DLBooster <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib9" title="">9</a>]</cite> focused on offloading preprocessing
operations to an FPGA for inference. PreGNN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib13" title="">13</a>]</cite> proposed offloading graph neural network (GNN) preprocessing tasks to an accelerator.
Several prior art <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib33" title="">33</a>]</cite>
proposed disaggregated preprocessing solutions but these works primarily
focused on efficiently managing CPU resources via software optimizations
using data caching or prefetching. Importantly, all of these prior art
strictly do not focus on RecSys, rendering the key
contribution of our work unique.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1"><span class="ltx_text ltx_font_bold" id="S7.p3.1.1">RecSys data storage and preprocessing.</span> Zhao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib70" title="">70</a>]</cite> discusses
a disaggregated data preprocessing service for Meta’s RecSys training pipeline.
XDL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib25" title="">25</a>]</cite> proposed a distributed ML
framework for Alibaba’s production RecSys model.
InTune <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib50" title="">50</a>]</cite> presents a reinforcement learning-based RecSys
data pipeline optimization to efficiently manage CPU resources. There also
exists prior work exploring feature deduplication to improve the performance of
RecSys data preprocessing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib71" title="">71</a>]</cite>. While not targeting the preprocessing stage of RecSys,
Tectonic-shift <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib72" title="">72</a>]</cite> explored the viability of a flash
storage tier in the data storage system of Meta’s production ML training
infrastructure, aiming to improve the performance and power efficiency of
their I/O operation in data storage stage. Overall, the contribution of <math alttext="PreSto" class="ltx_Math" display="inline" id="S7.p3.1.m1.1"><semantics id="S7.p3.1.m1.1a"><mrow id="S7.p3.1.m1.1.1" xref="S7.p3.1.m1.1.1.cmml"><mi id="S7.p3.1.m1.1.1.2" xref="S7.p3.1.m1.1.1.2.cmml">P</mi><mo id="S7.p3.1.m1.1.1.1" xref="S7.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p3.1.m1.1.1.3" xref="S7.p3.1.m1.1.1.3.cmml">r</mi><mo id="S7.p3.1.m1.1.1.1a" xref="S7.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p3.1.m1.1.1.4" xref="S7.p3.1.m1.1.1.4.cmml">e</mi><mo id="S7.p3.1.m1.1.1.1b" xref="S7.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p3.1.m1.1.1.5" xref="S7.p3.1.m1.1.1.5.cmml">S</mi><mo id="S7.p3.1.m1.1.1.1c" xref="S7.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p3.1.m1.1.1.6" xref="S7.p3.1.m1.1.1.6.cmml">t</mi><mo id="S7.p3.1.m1.1.1.1d" xref="S7.p3.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p3.1.m1.1.1.7" xref="S7.p3.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.p3.1.m1.1b"><apply id="S7.p3.1.m1.1.1.cmml" xref="S7.p3.1.m1.1.1"><times id="S7.p3.1.m1.1.1.1.cmml" xref="S7.p3.1.m1.1.1.1"></times><ci id="S7.p3.1.m1.1.1.2.cmml" xref="S7.p3.1.m1.1.1.2">𝑃</ci><ci id="S7.p3.1.m1.1.1.3.cmml" xref="S7.p3.1.m1.1.1.3">𝑟</ci><ci id="S7.p3.1.m1.1.1.4.cmml" xref="S7.p3.1.m1.1.1.4">𝑒</ci><ci id="S7.p3.1.m1.1.1.5.cmml" xref="S7.p3.1.m1.1.1.5">𝑆</ci><ci id="S7.p3.1.m1.1.1.6.cmml" xref="S7.p3.1.m1.1.1.6">𝑡</ci><ci id="S7.p3.1.m1.1.1.7.cmml" xref="S7.p3.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p3.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S7.p3.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> is
orthogonal to these studies.</p>
</div>
<div class="ltx_para" id="S7.p4">
<p class="ltx_p" id="S7.p4.1"><span class="ltx_text ltx_font_bold" id="S7.p4.1.1">RecSys model training/inference.</span>
The surge of interest in RecSys in both academia and industry has spawned numerous prior work accelerating RecSys training and inference utilizing near-/in-memory
processing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib56" title="">56</a>]</cite> as well as various hardware/software optimizations
 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib19" title="">19</a>]</cite>.
Importantly, <math alttext="PreSto" class="ltx_Math" display="inline" id="S7.p4.1.m1.1"><semantics id="S7.p4.1.m1.1a"><mrow id="S7.p4.1.m1.1.1" xref="S7.p4.1.m1.1.1.cmml"><mi id="S7.p4.1.m1.1.1.2" xref="S7.p4.1.m1.1.1.2.cmml">P</mi><mo id="S7.p4.1.m1.1.1.1" xref="S7.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p4.1.m1.1.1.3" xref="S7.p4.1.m1.1.1.3.cmml">r</mi><mo id="S7.p4.1.m1.1.1.1a" xref="S7.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p4.1.m1.1.1.4" xref="S7.p4.1.m1.1.1.4.cmml">e</mi><mo id="S7.p4.1.m1.1.1.1b" xref="S7.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p4.1.m1.1.1.5" xref="S7.p4.1.m1.1.1.5.cmml">S</mi><mo id="S7.p4.1.m1.1.1.1c" xref="S7.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p4.1.m1.1.1.6" xref="S7.p4.1.m1.1.1.6.cmml">t</mi><mo id="S7.p4.1.m1.1.1.1d" xref="S7.p4.1.m1.1.1.1.cmml">⁢</mo><mi id="S7.p4.1.m1.1.1.7" xref="S7.p4.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S7.p4.1.m1.1b"><apply id="S7.p4.1.m1.1.1.cmml" xref="S7.p4.1.m1.1.1"><times id="S7.p4.1.m1.1.1.1.cmml" xref="S7.p4.1.m1.1.1.1"></times><ci id="S7.p4.1.m1.1.1.2.cmml" xref="S7.p4.1.m1.1.1.2">𝑃</ci><ci id="S7.p4.1.m1.1.1.3.cmml" xref="S7.p4.1.m1.1.1.3">𝑟</ci><ci id="S7.p4.1.m1.1.1.4.cmml" xref="S7.p4.1.m1.1.1.4">𝑒</ci><ci id="S7.p4.1.m1.1.1.5.cmml" xref="S7.p4.1.m1.1.1.5">𝑆</ci><ci id="S7.p4.1.m1.1.1.6.cmml" xref="S7.p4.1.m1.1.1.6">𝑡</ci><ci id="S7.p4.1.m1.1.1.7.cmml" xref="S7.p4.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.p4.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S7.p4.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> stands apart from this body of work by focusing on accelerating RecSys preprocessing, which is as
discussed in this paper
completely orthogonal operation compared to model training/inference.</p>
</div>
<div class="ltx_para" id="S7.p5">
<p class="ltx_p" id="S7.p5.1"><span class="ltx_text ltx_font_bold" id="S7.p5.1.1">Domain-specific/general-purpose ISP designs.</span> There is a large body of prior literature exploring domain-specific/general-purpose ISP designs. GLIST<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib39" title="">39</a>]</cite> proposed an ISP architecture for SSD-based GNN inference. SmartSAGE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib38" title="">38</a>]</cite> proposed an ISP-based GNN training system to overcome I/O bottleneck of SSD-based training. Mahapatra et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib43" title="">43</a>]</cite> proposed an ASIC-based ISP in a disaggregated storage system for serverless functions, alleviating communication overhead of remote storage systems. RecSSD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib66" title="">66</a>]</cite> and RM-SSD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib61" title="">61</a>]</cite> proposed ISP to overcome the memory bottlenecks of RecSys inference. GraphSSD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib46" title="">46</a>]</cite> proposed an ISP architecture for graph semantics with a simple programming interface. ECSSD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib40" title="">40</a>]</cite> proposed an ISP architecture for extreme classification based on the approximate screening algorithm. Work by Hu et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib21" title="">21</a>]</cite> proposed an ISP-based dynamic multi-resolution storage system to mitigate the performance bottleneck of data preparation for approximate compute kernels. ASSASIN<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib73" title="">73</a>]</cite>, INSPIRE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib41" title="">41</a>]</cite>, and GenStore<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib45" title="">45</a>]</cite> proposed an ISP architecture for stream computing, private information retrieval, and genome sequence analysis, respectively. There also exists a large body of prior literature targeting ISP acceleration for data-intensive workloads. Morpheus<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib64" title="">64</a>]</cite>, DeepStore<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib44" title="">44</a>]</cite>, Active Flash<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib62" title="">62</a>]</cite>, GraFBoost<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib27" title="">27</a>]</cite>, Biscuit<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib15" title="">15</a>]</cite>, and BlueDBM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib26" title="">26</a>]</cite> proposed a domain-specific ISP architecture that targets data analytics, data management, object (de)serialization, or graph analytics. Summarizer<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib31" title="">31</a>]</cite> and INSIDER<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib58" title="">58</a>]</cite> proposed a hardware/software co-designed ISP architecture to offload data-intensive tasks with a set of flexible programming APIs. Willow<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.14571v1#bib.bib60" title="">60</a>]</cite> proposed architectural support to enhance the effectiveness and flexibility of a programmable ISP design targeting I/O-intensive applications. Unlike these prior work, PreSto demonstrates the merits of an ISP solution targeting <em class="ltx_emph ltx_font_italic" id="S7.p5.1.2">compute-bound</em> RecSys data preprocessing and uncovers its new system-level bottlenecks, rendering our key contributions unique.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.6">In this work, we propose an ISP based RecSys data preprocessing system called
<math alttext="PreSto" class="ltx_Math" display="inline" id="S8.p1.1.m1.1"><semantics id="S8.p1.1.m1.1a"><mrow id="S8.p1.1.m1.1.1" xref="S8.p1.1.m1.1.1.cmml"><mi id="S8.p1.1.m1.1.1.2" xref="S8.p1.1.m1.1.1.2.cmml">P</mi><mo id="S8.p1.1.m1.1.1.1" xref="S8.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S8.p1.1.m1.1.1.3" xref="S8.p1.1.m1.1.1.3.cmml">r</mi><mo id="S8.p1.1.m1.1.1.1a" xref="S8.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S8.p1.1.m1.1.1.4" xref="S8.p1.1.m1.1.1.4.cmml">e</mi><mo id="S8.p1.1.m1.1.1.1b" xref="S8.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S8.p1.1.m1.1.1.5" xref="S8.p1.1.m1.1.1.5.cmml">S</mi><mo id="S8.p1.1.m1.1.1.1c" xref="S8.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S8.p1.1.m1.1.1.6" xref="S8.p1.1.m1.1.1.6.cmml">t</mi><mo id="S8.p1.1.m1.1.1.1d" xref="S8.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S8.p1.1.m1.1.1.7" xref="S8.p1.1.m1.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S8.p1.1.m1.1b"><apply id="S8.p1.1.m1.1.1.cmml" xref="S8.p1.1.m1.1.1"><times id="S8.p1.1.m1.1.1.1.cmml" xref="S8.p1.1.m1.1.1.1"></times><ci id="S8.p1.1.m1.1.1.2.cmml" xref="S8.p1.1.m1.1.1.2">𝑃</ci><ci id="S8.p1.1.m1.1.1.3.cmml" xref="S8.p1.1.m1.1.1.3">𝑟</ci><ci id="S8.p1.1.m1.1.1.4.cmml" xref="S8.p1.1.m1.1.1.4">𝑒</ci><ci id="S8.p1.1.m1.1.1.5.cmml" xref="S8.p1.1.m1.1.1.5">𝑆</ci><ci id="S8.p1.1.m1.1.1.6.cmml" xref="S8.p1.1.m1.1.1.6">𝑡</ci><ci id="S8.p1.1.m1.1.1.7.cmml" xref="S8.p1.1.m1.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.1.m1.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S8.p1.1.m1.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> which conducts the preprocessing operation close to where the training samples are preserved. By fully leveraging inter-/intra-feature
parallelism available in feature generation/normalization, <math alttext="PreSto" class="ltx_Math" display="inline" id="S8.p1.2.m2.1"><semantics id="S8.p1.2.m2.1a"><mrow id="S8.p1.2.m2.1.1" xref="S8.p1.2.m2.1.1.cmml"><mi id="S8.p1.2.m2.1.1.2" xref="S8.p1.2.m2.1.1.2.cmml">P</mi><mo id="S8.p1.2.m2.1.1.1" xref="S8.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S8.p1.2.m2.1.1.3" xref="S8.p1.2.m2.1.1.3.cmml">r</mi><mo id="S8.p1.2.m2.1.1.1a" xref="S8.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S8.p1.2.m2.1.1.4" xref="S8.p1.2.m2.1.1.4.cmml">e</mi><mo id="S8.p1.2.m2.1.1.1b" xref="S8.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S8.p1.2.m2.1.1.5" xref="S8.p1.2.m2.1.1.5.cmml">S</mi><mo id="S8.p1.2.m2.1.1.1c" xref="S8.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S8.p1.2.m2.1.1.6" xref="S8.p1.2.m2.1.1.6.cmml">t</mi><mo id="S8.p1.2.m2.1.1.1d" xref="S8.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S8.p1.2.m2.1.1.7" xref="S8.p1.2.m2.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S8.p1.2.m2.1b"><apply id="S8.p1.2.m2.1.1.cmml" xref="S8.p1.2.m2.1.1"><times id="S8.p1.2.m2.1.1.1.cmml" xref="S8.p1.2.m2.1.1.1"></times><ci id="S8.p1.2.m2.1.1.2.cmml" xref="S8.p1.2.m2.1.1.2">𝑃</ci><ci id="S8.p1.2.m2.1.1.3.cmml" xref="S8.p1.2.m2.1.1.3">𝑟</ci><ci id="S8.p1.2.m2.1.1.4.cmml" xref="S8.p1.2.m2.1.1.4">𝑒</ci><ci id="S8.p1.2.m2.1.1.5.cmml" xref="S8.p1.2.m2.1.1.5">𝑆</ci><ci id="S8.p1.2.m2.1.1.6.cmml" xref="S8.p1.2.m2.1.1.6">𝑡</ci><ci id="S8.p1.2.m2.1.1.7.cmml" xref="S8.p1.2.m2.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.2.m2.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S8.p1.2.m2.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> can
effectively close the performance gap between preprocessing and model training
at a much lower cost and power consumption compared to the baseline CPU-centric
system. Overall, <math alttext="PreSto" class="ltx_Math" display="inline" id="S8.p1.3.m3.1"><semantics id="S8.p1.3.m3.1a"><mrow id="S8.p1.3.m3.1.1" xref="S8.p1.3.m3.1.1.cmml"><mi id="S8.p1.3.m3.1.1.2" xref="S8.p1.3.m3.1.1.2.cmml">P</mi><mo id="S8.p1.3.m3.1.1.1" xref="S8.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S8.p1.3.m3.1.1.3" xref="S8.p1.3.m3.1.1.3.cmml">r</mi><mo id="S8.p1.3.m3.1.1.1a" xref="S8.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S8.p1.3.m3.1.1.4" xref="S8.p1.3.m3.1.1.4.cmml">e</mi><mo id="S8.p1.3.m3.1.1.1b" xref="S8.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S8.p1.3.m3.1.1.5" xref="S8.p1.3.m3.1.1.5.cmml">S</mi><mo id="S8.p1.3.m3.1.1.1c" xref="S8.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S8.p1.3.m3.1.1.6" xref="S8.p1.3.m3.1.1.6.cmml">t</mi><mo id="S8.p1.3.m3.1.1.1d" xref="S8.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S8.p1.3.m3.1.1.7" xref="S8.p1.3.m3.1.1.7.cmml">o</mi></mrow><annotation-xml encoding="MathML-Content" id="S8.p1.3.m3.1b"><apply id="S8.p1.3.m3.1.1.cmml" xref="S8.p1.3.m3.1.1"><times id="S8.p1.3.m3.1.1.1.cmml" xref="S8.p1.3.m3.1.1.1"></times><ci id="S8.p1.3.m3.1.1.2.cmml" xref="S8.p1.3.m3.1.1.2">𝑃</ci><ci id="S8.p1.3.m3.1.1.3.cmml" xref="S8.p1.3.m3.1.1.3">𝑟</ci><ci id="S8.p1.3.m3.1.1.4.cmml" xref="S8.p1.3.m3.1.1.4">𝑒</ci><ci id="S8.p1.3.m3.1.1.5.cmml" xref="S8.p1.3.m3.1.1.5">𝑆</ci><ci id="S8.p1.3.m3.1.1.6.cmml" xref="S8.p1.3.m3.1.1.6">𝑡</ci><ci id="S8.p1.3.m3.1.1.7.cmml" xref="S8.p1.3.m3.1.1.7">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.p1.3.m3.1c">PreSto</annotation><annotation encoding="application/x-llamapun" id="S8.p1.3.m3.1d">italic_P italic_r italic_e italic_S italic_t italic_o</annotation></semantics></math> outperforms state-of-the-art preprocessing systems with
<math alttext="9.6\times" class="ltx_math_unparsed" display="inline" id="S8.p1.4.m4.1"><semantics id="S8.p1.4.m4.1a"><mrow id="S8.p1.4.m4.1b"><mn id="S8.p1.4.m4.1.1">9.6</mn><mo id="S8.p1.4.m4.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S8.p1.4.m4.1c">9.6\times</annotation><annotation encoding="application/x-llamapun" id="S8.p1.4.m4.1d">9.6 ×</annotation></semantics></math>
speedup in end-to-end preprocessing time, <math alttext="4.3\times" class="ltx_math_unparsed" display="inline" id="S8.p1.5.m5.1"><semantics id="S8.p1.5.m5.1a"><mrow id="S8.p1.5.m5.1b"><mn id="S8.p1.5.m5.1.1">4.3</mn><mo id="S8.p1.5.m5.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S8.p1.5.m5.1c">4.3\times</annotation><annotation encoding="application/x-llamapun" id="S8.p1.5.m5.1d">4.3 ×</annotation></semantics></math> improvement in cost-efficiency,
and <math alttext="11.3\times" class="ltx_math_unparsed" display="inline" id="S8.p1.6.m6.1"><semantics id="S8.p1.6.m6.1a"><mrow id="S8.p1.6.m6.1b"><mn id="S8.p1.6.m6.1.1">11.3</mn><mo id="S8.p1.6.m6.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S8.p1.6.m6.1c">11.3\times</annotation><annotation encoding="application/x-llamapun" id="S8.p1.6.m6.1d">11.3 ×</annotation></semantics></math> enhancement in energy-efficiency.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (NRF-2021R1A2C2091753), and Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korea government(MSIT) (No. 2022-0-01037, Development of High Performance Processing-in-Memory Technology based on DRAM). We also appreciate the support from SNU-SK Hynix Solution Research Center (S3RC) and the EDA tool supported by the IC Design Education Center(IDEC), Korea. Minsoo Rhu is the corresponding author.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
B. Acun, M. Murphy, X. Wang, J. Nie, C.-J. Wu, and K. Hazelwood,
“Understanding Training Efficiency of Deep Learning Recommendation Models
at Scale,” in <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the International Symposium on
High-Performance Computer Architecture (HPCA)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Amazon, “Amazon AWS Cloud Computing Services,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/" title="">https://aws.amazon.com/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Apache Software Foundation, “Apache Parquet,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://parquet.apache.org/" title="">https://parquet.apache.org/</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
B. Asgari, R. Hadidi, J. Cao, D. E. Shim, S.-K. Lim, and H. Kim, “FAFNIR:
Accelerating Sparse Gathering by Using Efficient Near-Memory Intelligent
Reduction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the International Symposium on
High-Performance Computer Architecture (HPCA)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Audibert, Y. Chen, D. Graur, A. Klimovic, J. Šimša, and C. A.
Thekkath, “tf.data service: A Case for Disaggregating ML Input Data
Processing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the ACM Symposium on Cloud Computing
(SoCC)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A. Barbalace and J. Do, “Computational Storage: Where Are We Today?” in
<em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">11th Annual Conference on Innovative Data Systems Research (CIDR)</em>,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
L. A. Barroso, U. Hölzle, and P. Ranganathan, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">The Datacenter as a
Computer: Designing Warehouse-Scale Machines, Third Edition</em>.   Springer Nature, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
W. Chen, S. He, Y. Xu, X. Zhang, S. Yang, S. Hu, X.-H. Sun, and G. Chen,
“iCache: An Importance-Sampling-Informed Cache for Accelerating I/O-Bound
DNN Model Training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the International Symposium on
High-Performance Computer Architecture (HPCA)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Y. Cheng, D. Li, Z. Guo, B. Jiang, J. Lin, X. Fan, J. Geng, X. Yu, W. Bai,
L. Qu, R. Shu, P. Cheng, Y. Xiong, and J. Wu, “DLBooster: Boosting
End-to-End Deep Learning Workflows with Offloading Data Preprocessing
Pipelines,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 48th International Conference on
Parallel Processing (ICPP)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Criteo, “Criteo Terabyte Click Logs,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://labs.criteo.com/2013/12/download-terabyte-click-logs/" title="">https://labs.criteo.com/2013/12/download-terabyte-click-logs/</a>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P. Damania, S. Li, A. Desmaison, A. Azzolini, B. Vaughan, E. Yang, G. Chanan,
G. J. Chen, H. Jia, H. Huang, J. Spisak, L. Wehrstedt, L. Hosseini,
M. Krishnan, O. Salpekar, P. Belevich, R. Varma, S. Gera, W. Liang, S. Xu,
S. Chintala, C. He, A. Ziashahabi, S. Avestimehr, A. Jain, and Z. DeVito,
“PyTorch RPC: Distributed Deep Learning Built on Tensor-Optimized Remote
Procedure Calls,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of Machine Learning and Systems
(MLSys)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Dell, “Dell R640,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.dell.com/en-us/dt/servers/poweredge-rack-servers.htm" title="">https://www.dell.com/en-us/dt/servers/poweredge-rack-servers.htm</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
D. Gouk, S. Kang, M. Kwon, J. Jang, H. Choi, S. Lee, and M. Jung, “PreGNN:
Hardware Acceleration to Take Preprocessing Off the Critical Path in Graph
Neural Networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">IEEE Computer Architecture Letters</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
D. Graur, D. Aymon, D. Kluser, T. Albrici, C. A. Thekkath, and A. Klimovic,
“Cachew: Machine Learning Input Data Processing as a Service,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of USENIX Conference on Annual Technical Conference
(ATC)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B. Gu, A. S. Yoon, D.-H. Bae, I. Jo, J. Lee, J. Yoon, J.-U. Kang, M. Kwon,
C. Yoon, S. Cho, J. Jeong, and D. Chang, “Biscuit: A Framework for
Near-Data Processing of Big Data Workloads,” in <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the
International Symposium on Computer Architecture (ISCA)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
U. Gupta, S. Hsia, V. Saraph, X. Wang, B. Reagen, G.-Y. Wei, H.-H. S. Lee,
D. Brooks, and C.-J. Wu, “DeepRecSys: A System for Optimizing End-to-End
At-Scale Neural Recommendation Inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the
International Symposium on Computer Architecture (ISCA)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
U. Gupta, S. Hsia, J. Zhang, M. Wilkening, J. Pombra, H.-H. S. Lee, G.-Y. Wei,
C.-J. Wu, and D. Brooks, “RecPipe: Co-Designing Models and Hardware to
Jointly Optimize Recommendation Quality and Performance,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the International Symposium on Microarchitecture
(MICRO)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
U. Gupta, C.-J. Wu, X. Wang, M. Naumov, B. Reagen, D. Brooks, B. Cottel,
K. Hazelwood, M. Hempstead, B. Jia, H.-H. S. Lee, A. Malevich, D. Mudigere,
M. Smelyanskiy, L. Xiong, and X. Zhang, “The Architectural Implications of
Facebook’s DNN-Based Personalized Recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of
the International Symposium on High-Performance Computer Architecture
(HPCA)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
K. Hazelwood, S. Bird, D. Brooks, S. Chintala, U. Diril, D. Dzhulgakov,
M. Fawzy, B. Jia, Y. Jia, A. Kalro, J. Law, K. Lee, J. Lu, P. Noordhuis,
M. Smelyanskiy, L. Xiong, and X. Wang, “Applied Machine Learning at
Facebook: A Datacenter Infrastructure Perspective,” in <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of
the International Symposium on High-Performance Computer Architecture
(HPCA)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
S. Hsia, U. Gupta, B. Acun, N. Ardalani, P. Zhong, G.-Y. Wei, D. Brooks, and
C.-J. Wu, “MP-Rec: Hardware-Software Co-design to Enable Multi-Path
Recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the International Conference on
Architectural Support for Programming Languages and Operating Systems
(ASPLOS)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Y.-C. Hu, M. T. Lokhandwala, T. I, and H.-W. Tseng, “Dynamic Multi-Resolution
Data Storage,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the International Symposium on
Microarchitecture (MICRO)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
R. Hwang, T. Kim, Y. Kwon, and M. Rhu, “Centaur: A Chiplet-Based, Hybrid
Sparse-Dense Accelerator for Personalized Recommendations,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the International Symposium on Computer Architecture
(ISCA)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Intel, “Intel Performance Counter Monitor,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/intel/pcm" title="">https://github.com/intel/pcm</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D. Ivchenko, D. Van Der Staay, C. Taylor, X. Liu, W. Feng, R. Kindi,
A. Sudarshan, and S. Sefati, “TorchRec: a PyTorch Domain Library for
Recommendation Systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the ACM Conference on
Recommender Systems (RecSys)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
B. Jiang, C. Deng, H. Yi, Z. Hu, G. Zhou, Y. Zheng, S. Huang, X. Guo, D. Wang,
Y. Song, L. Zhao, Z. Wang, P. Sun, Y. Zhang, D. Zhang, J. Li, J. Xu, X. Zhu,
and K. Gai, “XDL: An Industrial Deep Learning Framework for
High-dimensional Sparse Data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 1st
International Workshop on Deep Learning Practice for High-Dimensional Sparse
Data</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
S.-W. Jun, M. Liu, S. Lee, J. Hicks, J. Ankcorn, M. King, S. Xu, and Arvind,
“BlueDBM: An Appliance for Big Data Analytics,” in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of
the International Symposium on Computer Architecture (ISCA)</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
S.-W. Jun, A. Wright, S. Zhang, S. Xu, and Arvind, “GraFBoost: Using
Accelerated Flash Storage for External Graph Analytics,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the International Symposium on Computer Architecture
(ISCA)</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
M. Karpathiotakis, D. Wernli, and M. Stojanovic, “Scribe: Transporting
Petabytes per Hour via a Distributed, Buffered Queueing System,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://engineering.fb.com/2019/10/07/data-infrastructure/scribe/" title="">https://engineering.fb.com/2019/10/07/data-infrastructure/scribe/</a>,
2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
L. Ke, U. Gupta, B. Y. Cho, D. Brooks, V. Chandra, U. Diril, A. Firoozshahian,
K. Hazelwood, B. Jia, H.-H. S. Lee, M. Li, B. Maher, D. Mudigere, M. Naumov,
M. Schatz, M. Smelyanskiy, X. Wang, B. Reagen, C.-J. Wu, M. Hempstead, and
X. Zhang, “RecNMP: Accelerating Personalized Recommendation with
Near-Memory Processing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the International
Symposium on Computer Architecture (ISCA)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
L. Ke, U. Gupta, M. Hempstead, C.-J. Wu, H.-H. S. Lee, and X. Zhang,
“Hercules: Heterogeneity-Aware Inference Serving for At-Scale Personalized
Recommendation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the International Symposium on
High-Performance Computer Architecture (HPCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
G. Koo, K. K. Matam, T. I., H. K. G. Narra, J. Li, H.-W. Tseng, S. Swanson, and
M. Annavaram, “Summarizer: Trading Communication with Computing Near
Storage,” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the International Symposium on
Microarchitecture (MICRO)</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
M. Kuchnik, A. Klimovic, J. Simsa, V. Smith, and G. Amvrosiadis, “Plumber:
Diagnosing and Removing Performance Bottlenecks in Machine Learning Data
Pipelines,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of Machine Learning and Systems
(MLSys)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
A. V. Kumar and M. Sivathanu, “Quiver: An Informed Storage Cache for Deep
Learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of USENIX Conference on File and Storage
Technologies (FAST)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Y. Kwon, Y. Lee, and M. Rhu, “TensorDIMM: A Practical Near-Memory Processing
Architecture for Embeddings and Tensor Operations in Deep Learning,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the International Symposium on Microarchitecture
(MICRO)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y. Kwon, Y. Lee, and M. Rhu, “Tensor Casting: Co-Designing
Algorithm-Architecture for Personalized Recommendation Training,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the International Symposium on High-Performance Computer
Architecture (HPCA)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y. Kwon and M. Rhu, “Training Personalized Recommendation Systems from (GPU)
Scratch: Look Forward not Backwards,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the
International Symposium on Computer Architecture (ISCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Y. Lee, S. H. Seo, H. Choi, H. U. Sul, S. Kim, J. W. Lee, and T. J. Ham,
“MERCI: Efficient Embedding Reduction on Commodity Hardware via Sub-Query
Memoization,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the International Conference on
Architectural Support for Programming Languages and Operating Systems
(ASPLOS)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Y. Lee, J. Chung, and M. Rhu, “SmartSAGE: Training Large-scale Graph Neural
Networks using In-Storage Processing Architectures,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings
of the International Symposium on Computer Architecture (ISCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
C. Li, Y. Wang, C. Liu, S. Liang, H. Li, and X. Li, “GLIST: Towards
In-Storage Graph Learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of USENIX Conference on
Annual Technical Conference (ATC)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
S. Li, F. Tu, L. Liu, J. Lin, Z. Wang, Y. Kang, Y. Ding, and Y. Xie, “ECSSD:
Hardware/Data Layout Co-Designed In-Storage-Computing Architecture for
Extreme Classification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the International
Symposium on Computer Architecture (ISCA)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
J. Lin, L. Liang, Z. Qu, I. Ahmad, L. Liu, F. Tu, T. Gupta, Y. Ding, and
Y. Xie, “INSPIRE: IN-Storage Private Information REtrieval via Protocol and
Architecture Co-design,” in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the International
Symposium on Computer Architecture (ISCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
M. Liu, S. Peter, A. Krishnamurthy, and P. M. Phothilimthana, “E3:
Energy-Efficient Microservices on SmartNIC-Accelerated Servers,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of USENIX Conference on Annual Technical Conference
(ATC)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
R. Mahapatra, S. Ghodrati, B. H. Ahn, S. Kinzer, S. ting Wang, H. Xu,
L. Karthikeyan, H. Sharma, A. Yazdanbakhsh, M. Alian, and H. Esmaeilzadeh,
“Domain-Specific Computational Storage for Serverless Computing,”
<em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2303.03483</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
V. S. Mailthody, Z. Qureshi, W. Liang, Z. Feng, S. G. de Gonzalo, Y. Li,
H. Franke, J. Xiong, J. Huang, and W.-m. Hwu, “DeepStore: In-Storage
Acceleration for Intelligent Queries,” in <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the
International Symposium on Microarchitecture (MICRO)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
N. Mansouri Ghiasi, J. Park, H. Mustafa, J. Kim, A. Olgun, A. Gollwitzer,
D. Senol Cali, C. Firtina, H. Mao, N. Almadhoun Alserr, R. Ausavarungnirun,
N. Vijaykumar, M. Alser, and O. Mutlu, “GenStore: A High-Performance
In-Storage Processing System for Genome Sequence Analysis,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the International Conference on Architectural Support
for Programming Languages and Operating Systems (ASPLOS)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
K. K. Matam, G. Koo, H. Zha, H.-W. Tseng, and M. Annavaram, “GraphSSD: Graph
Semantics Aware SSD,” in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the International Symposium
on Computer Architecture (ISCA)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
J. Mohan, A. Phanishayee, A. Raniwala, and V. Chidambaram, “Analyzing and
Mitigating Data Stalls in DNN Training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the VLDB
Endowment (PVLDB)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
D. Mudigere, Y. Hao, J. Huang, Z. Jia, A. Tulloch, S. Sridharan, X. Liu,
M. Ozdal, J. Nie, J. Park, L. Luo, J. A. Yang, L. Gao, D. Ivchenko,
A. Basant, Y. Hu, J. Yang, E. K. Ardestani, X. Wang, R. Komuravelli, C.-H.
Chu, S. Yilmaz, H. Li, J. Qian, Z. Feng, Y. Ma, J. Yang, E. Wen, H. Li,
L. Yang, C. Sun, W. Zhao, D. Melts, K. Dhulipala, K. Kishore, T. Graf,
A. Eisenman, K. K. Matam, A. Gangidi, G. J. Chen, M. Krishnan, A. Nayak,
K. Nair, B. Muthiah, M. khorashadi, P. Bhattacharya, P. Lapukhov, M. Naumov,
A. Mathews, L. Qiao, M. Smelyanskiy, B. Jia, and V. Rao, “Software-Hardware
Co-design for Fast and Scalable Training of Deep Learning Recommendation
Models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the International Symposium on Computer
Architecture (ISCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
D. G. Murray, J. Simsa, A. Klimovic, and I. Indyk, “tf.data: A Machine
Learning Data Processing Framework,” in <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the VLDB
Endowment (PVLDB)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
K. Nagrecha, L. Liu, P. Delgado, and P. Padmanabhan, “InTune: Reinforcement
Learning-Based Data Pipeline Optimization for Deep Recommendation Models,”
in <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the ACM Conference on Recommender Systems (RecSys)</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
M. Naumov, D. Mudigere, H.-J. M. Shi, J. Huang, N. Sundaraman, J. Park,
X. Wang, U. Gupta, C.-J. Wu, A. G. Azzolini, D. Dzhulgakov, A. Mallevich,
I. Cherniavskii, Y. Lu, R. Krishnamoorthi, A. Yu, V. Kondratenko, S. Pereira,
X. Chen, W. Chen, V. Rao, B. Jia, L. Xiong, and M. Smelyanskiy, “Deep
Learning Recommendation Model for Personalization and Recommendation
Systems,” <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:1906.00091</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
NVIDIA, “NVIDIA DGX A100,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://images.nvidia.com/aem-dam/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf" title="">https://images.nvidia.com/aem-dam/Solutions/Data-Center/nvidia-dgx-a100-datasheet.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
NVIDIA, “NVIDIA Data Loading Library,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://developer.nvidia.com/dali" title="">https://developer.nvidia.com/dali</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
NVTabular, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/NVIDIA-Merlin/NVTabular" title="">https://github.com/NVIDIA-Merlin/NVTabular</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
S. Pan, T. Stavrinos, Y. Zhang, A. Sikaria, P. Zakharov, A. Sharma, M. Shuey,
R. Wareing, M. Gangapuram, G. Cao, C. Preseau, P. Singh, K. Patiejunas,
J. Tipton, E. Katz-Bassett, and W. Lloyd, “Facebook’s Tectonic Filesystem:
Efficiency from Exascale,” in <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of USENIX Conference on
File and Storage Technologies (FAST)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
J. Park, B. Kim, S. Yun, E. Lee, M. Rhu, and J. H. Ahn, “TRiM: Enhancing
Processor-Memory Interfaces with Scalable Tensor Reduction in Memory,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the International Symposium on Microarchitecture
(MICRO)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
P. Park, H. Jeong, and J. Kim, “TrainBox: An Extreme-Scale Neural Network
Training Server Architecture by Systematically Balancing Operations,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the International Symposium on Microarchitecture
(MICRO)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Z. Ruan, T. He, and J. Cong, “INSIDER: Designing In-Storage Computing
System for Emerging High-Performance Drive,” in <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of
USENIX Conference on Annual Technical Conference (ATC)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Samsung, “Samsung SmartSSD,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://samsungsemiconductor-us.com/smartssd/" title="">https://samsungsemiconductor-us.com/smartssd/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
S. Seshadri, M. Gahagan, S. Bhaskaran, T. Bunker, A. De, Y. Jin, Y. Liu, and
S. Swanson, “Willow: A User-Programmable SSD,” in <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Proceedings of
USENIX Symposium on Operating Systems Design and Implementation (OSDI)</em>,
2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
X. Sun, H. Wan, Q. Li, C.-L. Yang, T.-W. Kuo, and C. J. Xue, “RM-SSD:
In-Storage Computing for Large-Scale Recommendation Inference,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the International Symposium on High-Performance Computer
Architecture (HPCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
D. Tiwari, S. Boboila, S. Vazhkudai, Y. Kim, X. Ma, P. Desnoyers, and
Y. Solihin, “Active Flash: Towards Energy-Efficient, In-Situ Data Analytics
on Extreme-Scale Machines,” in <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Proceedings of USENIX Conference on
File and Storage Technologies (FAST)</em>, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
TorchArrow, <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/pytorch/torcharrow" title="">https://github.com/pytorch/torcharrow</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
H.-W. Tseng, Q. Zhao, Y. Zhou, M. Gahagan, and S. Swanson, “Morpheus:
Creating Application Objects Efficiently for Heterogeneous Computing,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the International Symposium on Computer Architecture
(ISCA)</em>, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
T. Um, B. Oh, B. Seo, M. Kweun, G. Kim, and W.-Y. Lee, “FastFlow:
Accelerating Deep Learning Model Training with Smart Offloading of Input Data
Pipeline,” in <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the VLDB Endowment (PVLDB)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
M. Wilkening, U. Gupta, S. Hsia, C. Trippel, C.-J. Wu, D. Brooks, and G.-Y.
Wei, “RecSSD: Near Data Processing for Solid State Drive Based
Recommendation Inference,” in <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the International
Conference on Architectural Support for Programming Languages and Operating
Systems (ASPLOS)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Xilinx, “Alveo U280 Data Center Accelerator Card,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.xilinx.com/products/boards-and-kits/alveo/u280.html" title="">https://www.xilinx.com/products/boards-and-kits/alveo/u280.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Xilinx, “Xilinx Vitis,”
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.xilinx.com/products/design-tools/vitis.html/" title="">https://www.xilinx.com/products/design-tools/vitis.html/</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly, M. J. Franklin,
S. Shenker, and I. Stoica, “Resilient Distributed Datasets: A
Fault-Tolerant Abstraction for In-Memory Cluster Computing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">9th
USENIX Symposium on Networked Systems Design and Implementation (NSDI)</em>,
2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
M. Zhao, N. Agarwal, A. Basant, B. Gedik, S. Pan, M. Ozdal, R. Komuravelli,
J. Pan, T. Bao, H. Lu, S. Narayanan, J. Langman, K. Wilfong, H. Rastogi,
C.-J. Wu, C. Kozyrakis, and P. Pol, “Understanding Data Storage and
Ingestion for Large-Scale Deep Recommendation Model Training,” in
<em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the International Symposium on Computer Architecture
(ISCA)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
M. Zhao, D. Choudhary, D. Tyagi, A. Somani, M. Kaplan, S.-H. Lin, S. Pumma,
J. Park, A. Basant, N. Agarwal, C.-J. Wu, and C. Kozyrakis, “RecD:
Deduplication for End-to-End Deep Learning Recommendation Model Training
Infrastructure,” in <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">Proceedings of Machine Learning and Systems
(MLSys)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
M. Zhao, S. Pan, N. Agarwal, Z. Wen, D. Xu, A. Natarajan, P. Kumar, S. S. P,
R. Tijoriwala, K. Asher, H. Wu, A. Basant, D. Ford, D. David, N. Yigitbasi,
P. Singh, and C.-J. Wu, “Tectonic-Shift: A Composite Storage Fabric for
Large-Scale ML Training,” in <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">Proceedings of USENIX Conference on
Annual Technical Conference (ATC)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
C. Zou and A. A. Chien, “ASSASIN: Architecture Support for Stream Computing
to Accelerate Computational Storage,” in <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Proceedings of the
International Symposium on Microarchitecture (MICRO)</em>, 2022.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jun  7 05:09:26 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
