<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2006.01412] Federated Learning in Vehicular Networks</title><meta property="og:description" content="Machine learning (ML) has recently been adopted in vehicular networks for applications such as autonomous driving, road safety prediction and vehicular object detection, due to its model-free characteristic, allowing a…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Federated Learning in Vehicular Networks">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Federated Learning in Vehicular Networks">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2006.01412">

<!--Generated on Thu Mar  7 06:37:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Machine learning,  federated learning,  vehicular networks,  edge intelligence,  edge efficiency.
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Federated Learning in Vehicular Networks
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Ahmet M. Elbir<sup id="id11.11.id1" class="ltx_sup"><span id="id11.11.id1.1" class="ltx_text ltx_font_italic">†</span></sup>, Burak Soner<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">‡</span></sup>, Sinem Çöleri<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">‡</span></sup>, Deniz Gündüz<sup id="id14.14.id4" class="ltx_sup"><span id="id14.14.id4.1" class="ltx_text ltx_font_italic">+</span></sup> and Mehdi Bennis<sup id="id15.15.id5" class="ltx_sup"><span id="id15.15.id5.1" class="ltx_text ltx_font_italic">++</span></sup>
</span>
<span class="ltx_contact ltx_role_affiliation"><math id="id6.6.m1.1" class="ltx_Math" alttext="{{\dagger}}" display="inline"><semantics id="id6.6.m1.1a"><mo id="id6.6.m1.1.1" xref="id6.6.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id6.6.m1.1b"><ci id="id6.6.m1.1.1.cmml" xref="id6.6.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m1.1c">{{\dagger}}</annotation></semantics></math>Department of Electrical and Electronics Engineering, Duzce University, Turkey
<br class="ltx_break"><math id="id7.7.m2.1" class="ltx_Math" alttext="{{\dagger}}" display="inline"><semantics id="id7.7.m2.1a"><mo id="id7.7.m2.1.1" xref="id7.7.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id7.7.m2.1b"><ci id="id7.7.m2.1.1.cmml" xref="id7.7.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id7.7.m2.1c">{{\dagger}}</annotation></semantics></math>Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg
<br class="ltx_break"><math id="id8.8.m3.1" class="ltx_Math" alttext="{{\ddagger}}" display="inline"><semantics id="id8.8.m3.1a"><mo id="id8.8.m3.1.1" xref="id8.8.m3.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id8.8.m3.1b"><ci id="id8.8.m3.1.1.cmml" xref="id8.8.m3.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id8.8.m3.1c">{{\ddagger}}</annotation></semantics></math>Department of Electrical and Electronics Engineering, Koc University, Istanbul, Turkey 
<br class="ltx_break"><math id="id9.9.m4.1" class="ltx_Math" alttext="+" display="inline"><semantics id="id9.9.m4.1a"><mo id="id9.9.m4.1.1" xref="id9.9.m4.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="id9.9.m4.1b"><plus id="id9.9.m4.1.1.cmml" xref="id9.9.m4.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="id9.9.m4.1c">+</annotation></semantics></math>Department of Electrical and Electronic Engineering, Imperial College London, U.K.
<br class="ltx_break"><math id="id10.10.m5.2" class="ltx_Math" alttext="++" display="inline"><semantics id="id10.10.m5.2a"><mrow id="id10.10.m5.2.3.2" xref="id10.10.m5.2.3.1.cmml"><mo id="id10.10.m5.1.1" xref="id10.10.m5.1.1.cmml">+</mo><mo lspace="0em" id="id10.10.m5.2.3.2.1" xref="id10.10.m5.2.3.1.cmml">⁣</mo><mo id="id10.10.m5.2.2" xref="id10.10.m5.2.2.cmml">+</mo></mrow><annotation-xml encoding="MathML-Content" id="id10.10.m5.2b"><list id="id10.10.m5.2.3.1.cmml" xref="id10.10.m5.2.3.2"><plus id="id10.10.m5.1.1.cmml" xref="id10.10.m5.1.1"></plus><plus id="id10.10.m5.2.2.cmml" xref="id10.10.m5.2.2"></plus></list></annotation-xml><annotation encoding="application/x-tex" id="id10.10.m5.2c">++</annotation></semantics></math>The Centre for Wireless Communications, the University of Oulu, Finland
</span>
<span class="ltx_contact ltx_role_affiliation">E-mail: ahmetmelbir@gmail.com, bsoner16@ku.edu.tr, scoleri@ku.edu.tr, 
<br class="ltx_break">d.gunduz@imperial.ac.uk, bennis@ee.oulu.fi

</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id16.id1" class="ltx_p">Machine learning (ML) has recently been adopted in vehicular networks for applications such as autonomous driving, road safety prediction and vehicular object detection, due to its model-free characteristic, allowing adaptive fast response. However, most of these ML applications employ centralized learning (CL), which brings significant overhead for data transmission between the parameter server and vehicular edge devices. Federated learning (FL) framework has been recently introduced as an efficient tool with the goal of reducing transmission overhead while achieving privacy through the transmission of model updates instead of the whole dataset. In this paper, we investigate the usage of FL over CL in vehicular network applications to develop intelligent transportation systems. We provide a comprehensive analysis on the feasibility of FL for the ML based vehicular applications, as well as investigating object detection by utilizing image-based datasets as a case study. Then, we identify the major challenges from both learning perspective, i.e., data labeling and model training, and from the communications point of view, i.e., data rate, reliability, transmission overhead, privacy and resource management. Finally, we highlight related future research directions for FL in vehicular networks.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Machine learning, federated learning, vehicular networks, edge intelligence, edge efficiency.

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">As vehicles evolve with advanced safety features and self-driving capabilities, massive amounts of data is generated by a variety of on-board sensors, such as camera, RADAR and LIDAR as well as proximity and temperature sensors. An autonomous vehicle is expected to generate about one gigabyte of data per second. However, currently, generated data is not systematically processed, stored, or analyzed for better inference. Recently, machine learning (ML) algorithms have been developed to learn from sensor measurements due to several advantages, including low computational complexity when solving optimization-based or combinatorial search problems and the ability to extrapolate new features from a limited set of features contained in a training dataset.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The current trend in the usage of ML in vehicular networks focuses on centralized algorithms, called centralized learning (CL), where a powerful learning algorithm, often a neural network (NN), is trained on a massive dataset collected from the edge devices on the vehicles, as illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The NN model provides a non-linear mapping between the input, which contains vehicle sensor data, and the output, which can be the labels of the sensor data. This mapping is learned by training the NN through the collection of the local sensor data from the edge devices, <span id="S1.p2.1.1" class="ltx_text">
which is a supervised learning scheme.<span id="S1.p2.1.1.1" class="ltx_text"> Once model training is completed, the model parameters are sent back to the edge devices for prediction. <span id="S1.p2.1.1.1.1" class="ltx_text">However, the size of the generated data is huge when aiming at wider and deeper NN architectures for successful training.
Thus, training a model with data transmission from the edge devices to the cloud center in a reliable manner may be too costly in terms of bandwidth, while incurring unacceptable delays, and infringing user privacy.<span id="S1.p2.1.1.1.1.1" class="ltx_text"></span></span></span></span></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2006.01412/assets/HFCL.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="957" height="398" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Model training for CL, FL and HFCL in a vehicular network. </figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Federated learning (FL) has been recently introduced with the goal of bringing CL down to the edge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, as illustrated in Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. In FL, instead of their local datasets, the edge devices only send the gradients of the learnable parameters derived from these local datasets to the cloud server. The cloud server aggregates these gradients and update the model parameters, which are then transmitted back to the edge devices. This procedure continues iteratively, until convergence. The training procedure is similar to that of CL, except that FL does not involve the transmission of the whole dataset. This enables reducing both the complexity of ever growing datasets at the edge devices in the vehicles and the transmission overhead of these datasets to the cloud servers. Hence, FL is a promising approach to efficiently train the learning models by preserving the privacy of raw data and reducing the transmission overhead in wireless communications. While FL has already received great interest within wireless networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, imparting FL to the vehicular networks is more challenging due to the dynamic nature of the channel characteristics of vehicular environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Furthermore, previous works either approach FL from only communication point-of-view <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, or suffer from investigating the use cases of FL in vehicular applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The main contribution of this paper is to provide a comprehensive analysis of FL by considering both learning and communications aspects, based on the description and analysis of several learning-based vehicular applications. Moreover, we investigate the performance of FL in vehicular use cases, such as vehicular object detection and vehicle-to-infrastructure (V2I) beam selection based on image and LIDAR data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">This paper aims to provide a comprehensive grasp on how vehicular networks can benefit from FL. First, we discuss ML based vehicular network applications in the context of vehicle management and traffic management. We present the advantages of FL over CL for training the models in these vehicular network applications. By utilizing image- and LIDAR-based datasets, we present a case study of FL, such as 3D object detection. Then, we provide an extensive discussion on both FL- and communications-related research challenges in a broad perspective. FL-related challenges include data diversity, labeling and model training, whereas communication-related challenges are transmission overhead, privacy, scheduling and resource allocation. Finally, we provide an extensive discussion on the major research issues and future research directions in making FL feasible in vehicular networks.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">ML for Vehicular Applications</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">ML-based techniques, particularly NN-based models, have become significantly effective in vehicular applications with the increase in the amount as well as diversity of data generated by the sensors. Specifically, NN originates from the imitation of the human brain containing billions of neurons forming a neural network. There are mainly three types of ML, namely, supervised, unsupervised and reinforcement learning. In a supervised learning model, an NN is trained on a labeled dataset where an answer key is provided beforehand. In contrast, unsupervised learning studies the clustering of the unlabeled data by exploiting the hidden features/patterns derived from the dataset. Reinforcement learning (RL) also uses unlabeled data and the model parameters are learned based on the award and penalty mechanism, which are formulated as a function of varying environment characteristics.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the following, we present several ML-based vehicular applications and the associated challenges in two categories: vehicle management and traffic management.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Vehicle Management: Autonomous Driving</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The aim of autonomous driving is to navigate a vehicle through a road environment without collisions. To achieve this, the vehicle needs to detect, identify, localize and track surrounding objects, such as pedestrians, trees and other vehicles relative to its own frame of reference, and adjust its driving dynamics accordingly. <span id="S2.SS1.p1.1.1" class="ltx_text"> The layered nature of this end-to-end autonomous driving task, depicted in Figure <a href="#S2.F2" title="Figure 2 ‣ II-B Traffic Management: Infotainment and Route Planning ‣ II ML for Vehicular Applications ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, renders conventional methods that use an ensemble of hand-crafted computer vision techniques for each layer sub-optimal. Deep neural networks that are trained in an end-to-end manner on a huge amount of RADAR/LIDAR/camera sensor data, such as convolutional neural networks (CNNs), have emerged in recent years as a better alternative <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.<span id="S2.SS1.p1.1.1.1" class="ltx_text"></span></span></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, a human-like decision-making method is proposed for autonomous driving by using CNNs. The input of CNN is the LIDAR data collected from multiple vehicles to provide depth information, whereas the <span id="S2.SS1.p2.1.1" class="ltx_text"> output of the <span id="S2.SS1.p2.1.1.1" class="ltx_text">CNN is the decision regarding the speed and steering of the vehicle. <span id="S2.SS1.p2.1.1.1.1" class="ltx_text"> While this demonstrates the advantage of using
trained CNNs for autonomous driving, CNNs require a large amount of precisely labeled data for accurate prediction.<span id="S2.SS1.p2.1.1.1.1.1" class="ltx_text"> Therefore, the training is usually conducted in a cloud server in an offline manner. The main drawback in offline training is that the trained model cannot adapt to the environment dynamics. The usage of FL can provide the adaptation to the environmental changes, such as feature learning in different geographical locations. However, FL utilizes gradient computation at the edge device, which necessitates immediate and accurate labeling of online data. Therefore, the higher layers of the autonomous driving ML application stack shown in Figure <a href="#S2.F2" title="Figure 2 ‣ II-B Traffic Management: Infotainment and Route Planning ‣ II ML for Vehicular Applications ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, such as intent estimation and driving decisions
can get higher benefit from FL strategies rather than the lower layers such as object detection and tracking. </span></span></span></span></p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Traffic Management: Infotainment and Route Planning</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><span id="S2.SS2.p1.1.1" class="ltx_text">Infotainment and route planning are crucial for preventing traffic jams, sustaining an efficient distribution of resources during normal traffic, and enabling effective emergency response during extreme situations.<span id="S2.SS2.p1.1.1.1" class="ltx_text"> Data-driven ML solutions are superior to conventional model-based approaches for these applications since they can easily adapt to the random changes in the system dynamics over time caused by human involvement, which can be difficult to model mathematically. <span id="S2.SS2.p1.1.1.1.1" class="ltx_text">For instance, in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, a deep learning based resource management scheme, which learns the hidden patterns in data traffic over a vehicular software-defined network (SDN), is shown to improve packet traffic efficiency compared to the conventional methods. Similar solutions exist for route planning and efficient management of vehicle traffic.<span id="S2.SS2.p1.1.1.1.1.1" class="ltx_text"> While these applications benefit from versatile ML solutions, they rely on frequent model retraining due to the ever-changing dynamics of the systems.</span></span></span></span></p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2006.01412/assets/autoDrvLayers.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="291" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The hierarchical structure of tasks for autonomous driving. </figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">FL for Model Training in Vehicular Networks</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p"><span id="S3.p1.1.1" class="ltx_text">Conventional approaches for training ML models in vehicular applications consider a central server, which collects the raw data from vehicular edge devices, computes gradients according to the current state of the model and the incoming data, then updates the model parameters accordingly. However, this approach cannot easily yield a model that can adapt to local changes as required by the applications. Many of the nodes in a given vehicular network cannot comply with the requirements for high transmission overhead and raw data privacy due to the highly dynamic and harsh communication channel, hindering the local adaptation capability of the model, since the local data from those nodes cannot participate in the model training to better represent the distribution of the global dataset. Decentralized training with FL scheme enables adaptation to the data distribution since the gradients are computed at the edge devices based on local datasets, and the model updates themselves, instead of the much larger raw data, are transmitted to the central server. This not only reduces the transmission overhead, but also provides some level of privacy. Thus, it enables more nodes to participate in model training, which results in a model that better adapts to the local changes inside the global dataset.</span></p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">The proposition of FL, which enables it to adapt to local changes, is based on the “mini-batch learning” technique in conventional ML model training, where the dataset is partitioned into smaller sub-blocks that are used for parameter updates rather than the whole dataset. The gradients are computed for each of these sub-blocks, i.e., mini-batches, and then a combination of these gradients, typically the average, is used as the gradient value for a parameter update step, which is repeated iteratively until convergence. Since the computation of the gradients for different mini-batches are independent, FL schemes can exploit the local processing capabilities of the edge devices in the vehicles to compute and transmit these in parallel, as illustrated in Figure 2. The central server then simply combines the collected gradients and performs model update, then transmits the new model parameters to each vehicle. This reduces the transmission overhead and preserves privacy to some extend since the much larger and vulnerable raw data is not transmitted, enabling significantly more nodes to participate in the model training, thus, increasing the local adaptivity of the ML model.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">Although FL has its advantages over CL, not all applications can benefit from FL strategies for model training. Using FL is advantageous for applications and environments that satisfy three conditions:</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2006.01412/assets/diag4a.png" id="S3.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="419" height="308" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2006.01412/assets/x1.png" id="S3.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="346" height="259" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span> (a) Visualization of input and output data, and (b) learning accuracy for 3D object detection.
</figcaption>
</figure>
<div id="S3.p4" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text">The application requires a model that adapts to new conditions, which constantly change on the edge, and retraining/update-from-server/deployment for each edge device needs to happen fast.</span></p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text">Data gathered by the edge device cannot be off-loaded to a cloud server within the required time frame, either because of its sheer volume, or due to privacy concerns</span></p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text">Data gathered by edge devices can be quickly and accurately labeled on the same device for gradient computations.</span></p>
</div>
</li>
</ol>
<p id="S3.p4.1" class="ltx_p">As a new emerging field still in its infancy, there are a very limited number of works considering FL in vehicular applications that satisfy these three conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, FL has been considered in a vehicular network, where the communication between the data center and the edge devices is assisted by road side units to ensure low latency for gradient data transmission. Specifically, a Lyapunov optimization based approach is proposed to minimize the delays incurred by the transmission of gradient data in FL. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, the authors propose a selective model aggregation approach, where the data center collects the gradient data from only “fine” edge clients in a vehicular network, such as the devices with high data quality and transmission power capability. Then, the performance of FL is compared to that of CL based on both MNIST and BelgiumTSC image classification datasets, which are composed of the images of the numbers and traffic signs, respectively. While these works demonstrate the above-mentioned advantages of using FL over CL for a constrained set of applications, they are not directly applicable for realistic ML-based vehicular applications. FL still faces significant open challenges for use in such realistic applications like autonomous driving and traffic management via infotainment and route planning.<span id="S3.p4.1.1" class="ltx_text"></span></p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">As discussed above, FL is advantageous as compared to CL in terms of communication overhead while FL requires computational resources from the edge devices to perform model computation. Therefore, not all the vehicles can participate in training if they do not have enough computational resources. To address this issue, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> devised a hybrid federated and centralized learning (HFCL) framework, in which only the vehicles with sufficient resources employ FL, while the remaining ones send their local datasets to the PS, which computes the model parameters on behalf of them (see Figure <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Then, the model parameters corresponding to all vehicles are aggregated at the PS. Thus, a part of the vehicles perform FL while the remaining ones employ CL, which demands dataset transmission prior to the training. As a result, HFCL provides a trade-off between the higher learning accuracy of CL and the lower communication overhead of FL at the expense of sacrificing privacy. The privacy concerns in HFCL can be addressed through coded FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Case Study 1: 3D Object Detection</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.20" class="ltx_p">As a case study, we evaluate the performance of learning frameworks, e.g., CL, FL and HFCL, on 3D object detection problem in vehicular networks, based on the Lyft Level 5 AV dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, collected from LIDAR equipment and cameras mounted on vehicles. The input data is selected as a top view image of the ego vehicle, which includes the received LIDAR signal strengths for different elevations, and the output is the classified representation of the vehicles/objects as boxes, which is obtained by the preprocessing of the images from the cameras, as illustrated in Figure <a href="#S3.F3.sf1" title="In Figure 3 ‣ III FL for Model Training in Vehicular Networks ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3a</span></a>. The training dataset is collected from <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mn id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><cn type="integer" id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">10</annotation></semantics></math> vehicles in different areas after preprocessing of camera and LIDAR data. We assume <math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><mn id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><cn type="integer" id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">3</annotation></semantics></math> out of <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mn id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><cn type="integer" id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">10</annotation></semantics></math> the vehicles are passive while the remaining ones are active clients. Each local dataset includes <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="10^{3}" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><msup id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mn id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">10</mn><mn id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">superscript</csymbol><cn type="integer" id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">10</cn><cn type="integer" id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">10^{3}</annotation></semantics></math> input-output pairs, whose sizes are <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="336\times 336\times 3" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mn id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">336</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">336</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.5.m5.1.1.1a" xref="S3.SS1.p1.5.m5.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.5.m5.1.1.4" xref="S3.SS1.p1.5.m5.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><times id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"></times><cn type="integer" id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2">336</cn><cn type="integer" id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">336</cn><cn type="integer" id="S3.SS1.p1.5.m5.1.1.4.cmml" xref="S3.SS1.p1.5.m5.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">336\times 336\times 3</annotation></semantics></math> and <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="336\times 336\times 1" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mrow id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml"><mn id="S3.SS1.p1.6.m6.1.1.2" xref="S3.SS1.p1.6.m6.1.1.2.cmml">336</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.6.m6.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.6.m6.1.1.3" xref="S3.SS1.p1.6.m6.1.1.3.cmml">336</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.6.m6.1.1.1a" xref="S3.SS1.p1.6.m6.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.6.m6.1.1.4" xref="S3.SS1.p1.6.m6.1.1.4.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><apply id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1"><times id="S3.SS1.p1.6.m6.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1"></times><cn type="integer" id="S3.SS1.p1.6.m6.1.1.2.cmml" xref="S3.SS1.p1.6.m6.1.1.2">336</cn><cn type="integer" id="S3.SS1.p1.6.m6.1.1.3.cmml" xref="S3.SS1.p1.6.m6.1.1.3">336</cn><cn type="integer" id="S3.SS1.p1.6.m6.1.1.4.cmml" xref="S3.SS1.p1.6.m6.1.1.4">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">336\times 336\times 1</annotation></semantics></math>, respectively. Hence, the total number of data symbols is <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="(336\times 336\times 3+336\times 336)\times 10^{4}\approx 4.5\times 10^{9}" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mrow id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml"><mrow id="S3.SS1.p1.7.m7.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.cmml"><mrow id="S3.SS1.p1.7.m7.1.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.7.m7.1.1.1.1.1.2" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.7.m7.1.1.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.cmml"><mrow id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.cmml"><mn id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.2" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.2.cmml">336</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.1" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.1.cmml">×</mo><mn id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.3" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.3.cmml">336</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.1a" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.1.cmml">×</mo><mn id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.4" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.4.cmml">3</mn></mrow><mo id="S3.SS1.p1.7.m7.1.1.1.1.1.1.1" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.SS1.p1.7.m7.1.1.1.1.1.1.3" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.cmml"><mn id="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.2" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.2.cmml">336</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.1" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.1.cmml">×</mo><mn id="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.3" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.3.cmml">336</mn></mrow></mrow><mo rspace="0.055em" stretchy="false" id="S3.SS1.p1.7.m7.1.1.1.1.1.3" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.SS1.p1.7.m7.1.1.1.2" xref="S3.SS1.p1.7.m7.1.1.1.2.cmml">×</mo><msup id="S3.SS1.p1.7.m7.1.1.1.3" xref="S3.SS1.p1.7.m7.1.1.1.3.cmml"><mn id="S3.SS1.p1.7.m7.1.1.1.3.2" xref="S3.SS1.p1.7.m7.1.1.1.3.2.cmml">10</mn><mn id="S3.SS1.p1.7.m7.1.1.1.3.3" xref="S3.SS1.p1.7.m7.1.1.1.3.3.cmml">4</mn></msup></mrow><mo id="S3.SS1.p1.7.m7.1.1.2" xref="S3.SS1.p1.7.m7.1.1.2.cmml">≈</mo><mrow id="S3.SS1.p1.7.m7.1.1.3" xref="S3.SS1.p1.7.m7.1.1.3.cmml"><mn id="S3.SS1.p1.7.m7.1.1.3.2" xref="S3.SS1.p1.7.m7.1.1.3.2.cmml">4.5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.7.m7.1.1.3.1" xref="S3.SS1.p1.7.m7.1.1.3.1.cmml">×</mo><msup id="S3.SS1.p1.7.m7.1.1.3.3" xref="S3.SS1.p1.7.m7.1.1.3.3.cmml"><mn id="S3.SS1.p1.7.m7.1.1.3.3.2" xref="S3.SS1.p1.7.m7.1.1.3.3.2.cmml">10</mn><mn id="S3.SS1.p1.7.m7.1.1.3.3.3" xref="S3.SS1.p1.7.m7.1.1.3.3.3.cmml">9</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><apply id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1"><approx id="S3.SS1.p1.7.m7.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.2"></approx><apply id="S3.SS1.p1.7.m7.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1"><times id="S3.SS1.p1.7.m7.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.1.2"></times><apply id="S3.SS1.p1.7.m7.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1"><plus id="S3.SS1.p1.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.1"></plus><apply id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2"><times id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.1"></times><cn type="integer" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.2.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.2">336</cn><cn type="integer" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.3.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.3">336</cn><cn type="integer" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.4.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.2.4">3</cn></apply><apply id="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.3"><times id="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.1"></times><cn type="integer" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.2">336</cn><cn type="integer" id="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.1.1.1.1.3.3">336</cn></apply></apply><apply id="S3.SS1.p1.7.m7.1.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS1.p1.7.m7.1.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.1.1.1.3.2">10</cn><cn type="integer" id="S3.SS1.p1.7.m7.1.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.1.3.3">4</cn></apply></apply><apply id="S3.SS1.p1.7.m7.1.1.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3"><times id="S3.SS1.p1.7.m7.1.1.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.3.1"></times><cn type="float" id="S3.SS1.p1.7.m7.1.1.3.2.cmml" xref="S3.SS1.p1.7.m7.1.1.3.2">4.5</cn><apply id="S3.SS1.p1.7.m7.1.1.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.7.m7.1.1.3.3.1.cmml" xref="S3.SS1.p1.7.m7.1.1.3.3">superscript</csymbol><cn type="integer" id="S3.SS1.p1.7.m7.1.1.3.3.2.cmml" xref="S3.SS1.p1.7.m7.1.1.3.3.2">10</cn><cn type="integer" id="S3.SS1.p1.7.m7.1.1.3.3.3.cmml" xref="S3.SS1.p1.7.m7.1.1.3.3.3">9</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">(336\times 336\times 3+336\times 336)\times 10^{4}\approx 4.5\times 10^{9}</annotation></semantics></math>. The dataset has <math id="S3.SS1.p1.8.m8.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S3.SS1.p1.8.m8.1a"><mn id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><cn type="integer" id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">9</annotation></semantics></math> classes, i.e., <math id="S3.SS1.p1.9.m9.1" class="ltx_Math" alttext="\mathrm{car}" display="inline"><semantics id="S3.SS1.p1.9.m9.1a"><mi id="S3.SS1.p1.9.m9.1.1" xref="S3.SS1.p1.9.m9.1.1.cmml">car</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.9.m9.1b"><ci id="S3.SS1.p1.9.m9.1.1.cmml" xref="S3.SS1.p1.9.m9.1.1">car</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.9.m9.1c">\mathrm{car}</annotation></semantics></math>, <math id="S3.SS1.p1.10.m10.1" class="ltx_Math" alttext="\mathrm{motorcycle}" display="inline"><semantics id="S3.SS1.p1.10.m10.1a"><mi id="S3.SS1.p1.10.m10.1.1" xref="S3.SS1.p1.10.m10.1.1.cmml">motorcycle</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.10.m10.1b"><ci id="S3.SS1.p1.10.m10.1.1.cmml" xref="S3.SS1.p1.10.m10.1.1">motorcycle</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.10.m10.1c">\mathrm{motorcycle}</annotation></semantics></math>, <math id="S3.SS1.p1.11.m11.1" class="ltx_Math" alttext="\mathrm{bus}" display="inline"><semantics id="S3.SS1.p1.11.m11.1a"><mi id="S3.SS1.p1.11.m11.1.1" xref="S3.SS1.p1.11.m11.1.1.cmml">bus</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.11.m11.1b"><ci id="S3.SS1.p1.11.m11.1.1.cmml" xref="S3.SS1.p1.11.m11.1.1">bus</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.11.m11.1c">\mathrm{bus}</annotation></semantics></math>, <math id="S3.SS1.p1.12.m12.1" class="ltx_Math" alttext="\mathrm{bicycle}" display="inline"><semantics id="S3.SS1.p1.12.m12.1a"><mi id="S3.SS1.p1.12.m12.1.1" xref="S3.SS1.p1.12.m12.1.1.cmml">bicycle</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.12.m12.1b"><ci id="S3.SS1.p1.12.m12.1.1.cmml" xref="S3.SS1.p1.12.m12.1.1">bicycle</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.12.m12.1c">\mathrm{bicycle}</annotation></semantics></math>, <math id="S3.SS1.p1.13.m13.1" class="ltx_Math" alttext="\mathrm{truck}" display="inline"><semantics id="S3.SS1.p1.13.m13.1a"><mi id="S3.SS1.p1.13.m13.1.1" xref="S3.SS1.p1.13.m13.1.1.cmml">truck</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.13.m13.1b"><ci id="S3.SS1.p1.13.m13.1.1.cmml" xref="S3.SS1.p1.13.m13.1.1">truck</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.13.m13.1c">\mathrm{truck}</annotation></semantics></math>, <math id="S3.SS1.p1.14.m14.1" class="ltx_Math" alttext="\mathrm{pedestrian}" display="inline"><semantics id="S3.SS1.p1.14.m14.1a"><mi id="S3.SS1.p1.14.m14.1.1" xref="S3.SS1.p1.14.m14.1.1.cmml">pedestrian</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.14.m14.1b"><ci id="S3.SS1.p1.14.m14.1.1.cmml" xref="S3.SS1.p1.14.m14.1.1">pedestrian</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.14.m14.1c">\mathrm{pedestrian}</annotation></semantics></math>, <math id="S3.SS1.p1.15.m15.1" class="ltx_Math" alttext="\mathrm{other}\hskip 2.0pt\mathrm{vehicle}" display="inline"><semantics id="S3.SS1.p1.15.m15.1a"><mrow id="S3.SS1.p1.15.m15.1.1" xref="S3.SS1.p1.15.m15.1.1.cmml"><mi id="S3.SS1.p1.15.m15.1.1.2" xref="S3.SS1.p1.15.m15.1.1.2.cmml">other</mi><mo lspace="0.200em" rspace="0em" id="S3.SS1.p1.15.m15.1.1.1" xref="S3.SS1.p1.15.m15.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.15.m15.1.1.3" xref="S3.SS1.p1.15.m15.1.1.3.cmml">vehicle</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.15.m15.1b"><apply id="S3.SS1.p1.15.m15.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1"><times id="S3.SS1.p1.15.m15.1.1.1.cmml" xref="S3.SS1.p1.15.m15.1.1.1"></times><ci id="S3.SS1.p1.15.m15.1.1.2.cmml" xref="S3.SS1.p1.15.m15.1.1.2">other</ci><ci id="S3.SS1.p1.15.m15.1.1.3.cmml" xref="S3.SS1.p1.15.m15.1.1.3">vehicle</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.15.m15.1c">\mathrm{other}\hskip 2.0pt\mathrm{vehicle}</annotation></semantics></math>, <math id="S3.SS1.p1.16.m16.1" class="ltx_Math" alttext="\mathrm{animal}" display="inline"><semantics id="S3.SS1.p1.16.m16.1a"><mi id="S3.SS1.p1.16.m16.1.1" xref="S3.SS1.p1.16.m16.1.1.cmml">animal</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.16.m16.1b"><ci id="S3.SS1.p1.16.m16.1.1.cmml" xref="S3.SS1.p1.16.m16.1.1">animal</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.16.m16.1c">\mathrm{animal}</annotation></semantics></math>, <math id="S3.SS1.p1.17.m17.1" class="ltx_Math" alttext="\mathrm{emergency}\hskip 2.0pt\mathrm{vehicle}" display="inline"><semantics id="S3.SS1.p1.17.m17.1a"><mrow id="S3.SS1.p1.17.m17.1.1" xref="S3.SS1.p1.17.m17.1.1.cmml"><mi id="S3.SS1.p1.17.m17.1.1.2" xref="S3.SS1.p1.17.m17.1.1.2.cmml">emergency</mi><mo lspace="0.200em" rspace="0em" id="S3.SS1.p1.17.m17.1.1.1" xref="S3.SS1.p1.17.m17.1.1.1.cmml">​</mo><mi id="S3.SS1.p1.17.m17.1.1.3" xref="S3.SS1.p1.17.m17.1.1.3.cmml">vehicle</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.17.m17.1b"><apply id="S3.SS1.p1.17.m17.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1"><times id="S3.SS1.p1.17.m17.1.1.1.cmml" xref="S3.SS1.p1.17.m17.1.1.1"></times><ci id="S3.SS1.p1.17.m17.1.1.2.cmml" xref="S3.SS1.p1.17.m17.1.1.2">emergency</ci><ci id="S3.SS1.p1.17.m17.1.1.3.cmml" xref="S3.SS1.p1.17.m17.1.1.3">vehicle</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.17.m17.1c">\mathrm{emergency}\hskip 2.0pt\mathrm{vehicle}</annotation></semantics></math>, which are represented by the boxes as shown in Figure <a href="#S3.F3.sf1" title="In Figure 3 ‣ III FL for Model Training in Vehicular Networks ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3a</span></a>. For 3D object detection and segmentation, we have used <span id="S3.SS1.p1.20.1" class="ltx_text ltx_font_typewriter">U-net</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> with <math id="S3.SS1.p1.18.m18.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS1.p1.18.m18.1a"><mn id="S3.SS1.p1.18.m18.1.1" xref="S3.SS1.p1.18.m18.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.18.m18.1b"><cn type="integer" id="S3.SS1.p1.18.m18.1.1.cmml" xref="S3.SS1.p1.18.m18.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.18.m18.1c">8</annotation></semantics></math> convolutional layers to learn the features of the input data. further, the total number of parameters in <span id="S3.SS1.p1.20.2" class="ltx_text ltx_font_typewriter">U-net</span> is approximately <math id="S3.SS1.p1.19.m19.1" class="ltx_Math" alttext="2\times 10^{6}" display="inline"><semantics id="S3.SS1.p1.19.m19.1a"><mrow id="S3.SS1.p1.19.m19.1.1" xref="S3.SS1.p1.19.m19.1.1.cmml"><mn id="S3.SS1.p1.19.m19.1.1.2" xref="S3.SS1.p1.19.m19.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p1.19.m19.1.1.1" xref="S3.SS1.p1.19.m19.1.1.1.cmml">×</mo><msup id="S3.SS1.p1.19.m19.1.1.3" xref="S3.SS1.p1.19.m19.1.1.3.cmml"><mn id="S3.SS1.p1.19.m19.1.1.3.2" xref="S3.SS1.p1.19.m19.1.1.3.2.cmml">10</mn><mn id="S3.SS1.p1.19.m19.1.1.3.3" xref="S3.SS1.p1.19.m19.1.1.3.3.cmml">6</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.19.m19.1b"><apply id="S3.SS1.p1.19.m19.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1"><times id="S3.SS1.p1.19.m19.1.1.1.cmml" xref="S3.SS1.p1.19.m19.1.1.1"></times><cn type="integer" id="S3.SS1.p1.19.m19.1.1.2.cmml" xref="S3.SS1.p1.19.m19.1.1.2">2</cn><apply id="S3.SS1.p1.19.m19.1.1.3.cmml" xref="S3.SS1.p1.19.m19.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.19.m19.1.1.3.1.cmml" xref="S3.SS1.p1.19.m19.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS1.p1.19.m19.1.1.3.2.cmml" xref="S3.SS1.p1.19.m19.1.1.3.2">10</cn><cn type="integer" id="S3.SS1.p1.19.m19.1.1.3.3.cmml" xref="S3.SS1.p1.19.m19.1.1.3.3">6</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.19.m19.1c">2\times 10^{6}</annotation></semantics></math>, and <math id="S3.SS1.p1.20.m20.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S3.SS1.p1.20.m20.1a"><mn id="S3.SS1.p1.20.m20.1.1" xref="S3.SS1.p1.20.m20.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.20.m20.1b"><cn type="integer" id="S3.SS1.p1.20.m20.1.1.cmml" xref="S3.SS1.p1.20.m20.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.20.m20.1c">40</annotation></semantics></math> communication rounds for model training are carried out <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.6" class="ltx_p">The training performance of the all methods is presented in Figure <a href="#S3.F3.sf2" title="In Figure 3 ‣ III FL for Model Training in Vehicular Networks ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3b</span></a>, from which we observe that the HFCL provides a moderate performance between CL and FL. Nevertheless, all of the vehicles can participate in training while conventional FL methods cannot support it whereas the transmission overhead of CL is prohibitive. Thus, HFCL solves the trade-off between the computational capability of the clients and the transmission overhead. The overhead of CL is due to the transmission of whole data symbols, i.e., approximately <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="4.5\times 10^{9}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mn id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">4.5</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">×</mo><msup id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mn id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml">10</mn><mn id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml">9</mn></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><cn type="float" id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">4.5</cn><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">10</cn><cn type="integer" id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3">9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">4.5\times 10^{9}</annotation></semantics></math>. In contrast, the complexity of FL is due to the two way (edge <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="\leftrightarrows" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mo stretchy="false" id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">⇆</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">⇆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\leftrightarrows</annotation></semantics></math> server) transmission of the model updates during training until convergence, i.e., <math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="2\times 40\times(2\times 10^{6})=160\times 10^{6}" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mrow id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mrow id="S3.SS1.p2.3.m3.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.cmml"><mn id="S3.SS1.p2.3.m3.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.3.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.2.cmml">×</mo><mn id="S3.SS1.p2.3.m3.1.1.1.4" xref="S3.SS1.p2.3.m3.1.1.1.4.cmml">40</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.1.2a" xref="S3.SS1.p2.3.m3.1.1.1.2.cmml">×</mo><mrow id="S3.SS1.p2.3.m3.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.3.m3.1.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml"><mn id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.1" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.1.cmml">×</mo><msup id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.cmml"><mn id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.2.cmml">10</mn><mn id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.3.cmml">6</mn></msup></mrow><mo stretchy="false" id="S3.SS1.p2.3.m3.1.1.1.1.1.3" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">=</mo><mrow id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml"><mn id="S3.SS1.p2.3.m3.1.1.3.2" xref="S3.SS1.p2.3.m3.1.1.3.2.cmml">160</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p2.3.m3.1.1.3.1" xref="S3.SS1.p2.3.m3.1.1.3.1.cmml">×</mo><msup id="S3.SS1.p2.3.m3.1.1.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.cmml"><mn id="S3.SS1.p2.3.m3.1.1.3.3.2" xref="S3.SS1.p2.3.m3.1.1.3.3.2.cmml">10</mn><mn id="S3.SS1.p2.3.m3.1.1.3.3.3" xref="S3.SS1.p2.3.m3.1.1.3.3.3.cmml">6</mn></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><eq id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2"></eq><apply id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1"><times id="S3.SS1.p2.3.m3.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.2"></times><cn type="integer" id="S3.SS1.p2.3.m3.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.3">2</cn><cn type="integer" id="S3.SS1.p2.3.m3.1.1.1.4.cmml" xref="S3.SS1.p2.3.m3.1.1.1.4">40</cn><apply id="S3.SS1.p2.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1"><times id="S3.SS1.p2.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.1"></times><cn type="integer" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.2">2</cn><apply id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3">superscript</csymbol><cn type="integer" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.2">10</cn><cn type="integer" id="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.1.1.1.1.3.3">6</cn></apply></apply></apply><apply id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3"><times id="S3.SS1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.1"></times><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.2">160</cn><apply id="S3.SS1.p2.3.m3.1.1.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3">superscript</csymbol><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.2">10</cn><cn type="integer" id="S3.SS1.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3.3.3">6</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">2\times 40\times(2\times 10^{6})=160\times 10^{6}</annotation></semantics></math> for <math id="S3.SS1.p2.4.m4.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S3.SS1.p2.4.m4.1a"><mn id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><cn type="integer" id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">40</annotation></semantics></math> iterations. As a result, FL and HFCL have approximately <math id="S3.SS1.p2.5.m5.1" class="ltx_Math" alttext="28" display="inline"><semantics id="S3.SS1.p2.5.m5.1a"><mn id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">28</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><cn type="integer" id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">28</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">28</annotation></semantics></math> and <math id="S3.SS1.p2.6.m6.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S3.SS1.p2.6.m6.1a"><mn id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><cn type="integer" id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">3</annotation></semantics></math> times lower transmission overhead as compared to CL, respectively.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Case Study 2: Millimeter-wave Beam Selection</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.7" class="ltx_p">In this scenario, the performance of FL is evaluated for vehicular to infrastructure (V2I) millimeter-wave (mm-Wave) communication <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. In particular, for a vehicle moving on a road, a laser scan of the environment is obtained via LIDAR equipment to obtain the beam directions with the best mm-Wave channel conditions to establish communication with a base station (BS) on the road. The LIDAR dataset is collected for <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">10</annotation></semantics></math> vehicles in a similar scenario, for which the input of the learning model is the LIDAR point cloud measurement, as shown in Figure <a href="#S3.F4" title="Figure 4 ‣ III-B Case Study 2: Millimeter-wave Beam Selection ‣ III FL for Model Training in Vehicular Networks ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, and the output is the index of best beam directions. In order to construct the non-linear mapping between the LIDAR data and the beam directions, a CNN with <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">8</annotation></semantics></math> layers and <math id="S3.SS2.p1.3.m3.1" class="ltx_Math" alttext="7462" display="inline"><semantics id="S3.SS2.p1.3.m3.1a"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">7462</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><cn type="integer" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">7462</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">7462</annotation></semantics></math> learnable parameters is trained for <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mn id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><cn type="integer" id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">20</annotation></semantics></math> epochs, then the trained models for FL and CL are deployed for beam selection. The results show that FL and CL have <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="88.6\%" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mrow id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mn id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">88.6</mn><mo id="S3.SS2.p1.5.m5.1.1.1" xref="S3.SS2.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">88.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">88.6\%</annotation></semantics></math> and <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="89.1\%" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mrow id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mn id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">89.1</mn><mo id="S3.SS2.p1.6.m6.1.1.1" xref="S3.SS2.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="latexml" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">89.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">89.1\%</annotation></semantics></math> beam selection accuracy to select the best available mm-Wave beams between the vehicle and the BS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. While a slight performance loss is observed in FL, it has approximately <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="300" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mn id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><cn type="integer" id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">300</annotation></semantics></math> times lower transmission overhead as compared to CL. The effectiveness of FL is due to the transmission of only the model parameters, while CL involves the transmission of the whole dataset.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2006.01412/assets/lidarFL.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="395" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> Visualization of the LIDAR point cloud data for a single vehicle moving on a road
for mm-Wave beam training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Research Challenges and Future Directions</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">FL-Related Research Challenges</span>
</h3>

<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS1.5.1.1" class="ltx_text">IV-A</span>1 </span>Data Diversity</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">In FL, training data is located at the edge devices, which causes data diversity due to the non-uniform distribution of the datasets at the edge devices. For example, in autonomous driving scenario, the image data obtained from vehicles in different locations have different distributions. Data diversity causes large variances in the averaged gradient data, and therefore, decreases the convergence rate for the learning models. <span id="S4.SS1.SSS1.p1.1.1" class="ltx_text">For example, the features of the collected image data in different locations increases the diversity of the dataset, which makes NN unable to perform feature-extraction and feature-representation well. Furthermore, the number of edge devices in vehicular networks is smaller than general wireless networks due to the large distance among the vehicles. This leads to the model aggregation from fewer number of edge devices, which also makes model training a challenging task in vehicular applications.<span id="S4.SS1.SSS1.p1.1.1.1" class="ltx_text"> To improve the model training performance against data diversity caused by either non-uniform data distribution or insufficient number of edge devices, one possible solution is to increase the model size, i.e., enlarging the width and the depth of the NN model so that the NN can provide robust feature representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</span></span></p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS2.5.1.1" class="ltx_text">IV-A</span>2 </span>Labeling</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p"><span id="S4.SS1.SSS2.p1.1.1" class="ltx_text">ML techniques are mostly supervised, i.e., the dataset is labeled. For example, an image dataset of vehicles includes labels, such as “car”, “motorcycle”, “truck” (see Figure <a href="#S3.F3" title="Figure 3 ‣ III FL for Model Training in Vehicular Networks ‣ Federated Learning in Vehicular Networks" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Data labeling demands certain amount of labor to label/annotate the collected training data, which is usually done in an offline manner in current commercially available autonomous vehicle prototypes. In FL based training, the dataset should be labeled so that each edge client can compute the model updates based on the local labeled dataset. This is one of the main challenges of FL in major learning based vehicular applications, such as autonomous driving and object detection. A possible approach to resolve this problem is to use reinforcement learning (RL) techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, which do not require labeled data. In particular, RL is based on the award and penalty mechanism, which are formulated as a function of varying environment characteristics to optimize the objective of the learning problem. The main challenge in RL is that RL requires longer training times and the performance is usually worse than the supervised techniques due to the absence of labels. <span id="S4.SS1.SSS2.p1.1.1.1" class="ltx_text"></span></span></p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS1.SSS3.5.1.1" class="ltx_text">IV-A</span>3 </span>Efficient Model Training</h4>

<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">The efficiency of model training can be improved by the use of transfer learning (TL) based approaches. TL is an ML method, where a model developed for a certain task is reused as the starting point for a model on a different task. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, TL has been proposed for cognitive RADAR applications, where an ML model is used for different sensor selection tasks. The application of TL for vehicular networks is advantageous. For instance, instead of training a model from scratch, a well-trained model with large dataset can be used with a soft parameter update for smaller datasets. In this case, the parameter update involves lower complexity since only a small portion of the NN is trained, which leads to more efficient model training. The success of TL in vehicular networks depends on data similarity. Specifically, the TL accuracy strongly relies on the similarity between the newly collected data at the edge devices in the vehicular network and the training data used to pre-train the model. In order to obtain higher accuracy from the NN with new data, a larger portion of the model should be updated. Thus, there is a trade-off between the similarity/diversity of the datasets and the required training complexity. While data similarity/diversity issue has been studied for cognitive RADAR applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, its effect on vehicular applications has not been exploited. In addition, the diversity of the datasets can incur difficulties when performing TL due to non-uniform distribution of the dataset, which has been studied in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, accommodating a shallow ML model without the focus on TL. As a result, new approaches need to be developed to make FL model training more feasible in vehicular network applications.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Communications-Related Research Challenges</span>
</h3>

<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS1.5.1.1" class="ltx_text">IV-B</span>1 </span><span id="S4.SS2.SSS1.6.2" class="ltx_text">Transmission Overhead</span>
</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p"><span id="S4.SS2.SSS1.p1.1.1" class="ltx_text">Compared to the CL based techniques, FL allows us to reduce the transmission overhead by replacing raw data transmission with model update parameter transmission. However, the size of the model update parameter set is directly proportional to the size of the learning model. Thus, the transmission of model parameters may become a bottleneck if the model involves massive number of learnable parameters.</span></p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p"><span id="S4.SS2.SSS1.p2.1.1" class="ltx_text">The solution is either to reduce the transmission overhead or increase the capacity of the channel.<span id="S4.SS2.SSS1.p2.1.1.1" class="ltx_text"> There are various methods to reduce the transmission overhead in FL-based framework including sparsification and quantization. The sparsity of the gradients, i.e., most of the gradients being zero, can be exploited to reduce the amount of transmitted data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>. While this approach reduces the transmission overhead, it increases encoding/decoding complexity at the parameter server and edge devices in a vehicular network for reliable performance.</span></span></p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p"><span id="S4.SS2.SSS1.p3.1.1" class="ltx_text">Alternatively, channel reliability can be improved to facilitate faster convergence to more accurate models. For example, the authors in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> attempt to design an FL-friendly communication protocol from scratch based on the exploitation of the broadcast/multiple-access nature of the downlink/uplink channels in an FL framework.<span id="S4.SS2.SSS1.p3.1.1.1" class="ltx_text"> Using analog links in noisy band-limited channels for both uplink and downlink rather than digital links has been demonstrated to provide higher reliability at lower transmit power, especially when the local data among edge devices is not uniformly distributed. <span id="S4.SS2.SSS1.p3.1.1.1.1" class="ltx_text">Development of environment-aware heterogeneous architectures combining the strengths of different standard-compliant vehicular communication technologies, e.g., choosing between IEEE 802.11p, millimeter-wave and visible light communications based on road/channel conditions, is a promising research direction for improving the overall network reliability of FL-based vehicular frameworks.<span id="S4.SS2.SSS1.p3.1.1.1.1.1" class="ltx_text"></span></span></span></span></p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS2.5.1.1" class="ltx_text">IV-B</span>2 </span><span id="S4.SS2.SSS2.6.2" class="ltx_text">Security and Privacy</span>
</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p"><span id="S4.SS2.SSS2.p1.1.1" class="ltx_text">During FL training, different types of devices may participate in the learning stage. Thus, untrusted devices can join the network more easily, which brings security and privacy issues. The security and privacy of the devices in the network can be achieved through the use of reputation management (reward and punishment) based approaches. In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, authors propose a method, where each edge device receives a reward in exchange for their computation of power and data quality. However, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> considers a simple FL framework with a single server. In a realistic vehicular network scenario, there can be multiple access points acting like servers in FL, increasing the dimension of the reputation management problem. As a result, their usage in vehicular networks requires further research for multi-server FL architectures.</span></p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S4.SS2.SSS3.5.1.1" class="ltx_text">IV-B</span>3 </span><span id="S4.SS2.SSS3.6.2" class="ltx_text">Scheduling and Resource Management</span>
</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">The availability of wireless communication resources and the packet error performance of each node in a wireless network vary greatly among nodes due to both device heterogeneity and spatial distribution.<span id="S4.SS2.SSS3.p1.1.1" class="ltx_text"> <span id="S4.SS2.SSS3.p1.1.1.1" class="ltx_text">Since FL convergence rate is directly affected by the performance of the communication link between each edge device and the central FL controller, scheduling and resource management of nodes participating in an FL scheme need to be explicitly optimized with the objective of maximizing the FL convergence rate. However, this problem becomes ill-defined for vehicular networks where the links between the vehicular edge devices on the road are usually intact whereas the links to the central controller from each device experiences frequent and sporadic handovers and drop-outs. A “collaborative” FL solution where the vehicular edge devices instead utilize multiple hops between neighboring vehicular edge devices to reach the central controller can decrease the sensitivity of the FL framework performance to such network-related shortcomings. While such a framework is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, it is specifically tuned to the network topology and dynamics of regular wireless/cellular networks, which are significantly different from vehicular networks. Therefore, further research on scheduling and resource management strategies for FL, focusing on the low-density / high-mobility network topology in the vehicular environment, is needed.<span id="S4.SS2.SSS3.p1.1.1.1.1" class="ltx_text"></span></span></span></p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Summary</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we presented the FL-based framework <span id="S5.p1.1.1" class="ltx_text">for decentralized training of ML models<span id="S5.p1.1.1.1" class="ltx_text">, as an efficient learning scheme for vehicular networks and edge intelligence, in <span id="S5.p1.1.1.1.1" class="ltx_text"> contrast to classical ML techniques based on CL<span id="S5.p1.1.1.1.1.1" class="ltx_text">. We enlisted several applications of vehicular networks that are particularly amenable to FL, such as autonomous driving, infotainment and route planning. We identified a number of major research challenges for FL in vehicular networks, involving both learning challenges, i.e., data labeling and model training, and the communications challenges, i.e., data rate, reliability, transmission overhead, privacy and resource management, along with the related future research directions.</span></span></span></span></p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by CHIST-ERA grant CHIST-ERA-18-SDCDN-001, and the Scientific and Technological Council of Turkey 119E350.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
T. Li, A. K. Sahu, A. Talwalkar, and V. Smith, “Federated Learning:
Challenges, Methods, and Future Directions,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Signal Process.
Mag.</em>, vol. 37, no. 3, pp. 50–60, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Posner <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated Learning in Vehicular Networks:
Opportunities and Solutions,” <em id="bib.bib2.2.2" class="ltx_emph ltx_font_italic">IEEE Network</em>, vol. 35, no. 2, pp.
152–159, Feb. 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Z. Du <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated Learning for Vehicular Internet of Things:
Recent Advances and Open Issues,” <em id="bib.bib3.2.2" class="ltx_emph ltx_font_italic">IEEE Open J. Comput. Soc.</em>, vol. 1,
pp. 45–61, May 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
L. Li, K. Ota, and M. Dong, “Humanlike Driving: Empirical
Decision-Making System for Autonomous Vehicles,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Veh.
Technol.</em>, vol. 67, no. 8, pp. 6814–6823, 2018.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
A. Jindal <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Sedative: Sdn-enabled deep learning architecture for
network traffic control in vehicular cyber-physical systems,” <em id="bib.bib5.2.2" class="ltx_emph ltx_font_italic">IEEE
Network</em>, vol. 32, no. 6, pp. 66–73, 2018.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Samarakoon <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Distributed Federated Learning for
Ultra-Reliable Low-Latency Vehicular Communications,” <em id="bib.bib6.2.2" class="ltx_emph ltx_font_italic">IEEE Trans.
Commun.</em>, vol. 68, no. 2, pp. 1146–1159, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D. Ye <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated learning in vehicular edge computing: A
selective model aggregation approach,” <em id="bib.bib7.2.2" class="ltx_emph ltx_font_italic">IEEE Access</em>, vol. 8, pp.
23 920–23 935, 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
A. M. Elbir <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “A Hybrid Architecture for Federated and
Centralized Learning,” <em id="bib.bib8.2.2" class="ltx_emph ltx_font_italic">IEEE Trans. Cognit. Commun. Networking</em>, p. 1,
Jun. 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Prakash <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Coded Computing for Low-Latency Federated Learning
Over Wireless Edge Networks,” <em id="bib.bib9.2.2" class="ltx_emph ltx_font_italic">IEEE J. Sel. Areas Commun.</em>, vol. 39,
no. 1, pp. 233–250, Nov. 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
O. Ronneberger <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “U-net: Convolutional networks for biomedical
image segmentation,” in <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">International Conference on Medical image
computing and computer-assisted intervention</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
M. B. Mashhadi <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Federated mmWave Beam Selection Utilizing
LIDAR Data,” <em id="bib.bib11.2.2" class="ltx_emph ltx_font_italic">IEEE Wireless Commun. Lett.</em>, vol. 10, no. 10, pp.
2269–2273, Jul. 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
A. M. Elbir and S. Coleri, “Federated Learning for Hybrid Beamforming in
mm-Wave Massive MIMO,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Commun. Lett.</em>, vol. 24, no. 12, pp.
2795–2799, Aug 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Duan <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Hierarchical reinforcement learning for self-driving
decision-making without reliance on labelled driving data,” <em id="bib.bib13.2.2" class="ltx_emph ltx_font_italic">IET
Intelligent Transport Systems</em>, vol. 14, no. 5, pp. 297–305, 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
A. M. Elbir and K. V. Mishra, “Sparse Array Selection Across Arbitrary Sensor
Geometries with Deep Transfer Learning,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Cognit. Commun.
Networking</em>, p. 1, Jun 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
M. Mohammadi Amiri and D. Gündüz, “Machine Learning at the
Wireless Edge: Distributed Stochastic Gradient Descent Over-the-Air,”
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">IEEE Trans. Signal Process.</em>, vol. 68, pp. 2155–2169, 2020.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M. Chen <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Wireless Communications for Collaborative Federated
Learning,” <em id="bib.bib16.2.2" class="ltx_emph ltx_font_italic">IEEE Commun. Mag.</em>, vol. 58, no. 12, pp. 48–54, Dec 2020.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2006.01411" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2006.01412" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2006.01412">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2006.01412" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2006.01414" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar  7 06:37:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
