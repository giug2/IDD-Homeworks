<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2206.10845] Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation</title><meta property="og:description" content="Recently, Synthetic data-based Instance Segmentation has become an exceedingly favorable optimization paradigm since it leverages simulation rendering and physics to generate high-quality image-annotation pairs.
In thiâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2206.10845">

<!--Generated on Mon Mar 11 16:28:05 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Parallel Pre-trained Transformers (PPT) 
<br class="ltx_break">for Synthetic Data-based Instance Segmentation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Ming Li<span id="footnotex1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal Contribution. Corresponding author is Jie Wu.</span></span></span>
â€ƒâ€ƒâ€ƒJie Wu<span id="footnotex2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Equal Contribution. Corresponding author is Jie Wu.</span></span></span>
â€ƒâ€ƒâ€ƒJinhang Cai
â€ƒâ€ƒâ€ƒJie Qin
â€ƒâ€ƒâ€ƒYuxi Ren 
<br class="ltx_break">Xuefeng Xiao
â€ƒâ€ƒâ€ƒMin Zheng
â€ƒâ€ƒâ€ƒRui Wang
â€ƒâ€ƒâ€ƒXin Pan

<br class="ltx_break">ByteDance Inc.
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recently, Synthetic data-based Instance Segmentation has become an exceedingly favorable optimization paradigm since it leverages simulation rendering and physics to generate high-quality image-annotation pairs.
In this paper, we propose a <span id="id1.id1.1" class="ltx_text ltx_font_bold ltx_font_italic">Parallel Pre-trained Transformers (PPT)</span> framework to accomplish the synthetic data-based Instance Segmentation task.
Specifically, we leverage the off-the-shelf pre-trained vision Transformers to alleviate the gap between natural and synthetic data, which helps to provide good generalization in the downstream synthetic data scene with few samples.
Swin-B-based CBNet V2, Swin-L-based CBNet V2 and Swin-L-based Uniformer are employed for parallel feature learning, and the results of these three models are fused by pixel-level Non-maximum Suppression (NMS) algorithm to obtain more robust results.
The experimental results reveal that PPT ranks first in the CVPR2022 AVA Accessibility Vision and Autonomy Challenge, with a 65.155% AP.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Instance segmentation is a fundamental and crucial task due to extensive vision applications such as scene understanding and autonomous driving. However, this task is still highly dependent on adequate granular pixel-level annotations, which requires a considerable amount of manual effort.
To alleviate such expensive and unwieldy annotations, some researchers attempt to address this task via synthetic data, where a series of simulation rendering and physics are designed to provide high-quality image-annotation pairs.
To provide vision-based benchmarks relevant to accessibility, â€CVPR2022 AVA Accessibility Vision and Autonomy Challengeâ€ is set up to spark the interest of AI researchers working on more robust visual reasoning models for accessibility and boost the performance of synthetic data-based instance segmentationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
The challenge involves a synthetic instance segmentation benchmark incorporating use-cases of autonomous systems interacting with pedestrians with disabilities, which contains challenging accessibility-related person and object categories, such as â€˜caneâ€™ and â€˜wheelchair.â€™
Compared with traditional instance segmentation, this challenge confronts three additional difficulties:
(i) domain generalization problem between synthetic and natural data.
(ii) long-tailed and few sample issues.
(iii) uncertain problem of segmentation in the edge.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2206.10845/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">The pipeline of our Parallel Pre-trained Transformers (PPT).</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This paper proposes a <span id="S1.p2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Parallel Pre-trained Transformers (PPT)</span> framework to accomplish this challenge.
Some studies have revealed that the model pre-trained with large-scale datasets can generalize well in diverse downstream scenarios.
Hence we use the off-the-shelf pre-trained model (from COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and BigDetectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>) to alleviate the gap between natural and synthetic data so that it can be quickly generalized in the downstream synthetic data scene with few samples.
We employ Swin-B-based CBNet V2, Swin-L-based CBNet V2Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and Swin-L-based UniformerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> for parallel feature learning, and the results of these thress models are fused by pixel-level NMSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to obtain more robust results. In the training process, we design a balance copy-and-paste data augmentation approach to alleviate the problem of few samples and long-tail distribution.
Experimental results on â€œCVPR2022 AVA Accessibility Vision and Autonomy Challengeâ€ show that our proposed PPT rank first in the competition.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methodology</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we first illustrate the overview of the proposed PPT framework in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Then the pre-trained Transformers and data augmentation are presented in Sec.Â <a href="#S2.SS1" title="2.1 Pre-trained Transformer â€£ 2 Methodology â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a> and Sec.Â <a href="#S2.SS2" title="2.2 Data Augmentation â€£ 2 Methodology â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>, respectively. The pixel-level non-maximum suppression is introduced Sec.Â <a href="#S2.SS3" title="2.3 Pixel-level Non-maximum Suppression â€£ 2 Methodology â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Pre-trained Transformer</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Pre-trained models have demonstrated their strong generalization performance on diverse tasks, i.e., the models trained on large-scale datasets can quickly and well transfer to other datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Since Vision Transformer (ViT) does not have the inductive bias introduced by CNN, it can capture more generalized and robust global concepts on large-scale datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Therefore, we choose to fine-tune a series of competitive ViT-based detection models, which are pre-trained on large-scale object detection or instance segmentation datasets like BigDetectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">UniFormerÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> integrates the advantages of CNN and self-attention in a concise transformer format, which explores local and global token affinity in shallow and deep layers, respectively. CBNet v2 Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> groups multiple identical backbones through composite connections to construct high-performance detectors. BigDetectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> is a larger dataset for improved detector pre-training by simply combining the training data from existing datasets (LVISÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, OpenImagesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and Objects365Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>) with carefully designed principles. The pre-trained CBNet v2 on BigDetection has shown significant generalization abilityÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Augmentation</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Data augmentation is essential for many vision tasks. Multi-scale training is a common and valuable data augmentation used for object detection and instance segmentation. Although it has a good performance improvement for datasets with multi-scale images like COCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, it does not improve for the current challenge dataset, which we assume is because all images are of the same size.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Simple copy-pasteÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> technology provides impressive gains for the instance segmentation model by pasting objects randomly, especially for datasets under long-tailed distribution. However, we argue that this simple approach equally increases the samples for all classes but can not provide additional supervision signals for the categories with few samples. Therefore, we propose the balanced-copy-paste data augmentation approach. Balanced-copy-paste adaptively samples the objects according to their effective number of objectsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, which further alleviates the long-tail effect and improves the overall performance.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Pixel-level Non-maximum Suppression</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">We use Test-Time Augmentation (TTA) to obtain more robust prediction results. Specifically, we resize the test image to different scales and fuse the predictions from multiple scales to get the final result, which can be seen as the fusion of a single image under multiple data augmentation.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Model ensemble is an effective way to improve final performance by integrating the outputs of different models to obtain more robust and complete prediction results. After training all single models and obtaining mask predictions with TTA, we simply ensemble all results by pixel-level NMSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> with Gaussian kernel of 2.0.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Experimental Settings.</h3>

<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Models</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px1.p1.1" class="ltx_p">In this paper, we use Swin-Base-based CBNet V2, Swin-Large-based CBNet V2 and Swin-Base-based Uniformer.
We leverage Big-Detection pre-trained weights to optimize Swin-Base-based CBNet V2 and employ COCO pre-trained weights for other models. These pre-trained weights are taken from their official release. We also provide smaller detection models for analysis, such as Mask RCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and DetectoRSÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px2.p1.1" class="ltx_p">We adopt the challenge dataÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> for training and testing. It should be noted that this dataset is exposed in long-tailed data distribution, and the number of instances of normal pedestrians and vehicles is much higher than that of accessibility-related person and object categories.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Implementation Details.</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS1.SSS0.Px3.p1.1" class="ltx_p">We combine the training and validation sets to train our model with the input image size <math id="S3.SS1.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1"><times id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">1920\times 1080</annotation></semantics></math>. Padding is used to make sure each side can be divided by 32. We adopt simple-copy-pasteÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> data augmentation with a 0.5 probability horizontal flip to relieve the long-tailed distribution issue.
Training hyper-parameters such as learning rate and weight decay mostly follow the setting from the COCO datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. Test-Time Augmentation (TTA) is used to obtain more competitive results.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.3" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:269pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(71.7pt,-44.5pt) scale(1.49455186353279,1.49455186353279) ;">
<table id="S3.T1.3.3" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.3.3.4.1" class="ltx_tr">
<th id="S3.T1.3.3.4.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Method</th>
<th id="S3.T1.3.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Backbone</th>
<th id="S3.T1.3.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Pretrain</th>
<th id="S3.T1.3.3.4.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Epochs</th>
<th id="S3.T1.3.3.4.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP (Val)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Mask RCNN<sup id="S3.T1.1.1.1.1.1" class="ltx_sup">âˆ—</sup>
</td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">R50</td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Rand</td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">12</td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">31.9</td>
</tr>
<tr id="S3.T1.2.2.2" class="ltx_tr">
<td id="S3.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_border_r">Mask RCNN<sup id="S3.T1.2.2.2.1.1" class="ltx_sup">âˆ—</sup>
</td>
<td id="S3.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">R50</td>
<td id="S3.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">ImageNet</td>
<td id="S3.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r">12</td>
<td id="S3.T1.2.2.2.5" class="ltx_td ltx_align_center">42.0</td>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.1" class="ltx_td ltx_align_center ltx_border_r">Mask RCNN<sup id="S3.T1.3.3.3.1.1" class="ltx_sup">âˆ—</sup>
</td>
<td id="S3.T1.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r">R50</td>
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">COCO</td>
<td id="S3.T1.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r">12</td>
<td id="S3.T1.3.3.3.5" class="ltx_td ltx_align_center">44.3</td>
</tr>
<tr id="S3.T1.3.3.5.1" class="ltx_tr">
<td id="S3.T1.3.3.5.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">DetectoRS</td>
<td id="S3.T1.3.3.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R101</td>
<td id="S3.T1.3.3.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">COCO</td>
<td id="S3.T1.3.3.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12</td>
<td id="S3.T1.3.3.5.1.5" class="ltx_td ltx_align_center ltx_border_t">54.7</td>
</tr>
<tr id="S3.T1.3.3.6.2" class="ltx_tr">
<td id="S3.T1.3.3.6.2.1" class="ltx_td ltx_align_center ltx_border_r">DetectoRS</td>
<td id="S3.T1.3.3.6.2.2" class="ltx_td ltx_align_center ltx_border_r">R101</td>
<td id="S3.T1.3.3.6.2.3" class="ltx_td ltx_align_center ltx_border_r">COCO</td>
<td id="S3.T1.3.3.6.2.4" class="ltx_td ltx_align_center ltx_border_r">36</td>
<td id="S3.T1.3.3.6.2.5" class="ltx_td ltx_align_center">55.2</td>
</tr>
<tr id="S3.T1.3.3.7.3" class="ltx_tr">
<td id="S3.T1.3.3.7.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">UniFormer</td>
<td id="S3.T1.3.3.7.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Swin-B</td>
<td id="S3.T1.3.3.7.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">COCO</td>
<td id="S3.T1.3.3.7.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36</td>
<td id="S3.T1.3.3.7.3.5" class="ltx_td ltx_align_center ltx_border_t">59.0</td>
</tr>
<tr id="S3.T1.3.3.8.4" class="ltx_tr">
<td id="S3.T1.3.3.8.4.1" class="ltx_td ltx_align_center ltx_border_r">HTC</td>
<td id="S3.T1.3.3.8.4.2" class="ltx_td ltx_align_center ltx_border_r">Swinv2-L</td>
<td id="S3.T1.3.3.8.4.3" class="ltx_td ltx_align_center ltx_border_r">ImageNet</td>
<td id="S3.T1.3.3.8.4.4" class="ltx_td ltx_align_center ltx_border_r">36</td>
<td id="S3.T1.3.3.8.4.5" class="ltx_td ltx_align_center">59.2</td>
</tr>
<tr id="S3.T1.3.3.9.5" class="ltx_tr">
<td id="S3.T1.3.3.9.5.1" class="ltx_td ltx_align_center ltx_border_r">CBNet v2</td>
<td id="S3.T1.3.3.9.5.2" class="ltx_td ltx_align_center ltx_border_r">Swin-B</td>
<td id="S3.T1.3.3.9.5.3" class="ltx_td ltx_align_center ltx_border_r">BigDet</td>
<td id="S3.T1.3.3.9.5.4" class="ltx_td ltx_align_center ltx_border_r">36</td>
<td id="S3.T1.3.3.9.5.5" class="ltx_td ltx_align_center">60.6</td>
</tr>
<tr id="S3.T1.3.3.10.6" class="ltx_tr">
<td id="S3.T1.3.3.10.6.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">CBNet v2</td>
<td id="S3.T1.3.3.10.6.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Swin-L</td>
<td id="S3.T1.3.3.10.6.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">COCO</td>
<td id="S3.T1.3.3.10.6.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">36</td>
<td id="S3.T1.3.3.10.6.5" class="ltx_td ltx_align_center ltx_border_bb">60.8</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.7.2.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.5.1" class="ltx_text" style="font-size:90%;">Instance segmentation mAP on validation dataset for different methods. Better pre-trained weights, stronger augmentation, longer training epochs and larger models can improve the final result. <math id="S3.T1.5.1.m1.1" class="ltx_Math" alttext="*" display="inline"><semantics id="S3.T1.5.1.m1.1b"><mo id="S3.T1.5.1.m1.1.1" xref="S3.T1.5.1.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.1.m1.1c"><times id="S3.T1.5.1.m1.1.1.cmml" xref="S3.T1.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.1.m1.1d">*</annotation></semantics></math> denotes we use simple augmentation, and stronger augmentation and the original image size is used by default.</span></figcaption>
</figure>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental Results</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Better Pre-trained Weights</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px1.p1.3" class="ltx_p">We extensively evaluate the effectiveness of different models and pre-trained weights in TableÂ <a href="#S3.T1" title="Table 1 â€£ Implementation Details. â€£ 3.1 Experimental Settings. â€£ 3 Experiments â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We use Mask RCNN with ResNet-50Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> to evaluate different pre-training weights. When the model weights are randomly initialized, the final mAP is <math id="S3.SS2.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="31.9\%" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml">31.9</mn><mo id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.1.m1.1.1.2">31.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.1.m1.1c">31.9\%</annotation></semantics></math> on the validation dataset. ImageNet pre-train (backbone only) and COCO pre-train (entire model) significantly boost the performance, which improves <math id="S3.SS2.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="10.1\%" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml">10.1</mn><mo id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.2.m2.1.1.2">10.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.2.m2.1c">10.1\%</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="12.3\%" display="inline"><semantics id="S3.SS2.SSS0.Px1.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml"><mn id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml">12.3</mn><mo id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px1.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px1.p1.3.m3.1.1.2">12.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px1.p1.3.m3.1c">12.3\%</annotation></semantics></math> mask mAP, respectively. For CBNet v2, the result of BigDet pre-training with swin-B backbone is close to the COCO pre-training with swin-L backbone, which further indicates that better pre-trained leads to better performance improvement.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Stronger Models</h4>

<div id="S3.SS2.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px2.p1.1" class="ltx_p">In addition to Mask RCNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, we also evaluate other stronger models that work better for COCO instance segmentation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, including DetectoRS, UniFormer, HTC, and CBNet v2 Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> in TableÂ <a href="#S3.T1" title="Table 1 â€£ Implementation Details. â€£ 3.1 Experimental Settings. â€£ 3 Experiments â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The mAP of these four models on COCO is incremental, and their fine-tune performance on the challenge dataset is also incremental, which means that the stronger pre-trained models have better fine-tune results.</p>
</div>
</section>
<section id="S3.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Improved Training Strategies</h4>

<div id="S3.SS2.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px3.p1.5" class="ltx_p">We evaluate different training strategies in TableÂ <a href="#S3.T3" title="Table 3 â€£ Improved Test Strategies â€£ 3.2 Experimental Results â€£ 3 Experiments â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, including different input resolutions, whether to use multi-scale training, whether to use SCP and different mask loss weights. Multi-scale training does not bring performance gains, it may be because all images are of <math id="S3.SS2.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1"><times id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.1.m1.1c">1920\times 1080</annotation></semantics></math> resolution and the fixed size is enough.
Copy-paste augmentation and increasing the weight of mask loss can further improve the final result, <em id="S3.SS2.SSS0.Px3.p1.5.1" class="ltx_emph ltx_font_italic">i.e.,</em> from baseline <math id="S3.SS2.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="44.3\%" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml">44.3</mn><mo id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2">44.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.2.m2.1c">44.3\%</annotation></semantics></math> mAP to <math id="S3.SS2.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="51.7\%" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml"><mn id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml">51.7</mn><mo id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.3.m3.1.1.2">51.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.3.m3.1c">51.7\%</annotation></semantics></math> mAP for Mask RCNN. In order to fully unleash the power of the large model, we also include the validation set into the training process, adjust the test hyper-parameters and introduce TTA. These strategies improve the UniFormer and CBNet v2 by <math id="S3.SS2.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="2.4\%" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml"><mn id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.2.cmml">2.4</mn><mo id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.1" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.4.m4.1.1.2">2.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.4.m4.1c">2.4\%</annotation></semantics></math> mAP and <math id="S3.SS2.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="2.5\%" display="inline"><semantics id="S3.SS2.SSS0.Px3.p1.5.m5.1a"><mrow id="S3.SS2.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.cmml"><mn id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2.cmml">2.5</mn><mo id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.1" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.5.m5.1b"><apply id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.5.m5.1.1.2">2.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.5.m5.1c">2.5\%</annotation></semantics></math> mAP, respectively.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div id="S3.T2.2" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:232.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(65.7pt,-35.2pt) scale(1.43450620607482,1.43450620607482) ;">
<table id="S3.T2.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.2.1.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Method</th>
<th id="S3.T2.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Training Data</th>
<th id="S3.T2.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">TTA</th>
<th id="S3.T2.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Hyper-parm</th>
<th id="S3.T2.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP (Test)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.2.1.2.1" class="ltx_tr">
<td id="S3.T2.2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">UniFormer</td>
<td id="S3.T2.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Train</td>
<td id="S3.T2.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">âœ—</td>
<td id="S3.T2.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">default</td>
<td id="S3.T2.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">60.2</td>
</tr>
<tr id="S3.T2.2.1.3.2" class="ltx_tr">
<td id="S3.T2.2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">UniFormer</td>
<td id="S3.T2.2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">Train</td>
<td id="S3.T2.2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S3.T2.2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">default</td>
<td id="S3.T2.2.1.3.2.5" class="ltx_td ltx_align_center">60.8</td>
</tr>
<tr id="S3.T2.2.1.4.3" class="ltx_tr">
<td id="S3.T2.2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r">UniFormer</td>
<td id="S3.T2.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">Train</td>
<td id="S3.T2.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S3.T2.2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">optimal</td>
<td id="S3.T2.2.1.4.3.5" class="ltx_td ltx_align_center">61.4</td>
</tr>
<tr id="S3.T2.2.1.5.4" class="ltx_tr">
<td id="S3.T2.2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r">UniFormer</td>
<td id="S3.T2.2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">Train + Val</td>
<td id="S3.T2.2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S3.T2.2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">optimal</td>
<td id="S3.T2.2.1.5.4.5" class="ltx_td ltx_align_center">62.6</td>
</tr>
<tr id="S3.T2.2.1.6.5" class="ltx_tr">
<td id="S3.T2.2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CBNet v2</td>
<td id="S3.T2.2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Train</td>
<td id="S3.T2.2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T2.2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">default</td>
<td id="S3.T2.2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_t">61.7</td>
</tr>
<tr id="S3.T2.2.1.7.6" class="ltx_tr">
<td id="S3.T2.2.1.7.6.1" class="ltx_td ltx_align_center ltx_border_r">CBNet v2</td>
<td id="S3.T2.2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">Train</td>
<td id="S3.T2.2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S3.T2.2.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">default</td>
<td id="S3.T2.2.1.7.6.5" class="ltx_td ltx_align_center">62.1</td>
</tr>
<tr id="S3.T2.2.1.8.7" class="ltx_tr">
<td id="S3.T2.2.1.8.7.1" class="ltx_td ltx_align_center ltx_border_r">CBNet v2</td>
<td id="S3.T2.2.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">Train</td>
<td id="S3.T2.2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S3.T2.2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r">optimal</td>
<td id="S3.T2.2.1.8.7.5" class="ltx_td ltx_align_center">62.6</td>
</tr>
<tr id="S3.T2.2.1.9.8" class="ltx_tr">
<td id="S3.T2.2.1.9.8.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">CBNet v2</td>
<td id="S3.T2.2.1.9.8.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Train + Val</td>
<td id="S3.T2.2.1.9.8.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">âœ“</td>
<td id="S3.T2.2.1.9.8.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">optimal</td>
<td id="S3.T2.2.1.9.8.5" class="ltx_td ltx_align_center ltx_border_bb">64.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S3.T2.4.2" class="ltx_text" style="font-size:90%;">Results on the test dataset. TTA denotes we apply test-time augmentation during testing. Hyper-parm denotes the test hyper-parameters, default means standard setting for each method, optimal denotes the optimal test hyper-parameters on the validation dataset. All the models are trained with mask loss weight 1.0.</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Improved Test Strategies</h4>

<div id="S3.SS2.SSS0.Px4.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px4.p1.4" class="ltx_p">We evaluate different test strategies for UniFormer and CBNet v2 in TableÂ <a href="#S3.T2" title="Table 2 â€£ Improved Training Strategies â€£ 3.2 Experimental Results â€£ 3 Experiments â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. TTA denotes test-time augmentation and enhances <math id="S3.SS2.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="0.6\%" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px4.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.2.cmml">0.6</mn><mo id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.1.m1.1.1.2">0.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.1.m1.1c">0.6\%</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="0.4\%" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.2.m2.1a"><mrow id="S3.SS2.SSS0.Px4.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.cmml"><mn id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.cmml">0.4</mn><mo id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.1" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.2.m2.1.1.2">0.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.2.m2.1c">0.4\%</annotation></semantics></math> mAP for UniFormer and CBNet v2, respectively. We also search for different test hyper-parameters (such as the type and threshold of NMS) on CBNet v2, and apply them to other models. UniFormer and CBNet v2 achieve the mAP performance of <math id="S3.SS2.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="61.4\%" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.3.m3.1a"><mrow id="S3.SS2.SSS0.Px4.p1.3.m3.1.1" xref="S3.SS2.SSS0.Px4.p1.3.m3.1.1.cmml"><mn id="S3.SS2.SSS0.Px4.p1.3.m3.1.1.2" xref="S3.SS2.SSS0.Px4.p1.3.m3.1.1.2.cmml">61.4</mn><mo id="S3.SS2.SSS0.Px4.p1.3.m3.1.1.1" xref="S3.SS2.SSS0.Px4.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.3.m3.1b"><apply id="S3.SS2.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.3.m3.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.3.m3.1.1.2">61.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.3.m3.1c">61.4\%</annotation></semantics></math> and <math id="S3.SS2.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="62.6\%" display="inline"><semantics id="S3.SS2.SSS0.Px4.p1.4.m4.1a"><mrow id="S3.SS2.SSS0.Px4.p1.4.m4.1.1" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.cmml"><mn id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.2" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.2.cmml">62.6</mn><mo id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px4.p1.4.m4.1b"><apply id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px4.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS0.Px4.p1.4.m4.1.1.2">62.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px4.p1.4.m4.1c">62.6\%</annotation></semantics></math> on the test dataset with the best hyper-parameters, respectively.</p>
</div>
<figure id="S3.T3" class="ltx_table">
<div id="S3.T3.10" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:297.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(85.6pt,-58.7pt) scale(1.65181896308946,1.65181896308946) ;">
<table id="S3.T3.10.10" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T3.1.1.1" class="ltx_tr">
<th id="S3.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt">Resolution</th>
<th id="S3.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">MS-Train</th>
<th id="S3.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">CP</th>
<th id="S3.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><math id="S3.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\lambda_{mask}" display="inline"><semantics id="S3.T3.1.1.1.1.m1.1a"><msub id="S3.T3.1.1.1.1.m1.1.1" xref="S3.T3.1.1.1.1.m1.1.1.cmml"><mi id="S3.T3.1.1.1.1.m1.1.1.2" xref="S3.T3.1.1.1.1.m1.1.1.2.cmml">Î»</mi><mrow id="S3.T3.1.1.1.1.m1.1.1.3" xref="S3.T3.1.1.1.1.m1.1.1.3.cmml"><mi id="S3.T3.1.1.1.1.m1.1.1.3.2" xref="S3.T3.1.1.1.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.T3.1.1.1.1.m1.1.1.3.1" xref="S3.T3.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.T3.1.1.1.1.m1.1.1.3.3" xref="S3.T3.1.1.1.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.T3.1.1.1.1.m1.1.1.3.1a" xref="S3.T3.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.T3.1.1.1.1.m1.1.1.3.4" xref="S3.T3.1.1.1.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.T3.1.1.1.1.m1.1.1.3.1b" xref="S3.T3.1.1.1.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.T3.1.1.1.1.m1.1.1.3.5" xref="S3.T3.1.1.1.1.m1.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><apply id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.1.1.1.1.m1.1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T3.1.1.1.1.m1.1.1.2.cmml" xref="S3.T3.1.1.1.1.m1.1.1.2">ğœ†</ci><apply id="S3.T3.1.1.1.1.m1.1.1.3.cmml" xref="S3.T3.1.1.1.1.m1.1.1.3"><times id="S3.T3.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1.3.1"></times><ci id="S3.T3.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T3.1.1.1.1.m1.1.1.3.2">ğ‘š</ci><ci id="S3.T3.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T3.1.1.1.1.m1.1.1.3.3">ğ‘</ci><ci id="S3.T3.1.1.1.1.m1.1.1.3.4.cmml" xref="S3.T3.1.1.1.1.m1.1.1.3.4">ğ‘ </ci><ci id="S3.T3.1.1.1.1.m1.1.1.3.5.cmml" xref="S3.T3.1.1.1.1.m1.1.1.3.5">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\lambda_{mask}</annotation></semantics></math></th>
<th id="S3.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP (Val)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T3.2.2.2" class="ltx_tr">
<th id="S3.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt"><math id="S3.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="1333\times 800" display="inline"><semantics id="S3.T3.2.2.2.1.m1.1a"><mrow id="S3.T3.2.2.2.1.m1.1.1" xref="S3.T3.2.2.2.1.m1.1.1.cmml"><mn id="S3.T3.2.2.2.1.m1.1.1.2" xref="S3.T3.2.2.2.1.m1.1.1.2.cmml">1333</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.2.2.2.1.m1.1.1.1" xref="S3.T3.2.2.2.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.2.2.2.1.m1.1.1.3" xref="S3.T3.2.2.2.1.m1.1.1.3.cmml">800</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.1.m1.1b"><apply id="S3.T3.2.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.2.1.m1.1.1"><times id="S3.T3.2.2.2.1.m1.1.1.1.cmml" xref="S3.T3.2.2.2.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.2.2.2.1.m1.1.1.2.cmml" xref="S3.T3.2.2.2.1.m1.1.1.2">1333</cn><cn type="integer" id="S3.T3.2.2.2.1.m1.1.1.3.cmml" xref="S3.T3.2.2.2.1.m1.1.1.3">800</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.1.m1.1c">1333\times 800</annotation></semantics></math></th>
<td id="S3.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">âœ—</td>
<td id="S3.T3.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">None</td>
<td id="S3.T3.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">1.0</td>
<td id="S3.T3.2.2.2.5" class="ltx_td ltx_align_center ltx_border_tt">44.3</td>
</tr>
<tr id="S3.T3.3.3.3" class="ltx_tr">
<th id="S3.T3.3.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S3.T3.3.3.3.1.m1.1" class="ltx_Math" alttext="1333\times 800" display="inline"><semantics id="S3.T3.3.3.3.1.m1.1a"><mrow id="S3.T3.3.3.3.1.m1.1.1" xref="S3.T3.3.3.3.1.m1.1.1.cmml"><mn id="S3.T3.3.3.3.1.m1.1.1.2" xref="S3.T3.3.3.3.1.m1.1.1.2.cmml">1333</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.3.3.3.1.m1.1.1.1" xref="S3.T3.3.3.3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.3.3.3.1.m1.1.1.3" xref="S3.T3.3.3.3.1.m1.1.1.3.cmml">800</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.1.m1.1b"><apply id="S3.T3.3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.3.1.m1.1.1"><times id="S3.T3.3.3.3.1.m1.1.1.1.cmml" xref="S3.T3.3.3.3.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.3.3.3.1.m1.1.1.2.cmml" xref="S3.T3.3.3.3.1.m1.1.1.2">1333</cn><cn type="integer" id="S3.T3.3.3.3.1.m1.1.1.3.cmml" xref="S3.T3.3.3.3.1.m1.1.1.3">800</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.1.m1.1c">1333\times 800</annotation></semantics></math></th>
<td id="S3.T3.3.3.3.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S3.T3.3.3.3.3" class="ltx_td ltx_align_center ltx_border_r">None</td>
<td id="S3.T3.3.3.3.4" class="ltx_td ltx_align_center ltx_border_r">1.0</td>
<td id="S3.T3.3.3.3.5" class="ltx_td ltx_align_center">45.3</td>
</tr>
<tr id="S3.T3.4.4.4" class="ltx_tr">
<th id="S3.T3.4.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S3.T3.4.4.4.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.T3.4.4.4.1.m1.1a"><mrow id="S3.T3.4.4.4.1.m1.1.1" xref="S3.T3.4.4.4.1.m1.1.1.cmml"><mn id="S3.T3.4.4.4.1.m1.1.1.2" xref="S3.T3.4.4.4.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.4.4.4.1.m1.1.1.1" xref="S3.T3.4.4.4.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.4.4.4.1.m1.1.1.3" xref="S3.T3.4.4.4.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.4.1.m1.1b"><apply id="S3.T3.4.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.4.1.m1.1.1"><times id="S3.T3.4.4.4.1.m1.1.1.1.cmml" xref="S3.T3.4.4.4.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.4.4.4.1.m1.1.1.2.cmml" xref="S3.T3.4.4.4.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.T3.4.4.4.1.m1.1.1.3.cmml" xref="S3.T3.4.4.4.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.4.1.m1.1c">1920\times 1080</annotation></semantics></math></th>
<td id="S3.T3.4.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.4.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">None</td>
<td id="S3.T3.4.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.0</td>
<td id="S3.T3.4.4.4.5" class="ltx_td ltx_align_center ltx_border_t">50.3</td>
</tr>
<tr id="S3.T3.5.5.5" class="ltx_tr">
<th id="S3.T3.5.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S3.T3.5.5.5.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.T3.5.5.5.1.m1.1a"><mrow id="S3.T3.5.5.5.1.m1.1.1" xref="S3.T3.5.5.5.1.m1.1.1.cmml"><mn id="S3.T3.5.5.5.1.m1.1.1.2" xref="S3.T3.5.5.5.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.5.5.5.1.m1.1.1.1" xref="S3.T3.5.5.5.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.5.5.5.1.m1.1.1.3" xref="S3.T3.5.5.5.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.5.5.5.1.m1.1b"><apply id="S3.T3.5.5.5.1.m1.1.1.cmml" xref="S3.T3.5.5.5.1.m1.1.1"><times id="S3.T3.5.5.5.1.m1.1.1.1.cmml" xref="S3.T3.5.5.5.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.5.5.5.1.m1.1.1.2.cmml" xref="S3.T3.5.5.5.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.T3.5.5.5.1.m1.1.1.3.cmml" xref="S3.T3.5.5.5.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.5.5.5.1.m1.1c">1920\times 1080</annotation></semantics></math></th>
<td id="S3.T3.5.5.5.2" class="ltx_td ltx_align_center ltx_border_r">âœ“</td>
<td id="S3.T3.5.5.5.3" class="ltx_td ltx_align_center ltx_border_r">None</td>
<td id="S3.T3.5.5.5.4" class="ltx_td ltx_align_center ltx_border_r">1.0</td>
<td id="S3.T3.5.5.5.5" class="ltx_td ltx_align_center">50.3</td>
</tr>
<tr id="S3.T3.6.6.6" class="ltx_tr">
<th id="S3.T3.6.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><math id="S3.T3.6.6.6.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.T3.6.6.6.1.m1.1a"><mrow id="S3.T3.6.6.6.1.m1.1.1" xref="S3.T3.6.6.6.1.m1.1.1.cmml"><mn id="S3.T3.6.6.6.1.m1.1.1.2" xref="S3.T3.6.6.6.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.6.6.6.1.m1.1.1.1" xref="S3.T3.6.6.6.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.6.6.6.1.m1.1.1.3" xref="S3.T3.6.6.6.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.6.6.6.1.m1.1b"><apply id="S3.T3.6.6.6.1.m1.1.1.cmml" xref="S3.T3.6.6.6.1.m1.1.1"><times id="S3.T3.6.6.6.1.m1.1.1.1.cmml" xref="S3.T3.6.6.6.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.6.6.6.1.m1.1.1.2.cmml" xref="S3.T3.6.6.6.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.T3.6.6.6.1.m1.1.1.3.cmml" xref="S3.T3.6.6.6.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.6.6.6.1.m1.1c">1920\times 1080</annotation></semantics></math></th>
<td id="S3.T3.6.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.6.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Simple</td>
<td id="S3.T3.6.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.0</td>
<td id="S3.T3.6.6.6.5" class="ltx_td ltx_align_center ltx_border_t">50.9</td>
</tr>
<tr id="S3.T3.7.7.7" class="ltx_tr">
<th id="S3.T3.7.7.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S3.T3.7.7.7.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.T3.7.7.7.1.m1.1a"><mrow id="S3.T3.7.7.7.1.m1.1.1" xref="S3.T3.7.7.7.1.m1.1.1.cmml"><mn id="S3.T3.7.7.7.1.m1.1.1.2" xref="S3.T3.7.7.7.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.7.7.7.1.m1.1.1.1" xref="S3.T3.7.7.7.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.7.7.7.1.m1.1.1.3" xref="S3.T3.7.7.7.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.7.7.7.1.m1.1b"><apply id="S3.T3.7.7.7.1.m1.1.1.cmml" xref="S3.T3.7.7.7.1.m1.1.1"><times id="S3.T3.7.7.7.1.m1.1.1.1.cmml" xref="S3.T3.7.7.7.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.7.7.7.1.m1.1.1.2.cmml" xref="S3.T3.7.7.7.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.T3.7.7.7.1.m1.1.1.3.cmml" xref="S3.T3.7.7.7.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.7.7.1.m1.1c">1920\times 1080</annotation></semantics></math></th>
<td id="S3.T3.7.7.7.2" class="ltx_td ltx_align_center ltx_border_r">âœ—</td>
<td id="S3.T3.7.7.7.3" class="ltx_td ltx_align_center ltx_border_r">Simple</td>
<td id="S3.T3.7.7.7.4" class="ltx_td ltx_align_center ltx_border_r">1.2</td>
<td id="S3.T3.7.7.7.5" class="ltx_td ltx_align_center">51.4</td>
</tr>
<tr id="S3.T3.8.8.8" class="ltx_tr">
<th id="S3.T3.8.8.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S3.T3.8.8.8.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.T3.8.8.8.1.m1.1a"><mrow id="S3.T3.8.8.8.1.m1.1.1" xref="S3.T3.8.8.8.1.m1.1.1.cmml"><mn id="S3.T3.8.8.8.1.m1.1.1.2" xref="S3.T3.8.8.8.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.8.8.8.1.m1.1.1.1" xref="S3.T3.8.8.8.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.8.8.8.1.m1.1.1.3" xref="S3.T3.8.8.8.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.8.8.8.1.m1.1b"><apply id="S3.T3.8.8.8.1.m1.1.1.cmml" xref="S3.T3.8.8.8.1.m1.1.1"><times id="S3.T3.8.8.8.1.m1.1.1.1.cmml" xref="S3.T3.8.8.8.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.8.8.8.1.m1.1.1.2.cmml" xref="S3.T3.8.8.8.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.T3.8.8.8.1.m1.1.1.3.cmml" xref="S3.T3.8.8.8.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.8.8.1.m1.1c">1920\times 1080</annotation></semantics></math></th>
<td id="S3.T3.8.8.8.2" class="ltx_td ltx_align_center ltx_border_r">âœ—</td>
<td id="S3.T3.8.8.8.3" class="ltx_td ltx_align_center ltx_border_r">Simple</td>
<td id="S3.T3.8.8.8.4" class="ltx_td ltx_align_center ltx_border_r">1.6</td>
<td id="S3.T3.8.8.8.5" class="ltx_td ltx_align_center">51.0</td>
</tr>
<tr id="S3.T3.9.9.9" class="ltx_tr">
<th id="S3.T3.9.9.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><math id="S3.T3.9.9.9.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.T3.9.9.9.1.m1.1a"><mrow id="S3.T3.9.9.9.1.m1.1.1" xref="S3.T3.9.9.9.1.m1.1.1.cmml"><mn id="S3.T3.9.9.9.1.m1.1.1.2" xref="S3.T3.9.9.9.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.9.9.9.1.m1.1.1.1" xref="S3.T3.9.9.9.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.9.9.9.1.m1.1.1.3" xref="S3.T3.9.9.9.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.9.9.9.1.m1.1b"><apply id="S3.T3.9.9.9.1.m1.1.1.cmml" xref="S3.T3.9.9.9.1.m1.1.1"><times id="S3.T3.9.9.9.1.m1.1.1.1.cmml" xref="S3.T3.9.9.9.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.9.9.9.1.m1.1.1.2.cmml" xref="S3.T3.9.9.9.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.T3.9.9.9.1.m1.1.1.3.cmml" xref="S3.T3.9.9.9.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.9.9.9.1.m1.1c">1920\times 1080</annotation></semantics></math></th>
<td id="S3.T3.9.9.9.2" class="ltx_td ltx_align_center ltx_border_r">âœ—</td>
<td id="S3.T3.9.9.9.3" class="ltx_td ltx_align_center ltx_border_r">Simple</td>
<td id="S3.T3.9.9.9.4" class="ltx_td ltx_align_center ltx_border_r">2.0</td>
<td id="S3.T3.9.9.9.5" class="ltx_td ltx_align_center">51.4</td>
</tr>
<tr id="S3.T3.10.10.10" class="ltx_tr">
<th id="S3.T3.10.10.10.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t"><math id="S3.T3.10.10.10.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="S3.T3.10.10.10.1.m1.1a"><mrow id="S3.T3.10.10.10.1.m1.1.1" xref="S3.T3.10.10.10.1.m1.1.1.cmml"><mn id="S3.T3.10.10.10.1.m1.1.1.2" xref="S3.T3.10.10.10.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="S3.T3.10.10.10.1.m1.1.1.1" xref="S3.T3.10.10.10.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.T3.10.10.10.1.m1.1.1.3" xref="S3.T3.10.10.10.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.T3.10.10.10.1.m1.1b"><apply id="S3.T3.10.10.10.1.m1.1.1.cmml" xref="S3.T3.10.10.10.1.m1.1.1"><times id="S3.T3.10.10.10.1.m1.1.1.1.cmml" xref="S3.T3.10.10.10.1.m1.1.1.1"></times><cn type="integer" id="S3.T3.10.10.10.1.m1.1.1.2.cmml" xref="S3.T3.10.10.10.1.m1.1.1.2">1920</cn><cn type="integer" id="S3.T3.10.10.10.1.m1.1.1.3.cmml" xref="S3.T3.10.10.10.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.10.10.10.1.m1.1c">1920\times 1080</annotation></semantics></math></th>
<td id="S3.T3.10.10.10.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">âœ—</td>
<td id="S3.T3.10.10.10.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Balance</td>
<td id="S3.T3.10.10.10.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">2.0</td>
<td id="S3.T3.10.10.10.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">52.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T3.14.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S3.T3.12.1" class="ltx_text" style="font-size:90%;">Results with different training strategies on the validation dataset. All the experiments are evaluated with Mask RCNN (R50) training with standard 1x strategy, and validation is not used during training. MS-Train denotes multi-scale training, CP denotes copy-paste, and <math id="S3.T3.12.1.m1.1" class="ltx_Math" alttext="\lambda_{mask}" display="inline"><semantics id="S3.T3.12.1.m1.1b"><msub id="S3.T3.12.1.m1.1.1" xref="S3.T3.12.1.m1.1.1.cmml"><mi id="S3.T3.12.1.m1.1.1.2" xref="S3.T3.12.1.m1.1.1.2.cmml">Î»</mi><mrow id="S3.T3.12.1.m1.1.1.3" xref="S3.T3.12.1.m1.1.1.3.cmml"><mi id="S3.T3.12.1.m1.1.1.3.2" xref="S3.T3.12.1.m1.1.1.3.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S3.T3.12.1.m1.1.1.3.1" xref="S3.T3.12.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.T3.12.1.m1.1.1.3.3" xref="S3.T3.12.1.m1.1.1.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S3.T3.12.1.m1.1.1.3.1b" xref="S3.T3.12.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.T3.12.1.m1.1.1.3.4" xref="S3.T3.12.1.m1.1.1.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S3.T3.12.1.m1.1.1.3.1c" xref="S3.T3.12.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S3.T3.12.1.m1.1.1.3.5" xref="S3.T3.12.1.m1.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T3.12.1.m1.1c"><apply id="S3.T3.12.1.m1.1.1.cmml" xref="S3.T3.12.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.12.1.m1.1.1.1.cmml" xref="S3.T3.12.1.m1.1.1">subscript</csymbol><ci id="S3.T3.12.1.m1.1.1.2.cmml" xref="S3.T3.12.1.m1.1.1.2">ğœ†</ci><apply id="S3.T3.12.1.m1.1.1.3.cmml" xref="S3.T3.12.1.m1.1.1.3"><times id="S3.T3.12.1.m1.1.1.3.1.cmml" xref="S3.T3.12.1.m1.1.1.3.1"></times><ci id="S3.T3.12.1.m1.1.1.3.2.cmml" xref="S3.T3.12.1.m1.1.1.3.2">ğ‘š</ci><ci id="S3.T3.12.1.m1.1.1.3.3.cmml" xref="S3.T3.12.1.m1.1.1.3.3">ğ‘</ci><ci id="S3.T3.12.1.m1.1.1.3.4.cmml" xref="S3.T3.12.1.m1.1.1.3.4">ğ‘ </ci><ci id="S3.T3.12.1.m1.1.1.3.5.cmml" xref="S3.T3.12.1.m1.1.1.3.5">ğ‘˜</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.12.1.m1.1d">\lambda_{mask}</annotation></semantics></math> is the weight of mask loss.</span></figcaption>
</figure>
</section>
<section id="S3.SS2.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Model Ensemble</h4>

<div id="S3.SS2.SSS0.Px5.p1" class="ltx_para">
<p id="S3.SS2.SSS0.Px5.p1.1" class="ltx_p">We combine the mask results of all models after post-processing and obtain the final result by mask NMS, which can be seen as a parallel model ensemble. The ensemble result with Gaussian kernel is <math id="S3.SS2.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="0.9\%" display="inline"><semantics id="S3.SS2.SSS0.Px5.p1.1.m1.1a"><mrow id="S3.SS2.SSS0.Px5.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.cmml"><mn id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.2.cmml">0.9</mn><mo id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.1" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px5.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1"><csymbol cd="latexml" id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS2.SSS0.Px5.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px5.p1.1.m1.1.1.2">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px5.p1.1.m1.1c">0.9\%</annotation></semantics></math> MAP higher than the best single model, as illustrated in TableÂ <a href="#S3.T4" title="Table 4 â€£ Model Ensemble â€£ 3.2 Experimental Results â€£ 3 Experiments â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. Finally, we ensemble the best models and obtain the mask mAP of 65.2% on the test dataset, as shown in TableÂ <a href="#S3.T5" title="Table 5 â€£ Model Ensemble â€£ 3.2 Experimental Results â€£ 3 Experiments â€£ Parallel Pre-trained Transformers (PPT) for Synthetic Data-based Instance Segmentation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="S3.T4" class="ltx_table">
<div id="S3.T4.2" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:151.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(62.1pt,-21.7pt) scale(1.40109455859367,1.40109455859367) ;">
<table id="S3.T4.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T4.2.1.1.1" class="ltx_tr">
<th id="S3.T4.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Method</th>
<th id="S3.T4.2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Backbone</th>
<th id="S3.T4.2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Pretrain</th>
<th id="S3.T4.2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt">Data</th>
<th id="S3.T4.2.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">mAP (Val)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T4.2.1.2.1" class="ltx_tr">
<td id="S3.T4.2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">UniFormer</td>
<td id="S3.T4.2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Swin-B</td>
<td id="S3.T4.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">COCO</td>
<td id="S3.T4.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Train</td>
<td id="S3.T4.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">60.0</td>
</tr>
<tr id="S3.T4.2.1.3.2" class="ltx_tr">
<td id="S3.T4.2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">CBNet v2</td>
<td id="S3.T4.2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">Swin-L</td>
<td id="S3.T4.2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">COCO</td>
<td id="S3.T4.2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">Train</td>
<td id="S3.T4.2.1.3.2.5" class="ltx_td ltx_align_center">61.8</td>
</tr>
<tr id="S3.T4.2.1.4.3" class="ltx_tr">
<td id="S3.T4.2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r">CBNet v2</td>
<td id="S3.T4.2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">Swin-B</td>
<td id="S3.T4.2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">BigDet</td>
<td id="S3.T4.2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">Train</td>
<td id="S3.T4.2.1.4.3.5" class="ltx_td ltx_align_center">61.6</td>
</tr>
<tr id="S3.T4.2.1.5.4" class="ltx_tr">
<td id="S3.T4.2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Ensemble (linear)</td>
<td id="S3.T4.2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S3.T4.2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">-</td>
<td id="S3.T4.2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Train</td>
<td id="S3.T4.2.1.5.4.5" class="ltx_td ltx_align_center ltx_border_tt">62.6</td>
</tr>
<tr id="S3.T4.2.1.6.5" class="ltx_tr">
<td id="S3.T4.2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Ensemble (Gaussian)</td>
<td id="S3.T4.2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S3.T4.2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">-</td>
<td id="S3.T4.2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">Train</td>
<td id="S3.T4.2.1.6.5.5" class="ltx_td ltx_align_center ltx_border_bb">62.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S3.T4.4.2" class="ltx_text" style="font-size:90%;">Ensemble ablation study on the validation dataset. In order to make the ensemble more reasonable and effective, the models here only use the training dataset. We use the optimal ensemble config for the final ensemble on the test dataset.</span></figcaption>
</figure>
<figure id="S3.T5" class="ltx_table">
<div id="S3.T5.2" class="ltx_inline-block ltx_transformed_outer" style="width:433.6pt;height:134.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(71.4pt,-22.1pt) scale(1.49138190768718,1.49138190768718) ;">
<table id="S3.T5.2.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T5.2.1.1.1" class="ltx_tr">
<td id="S3.T5.2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Method</td>
<td id="S3.T5.2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Backbone</td>
<td id="S3.T5.2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Pretrain</td>
<td id="S3.T5.2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">Data</td>
<td id="S3.T5.2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt">mAP (Test)</td>
</tr>
<tr id="S3.T5.2.1.2.2" class="ltx_tr">
<td id="S3.T5.2.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">UniFormer</td>
<td id="S3.T5.2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Swin-B</td>
<td id="S3.T5.2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">COCO</td>
<td id="S3.T5.2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Train+Val</td>
<td id="S3.T5.2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">63.3</td>
</tr>
<tr id="S3.T5.2.1.3.3" class="ltx_tr">
<td id="S3.T5.2.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">CBNet v2</td>
<td id="S3.T5.2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Swin-L</td>
<td id="S3.T5.2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">COCO</td>
<td id="S3.T5.2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Train+Val</td>
<td id="S3.T5.2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">64.7</td>
</tr>
<tr id="S3.T5.2.1.4.4" class="ltx_tr">
<td id="S3.T5.2.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r">CBNet v2</td>
<td id="S3.T5.2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">Swin-B</td>
<td id="S3.T5.2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">BigDet</td>
<td id="S3.T5.2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">Train+Val</td>
<td id="S3.T5.2.1.4.4.5" class="ltx_td ltx_align_center">64.2</td>
</tr>
<tr id="S3.T5.2.1.5.5" class="ltx_tr">
<td id="S3.T5.2.1.5.5.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Ensemble</td>
<td id="S3.T5.2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">-</td>
<td id="S3.T5.2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">-</td>
<td id="S3.T5.2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t">Train+Val</td>
<td id="S3.T5.2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">65.2</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S3.T5.3.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S3.T5.4.2" class="ltx_text" style="font-size:90%;">Ensemble results with Gaussian kernel on the test dataset. CBNet v2 with backbone Swin-L is pre-trained on COCO and CBNet v2 with backbone Swin-B is pretarined on BigDet. All the results are evaluated with TTA and optimal searched test config.</span></figcaption>
</figure>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we propose a <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">Parallel Pre-trained Transformers (PPT)</span> framework to accomplish the synthetic data-based Instance Segmentation task.
Swin-B-based CBNet V2, Swin-L-based CBNet V2 and Swin-L-based Uniformer are employed for parallel feature learning, and the results of these four models are fused by mask NMS to obtain more robust results.
Experimental results on â€œCVPR2022 AVA Accessibility Vision and Autonomy Challengeâ€ demonstrate that PPT rank first in this competition.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Likun Cai, Zhi Zhang, Yi Zhu, Li Zhang, Mu Li, and Xiangyang Xue.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Bigdetection: A large-scale benchmark for improved object detector
pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2203.13249</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun,
Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Hybrid task cascade for instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Class-balanced loss based on effective number of samples.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, EkinÂ D Cubuk,
QuocÂ V Le, and Barret Zoph.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Simple copy-paste is a strong data augmentation method for instance
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Agrim Gupta, Piotr Dollar, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Lvis: A dataset for large vocabulary instance segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr DollÃ¡r, and Ross
Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Masked autoencoders are scalable vision learners.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2111.06377</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Momentum contrast for unsupervised visual representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr DollÃ¡r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi
Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander
Kolesnikov, etÂ al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">The open images dataset v4.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IJCV</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Kunchang Li, Yali Wang, Junhao Zhang, Peng Gao, Guanglu Song, Yu Liu, Hongsheng
Li, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Uniformer: Unifying convolution and self-attention for visual
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2201.09450</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Tingting Liang, Xiaojie Chu, Yudong Liu, Yongtao Wang, Zhi Tang, Wei Chu,
Jingdong Chen, and Haibin Ling.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Cbnetv2: A composite backbone network architecture for object
detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.00420</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr DollÃ¡r, and CÂ Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
Baining Guo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Swin transformer: Hierarchical vision transformer using shifted
windows.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Detectors: Detecting objects with recursive feature pyramid and
switchable atrous convolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing
Li, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Objects365: A large-scale, high-quality dataset for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Solo: Segmenting objects by locations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ECCV</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Jimuyang Zhang, Minglan Zheng, Matthew Boyd, and Eshed Ohn-Bar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">X-world: Accessibility, vision, and autonomy meet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">ICCV</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2206.10844" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2206.10845" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2206.10845">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2206.10845" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2206.10846" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 16:28:05 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
