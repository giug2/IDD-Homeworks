<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Instruction-tuned Large Language Models for Machine Translation in the Medical Domain</title>
<!--Generated on Thu Aug 29 11:03:54 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.16440v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S1" title="In Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S2" title="In Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S3" title="In Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S3.SS1" title="In 3 Experimental Setup ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S3.SS1.SSS0.Px1" title="In 3.1 Data ‣ 3 Experimental Setup ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title">Terminology Annotation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S3.SS2" title="In 3 Experimental Setup ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S4" title="In Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S5" title="In Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S6" title="In Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#A1" title="In Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Instruction Templates</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#A2" title="In Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>QLoRA Fine-tuning Setup</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
Instruction-tuned Large Language Models for Machine Translation in the Medical Domain
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Miguel Rios 
<br class="ltx_break"/>Centre for Translation Studies, University of Vienna 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">miguel.angel.rios.gaona@univie.ac.at</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs significantly outperform the baseline models with automatic metrics.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Current state-of-the-art Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib5" title="">5</a>]</cite>. However, in low-resource domains (e.g. medical, or engineering manuals) LLMs have shown lower performance compared to standard neural machine translation (NMT) models <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib21" title="">21</a>]</cite>. The accuracy and consistency in the machine translation of terminology, syntax, and document structure is crucial for users, researchers, and translators that post-edit machine translated documents in high-risk domains <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib18" title="">18</a>]</cite>. Moreover, the introduction of in-domain translation constraints during generation into neural models is currently an open problem <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">LLMs are trained to perform different Natural Language Processing (NLP) tasks such as summarisation, question answering, and translation, where the users interact with the models via instructions (i.e. chat) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib9" title="">9</a>]</cite>. Instruction-tuning is a technique to fine-tune LLMs with datasets form different tasks formatted as instructions to improve generalisation to unseen tasks or domains <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib7" title="">7</a>]</cite>, for example, "<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Glossary: medicine -&gt; medicamento Translate the source text from English to Spanish following the provided translation glossaries. English: The medicine was effective in patients with all three types of homocystinuria. Spanish: </span>"</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Moreover, <cite class="ltx_cite ltx_citemacro_citet">Alves et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib4" title="">4</a>]</cite> instruction-tuned LLaMA-2 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib24" title="">24</a>]</cite> to perform translation related tasks, such as, segment and document level translation, post-editing, and error annotation. The controlled generation of MT output with the correct medical terms, segment length, or syntax can be framed as an instruction-tuning task for LLMs. Thus, improving the workflow of translators during post-editing with an instruction-following (i.e. chat) interface to a LLM for the medical domain.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our goal is to incorporate terminology, syntax information, and document structure constraints into a LLM for the medical domain. In this paper, we show preliminary results for adding specialised medical dictionaries into LLMs. In particular, we follow the methodology from <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib4" title="">4</a>]</cite> by adding terminology information into the instruction-tuning datasets. We use parameter-efficient fine-tuning (PEFT) for LLMs with in-domain translation datasets, where the prompts include instructions to follow the translation of term pairs from medical dictionaries matched with parallel data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">We evaluate FLAN-T5 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib7" title="">7</a>]</cite>, LLaMA3 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib9" title="">9</a>]</cite>, and Tower <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib4" title="">4</a>]</cite> for English-Spanish, English-German, and English-Romanian language pairs on a validation split of a medical domain dataset, and we show the optimal hyper-parameter setup for instruction-tuning with LLMs.
The instruction-tuned models outperformed the baseline models with the automatic metrics BLEU <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib19" title="">19</a>]</cite>, chrF <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib20" title="">20</a>]</cite>, and COMET <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Auto-regressive language models predict the next token in a sequence given a prefix context <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib6" title="">6</a>]</cite>, where they are pre-trained with large amounts of texts followed by fine-tuning on different downstream tasks <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib17" title="">17</a>]</cite>. In addition, <cite class="ltx_cite ltx_citemacro_citet">Chung et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib7" title="">7</a>]</cite> propose to fine-tune LLMs with a mixture of several NLP datasets into a instruction format to improve: generalisation to unseen tasks, and generation given instruction prompts. For machine translation the LLM is conditioned on a user defined prompt that consists of a translation instruction along with the source text to be translated <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib18" title="">18</a>]</cite>. During testing, zero-shot prompting involves the querying of a LLM with an test input that was not present in the training data. On the other hand, few-shot prompting provides few examples of the task along with the test input to guide the LLM generation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Supervised fine-tuning (SFT) is the one of the most popular techniques for domain adaptation in LLMs, where models continue their training with a sample of in-domain data <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib10" title="">10</a>]</cite>. However, SFT for LLMs requires large amounts of computational resources, given that during training models update billions of parameters. The goal of Parameter-efficient fine-tuning (PEFT) is to update (i.e. tune) a minimal set parameters to achieve a similar performance compared to full SFT on downstream tasks. <cite class="ltx_cite ltx_citemacro_citet">Hu et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib14" title="">14</a>]</cite> propose low-Rank adaptation (LoRA) that freezes all the pre-trained model parameters and adds adapter trainable low-rank decomposition matrices of parameters into each layer of the model. Moreover, <cite class="ltx_cite ltx_citemacro_citet">Dettmers et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib8" title="">8</a>]</cite> propose that during fine-tuning to quantise the parameters of the pre-trained model into fewer bits (e.g. 4-bit) and keep the LoRA adapters with standard precision, thus reducing the memory usage. PEFT and quantisation with QLoRA enables translation practitioners to fine-tune LLM with limited computing resources.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">LLaMA versions 2 and 3 <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib9" title="">9</a>]</cite> are family of open-source LLMs with different parameter scales, which are instruction-tuned for multilingual tasks. Moreover, LLaMA models have become the base for MT related work <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We use the corpus of the European Medicines Agency (EMEA) <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib1" title="">1</a>]</cite> for the English-Spanish (en-es), English-German (en-de), and English-Romanian (en-ro) language pairs. We split the EMEA corpus randomly with 20K tuning segments for each language pair and combined them into one dataset (60K segments), 500 validation segments for each language pair, and 500 test segments for each language pair.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Terminology Annotation</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">The Interactive Terminology for Europe (IATE)<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://iate.europa.eu/download-iate" title="">https://iate.europa.eu/download-iate</a></span></span></span> is a terminology management system from EU institutions that covers different domains (e.g. economics, law, health). For our source and target language pairs, we downloaded the IATE database under the <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.1">health</span> domain (<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.2">id 2841</span>). We only use terms with quality 3 (reliable) and 4 (very reliable) starts (human annotated quality scores) resulting in 38,898 terms for en-es, 49,828 terms for en-de, and 9551 terms for en-ro.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p2.1">We add medical terms as translation instructions by matching term pairs to each aligned segment. For each aligned segment we retrieve candidate terms based on a strict string matching, where a candidate pair must be present on both the source and target segments. If one or more candidates are matched in a segment the instruction template includes the candidates in the prompt. For example in en-es "<span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p2.1.1">spectrum of activity -&gt; espectro de actividad, amoxicillin -&gt; amoxicilina, and activity -&gt; actividad</span>" are term pairs matched on the parallel segment:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p3">
<div class="ltx_listing ltx_lstlisting ltx_listing" id="S3.SS1.SSS0.Px1.p3.1" style="background-color:#BFBFBF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,IEdsb3NzYXJpZXM6CiAic3BlY3RydW0gb2YgYWN0aXZpdHkiIC0+ICJlc3BlY3RybyBkZSBhY3RpdmlkYWQiCiAiYW1veGljaWxsaW4iIC0+ICJhbW94aWNpbGluYSIKICJhY3Rpdml0eSIgLT4gImFjdGl2aWRhZCIKIFRyYW5zbGF0ZSB0aGUgc291cmNlIHRleHQgZnJvbSBFbmdsaXNoIHRvIFNwYW5pc2ggZm9sbG93aW5nIHRoZSBwcm92aWRlZCB0cmFuc2xhdGlvbiBnbG9zc2FyaWVzLgogRW5nbGlzaDogQW1veGljaWxsaW4gaXMgc3VzY2VwdGlibGUgdG8gZGVncmFkYXRpb24gYnkgYmV0YS1sYWN0YW1hc2VzIHByb2R1Y2VkIGJ5IHJlc2lzdGFudCBiYWN0ZXJpYSBhbmQgdGhlcmVmb3JlIHRoZSBzcGVjdHJ1bSBvZiBhY3Rpdml0eSBvZiBhbW94aWNpbGxpbiBhbG9uZSBkb2VzIG5vdCBpbmNsdWRlIG9yZ2FuaXNtcyB3aGljaCBwcm9kdWNlIHRoZXNlIGVuenltZXMuCiBTcGFuaXNoOiBMYSBhbW94aWNpbGluYSBlcyBzZW5zaWJsZSBhIGxhIGRlZ3JhZGFjacOzbiBwb3IgbGFzIGJldGEtbGFjdGFtYXNhcyBwcm9kdWNpZGFzIHBvciBiYWN0ZXJpYXMgcmVzaXN0ZW50ZXMgeSBwb3IgdGFudG8gZWwgZXNwZWN0cm8gZGUgYWN0aXZpZGFkIGRlIGxhIGFtb3hpY2lsaW5hIHNvbGEgbm8gaW5jbHV5ZSBtaWNyb29yZ2FuaXNtb3MgcHJvZHVjdG9yZXMgZGUgZXN0YXMgZW56aW1hcy4=">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.1"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.2">Glossaries</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.3">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.1"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.2">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.3">spectrum</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.5">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.7">activity</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.8">"</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.9"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.10">-&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.11"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.12">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.13">espectro</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.15">de</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.17">actividad</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.18">"</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.1"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.2">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.3">amoxicillin</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.4">"</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.5"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.6">-&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.7"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.8">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.9">amoxicilina</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.10">"</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.1"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.2">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.3">activity</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.4">"</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.5"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.6">-&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.7"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.8">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.9">actividad</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.10">"</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.1"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.2">Translate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.4">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.6">source</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.8">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.10">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.12">English</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.14">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.16">Spanish</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.18">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.20">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.22">provided</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.24">translation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.26">glossaries</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.27">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.1"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.2">English</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.3">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.5">Amoxicillin</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.7">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.9">susceptible</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.11">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.13">degradation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.15">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.17">beta</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.18">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.19">lactamases</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.21">produced</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.23">by</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.25">resistant</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.27">bacteria</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.29">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.31">therefore</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.33">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.35">spectrum</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.36"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.37">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.38"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.39">activity</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.40"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.41">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.43">amoxicillin</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.45">alone</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.47">does</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.49">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.51">include</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.52"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.53">organisms</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.54"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.55">which</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.56"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.57">produce</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.58"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.59">these</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.60"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.61">enzymes</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.62">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.1"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.2">Spanish</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.3">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.5">La</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.7">amoxicilina</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.9">es</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.11">sensible</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.13">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.15">la</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.17">degradaci</span><span class="ltx_text ltx_lst_literate ltx_font_typewriter" id="lstnumberx7.18">ó</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.19">n</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.21">por</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.23">las</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.25">beta</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.26">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.27">lactamasas</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.29">producidas</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.31">por</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.33">bacterias</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.35">resistentes</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.36"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.37">y</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.38"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.39">por</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.40"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.41">tanto</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.43">el</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.45">espectro</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.47">de</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.49">actividad</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.51">de</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.52"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.53">la</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.54"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.55">amoxicilina</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.56"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.57">sola</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.58"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.59">no</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.60"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.61">incluye</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.62"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.63">microorganismos</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.64"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.65">productores</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.66"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.67">de</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.68"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.69">estas</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.70"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.71">enzimas</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.72">.</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p4">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p4.1">If no candidates are matched in a segment the instruction only includes the translation task prompt. For example in en-es:</p>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="S3.SS1.SSS0.Px1.p4.2" style="background-color:#BFBFBF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,VHJhbnNsYXRlIHRoZSBzb3VyY2UgdGV4dCBmcm9tIEVuZ2xpc2ggdG8gU3BhbmlzaC4KRW5nbGlzaDogRG8gbm90IHVzZSBDeW1ldmVuZSBpZiB5b3UgYXJlIGJyZWFzdC1mZWVkaW5nLgpTcGFuaXNoOiBObyB1c2UgQ3ltZXZlbmUgc2kgZXN0w6EgZW4gcGVyaW9kbyBkZSBsYWN0YW5jaWEu">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.1">Translate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.5">source</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.7">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.9">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.11">English</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.13">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.15">Spanish</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.16">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.1">English</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.4">Do</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.6">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.8">use</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.10">Cymevene</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.12">if</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.14">you</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.16">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.18">breast</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.19">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.20">feeding</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.21">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.1">Spanish</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.2">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.4">No</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.6">use</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.8">Cymevene</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.10">si</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.12">est</span><span class="ltx_text ltx_lst_literate ltx_font_typewriter" id="lstnumberx10.13">á</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.15">en</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.17">periodo</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.19">de</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.21">lactancia</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.22">.</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p5">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p5.1">The prompt templates for the baseline models are defined in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#A1" title="Appendix A Instruction Templates ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_tag">A</span></a>, and we use zero-shot prompting for evaluation.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Models</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We use the HuggingFace transformers framework for the baseline LLMs <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib25" title="">25</a>]</cite>, and PEFT <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib16" title="">16</a>]</cite> for the instruction-tuning with QLoRA . Our LLM baselines are: FLAN-T5<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="google/flan-t5-large" title="">google/flan-t5-large</a></span></span></span> encoder-decoder model (large version with 783M parameters), Llama-3-8B<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="meta-llama/Meta-Llama-3.1-8B-Instruct" title="">meta-llama/Meta-Llama-3.1-8B-Instruct</a></span></span></span> instruction-tuned LLM for downstream tasks, and Tower-7B<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="Unbabel/TowerInstruct-7B-v0.2" title="">Unbabel/TowerInstruct-7B-v0.2</a></span></span></span> LLM instruction-tuned for MT tasks. We test two different types of instruction-tuned models an encoder-decoder model based on FLAN-T5, and an auto-regressive LLMs based on LLaMA (i.e. base model for Tower).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.2">We use QLoRA with a 4-bit quantisation to fine-tune each baseline model for one epoch on the tuning dataset (60K segments). The values for the QLoRA and tuning hyper-parameters for each model are defined in Section <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#A2" title="Appendix B QLoRA Fine-tuning Setup ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_tag">B</span></a>. Finally, we use stochastic decoding with top-<math alttext="p" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_p</annotation></semantics></math> sampling <math alttext="p=0.9" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">p</mi><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝑝</ci><cn id="S3.SS2.p2.2.m2.1.1.3.cmml" type="float" xref="S3.SS2.p2.2.m2.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">p=0.9</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_p = 0.9</annotation></semantics></math>. For the experiments we use a Tesla T4 GPU (16GB) with an approximate time of 20 hours per model.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluate all the models with BLEU, chrF, and COMET on the validation split. Table <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#S4.T1" title="Table 1 ‣ 4 Results ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_tag">1</span></a> shows the comparison between the baselines and the instruction-tuned models with QLoRA. The BLEU scores for the instruction-tuned models are statistically significant (<math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">p</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><lt id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></lt><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝑝</ci><cn id="S4.p1.1.m1.1.1.3.cmml" type="float" xref="S4.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) on all models.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.3" style="width:433.6pt;height:127.1pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-30.7pt,8.9pt) scale(0.876096997206377,0.876096997206377) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.3.3.3.4">Model</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3.5"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1">en-es<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3.6"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3.7"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.2.2.2">en-de<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3.8"></th>
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3.9"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.3.3.3.3">en-ro<math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<td class="ltx_td ltx_border_tt" id="S4.T1.3.3.3.10"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.4.1">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T1.3.3.4.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.2">BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.3">chrF</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.4">COMET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.5">BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.6">chrF</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.7">COMET</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.8">BLEU</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.9">chrF</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.3.3.4.1.10">COMET</th>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.3.3.5.2.1">FLAN-T5</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.2">28.51</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.3">57.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.4">0.73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.5">14.76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.6">43.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.7">0.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.8">17.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.9">45.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.3.3.5.2.10">0.64</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.3.6.3.1">QLoRA FLAN-T5</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.2">36.43</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.3">63.40</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.4">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.5">25.45</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.6">54.93</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.7">0.72</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.8">28.65</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.9">57.44</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.6.3.10">0.77</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.3.7.4.1">
<span class="ltx_ERROR undefined" id="S4.T1.3.3.7.4.1.1">\hdashline</span>LLaMA-3-8B</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.2">34.07</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.3">63.02</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.4">0.79</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.5">25.44</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.6">58.08</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.7">0.78</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.8">24.99</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.9">53.17</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.7.4.10">0.76</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.3.8.5.1">QLoRA LLaMA-3-8B</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.2">45.07</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.3">67.74</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.4">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.5">36.30</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.6">62.21</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.7">0.84</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.8"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.8.5.8.1">35.97</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.9"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.8.5.9.1">61.19</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.8.5.10"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.8.5.10.1">0.85</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.9.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.3.3.9.6.1">
<span class="ltx_ERROR undefined" id="S4.T1.3.3.9.6.1.1">\hdashline</span>Tower-7B</th>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.2">42.27</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.3">66.31</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.4">0.86</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.5">34.80</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.6">62.45</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.7">0.85</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.8">18.20</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.9">44.86</td>
<td class="ltx_td ltx_align_center" id="S4.T1.3.3.9.6.10">0.69</td>
</tr>
<tr class="ltx_tr" id="S4.T1.3.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.3.3.10.7.1">QLoRA Tower-7B</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.2"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.10.7.2.1">48.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.3"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.10.7.3.1">70.36</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.4"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.10.7.4.1">0.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.5"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.10.7.5.1">42.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.6"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.10.7.6.1">67.62</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.7"><span class="ltx_text ltx_font_bold" id="S4.T1.3.3.10.7.7.1">0.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.8">23.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.9">50.57</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.3.3.10.7.10">0.78</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparing the baseline and QLoRA fine-tuned LLMs with automatic metrics for the en-es, en-de, and en-ro language pairs. </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">To prevent over generation and improve the performance of LLaMA-3, we post-processed the output by cutting it at the first appearance of the end-of-sequence token <span class="ltx_text ltx_font_italic" id="S4.p2.1.1">&lt;|eot_id|&gt;</span>. As noted by <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib26" title="">26</a>]</cite> the LLaMA models repeat the translation output or produce dialog suggestions to improve prompts along with the translation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Tower and QLoRA Tower outperform the other models in en-es, and en-de. However, Romanian (en-ro) is not present on the original Tower fine-tuning for MT. Tower is based on LLaMA-2 which is not focused in multilingual data, in contrast to LLaMA-3.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib26" title="">26</a>]</cite> compared 15 baseline LLMs and fine-tuned with QLoRA on different MT tasks (e.g. segment and document level translation) for the French-English language pair. LLaMA-2 outperformed other LLMs, fine-tuning improve performance on models that struggle on a few-shot setup, and QLoRA is potentially superior to full SFT in terms of efficiency. <cite class="ltx_cite ltx_citemacro_citet">Alves et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib3" title="">3</a>]</cite> compared instruction-tuning with LoRA to few-shot prompting for different language pairs LLaMA-2. Fine-tuning outperforms the few-shot, it is comparable to full SFT, it requires few training data, and it tackles over generation. However, LLMs struggle with translation directions out of English (English-xx). <cite class="ltx_cite ltx_citemacro_citet">Alves et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib4" title="">4</a>]</cite> proposed Tower with an specialisation on translation related tasks, for example, document level translation, post-editing, and terminology aware prompts. Tower is based on the continued training of LLaMA-2 with parallel translation data, and followed by instruction-tuning for MT tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Zheng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib27" title="">27</a>]</cite> proposed to fine-tune LLMs based on prompts, and they compared it to LoRA for domain adaption in IT for Chinese-English and English-Chinese MT. Moreover, <cite class="ltx_cite ltx_citemacro_citet">Zheng et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib27" title="">27</a>]</cite> incorporate IT terminology by few-shot promoting and chain-of-thought. The template used for proposed prompt-tuning model has a substantial impact on performance, and the introduction of terminology with simple prompt rephrasing outperforms chain-of-thought. <cite class="ltx_cite ltx_citemacro_citet">Eschbach-Dymanus et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib10" title="">10</a>]</cite> studied domain adaption for business IT with LLMs. They compared full SFT, LoRA, different prompting techniques, and standard NMT. Finally, <cite class="ltx_cite ltx_citemacro_citet">Eschbach-Dymanus et al. [<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib10" title="">10</a>]</cite> defined guidelines for domain adaption with LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">We followed <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib4" title="">4</a>]</cite> for our experimental design. In particular, we focus on medical terminology from IATE for the instruction formatted datasets, and out of English translation directions.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Future Work</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this study, we show preliminary results with a comparison between baseline LLMs and QLoRA instruction-tuned models in the medical domain for en-es, en-de, and en-ro. We introduce terminology from IATE into an instruction formatted dataset for controlled generation in LLMs. Finally, we show hyper-parameter values for instruction-tuned LLMs on a validation split. The instruction-tuned models significantly outperform the baseline in terms of automatic metrics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">For future work, we will evaluate the baselines with few-shot instead of zero-shot. The limitations of our preliminary work are as follows: the results are based on the validation data split, and the evaluation is performed with automatic metrics for terminology. We will perform an evaluation on a balanced test split in terms of number and type of present terms with respect of the training data. Finally, we will perform a manual error annotation, given that automatic metrics may not test for the correct generation of terminology on the MT output <cite class="ltx_cite ltx_citemacro_citep">[<a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work was was supported by the ZID from the University of Vienna with Azure cloud credits.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
ELRC3.0 Multilingual corpus made out of PDF documents from the European Medicines Agency (EMEA), (February 2020) ELRC-SHARE.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://elrc-share.eu/repository/browse/elrc30-multilingual-corpus-made-out-of-pdf-documents-from-the-european-medicines-agency-emea-httpswwwemaeuropaeu-february-2020/3cf9da8e858511ea913100155d0267062d01c2d847c349628584d10293948de3/" title="">https://elrc-share.eu/repository/browse/elrc30-multilingual-corpus-made-out-of-pdf-documents-from-the-european-medicines-agency-emea-httpswwwemaeuropaeu-february-2020/3cf9da8e858511ea913100155d0267062d01c2d847c349628584d10293948de3/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Almahasees et al. [2021]</span>
<span class="ltx_bibblock">
Zakaryia Almahasees, Samah Meqdadi, and Yousef Albudairi.

</span>
<span class="ltx_bibblock">Evaluation of google translate in rendering english covid-19 texts into arabic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Journal of Language and Linguistic Studies</em>, 17(4):2065–2080, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.52462/jlls.149</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alves et al. [2023]</span>
<span class="ltx_bibblock">
Duarte M. Alves, Nuno M. Guerreiro, João Alves, José Pombal, Ricardo Rei, José G. C. de Souza, Pierre Colombo, and André F. T. Martins.

</span>
<span class="ltx_bibblock">Steering large language models for machine translation with finetuning and in-context learning, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.13448" title="">https://arxiv.org/abs/2310.13448</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alves et al. [2024]</span>
<span class="ltx_bibblock">
Duarte M. Alves, José P. Pombal, Nuno M. Guerreiro, Pedro H. Martins, Joao Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and André Martins.

</span>
<span class="ltx_bibblock">Tower: An open multilingual large language model for translation-related tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">ArXiv</em>, abs/2402.17733, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.17733" title="">https://arxiv.org/abs/2402.17733</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bawden and Yvon [2023]</span>
<span class="ltx_bibblock">
Rachel Bawden and François Yvon.

</span>
<span class="ltx_bibblock">Investigating the translation performance of a large multilingual language model: the case of BLOOM.

</span>
<span class="ltx_bibblock">In Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 157–170, Tampere, Finland, June 2023. European Association for Machine Translation.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.eamt-1.16" title="">https://aclanthology.org/2023.eamt-1.16</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bengio et al. [2000]</span>
<span class="ltx_bibblock">
Yoshua Bengio, Réjean Ducharme, and Pascal Vincent.

</span>
<span class="ltx_bibblock">A neural probabilistic language model.

</span>
<span class="ltx_bibblock">In T. Leen, T. Dietterich, and V. Tresp, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">NeurIPS</em>, volume 13. MIT Press, 2000.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al. [2022]</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2210.11416" title="">https://arxiv.org/abs/2210.11416</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al. [2023]</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Qlora: Efficient finetuning of quantized llms, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2305.14314" title="">https://arxiv.org/abs/2305.14314</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. [2024]</span>
<span class="ltx_bibblock">
Abhimanyu Dubey et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.21783" title="">https://arxiv.org/abs/2407.21783</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eschbach-Dymanus et al. [2024]</span>
<span class="ltx_bibblock">
Johannes Eschbach-Dymanus, Frank Essenberger, Bianka Buschbeck, and Miriam Exel.

</span>
<span class="ltx_bibblock">Exploring the effectiveness of llm domain adaptation for business it machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 25th Annual Conference of the European Association for Machine Translation</em>, June 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://eamt2024.sheffield.ac.uk/" title="">https://eamt2024.sheffield.ac.uk/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gaona et al. [2023]</span>
<span class="ltx_bibblock">
Miguel Angel Rios Gaona, Raluca-Maria Chereji, Alina Secara, and Dragos Ciobanu.

</span>
<span class="ltx_bibblock">Quality analysis of multilingual neural machine translation systems and reference test translations for the English-Romanian language pair in the medical domain.

</span>
<span class="ltx_bibblock">In Mary Nurminen, Judith Brenner, Maarit Koponen, Sirkku Latomaa, Mikhail Mikhailov, Frederike Schierl, Tharindu Ranasinghe, Eva Vanmassenhove, Sergi Alvarez Vidal, Nora Aranberri, Mara Nunziatini, Carla Parra Escartín, Mikel Forcada, Maja Popovic, Carolina Scarton, and Helena Moniz, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 355–364, Tampere, Finland, June 2023. European Association for Machine Translation.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.eamt-1.35" title="">https://aclanthology.org/2023.eamt-1.35</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haque et al. [2019]</span>
<span class="ltx_bibblock">
Rejwanul Haque, Md Hasanuzzaman, and Andy Way.

</span>
<span class="ltx_bibblock">Investigating terminology translation in statistical and neural machine translation: A case study on English-to-Hindi and Hindi-to-English.

</span>
<span class="ltx_bibblock">In Ruslan Mitkov and Galia Angelova, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)</em>, pages 437–446, Varna, Bulgaria, September 2019. INCOMA Ltd.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.26615/978-954-452-056-4_052</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/R19-1052" title="">https://aclanthology.org/R19-1052</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hauhio and Friberg [2024]</span>
<span class="ltx_bibblock">
Iikka Hauhio and Théo Kalevi Max Friberg.

</span>
<span class="ltx_bibblock">Mitra: Improving terminologically constrained translation quality with backtranslations and flag diacritics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 25th Annual Conference of The European Association for Machine Translation</em>, Switzerland, June 2024. European Association for Machine Translation.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://eamt2024.sheffield.ac.uk/" title="">https://eamt2024.sheffield.ac.uk/</a>.

</span>
<span class="ltx_bibblock">Annual Conference of The European Association for Machine Translation, EAMT ; Conference date: 24-06-2024 Through 27-06-2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2021]</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2106.09685" title="">https://arxiv.org/abs/2106.09685</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jelinek [1998]</span>
<span class="ltx_bibblock">
Frederick Jelinek.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Statistical Methods for Speech Recognition (Language, Speech, and Communication)</em>.

</span>
<span class="ltx_bibblock">The MIT Press, January 1998.

</span>
<span class="ltx_bibblock">ISBN 0262100665.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://www.amazon.fr/exec/obidos/ASIN/0262100665/citeulike04-21" title="">http://www.amazon.fr/exec/obidos/ASIN/0262100665/citeulike04-21</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangrulkar et al. [2022]</span>
<span class="ltx_bibblock">
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan.

</span>
<span class="ltx_bibblock">Peft: State-of-the-art parameter-efficient fine-tuning methods.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/huggingface/peft" title="">https://github.com/huggingface/peft</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al. [2024]</span>
<span class="ltx_bibblock">
OpenAI et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.08774" title="">https://arxiv.org/abs/2303.08774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang et al. [2024]</span>
<span class="ltx_bibblock">
Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu, Derek F. Wong, Shuming Shi, and Zhaopeng Tu.

</span>
<span class="ltx_bibblock">Salute the classic: Revisiting challenges of machine translation in the age of large language models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.08350" title="">https://arxiv.org/abs/2401.08350</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. [2002]</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In Pierre Isabelle, Eugene Charniak, and Dekang Lin, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3115/1073083.1073135</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P02-1040" title="">https://aclanthology.org/P02-1040</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović [2015]</span>
<span class="ltx_bibblock">
Maja Popović.

</span>
<span class="ltx_bibblock">chrF: character n-gram F-score for automatic MT evaluation.

</span>
<span class="ltx_bibblock">In Ondřej Bojar, Rajan Chatterjee, Christian Federmann, Barry Haddow, Chris Hokamp, Matthias Huck, Varvara Logacheva, and Pavel Pecina, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Tenth Workshop on Statistical Machine Translation</em>, pages 392–395, Lisbon, Portugal, September 2015. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/W15-3049</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W15-3049" title="">https://aclanthology.org/W15-3049</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pourkamali and Sharifi [2024]</span>
<span class="ltx_bibblock">
Nooshin Pourkamali and Shler Ebrahim Sharifi.

</span>
<span class="ltx_bibblock">Machine translation with large language models: Prompt engineering for persian, english, and russian directions, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.08429" title="">https://arxiv.org/abs/2401.08429</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. [2020]</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie.

</span>
<span class="ltx_bibblock">COMET: A neural framework for MT evaluation.

</span>
<span class="ltx_bibblock">In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 2685–2702, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-main.213</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.213" title="">https://aclanthology.org/2020.emnlp-main.213</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saunders et al. [2019]</span>
<span class="ltx_bibblock">
Danielle Saunders, Felix Stahlberg, Adrià de Gispert, and Bill Byrne.

</span>
<span class="ltx_bibblock">Domain adaptive inference for neural machine translation.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 222–228, Florence, Italy, July 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1022</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P19-1022" title="">https://aclanthology.org/P19-1022</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
Hugo Touvron et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2307.09288" title="">https://arxiv.org/abs/2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. [2020]</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language processing, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1910.03771" title="">https://arxiv.org/abs/1910.03771</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2023]</span>
<span class="ltx_bibblock">
Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn.

</span>
<span class="ltx_bibblock">Machine translation with large language models: Prompting, few-shot learning, and fine-tuning with QLoRA.

</span>
<span class="ltx_bibblock">In Philipp Koehn, Barry Haddow, Tom Kocmi, and Christof Monz, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 468–481, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.wmt-1.43</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.wmt-1.43" title="">https://aclanthology.org/2023.wmt-1.43</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. [2024]</span>
<span class="ltx_bibblock">
Jiawei Zheng, Hanghai Hong, Xiaoli Wang, Jingsong Su, Yonggui Liang, and Shikai Wu.

</span>
<span class="ltx_bibblock">Fine-tuning large language models for domain-specific machine translation, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.15061" title="">https://arxiv.org/abs/2402.15061</a>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Instruction Templates</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Instruction templates for FLAN-T5, LLaMA3 and Tower. The source_term is the source entry from IATE, the target_term is the target entry from IATE, source_language is the source language (i.e. English), target_id is the target language (i.e. Spanish, German, and Romanian), and glossary_type is Glossary with one candidate term pair or Glossaries with several candidate terms.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">FLAN-T5</span> instruction template for a segment with candidate term pairs. The prompt is the input for the encoder and the target segment is the input for the decoder:</p>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.p2.2" style="background-color:#BFBFBF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,e2dsb3NzYXJ5X3R5cGV9Ogoie3NvdXJjZV90ZXJtfSIgLT4gInt0YXJnZXRfdGVybX0iCi4uLgpUcmFuc2xhdGUgdGhlIHNvdXJjZSB0ZXh0IGZyb20ge3NvdXJjZV9pZH0gdG8ge3RhcmdldF9pZH0gZm9sbG93aW5nIHRoZSBwcm92aWRlZCB0cmFuc2xhdGlvbiBnbG9zc2FyaWVzLgp7c291cmNlX2lkfToge3NvdXJjZV9zZWdtZW50fQ==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx11.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.2">glossary_type</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.3">}:</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx12.1">"{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.2">source_term</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.3">}"</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.5">-&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.6"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.7">"{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.8">target_term</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.9">}"</span>
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx13.1">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx14">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.1">Translate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.5">source</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.7">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.9">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.10"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.11">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.12">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.13">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.15">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.17">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.18">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.19">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.21">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.23">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.25">provided</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.27">translation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx14.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx14.29">glossaries</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx14.30">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx15.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.2">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.3">}:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.5">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.6">source_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.7">}</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p3">
<p class="ltx_p" id="A1.p3.1"><span class="ltx_text ltx_font_bold" id="A1.p3.1.1">FLAN-T5</span> instruction template with a segment with no candidate terms. The prompt is the input for the encoder and the target segment is the input for the decoder:</p>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.p3.2" style="background-color:#BFBFBF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,VHJhbnNsYXRlIHRoZSBzb3VyY2UgdGV4dCBmcm9tIHtzb3VyY2VfaWR9IHRvIHt0YXJnZXRfaWR9Lgp7c291cmNlX2lkfToge3NvdXJjZV9zZWdtZW50fQ==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.1">Translate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.5">source</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.7">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.9">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.10"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.11">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.12">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.13">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.15">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx16.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.17">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx16.18">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx16.19">}.</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx17.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.2">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.3">}:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.5">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.6">source_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.7">}</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p4">
<p class="ltx_p" id="A1.p4.1"><span class="ltx_text ltx_font_bold" id="A1.p4.1.1">LLaMA3-8B</span> instruction template for a segment with candidate term pairs:</p>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.p4.2" style="background-color:#BFBFBF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,PHxiZWdpbl9vZl90ZXh0fD48fHN0YXJ0X2hlYWRlcl9pZHw+c3lzdGVtPHxlbmRfaGVhZGVyX2lkfD4KWW91IGFyZSBhIGhlbHBmdWwgdHJhbnNsYXRpb24gYXNzaXN0YW50Ljx8ZW90X2lkfD48fHN0YXJ0X2hlYWRlcl9pZHw+dXNlcjx8ZW5kX2hlYWRlcl9pZHw+CntnbG9zc2FyeV90eXBlfToKIntzb3VyY2VfdGVybX0iIC0+ICJ7dGFyZ2V0X3Rlcm19IgouLi4KVHJhbnNsYXRlIHRoZSBzb3VyY2UgdGV4dCBmcm9tIHtzb3VyY2VfaWR9IHRvIHt0YXJnZXRfaWR9IGZvbGxvd2luZyB0aGUgcHJvdmlkZWQgdHJhbnNsYXRpb24gZ2xvc3Nhcmllcy4Ke3NvdXJjZV9pZH06IHtzb3VyY2Vfc2VnbWVudH0Ke3RhcmdldF9pZH06PHxlb3RfaWR8Pgo8fHN0YXJ0X2hlYWRlcl9pZHw+YXNzaXN0YW50PHxlbmRfaGVhZGVyX2lkfD4Ke3RhcmdldF9zZWdtZW50fTx8ZW90X2lkfD4=">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx18.1">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.2">begin_of_text</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.3">|&gt;&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.4">start_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.5">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.6">system</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.7">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.8">end_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.9">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.5">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.7">helpful</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.9">translation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.11">assistant</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.12">.&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.13">eot_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.14">|&gt;&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.15">start_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.16">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.17">user</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.18">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.19">end_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.20">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx20.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.2">glossary_type</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.3">}:</span>
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx21.1">"{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.2">source_term</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.3">}"</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.5">-&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.6"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.7">"{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.8">target_term</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.9">}"</span>
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx22.1">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx23">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.1">Translate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.5">source</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.7">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.9">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.10"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.11">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.12">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.13">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.15">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.17">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.18">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.19">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.21">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.23">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.25">provided</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.27">translation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.29">glossaries</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.30">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx24">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx24.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.2">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.3">}:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.5">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.6">source_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.7">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx25">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx25.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.2">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.3">}:&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx25.4">eot_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx25.5">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx26">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx26.1">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.2">start_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.3">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.4">assistant</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.5">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.6">end_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.7">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx27">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx27.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.2">target_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.3">}&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.4">eot_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.5">|&gt;</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p5">
<p class="ltx_p" id="A1.p5.1"><span class="ltx_text ltx_font_bold" id="A1.p5.1.1">LLaMA3-8B</span> instruction template for a segment with no candidate term pairs:</p>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.p5.2" style="background-color:#BFBFBF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,PHxiZWdpbl9vZl90ZXh0fD48fHN0YXJ0X2hlYWRlcl9pZHw+c3lzdGVtPHxlbmRfaGVhZGVyX2lkfD4KWW91IGFyZSBhIGhlbHBmdWwgdHJhbnNsYXRpb24gYXNzaXN0YW50Ljx8ZW90X2lkfD48fHN0YXJ0X2hlYWRlcl9pZHw+dXNlcjx8ZW5kX2hlYWRlcl9pZHw+ClRyYW5zbGF0ZSB0aGUgc291cmNlIHRleHQgZnJvbSB7c291cmNlX2lkfSB0byB7dGFyZ2V0X2lkfS4Ke3NvdXJjZV9pZH06IHtzb3VyY2Vfc2VnbWVudH0Ke3RhcmdldF9pZH06PHxlb3RfaWR8Pgo8fHN0YXJ0X2hlYWRlcl9pZHw+YXNzaXN0YW50PHxlbmRfaGVhZGVyX2lkfD4Ke3RhcmdldF9zZWdtZW50fTx8ZW90X2lkfD4=">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx28">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx28.1">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.2">begin_of_text</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.3">|&gt;&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.4">start_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.5">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.6">system</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.7">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx28.8">end_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx28.9">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx29">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.5">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.7">helpful</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.9">translation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.11">assistant</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.12">.&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.13">eot_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.14">|&gt;&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.15">start_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.16">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.17">user</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.18">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.19">end_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.20">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx30">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.1">Translate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.5">source</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.7">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.9">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.10"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.11">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.12">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.13">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.15">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.17">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.18">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.19">}.</span>
</div>
<div class="ltx_listingline" id="lstnumberx31">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx31.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.2">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.3">}:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.5">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.6">source_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.7">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx32">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx32.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.2">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.3">}:&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.4">eot_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.5">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx33">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx33.1">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.2">start_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.3">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.4">assistant</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.5">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.6">end_header_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.7">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx34">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx34.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.2">target_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.3">}&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.4">eot_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.5">|&gt;</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p6">
<p class="ltx_p" id="A1.p6.1"><span class="ltx_text ltx_font_bold" id="A1.p6.1.1">Tower-7B</span> instruction template for a segment with candidate term pairs:</p>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.p6.2" style="background-color:#BFBFBF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,PHxpbV9zdGFydHw+dXNlcgp7Z2xvc3NhcnlfdHlwZX06CiJ7c291cmNlX3Rlcm19IiAtPiAie3RhcmdldF90ZXJtfSIKLi4uClRyYW5zbGF0ZSB0aGUgc291cmNlIHRleHQgZnJvbSB7c291cmNlX2lkfSB0byB7dGFyZ2V0X2lkfSBmb2xsb3dpbmcgdGhlIHByb3ZpZGVkIHRyYW5zbGF0aW9uIGdsb3NzYXJpZXMuCntzb3VyY2VfaWR9OiB7c291cmNlX3NlZ21lbnR9Cnt0YXJnZXRfaWR9Ojx8aW1fZW5kfD4KPHxpbV9zdGFydHw+YXNzaXN0YW50Cnt0YXJnZXRfc2VnbWVudH08fGltIGVuZHw+">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx35">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx35.1">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.2">im_start</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.3">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.4">user</span>
</div>
<div class="ltx_listingline" id="lstnumberx36">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx36.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx36.2">glossary_type</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx36.3">}:</span>
</div>
<div class="ltx_listingline" id="lstnumberx37">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx37.1">"{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.2">source_term</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.3">}"</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.5">-&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx37.6"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.7">"{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.8">target_term</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.9">}"</span>
</div>
<div class="ltx_listingline" id="lstnumberx38">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx38.1">...</span>
</div>
<div class="ltx_listingline" id="lstnumberx39">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.1">Translate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.5">source</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.7">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.9">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.10"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.11">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.12">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.13">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.15">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.17">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.18">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.19">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.21">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.23">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.25">provided</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.27">translation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx39.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx39.29">glossaries</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx39.30">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx40">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx40.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.2">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.3">}:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.5">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.6">source_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.7">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx41">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx41.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.2">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx41.3">}:&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx41.4">im_end</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx41.5">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx42">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx42.1">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.2">im_start</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx42.3">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.4">assistant</span>
</div>
<div class="ltx_listingline" id="lstnumberx43">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx43.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx43.2">target_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx43.3">}&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx43.4">im</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx43.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx43.6">end</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx43.7">|&gt;</span>
</div>
</div>
</div>
<div class="ltx_para ltx_noindent" id="A1.p7">
<p class="ltx_p" id="A1.p7.1"><span class="ltx_text ltx_font_bold" id="A1.p7.1.1">Tower-7B</span> instruction template for a segment with no candidate term pairs:</p>
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.p7.2" style="background-color:#BFBFBF;">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,PHxpbV9zdGFydHw+dXNlcgpUcmFuc2xhdGUgdGhlIHNvdXJjZSB0ZXh0IGZyb20ge3NvdXJjZV9pZH0gdG8ge3RhcmdldF9pZH0uCntzb3VyY2VfaWR9OiB7c291cmNlX3NlZ21lbnR9Cnt0YXJnZXRfaWR9Ojx8aW1fZW5kfD4KPHxpbV9zdGFydHw+YXNzaXN0YW50Cnt0YXJnZXRfc2VnbWVudH08fGltIGVuZHw+">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx44">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx44.1">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.2">im_start</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx44.3">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx44.4">user</span>
</div>
<div class="ltx_listingline" id="lstnumberx45">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.1">Translate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.5">source</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.7">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.9">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.10"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.11">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.12">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.13">}</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.15">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx45.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.17">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx45.18">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx45.19">}.</span>
</div>
<div class="ltx_listingline" id="lstnumberx46">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx46.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx46.2">source_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx46.3">}:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx46.4"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx46.5">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx46.6">source_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx46.7">}</span>
</div>
<div class="ltx_listingline" id="lstnumberx47">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx47.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.2">target_id</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx47.3">}:&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx47.4">im_end</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx47.5">|&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx48">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx48.1">&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx48.2">im_start</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx48.3">|&gt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx48.4">assistant</span>
</div>
<div class="ltx_listingline" id="lstnumberx49">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx49.1">{</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx49.2">target_segment</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx49.3">}&lt;|</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx49.4">im</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx49.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx49.6">end</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx49.7">|&gt;</span>
</div>
</div>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>QLoRA Fine-tuning Setup</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">The hyper-parameter values tables for FLAN-T5 <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#A2.T2" title="Table 2 ‣ Appendix B QLoRA Fine-tuning Setup ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_tag">2</span></a>, Llama3-8B <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#A2.T3" title="Table 3 ‣ Appendix B QLoRA Fine-tuning Setup ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_tag">3</span></a>, and Tower-7B <a class="ltx_ref" href="https://arxiv.org/html/2408.16440v1#A2.T4" title="Table 4 ‣ Appendix B QLoRA Fine-tuning Setup ‣ Instruction-tuned Large Language Models for Machine Translation in the Medical Domain"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="A2.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T2.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A2.T2.2.3.1.1">Hyper-parameter</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T2.2.3.1.2">Value</td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T2.2.4.2.1">r</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T2.2.4.2.2">8</td>
</tr>
<tr class="ltx_tr" id="A2.T2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.1.1.1"><math alttext="\alpha" class="ltx_Math" display="inline" id="A2.T2.1.1.1.m1.1"><semantics id="A2.T2.1.1.1.m1.1a"><mi id="A2.T2.1.1.1.m1.1.1" xref="A2.T2.1.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A2.T2.1.1.1.m1.1b"><ci id="A2.T2.1.1.1.m1.1.1.cmml" xref="A2.T2.1.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A2.T2.1.1.1.m1.1d">italic_α</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="A2.T2.1.1.2">32</td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.2.5.3.1">Dropout</th>
<td class="ltx_td ltx_align_center" id="A2.T2.2.5.3.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.2.6.4.1">Target modules</th>
<td class="ltx_td ltx_align_center" id="A2.T2.2.6.4.2">q, v</td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T2.2.7.5.1">Max source length</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T2.2.7.5.2">512</td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.2.8.6.1">Max target length</th>
<td class="ltx_td ltx_align_center" id="A2.T2.2.8.6.2">512</td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.2.9.7.1">Batch size</th>
<td class="ltx_td ltx_align_center" id="A2.T2.2.9.7.2">6</td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.2.2.2">Learning rate</th>
<td class="ltx_td ltx_align_center" id="A2.T2.2.2.1"><math alttext="2e-4" class="ltx_Math" display="inline" id="A2.T2.2.2.1.m1.1"><semantics id="A2.T2.2.2.1.m1.1a"><mrow id="A2.T2.2.2.1.m1.1.1" xref="A2.T2.2.2.1.m1.1.1.cmml"><mrow id="A2.T2.2.2.1.m1.1.1.2" xref="A2.T2.2.2.1.m1.1.1.2.cmml"><mn id="A2.T2.2.2.1.m1.1.1.2.2" xref="A2.T2.2.2.1.m1.1.1.2.2.cmml">2</mn><mo id="A2.T2.2.2.1.m1.1.1.2.1" xref="A2.T2.2.2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A2.T2.2.2.1.m1.1.1.2.3" xref="A2.T2.2.2.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A2.T2.2.2.1.m1.1.1.1" xref="A2.T2.2.2.1.m1.1.1.1.cmml">−</mo><mn id="A2.T2.2.2.1.m1.1.1.3" xref="A2.T2.2.2.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T2.2.2.1.m1.1b"><apply id="A2.T2.2.2.1.m1.1.1.cmml" xref="A2.T2.2.2.1.m1.1.1"><minus id="A2.T2.2.2.1.m1.1.1.1.cmml" xref="A2.T2.2.2.1.m1.1.1.1"></minus><apply id="A2.T2.2.2.1.m1.1.1.2.cmml" xref="A2.T2.2.2.1.m1.1.1.2"><times id="A2.T2.2.2.1.m1.1.1.2.1.cmml" xref="A2.T2.2.2.1.m1.1.1.2.1"></times><cn id="A2.T2.2.2.1.m1.1.1.2.2.cmml" type="integer" xref="A2.T2.2.2.1.m1.1.1.2.2">2</cn><ci id="A2.T2.2.2.1.m1.1.1.2.3.cmml" xref="A2.T2.2.2.1.m1.1.1.2.3">𝑒</ci></apply><cn id="A2.T2.2.2.1.m1.1.1.3.cmml" type="integer" xref="A2.T2.2.2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T2.2.2.1.m1.1c">2e-4</annotation><annotation encoding="application/x-llamapun" id="A2.T2.2.2.1.m1.1d">2 italic_e - 4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T2.2.10.8.1">Warm-up steps</th>
<td class="ltx_td ltx_align_center" id="A2.T2.2.10.8.2">0.03</td>
</tr>
<tr class="ltx_tr" id="A2.T2.2.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T2.2.11.9.1">Scheduler type</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T2.2.11.9.2">linear</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>FLAN-T5 seq2seq hyper-parameter values. The upper section contains the QLoRA hyper-parameters, and the lower section contains the overall fine-tuning. </figcaption>
</figure>
<figure class="ltx_table" id="A2.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T3.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A2.T3.2.3.1.1">Hyper-parameter</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T3.2.3.1.2">Value</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T3.2.4.2.1">r</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.2.4.2.2">64</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.1.1.1"><math alttext="\alpha" class="ltx_Math" display="inline" id="A2.T3.1.1.1.m1.1"><semantics id="A2.T3.1.1.1.m1.1a"><mi id="A2.T3.1.1.1.m1.1.1" xref="A2.T3.1.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.m1.1b"><ci id="A2.T3.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A2.T3.1.1.1.m1.1d">italic_α</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="A2.T3.1.1.2">128</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.2.5.3.1">Dropout</th>
<td class="ltx_td ltx_align_center" id="A2.T3.2.5.3.2">0.05</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.2.6.4.1">Target modules</th>
<td class="ltx_td ltx_align_center" id="A2.T3.2.6.4.2">q_proj, v_proj</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T3.2.7.5.1">Max sequence length</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T3.2.7.5.2">512</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.2.8.6.1">Batch size</th>
<td class="ltx_td ltx_align_center" id="A2.T3.2.8.6.2">2</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.2.9.7.1">Gradient accumulation</th>
<td class="ltx_td ltx_align_center" id="A2.T3.2.9.7.2">4</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.2.2.2">Learning rate</th>
<td class="ltx_td ltx_align_center" id="A2.T3.2.2.1"><math alttext="2e-4" class="ltx_Math" display="inline" id="A2.T3.2.2.1.m1.1"><semantics id="A2.T3.2.2.1.m1.1a"><mrow id="A2.T3.2.2.1.m1.1.1" xref="A2.T3.2.2.1.m1.1.1.cmml"><mrow id="A2.T3.2.2.1.m1.1.1.2" xref="A2.T3.2.2.1.m1.1.1.2.cmml"><mn id="A2.T3.2.2.1.m1.1.1.2.2" xref="A2.T3.2.2.1.m1.1.1.2.2.cmml">2</mn><mo id="A2.T3.2.2.1.m1.1.1.2.1" xref="A2.T3.2.2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A2.T3.2.2.1.m1.1.1.2.3" xref="A2.T3.2.2.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A2.T3.2.2.1.m1.1.1.1" xref="A2.T3.2.2.1.m1.1.1.1.cmml">−</mo><mn id="A2.T3.2.2.1.m1.1.1.3" xref="A2.T3.2.2.1.m1.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T3.2.2.1.m1.1b"><apply id="A2.T3.2.2.1.m1.1.1.cmml" xref="A2.T3.2.2.1.m1.1.1"><minus id="A2.T3.2.2.1.m1.1.1.1.cmml" xref="A2.T3.2.2.1.m1.1.1.1"></minus><apply id="A2.T3.2.2.1.m1.1.1.2.cmml" xref="A2.T3.2.2.1.m1.1.1.2"><times id="A2.T3.2.2.1.m1.1.1.2.1.cmml" xref="A2.T3.2.2.1.m1.1.1.2.1"></times><cn id="A2.T3.2.2.1.m1.1.1.2.2.cmml" type="integer" xref="A2.T3.2.2.1.m1.1.1.2.2">2</cn><ci id="A2.T3.2.2.1.m1.1.1.2.3.cmml" xref="A2.T3.2.2.1.m1.1.1.2.3">𝑒</ci></apply><cn id="A2.T3.2.2.1.m1.1.1.3.cmml" type="integer" xref="A2.T3.2.2.1.m1.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.2.2.1.m1.1c">2e-4</annotation><annotation encoding="application/x-llamapun" id="A2.T3.2.2.1.m1.1d">2 italic_e - 4</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T3.2.10.8.1">Warm-up steps</th>
<td class="ltx_td ltx_align_center" id="A2.T3.2.10.8.2">0.03</td>
</tr>
<tr class="ltx_tr" id="A2.T3.2.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T3.2.11.9.1">Scheduler type</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T3.2.11.9.2">cosine</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>LLaMA3-8B hyper-parameter values. The upper section contains the QLoRA hyper-parameters, and the lower section contains the overall fine-tuning. </figcaption>
</figure>
<figure class="ltx_table" id="A2.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A2.T4.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T4.2.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A2.T4.2.3.1.1">Hyper-parameter</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A2.T4.2.3.1.2">Value</td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T4.2.4.2.1">r</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.2.4.2.2">64</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.1.1.1"><math alttext="\alpha" class="ltx_Math" display="inline" id="A2.T4.1.1.1.m1.1"><semantics id="A2.T4.1.1.1.m1.1a"><mi id="A2.T4.1.1.1.m1.1.1" xref="A2.T4.1.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="A2.T4.1.1.1.m1.1b"><ci id="A2.T4.1.1.1.m1.1.1.cmml" xref="A2.T4.1.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="A2.T4.1.1.1.m1.1d">italic_α</annotation></semantics></math></th>
<td class="ltx_td ltx_align_center" id="A2.T4.1.1.2">16</td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.2.5.3.1">Dropout</th>
<td class="ltx_td ltx_align_center" id="A2.T4.2.5.3.2">0.1</td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.2.6.4.1">Target modules</th>
<td class="ltx_td ltx_align_center" id="A2.T4.2.6.4.2">q_proj, k_proj, v_proj, o_proj</td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T4.2.7.5.1">Max sequence length</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T4.2.7.5.2">512</td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.2.8.6.1">Batch size</th>
<td class="ltx_td ltx_align_center" id="A2.T4.2.8.6.2">2</td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.2.9.7.1">Gradient accumulation</th>
<td class="ltx_td ltx_align_center" id="A2.T4.2.9.7.2">2</td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.2.2.2">Learning rate</th>
<td class="ltx_td ltx_align_center" id="A2.T4.2.2.1"><math alttext="2e-5" class="ltx_Math" display="inline" id="A2.T4.2.2.1.m1.1"><semantics id="A2.T4.2.2.1.m1.1a"><mrow id="A2.T4.2.2.1.m1.1.1" xref="A2.T4.2.2.1.m1.1.1.cmml"><mrow id="A2.T4.2.2.1.m1.1.1.2" xref="A2.T4.2.2.1.m1.1.1.2.cmml"><mn id="A2.T4.2.2.1.m1.1.1.2.2" xref="A2.T4.2.2.1.m1.1.1.2.2.cmml">2</mn><mo id="A2.T4.2.2.1.m1.1.1.2.1" xref="A2.T4.2.2.1.m1.1.1.2.1.cmml">⁢</mo><mi id="A2.T4.2.2.1.m1.1.1.2.3" xref="A2.T4.2.2.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="A2.T4.2.2.1.m1.1.1.1" xref="A2.T4.2.2.1.m1.1.1.1.cmml">−</mo><mn id="A2.T4.2.2.1.m1.1.1.3" xref="A2.T4.2.2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.T4.2.2.1.m1.1b"><apply id="A2.T4.2.2.1.m1.1.1.cmml" xref="A2.T4.2.2.1.m1.1.1"><minus id="A2.T4.2.2.1.m1.1.1.1.cmml" xref="A2.T4.2.2.1.m1.1.1.1"></minus><apply id="A2.T4.2.2.1.m1.1.1.2.cmml" xref="A2.T4.2.2.1.m1.1.1.2"><times id="A2.T4.2.2.1.m1.1.1.2.1.cmml" xref="A2.T4.2.2.1.m1.1.1.2.1"></times><cn id="A2.T4.2.2.1.m1.1.1.2.2.cmml" type="integer" xref="A2.T4.2.2.1.m1.1.1.2.2">2</cn><ci id="A2.T4.2.2.1.m1.1.1.2.3.cmml" xref="A2.T4.2.2.1.m1.1.1.2.3">𝑒</ci></apply><cn id="A2.T4.2.2.1.m1.1.1.3.cmml" type="integer" xref="A2.T4.2.2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T4.2.2.1.m1.1c">2e-5</annotation><annotation encoding="application/x-llamapun" id="A2.T4.2.2.1.m1.1d">2 italic_e - 5</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T4.2.10.8.1">Warm-up steps</th>
<td class="ltx_td ltx_align_center" id="A2.T4.2.10.8.2">0.03</td>
</tr>
<tr class="ltx_tr" id="A2.T4.2.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A2.T4.2.11.9.1">Scheduler type</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T4.2.11.9.2">cosine</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Tower-7B hyper-parameter values. The upper section contains the QLoRA hyper-parameters, and the lower section contains the overall fine-tuning. </figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Aug 29 11:03:54 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
