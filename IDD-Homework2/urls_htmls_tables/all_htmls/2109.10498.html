<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.10498] Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification</title><meta property="og:description" content="Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.10498">

<!--Generated on Sat Mar  2 03:29:56 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Suncheng Xiang<sup id="id12.10.id1" class="ltx_sup"><span id="id12.10.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Guanjie You<sup id="id13.11.id2" class="ltx_sup"><span id="id13.11.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Mengyuan Guan<sup id="id14.12.id3" class="ltx_sup"><span id="id14.12.id3.1" class="ltx_text ltx_font_italic">1</span></sup>, Hao Chen<sup id="id15.13.id4" class="ltx_sup"><span id="id15.13.id4.1" class="ltx_text ltx_font_italic">1</span></sup>, Binjie Yan<sup id="id16.14.id5" class="ltx_sup"><span id="id16.14.id5.1" class="ltx_text ltx_font_italic">1</span></sup>, Ting Liu<sup id="id17.15.id6" class="ltx_sup"><span id="id17.15.id6.1" class="ltx_text ltx_font_italic">1</span></sup>, Yuzhuo Fu<sup id="id18.16.id7" class="ltx_sup"><span id="id18.16.id7.1" class="ltx_text ltx_font_italic">1</span></sup>
<br class="ltx_break"><sup id="id19.17.id8" class="ltx_sup"><span id="id19.17.id8.1" class="ltx_text ltx_font_italic">1</span></sup>Shanghai Jiao Tong University, <sup id="id20.18.id9" class="ltx_sup"><span id="id20.18.id9.1" class="ltx_text ltx_font_italic">2</span></sup>National University of Defense Technology
<br class="ltx_break"><span id="id21.19.id10" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{xiangsuncheng17, gemini.my, 958577057, yanbinjie, louisa_liu, yzfu}@sjtu.edu.cn, ygjssxz@163.com</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id22.id1" class="ltx_p">Person re-identification (re-ID) plays an important role in applications such as public security and video surveillance. Recently, learning from synthetic data, which benefits from the popularity of synthetic data engine, has attracted great attention from the public eyes. However, existing datasets are limited in quantity, diversity and realisticity, and cannot be efficiently used for re-ID problem. To address this challenge, we manually construct a large-scale person dataset named <span id="id22.id1.1" class="ltx_text ltx_font_italic">FineGPR</span> with fine-grained attribute annotations. Moreover, aiming to fully exploit the potential of <span id="id22.id1.2" class="ltx_text ltx_font_italic">FineGPR</span> and promote the efficient training from millions of synthetic data, we propose an attribute analysis pipeline called AOST, which dynamically learns attribute distribution
in real domain, then eliminates the gap between synthetic and real-world data and thus is freely deployed to new scenarios. Experiments conducted on benchmarks demonstrate that <span id="id22.id1.3" class="ltx_text ltx_font_italic">FineGPR</span> with AOST outperforms (or is on par with) existing real and synthetic datasets, which suggests its feasibility for re-ID task and proves the proverbial <span id="id22.id1.4" class="ltx_text ltx_font_bold ltx_font_italic">less-is-more</span> principle.
Our synthetic <span id="id22.id1.5" class="ltx_text ltx_font_italic">FineGPR</span> dataset is publicly available at <a target="_blank" href="https://github.com/JeremyXSC/FineGPR" title="" class="ltx_ref ltx_url ltx_font_typewriter ltx_font_italic">https://github.com/JeremyXSC/FineGPR</a>.</p>
</div>
<div id="id11" class="ltx_logical-block">
<div id="id11.p1" class="ltx_para">
<img src="/html/2109.10498/assets/x1.png" id="id10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="118" alt="[Uncaptioned image]">
</div>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.7.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.8.2" class="ltx_text" style="font-size:90%;">Sample images from the proposed <em id="S0.F1.8.2.1" class="ltx_emph ltx_font_italic">FineGPR</em> dataset, which contains 2,028,600 images of 1,150 identities. We manually labeled fine-grained attribute annotations both at environment level and identity level. <span id="S0.F1.8.2.2" class="ltx_text ltx_font_italic">First row</span>: With the same characters in different scenes, in each scene, a person can face toward a manually denoted direction. <span id="S0.F1.8.2.3" class="ltx_text ltx_font_italic">Second row</span>: Different characters in the same scene (<span id="S0.F1.8.2.4" class="ltx_text ltx_font_italic">i.e.</span> <span id="S0.F1.8.2.5" class="ltx_text ltx_font_italic">Scene #6</span>).</span></figcaption>
</figure>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Given a query image, person re-identification aims to match images of the same person across non-overlapping camera views, which has attracted lots of interests and attention in both academia and industry. Encouraged by the remarkable success of deep learning networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and the availability of re-ID datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, performance of person re-ID has been significantly boosted and made great progress. However, in practice, manually labelling a large diversity of training data is time-consuming and labor-intensive when directly deploying re-ID system to new scenarios. During intensive annotation, one needs to associate a pedestrian across different cameras, which is a difficult and laborious process as people might exhibit very different appearances in different cameras. In addition, there also has been an increasing concern over data safety and ethical issues,
<span id="S1.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span> DukeMTMC-reID <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> has been taken down due to privacy problem.
Some European countries already passed privacy-protecting laws <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> to prohibit the acquisition of personal data
without authorization, which makes collection of large-scale datasets extremely difficult.</p>
</div>
<figure id="S1.F2" class="ltx_figure">
<p id="S1.F2.1" class="ltx_p ltx_align_center"><span id="S1.F2.1.1" class="ltx_text"><img src="/html/2109.10498/assets/x2.png" id="S1.F2.1.1.g1" class="ltx_graphics ltx_img_landscape" width="438" height="128" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.4.2" class="ltx_text" style="font-size:90%;">System workflow of the AOST method, which is based on the dataset with fine-grained attribute annotations.</span></figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address this challenge, several works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> have proposed to employ off-the-shelf game engines to generate synthetic person images. For example, Barbosa <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> construct a SOMAset which contains 50 3D human models. SyRI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> provides 100 virtual humans rendered with 140 HDR environment maps. Wang <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> collect a RandPerson dataset with 1,801,816 synthesized person images.
Although these datasets provide considerable benefits of the data scale and enable some preliminary research in person re-ID, they are quite limited in both the attribute distribution and collected environment,
<span id="S1.p2.1.3" class="ltx_text ltx_font_italic">e.g.</span>, SyRI does not have concept of cameras, and SOMAset is uniformly distributed along an environment with clothing variations. In essence, these synthetic datasets either focus on independent attribute, or require annotators to carefully simulate specific scenes in detail, few datasets consider fine-grained attribute annotations or high-quality image resolution, which limits their scalability and diversity in terms of synthesized person. Another challenge we could observe is that, previous methods mainly focus on achieving competitive performance with large-scale data at the sacrifice of expensive time costs and intensive human labors, while neglect to perform efficient training with a higher quality of attribute annotations from millions of synthetic data. Considering the fact that
existing real-world datasets can be very different in terms of content and style, <span id="S1.p2.1.4" class="ltx_text ltx_font_italic">e.g.</span>, Market-1501 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> consists mostly of summer scenes captured in campus, while light in the CUHK03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> covers a wide range of indoor scenes, directly using all synthetic dataset for training will undoubtedly produce negative effects for domain adaptation, which makes it infeasible in practical scenarios.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In order to alleviate the problems identified above and facilitate the study of re-ID community, we start from two perspectives, namely
data and methodology. From the aspect of the data, we propose to collect data from synthetic world on the basis of GTA5 game engine, and
manually construct a <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Fine</span>-grained <span id="S1.p3.1.2" class="ltx_text ltx_font_bold">G</span>TA <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">P</span>erson <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">R</span>e-ID dataset called <span id="S1.p3.1.5" class="ltx_text ltx_font_bold ltx_font_italic">FineGPR</span>, which provides accurate and configurable annotations, <span id="S1.p3.1.6" class="ltx_text ltx_font_italic">e.g.</span>, <span id="S1.p3.1.7" class="ltx_text ltx_font_italic">viewpoint, weather, diverse and informative illumination and background</span>, as well as the various <span id="S1.p3.1.8" class="ltx_text ltx_font_italic">pedestrian attribute annotations at the identity level</span>. Compared to existing person re-ID datasets, <span id="S1.p3.1.9" class="ltx_text ltx_font_italic">FineGPR</span> is explicitly distinguished in richness, quality and diversity. It is worth noting that our data synthesis engine is still extendable to generate more data, which can be edited/extended not only for this study, but also for future research in re-ID community.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">From the aspect of methodology, we introduce a novel <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">A</span>ttribute <span id="S1.p4.1.2" class="ltx_text ltx_font_bold">O</span>ptimization and <span id="S1.p4.1.3" class="ltx_text ltx_font_bold">S</span>tyle <span id="S1.p4.1.4" class="ltx_text ltx_font_bold">T</span>ransfer pipeline <span id="S1.p4.1.5" class="ltx_text ltx_font_bold">AOST</span> to perform training on re-ID task in a data-efficient manner. AOST can dynamically select samples which approximates the attribute distribution in real domain. As illustrated in Fig. <a href="#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, the proposed AOST contains Stage-I (<span id="S1.p4.1.6" class="ltx_text ltx_font_italic">Attribute Optimization</span>) and Stage-II (<span id="S1.p4.1.7" class="ltx_text ltx_font_italic">Style Transfer</span>). Firstly, Stage-I is adopted to mine the attribute distribution of real
domain, following by the Stage-II to reduce the intrinsic gap between synthetic
and real domain. Finally, the transferred data are adopted for performing training on downstream vision task.
This is the first time as far as we know, to greatly promote efficient training from millions of synthetic data on re-ID task,
experiments across diverse datasets suggest that the “<span id="S1.p4.1.8" class="ltx_text ltx_font_bold ltx_font_italic">less-is-more</span>” principle is highly effective in practice.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our contributions can be summarized into three aspects:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We open source the largest person dataset with fine-grained attribute annotations for the community without privacy and
ethics concerns.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Based on it, we propose a two-stage pipeline AOST to learn from fine-grained attributes, then eliminate style differences between synthetic and real domain for more efficient training.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Extensive experiments conducted on benchmarks show that our <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_italic">FineGPR</span> is promising and can achieve competitive performance with AOST in re-ID task.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.T1" class="ltx_table">
<table id="S1.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S1.T1.2.1" class="ltx_tr">
<td id="S1.T1.2.1.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="12">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span><span id="S1.T1.2.1.1.1" class="ltx_text" style="font-size:80%;">                                                           Dataset</span>
</td>
<td id="S1.T1.2.1.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.2.1" class="ltx_text" style="font-size:80%;">#Identities</span></td>
<td id="S1.T1.2.1.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.3.1" class="ltx_text" style="font-size:80%;">#Bboxes</span></td>
<td id="S1.T1.2.1.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.4.1" class="ltx_text" style="font-size:80%;">#Cameras</span></td>
<td id="S1.T1.2.1.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.5.1" class="ltx_text" style="font-size:80%;">#Wea.</span></td>
<td id="S1.T1.2.1.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.6.1" class="ltx_text" style="font-size:80%;">#Illum.</span></td>
<td id="S1.T1.2.1.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.7.1" class="ltx_text" style="font-size:80%;">#Scenes</span></td>
<td id="S1.T1.2.1.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.8.1" class="ltx_text" style="font-size:80%;">#ID-level</span></td>
<td id="S1.T1.2.1.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.9.1" class="ltx_text" style="font-size:80%;">#Resolution</span></td>
<td id="S1.T1.2.1.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.10.1" class="ltx_text" style="font-size:80%;">Hard samples</span></td>
<td id="S1.T1.2.1.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.1.11.1" class="ltx_text" style="font-size:80%;">Ethical Considerations</span></td>
</tr>
<tr id="S1.T1.2.2" class="ltx_tr">
<td id="S1.T1.2.2.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;" rowspan="3"><span id="S1.T1.2.2.1.1" class="ltx_text" style="font-size:80%;">Real</span></td>
<td id="S1.T1.2.2.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S1.T1.2.2.2.1" class="ltx_text" style="font-size:80%;">Market-1501 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.2.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S1.T1.2.2.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.2.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.3.1" class="ltx_text" style="font-size:80%;">1,501</span></td>
<td id="S1.T1.2.2.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.4.1" class="ltx_text" style="font-size:80%;">32,668</span></td>
<td id="S1.T1.2.2.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.5.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="S1.T1.2.2.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.6.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.2.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.7.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.2.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.8.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.2.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.9.1" class="ltx_text" style="font-size:80%;color:#008000;">✔</span></td>
<td id="S1.T1.2.2.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.10.1" class="ltx_text" style="font-size:80%;">low</span></td>
<td id="S1.T1.2.2.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.11.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="S1.T1.2.2.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.2.12.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S1.T1.2.3" class="ltx_tr">
<td id="S1.T1.2.3.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S1.T1.2.3.1.1" class="ltx_text" style="font-size:80%;">CUHK03 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.3.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S1.T1.2.3.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.3.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.2.1" class="ltx_text" style="font-size:80%;">1,467</span></td>
<td id="S1.T1.2.3.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.3.1" class="ltx_text" style="font-size:80%;">14,096</span></td>
<td id="S1.T1.2.3.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.4.1" class="ltx_text" style="font-size:80%;">2</span></td>
<td id="S1.T1.2.3.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.5.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.3.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.6.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.3.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.7.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.3.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.8.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.3.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.9.1" class="ltx_text" style="font-size:80%;">low</span></td>
<td id="S1.T1.2.3.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.10.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="S1.T1.2.3.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.3.11.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S1.T1.2.4" class="ltx_tr">
<td id="S1.T1.2.4.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S1.T1.2.4.1.1" class="ltx_text" style="font-size:80%;">MSMT17 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.4.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S1.T1.2.4.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.4.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.2.1" class="ltx_text" style="font-size:80%;">4,101</span></td>
<td id="S1.T1.2.4.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.3.1" class="ltx_text" style="font-size:80%;">126,441</span></td>
<td id="S1.T1.2.4.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.4.1" class="ltx_text" style="font-size:80%;">15</span></td>
<td id="S1.T1.2.4.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.5.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.4.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.6.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.4.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.7.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.4.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.8.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.4.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.9.1" class="ltx_text" style="font-size:80%;">vary</span></td>
<td id="S1.T1.2.4.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.10.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="S1.T1.2.4.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.4.11.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S1.T1.2.5" class="ltx_tr">
<td id="S1.T1.2.5.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" rowspan="5"><span id="S1.T1.2.5.1.1" class="ltx_text" style="font-size:80%;">Synthetic</span></td>
<td id="S1.T1.2.5.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S1.T1.2.5.2.1" class="ltx_text" style="font-size:80%;">SOMAset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S1.T1.2.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.5.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.3.1" class="ltx_text" style="font-size:80%;">50</span></td>
<td id="S1.T1.2.5.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.4.1" class="ltx_text" style="font-size:80%;">100,000</span></td>
<td id="S1.T1.2.5.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.5.1" class="ltx_text" style="font-size:80%;">250</span></td>
<td id="S1.T1.2.5.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.6.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.5.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.7.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.5.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.8.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.5.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.9.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.5.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.10.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S1.T1.2.5.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.11.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="S1.T1.2.5.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.5.12.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S1.T1.2.6" class="ltx_tr">
<td id="S1.T1.2.6.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S1.T1.2.6.1.1" class="ltx_text" style="font-size:80%;">SyRI </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.6.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S1.T1.2.6.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.6.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.2.1" class="ltx_text" style="font-size:80%;">100</span></td>
<td id="S1.T1.2.6.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.3.1" class="ltx_text" style="font-size:80%;">1,680,000</span></td>
<td id="S1.T1.2.6.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.4.1" class="ltx_text" style="font-size:80%;">280</span></td>
<td id="S1.T1.2.6.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.5.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.6.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.6.1" class="ltx_text" style="font-size:80%;">140</span></td>
<td id="S1.T1.2.6.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.7.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.6.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.8.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.6.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.9.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S1.T1.2.6.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.10.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="S1.T1.2.6.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.6.11.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S1.T1.2.7" class="ltx_tr">
<td id="S1.T1.2.7.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S1.T1.2.7.1.1" class="ltx_text" style="font-size:80%;">PersonX </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.7.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S1.T1.2.7.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.7.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.2.1" class="ltx_text" style="font-size:80%;">1,266</span></td>
<td id="S1.T1.2.7.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.3.1" class="ltx_text" style="font-size:80%;">273,456</span></td>
<td id="S1.T1.2.7.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.4.1" class="ltx_text" style="font-size:80%;">6</span></td>
<td id="S1.T1.2.7.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.5.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.7.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.6.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.7.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.7.1" class="ltx_text" style="font-size:80%;">3</span></td>
<td id="S1.T1.2.7.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.8.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.7.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.9.1" class="ltx_text" style="font-size:80%;">vary</span></td>
<td id="S1.T1.2.7.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.10.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="S1.T1.2.7.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.7.11.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S1.T1.2.8" class="ltx_tr">
<td id="S1.T1.2.8.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S1.T1.2.8.1.1" class="ltx_text" style="font-size:80%;">Unreal </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.8.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S1.T1.2.8.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.8.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.2.1" class="ltx_text" style="font-size:80%;">3,000</span></td>
<td id="S1.T1.2.8.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.3.1" class="ltx_text" style="font-size:80%;">120,000</span></td>
<td id="S1.T1.2.8.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.4.1" class="ltx_text" style="font-size:80%;">34</span></td>
<td id="S1.T1.2.8.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.5.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.8.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.6.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.8.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.7.1" class="ltx_text" style="font-size:80%;">4</span></td>
<td id="S1.T1.2.8.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.8.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.8.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.9.1" class="ltx_text" style="font-size:80%;">low</span></td>
<td id="S1.T1.2.8.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.10.1" class="ltx_text" style="font-size:80%;">Many</span></td>
<td id="S1.T1.2.8.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.8.11.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S1.T1.2.9" class="ltx_tr">
<td id="S1.T1.2.9.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;">
<span id="S1.T1.2.9.1.1" class="ltx_text" style="font-size:80%;">RandPerson </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S1.T1.2.9.1.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S1.T1.2.9.1.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S1.T1.2.9.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.2.1" class="ltx_text" style="font-size:80%;">8,000</span></td>
<td id="S1.T1.2.9.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.3.1" class="ltx_text" style="font-size:80%;">1,801,816</span></td>
<td id="S1.T1.2.9.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.4.1" class="ltx_text" style="font-size:80%;">19</span></td>
<td id="S1.T1.2.9.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.5.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.9.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.6.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.9.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.7.1" class="ltx_text" style="font-size:80%;">11</span></td>
<td id="S1.T1.2.9.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.8.1" class="ltx_text" style="font-size:80%;color:#FF0000;">✘</span></td>
<td id="S1.T1.2.9.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.9.1" class="ltx_text" style="font-size:80%;">low</span></td>
<td id="S1.T1.2.9.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.10.1" class="ltx_text" style="font-size:80%;">No</span></td>
<td id="S1.T1.2.9.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.9.11.1" class="ltx_text" style="font-size:80%;">No</span></td>
</tr>
<tr id="S1.T1.2.10" class="ltx_tr">
<td id="S1.T1.2.10.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.10.2" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.2.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:80%;">FineGPR<span id="S1.T1.2.10.2.1.1" class="ltx_text ltx_font_upright"> (Ours)</span></span></td>
<td id="S1.T1.2.10.3" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">1,150</span></td>
<td id="S1.T1.2.10.4" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">2,028,600</span></td>
<td id="S1.T1.2.10.5" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">36</span></td>
<td id="S1.T1.2.10.6" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">7</span></td>
<td id="S1.T1.2.10.7" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;">7</span></td>
<td id="S1.T1.2.10.8" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;">9</span></td>
<td id="S1.T1.2.10.9" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.9.1" class="ltx_text" style="font-size:80%;color:#008000;">✔</span></td>
<td id="S1.T1.2.10.10" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;">high</span></td>
<td id="S1.T1.2.10.11" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Many</span></td>
<td id="S1.T1.2.10.12" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S1.T1.2.10.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Addressed</span></td>
</tr>
<tr id="S1.T1.2.11" class="ltx_tr">
<td id="S1.T1.2.11.1" class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_r" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S1.T1.2.11.2" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.3" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.4" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.5" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.6" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.7" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.8" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.9" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.10" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.11" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
<td id="S1.T1.2.11.12" class="ltx_td" style="padding-left:2.0pt;padding-right:2.0pt;"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.5.1.1" class="ltx_text" style="font-size:113%;">Table 1</span>: </span><span id="S1.T1.6.2" class="ltx_text" style="font-size:113%;">Comparison of some real-world and synthetic person re-ID datasets. In particular, “#Wea.” , “#Illum.” , “#Scenes” and “ID-level” indicate whether dataset has human-annotated labels in terms of weather, illumination, background and ID-level attributes, respectively.</span></figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Person re-ID Methods</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">In the field of person re-ID, early works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> either concentrate on hand-crafted feature or low-level semantic feature. Unfortunately, these methods always fail to produce competitive results because of their limited discriminative learning ability. Recently, benefited from the advances of deep neural networks, person re-ID performance in supervised learning has been significantly boosted to a new level <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>, which learned robust feature extraction and reliable metric learning in an end-to-end manner. Typically, person re-ID model can be trained with the identification loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>, contrastive loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> and triplet loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Recently, a strong baseline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for re-ID is employed to extract the discriminative feature, which has been proved to have great potential to learn a robust and discriminative model in person re-ID models.
Besides, several literatures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> focus on the image-level to allow different domains to have similar feature distributions, or adopt an adversarial domain adaptation approach to mitigate the distribution shift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which has attracted considerable attention from various fields in re-ID community.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Person re-ID Datasets</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Being the foundation of more sophisticated re-ID techniques, the pursuit of better datasets never stops in the area of person re-ID. Early attempts could be traced back to VIPeR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, ETHZ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and RAiD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. More challenging datasets are proposed subsequently, including Market-1501 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite>, CUHK03 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, MSMT17 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite>, <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">etc.</span> However, labelling such a large-scale real-world dataset is labor-intensive and time-consuming, sometimes there even exists security and privacy problems.
Besides, all of these datasets only have limited attribute distribution and are lack of diversity. As the performance gain is gradually saturated on the above datasets, newly large-scale datasets are needed urgently to further boost re-ID performance.
Recently, leveraging synthetic data is an effective idea to alleviate the reliance on large-scale real-world datasets.
This strategy has been applied in various computer vision tasks, <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">e.g.</span>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, crowd counting <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
In the person re-ID community,
many re-ID methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> have proposed to take advantage of game engine to construct large-scale synthetic re-ID datasets, which can be used to pre-train or fine-tune CNN network.
For example, Barbosa <span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> propose a synthetic dataset SOMAset created by photo-realistic human body generation software to enrich the diversity. Recently, Wang <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> collect a virtual dataset RandPerson with 3D characters containing 1,801,816 synthetic images of 8,000 identities. However, these datasets are either in a small scale or lack of diversity, few of them provide rich attribute annotations, which cannot satisfy the need of attribute learning in person re-ID task.
So new fine-grained annotated datasets are urgently needed.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The <span id="S3.1.1" class="ltx_text ltx_font_italic">FineGPR</span> Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we describe <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">FineGPR</span>, a new dataset with high-quality annotations and multiple attribute distributions to re-ID community. Below we at first review the process of constructing and annotation collection, then present an analysis over
dataset statistics. We show some sample images from the proposed <span id="S3.p1.1.2" class="ltx_text ltx_font_italic">FineGPR</span> dataset in Fig. <a href="#S0.F1" title="Figure 1 ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<p id="S3.F3.1" class="ltx_p ltx_align_center"><span id="S3.F3.1.1" class="ltx_text"><img src="/html/2109.10498/assets/x3.png" id="S3.F3.1.1.g1" class="ltx_graphics ltx_img_landscape" width="415" height="112" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.5.2" class="ltx_text" style="font-size:90%;">The distributions of attributes at the identity level on <span id="S3.F3.5.2.1" class="ltx_text ltx_font_italic">FineGPR</span>. The left figure shows the numbers of IDs and category cloud for each attribute.
The middle and right pies illustrate the distribution of the colors of upper-body and low-body clothes respectively.</span></figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset Collection</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">FineGPR</span> dataset is collected from a popular game engine called the Grand Theft Auto V (GTA5). Practically, we create a synthetic controllable world containing 2,028,600 synthesized person images of 1,150 identities. Images in this dataset generally contain different attributes in a large scope, <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_italic">e.g.</span>, <span id="S3.SS1.p1.1.3" class="ltx_text ltx_font_bold ltx_font_italic">Viewpoint</span>, <span id="S3.SS1.p1.1.4" class="ltx_text ltx_font_bold ltx_font_italic">Weather</span>, <span id="S3.SS1.p1.1.5" class="ltx_text ltx_font_bold ltx_font_italic">Illumination</span>, <span id="S3.SS1.p1.1.6" class="ltx_text ltx_font_bold ltx_font_italic">Background</span> and <span id="S3.SS1.p1.1.7" class="ltx_text ltx_font_bold ltx_font_italic">ID-level annotations</span>, also including many hard samples with occlusion. It is worth noting that, all images are simultaneously captured by 36 non-overlapping cameras with a high resolution and image quality.
In the process of image generation, each person walks along a schedule route, and cameras are set up and fixed at the chosen locations. As a controllable system, it can satisfy various data requirements in a fine-grained fashion.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Properties of <span id="S3.SS2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">FineGPR</span> dataset</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The goal of our <span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_italic">FineGPR</span> dataset is to introduce a new challenging benchmark with high-quality annotations and multiple attribute distribution to re-ID community. To the best of our knowledge, this is the first large-scale person dataset with over 4 environment-level attributes and 13 ID-level attribute annotations.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Identities.</span> According to the Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">FineGPR</span> contains 1,150 hand-crafted identities including females and males, with resolution of <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="200\times 480" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">200</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml">480</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">200</cn><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3">480</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">200\times 480</annotation></semantics></math>.
To ensure diversity, we cropped human region with different angles. As shown in Fig. <a href="#S0.F1" title="Figure 1 ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (<span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_italic">second row</span>), different person has different body shape, clothing, hairstyle, and the motion can be randomly set as walking, running, standing and so on.
Particularly, the clothes of these characters include jeans, pants, shorts, skirts, T-shirts, dress shirts, <span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_italic">etc.</span>, and some of these identities have a backpack, shoulder bag, and wear glasses or hat. In total, we manually annotate the <span id="S3.SS2.p2.1.5" class="ltx_text ltx_font_italic">FineGPR</span> with 13 different pedestrian attributes at the identity level (<span id="S3.SS2.p2.1.6" class="ltx_text ltx_font_italic">e.g.</span>, wearing dress or not), the distribution diagram is demonstrated in Fig. <a href="#S3.F3" title="Figure 3 ‣ 3 The FineGPR Dataset ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.4" class="ltx_p"><span id="S3.SS2.p3.4.1" class="ltx_text ltx_font_bold">Viewpoint.</span>
We construct the image exemplars under specified viewpoints. Those images are randomly sampled during normal walking, running, <span id="S3.SS2.p3.4.2" class="ltx_text ltx_font_italic">etc.</span> Formally, a person image is sampled every <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="10^{\circ}" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><msup id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml"><mn id="S3.SS2.p3.1.m1.1.1.2" xref="S3.SS2.p3.1.m1.1.1.2.cmml">10</mn><mo id="S3.SS2.p3.1.m1.1.1.3" xref="S3.SS2.p3.1.m1.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><apply id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p3.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.1.1.2">10</cn><compose id="S3.SS2.p3.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">10^{\circ}</annotation></semantics></math> from <math id="S3.SS2.p3.2.m2.1" class="ltx_Math" alttext="0^{\circ}" display="inline"><semantics id="S3.SS2.p3.2.m2.1a"><msup id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mn id="S3.SS2.p3.2.m2.1.1.2" xref="S3.SS2.p3.2.m2.1.1.2.cmml">0</mn><mo id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2">0</cn><compose id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">0^{\circ}</annotation></semantics></math> to <math id="S3.SS2.p3.3.m3.1" class="ltx_Math" alttext="350^{\circ}" display="inline"><semantics id="S3.SS2.p3.3.m3.1a"><msup id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml"><mn id="S3.SS2.p3.3.m3.1.1.2" xref="S3.SS2.p3.3.m3.1.1.2.cmml">350</mn><mo id="S3.SS2.p3.3.m3.1.1.3" xref="S3.SS2.p3.3.m3.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><apply id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.3.m3.1.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">superscript</csymbol><cn type="integer" id="S3.SS2.p3.3.m3.1.1.2.cmml" xref="S3.SS2.p3.3.m3.1.1.2">350</cn><compose id="S3.SS2.p3.3.m3.1.1.3.cmml" xref="S3.SS2.p3.3.m3.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">350^{\circ}</annotation></semantics></math>. (36 different types of viewpoints in total). There are 49 images for each viewpoint of an identity in the entire <span id="S3.SS2.p3.4.3" class="ltx_text ltx_font_italic">FineGPR</span>, so each person has 1,764 (<math id="S3.SS2.p3.4.m4.1" class="ltx_Math" alttext="49\times 36" display="inline"><semantics id="S3.SS2.p3.4.m4.1a"><mrow id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mn id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">49</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p3.4.m4.1.1.1" xref="S3.SS2.p3.4.m4.1.1.1.cmml">×</mo><mn id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">36</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><times id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1.1"></times><cn type="integer" id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">49</cn><cn type="integer" id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">36</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">49\times 36</annotation></semantics></math>) images in total.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Weather.</span>
Currently, the proposed <span id="S3.SS2.p4.1.2" class="ltx_text ltx_font_italic">FineGPR</span> has 7 different weather conditions, including Sunny, Clouds, Overcast, Foggy, Neutral, Blizzard and Snowlight. It is worth mentioning that the number of instances in each weather condition is the same, but not a natural heavy-tail distribution, which makes it adaptable to various real-world scenarios.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Illumination.</span>
Illumination is another critical factor that contributes to the success of generalizable re-ID, which consists of 7 different types of illumination, <span id="S3.SS2.p5.1.2" class="ltx_text ltx_font_italic">e.g.</span> midnight (time period during 23:00–4:00 in 24 hours a day.), dawn (4:00–6:00), forenoon (6:00–11:00), noon (11:00–13:00), afternoon (13:00–18:00), dusk(18:00–20:00) and night (20:00–23:00). Parameters like time setting can be modified manually for each illumination type. By editing the values of these terms, various kinds of illumination environments can be created.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p"><span id="S3.SS2.p6.1.1" class="ltx_text ltx_font_bold">Background.</span>
GTA5 has a very large environment map, including thousands of realistic urban areas and wild scenes. From now, 9 different scenes are selected to represent real-world scenarios with annotations, <span id="S3.SS2.p6.1.2" class="ltx_text ltx_font_italic">e.g.</span> street, mall, school, park and mountain, <span id="S3.SS2.p6.1.3" class="ltx_text ltx_font_italic">etc.</span>, which are distributed evenly across all identities. The different scenes are shown in Fig. <a href="#S0.F1" title="Figure 1 ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (<span id="S3.SS2.p6.1.4" class="ltx_text ltx_font_italic">first row</span>). More additional details related to our <span id="S3.SS2.p6.1.5" class="ltx_text ltx_font_italic">FineGPR</span> can be found in the Supplementary Material.</p>
</div>
<div id="S3.SS2.p7" class="ltx_para">
<p id="S3.SS2.p7.1" class="ltx_p"><span id="S3.SS2.p7.1.1" class="ltx_text ltx_font_bold">Ethical Considerations.</span>
People-centric datasets pose some challenges of data privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and intersectional accuracy disparities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. To address such concerns, our dataset were created with
careful attention to ethical
questions, which we encountered
throughout our work. Access to our dataset will be provided
for research purposes only and with restrictions on redistribution. Furthermore,
we are very cautious of annotation procedure of <span id="S3.SS2.p7.1.2" class="ltx_text ltx_font_italic">FineGPR</span> dataset to avoid the social and ethical implications.
As for re-ID system, governments and officials must establish strict regulations to control the usage of this technology since it mainly relies on (not all) surveillance data. Motivated by this,
we do not consider the dataset for developing non-research systems unless further professional processing or augmentation.</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<p id="S3.F4.1" class="ltx_p ltx_align_center"><span id="S3.F4.1.1" class="ltx_text"><img src="/html/2109.10498/assets/x4.png" id="S3.F4.1.1.g1" class="ltx_graphics ltx_img_landscape" width="415" height="192" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.5.2" class="ltx_text" style="font-size:90%;">The two-stage pipeline <span id="S3.F4.5.2.1" class="ltx_text ltx_font_bold">AOST</span> to learn attribute distribution of target domain. Firstly, we learn attribute distribution of real domain on the basis of XGBoost &amp; PSO learning system. Secondly, we perform style transfer to enhance the reality of optimal dataset. Finally, the transferred data are adopted for downstream re-ID task.</span></figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure">
<p id="S3.F5.1" class="ltx_p ltx_align_center"><span id="S3.F5.1.1" class="ltx_text"><img src="/html/2109.10498/assets/x5.png" id="S3.F5.1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="97" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F5.4.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S3.F5.5.2" class="ltx_text" style="font-size:90%;">Some visual examples of collected <span id="S3.F5.5.2.1" class="ltx_text ltx_font_bold">MSCO</span> dataset.</span></figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology of Proposed AOST</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we design an effective training strategy AOST to directly select samples on the basis of synthetic <span id="S4.p1.1.1" class="ltx_text ltx_font_italic">FineGPR</span> for
initializing the re-ID backbone. And the overall framework is illustrated in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Properties of FineGPR dataset ‣ 3 The FineGPR Dataset ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, which includes two stages: Attribute Optimization and Style Transfer.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.2" class="ltx_p"><span id="S4.p2.2.1" class="ltx_text ltx_font_bold">Attribute Optimization.</span>
Intuitively, since <span id="S4.p2.2.2" class="ltx_text ltx_font_italic">FineGPR</span> is a large-attribute-range dataset, using entire <span id="S4.p2.2.3" class="ltx_text ltx_font_italic">FineGPR</span> for training is time-consuming and low-efficient. To further exploit the potential of <span id="S4.p2.2.4" class="ltx_text ltx_font_italic">FineGPR</span> and promote the training efficiency, we introduce a novel strategy to learn some representative attributes with prior target knowledge. Following the procedure in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, we adopt a widely used backbone VGG-19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> to obtain the style distance <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{style}}" display="inline"><semantics id="S4.p2.1.m1.1a"><msub id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">𝒟</mi><mtext id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3a.cmml">style</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">𝒟</ci><ci id="S4.p2.1.m1.1.1.3a.cmml" xref="S4.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">style</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\mathcal{D}_{\text{style}}</annotation></semantics></math> and content distance <math id="S4.p2.2.m2.1" class="ltx_Math" alttext="\mathcal{D}_{\text{content}}" display="inline"><semantics id="S4.p2.2.m2.1a"><msub id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">𝒟</mi><mtext id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3a.cmml">content</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1">subscript</csymbol><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">𝒟</ci><ci id="S4.p2.2.m2.1.1.3a.cmml" xref="S4.p2.2.m2.1.1.3"><mtext mathsize="70%" id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">content</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">\mathcal{D}_{\text{content}}</annotation></semantics></math> respectively, which are formulated as:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.3" class="ltx_Math" alttext="\mathcal{D}_{\text{content }}=\frac{1}{2}\sum_{i,j}\left(F_{ij}^{l}-P_{ij}^{l}\right)^{2}" display="block"><semantics id="S4.E1.m1.3a"><mrow id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml"><msub id="S4.E1.m1.3.3.3" xref="S4.E1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.3.3.3.2" xref="S4.E1.m1.3.3.3.2.cmml">𝒟</mi><mtext id="S4.E1.m1.3.3.3.3" xref="S4.E1.m1.3.3.3.3a.cmml">content </mtext></msub><mo id="S4.E1.m1.3.3.2" xref="S4.E1.m1.3.3.2.cmml">=</mo><mrow id="S4.E1.m1.3.3.1" xref="S4.E1.m1.3.3.1.cmml"><mfrac id="S4.E1.m1.3.3.1.3" xref="S4.E1.m1.3.3.1.3.cmml"><mn id="S4.E1.m1.3.3.1.3.2" xref="S4.E1.m1.3.3.1.3.2.cmml">1</mn><mn id="S4.E1.m1.3.3.1.3.3" xref="S4.E1.m1.3.3.1.3.3.cmml">2</mn></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.3.1.2" xref="S4.E1.m1.3.3.1.2.cmml">​</mo><mrow id="S4.E1.m1.3.3.1.1" xref="S4.E1.m1.3.3.1.1.cmml"><munder id="S4.E1.m1.3.3.1.1.2" xref="S4.E1.m1.3.3.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E1.m1.3.3.1.1.2.2" xref="S4.E1.m1.3.3.1.1.2.2.cmml">∑</mo><mrow id="S4.E1.m1.2.2.2.4" xref="S4.E1.m1.2.2.2.3.cmml"><mi id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml">i</mi><mo id="S4.E1.m1.2.2.2.4.1" xref="S4.E1.m1.2.2.2.3.cmml">,</mo><mi id="S4.E1.m1.2.2.2.2" xref="S4.E1.m1.2.2.2.2.cmml">j</mi></mrow></munder><msup id="S4.E1.m1.3.3.1.1.1" xref="S4.E1.m1.3.3.1.1.1.cmml"><mrow id="S4.E1.m1.3.3.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.3.3.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml"><msubsup id="S4.E1.m1.3.3.1.1.1.1.1.1.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml">F</mi><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3.cmml">j</mi></mrow><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.2.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.3.cmml">l</mi></msubsup><mo id="S4.E1.m1.3.3.1.1.1.1.1.1.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S4.E1.m1.3.3.1.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.2.cmml">P</mi><mrow id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.2" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.1" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.3.cmml">j</mi></mrow><mi id="S4.E1.m1.3.3.1.1.1.1.1.1.3.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml">l</mi></msubsup></mrow><mo id="S4.E1.m1.3.3.1.1.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S4.E1.m1.3.3.1.1.1.3" xref="S4.E1.m1.3.3.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.3b"><apply id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3"><eq id="S4.E1.m1.3.3.2.cmml" xref="S4.E1.m1.3.3.2"></eq><apply id="S4.E1.m1.3.3.3.cmml" xref="S4.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.3.1.cmml" xref="S4.E1.m1.3.3.3">subscript</csymbol><ci id="S4.E1.m1.3.3.3.2.cmml" xref="S4.E1.m1.3.3.3.2">𝒟</ci><ci id="S4.E1.m1.3.3.3.3a.cmml" xref="S4.E1.m1.3.3.3.3"><mtext mathsize="70%" id="S4.E1.m1.3.3.3.3.cmml" xref="S4.E1.m1.3.3.3.3">content </mtext></ci></apply><apply id="S4.E1.m1.3.3.1.cmml" xref="S4.E1.m1.3.3.1"><times id="S4.E1.m1.3.3.1.2.cmml" xref="S4.E1.m1.3.3.1.2"></times><apply id="S4.E1.m1.3.3.1.3.cmml" xref="S4.E1.m1.3.3.1.3"><divide id="S4.E1.m1.3.3.1.3.1.cmml" xref="S4.E1.m1.3.3.1.3"></divide><cn type="integer" id="S4.E1.m1.3.3.1.3.2.cmml" xref="S4.E1.m1.3.3.1.3.2">1</cn><cn type="integer" id="S4.E1.m1.3.3.1.3.3.cmml" xref="S4.E1.m1.3.3.1.3.3">2</cn></apply><apply id="S4.E1.m1.3.3.1.1.cmml" xref="S4.E1.m1.3.3.1.1"><apply id="S4.E1.m1.3.3.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.2">subscript</csymbol><sum id="S4.E1.m1.3.3.1.1.2.2.cmml" xref="S4.E1.m1.3.3.1.1.2.2"></sum><list id="S4.E1.m1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.4"><ci id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1">𝑖</ci><ci id="S4.E1.m1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2">𝑗</ci></list></apply><apply id="S4.E1.m1.3.3.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1">superscript</csymbol><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1"><minus id="S4.E1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.1"></minus><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.2">𝐹</ci><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3"><times id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.1"></times><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.2">𝑖</ci><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.2.3.3">𝑗</ci></apply></apply><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.2.3">𝑙</ci></apply><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.2">𝑃</ci><apply id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3"><times id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.1"></times><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.2">𝑖</ci><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.2.3.3">𝑗</ci></apply></apply><ci id="S4.E1.m1.3.3.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.3.3.1.1.1.1.1.1.3.3">𝑙</ci></apply></apply><cn type="integer" id="S4.E1.m1.3.3.1.1.1.3.cmml" xref="S4.E1.m1.3.3.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.3c">\mathcal{D}_{\text{content }}=\frac{1}{2}\sum_{i,j}\left(F_{ij}^{l}-P_{ij}^{l}\right)^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.3" class="ltx_Math" alttext="\mathcal{D}_{\text{style}}=\sum_{l=0}^{L}w_{l}\frac{1}{4N_{l}^{2}M_{l}^{2}}\sum_{i,j}\left(G_{ij}^{l}-A_{ij}^{l}\right)^{2}" display="block"><semantics id="S4.E2.m1.3a"><mrow id="S4.E2.m1.3.3" xref="S4.E2.m1.3.3.cmml"><msub id="S4.E2.m1.3.3.3" xref="S4.E2.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E2.m1.3.3.3.2" xref="S4.E2.m1.3.3.3.2.cmml">𝒟</mi><mtext id="S4.E2.m1.3.3.3.3" xref="S4.E2.m1.3.3.3.3a.cmml">style</mtext></msub><mo rspace="0.111em" id="S4.E2.m1.3.3.2" xref="S4.E2.m1.3.3.2.cmml">=</mo><mrow id="S4.E2.m1.3.3.1" xref="S4.E2.m1.3.3.1.cmml"><munderover id="S4.E2.m1.3.3.1.2" xref="S4.E2.m1.3.3.1.2.cmml"><mo movablelimits="false" id="S4.E2.m1.3.3.1.2.2.2" xref="S4.E2.m1.3.3.1.2.2.2.cmml">∑</mo><mrow id="S4.E2.m1.3.3.1.2.2.3" xref="S4.E2.m1.3.3.1.2.2.3.cmml"><mi id="S4.E2.m1.3.3.1.2.2.3.2" xref="S4.E2.m1.3.3.1.2.2.3.2.cmml">l</mi><mo id="S4.E2.m1.3.3.1.2.2.3.1" xref="S4.E2.m1.3.3.1.2.2.3.1.cmml">=</mo><mn id="S4.E2.m1.3.3.1.2.2.3.3" xref="S4.E2.m1.3.3.1.2.2.3.3.cmml">0</mn></mrow><mi id="S4.E2.m1.3.3.1.2.3" xref="S4.E2.m1.3.3.1.2.3.cmml">L</mi></munderover><mrow id="S4.E2.m1.3.3.1.1" xref="S4.E2.m1.3.3.1.1.cmml"><msub id="S4.E2.m1.3.3.1.1.3" xref="S4.E2.m1.3.3.1.1.3.cmml"><mi id="S4.E2.m1.3.3.1.1.3.2" xref="S4.E2.m1.3.3.1.1.3.2.cmml">w</mi><mi id="S4.E2.m1.3.3.1.1.3.3" xref="S4.E2.m1.3.3.1.1.3.3.cmml">l</mi></msub><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.2" xref="S4.E2.m1.3.3.1.1.2.cmml">​</mo><mfrac id="S4.E2.m1.3.3.1.1.4" xref="S4.E2.m1.3.3.1.1.4.cmml"><mn id="S4.E2.m1.3.3.1.1.4.2" xref="S4.E2.m1.3.3.1.1.4.2.cmml">1</mn><mrow id="S4.E2.m1.3.3.1.1.4.3" xref="S4.E2.m1.3.3.1.1.4.3.cmml"><mn id="S4.E2.m1.3.3.1.1.4.3.2" xref="S4.E2.m1.3.3.1.1.4.3.2.cmml">4</mn><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.4.3.1" xref="S4.E2.m1.3.3.1.1.4.3.1.cmml">​</mo><msubsup id="S4.E2.m1.3.3.1.1.4.3.3" xref="S4.E2.m1.3.3.1.1.4.3.3.cmml"><mi id="S4.E2.m1.3.3.1.1.4.3.3.2.2" xref="S4.E2.m1.3.3.1.1.4.3.3.2.2.cmml">N</mi><mi id="S4.E2.m1.3.3.1.1.4.3.3.2.3" xref="S4.E2.m1.3.3.1.1.4.3.3.2.3.cmml">l</mi><mn id="S4.E2.m1.3.3.1.1.4.3.3.3" xref="S4.E2.m1.3.3.1.1.4.3.3.3.cmml">2</mn></msubsup><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.4.3.1a" xref="S4.E2.m1.3.3.1.1.4.3.1.cmml">​</mo><msubsup id="S4.E2.m1.3.3.1.1.4.3.4" xref="S4.E2.m1.3.3.1.1.4.3.4.cmml"><mi id="S4.E2.m1.3.3.1.1.4.3.4.2.2" xref="S4.E2.m1.3.3.1.1.4.3.4.2.2.cmml">M</mi><mi id="S4.E2.m1.3.3.1.1.4.3.4.2.3" xref="S4.E2.m1.3.3.1.1.4.3.4.2.3.cmml">l</mi><mn id="S4.E2.m1.3.3.1.1.4.3.4.3" xref="S4.E2.m1.3.3.1.1.4.3.4.3.cmml">2</mn></msubsup></mrow></mfrac><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.2a" xref="S4.E2.m1.3.3.1.1.2.cmml">​</mo><mrow id="S4.E2.m1.3.3.1.1.1" xref="S4.E2.m1.3.3.1.1.1.cmml"><munder id="S4.E2.m1.3.3.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.2.cmml"><mo movablelimits="false" rspace="0em" id="S4.E2.m1.3.3.1.1.1.2.2" xref="S4.E2.m1.3.3.1.1.1.2.2.cmml">∑</mo><mrow id="S4.E2.m1.2.2.2.4" xref="S4.E2.m1.2.2.2.3.cmml"><mi id="S4.E2.m1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.cmml">i</mi><mo id="S4.E2.m1.2.2.2.4.1" xref="S4.E2.m1.2.2.2.3.cmml">,</mo><mi id="S4.E2.m1.2.2.2.2" xref="S4.E2.m1.2.2.2.2.cmml">j</mi></mrow></munder><msup id="S4.E2.m1.3.3.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.cmml"><mrow id="S4.E2.m1.3.3.1.1.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S4.E2.m1.3.3.1.1.1.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m1.3.3.1.1.1.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml">G</mi><mrow id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.1" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.1.cmml">​</mo><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.3.cmml">j</mi></mrow><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.3.cmml">l</mi></msubsup><mo id="S4.E2.m1.3.3.1.1.1.1.1.1.1.1" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml">−</mo><msubsup id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.2.cmml">A</mi><mrow id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.2" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.1" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.1.cmml">​</mo><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.3.cmml">j</mi></mrow><mi id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml">l</mi></msubsup></mrow><mo id="S4.E2.m1.3.3.1.1.1.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S4.E2.m1.3.3.1.1.1.1.3" xref="S4.E2.m1.3.3.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.3b"><apply id="S4.E2.m1.3.3.cmml" xref="S4.E2.m1.3.3"><eq id="S4.E2.m1.3.3.2.cmml" xref="S4.E2.m1.3.3.2"></eq><apply id="S4.E2.m1.3.3.3.cmml" xref="S4.E2.m1.3.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.3.1.cmml" xref="S4.E2.m1.3.3.3">subscript</csymbol><ci id="S4.E2.m1.3.3.3.2.cmml" xref="S4.E2.m1.3.3.3.2">𝒟</ci><ci id="S4.E2.m1.3.3.3.3a.cmml" xref="S4.E2.m1.3.3.3.3"><mtext mathsize="70%" id="S4.E2.m1.3.3.3.3.cmml" xref="S4.E2.m1.3.3.3.3">style</mtext></ci></apply><apply id="S4.E2.m1.3.3.1.cmml" xref="S4.E2.m1.3.3.1"><apply id="S4.E2.m1.3.3.1.2.cmml" xref="S4.E2.m1.3.3.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.2.1.cmml" xref="S4.E2.m1.3.3.1.2">superscript</csymbol><apply id="S4.E2.m1.3.3.1.2.2.cmml" xref="S4.E2.m1.3.3.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.2.2.1.cmml" xref="S4.E2.m1.3.3.1.2">subscript</csymbol><sum id="S4.E2.m1.3.3.1.2.2.2.cmml" xref="S4.E2.m1.3.3.1.2.2.2"></sum><apply id="S4.E2.m1.3.3.1.2.2.3.cmml" xref="S4.E2.m1.3.3.1.2.2.3"><eq id="S4.E2.m1.3.3.1.2.2.3.1.cmml" xref="S4.E2.m1.3.3.1.2.2.3.1"></eq><ci id="S4.E2.m1.3.3.1.2.2.3.2.cmml" xref="S4.E2.m1.3.3.1.2.2.3.2">𝑙</ci><cn type="integer" id="S4.E2.m1.3.3.1.2.2.3.3.cmml" xref="S4.E2.m1.3.3.1.2.2.3.3">0</cn></apply></apply><ci id="S4.E2.m1.3.3.1.2.3.cmml" xref="S4.E2.m1.3.3.1.2.3">𝐿</ci></apply><apply id="S4.E2.m1.3.3.1.1.cmml" xref="S4.E2.m1.3.3.1.1"><times id="S4.E2.m1.3.3.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.2"></times><apply id="S4.E2.m1.3.3.1.1.3.cmml" xref="S4.E2.m1.3.3.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.3.1.cmml" xref="S4.E2.m1.3.3.1.1.3">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.3.2.cmml" xref="S4.E2.m1.3.3.1.1.3.2">𝑤</ci><ci id="S4.E2.m1.3.3.1.1.3.3.cmml" xref="S4.E2.m1.3.3.1.1.3.3">𝑙</ci></apply><apply id="S4.E2.m1.3.3.1.1.4.cmml" xref="S4.E2.m1.3.3.1.1.4"><divide id="S4.E2.m1.3.3.1.1.4.1.cmml" xref="S4.E2.m1.3.3.1.1.4"></divide><cn type="integer" id="S4.E2.m1.3.3.1.1.4.2.cmml" xref="S4.E2.m1.3.3.1.1.4.2">1</cn><apply id="S4.E2.m1.3.3.1.1.4.3.cmml" xref="S4.E2.m1.3.3.1.1.4.3"><times id="S4.E2.m1.3.3.1.1.4.3.1.cmml" xref="S4.E2.m1.3.3.1.1.4.3.1"></times><cn type="integer" id="S4.E2.m1.3.3.1.1.4.3.2.cmml" xref="S4.E2.m1.3.3.1.1.4.3.2">4</cn><apply id="S4.E2.m1.3.3.1.1.4.3.3.cmml" xref="S4.E2.m1.3.3.1.1.4.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.4.3.3.1.cmml" xref="S4.E2.m1.3.3.1.1.4.3.3">superscript</csymbol><apply id="S4.E2.m1.3.3.1.1.4.3.3.2.cmml" xref="S4.E2.m1.3.3.1.1.4.3.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.4.3.3.2.1.cmml" xref="S4.E2.m1.3.3.1.1.4.3.3">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.4.3.3.2.2.cmml" xref="S4.E2.m1.3.3.1.1.4.3.3.2.2">𝑁</ci><ci id="S4.E2.m1.3.3.1.1.4.3.3.2.3.cmml" xref="S4.E2.m1.3.3.1.1.4.3.3.2.3">𝑙</ci></apply><cn type="integer" id="S4.E2.m1.3.3.1.1.4.3.3.3.cmml" xref="S4.E2.m1.3.3.1.1.4.3.3.3">2</cn></apply><apply id="S4.E2.m1.3.3.1.1.4.3.4.cmml" xref="S4.E2.m1.3.3.1.1.4.3.4"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.4.3.4.1.cmml" xref="S4.E2.m1.3.3.1.1.4.3.4">superscript</csymbol><apply id="S4.E2.m1.3.3.1.1.4.3.4.2.cmml" xref="S4.E2.m1.3.3.1.1.4.3.4"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.4.3.4.2.1.cmml" xref="S4.E2.m1.3.3.1.1.4.3.4">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.4.3.4.2.2.cmml" xref="S4.E2.m1.3.3.1.1.4.3.4.2.2">𝑀</ci><ci id="S4.E2.m1.3.3.1.1.4.3.4.2.3.cmml" xref="S4.E2.m1.3.3.1.1.4.3.4.2.3">𝑙</ci></apply><cn type="integer" id="S4.E2.m1.3.3.1.1.4.3.4.3.cmml" xref="S4.E2.m1.3.3.1.1.4.3.4.3">2</cn></apply></apply></apply><apply id="S4.E2.m1.3.3.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1"><apply id="S4.E2.m1.3.3.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.2">subscript</csymbol><sum id="S4.E2.m1.3.3.1.1.1.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.2.2"></sum><list id="S4.E2.m1.2.2.2.3.cmml" xref="S4.E2.m1.2.2.2.4"><ci id="S4.E2.m1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1">𝑖</ci><ci id="S4.E2.m1.2.2.2.2.cmml" xref="S4.E2.m1.2.2.2.2">𝑗</ci></list></apply><apply id="S4.E2.m1.3.3.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1">superscript</csymbol><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1"><minus id="S4.E2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.1"></minus><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.2">𝐺</ci><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3"><times id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.1"></times><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.2.3.3">𝑗</ci></apply></apply><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.2.3">𝑙</ci></apply><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.2">𝐴</ci><apply id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3"><times id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.1"></times><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.2">𝑖</ci><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.2.3.3">𝑗</ci></apply></apply><ci id="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.1.1.1.3.3">𝑙</ci></apply></apply><cn type="integer" id="S4.E2.m1.3.3.1.1.1.1.3.cmml" xref="S4.E2.m1.3.3.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.3c">\mathcal{D}_{\text{style}}=\sum_{l=0}^{L}w_{l}\frac{1}{4N_{l}^{2}M_{l}^{2}}\sum_{i,j}\left(G_{ij}^{l}-A_{ij}^{l}\right)^{2}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.p2.13" class="ltx_p">where <math id="S4.p2.3.m1.1" class="ltx_Math" alttext="F_{ij}^{l}" display="inline"><semantics id="S4.p2.3.m1.1a"><msubsup id="S4.p2.3.m1.1.1" xref="S4.p2.3.m1.1.1.cmml"><mi id="S4.p2.3.m1.1.1.2.2" xref="S4.p2.3.m1.1.1.2.2.cmml">F</mi><mrow id="S4.p2.3.m1.1.1.2.3" xref="S4.p2.3.m1.1.1.2.3.cmml"><mi id="S4.p2.3.m1.1.1.2.3.2" xref="S4.p2.3.m1.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p2.3.m1.1.1.2.3.1" xref="S4.p2.3.m1.1.1.2.3.1.cmml">​</mo><mi id="S4.p2.3.m1.1.1.2.3.3" xref="S4.p2.3.m1.1.1.2.3.3.cmml">j</mi></mrow><mi id="S4.p2.3.m1.1.1.3" xref="S4.p2.3.m1.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.p2.3.m1.1b"><apply id="S4.p2.3.m1.1.1.cmml" xref="S4.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m1.1.1.1.cmml" xref="S4.p2.3.m1.1.1">superscript</csymbol><apply id="S4.p2.3.m1.1.1.2.cmml" xref="S4.p2.3.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.3.m1.1.1.2.1.cmml" xref="S4.p2.3.m1.1.1">subscript</csymbol><ci id="S4.p2.3.m1.1.1.2.2.cmml" xref="S4.p2.3.m1.1.1.2.2">𝐹</ci><apply id="S4.p2.3.m1.1.1.2.3.cmml" xref="S4.p2.3.m1.1.1.2.3"><times id="S4.p2.3.m1.1.1.2.3.1.cmml" xref="S4.p2.3.m1.1.1.2.3.1"></times><ci id="S4.p2.3.m1.1.1.2.3.2.cmml" xref="S4.p2.3.m1.1.1.2.3.2">𝑖</ci><ci id="S4.p2.3.m1.1.1.2.3.3.cmml" xref="S4.p2.3.m1.1.1.2.3.3">𝑗</ci></apply></apply><ci id="S4.p2.3.m1.1.1.3.cmml" xref="S4.p2.3.m1.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m1.1c">F_{ij}^{l}</annotation></semantics></math> and <math id="S4.p2.4.m2.1" class="ltx_Math" alttext="P_{ij}^{l}" display="inline"><semantics id="S4.p2.4.m2.1a"><msubsup id="S4.p2.4.m2.1.1" xref="S4.p2.4.m2.1.1.cmml"><mi id="S4.p2.4.m2.1.1.2.2" xref="S4.p2.4.m2.1.1.2.2.cmml">P</mi><mrow id="S4.p2.4.m2.1.1.2.3" xref="S4.p2.4.m2.1.1.2.3.cmml"><mi id="S4.p2.4.m2.1.1.2.3.2" xref="S4.p2.4.m2.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p2.4.m2.1.1.2.3.1" xref="S4.p2.4.m2.1.1.2.3.1.cmml">​</mo><mi id="S4.p2.4.m2.1.1.2.3.3" xref="S4.p2.4.m2.1.1.2.3.3.cmml">j</mi></mrow><mi id="S4.p2.4.m2.1.1.3" xref="S4.p2.4.m2.1.1.3.cmml">l</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.p2.4.m2.1b"><apply id="S4.p2.4.m2.1.1.cmml" xref="S4.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m2.1.1.1.cmml" xref="S4.p2.4.m2.1.1">superscript</csymbol><apply id="S4.p2.4.m2.1.1.2.cmml" xref="S4.p2.4.m2.1.1"><csymbol cd="ambiguous" id="S4.p2.4.m2.1.1.2.1.cmml" xref="S4.p2.4.m2.1.1">subscript</csymbol><ci id="S4.p2.4.m2.1.1.2.2.cmml" xref="S4.p2.4.m2.1.1.2.2">𝑃</ci><apply id="S4.p2.4.m2.1.1.2.3.cmml" xref="S4.p2.4.m2.1.1.2.3"><times id="S4.p2.4.m2.1.1.2.3.1.cmml" xref="S4.p2.4.m2.1.1.2.3.1"></times><ci id="S4.p2.4.m2.1.1.2.3.2.cmml" xref="S4.p2.4.m2.1.1.2.3.2">𝑖</ci><ci id="S4.p2.4.m2.1.1.2.3.3.cmml" xref="S4.p2.4.m2.1.1.2.3.3">𝑗</ci></apply></apply><ci id="S4.p2.4.m2.1.1.3.cmml" xref="S4.p2.4.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m2.1c">P_{ij}^{l}</annotation></semantics></math> denote representations extracted by the <math id="S4.p2.5.m3.1" class="ltx_Math" alttext="i^{th}" display="inline"><semantics id="S4.p2.5.m3.1a"><msup id="S4.p2.5.m3.1.1" xref="S4.p2.5.m3.1.1.cmml"><mi id="S4.p2.5.m3.1.1.2" xref="S4.p2.5.m3.1.1.2.cmml">i</mi><mrow id="S4.p2.5.m3.1.1.3" xref="S4.p2.5.m3.1.1.3.cmml"><mi id="S4.p2.5.m3.1.1.3.2" xref="S4.p2.5.m3.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p2.5.m3.1.1.3.1" xref="S4.p2.5.m3.1.1.3.1.cmml">​</mo><mi id="S4.p2.5.m3.1.1.3.3" xref="S4.p2.5.m3.1.1.3.3.cmml">h</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.p2.5.m3.1b"><apply id="S4.p2.5.m3.1.1.cmml" xref="S4.p2.5.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.5.m3.1.1.1.cmml" xref="S4.p2.5.m3.1.1">superscript</csymbol><ci id="S4.p2.5.m3.1.1.2.cmml" xref="S4.p2.5.m3.1.1.2">𝑖</ci><apply id="S4.p2.5.m3.1.1.3.cmml" xref="S4.p2.5.m3.1.1.3"><times id="S4.p2.5.m3.1.1.3.1.cmml" xref="S4.p2.5.m3.1.1.3.1"></times><ci id="S4.p2.5.m3.1.1.3.2.cmml" xref="S4.p2.5.m3.1.1.3.2">𝑡</ci><ci id="S4.p2.5.m3.1.1.3.3.cmml" xref="S4.p2.5.m3.1.1.3.3">ℎ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.5.m3.1c">i^{th}</annotation></semantics></math> filter at position <math id="S4.p2.6.m4.1" class="ltx_Math" alttext="j" display="inline"><semantics id="S4.p2.6.m4.1a"><mi id="S4.p2.6.m4.1.1" xref="S4.p2.6.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S4.p2.6.m4.1b"><ci id="S4.p2.6.m4.1.1.cmml" xref="S4.p2.6.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.6.m4.1c">j</annotation></semantics></math> in layer <math id="S4.p2.7.m5.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.p2.7.m5.1a"><mi id="S4.p2.7.m5.1.1" xref="S4.p2.7.m5.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.p2.7.m5.1b"><ci id="S4.p2.7.m5.1.1.cmml" xref="S4.p2.7.m5.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.7.m5.1c">l</annotation></semantics></math> of VGG-19. <math id="S4.p2.8.m6.1" class="ltx_Math" alttext="w_{l}" display="inline"><semantics id="S4.p2.8.m6.1a"><msub id="S4.p2.8.m6.1.1" xref="S4.p2.8.m6.1.1.cmml"><mi id="S4.p2.8.m6.1.1.2" xref="S4.p2.8.m6.1.1.2.cmml">w</mi><mi id="S4.p2.8.m6.1.1.3" xref="S4.p2.8.m6.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.8.m6.1b"><apply id="S4.p2.8.m6.1.1.cmml" xref="S4.p2.8.m6.1.1"><csymbol cd="ambiguous" id="S4.p2.8.m6.1.1.1.cmml" xref="S4.p2.8.m6.1.1">subscript</csymbol><ci id="S4.p2.8.m6.1.1.2.cmml" xref="S4.p2.8.m6.1.1.2">𝑤</ci><ci id="S4.p2.8.m6.1.1.3.cmml" xref="S4.p2.8.m6.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.8.m6.1c">w_{l}</annotation></semantics></math> is a hyper-parameter which controls the importance of each layer to the style . <math id="S4.p2.9.m7.1" class="ltx_Math" alttext="N_{l}" display="inline"><semantics id="S4.p2.9.m7.1a"><msub id="S4.p2.9.m7.1.1" xref="S4.p2.9.m7.1.1.cmml"><mi id="S4.p2.9.m7.1.1.2" xref="S4.p2.9.m7.1.1.2.cmml">N</mi><mi id="S4.p2.9.m7.1.1.3" xref="S4.p2.9.m7.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.9.m7.1b"><apply id="S4.p2.9.m7.1.1.cmml" xref="S4.p2.9.m7.1.1"><csymbol cd="ambiguous" id="S4.p2.9.m7.1.1.1.cmml" xref="S4.p2.9.m7.1.1">subscript</csymbol><ci id="S4.p2.9.m7.1.1.2.cmml" xref="S4.p2.9.m7.1.1.2">𝑁</ci><ci id="S4.p2.9.m7.1.1.3.cmml" xref="S4.p2.9.m7.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.9.m7.1c">N_{l}</annotation></semantics></math> represents the number of filters and <math id="S4.p2.10.m8.1" class="ltx_Math" alttext="M_{l}" display="inline"><semantics id="S4.p2.10.m8.1a"><msub id="S4.p2.10.m8.1.1" xref="S4.p2.10.m8.1.1.cmml"><mi id="S4.p2.10.m8.1.1.2" xref="S4.p2.10.m8.1.1.2.cmml">M</mi><mi id="S4.p2.10.m8.1.1.3" xref="S4.p2.10.m8.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p2.10.m8.1b"><apply id="S4.p2.10.m8.1.1.cmml" xref="S4.p2.10.m8.1.1"><csymbol cd="ambiguous" id="S4.p2.10.m8.1.1.1.cmml" xref="S4.p2.10.m8.1.1">subscript</csymbol><ci id="S4.p2.10.m8.1.1.2.cmml" xref="S4.p2.10.m8.1.1.2">𝑀</ci><ci id="S4.p2.10.m8.1.1.3.cmml" xref="S4.p2.10.m8.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.10.m8.1c">M_{l}</annotation></semantics></math> is size of the feature map.
<math id="S4.p2.11.m9.1" class="ltx_Math" alttext="G_{ij}" display="inline"><semantics id="S4.p2.11.m9.1a"><msub id="S4.p2.11.m9.1.1" xref="S4.p2.11.m9.1.1.cmml"><mi id="S4.p2.11.m9.1.1.2" xref="S4.p2.11.m9.1.1.2.cmml">G</mi><mrow id="S4.p2.11.m9.1.1.3" xref="S4.p2.11.m9.1.1.3.cmml"><mi id="S4.p2.11.m9.1.1.3.2" xref="S4.p2.11.m9.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p2.11.m9.1.1.3.1" xref="S4.p2.11.m9.1.1.3.1.cmml">​</mo><mi id="S4.p2.11.m9.1.1.3.3" xref="S4.p2.11.m9.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.11.m9.1b"><apply id="S4.p2.11.m9.1.1.cmml" xref="S4.p2.11.m9.1.1"><csymbol cd="ambiguous" id="S4.p2.11.m9.1.1.1.cmml" xref="S4.p2.11.m9.1.1">subscript</csymbol><ci id="S4.p2.11.m9.1.1.2.cmml" xref="S4.p2.11.m9.1.1.2">𝐺</ci><apply id="S4.p2.11.m9.1.1.3.cmml" xref="S4.p2.11.m9.1.1.3"><times id="S4.p2.11.m9.1.1.3.1.cmml" xref="S4.p2.11.m9.1.1.3.1"></times><ci id="S4.p2.11.m9.1.1.3.2.cmml" xref="S4.p2.11.m9.1.1.3.2">𝑖</ci><ci id="S4.p2.11.m9.1.1.3.3.cmml" xref="S4.p2.11.m9.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.11.m9.1c">G_{ij}</annotation></semantics></math> and <math id="S4.p2.12.m10.1" class="ltx_Math" alttext="A_{ij}" display="inline"><semantics id="S4.p2.12.m10.1a"><msub id="S4.p2.12.m10.1.1" xref="S4.p2.12.m10.1.1.cmml"><mi id="S4.p2.12.m10.1.1.2" xref="S4.p2.12.m10.1.1.2.cmml">A</mi><mrow id="S4.p2.12.m10.1.1.3" xref="S4.p2.12.m10.1.1.3.cmml"><mi id="S4.p2.12.m10.1.1.3.2" xref="S4.p2.12.m10.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p2.12.m10.1.1.3.1" xref="S4.p2.12.m10.1.1.3.1.cmml">​</mo><mi id="S4.p2.12.m10.1.1.3.3" xref="S4.p2.12.m10.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.12.m10.1b"><apply id="S4.p2.12.m10.1.1.cmml" xref="S4.p2.12.m10.1.1"><csymbol cd="ambiguous" id="S4.p2.12.m10.1.1.1.cmml" xref="S4.p2.12.m10.1.1">subscript</csymbol><ci id="S4.p2.12.m10.1.1.2.cmml" xref="S4.p2.12.m10.1.1.2">𝐴</ci><apply id="S4.p2.12.m10.1.1.3.cmml" xref="S4.p2.12.m10.1.1.3"><times id="S4.p2.12.m10.1.1.3.1.cmml" xref="S4.p2.12.m10.1.1.3.1"></times><ci id="S4.p2.12.m10.1.1.3.2.cmml" xref="S4.p2.12.m10.1.1.3.2">𝑖</ci><ci id="S4.p2.12.m10.1.1.3.3.cmml" xref="S4.p2.12.m10.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.12.m10.1c">A_{ij}</annotation></semantics></math> denote the Gram Matrix of real and synthetic images in layer <math id="S4.p2.13.m11.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S4.p2.13.m11.1a"><mi id="S4.p2.13.m11.1.1" xref="S4.p2.13.m11.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S4.p2.13.m11.1b"><ci id="S4.p2.13.m11.1.1.cmml" xref="S4.p2.13.m11.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.13.m11.1c">l</annotation></semantics></math>.
Then total distance for attribute metric is represented as</p>
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{total}}=\alpha*\mathcal{D}_{\text{style}}+\beta*\mathcal{D}_{\text{content}}" display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><msub id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.1.1.2.2" xref="S4.E3.m1.1.1.2.2.cmml">𝒟</mi><mtext id="S4.E3.m1.1.1.2.3" xref="S4.E3.m1.1.1.2.3a.cmml">total</mtext></msub><mo id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml">=</mo><mrow id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml"><mrow id="S4.E3.m1.1.1.3.2" xref="S4.E3.m1.1.1.3.2.cmml"><mi id="S4.E3.m1.1.1.3.2.2" xref="S4.E3.m1.1.1.3.2.2.cmml">α</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.1.1.3.2.1" xref="S4.E3.m1.1.1.3.2.1.cmml">∗</mo><msub id="S4.E3.m1.1.1.3.2.3" xref="S4.E3.m1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.1.1.3.2.3.2" xref="S4.E3.m1.1.1.3.2.3.2.cmml">𝒟</mi><mtext id="S4.E3.m1.1.1.3.2.3.3" xref="S4.E3.m1.1.1.3.2.3.3a.cmml">style</mtext></msub></mrow><mo id="S4.E3.m1.1.1.3.1" xref="S4.E3.m1.1.1.3.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.3.3" xref="S4.E3.m1.1.1.3.3.cmml"><mi id="S4.E3.m1.1.1.3.3.2" xref="S4.E3.m1.1.1.3.3.2.cmml">β</mi><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.1.1.3.3.1" xref="S4.E3.m1.1.1.3.3.1.cmml">∗</mo><msub id="S4.E3.m1.1.1.3.3.3" xref="S4.E3.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E3.m1.1.1.3.3.3.2" xref="S4.E3.m1.1.1.3.3.3.2.cmml">𝒟</mi><mtext id="S4.E3.m1.1.1.3.3.3.3" xref="S4.E3.m1.1.1.3.3.3.3a.cmml">content</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"></eq><apply id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.2">subscript</csymbol><ci id="S4.E3.m1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.2.2">𝒟</ci><ci id="S4.E3.m1.1.1.2.3a.cmml" xref="S4.E3.m1.1.1.2.3"><mtext mathsize="70%" id="S4.E3.m1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.2.3">total</mtext></ci></apply><apply id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3"><plus id="S4.E3.m1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.3.1"></plus><apply id="S4.E3.m1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.3.2"><times id="S4.E3.m1.1.1.3.2.1.cmml" xref="S4.E3.m1.1.1.3.2.1"></times><ci id="S4.E3.m1.1.1.3.2.2.cmml" xref="S4.E3.m1.1.1.3.2.2">𝛼</ci><apply id="S4.E3.m1.1.1.3.2.3.cmml" xref="S4.E3.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.3.2.3.1.cmml" xref="S4.E3.m1.1.1.3.2.3">subscript</csymbol><ci id="S4.E3.m1.1.1.3.2.3.2.cmml" xref="S4.E3.m1.1.1.3.2.3.2">𝒟</ci><ci id="S4.E3.m1.1.1.3.2.3.3a.cmml" xref="S4.E3.m1.1.1.3.2.3.3"><mtext mathsize="70%" id="S4.E3.m1.1.1.3.2.3.3.cmml" xref="S4.E3.m1.1.1.3.2.3.3">style</mtext></ci></apply></apply><apply id="S4.E3.m1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.3.3"><times id="S4.E3.m1.1.1.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3.1"></times><ci id="S4.E3.m1.1.1.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.2">𝛽</ci><apply id="S4.E3.m1.1.1.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.3.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3.3">subscript</csymbol><ci id="S4.E3.m1.1.1.3.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.3.2">𝒟</ci><ci id="S4.E3.m1.1.1.3.3.3.3a.cmml" xref="S4.E3.m1.1.1.3.3.3.3"><mtext mathsize="70%" id="S4.E3.m1.1.1.3.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3.3">content</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\mathcal{D}_{\text{total}}=\alpha*\mathcal{D}_{\text{style}}+\beta*\mathcal{D}_{\text{content}}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p id="S4.p2.18" class="ltx_p">where <math id="S4.p2.14.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S4.p2.14.m1.1a"><mi id="S4.p2.14.m1.1.1" xref="S4.p2.14.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.p2.14.m1.1b"><ci id="S4.p2.14.m1.1.1.cmml" xref="S4.p2.14.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.14.m1.1c">\alpha</annotation></semantics></math> and <math id="S4.p2.15.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S4.p2.15.m2.1a"><mi id="S4.p2.15.m2.1.1" xref="S4.p2.15.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.p2.15.m2.1b"><ci id="S4.p2.15.m2.1.1.cmml" xref="S4.p2.15.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.15.m2.1c">\beta</annotation></semantics></math> are two hyper-parameters which control the relative importance of style and content distance respectively. As depicted in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 4 Methodology of Proposed AOST ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, a tree boosting system named XGBoost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> model <math id="S4.p2.16.m3.1" class="ltx_Math" alttext="\theta_{0}" display="inline"><semantics id="S4.p2.16.m3.1a"><msub id="S4.p2.16.m3.1.1" xref="S4.p2.16.m3.1.1.cmml"><mi id="S4.p2.16.m3.1.1.2" xref="S4.p2.16.m3.1.1.2.cmml">θ</mi><mn id="S4.p2.16.m3.1.1.3" xref="S4.p2.16.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.p2.16.m3.1b"><apply id="S4.p2.16.m3.1.1.cmml" xref="S4.p2.16.m3.1.1"><csymbol cd="ambiguous" id="S4.p2.16.m3.1.1.1.cmml" xref="S4.p2.16.m3.1.1">subscript</csymbol><ci id="S4.p2.16.m3.1.1.2.cmml" xref="S4.p2.16.m3.1.1.2">𝜃</ci><cn type="integer" id="S4.p2.16.m3.1.1.3.cmml" xref="S4.p2.16.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.16.m3.1c">\theta_{0}</annotation></semantics></math> is trained with <span id="S4.p2.18.1" class="ltx_text ltx_font_italic">FineGPR</span> and <math id="S4.p2.17.m4.1" class="ltx_Math" alttext="\mathcal{D}_{\text{total}}" display="inline"><semantics id="S4.p2.17.m4.1a"><msub id="S4.p2.17.m4.1.1" xref="S4.p2.17.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.p2.17.m4.1.1.2" xref="S4.p2.17.m4.1.1.2.cmml">𝒟</mi><mtext id="S4.p2.17.m4.1.1.3" xref="S4.p2.17.m4.1.1.3a.cmml">total</mtext></msub><annotation-xml encoding="MathML-Content" id="S4.p2.17.m4.1b"><apply id="S4.p2.17.m4.1.1.cmml" xref="S4.p2.17.m4.1.1"><csymbol cd="ambiguous" id="S4.p2.17.m4.1.1.1.cmml" xref="S4.p2.17.m4.1.1">subscript</csymbol><ci id="S4.p2.17.m4.1.1.2.cmml" xref="S4.p2.17.m4.1.1.2">𝒟</ci><ci id="S4.p2.17.m4.1.1.3a.cmml" xref="S4.p2.17.m4.1.1.3"><mtext mathsize="70%" id="S4.p2.17.m4.1.1.3.cmml" xref="S4.p2.17.m4.1.1.3">total</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.17.m4.1c">\mathcal{D}_{\text{total}}</annotation></semantics></math>. Based on the upgraded Xgboost model <math id="S4.p2.18.m5.1" class="ltx_Math" alttext="\theta^{*}" display="inline"><semantics id="S4.p2.18.m5.1a"><msup id="S4.p2.18.m5.1.1" xref="S4.p2.18.m5.1.1.cmml"><mi id="S4.p2.18.m5.1.1.2" xref="S4.p2.18.m5.1.1.2.cmml">θ</mi><mo id="S4.p2.18.m5.1.1.3" xref="S4.p2.18.m5.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S4.p2.18.m5.1b"><apply id="S4.p2.18.m5.1.1.cmml" xref="S4.p2.18.m5.1.1"><csymbol cd="ambiguous" id="S4.p2.18.m5.1.1.1.cmml" xref="S4.p2.18.m5.1.1">superscript</csymbol><ci id="S4.p2.18.m5.1.1.2.cmml" xref="S4.p2.18.m5.1.1.2">𝜃</ci><times id="S4.p2.18.m5.1.1.3.cmml" xref="S4.p2.18.m5.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.18.m5.1c">\theta^{*}</annotation></semantics></math>,
we continuously adopt a wildly used Particle Swarm Optimization (PSO) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> method to search some optimized attributes. Typically, it selects a similar style or content distribution with respect to a real target dataset. The optimization framework can be shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Properties of FineGPR dataset ‣ 3 The FineGPR Dataset ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (<span id="S4.p2.18.2" class="ltx_text ltx_font_bold">Stage I</span>). To our knowledge this is the first demonstration to perform attribute optimization with large-scale synthetic dataset on person re-ID task. In essence, compared with existing methods for attribute optimization, such as reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> and attribute descent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>, our method investigates and learns these attribute distributions only with few parameters to optimize, which makes it more flexible and adaptable.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">

<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.3.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> The Proposed AOST Method</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="alg1.4" class="ltx_listing ltx_figure_panel ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span id="alg1.l1.1" class="ltx_text" style="font-size:90%;">Labeled synthetic data </span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="alg1.l1.m1.1a"><mi mathsize="90%" id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">L</annotation></semantics></math><span id="alg1.l1.2" class="ltx_text" style="font-size:90%;">;
Unlabeled real target data </span><math id="alg1.l1.m2.1" class="ltx_Math" alttext="U" display="inline"><semantics id="alg1.l1.m2.1a"><mi mathsize="90%" id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><ci id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">U</annotation></semantics></math><span id="alg1.l1.3" class="ltx_text" style="font-size:90%;">;</span>
</div>
<div id="alg1.l2" class="ltx_listingline">
<span id="alg1.l2.1" class="ltx_text" style="font-size:90%;">Initialized VGG model </span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="alg1.l2.m1.1a"><mi mathsize="90%" id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">\phi</annotation></semantics></math><span id="alg1.l2.2" class="ltx_text" style="font-size:90%;">;
Xgboost model </span><math id="alg1.l2.m2.1" class="ltx_Math" alttext="\theta_{0}" display="inline"><semantics id="alg1.l2.m2.1a"><msub id="alg1.l2.m2.1.1" xref="alg1.l2.m2.1.1.cmml"><mi mathsize="90%" id="alg1.l2.m2.1.1.2" xref="alg1.l2.m2.1.1.2.cmml">θ</mi><mn mathsize="90%" id="alg1.l2.m2.1.1.3" xref="alg1.l2.m2.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.l2.m2.1b"><apply id="alg1.l2.m2.1.1.cmml" xref="alg1.l2.m2.1.1"><csymbol cd="ambiguous" id="alg1.l2.m2.1.1.1.cmml" xref="alg1.l2.m2.1.1">subscript</csymbol><ci id="alg1.l2.m2.1.1.2.cmml" xref="alg1.l2.m2.1.1.2">𝜃</ci><cn type="integer" id="alg1.l2.m2.1.1.3.cmml" xref="alg1.l2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m2.1c">\theta_{0}</annotation></semantics></math><span id="alg1.l2.3" class="ltx_text" style="font-size:90%;">;</span>
</div>
<div id="alg1.l3" class="ltx_listingline">
<span id="alg1.l3.1" class="ltx_text" style="font-size:90%;">Two hyper-parameters </span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="alg1.l3.m1.1a"><mi mathsize="90%" id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">\alpha</annotation></semantics></math><span id="alg1.l3.2" class="ltx_text" style="font-size:90%;"> and </span><math id="alg1.l3.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="alg1.l3.m2.1a"><mi mathsize="90%" id="alg1.l3.m2.1.1" xref="alg1.l3.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="alg1.l3.m2.1b"><ci id="alg1.l3.m2.1.1.cmml" xref="alg1.l3.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m2.1c">\beta</annotation></semantics></math><span id="alg1.l3.3" class="ltx_text" style="font-size:90%;">; Iteration rounds </span><span id="alg1.l3.4" class="ltx_text ltx_font_italic" style="font-size:90%;">n</span><span id="alg1.l3.5" class="ltx_text" style="font-size:90%;">;
</span>
</div>
<div id="alg1.l4" class="ltx_listingline">
<span id="alg1.l4.1" class="ltx_text" style="font-size:90%;">Best re-ID model </span><math id="alg1.l4.m1.2" class="ltx_Math" alttext="f\left(\boldsymbol{w},x_{i}\right)" display="inline"><semantics id="alg1.l4.m1.2a"><mrow id="alg1.l4.m1.2.2" xref="alg1.l4.m1.2.2.cmml"><mi mathsize="90%" id="alg1.l4.m1.2.2.3" xref="alg1.l4.m1.2.2.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.2.2.2" xref="alg1.l4.m1.2.2.2.cmml">​</mo><mrow id="alg1.l4.m1.2.2.1.1" xref="alg1.l4.m1.2.2.1.2.cmml"><mo id="alg1.l4.m1.2.2.1.1.2" xref="alg1.l4.m1.2.2.1.2.cmml">(</mo><mi mathsize="90%" id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml">𝒘</mi><mo mathsize="90%" id="alg1.l4.m1.2.2.1.1.3" xref="alg1.l4.m1.2.2.1.2.cmml">,</mo><msub id="alg1.l4.m1.2.2.1.1.1" xref="alg1.l4.m1.2.2.1.1.1.cmml"><mi mathsize="90%" id="alg1.l4.m1.2.2.1.1.1.2" xref="alg1.l4.m1.2.2.1.1.1.2.cmml">x</mi><mi mathsize="90%" id="alg1.l4.m1.2.2.1.1.1.3" xref="alg1.l4.m1.2.2.1.1.1.3.cmml">i</mi></msub><mo id="alg1.l4.m1.2.2.1.1.4" xref="alg1.l4.m1.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.2b"><apply id="alg1.l4.m1.2.2.cmml" xref="alg1.l4.m1.2.2"><times id="alg1.l4.m1.2.2.2.cmml" xref="alg1.l4.m1.2.2.2"></times><ci id="alg1.l4.m1.2.2.3.cmml" xref="alg1.l4.m1.2.2.3">𝑓</ci><interval closure="open" id="alg1.l4.m1.2.2.1.2.cmml" xref="alg1.l4.m1.2.2.1.1"><ci id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1">𝒘</ci><apply id="alg1.l4.m1.2.2.1.1.1.cmml" xref="alg1.l4.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="alg1.l4.m1.2.2.1.1.1.1.cmml" xref="alg1.l4.m1.2.2.1.1.1">subscript</csymbol><ci id="alg1.l4.m1.2.2.1.1.1.2.cmml" xref="alg1.l4.m1.2.2.1.1.1.2">𝑥</ci><ci id="alg1.l4.m1.2.2.1.1.1.3.cmml" xref="alg1.l4.m1.2.2.1.1.1.3">𝑖</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.2c">f\left(\boldsymbol{w},x_{i}\right)</annotation></semantics></math><span id="alg1.l4.2" class="ltx_text" style="font-size:90%;">

</span>
</div>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="alg1.5" class="ltx_listing ltx_figure_panel ltx_listing">
<div id="alg1.l1a" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1a.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><span id="alg1.l1a.2" class="ltx_text" style="font-size:90%;">Initialize: </span><span id="alg1.l1a.3" class="ltx_text ltx_font_italic" style="font-size:90%;">m</span><span id="alg1.l1a.4" class="ltx_text" style="font-size:90%;"> = 1, </span><span id="alg1.l1a.5" class="ltx_text ltx_font_italic" style="font-size:90%;">iter</span><span id="alg1.l1a.6" class="ltx_text" style="font-size:90%;"> = 1;
</span>
</div>
<div id="alg1.l2a" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2a.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><math id="alg1.l2a.m1.1" class="ltx_Math" alttext="\rhd" display="inline"><semantics id="alg1.l2a.m1.1a"><mo mathcolor="#1C36A6" mathsize="90%" id="alg1.l2a.m1.1.1" xref="alg1.l2a.m1.1.1.cmml">⊳</mo><annotation-xml encoding="MathML-Content" id="alg1.l2a.m1.1b"><csymbol cd="latexml" id="alg1.l2a.m1.1.1.cmml" xref="alg1.l2a.m1.1.1">contains-as-subgroup</csymbol></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2a.m1.1c">\rhd</annotation></semantics></math><span id="alg1.l2a.2" class="ltx_text" style="font-size:90%;color:#1C36A6;"> <span id="alg1.l2a.2.1" class="ltx_text ltx_font_italic">Attribute Optimization ***</span></span><span id="alg1.l2a.3" class="ltx_text" style="font-size:90%;">

</span>
</div>
<div id="alg1.l3a" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3a.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3a.2" class="ltx_text" style="font-size:90%;">Extract </span><math id="alg1.l3a.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{style}}\&amp;\mathcal{D}_{\text{content}}" display="inline"><semantics id="alg1.l3a.m1.1a"><mrow id="alg1.l3a.m1.1.1" xref="alg1.l3a.m1.1.1.cmml"><msub id="alg1.l3a.m1.1.1.2" xref="alg1.l3a.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l3a.m1.1.1.2.2" xref="alg1.l3a.m1.1.1.2.2.cmml">𝒟</mi><mtext mathsize="90%" id="alg1.l3a.m1.1.1.2.3" xref="alg1.l3a.m1.1.1.2.3a.cmml">style</mtext></msub><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="alg1.l3a.m1.1.1.1" xref="alg1.l3a.m1.1.1.1.cmml">&amp;</mo><msub id="alg1.l3a.m1.1.1.3" xref="alg1.l3a.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l3a.m1.1.1.3.2" xref="alg1.l3a.m1.1.1.3.2.cmml">𝒟</mi><mtext mathsize="90%" id="alg1.l3a.m1.1.1.3.3" xref="alg1.l3a.m1.1.1.3.3a.cmml">content</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3a.m1.1b"><apply id="alg1.l3a.m1.1.1.cmml" xref="alg1.l3a.m1.1.1"><and id="alg1.l3a.m1.1.1.1.cmml" xref="alg1.l3a.m1.1.1.1"></and><apply id="alg1.l3a.m1.1.1.2.cmml" xref="alg1.l3a.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l3a.m1.1.1.2.1.cmml" xref="alg1.l3a.m1.1.1.2">subscript</csymbol><ci id="alg1.l3a.m1.1.1.2.2.cmml" xref="alg1.l3a.m1.1.1.2.2">𝒟</ci><ci id="alg1.l3a.m1.1.1.2.3a.cmml" xref="alg1.l3a.m1.1.1.2.3"><mtext mathsize="63%" id="alg1.l3a.m1.1.1.2.3.cmml" xref="alg1.l3a.m1.1.1.2.3">style</mtext></ci></apply><apply id="alg1.l3a.m1.1.1.3.cmml" xref="alg1.l3a.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l3a.m1.1.1.3.1.cmml" xref="alg1.l3a.m1.1.1.3">subscript</csymbol><ci id="alg1.l3a.m1.1.1.3.2.cmml" xref="alg1.l3a.m1.1.1.3.2">𝒟</ci><ci id="alg1.l3a.m1.1.1.3.3a.cmml" xref="alg1.l3a.m1.1.1.3.3"><mtext mathsize="63%" id="alg1.l3a.m1.1.1.3.3.cmml" xref="alg1.l3a.m1.1.1.3.3">content</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3a.m1.1c">\mathcal{D}_{\text{style}}\&amp;\mathcal{D}_{\text{content}}</annotation></semantics></math><span id="alg1.l3a.3" class="ltx_text" style="font-size:90%;"> beween </span><math id="alg1.l3a.m2.1" class="ltx_Math" alttext="L" display="inline"><semantics id="alg1.l3a.m2.1a"><mi mathsize="90%" id="alg1.l3a.m2.1.1" xref="alg1.l3a.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="alg1.l3a.m2.1b"><ci id="alg1.l3a.m2.1.1.cmml" xref="alg1.l3a.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3a.m2.1c">L</annotation></semantics></math><span id="alg1.l3a.4" class="ltx_text" style="font-size:90%;"> and </span><math id="alg1.l3a.m3.1" class="ltx_Math" alttext="U" display="inline"><semantics id="alg1.l3a.m3.1a"><mi mathsize="90%" id="alg1.l3a.m3.1.1" xref="alg1.l3a.m3.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="alg1.l3a.m3.1b"><ci id="alg1.l3a.m3.1.1.cmml" xref="alg1.l3a.m3.1.1">𝑈</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3a.m3.1c">U</annotation></semantics></math><span id="alg1.l3a.5" class="ltx_text" style="font-size:90%;"> with model </span><math id="alg1.l3a.m4.1" class="ltx_Math" alttext="\phi" display="inline"><semantics id="alg1.l3a.m4.1a"><mi mathsize="90%" id="alg1.l3a.m4.1.1" xref="alg1.l3a.m4.1.1.cmml">ϕ</mi><annotation-xml encoding="MathML-Content" id="alg1.l3a.m4.1b"><ci id="alg1.l3a.m4.1.1.cmml" xref="alg1.l3a.m4.1.1">italic-ϕ</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3a.m4.1c">\phi</annotation></semantics></math><span id="alg1.l3a.6" class="ltx_text" style="font-size:90%;">;

</span>
</div>
<div id="alg1.l4a" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4a.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><math id="alg1.l4a.m1.1" class="ltx_Math" alttext="\mathcal{D}_{\text{total}}\leftarrow\alpha*\mathcal{D}_{\text{style}}+\beta*\mathcal{D}_{\text{content}}" display="inline"><semantics id="alg1.l4a.m1.1a"><mrow id="alg1.l4a.m1.1.1" xref="alg1.l4a.m1.1.1.cmml"><msub id="alg1.l4a.m1.1.1.2" xref="alg1.l4a.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l4a.m1.1.1.2.2" xref="alg1.l4a.m1.1.1.2.2.cmml">𝒟</mi><mtext mathsize="90%" id="alg1.l4a.m1.1.1.2.3" xref="alg1.l4a.m1.1.1.2.3a.cmml">total</mtext></msub><mo mathsize="90%" stretchy="false" id="alg1.l4a.m1.1.1.1" xref="alg1.l4a.m1.1.1.1.cmml">←</mo><mrow id="alg1.l4a.m1.1.1.3" xref="alg1.l4a.m1.1.1.3.cmml"><mrow id="alg1.l4a.m1.1.1.3.2" xref="alg1.l4a.m1.1.1.3.2.cmml"><mi mathsize="90%" id="alg1.l4a.m1.1.1.3.2.2" xref="alg1.l4a.m1.1.1.3.2.2.cmml">α</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="alg1.l4a.m1.1.1.3.2.1" xref="alg1.l4a.m1.1.1.3.2.1.cmml">∗</mo><msub id="alg1.l4a.m1.1.1.3.2.3" xref="alg1.l4a.m1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l4a.m1.1.1.3.2.3.2" xref="alg1.l4a.m1.1.1.3.2.3.2.cmml">𝒟</mi><mtext mathsize="90%" id="alg1.l4a.m1.1.1.3.2.3.3" xref="alg1.l4a.m1.1.1.3.2.3.3a.cmml">style</mtext></msub></mrow><mo mathsize="90%" id="alg1.l4a.m1.1.1.3.1" xref="alg1.l4a.m1.1.1.3.1.cmml">+</mo><mrow id="alg1.l4a.m1.1.1.3.3" xref="alg1.l4a.m1.1.1.3.3.cmml"><mi mathsize="90%" id="alg1.l4a.m1.1.1.3.3.2" xref="alg1.l4a.m1.1.1.3.3.2.cmml">β</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="alg1.l4a.m1.1.1.3.3.1" xref="alg1.l4a.m1.1.1.3.3.1.cmml">∗</mo><msub id="alg1.l4a.m1.1.1.3.3.3" xref="alg1.l4a.m1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l4a.m1.1.1.3.3.3.2" xref="alg1.l4a.m1.1.1.3.3.3.2.cmml">𝒟</mi><mtext mathsize="90%" id="alg1.l4a.m1.1.1.3.3.3.3" xref="alg1.l4a.m1.1.1.3.3.3.3a.cmml">content</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4a.m1.1b"><apply id="alg1.l4a.m1.1.1.cmml" xref="alg1.l4a.m1.1.1"><ci id="alg1.l4a.m1.1.1.1.cmml" xref="alg1.l4a.m1.1.1.1">←</ci><apply id="alg1.l4a.m1.1.1.2.cmml" xref="alg1.l4a.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l4a.m1.1.1.2.1.cmml" xref="alg1.l4a.m1.1.1.2">subscript</csymbol><ci id="alg1.l4a.m1.1.1.2.2.cmml" xref="alg1.l4a.m1.1.1.2.2">𝒟</ci><ci id="alg1.l4a.m1.1.1.2.3a.cmml" xref="alg1.l4a.m1.1.1.2.3"><mtext mathsize="63%" id="alg1.l4a.m1.1.1.2.3.cmml" xref="alg1.l4a.m1.1.1.2.3">total</mtext></ci></apply><apply id="alg1.l4a.m1.1.1.3.cmml" xref="alg1.l4a.m1.1.1.3"><plus id="alg1.l4a.m1.1.1.3.1.cmml" xref="alg1.l4a.m1.1.1.3.1"></plus><apply id="alg1.l4a.m1.1.1.3.2.cmml" xref="alg1.l4a.m1.1.1.3.2"><times id="alg1.l4a.m1.1.1.3.2.1.cmml" xref="alg1.l4a.m1.1.1.3.2.1"></times><ci id="alg1.l4a.m1.1.1.3.2.2.cmml" xref="alg1.l4a.m1.1.1.3.2.2">𝛼</ci><apply id="alg1.l4a.m1.1.1.3.2.3.cmml" xref="alg1.l4a.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="alg1.l4a.m1.1.1.3.2.3.1.cmml" xref="alg1.l4a.m1.1.1.3.2.3">subscript</csymbol><ci id="alg1.l4a.m1.1.1.3.2.3.2.cmml" xref="alg1.l4a.m1.1.1.3.2.3.2">𝒟</ci><ci id="alg1.l4a.m1.1.1.3.2.3.3a.cmml" xref="alg1.l4a.m1.1.1.3.2.3.3"><mtext mathsize="63%" id="alg1.l4a.m1.1.1.3.2.3.3.cmml" xref="alg1.l4a.m1.1.1.3.2.3.3">style</mtext></ci></apply></apply><apply id="alg1.l4a.m1.1.1.3.3.cmml" xref="alg1.l4a.m1.1.1.3.3"><times id="alg1.l4a.m1.1.1.3.3.1.cmml" xref="alg1.l4a.m1.1.1.3.3.1"></times><ci id="alg1.l4a.m1.1.1.3.3.2.cmml" xref="alg1.l4a.m1.1.1.3.3.2">𝛽</ci><apply id="alg1.l4a.m1.1.1.3.3.3.cmml" xref="alg1.l4a.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="alg1.l4a.m1.1.1.3.3.3.1.cmml" xref="alg1.l4a.m1.1.1.3.3.3">subscript</csymbol><ci id="alg1.l4a.m1.1.1.3.3.3.2.cmml" xref="alg1.l4a.m1.1.1.3.3.3.2">𝒟</ci><ci id="alg1.l4a.m1.1.1.3.3.3.3a.cmml" xref="alg1.l4a.m1.1.1.3.3.3.3"><mtext mathsize="63%" id="alg1.l4a.m1.1.1.3.3.3.3.cmml" xref="alg1.l4a.m1.1.1.3.3.3.3">content</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4a.m1.1c">\mathcal{D}_{\text{total}}\leftarrow\alpha*\mathcal{D}_{\text{style}}+\beta*\mathcal{D}_{\text{content}}</annotation></semantics></math><span id="alg1.l4a.2" class="ltx_text" style="font-size:90%;">;
</span>
</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><span id="alg1.l5.2" class="ltx_text ltx_font_bold" style="font-size:90%;">while</span><span id="alg1.l5.3" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l5.m1.1" class="ltx_Math" alttext="m\leq\|U\|" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.2" xref="alg1.l5.m1.1.2.cmml"><mi mathsize="90%" id="alg1.l5.m1.1.2.2" xref="alg1.l5.m1.1.2.2.cmml">m</mi><mo mathsize="90%" id="alg1.l5.m1.1.2.1" xref="alg1.l5.m1.1.2.1.cmml">≤</mo><mrow id="alg1.l5.m1.1.2.3.2" xref="alg1.l5.m1.1.2.3.1.cmml"><mo maxsize="90%" minsize="90%" id="alg1.l5.m1.1.2.3.2.1" xref="alg1.l5.m1.1.2.3.1.1.cmml">‖</mo><mi mathsize="90%" id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">U</mi><mo maxsize="90%" minsize="90%" id="alg1.l5.m1.1.2.3.2.2" xref="alg1.l5.m1.1.2.3.1.1.cmml">‖</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.2.cmml" xref="alg1.l5.m1.1.2"><leq id="alg1.l5.m1.1.2.1.cmml" xref="alg1.l5.m1.1.2.1"></leq><ci id="alg1.l5.m1.1.2.2.cmml" xref="alg1.l5.m1.1.2.2">𝑚</ci><apply id="alg1.l5.m1.1.2.3.1.cmml" xref="alg1.l5.m1.1.2.3.2"><csymbol cd="latexml" id="alg1.l5.m1.1.2.3.1.1.cmml" xref="alg1.l5.m1.1.2.3.2.1">norm</csymbol><ci id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">𝑈</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">m\leq\|U\|</annotation></semantics></math><span id="alg1.l5.4" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l5.5" class="ltx_text ltx_font_bold" style="font-size:90%;">do</span><span id="alg1.l5.6" class="ltx_text" style="font-size:90%;"> 
</span>
</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg1.l6.2" class="ltx_text" style="font-size:90%;">    Optimized model </span><math id="alg1.l6.m1.1" class="ltx_Math" alttext="\theta^{*}" display="inline"><semantics id="alg1.l6.m1.1a"><msup id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">θ</mi><mo mathsize="90%" id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><csymbol cd="ambiguous" id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1">superscript</csymbol><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝜃</ci><times id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">\theta^{*}</annotation></semantics></math><span id="alg1.l6.3" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l6.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l6.m2.1a"><mo mathsize="90%" stretchy="false" id="alg1.l6.m2.1.1" xref="alg1.l6.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l6.m2.1b"><ci id="alg1.l6.m2.1.1.cmml" xref="alg1.l6.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m2.1c">\leftarrow</annotation></semantics></math><span id="alg1.l6.4" class="ltx_text" style="font-size:90%;"> train </span><math id="alg1.l6.m3.1" class="ltx_Math" alttext="\theta_{0}" display="inline"><semantics id="alg1.l6.m3.1a"><msub id="alg1.l6.m3.1.1" xref="alg1.l6.m3.1.1.cmml"><mi mathsize="90%" id="alg1.l6.m3.1.1.2" xref="alg1.l6.m3.1.1.2.cmml">θ</mi><mn mathsize="90%" id="alg1.l6.m3.1.1.3" xref="alg1.l6.m3.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m3.1b"><apply id="alg1.l6.m3.1.1.cmml" xref="alg1.l6.m3.1.1"><csymbol cd="ambiguous" id="alg1.l6.m3.1.1.1.cmml" xref="alg1.l6.m3.1.1">subscript</csymbol><ci id="alg1.l6.m3.1.1.2.cmml" xref="alg1.l6.m3.1.1.2">𝜃</ci><cn type="integer" id="alg1.l6.m3.1.1.3.cmml" xref="alg1.l6.m3.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m3.1c">\theta_{0}</annotation></semantics></math><span id="alg1.l6.5" class="ltx_text" style="font-size:90%;"> with </span><math id="alg1.l6.m4.1" class="ltx_Math" alttext="L" display="inline"><semantics id="alg1.l6.m4.1a"><mi mathsize="90%" id="alg1.l6.m4.1.1" xref="alg1.l6.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="alg1.l6.m4.1b"><ci id="alg1.l6.m4.1.1.cmml" xref="alg1.l6.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m4.1c">L</annotation></semantics></math><span id="alg1.l6.6" class="ltx_text" style="font-size:90%;"> and </span><math id="alg1.l6.m5.1" class="ltx_Math" alttext="\mathcal{D}_{\text{total}}" display="inline"><semantics id="alg1.l6.m5.1a"><msub id="alg1.l6.m5.1.1" xref="alg1.l6.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l6.m5.1.1.2" xref="alg1.l6.m5.1.1.2.cmml">𝒟</mi><mtext mathsize="90%" id="alg1.l6.m5.1.1.3" xref="alg1.l6.m5.1.1.3a.cmml">total</mtext></msub><annotation-xml encoding="MathML-Content" id="alg1.l6.m5.1b"><apply id="alg1.l6.m5.1.1.cmml" xref="alg1.l6.m5.1.1"><csymbol cd="ambiguous" id="alg1.l6.m5.1.1.1.cmml" xref="alg1.l6.m5.1.1">subscript</csymbol><ci id="alg1.l6.m5.1.1.2.cmml" xref="alg1.l6.m5.1.1.2">𝒟</ci><ci id="alg1.l6.m5.1.1.3a.cmml" xref="alg1.l6.m5.1.1.3"><mtext mathsize="63%" id="alg1.l6.m5.1.1.3.cmml" xref="alg1.l6.m5.1.1.3">total</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m5.1c">\mathcal{D}_{\text{total}}</annotation></semantics></math><span id="alg1.l6.7" class="ltx_text" style="font-size:90%;">;

</span>
</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span><span id="alg1.l7.2" class="ltx_text" style="font-size:90%;">    Optimized attributes </span><math id="alg1.l7.m1.1" class="ltx_Math" alttext="\mathcal{V^{*}}" display="inline"><semantics id="alg1.l7.m1.1a"><msup id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml">𝒱</mi><mo mathsize="90%" id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><csymbol cd="ambiguous" id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1">superscript</csymbol><ci id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">𝒱</ci><times id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">\mathcal{V^{*}}</annotation></semantics></math><span id="alg1.l7.3" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l7.m2.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l7.m2.1a"><mo mathsize="90%" stretchy="false" id="alg1.l7.m2.1.1" xref="alg1.l7.m2.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l7.m2.1b"><ci id="alg1.l7.m2.1.1.cmml" xref="alg1.l7.m2.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m2.1c">\leftarrow</annotation></semantics></math><span id="alg1.l7.4" class="ltx_text" style="font-size:90%;"> update </span><math id="alg1.l7.m3.1" class="ltx_Math" alttext="\theta^{*}" display="inline"><semantics id="alg1.l7.m3.1a"><msup id="alg1.l7.m3.1.1" xref="alg1.l7.m3.1.1.cmml"><mi mathsize="90%" id="alg1.l7.m3.1.1.2" xref="alg1.l7.m3.1.1.2.cmml">θ</mi><mo mathsize="90%" id="alg1.l7.m3.1.1.3" xref="alg1.l7.m3.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l7.m3.1b"><apply id="alg1.l7.m3.1.1.cmml" xref="alg1.l7.m3.1.1"><csymbol cd="ambiguous" id="alg1.l7.m3.1.1.1.cmml" xref="alg1.l7.m3.1.1">superscript</csymbol><ci id="alg1.l7.m3.1.1.2.cmml" xref="alg1.l7.m3.1.1.2">𝜃</ci><times id="alg1.l7.m3.1.1.3.cmml" xref="alg1.l7.m3.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m3.1c">\theta^{*}</annotation></semantics></math><span id="alg1.l7.5" class="ltx_text" style="font-size:90%;"> with PSO;

</span>
</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span><span id="alg1.l8.2" class="ltx_text" style="font-size:90%;">    Update the sample size: </span><math id="alg1.l8.m1.1" class="ltx_Math" alttext="m\leftarrow m+1" display="inline"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml">m</mi><mo mathsize="90%" stretchy="false" id="alg1.l8.m1.1.1.1" xref="alg1.l8.m1.1.1.1.cmml">←</mo><mrow id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml"><mi mathsize="90%" id="alg1.l8.m1.1.1.3.2" xref="alg1.l8.m1.1.1.3.2.cmml">m</mi><mo mathsize="90%" id="alg1.l8.m1.1.1.3.1" xref="alg1.l8.m1.1.1.3.1.cmml">+</mo><mn mathsize="90%" id="alg1.l8.m1.1.1.3.3" xref="alg1.l8.m1.1.1.3.3.cmml">1</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><ci id="alg1.l8.m1.1.1.1.cmml" xref="alg1.l8.m1.1.1.1">←</ci><ci id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2">𝑚</ci><apply id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3"><plus id="alg1.l8.m1.1.1.3.1.cmml" xref="alg1.l8.m1.1.1.3.1"></plus><ci id="alg1.l8.m1.1.1.3.2.cmml" xref="alg1.l8.m1.1.1.3.2">𝑚</ci><cn type="integer" id="alg1.l8.m1.1.1.3.3.cmml" xref="alg1.l8.m1.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">m\leftarrow m+1</annotation></semantics></math><span id="alg1.l8.3" class="ltx_text" style="font-size:90%;">;
</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span><span id="alg1.l9.2" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l9.3" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l9.4" class="ltx_text ltx_font_bold" style="font-size:90%;">while</span><span id="alg1.l9.5" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><span id="alg1.l10.2" class="ltx_text" style="font-size:90%;">Generate a new dataset </span><math id="alg1.l10.m1.1" class="ltx_Math" alttext="L^{*}" display="inline"><semantics id="alg1.l10.m1.1a"><msup id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l10.m1.1.1.2" xref="alg1.l10.m1.1.1.2.cmml">L</mi><mo mathsize="90%" id="alg1.l10.m1.1.1.3" xref="alg1.l10.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><apply id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1"><csymbol cd="ambiguous" id="alg1.l10.m1.1.1.1.cmml" xref="alg1.l10.m1.1.1">superscript</csymbol><ci id="alg1.l10.m1.1.1.2.cmml" xref="alg1.l10.m1.1.1.2">𝐿</ci><times id="alg1.l10.m1.1.1.3.cmml" xref="alg1.l10.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">L^{*}</annotation></semantics></math><span id="alg1.l10.3" class="ltx_text" style="font-size:90%;"> according to </span><math id="alg1.l10.m2.1" class="ltx_Math" alttext="\mathcal{V^{*}}" display="inline"><semantics id="alg1.l10.m2.1a"><msup id="alg1.l10.m2.1.1" xref="alg1.l10.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="alg1.l10.m2.1.1.2" xref="alg1.l10.m2.1.1.2.cmml">𝒱</mi><mo mathsize="90%" id="alg1.l10.m2.1.1.3" xref="alg1.l10.m2.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l10.m2.1b"><apply id="alg1.l10.m2.1.1.cmml" xref="alg1.l10.m2.1.1"><csymbol cd="ambiguous" id="alg1.l10.m2.1.1.1.cmml" xref="alg1.l10.m2.1.1">superscript</csymbol><ci id="alg1.l10.m2.1.1.2.cmml" xref="alg1.l10.m2.1.1.2">𝒱</ci><times id="alg1.l10.m2.1.1.3.cmml" xref="alg1.l10.m2.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m2.1c">\mathcal{V^{*}}</annotation></semantics></math><span id="alg1.l10.4" class="ltx_text" style="font-size:90%;">;
</span>
</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span><math id="alg1.l11.m1.1" class="ltx_Math" alttext="\rhd" display="inline"><semantics id="alg1.l11.m1.1a"><mo mathcolor="#1C36A6" mathsize="90%" id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">⊳</mo><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.1b"><csymbol cd="latexml" id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">contains-as-subgroup</csymbol></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.1c">\rhd</annotation></semantics></math><span id="alg1.l11.2" class="ltx_text" style="font-size:90%;color:#1C36A6;"> <span id="alg1.l11.2.1" class="ltx_text ltx_font_italic">Style Transfer ***</span></span><span id="alg1.l11.3" class="ltx_text" style="font-size:90%;">

</span>
</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span><span id="alg1.l12.2" class="ltx_text" style="font-size:90%;">Performing style transfer with GAN on </span><math id="alg1.l12.m1.1" class="ltx_Math" alttext="L^{*}" display="inline"><semantics id="alg1.l12.m1.1a"><msup id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l12.m1.1.1.2" xref="alg1.l12.m1.1.1.2.cmml">L</mi><mo mathsize="90%" id="alg1.l12.m1.1.1.3" xref="alg1.l12.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.1b"><apply id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1"><csymbol cd="ambiguous" id="alg1.l12.m1.1.1.1.cmml" xref="alg1.l12.m1.1.1">superscript</csymbol><ci id="alg1.l12.m1.1.1.2.cmml" xref="alg1.l12.m1.1.1.2">𝐿</ci><times id="alg1.l12.m1.1.1.3.cmml" xref="alg1.l12.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.1c">L^{*}</annotation></semantics></math><span id="alg1.l12.3" class="ltx_text" style="font-size:90%;">;

</span>
</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span><span id="alg1.l13.2" class="ltx_text ltx_font_bold" style="font-size:90%;">if</span><span id="alg1.l13.3" class="ltx_text" style="font-size:90%;"> iter </span><math id="alg1.l13.m1.1" class="ltx_Math" alttext="\leq" display="inline"><semantics id="alg1.l13.m1.1a"><mo mathsize="90%" id="alg1.l13.m1.1.1" xref="alg1.l13.m1.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.1b"><leq id="alg1.l13.m1.1.1.cmml" xref="alg1.l13.m1.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.1c">\leq</annotation></semantics></math><span id="alg1.l13.4" class="ltx_text" style="font-size:90%;"> n </span><span id="alg1.l13.5" class="ltx_text ltx_font_bold" style="font-size:90%;">then</span><span id="alg1.l13.6" class="ltx_text" style="font-size:90%;">
</span>
</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span><span id="alg1.l14.2" class="ltx_text" style="font-size:90%;">    Initializing re-ID model with </span><math id="alg1.l14.m1.1" class="ltx_Math" alttext="L^{*}" display="inline"><semantics id="alg1.l14.m1.1a"><msup id="alg1.l14.m1.1.1" xref="alg1.l14.m1.1.1.cmml"><mi mathsize="90%" id="alg1.l14.m1.1.1.2" xref="alg1.l14.m1.1.1.2.cmml">L</mi><mo mathsize="90%" id="alg1.l14.m1.1.1.3" xref="alg1.l14.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l14.m1.1b"><apply id="alg1.l14.m1.1.1.cmml" xref="alg1.l14.m1.1.1"><csymbol cd="ambiguous" id="alg1.l14.m1.1.1.1.cmml" xref="alg1.l14.m1.1.1">superscript</csymbol><ci id="alg1.l14.m1.1.1.2.cmml" xref="alg1.l14.m1.1.1.2">𝐿</ci><times id="alg1.l14.m1.1.1.3.cmml" xref="alg1.l14.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m1.1c">L^{*}</annotation></semantics></math><span id="alg1.l14.3" class="ltx_text" style="font-size:90%;"> by softmax loss ;
</span>
</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span><span id="alg1.l15.2" class="ltx_text" style="font-size:90%;">    </span><span id="alg1.l15.3" class="ltx_text ltx_font_italic" style="font-size:90%;">iter</span><span id="alg1.l15.4" class="ltx_text" style="font-size:90%;"> </span><math id="alg1.l15.m1.1" class="ltx_Math" alttext="\leftarrow" display="inline"><semantics id="alg1.l15.m1.1a"><mo mathsize="90%" stretchy="false" id="alg1.l15.m1.1.1" xref="alg1.l15.m1.1.1.cmml">←</mo><annotation-xml encoding="MathML-Content" id="alg1.l15.m1.1b"><ci id="alg1.l15.m1.1.1.cmml" xref="alg1.l15.m1.1.1">←</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l15.m1.1c">\leftarrow</annotation></semantics></math><span id="alg1.l15.5" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l15.6" class="ltx_text ltx_font_italic" style="font-size:90%;">iter</span><span id="alg1.l15.7" class="ltx_text" style="font-size:90%;"> + 1;
</span>
</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span><span id="alg1.l16.2" class="ltx_text ltx_font_bold" style="font-size:90%;">end</span><span id="alg1.l16.3" class="ltx_text" style="font-size:90%;"> </span><span id="alg1.l16.4" class="ltx_text ltx_font_bold" style="font-size:90%;">if</span>
</div>
</div>
</div>
</div>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.11" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_left" style="padding-left:6.3pt;padding-right:6.3pt;" colspan="7">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span><span id="S4.T2.1.1.1.1" class="ltx_text" style="font-size:80%;">                              Testing set </span><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S4.T2.1.1.1.2" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;" colspan="3"><span id="S4.T2.1.1.2.1" class="ltx_text" style="font-size:80%;">Market-1501</span></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;" colspan="2"><span id="S4.T2.1.1.3.1" class="ltx_text" style="font-size:80%;">MSMT17</span></td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.1.1.4.1" class="ltx_text" style="font-size:80%;">        CUHK03    </span><span id="S4.T2.1.1.4.2" class="ltx_ERROR undefined">\bigstrut</span>
</td>
</tr>
<tr id="S4.T2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.2.2.1.1" class="ltx_text" style="font-size:80%;">Training set </span><math id="S4.T2.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S4.T2.2.2.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S4.T2.2.2.1.m1.1.1" xref="S4.T2.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.m1.1b"><ci id="S4.T2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S4.T2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.2.1" class="ltx_text" style="font-size:80%;">Reference</span></td>
<td id="S4.T2.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.3.1" class="ltx_text" style="font-size:80%;">Synthetic data</span></td>
<td id="S4.T2.2.2.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.4.1" class="ltx_text" style="font-size:80%;">Rank-1</span></td>
<td id="S4.T2.2.2.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.5.1" class="ltx_text" style="font-size:80%;">Rank-5</span></td>
<td id="S4.T2.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.6.1" class="ltx_text" style="font-size:80%;">mAP</span></td>
<td id="S4.T2.2.2.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.7.1" class="ltx_text" style="font-size:80%;">Rank-1</span></td>
<td id="S4.T2.2.2.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.8.1" class="ltx_text" style="font-size:80%;">Rank-5</span></td>
<td id="S4.T2.2.2.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.9.1" class="ltx_text" style="font-size:80%;">mAP</span></td>
<td id="S4.T2.2.2.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.10.1" class="ltx_text" style="font-size:80%;">Rank-1</span></td>
<td id="S4.T2.2.2.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.2.2.11.1" class="ltx_text" style="font-size:80%;">Rank-5</span></td>
<td id="S4.T2.2.2.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.2.2.12.1" class="ltx_text" style="font-size:80%;">mAP </span><span id="S4.T2.2.2.12.2" class="ltx_ERROR undefined">\bigstrut</span>
</td>
</tr>
<tr id="S4.T2.3.3" class="ltx_tr">
<td id="S4.T2.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.3.3.2.1" class="ltx_text" style="font-size:80%;">Market-1501 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.3.3.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a><span id="S4.T2.3.3.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.3.1" class="ltx_text" style="font-size:80%;">ICCV 15</span></td>
<td id="S4.T2.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><math id="S4.T2.3.3.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.3.3.1.m1.1a"><mo mathsize="80%" id="S4.T2.3.3.1.m1.1.1" xref="S4.T2.3.3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.m1.1b"><times id="S4.T2.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T2.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.4.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">92.7</span></td>
<td id="S4.T2.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.5.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">97.9</span></td>
<td id="S4.T2.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.6.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">81.4</span></td>
<td id="S4.T2.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.7.1" class="ltx_text" style="font-size:80%;">6.0</span></td>
<td id="S4.T2.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.8.1" class="ltx_text" style="font-size:80%;">11.2</span></td>
<td id="S4.T2.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.9.1" class="ltx_text" style="font-size:80%;">1.9</span></td>
<td id="S4.T2.3.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.10.1" class="ltx_text" style="font-size:80%;">5.3</span></td>
<td id="S4.T2.3.3.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.11.1" class="ltx_text" style="font-size:80%;">12.4</span></td>
<td id="S4.T2.3.3.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.3.3.12.1" class="ltx_text" style="font-size:80%;">6.2</span></td>
</tr>
<tr id="S4.T2.4.4" class="ltx_tr">
<td id="S4.T2.4.4.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.4.4.2.1" class="ltx_text" style="font-size:80%;">MSMT17 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.4.4.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="S4.T2.4.4.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.4.4.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.3.1" class="ltx_text" style="font-size:80%;">CVPR 18</span></td>
<td id="S4.T2.4.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><math id="S4.T2.4.4.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.4.4.1.m1.1a"><mo mathsize="80%" id="S4.T2.4.4.1.m1.1.1" xref="S4.T2.4.4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><times id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T2.4.4.4" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.4.1" class="ltx_text" style="font-size:80%;">50.2</span></td>
<td id="S4.T2.4.4.5" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.5.1" class="ltx_text" style="font-size:80%;">67.7</span></td>
<td id="S4.T2.4.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.6.1" class="ltx_text" style="font-size:80%;">25.7</span></td>
<td id="S4.T2.4.4.7" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.7.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">75.7</span></td>
<td id="S4.T2.4.4.8" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.8.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">86.9</span></td>
<td id="S4.T2.4.4.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.9.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">51.5</span></td>
<td id="S4.T2.4.4.10" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.10.1" class="ltx_text" style="font-size:80%;">9.9</span></td>
<td id="S4.T2.4.4.11" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.11.1" class="ltx_text" style="font-size:80%;">20.4</span></td>
<td id="S4.T2.4.4.12" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.4.4.12.1" class="ltx_text" style="font-size:80%;">10.7</span></td>
</tr>
<tr id="S4.T2.5.5" class="ltx_tr">
<td id="S4.T2.5.5.2" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.5.5.2.1" class="ltx_text" style="font-size:80%;">CUHK03 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.5.5.2.2.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="S4.T2.5.5.2.3.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.5.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.3.1" class="ltx_text" style="font-size:80%;">CVPR 14</span></td>
<td id="S4.T2.5.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><math id="S4.T2.5.5.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S4.T2.5.5.1.m1.1a"><mo mathsize="80%" id="S4.T2.5.5.1.m1.1.1" xref="S4.T2.5.5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.1.m1.1b"><times id="S4.T2.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.1.m1.1c">\times</annotation></semantics></math></td>
<td id="S4.T2.5.5.4" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.4.1" class="ltx_text" style="font-size:80%;">36.6</span></td>
<td id="S4.T2.5.5.5" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.5.1" class="ltx_text" style="font-size:80%;">53.9</span></td>
<td id="S4.T2.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.6.1" class="ltx_text" style="font-size:80%;">16.6</span></td>
<td id="S4.T2.5.5.7" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.7.1" class="ltx_text" style="font-size:80%;">4.6</span></td>
<td id="S4.T2.5.5.8" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.8.1" class="ltx_text" style="font-size:80%;">9.1</span></td>
<td id="S4.T2.5.5.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.9.1" class="ltx_text" style="font-size:80%;">1.3</span></td>
<td id="S4.T2.5.5.10" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.10.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">43.6</span></td>
<td id="S4.T2.5.5.11" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.11.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">62.9</span></td>
<td id="S4.T2.5.5.12" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.5.5.12.1" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline" style="font-size:80%;">41.5</span></td>
</tr>
<tr id="S4.T2.6.6" class="ltx_tr">
<td id="S4.T2.6.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.6.6.1.1" class="ltx_text" style="font-size:80%;">SOMAset</span><sup id="S4.T2.6.6.1.2" class="ltx_sup"><span id="S4.T2.6.6.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">∗</span></sup><span id="S4.T2.6.6.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.6.6.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib2" title="" class="ltx_ref">2</a><span id="S4.T2.6.6.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.6.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.2.1" class="ltx_text" style="font-size:80%;">CVIU 18</span></td>
<td id="S4.T2.6.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.3.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T2.6.6.4" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.4.1" class="ltx_text" style="font-size:80%;">4.5</span></td>
<td id="S4.T2.6.6.5" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.5.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.6.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.6.1" class="ltx_text" style="font-size:80%;">1.3</span></td>
<td id="S4.T2.6.6.7" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.7.1" class="ltx_text" style="font-size:80%;">1.4</span></td>
<td id="S4.T2.6.6.8" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.8.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.6.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.9.1" class="ltx_text" style="font-size:80%;">0.3</span></td>
<td id="S4.T2.6.6.10" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.10.1" class="ltx_text" style="font-size:80%;">0.4</span></td>
<td id="S4.T2.6.6.11" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.11.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.6.6.12" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.6.6.12.1" class="ltx_text" style="font-size:80%;">0.4</span></td>
</tr>
<tr id="S4.T2.7.7" class="ltx_tr">
<td id="S4.T2.7.7.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.7.7.1.1" class="ltx_text" style="font-size:80%;">SyRI</span><sup id="S4.T2.7.7.1.2" class="ltx_sup"><span id="S4.T2.7.7.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">∗</span></sup><span id="S4.T2.7.7.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.7.7.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.T2.7.7.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.7.7.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.2.1" class="ltx_text" style="font-size:80%;">ECCV 18</span></td>
<td id="S4.T2.7.7.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.3.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.4.1" class="ltx_text" style="font-size:80%;">29.0</span></td>
<td id="S4.T2.7.7.5" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.5.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.7.7.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.6.1" class="ltx_text" style="font-size:80%;">10.8</span></td>
<td id="S4.T2.7.7.7" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.7.1" class="ltx_text" style="font-size:80%;">16.4</span></td>
<td id="S4.T2.7.7.8" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.8.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.7.7.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.9.1" class="ltx_text" style="font-size:80%;">4.4</span></td>
<td id="S4.T2.7.7.10" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.10.1" class="ltx_text" style="font-size:80%;">4.1</span></td>
<td id="S4.T2.7.7.11" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.11.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.7.7.12" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.7.7.12.1" class="ltx_text" style="font-size:80%;">3.5</span></td>
</tr>
<tr id="S4.T2.8.8" class="ltx_tr">
<td id="S4.T2.8.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.8.8.1.1" class="ltx_text" style="font-size:80%;">Unreal</span><sup id="S4.T2.8.8.1.2" class="ltx_sup"><span id="S4.T2.8.8.1.2.1" class="ltx_text" style="font-size:80%;">‡</span></sup><span id="S4.T2.8.8.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.8.8.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="S4.T2.8.8.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.8.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.2.1" class="ltx_text" style="font-size:80%;">CVPR 21</span></td>
<td id="S4.T2.8.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.3.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T2.8.8.4" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.4.1" class="ltx_text" style="font-size:80%;">37.4</span></td>
<td id="S4.T2.8.8.5" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.5.1" class="ltx_text" style="font-size:80%;">55.2</span></td>
<td id="S4.T2.8.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.6.1" class="ltx_text" style="font-size:80%;">15.9</span></td>
<td id="S4.T2.8.8.7" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.7.1" class="ltx_text" style="font-size:80%;">3.9</span></td>
<td id="S4.T2.8.8.8" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.8.1" class="ltx_text" style="font-size:80%;">7.4</span></td>
<td id="S4.T2.8.8.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.9.1" class="ltx_text" style="font-size:80%;">1.3</span></td>
<td id="S4.T2.8.8.10" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.10.1" class="ltx_text" style="font-size:80%;">4.3</span></td>
<td id="S4.T2.8.8.11" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.11.1" class="ltx_text" style="font-size:80%;">10.0</span></td>
<td id="S4.T2.8.8.12" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.8.8.12.1" class="ltx_text" style="font-size:80%;">4.7</span></td>
</tr>
<tr id="S4.T2.9.9" class="ltx_tr">
<td id="S4.T2.9.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.9.9.1.1" class="ltx_text" style="font-size:80%;">PersonX</span><sup id="S4.T2.9.9.1.2" class="ltx_sup"><span id="S4.T2.9.9.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">∗</span></sup><span id="S4.T2.9.9.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.9.9.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib32" title="" class="ltx_ref">32</a><span id="S4.T2.9.9.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.9.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.2.1" class="ltx_text" style="font-size:80%;">CVPR 19</span></td>
<td id="S4.T2.9.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.3.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T2.9.9.4" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.4.1" class="ltx_text" style="font-size:80%;">44.0</span></td>
<td id="S4.T2.9.9.5" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.5.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.9.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.6.1" class="ltx_text" style="font-size:80%;">20.4</span></td>
<td id="S4.T2.9.9.7" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.7.1" class="ltx_text" style="font-size:80%;">11.7</span></td>
<td id="S4.T2.9.9.8" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.8.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.9.9.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.9.1" class="ltx_text" style="font-size:80%;">3.6</span></td>
<td id="S4.T2.9.9.10" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.10.1" class="ltx_text" style="font-size:80%;">7.4</span></td>
<td id="S4.T2.9.9.11" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.11.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.9.9.12" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.9.9.12.1" class="ltx_text" style="font-size:80%;">6.2</span></td>
</tr>
<tr id="S4.T2.10.10" class="ltx_tr">
<td id="S4.T2.10.10.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;">
<span id="S4.T2.10.10.1.1" class="ltx_text" style="font-size:80%;">RandPerson</span><sup id="S4.T2.10.10.1.2" class="ltx_sup"><span id="S4.T2.10.10.1.2.1" class="ltx_text ltx_font_italic" style="font-size:80%;">∗</span></sup><span id="S4.T2.10.10.1.3" class="ltx_text" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.10.10.1.4.1" class="ltx_text" style="font-size:80%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="S4.T2.10.10.1.5.2" class="ltx_text" style="font-size:80%;">]</span></cite>
</td>
<td id="S4.T2.10.10.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.2.1" class="ltx_text" style="font-size:80%;">MM 20</span></td>
<td id="S4.T2.10.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.3.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T2.10.10.4" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">55.6</span></td>
<td id="S4.T2.10.10.5" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.5.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.10.10.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">28.8</span></td>
<td id="S4.T2.10.10.7" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">20.1</span></td>
<td id="S4.T2.10.10.8" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.8.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.10.10.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">6.3</span></td>
<td id="S4.T2.10.10.10" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">13.4</span></td>
<td id="S4.T2.10.10.11" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.11.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S4.T2.10.10.12" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.10.10.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">10.8</span></td>
</tr>
<tr id="S4.T2.11.12" class="ltx_tr">
<td id="S4.T2.11.12.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">FineGPR</span></td>
<td id="S4.T2.11.12.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.2.1" class="ltx_text" style="font-size:80%;">Ours</span></td>
<td id="S4.T2.11.12.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.3.1" class="ltx_text" style="font-size:80%;">✓</span></td>
<td id="S4.T2.11.12.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.4.1" class="ltx_text" style="font-size:80%;">50.5</span></td>
<td id="S4.T2.11.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.5.1" class="ltx_text" style="font-size:80%;">67.7</span></td>
<td id="S4.T2.11.12.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.6.1" class="ltx_text" style="font-size:80%;">24.6</span></td>
<td id="S4.T2.11.12.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.7.1" class="ltx_text" style="font-size:80%;">12.5</span></td>
<td id="S4.T2.11.12.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.8.1" class="ltx_text" style="font-size:80%;">18.5</span></td>
<td id="S4.T2.11.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.9.1" class="ltx_text" style="font-size:80%;">3.9</span></td>
<td id="S4.T2.11.12.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.10.1" class="ltx_text" style="font-size:80%;">8.7</span></td>
<td id="S4.T2.11.12.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.11.1" class="ltx_text" style="font-size:80%;">18.2</span></td>
<td id="S4.T2.11.12.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.12.12.1" class="ltx_text" style="font-size:80%;">8.4</span></td>
</tr>
<tr id="S4.T2.11.11" class="ltx_tr" style="background-color:#CCCCCC;">
<td id="S4.T2.11.11.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;background-color:#CCCCCC;">FineGPR<sup id="S4.T2.11.11.1.1.1" class="ltx_sup"><span id="S4.T2.11.11.1.1.1.1" class="ltx_text ltx_font_upright">†</span></sup></span></td>
<td id="S4.T2.11.11.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.2.1" class="ltx_text" style="font-size:80%;background-color:#CCCCCC;">Ours</span></td>
<td id="S4.T2.11.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.3.1" class="ltx_text" style="font-size:80%;background-color:#CCCCCC;">✓</span></td>
<td id="S4.T2.11.11.4" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;background-color:#CCCCCC;">56.3</span></td>
<td id="S4.T2.11.11.5" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;background-color:#CCCCCC;">70.4</span></td>
<td id="S4.T2.11.11.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;background-color:#CCCCCC;">29.2</span></td>
<td id="S4.T2.11.11.7" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;background-color:#CCCCCC;">19.7</span></td>
<td id="S4.T2.11.11.8" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;background-color:#CCCCCC;">27.4</span></td>
<td id="S4.T2.11.11.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;background-color:#CCCCCC;">6.1</span></td>
<td id="S4.T2.11.11.10" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;background-color:#CCCCCC;">14.2</span></td>
<td id="S4.T2.11.11.11" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;background-color:#CCCCCC;">20.6</span></td>
<td id="S4.T2.11.11.12" class="ltx_td ltx_align_center" style="padding-left:6.3pt;padding-right:6.3pt;"><span id="S4.T2.11.11.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;background-color:#CCCCCC;">11.2</span></td>
</tr>
<tr id="S4.T2.11.13" class="ltx_tr">
<td id="S4.T2.11.13.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:6.3pt;padding-right:6.3pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S4.T2.11.13.2" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.3" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.4" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.5" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.6" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.7" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.8" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.9" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.10" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.11" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
<td id="S4.T2.11.13.12" class="ltx_td" style="padding-left:6.3pt;padding-right:6.3pt;"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.36.4.1" class="ltx_text" style="font-size:113%;">Table 2</span>: </span><span id="S4.T2.17.3" class="ltx_text" style="font-size:113%;">Performance comparison with existing Real and Synthetic datasets on Market-1501, MSMT17 and CUHK03, respectively. <span id="S4.T2.17.3.1" class="ltx_text ltx_font_bold" style="color:#FF0000;">Red</span> indicates the best and <span id="S4.T2.17.3.2" class="ltx_text ltx_font_bold" style="color:#0000FF;">Blue</span> the second best. <sup id="S4.T2.17.3.3" class="ltx_sup"><span id="S4.T2.17.3.3.1" class="ltx_text ltx_font_italic">∗</span></sup> means results are reported by RandPerson <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. <sup id="S4.T2.17.3.4" class="ltx_sup">‡</sup> represents results reproduced with Unreal_v2.1 on our baseline. <span id="S4.T2.17.3.5" class="ltx_text ltx_font_italic ltx_framed ltx_framed_underline">Underline</span> denotes supervised learning. <sup id="S4.T2.17.3.6" class="ltx_sup">†</sup> means performing selecting with our AOST method.</span></figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Style Transfer.</span>
In the above attribute optimization stage, there exists serious domain gap or distribution shift between synthetic and real-world scenario. Generative Adversarial Networks (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> which have demonstrated impressive results on image-to-image translation seem to be a natural solution to this problem. However, existing methods are both inefficient and ineffective in practical application. Their inefficiency results from the fact that a new generator needs to be retrained when given a new real-world scenario. Meanwhile, these methods mainly employ low-resolution images to train a generator, and they are incapable of fully exploiting the potential of GAN, which is likely to limit the quality of generated images.
To provide a remedy to this dilemma, we build a high-resolution dataset MSCO and crawled over 20K images with a size of nearly <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="200\times 480" display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml"><mn id="S4.p3.1.m1.1.1.2" xref="S4.p3.1.m1.1.1.2.cmml">200</mn><mo lspace="0.222em" rspace="0.222em" id="S4.p3.1.m1.1.1.1" xref="S4.p3.1.m1.1.1.1.cmml">×</mo><mn id="S4.p3.1.m1.1.1.3" xref="S4.p3.1.m1.1.1.3.cmml">480</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><apply id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1"><times id="S4.p3.1.m1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1"></times><cn type="integer" id="S4.p3.1.m1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.2">200</cn><cn type="integer" id="S4.p3.1.m1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.3">480</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">200\times 480</annotation></semantics></math>, which mainly from COCO <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> dataset and few from other real-world person datasets. Different locations are also considered to cover a large diversity. We believe that a unified dataset with high-resolution can provide more useful and discriminative information during translation. Some visual examples of collected
MSCO dataset are illustrated in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.2 Properties of FineGPR dataset ‣ 3 The FineGPR Dataset ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.
By doing so, we only need to train one generator and translate the synthetic images into photo-realistic style at testing phase.
The details can be seen in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.2 Properties of FineGPR dataset ‣ 3 The FineGPR Dataset ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> (<span id="S4.p3.1.2" class="ltx_text ltx_font_bold">Stage II</span>).
To verify the priority of MSCO, we adopt several state-of-the-art methods for style-level domain adaptation, <span id="S4.p3.1.3" class="ltx_text ltx_font_italic">e.g.</span>, CycleGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite>, PTGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and SPGAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Datasets and Evaluation</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text ltx_font_bold">Market-1501</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> contains 32,668 labeled images of 1,501 identities captured from campus in Tsinghua University. Each identity is captured by at most 6 cameras. The training set contains 12,936 images from 751 identities and the test set contains 19,732 images from 750 identities.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">MSMT17</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> has 126,441 labeled images belonging to 4,101 identities and contains 32,621 training images from 1,041 identities. For the testing set, 11,659 bounding boxes are used as query images and other 82,161
bounding boxes are used as gallery images.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text ltx_font_bold">CUHK03</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> contains 14,097 images of 1,467 identities.
Following the CUHK03-NP protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, it is divided into 7,365 images of 767 identities as the training set, and the remaining 6,732 images of 700 identities as the testing set.
We adopt mean Average Precision (mAP) and Cumulative Matching Characteristics (CMC) at rank-1 and rank-5 for evaluation on re-ID task.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Experiment Settings</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.10" class="ltx_p">We mainly use the newly-built <span id="S5.SS2.p1.10.1" class="ltx_text ltx_font_italic">FineGPR</span> to conduct the experiments. For attribute optimization, we empirically set
<math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="w_{l}=0.2" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><msub id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2.2" xref="S5.SS2.p1.1.m1.1.1.2.2.cmml">w</mi><mi id="S5.SS2.p1.1.m1.1.1.2.3" xref="S5.SS2.p1.1.m1.1.1.2.3.cmml">l</mi></msub><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">0.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><eq id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></eq><apply id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.p1.1.m1.1.1.2.1.cmml" xref="S5.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S5.SS2.p1.1.m1.1.1.2.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2.2">𝑤</ci><ci id="S5.SS2.p1.1.m1.1.1.2.3.cmml" xref="S5.SS2.p1.1.m1.1.1.2.3">𝑙</ci></apply><cn type="float" id="S5.SS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.p1.1.m1.1.1.3">0.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">w_{l}=0.2</annotation></semantics></math> in Eq. <a href="#S4.E2" title="Equation 2 ‣ 4 Methodology of Proposed AOST ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, and <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="\alpha=0.9" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">α</mi><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><eq id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></eq><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">𝛼</ci><cn type="float" id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">\alpha=0.9</annotation></semantics></math>, <math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="\beta=1" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mi id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">β</mi><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.3.m3.1.1.3" xref="S5.SS2.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><eq id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1"></eq><ci id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">𝛽</ci><cn type="integer" id="S5.SS2.p1.3.m3.1.1.3.cmml" xref="S5.SS2.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">\beta=1</annotation></semantics></math> in Eq. <a href="#S4.E3" title="Equation 3 ‣ 4 Methodology of Proposed AOST ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
It is worth mentioning that our re-ID baseline system is built only with commonly used softmax cross-entropy loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> on vanilla ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> with no bells and whistles. Following the practice in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, person images are resized to 256 <math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mo id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><times id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">\times</annotation></semantics></math> 128, then a random horizontal flipping with 0.5 probability is used for data augmentation. The batch size of training samples is set as 128.
Adam method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> is adopted for optimization. The initial learning rate is set to 3.5<math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><mo id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><times id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">\times</annotation></semantics></math>10<sup id="S5.SS2.p1.10.2" class="ltx_sup"><span id="S5.SS2.p1.10.2.1" class="ltx_text ltx_font_italic">-4</span></sup> for the backbone network. Then, these learning rates are decayed to 3.5<math id="S5.SS2.p1.7.m7.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.p1.7.m7.1a"><mo id="S5.SS2.p1.7.m7.1.1" xref="S5.SS2.p1.7.m7.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.7.m7.1b"><times id="S5.SS2.p1.7.m7.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.7.m7.1c">\times</annotation></semantics></math>10<sup id="S5.SS2.p1.10.3" class="ltx_sup"><span id="S5.SS2.p1.10.3.1" class="ltx_text ltx_font_italic">-5</span></sup> and 3.5<math id="S5.SS2.p1.9.m9.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS2.p1.9.m9.1a"><mo id="S5.SS2.p1.9.m9.1.1" xref="S5.SS2.p1.9.m9.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.9.m9.1b"><times id="S5.SS2.p1.9.m9.1.1.cmml" xref="S5.SS2.p1.9.m9.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.9.m9.1c">\times</annotation></semantics></math>10<sup id="S5.SS2.p1.10.4" class="ltx_sup"><span id="S5.SS2.p1.10.4.1" class="ltx_text ltx_font_italic">-6</span></sup> at 40th epoch and 70th epoch respectively, and the training stops after 120 epochs.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Comparison with the State-of-the-arts</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">To evaluate the superiority of our synthetic dataset, we perform training on <span id="S5.SS3.p1.1.1" class="ltx_text ltx_font_italic">FineGPR</span> and testing on each individual real dataset. The evaluation results are reported in Table <a href="#S4.T2" title="Table 2 ‣ 4 Methodology of Proposed AOST ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Surprisingly, when initializing with whole <span id="S5.SS3.p1.1.2" class="ltx_text ltx_font_italic">FineGPR</span> dataset, we can achieve a rank-1 accuracy of <span id="S5.SS3.p1.1.3" class="ltx_text ltx_font_bold">50.5%</span>, <span id="S5.SS3.p1.1.4" class="ltx_text ltx_font_bold">12.5%</span> and <span id="S5.SS3.p1.1.5" class="ltx_text ltx_font_bold">8.7%</span> when tested on Market-1501, MSMT17 and CUHK03 respectively.
Although there is a slight inferiority of performance when compared with RandPerson <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>, our <span id="S5.SS3.p1.1.6" class="ltx_text ltx_font_italic">FineGPR</span> selected by AOST with fine-grained attributes can lead a significant improvement by <span id="S5.SS3.p1.1.7" class="ltx_text ltx_font_bold">+0.7%</span> and <span id="S5.SS3.p1.1.8" class="ltx_text ltx_font_bold">+0.8%</span> in rank-1 accuracy on Market-1501 and CUHK03 dataset respectively. When compared with real-world datasets, <span id="S5.SS3.p1.1.9" class="ltx_text ltx_font_italic">FineGPR</span> also outperforms these benchmarks by an impressively large margin in terms of rank-1 accuracy, leading <span id="S5.SS3.p1.1.10" class="ltx_text ltx_font_bold">+0.3%</span> and <span id="S5.SS3.p1.1.11" class="ltx_text ltx_font_bold">+13.9%</span> improvement on Market-1501 compared with MSMT17 and CUHK03 separately. However, initializing with whole <span id="S5.SS3.p1.1.12" class="ltx_text ltx_font_italic">FineGPR</span> dataset is time-consuming and low-efficient, this motivates the investigation of data selecting techniques that can potentially address this problem.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S5.T3.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1" class="ltx_td ltx_align_left" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="7">
<span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span><span id="S5.T3.1.1.1.1" class="ltx_text" style="font-size:80%;">                              Testing set </span><math id="S5.T3.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.T3.1.1.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S5.T3.1.1.1.m1.1.1" xref="S5.T3.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.m1.1c">\rightarrow</annotation></semantics></math><span id="S5.T3.1.1.1.2" class="ltx_text" style="font-size:80%;"></span>
</td>
<td id="S5.T3.1.1.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="3"><span id="S5.T3.1.1.2.1" class="ltx_text" style="font-size:80%;">Market-1501</span></td>
<td id="S5.T3.1.1.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;" colspan="2"><span id="S5.T3.1.1.3.1" class="ltx_text" style="font-size:80%;">MSMT17</span></td>
<td id="S5.T3.1.1.4" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T3.1.1.4.1" class="ltx_text" style="font-size:80%;">        CUHK03    </span><span id="S5.T3.1.1.4.2" class="ltx_ERROR undefined">\bigstrut</span>
</td>
</tr>
<tr id="S5.T3.3.3" class="ltx_tr">
<td id="S5.T3.2.2.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T3.2.2.1.1" class="ltx_text" style="font-size:80%;">Training set </span><math id="S5.T3.2.2.1.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.2.2.1.m1.1a"><mo mathsize="80%" stretchy="false" id="S5.T3.2.2.1.m1.1.1" xref="S5.T3.2.2.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.1.m1.1b"><ci id="S5.T3.2.2.1.m1.1.1.cmml" xref="S5.T3.2.2.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.1.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.3.1" class="ltx_text" style="font-size:80%;">Bboxes</span></td>
<td id="S5.T3.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T3.3.3.2.1" class="ltx_text" style="font-size:80%;">Time (GPU-days) </span><math id="S5.T3.3.3.2.m1.1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics id="S5.T3.3.3.2.m1.1a"><mo mathsize="80%" stretchy="false" id="S5.T3.3.3.2.m1.1.1" xref="S5.T3.3.3.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.2.m1.1b"><ci id="S5.T3.3.3.2.m1.1.1.cmml" xref="S5.T3.3.3.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.2.m1.1c">\downarrow</annotation></semantics></math>
</td>
<td id="S5.T3.3.3.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.4.1" class="ltx_text" style="font-size:80%;">Rank-1</span></td>
<td id="S5.T3.3.3.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.5.1" class="ltx_text" style="font-size:80%;">Rank-5</span></td>
<td id="S5.T3.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.6.1" class="ltx_text" style="font-size:80%;">mAP</span></td>
<td id="S5.T3.3.3.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.7.1" class="ltx_text" style="font-size:80%;">Rank-1</span></td>
<td id="S5.T3.3.3.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.8.1" class="ltx_text" style="font-size:80%;">Rank-5</span></td>
<td id="S5.T3.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.9.1" class="ltx_text" style="font-size:80%;">mAP</span></td>
<td id="S5.T3.3.3.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.10.1" class="ltx_text" style="font-size:80%;">Rank-1</span></td>
<td id="S5.T3.3.3.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.3.11.1" class="ltx_text" style="font-size:80%;">Rank-5</span></td>
<td id="S5.T3.3.3.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T3.3.3.12.1" class="ltx_text" style="font-size:80%;">mAP </span><span id="S5.T3.3.3.12.2" class="ltx_ERROR undefined">\bigstrut</span>
</td>
</tr>
<tr id="S5.T3.3.4" class="ltx_tr">
<td id="S5.T3.3.4.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">FineGPR</span></td>
<td id="S5.T3.3.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.2.1" class="ltx_text" style="font-size:80%;">2,028,600</span></td>
<td id="S5.T3.3.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.3.1" class="ltx_text" style="font-size:80%;">20</span></td>
<td id="S5.T3.3.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.4.1" class="ltx_text" style="font-size:80%;">50.5</span></td>
<td id="S5.T3.3.4.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.5.1" class="ltx_text" style="font-size:80%;">67.7</span></td>
<td id="S5.T3.3.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.6.1" class="ltx_text" style="font-size:80%;">24.6</span></td>
<td id="S5.T3.3.4.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.7.1" class="ltx_text" style="font-size:80%;">12.5</span></td>
<td id="S5.T3.3.4.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.8.1" class="ltx_text" style="font-size:80%;">18.5</span></td>
<td id="S5.T3.3.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.9.1" class="ltx_text" style="font-size:80%;">3.9</span></td>
<td id="S5.T3.3.4.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.10.1" class="ltx_text" style="font-size:80%;">8.7</span></td>
<td id="S5.T3.3.4.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.11.1" class="ltx_text" style="font-size:80%;">18.2</span></td>
<td id="S5.T3.3.4.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.4.12.1" class="ltx_text" style="font-size:80%;">8.4</span></td>
</tr>
<tr id="S5.T3.3.5" class="ltx_tr">
<td id="S5.T3.3.5.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T3.3.5.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">FineGPR</span><span id="S5.T3.3.5.1.2" class="ltx_text" style="font-size:80%;">+R</span>
</td>
<td id="S5.T3.3.5.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.2.1" class="ltx_text" style="font-size:80%;">124,200</span></td>
<td id="S5.T3.3.5.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.3.1" class="ltx_text" style="font-size:80%;">1.3</span></td>
<td id="S5.T3.3.5.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.4.1" class="ltx_text" style="font-size:80%;">39.9</span></td>
<td id="S5.T3.3.5.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.5.1" class="ltx_text" style="font-size:80%;">57.1</span></td>
<td id="S5.T3.3.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.6.1" class="ltx_text" style="font-size:80%;">18.3</span></td>
<td id="S5.T3.3.5.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.7.1" class="ltx_text" style="font-size:80%;">4.6</span></td>
<td id="S5.T3.3.5.8" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.8.1" class="ltx_text" style="font-size:80%;">9.7</span></td>
<td id="S5.T3.3.5.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.9.1" class="ltx_text" style="font-size:80%;">1.4</span></td>
<td id="S5.T3.3.5.10" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.10.1" class="ltx_text" style="font-size:80%;">5.9</span></td>
<td id="S5.T3.3.5.11" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.11.1" class="ltx_text" style="font-size:80%;">15.4</span></td>
<td id="S5.T3.3.5.12" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.5.12.1" class="ltx_text" style="font-size:80%;">5.4</span></td>
</tr>
<tr id="S5.T3.3.6" class="ltx_tr">
<td id="S5.T3.3.6.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;">
<span id="S5.T3.3.6.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;">FineGPR</span><span id="S5.T3.3.6.1.2" class="ltx_text" style="font-size:80%;">+AO (</span><span id="S5.T3.3.6.1.3" class="ltx_text ltx_font_italic" style="font-size:80%;">w/o</span><span id="S5.T3.3.6.1.4" class="ltx_text" style="font-size:80%;"> transfer)</span>
</td>
<td id="S5.T3.3.6.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.2.1" class="ltx_text" style="font-size:80%;">124,200</span></td>
<td id="S5.T3.3.6.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.3.1" class="ltx_text" style="font-size:80%;">1.3</span></td>
<td id="S5.T3.3.6.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.4.1" class="ltx_text" style="font-size:80%;">45.5</span></td>
<td id="S5.T3.3.6.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.5.1" class="ltx_text" style="font-size:80%;">63.2</span></td>
<td id="S5.T3.3.6.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.6.1" class="ltx_text" style="font-size:80%;">23.8</span></td>
<td id="S5.T3.3.6.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.7.1" class="ltx_text" style="font-size:80%;">9.1</span></td>
<td id="S5.T3.3.6.8" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.8.1" class="ltx_text" style="font-size:80%;">14.7</span></td>
<td id="S5.T3.3.6.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.9.1" class="ltx_text" style="font-size:80%;">3.1</span></td>
<td id="S5.T3.3.6.10" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.10.1" class="ltx_text" style="font-size:80%;">8.5</span></td>
<td id="S5.T3.3.6.11" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.11.1" class="ltx_text" style="font-size:80%;">16.9</span></td>
<td id="S5.T3.3.6.12" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.6.12.1" class="ltx_text" style="font-size:80%;">8.3</span></td>
</tr>
<tr id="S5.T3.3.7" class="ltx_tr">
<td id="S5.T3.3.7.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.1.1" class="ltx_text" style="font-size:80%;">PersonX+R+ST</span></td>
<td id="S5.T3.3.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.2.1" class="ltx_text" style="font-size:80%;">124,200</span></td>
<td id="S5.T3.3.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.3.1" class="ltx_text" style="font-size:80%;">1.3</span></td>
<td id="S5.T3.3.7.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.4.1" class="ltx_text" style="font-size:80%;">28.7</span></td>
<td id="S5.T3.3.7.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.5.1" class="ltx_text" style="font-size:80%;">45.9</span></td>
<td id="S5.T3.3.7.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.6.1" class="ltx_text" style="font-size:80%;">11.8</span></td>
<td id="S5.T3.3.7.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.7.1" class="ltx_text" style="font-size:80%;">7.1</span></td>
<td id="S5.T3.3.7.8" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.8.1" class="ltx_text" style="font-size:80%;">13.4</span></td>
<td id="S5.T3.3.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.9.1" class="ltx_text" style="font-size:80%;">2.1</span></td>
<td id="S5.T3.3.7.10" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.10.1" class="ltx_text" style="font-size:80%;">3.1</span></td>
<td id="S5.T3.3.7.11" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.11.1" class="ltx_text" style="font-size:80%;">7.4</span></td>
<td id="S5.T3.3.7.12" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.7.12.1" class="ltx_text" style="font-size:80%;">3.1</span></td>
</tr>
<tr id="S5.T3.3.8" class="ltx_tr">
<td id="S5.T3.3.8.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.1.1" class="ltx_text" style="font-size:80%;">Unreal+R+ST</span></td>
<td id="S5.T3.3.8.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.2.1" class="ltx_text" style="font-size:80%;">124,200</span></td>
<td id="S5.T3.3.8.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.3.1" class="ltx_text" style="font-size:80%;">1.3</span></td>
<td id="S5.T3.3.8.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.4.1" class="ltx_text" style="font-size:80%;">42.8</span></td>
<td id="S5.T3.3.8.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.5.1" class="ltx_text" style="font-size:80%;">59.3</span></td>
<td id="S5.T3.3.8.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.6.1" class="ltx_text" style="font-size:80%;">18.4</span></td>
<td id="S5.T3.3.8.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.7.1" class="ltx_text" style="font-size:80%;">11.3</span></td>
<td id="S5.T3.3.8.8" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.8.1" class="ltx_text" style="font-size:80%;">20.5</span></td>
<td id="S5.T3.3.8.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.9.1" class="ltx_text" style="font-size:80%;">3.5</span></td>
<td id="S5.T3.3.8.10" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.10.1" class="ltx_text" style="font-size:80%;">5.4</span></td>
<td id="S5.T3.3.8.11" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.11.1" class="ltx_text" style="font-size:80%;">12.6</span></td>
<td id="S5.T3.3.8.12" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.8.12.1" class="ltx_text" style="font-size:80%;">5.1</span></td>
</tr>
<tr id="S5.T3.3.9" class="ltx_tr">
<td id="S5.T3.3.9.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.1.1" class="ltx_text" style="font-size:80%;">RandPerson+R+ST</span></td>
<td id="S5.T3.3.9.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.2.1" class="ltx_text" style="font-size:80%;">124,200</span></td>
<td id="S5.T3.3.9.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.3.1" class="ltx_text" style="font-size:80%;">1.3</span></td>
<td id="S5.T3.3.9.4" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.4.1" class="ltx_text" style="font-size:80%;">51.4</span></td>
<td id="S5.T3.3.9.5" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.5.1" class="ltx_text" style="font-size:80%;">68.3</span></td>
<td id="S5.T3.3.9.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.6.1" class="ltx_text" style="font-size:80%;">25.0</span></td>
<td id="S5.T3.3.9.7" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.7.1" class="ltx_text" style="font-size:80%;">15.8</span></td>
<td id="S5.T3.3.9.8" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.8.1" class="ltx_text" style="font-size:80%;">26.7</span></td>
<td id="S5.T3.3.9.9" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.9.1" class="ltx_text" style="font-size:80%;">5.0</span></td>
<td id="S5.T3.3.9.10" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.10.1" class="ltx_text" style="font-size:80%;">8.4</span></td>
<td id="S5.T3.3.9.11" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.11.1" class="ltx_text" style="font-size:80%;">18.1</span></td>
<td id="S5.T3.3.9.12" class="ltx_td ltx_align_center" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.9.12.1" class="ltx_text" style="font-size:80%;">7.5</span></td>
</tr>
<tr id="S5.T3.3.10" class="ltx_tr" style="background-color:#CCCCCC;">
<td id="S5.T3.3.10.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.1.1" class="ltx_text ltx_font_italic" style="font-size:80%;background-color:#CCCCCC;">FineGPR<span id="S5.T3.3.10.1.1.1" class="ltx_text ltx_font_upright">+AOST (Ours)</span></span></td>
<td id="S5.T3.3.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">124,200</span></td>
<td id="S5.T3.3.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">1.3</span></td>
<td id="S5.T3.3.10.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">56.3</span></td>
<td id="S5.T3.3.10.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">70.4</span></td>
<td id="S5.T3.3.10.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">29.2</span></td>
<td id="S5.T3.3.10.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">19.7</span></td>
<td id="S5.T3.3.10.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">27.4</span></td>
<td id="S5.T3.3.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.9.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">6.1</span></td>
<td id="S5.T3.3.10.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.10.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">14.2</span></td>
<td id="S5.T3.3.10.11" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.11.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">20.6</span></td>
<td id="S5.T3.3.10.12" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:4.3pt;padding-right:4.3pt;"><span id="S5.T3.3.10.12.1" class="ltx_text ltx_font_bold" style="font-size:80%;background-color:#CCCCCC;">11.2</span></td>
</tr>
<tr id="S5.T3.3.11" class="ltx_tr">
<td id="S5.T3.3.11.1" class="ltx_td ltx_align_left ltx_border_r" style="padding-left:4.3pt;padding-right:4.3pt;"><span class="ltx_rule" style="width:100%;height:0.8pt;background:black;display:inline-block;"> </span></td>
<td id="S5.T3.3.11.2" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.3" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.4" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.5" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.6" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.7" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.8" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.9" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.10" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.11" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
<td id="S5.T3.3.11.12" class="ltx_td" style="padding-left:4.3pt;padding-right:4.3pt;"></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.7.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S5.T3.8.2" class="ltx_text" style="font-size:113%;">Controlled experiments by different regulations of our proposed AOST method on Market-1501, MSMT17 and CUHK03, respectively. “R” indicates random sampling, “AO” represents Attribute Optimization. “ST” means Style Transfer.</span></figcaption>
</figure>
<figure id="S5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S5.F6.2" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:108.0pt;">
<img src="/html/2109.10498/assets/x6.png" id="S5.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="144" height="82" alt="Refer to caption">
<p id="S5.F6.2.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F6.2.1.1" class="ltx_text">(a) Sensitivity to <math id="S5.F6.2.1.1.m1.1" class="ltx_Math" alttext="\alpha/\beta" display="inline"><semantics id="S5.F6.2.1.1.m1.1a"><mrow id="S5.F6.2.1.1.m1.1.1" xref="S5.F6.2.1.1.m1.1.1.cmml"><mi id="S5.F6.2.1.1.m1.1.1.2" xref="S5.F6.2.1.1.m1.1.1.2.cmml">α</mi><mo id="S5.F6.2.1.1.m1.1.1.1" xref="S5.F6.2.1.1.m1.1.1.1.cmml">/</mo><mi id="S5.F6.2.1.1.m1.1.1.3" xref="S5.F6.2.1.1.m1.1.1.3.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.F6.2.1.1.m1.1b"><apply id="S5.F6.2.1.1.m1.1.1.cmml" xref="S5.F6.2.1.1.m1.1.1"><divide id="S5.F6.2.1.1.m1.1.1.1.cmml" xref="S5.F6.2.1.1.m1.1.1.1"></divide><ci id="S5.F6.2.1.1.m1.1.1.2.cmml" xref="S5.F6.2.1.1.m1.1.1.2">𝛼</ci><ci id="S5.F6.2.1.1.m1.1.1.3.cmml" xref="S5.F6.2.1.1.m1.1.1.3">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.2.1.1.m1.1c">\alpha/\beta</annotation></semantics></math></span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S5.F6.4" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:108.0pt;">
<img src="/html/2109.10498/assets/x7.png" id="S5.F6.3.g1" class="ltx_graphics ltx_img_landscape" width="144" height="82" alt="Refer to caption">
<p id="S5.F6.4.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F6.4.1.1" class="ltx_text">(b) <span id="S5.F6.4.1.1.1" class="ltx_text ltx_font_italic">FineGPR<math id="S5.F6.4.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.F6.4.1.1.1.m1.1a"><mo stretchy="false" id="S5.F6.4.1.1.1.m1.1.1" xref="S5.F6.4.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.F6.4.1.1.1.m1.1b"><ci id="S5.F6.4.1.1.1.m1.1.1.cmml" xref="S5.F6.4.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.4.1.1.1.m1.1c">\rightarrow</annotation></semantics></math></span>Market</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S5.F6.6" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:108.0pt;">
<img src="/html/2109.10498/assets/x8.png" id="S5.F6.5.g1" class="ltx_graphics ltx_img_landscape" width="144" height="82" alt="Refer to caption">
<p id="S5.F6.6.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F6.6.1.1" class="ltx_text">(c) <span id="S5.F6.6.1.1.1" class="ltx_text ltx_font_italic">FineGPR<math id="S5.F6.6.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.F6.6.1.1.1.m1.1a"><mo stretchy="false" id="S5.F6.6.1.1.1.m1.1.1" xref="S5.F6.6.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.F6.6.1.1.1.m1.1b"><ci id="S5.F6.6.1.1.1.m1.1.1.cmml" xref="S5.F6.6.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.6.1.1.1.m1.1c">\rightarrow</annotation></semantics></math></span>MSMT17</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_4">
<div id="S5.F6.8" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_top" style="width:108.0pt;">
<img src="/html/2109.10498/assets/x9.png" id="S5.F6.7.g1" class="ltx_graphics ltx_img_landscape" width="144" height="82" alt="Refer to caption">
<p id="S5.F6.8.1" class="ltx_p ltx_align_center ltx_align_center"><span id="S5.F6.8.1.1" class="ltx_text">(d) <span id="S5.F6.8.1.1.1" class="ltx_text ltx_font_italic">FineGPR<math id="S5.F6.8.1.1.1.m1.1" class="ltx_Math" alttext="\rightarrow" display="inline"><semantics id="S5.F6.8.1.1.1.m1.1a"><mo stretchy="false" id="S5.F6.8.1.1.1.m1.1.1" xref="S5.F6.8.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S5.F6.8.1.1.1.m1.1b"><ci id="S5.F6.8.1.1.1.m1.1.1.cmml" xref="S5.F6.8.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.8.1.1.1.m1.1c">\rightarrow</annotation></semantics></math></span>CUHK03</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F6.12.2.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S5.F6.10.1" class="ltx_text" style="font-size:90%;">(a) Sensitivity of AOST to key parameter <math id="S5.F6.10.1.m1.1" class="ltx_Math" alttext="\alpha/\beta" display="inline"><semantics id="S5.F6.10.1.m1.1b"><mrow id="S5.F6.10.1.m1.1.1" xref="S5.F6.10.1.m1.1.1.cmml"><mi id="S5.F6.10.1.m1.1.1.2" xref="S5.F6.10.1.m1.1.1.2.cmml">α</mi><mo id="S5.F6.10.1.m1.1.1.1" xref="S5.F6.10.1.m1.1.1.1.cmml">/</mo><mi id="S5.F6.10.1.m1.1.1.3" xref="S5.F6.10.1.m1.1.1.3.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.F6.10.1.m1.1c"><apply id="S5.F6.10.1.m1.1.1.cmml" xref="S5.F6.10.1.m1.1.1"><divide id="S5.F6.10.1.m1.1.1.1.cmml" xref="S5.F6.10.1.m1.1.1.1"></divide><ci id="S5.F6.10.1.m1.1.1.2.cmml" xref="S5.F6.10.1.m1.1.1.2">𝛼</ci><ci id="S5.F6.10.1.m1.1.1.3.cmml" xref="S5.F6.10.1.m1.1.1.3">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.F6.10.1.m1.1d">\alpha/\beta</annotation></semantics></math> in Eq. <a href="#S4.E3" title="Equation 3 ‣ 4 Methodology of Proposed AOST ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. (b-d) Feature importance of XGBoost with Feature Importance Score (higher is more important) on Market-1501, MSMT17 and CUHK03 respectively. Please zoom in for the best view.</span></figcaption>
</figure>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Ablation Study</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.8" class="ltx_p"><span id="S5.SS4.p1.8.1" class="ltx_text ltx_font_bold">Important Parameters.</span>
In Eq. <a href="#S4.E3" title="Equation 3 ‣ 4 Methodology of Proposed AOST ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, both <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><mi id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><ci id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">\alpha</annotation></semantics></math> and <math id="S5.SS4.p1.2.m2.1" class="ltx_Math" alttext="\beta" display="inline"><semantics id="S5.SS4.p1.2.m2.1a"><mi id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><ci id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">\beta</annotation></semantics></math> controls the relative importance of the style and content distance respectively between real and synthetic samples. Since <math id="S5.SS4.p1.3.m3.1" class="ltx_Math" alttext="\mathcal{D}_{\text{total}}" display="inline"><semantics id="S5.SS4.p1.3.m3.1a"><msub id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS4.p1.3.m3.1.1.2" xref="S5.SS4.p1.3.m3.1.1.2.cmml">𝒟</mi><mtext id="S5.SS4.p1.3.m3.1.1.3" xref="S5.SS4.p1.3.m3.1.1.3a.cmml">total</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><apply id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.3.m3.1.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS4.p1.3.m3.1.1.2.cmml" xref="S5.SS4.p1.3.m3.1.1.2">𝒟</ci><ci id="S5.SS4.p1.3.m3.1.1.3a.cmml" xref="S5.SS4.p1.3.m3.1.1.3"><mtext mathsize="70%" id="S5.SS4.p1.3.m3.1.1.3.cmml" xref="S5.SS4.p1.3.m3.1.1.3">total</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">\mathcal{D}_{\text{total}}</annotation></semantics></math> is a linear combination between <math id="S5.SS4.p1.4.m4.1" class="ltx_Math" alttext="\mathcal{D}_{\text{style}}" display="inline"><semantics id="S5.SS4.p1.4.m4.1a"><msub id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS4.p1.4.m4.1.1.2" xref="S5.SS4.p1.4.m4.1.1.2.cmml">𝒟</mi><mtext id="S5.SS4.p1.4.m4.1.1.3" xref="S5.SS4.p1.4.m4.1.1.3a.cmml">style</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><apply id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.4.m4.1.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS4.p1.4.m4.1.1.2.cmml" xref="S5.SS4.p1.4.m4.1.1.2">𝒟</ci><ci id="S5.SS4.p1.4.m4.1.1.3a.cmml" xref="S5.SS4.p1.4.m4.1.1.3"><mtext mathsize="70%" id="S5.SS4.p1.4.m4.1.1.3.cmml" xref="S5.SS4.p1.4.m4.1.1.3">style</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">\mathcal{D}_{\text{style}}</annotation></semantics></math> and <math id="S5.SS4.p1.5.m5.1" class="ltx_Math" alttext="\mathcal{D}_{\text{content}}" display="inline"><semantics id="S5.SS4.p1.5.m5.1a"><msub id="S5.SS4.p1.5.m5.1.1" xref="S5.SS4.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S5.SS4.p1.5.m5.1.1.2" xref="S5.SS4.p1.5.m5.1.1.2.cmml">𝒟</mi><mtext id="S5.SS4.p1.5.m5.1.1.3" xref="S5.SS4.p1.5.m5.1.1.3a.cmml">content</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><apply id="S5.SS4.p1.5.m5.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.5.m5.1.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S5.SS4.p1.5.m5.1.1.2.cmml" xref="S5.SS4.p1.5.m5.1.1.2">𝒟</ci><ci id="S5.SS4.p1.5.m5.1.1.3a.cmml" xref="S5.SS4.p1.5.m5.1.1.3"><mtext mathsize="70%" id="S5.SS4.p1.5.m5.1.1.3.cmml" xref="S5.SS4.p1.5.m5.1.1.3">content</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">\mathcal{D}_{\text{content}}</annotation></semantics></math>, we can smoothly regulate the emphasis or adjust the trade-off between the content or the style.
As depicted in Fig. <a href="#S5.F6" title="Figure 6 ‣ 5.3 Comparison with the State-of-the-arts ‣ 5 Experiments ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (a), we observe that when <math id="S5.SS4.p1.6.m6.1" class="ltx_Math" alttext="\alpha/\beta" display="inline"><semantics id="S5.SS4.p1.6.m6.1a"><mrow id="S5.SS4.p1.6.m6.1.1" xref="S5.SS4.p1.6.m6.1.1.cmml"><mi id="S5.SS4.p1.6.m6.1.1.2" xref="S5.SS4.p1.6.m6.1.1.2.cmml">α</mi><mo id="S5.SS4.p1.6.m6.1.1.1" xref="S5.SS4.p1.6.m6.1.1.1.cmml">/</mo><mi id="S5.SS4.p1.6.m6.1.1.3" xref="S5.SS4.p1.6.m6.1.1.3.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.6.m6.1b"><apply id="S5.SS4.p1.6.m6.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1"><divide id="S5.SS4.p1.6.m6.1.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1.1"></divide><ci id="S5.SS4.p1.6.m6.1.1.2.cmml" xref="S5.SS4.p1.6.m6.1.1.2">𝛼</ci><ci id="S5.SS4.p1.6.m6.1.1.3.cmml" xref="S5.SS4.p1.6.m6.1.1.3">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.6.m6.1c">\alpha/\beta</annotation></semantics></math> is small, the performances is not optimal because the style representation is way too limited to a very small portion, and thus our AOST could only mine the discriminative information in terms of content representation of the re-ID data. The <math id="S5.SS4.p1.7.m7.1" class="ltx_Math" alttext="\alpha/\beta" display="inline"><semantics id="S5.SS4.p1.7.m7.1a"><mrow id="S5.SS4.p1.7.m7.1.1" xref="S5.SS4.p1.7.m7.1.1.cmml"><mi id="S5.SS4.p1.7.m7.1.1.2" xref="S5.SS4.p1.7.m7.1.1.2.cmml">α</mi><mo id="S5.SS4.p1.7.m7.1.1.1" xref="S5.SS4.p1.7.m7.1.1.1.cmml">/</mo><mi id="S5.SS4.p1.7.m7.1.1.3" xref="S5.SS4.p1.7.m7.1.1.3.cmml">β</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.7.m7.1b"><apply id="S5.SS4.p1.7.m7.1.1.cmml" xref="S5.SS4.p1.7.m7.1.1"><divide id="S5.SS4.p1.7.m7.1.1.1.cmml" xref="S5.SS4.p1.7.m7.1.1.1"></divide><ci id="S5.SS4.p1.7.m7.1.1.2.cmml" xref="S5.SS4.p1.7.m7.1.1.2">𝛼</ci><ci id="S5.SS4.p1.7.m7.1.1.3.cmml" xref="S5.SS4.p1.7.m7.1.1.3">𝛽</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.7.m7.1c">\alpha/\beta</annotation></semantics></math> should also not be set too large, otherwise the performances drops dramatically since the model mine too many samples in style representation. Specially, <math id="S5.SS4.p1.8.m8.1" class="ltx_Math" alttext="\alpha/\beta=0.9" display="inline"><semantics id="S5.SS4.p1.8.m8.1a"><mrow id="S5.SS4.p1.8.m8.1.1" xref="S5.SS4.p1.8.m8.1.1.cmml"><mrow id="S5.SS4.p1.8.m8.1.1.2" xref="S5.SS4.p1.8.m8.1.1.2.cmml"><mi id="S5.SS4.p1.8.m8.1.1.2.2" xref="S5.SS4.p1.8.m8.1.1.2.2.cmml">α</mi><mo id="S5.SS4.p1.8.m8.1.1.2.1" xref="S5.SS4.p1.8.m8.1.1.2.1.cmml">/</mo><mi id="S5.SS4.p1.8.m8.1.1.2.3" xref="S5.SS4.p1.8.m8.1.1.2.3.cmml">β</mi></mrow><mo id="S5.SS4.p1.8.m8.1.1.1" xref="S5.SS4.p1.8.m8.1.1.1.cmml">=</mo><mn id="S5.SS4.p1.8.m8.1.1.3" xref="S5.SS4.p1.8.m8.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.8.m8.1b"><apply id="S5.SS4.p1.8.m8.1.1.cmml" xref="S5.SS4.p1.8.m8.1.1"><eq id="S5.SS4.p1.8.m8.1.1.1.cmml" xref="S5.SS4.p1.8.m8.1.1.1"></eq><apply id="S5.SS4.p1.8.m8.1.1.2.cmml" xref="S5.SS4.p1.8.m8.1.1.2"><divide id="S5.SS4.p1.8.m8.1.1.2.1.cmml" xref="S5.SS4.p1.8.m8.1.1.2.1"></divide><ci id="S5.SS4.p1.8.m8.1.1.2.2.cmml" xref="S5.SS4.p1.8.m8.1.1.2.2">𝛼</ci><ci id="S5.SS4.p1.8.m8.1.1.2.3.cmml" xref="S5.SS4.p1.8.m8.1.1.2.3">𝛽</ci></apply><cn type="float" id="S5.SS4.p1.8.m8.1.1.3.cmml" xref="S5.SS4.p1.8.m8.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.8.m8.1c">\alpha/\beta=0.9</annotation></semantics></math> yields the best accuracy.</p>
</div>
<div id="S5.SS4.p2" class="ltx_para">
<p id="S5.SS4.p2.1" class="ltx_p"><span id="S5.SS4.p2.1.1" class="ltx_text ltx_font_bold">Evaluation of Attribute Importance.</span>
Based on the end-to-end AOST system, we evaluate the impacts of different attributes in a fine-grained manner by the
XGBoost in terms of <span id="S5.SS4.p2.1.2" class="ltx_text ltx_font_italic">gain</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> (feature importance score). As illustrated in Fig. <a href="#S5.F6" title="Figure 6 ‣ 5.3 Comparison with the State-of-the-arts ‣ 5 Experiments ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (b-d), it can be easily observed that <span id="S5.SS4.p2.1.3" class="ltx_text ltx_font_bold">Identity</span> accounts for the largest proportion no matter which real dataset is employed for testing, followed by <span id="S5.SS4.p2.1.4" class="ltx_text ltx_font_bold">Viewpoint</span> attribute. Typically, this conclusion is in accordance with our prior knowledge on generalizable re-ID problem, that is, <span id="S5.SS4.p2.1.5" class="ltx_text ltx_font_italic">using more IDs as training samples is always beneficial to the re-ID system, and viewpoint also plays a key role in recognizing the clothes appearance.</span>
More details about the importance related to ID-level attributes is provided in the Supplementary. We hope these analysis about attribute importance will provide useful insights
for dataset building and future practical usage to the community.</p>
</div>
<div id="S5.SS4.p3" class="ltx_para">
<p id="S5.SS4.p3.1" class="ltx_p"><span id="S5.SS4.p3.1.2" class="ltx_text ltx_font_bold">Effectiveness of Attribute Optimization.</span>
We proceed study on dependency by testing whether the Attribute Optimization (AO) matters. According to Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Comparison with the State-of-the-arts ‣ 5 Experiments ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our attribute optimization strategy <span id="S5.SS4.p3.1.3" class="ltx_text ltx_font_italic">FineGPR</span>+AO (<span id="S5.SS4.p3.1.4" class="ltx_text ltx_font_italic">w/o</span> transfer) can lead a
significant improvement in rank-1 of <span id="S5.SS4.p3.1.5" class="ltx_text ltx_font_bold">+5.6%</span>, <span id="S5.SS4.p3.1.6" class="ltx_text ltx_font_bold">+4.5%</span> and <span id="S5.SS4.p3.1.7" class="ltx_text ltx_font_bold">+2.6%</span> on Market-1501, MSMT17 and CUHK03 respectively when compared with random sampling (<span id="S5.SS4.p3.1.8" class="ltx_text ltx_font_italic">FineGPR</span>+R). We suspect this is due to samples selected by attribute optimization strategy are much closer to real target domain, and the learned attribute distribution has a higher quality, which have a direct impact on downstream re-ID task.
Meanwhile, fast training is our second main advantage since the scale of training set can be largely decreased by attribute optimization, <span id="S5.SS4.p3.1.9" class="ltx_text ltx_font_italic">e.g.</span>, it costs nearly 20 GPU-days<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>All timings use one Nvidia Tesla P100 GPU on a server equipped with a Intel Xeon E5-2690 V4 CPU.</span></span></span> when pre-training on entire <span id="S5.SS4.p3.1.10" class="ltx_text ltx_font_italic">FineGPR</span> with 2,028,600 images. Fortunately, training time will be considerably reduced by <span id="S5.SS4.p3.1.1" class="ltx_text ltx_font_bold">15<math id="S5.SS4.p3.1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S5.SS4.p3.1.1.m1.1a"><mo id="S5.SS4.p3.1.1.m1.1.1" xref="S5.SS4.p3.1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS4.p3.1.1.m1.1b"><times id="S5.SS4.p3.1.1.m1.1.1.cmml" xref="S5.SS4.p3.1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p3.1.1.m1.1c">\times</annotation></semantics></math></span> (<span id="S5.SS4.p3.1.11" class="ltx_text ltx_font_bold" style="color:#3366CC;">20</span> vs. <span id="S5.SS4.p3.1.12" class="ltx_text ltx_font_bold" style="color:#FF0000;">1.3</span> GPU-days) by our proposed AOST without performance degradation, which leads a more efficient deployment to real-world scenarios. Surprisingly, even with fewer samples for training, our approach still yields its
competitiveness when compared with existing datasets, <span id="S5.SS4.p3.1.13" class="ltx_text ltx_font_italic">e.g.</span> <span id="S5.SS4.p3.1.14" class="ltx_text ltx_font_bold" style="color:#FF0000;">56.3%</span> vs. <span id="S5.SS4.p3.1.15" class="ltx_text ltx_font_bold" style="color:#0000FF;">55.6%</span> in rank-1 on Market in Table <a href="#S4.T2" title="Table 2 ‣ 4 Methodology of Proposed AOST ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, proving the proverbial <span id="S5.SS4.p3.1.16" class="ltx_text ltx_font_bold ltx_font_italic">less-is-more</span> principle.</p>
</div>
<div id="S5.SS4.p4" class="ltx_para">
<p id="S5.SS4.p4.1" class="ltx_p">To go even further, we also adopt AOST on synthetic datasets to prove the priority of <span id="S5.SS4.p4.1.1" class="ltx_text ltx_font_italic">FineGPR</span>. Unfortunately, due to lack of fine-grained attribute annotations, these datasets (<span id="S5.SS4.p4.1.2" class="ltx_text ltx_font_italic">e.g.</span> PersonX, Unreal and RandPerson) cannot
satisfy the need for AOST in re-ID. We instead randomly sample 124,200 images from
these datasets individually, and then perform style transfer.
As illustrated in Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Comparison with the State-of-the-arts ‣ 5 Experiments ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, it can be easily observed that <span id="S5.SS4.p4.1.3" class="ltx_text ltx_font_italic">FineGPR</span>+AOST can perform significantly greater than PersonX+R+ST, Unreal+R+ST and RandPerson+R+ST separately, which successfully proves the applicability of our proposed dataset and approach.</p>
</div>
<div id="S5.SS4.p5" class="ltx_para">
<p id="S5.SS4.p5.1" class="ltx_p"><span id="S5.SS4.p5.1.1" class="ltx_text ltx_font_bold">Effectiveness of Style Transfer.</span>
Synthetic data engine can generate images and annotations at lower labor costs. However, there exists obvious domain gap between synthetic and real-world scenario, which hinders the further improvement in performance on downstream task.
Note that for training efficiency, we instead consider an easier but practical strategy, that is, employing off-the-shelf style transfer model to generate photo-realistic images for further effective training. As shown in Table <a href="#S5.T3" title="Table 3 ‣ 5.3 Comparison with the State-of-the-arts ‣ 5 Experiments ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, <span id="S5.SS4.p5.1.2" class="ltx_text ltx_font_italic">w/o</span> style transfer by AOST, the rank-1 accuracy drops sharply from 56.3% to 45.5% and the mAP drops from 29.2% to 23.8%.
<span id="S5.SS4.p5.1.3" class="ltx_text ltx_font_italic">This confirms that mitigating domain gap between synthetic and real dataset is a crucial ingredient to make the performance to an excellent level.</span> For simplicity, the SPGAN is used as the style transfer model in the following experiments.</p>
</div>
<figure id="S5.F7" class="ltx_figure">
<p id="S5.F7.1" class="ltx_p ltx_align_center"><span id="S5.F7.1.1" class="ltx_text"><img src="/html/2109.10498/assets/x10.png" id="S5.F7.1.1.g1" class="ltx_graphics ltx_img_landscape" width="428" height="250" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S5.F7.4.2" class="ltx_text" style="font-size:90%;">Qualitative comparisons of different GAN methods when trained on Market and our MSCO, respectively.</span></figcaption>
</figure>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Qualitative and Quantitative Results</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p"><span id="S5.SS5.p1.1.1" class="ltx_text ltx_font_bold">Qualitative Evaluations.</span> Fig. <a href="#S5.F7" title="Figure 7 ‣ 5.4 Ablation Study ‣ 5 Experiments ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> presents a visual comparison of different transfer methods trained on low-resolution Market and high-resolution MSCO respectively. It can be easily noticed that GANs create artifacts and coarse results (indicated by <span id="S5.SS5.p1.1.2" class="ltx_text" style="color:#CCCC00;">yellow box</span>) when trained on low-resolution images, which still remains problematic.
In comparison, our method with MSCO can successfully address the artifacts and produce most visually pleasant results (indicated by <span id="S5.SS5.p1.1.3" class="ltx_text" style="color:#FF0000;">red box</span>) in an even better fashion, which is implicitly beneficial to the downstream re-ID mission.</p>
</div>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p"><span id="S5.SS5.p2.1.1" class="ltx_text ltx_font_bold">Quantitative Evaluations.</span>
Our qualitative observations above are confirmed by the quantitative evaluations. To be more specific, we adopt
Fréchet Inception Distance (FID) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> to measure the distribution difference between synthetic and real photos. Generally, FID measures how close the distribution of generated images is to the real. As shown in Fig. <a href="#S5.F8" title="Figure 8 ‣ 5.5 Qualitative and Quantitative Results ‣ 5 Experiments ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, by adding new regulation terms, <span id="S5.SS5.p2.1.2" class="ltx_text ltx_font_italic">e.g.</span> attribute optimization or style transfer, the FID score gradually decreases no matter which dataset is employed for evaluation, suggesting the learned attribute distributions are more and more similar to the real images. Even prior to this point, according to Fig. <a href="#S5.F9" title="Figure 9 ‣ 5.5 Qualitative and Quantitative Results ‣ 5 Experiments ‣ Less is More: Learning from Synthetic Data with Fine-grained Attributes for Person Re-Identification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>, training a generator with low-resolution images always produce low-quality images (indicated by higher FID score) no matter which style transfer model is employed. Still, SPGAN and PTGAN rank the best, while SPGAN shows a slight quantitative advantage. In all, the introduction of high-resolution MSCO dataset can always improve the adaptability to style changes and mitigate previously mentioned domain gap effectively, even in much more complex scenarios.</p>
</div>
<figure id="S5.F8" class="ltx_figure">
<p id="S5.F8.1" class="ltx_p ltx_align_center"><span id="S5.F8.1.1" class="ltx_text"><img src="/html/2109.10498/assets/x11.png" id="S5.F8.1.1.g1" class="ltx_graphics ltx_img_landscape" width="438" height="187" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S5.F8.4.2" class="ltx_text" style="font-size:90%;">Comparison of FID (lower is better) to evaluate the effectiveness of different regulation terms of AOST.</span></figcaption>
</figure>
<figure id="S5.F9" class="ltx_figure">
<p id="S5.F9.1" class="ltx_p ltx_align_center"><span id="S5.F9.1.1" class="ltx_text"><img src="/html/2109.10498/assets/x12.png" id="S5.F9.1.1.g1" class="ltx_graphics ltx_img_landscape" width="438" height="156" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F9.3.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S5.F9.4.2" class="ltx_text" style="font-size:90%;">Comparison of FID (lower is better) to evaluate the realism of generated images by CycleGAN, PTGAN and SPGAN when trained
on samples with different resolution.</span></figcaption>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this work, we take the first step to construct the largest person dataset <span id="S6.p1.1.1" class="ltx_text ltx_font_italic">FineGPR</span> with fine-grained attribute labels and high-quality annotations. On top of <span id="S6.p1.1.2" class="ltx_text ltx_font_italic">FineGPR</span>, we introduce an attribute analysis methodology called AOST to learn important attribute distribution, which enjoys the benefits of small-scale dataset for more efficient training. Continuously, style transfer is adopted to further mitigate domain gap between synthetic and real photos. With this, we proved, for the first time, that a model trained on limited synthetic data can yield a competitive performance in generalizable re-ID task. Extensive experiments also demonstrate the superiority of <span id="S6.p1.1.3" class="ltx_text ltx_font_italic">FineGPR</span> and effectiveness of AOST. We hope our dataset and method will shed light into potential tasks for the community to move forward.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Slawomir Bak, Peter Carr, and Jean-Francois Lalonde.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Domain adaptation through synthesis for unsupervised person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 189–205, 2018.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Igor Barros Barbosa, Marco Cristani, Barbara Caputo, Aleksander Rognhaugen, and
Theoharis Theoharis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Looking beyond appearances: Synthetic training data for deep cnns in
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision and Image Understanding</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 167:50–62, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Joy Buolamwini and Timnit Gebru.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Gender shades: Intersectional accuracy disparities in commercial
gender classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib3.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on fairness, accountability and transparency</span><span id="bib.bib3.5.3" class="ltx_text" style="font-size:90%;">,
pages 77–91. PMLR, 2018.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Tianqi Chen and Carlos Guestrin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Xgboost: A scalable tree boosting system.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 22nd acm sigkdd international conference
on knowledge discovery and data mining</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, pages 785–794, 2016.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Beyond triplet loss: a deep quadruplet network for person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 403–412, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Learning semantic segmentation from synthetic data: A geometrically
guided input-output adaptation approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, pages 1841–1850, 2019.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Abir Das, Anirban Chakraborty, and Amit K Roy-Chowdhury.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Consistent re-identification in a camera network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 330–345.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 248–255. Ieee, 2009.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Weijian Deng, Liang Zheng, Qixiang Ye, Guoliang Kang, Yi Yang, and Jianbin
Jiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Image-image domain adaptation with preserved self-similarity and
domain-dissimilarity for person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 994–1003, 2018.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Matteo Fabbri, Guillem Brasó, Gianluca Maugeri, Orcun Cetintas, Riccardo
Gasparini, Aljosa Osep, Simone Calderara, Laura Leal-Taixe, and Rita
Cucchiara.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Motsynth: How can synthetic data help pedestrian detection and
tracking?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on
Computer Vision</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 10849–10859, 2021.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Yaroslav Ganin and Victor Lempitsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Unsupervised domain adaptation by backpropagation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages
1180–1189. PMLR, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Leon A Gatys, Alexander S Ecker, and Matthias Bethge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Image style transfer using convolutional neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, pages 2414–2423, 2016.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Michelle Goddard.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">The eu general data protection regulation (gdpr): European regulation
that has a global impact.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Market Research</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 59(6):703–705, 2017.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Douglas Gray, Shane Brennan, and Hai Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Evaluating appearance models for recognition, reacquisition, and
tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE international workshop on performance evaluation
for tracking and surveillance (PETS)</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, volume 3, pages 1–7. Citeseer, 2007.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Alexander Hermans, Lucas Beyer, and Bastian Leibe.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">In defense of the triplet loss for person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1703.07737</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Gans trained by a two time-scale update rule converge to a local nash
equilibrium.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in neural information processing systems</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
James Kennedy and Russell Eberhart.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Particle swarm optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of ICNN’95-international conference on neural
networks</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, volume 4, pages 1942–1948. IEEE, 1995.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Diederik P Kingma and Jimmy Ba.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">Adam: A method for stochastic optimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1412.6980</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Deepreid: Deep filter pairing neural network for person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 152–159, 2014.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Wei Li, Xiatian Zhu, and Shaogang Gong.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Harmonious attention network for person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, pages 2285–2294, 2018.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Shengcai Liao, Yang Hu, Xiangyu Zhu, and Stan Z Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Person re-identification by local maximal occurrence representation
and metric learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, pages 2197–2206, 2015.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755.
Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Hao Luo, Youzhi Gu, Xingyu Liao, Shenqi Lai, and Wei Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Bag of tricks and a strong baseline for deep person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition Workshops</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, pages 0–0, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Two at once: Enhancing learning and generalization capacities via
ibn-net.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages 464–479, 2018.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Bojan Pepik, Michael Stark, Peter Gehler, and Bernt Schiele.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Teaching 3d geometry to deformable part models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2012 IEEE conference on computer vision and pattern
recognition</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages 3362–3369. IEEE, 2012.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Ergys Ristani, Francesco Solera, Roger Zou, Rita Cucchiara, and Carlo Tomasi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Performance measures and a data set for multi-target, multi-camera
tracking.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 17–35.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Nataniel Ruiz, Samuel Schulter, and Manmohan Chandraker.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Learning to simulate.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1810.02513</span><span id="bib.bib29.4.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
William Robson Schwartz and Larry S Davis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Learning discriminative appearance-based models using partial least
squares.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2009 XXII Brazilian symposium on computer graphics and image
processing</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 322–329. IEEE, 2009.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Karen Simonyan and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Very deep convolutional networks for large-scale image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1409.1556</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Xiaoxiao Sun and Liang Zheng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">Dissecting person re-identification from the viewpoint of viewpoint.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, pages 608–617, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Antonio Torralba and Alexei A Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Unbiased look at dataset bias.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CVPR 2011</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages 1521–1528. IEEE, 2011.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Rahul Rama Varior, Mrinal Haloi, and Gang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Gated siamese convolutional neural network architecture for human
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">, pages 791–808.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Rahul Rama Varior, Bing Shuai, Jiwen Lu, Dong Xu, and Gang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">A siamese long short-term memory architecture for human
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European conference on computer vision</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, pages 135–153.
Springer, 2016.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Faqiang Wang, Wangmeng Zuo, Liang Lin, David Zhang, and Lei Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Joint learning of single-image and cross-image representations for
person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 1288–1296, 2016.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Qi Wang, Junyu Gao, Wei Lin, and Yuan Yuan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Learning from synthetic data for crowd counting in the wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, pages 8198–8207, 2019.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Yanan Wang, Shengcai Liao, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Surpassing real-world source training data: Random 3d characters for
generalizable person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 28th ACM International Conference on
Multimedia</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 3422–3430, 2020.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Person transfer gan to bridge domain gap for person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib39.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib39.5.3" class="ltx_text" style="font-size:90%;">, pages 79–88, 2018.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Suncheng Xiang, Yuzhuo Fu, Guanjie You, and Ting Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Unsupervised domain adaptation through synthesis for person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE International Conference on Multimedia and Expo
(ICME)</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 1–6. IEEE, 2020.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Tong Xiao, Shuang Li, Bochao Wang, Liang Lin, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Joint detection and identification feature learning for person
search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib41.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib41.5.3" class="ltx_text" style="font-size:90%;">, pages 3415–3424, 2017.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Yue Yao, Liang Zheng, Xiaodong Yang, Milind Naphade, and Tom Gedeon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Simulating content consistent vehicle datasets with attribute
descent.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part VI 16</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 775–791.
Springer, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Tianyu Zhang, Lingxi Xie, Longhui Wei, Zijie Zhuang, Yongfei Zhang, Bo Li, and
Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Unrealperson: An adaptive pipeline towards costless person
re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, pages 11506–11515, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Zhilu Zhang and Mert R Sabuncu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Generalized cross entropy loss for training deep neural networks with
noisy labels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">32nd Conference on Neural Information Processing Systems
(NeurIPS)</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
Rui Zhao, Wanli Ouyang, and Xiaogang Wang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Learning mid-level filters for person re-identification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib45.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib45.5.3" class="ltx_text" style="font-size:90%;">, pages 144–151, 2014.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Scalable person re-identification: A benchmark.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, pages 1116–1124, 2015.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Zhedong Zheng, Liang Zheng, and Yi Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Unlabeled samples generated by gan improve the person
re-identification baseline in vitro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, pages 3754–3762, 2017.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Zhun Zhong, Liang Zheng, Donglin Cao, and Shaozi Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Re-ranking person re-identification with k-reciprocal encoding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, pages 1318–1327, 2017.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Unpaired image-to-image translation using cycle-consistent
adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, pages 2223–2232, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.10497" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.10498" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.10498">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.10498" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.10499" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  2 03:29:56 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
