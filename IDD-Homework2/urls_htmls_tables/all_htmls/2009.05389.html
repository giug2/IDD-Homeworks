<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2009.05389] ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data</title><meta property="og:description" content="This work introduces a novel strategy for generating synthetic training data for 2D and 3D pose estimation of animals using keyframe animations. With the objective to automate the process of creating animations for wil…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2009.05389">

<!--Generated on Sat Mar  9 05:03:29 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\SpecialIssuePaper</span><span id="p1.2" class="ltx_ERROR undefined">\BibtexOrBiblatex</span><span id="p1.3" class="ltx_ERROR undefined">\electronicVersion</span><span id="p1.4" class="ltx_ERROR undefined">\PrintedOrElectronic</span>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\teaser</span><img src="/html/2009.05389/assets/materials/top_image_8.jpg" id="p2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="99" alt="[Uncaptioned image]">
<p id="p2.2" class="ltx_p ltx_align_center"><span id="p2.2.1" class="ltx_text ltx_caption">Process from data generation to 3D pose estimation: (a) Original FBX animation file, (b) Camera setup in MotionBuilder, (c) sample training data, (d) 2D pose inference, (e) 3D pose inference.</span></p>
</div>
<h1 class="ltx_title ltx_title_document">ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id6.6.6" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:433.6pt;">
<span id="id6.6.6.6" class="ltx_p">A. S. Fangbemi<sup id="id6.6.6.6.1" class="ltx_sup">1</sup>, Y. F. Lu<sup id="id6.6.6.6.2" class="ltx_sup">1</sup>, M. Y. Xu<sup id="id6.6.6.6.3" class="ltx_sup">2</sup>, X. W. Luo<sup id="id6.6.6.6.4" class="ltx_sup">1</sup>, A. Rolland<sup id="id6.6.6.6.5" class="ltx_sup">1</sup>, C. Raissi<sup id="id6.6.6.6.6" class="ltx_sup"><span id="id6.6.6.6.6.1" class="ltx_text ltx_font_italic">3,4</span></sup></span>
</span>

<br class="ltx_break">
<span id="id10.10.10" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:433.6pt;">
<span id="id7.7.7.1" class="ltx_p"><sup id="id7.7.7.1.1" class="ltx_sup">1</sup>Ubisoft China AI &amp; Data Lab</span>
<span id="id8.8.8.2" class="ltx_p ltx_align_center"><sup id="id8.8.8.2.1" class="ltx_sup">2</sup>Sichuan University</span>
<span id="id9.9.9.3" class="ltx_p ltx_align_center"><sup id="id9.9.9.3.1" class="ltx_sup">3</sup>Ubisoft, Singapore</span>
<span id="id10.10.10.4" class="ltx_p ltx_align_center"><sup id="id10.10.10.4.1" class="ltx_sup">4</sup>INRIA Nancy Grand Est, France</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="11.1" class="ltx_p">This work introduces a novel strategy for generating synthetic training data for 2D and 3D pose estimation of animals using keyframe animations. With the objective to automate the process of creating animations for wildlife, we train several 2D and 3D pose estimation models with synthetic data, and put in place an end-to-end pipeline called <span id="11.1.1" class="ltx_text ltx_font_bold">ZooBuilder</span>. The pipeline takes as input a video of an animal in the wild, and generates the corresponding 2D and 3D coordinates for each joint of the animal’s skeleton. With this approach, we produce motion capture data that can be used to create animations for wildlife.
<span id="11.1.2" class="ltx_ERROR undefined">{CCSXML}</span>
&lt;ccs2012&gt;
&lt;concept&gt;
&lt;concept_id&gt;10010147.10010371.10010352.10010381&lt;/concept_id&gt;
&lt;concept_desc&gt;Computing methodologies Collision detection&lt;/concept_desc&gt;
&lt;concept_significance&gt;300&lt;/concept_significance&gt;
&lt;/concept&gt;
&lt;concept&gt;
&lt;concept_id&gt;10010583.10010588.10010559&lt;/concept_id&gt;
&lt;concept_desc&gt;Hardware Sensors and actuators&lt;/concept_desc&gt;
&lt;concept_significance&gt;300&lt;/concept_significance&gt;
&lt;/concept&gt;
&lt;concept&gt;
&lt;concept_id&gt;10010583.10010584.10010587&lt;/concept_id&gt;
&lt;concept_desc&gt;Hardware PCB design and layout&lt;/concept_desc&gt;
&lt;concept_significance&gt;100&lt;/concept_significance&gt;
&lt;/concept&gt;
&lt;/ccs2012&gt;</p>
<span id="12.2" class="ltx_ERROR undefined">\ccsdesc</span>
<p id="13.3" class="ltx_p">[300]Computing methodologies Animation</p>
<span id="14.4" class="ltx_ERROR undefined">\printccsdesc</span>
</div>
<span id="15" class="ltx_note ltx_note_frontmatter ltx_role_volume"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">volume: </span>39</span></span></span><span id="16" class="ltx_note ltx_note_frontmatter ltx_role_issue"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">issue: </span>8</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Thanks to the relative abundance of training datasets for humans, researchers have developed reliable deep learning approaches for 2D and 3D pose estimation<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">CHS<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>18</a>]</cite>. These datasets are usually created in a controlled environment such as a motion capture room where actors perform a variety of actions. In contrast to humans, bringing wild animals in a mocap room can be a challenging task which limits the availability of training datasets and research on the topic. In this paper, we address the difficult work of generating animal datasets for quadrupeds pose estimation by creating synthetic data from existing keyframe animations. Given an FBX animation file of a cougar, we create a set of cameras around it to build a virtual mocap room (Figure <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data</span></span>). From each camera view and for each frame of the animation, we extract the 2D and 3D coordinates of the skeleton, an image, as well as the parameters of the camera. Using this data, we retrain 2D and a 3D pose estimation models originally developed for humans. We include these models in an end-to-end pipeline called <em id="S1.p1.1.1" class="ltx_emph ltx_font_italic">“ZooBuilder”</em>, which takes as input a video of a cougar and outputs the corresponding 3D animation.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Approach</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Training data generation.</h3>

<figure id="S2.F1" class="ltx_figure"><img src="/html/2009.05389/assets/materials/dataset_3.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="299" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
Samples of the dataset used for training 2D pose estimation models.</figcaption>
</figure>
<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">We import FBX files containing keyframe animations of a cougar into Autodesk MotionBuilder and create a set of twelve cameras. For each camera and frame, we generate: (1) the 3D world coordinates for <math id="S2.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="37" display="inline"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.1a"><mn id="S2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">37</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.1b"><cn type="integer" id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1">37</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.1c">37</annotation></semantics></math> joints, (2) a rendered image, (3) the 3D world coordinates of the camera, its projection matrix and the 3D world coordinates of its focal point. We further generate the 3D view coordinates of the joints from the camera perspective by multiplying their 3D world coordinates by the view matrix. The 3D view coordinates are then used to compute the 2D coordinates of the joints on the projection plane, which are finally converted to 2D image coordinates.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">DP<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>10</a>]</cite>.</p>
</div>
<div id="S2.SS0.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p2.1" class="ltx_p">In addition, we post process images generated from MotionBuilder to add random backgrounds in order to increase their realism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">ZKL<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>16</a>]</cite>. The final training dataset consists of (1) cougar images with background, (2) 2D image coordinates (Figure <a href="#S2.F1" title="Figure 1 ‣ Training data generation. ‣ 2 Approach ‣ ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), and (3) 3D view coordinates of each joint for each image.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">ZooBuilder pipeline.</h3>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">ZooBuilder is an end-to-end pipeline that generates FBX animation files from 2D videos of animals. It involves three core steps leveraging machine learning models (ML) for (1) object detection (YoLoV3<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">RF18</a>]</cite>), (2) 2D pose estimation (OpenPose<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">CHS<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>18</a>]</cite>), and (3) 3D pose estimation (Pose_3D<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">RIHL18</a>]</cite>). We adapt the 2D and 3D pose estimation models to our synthetic cougar data and retrain them.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Challenges</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Irrealistic training data.</h3>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">The main challenge to overcome with our approach resides in the difference of distribution between the synthetic training data and the real test data collected from the wild. To ensure that models trained on synthetic data generalize well on realistic videos, we explore two main data transformation techniques, changing both training and test images into (1) gray-scale images or (2) using a style transfer technique to standardize the data (Figure <a href="#S3.F2" title="Figure 2 ‣ Irrealistic training data. ‣ 3 Challenges ‣ ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). Although training and testing our 2D pose estimation models on gray-scale images results in a significant improvement in the prediction accuracy, the results with style transfer under perform predictions done without style transfer.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2009.05389/assets/materials/data_trans_3.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="399" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>
Transformation to standardize train data (above) and test data (below) with style transfer (c, d) or gray-scale(e, f). </figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Training data volume.</h3>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">ML models are usually trained with a large quantity of images. Using transfer learning strategies, we leverage features learned by Resnet50 on the ImageNet dataset, and further train our 2D pose estimation model with our synthetic data composed of about <math id="S3.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="170" display="inline"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">170</mn><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><cn type="integer" id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">170</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">170</annotation></semantics></math> thousand images. To increase the diversity of images, we introduce different data augmentation techniques during the training phase such as image and joints rotation, scaling, flipping, Gaussian noise, color jittering, random brightness and contrast.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h3 class="ltx_title ltx_title_paragraph">Machine learning models tuning.</h3>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">Apart from the challenges mentioned above, retraining human based models for a cougar skeleton and dataset requires to invest efforts in fine-tuning the models. We explore different hyper-parameters configurations for the learning rate, optimizer, activation functions. We adopt a layer-specific learning rate approach coupled with a learning scheduler with the SGD optimizer to train a model that generalizes relatively well.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2009.05389/assets/materials/good_results.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="292" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Examples of 2D (above) and 3D (below) results on test data from the wild
</figcaption>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Quadrupeds pose estimation is a challenging task due to the lack of training data. In this work, we proposed a strategy to fill this gap by generating a synthetic dataset based on existing keyframe animations. Using the data generated, we successfully trained 2D and 3D pose estimation models, and integrated them into an end-to-end pipeline that infers 3D animations from videos of real cougars (Figure <a href="#S3.F3" title="Figure 3 ‣ Machine learning models tuning. ‣ 3 Challenges ‣ ZooBuilder: 2D and 3D Pose Estimation for Quadrupeds Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The pipeline still shows limitations when exposed to complex videos including multiple subjects, major occlusions, low contrast, etc. providing opportunities for further improvements on both the dataset generation and ML models development. This work could benefit from the contributions of both academic and industry peers who focus on quadrupeds pose estimation and animations.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CHS<sup id="bib.bibx1.4.4.1" class="ltx_sup"><span id="bib.bibx1.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx1.7.1" class="ltx_text ltx_font_smallcaps">Cao Z., Hidalgo G., Simon T., Wei S.-E., Sheikh Y.</span>:

</span>
<span class="ltx_bibblock">Openpose: realtime multi-person 2d pose estimation using part
affinity fields.

</span>
<span class="ltx_bibblock"><em id="bib.bibx1.8.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1812.08008</em> (2018).

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[DP<sup id="bib.bibx2.4.4.1" class="ltx_sup"><span id="bib.bibx2.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>10]</span>
<span class="ltx_bibblock">
<span id="bib.bibx2.7.1" class="ltx_text ltx_font_smallcaps">Dunn F., Parberry I., et al.</span>:

</span>
<span class="ltx_bibblock"><em id="bib.bibx2.8.1" class="ltx_emph ltx_font_italic">3D math primer for graphics and game development</em>.

</span>
<span class="ltx_bibblock">Jones &amp; Bartlett Publishers, 2010.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RF18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx3.1.1" class="ltx_text ltx_font_smallcaps">Redmon J., Farhadi A.</span>:

</span>
<span class="ltx_bibblock">Yolov3: An incremental improvement.

</span>
<span class="ltx_bibblock"><em id="bib.bibx3.2.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1804.02767</em> (2018).

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RIHL18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx4.1.1" class="ltx_text ltx_font_smallcaps">Rayat Imtiaz Hossain M., Little J. J.</span>:

</span>
<span class="ltx_bibblock">Exploiting temporal information for 3d human pose estimation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.2.1" class="ltx_emph ltx_font_italic">Proceedings of the European Conference on Computer Vision
(ECCV)</em> (2018), pp. 68–84.

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZKL<sup id="bib.bibx5.4.4.1" class="ltx_sup"><span id="bib.bibx5.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>16]</span>
<span class="ltx_bibblock">
<span id="bib.bibx5.7.1" class="ltx_text ltx_font_smallcaps">Zhou B., Khosla A., Lapedriza A., Torralba A., Oliva A.</span>:

</span>
<span class="ltx_bibblock">Places: An image database for deep scene understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bibx5.8.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1610.02055</em> (2016).

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="{\@shortauthor}"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="{Computer Graphics Forum"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="{\pdf@Subject}"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="{\@shorttitle}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2009.05388" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2009.05389" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2009.05389">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2009.05389" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2009.05390" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar  9 05:03:29 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
