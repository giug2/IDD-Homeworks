<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2205.06714] Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)</title><meta property="og:description" content="Robotic cloth manipulation
is challenging due to its deformability, which makes determining its full state infeasible. However, for cloth folding, it suffices to know the position of a few semantic keypoints.
Convoluti…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2205.06714">

<!--Generated on Mon Mar 11 12:59:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Synthetic Data,  Procedural Data Generation,  Keypoint Detection,  Robotic Cloth Folding
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\IEEEsettextheight</span>
<p id="p1.2" class="ltx_p">1in1in</p>
</div>
<h1 class="ltx_title ltx_title_document">Learning Keypoints from Synthetic Data for Robotic Cloth Folding
<span id="id3.id1" class="ltx_note ltx_role_thanks"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This research is supported by the Research Foundation Flanders
(FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Thomas Lips, Victor-Louis De Gusseme and Francis wyffels 
<br class="ltx_break">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id4.1.id1" class="ltx_text ltx_font_italic">AI and Robotics Lab (AIRO), IDLab,</span><span id="id5.2.id2" class="ltx_text ltx_font_italic">Ghent University - imec
<br class="ltx_break"></span>Thomas.Lips@UGent.be
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id6.id1" class="ltx_p">Robotic cloth manipulation
is challenging due to its deformability, which makes determining its full state infeasible. However, for cloth folding, it suffices to know the position of a few semantic keypoints.
Convolutional neural networks (CNN) can be used to detect these keypoints, but require large amounts of annotated data, which is expensive to collect.
To overcome this, we propose to learn these keypoint detectors purely from synthetic data, enabling low-cost data collection.
In this paper, we procedurally generate images of towels and use them to train a CNN. We evaluate the performance of this detector for folding towels on a unimanual robot setup and find that the grasp and fold success rates are 77% and 53%, respectively.
We conclude that learning keypoint detectors from synthetic data for cloth folding and related tasks is a promising research direction, discuss some failures and relate them to future work. A video of the system, as well as the codebase, more details on the CNN architecture and the training setup can be found at <a target="_blank" href="https://github.com/tlpss/workshop-icra-2022-cloth-keypoints.git" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tlpss/workshop-icra-2022-cloth-keypoints.git</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Synthetic Data, Procedural Data Generation, Keypoint Detection, Robotic Cloth Folding

</div>
<figure id="id1" class="ltx_figure ltx_align_center"><img src="/html/2205.06714/assets/figures/data-samples.jpg" id="id1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="399" alt="[Uncaptioned image]">
</figure>
<figure id="id2" class="ltx_figure ltx_align_center"><img src="/html/2205.06714/assets/figures/success-fold-examples.jpg" id="id2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="207" alt="[Uncaptioned image]">
</figure>
<figure id="S0.F1" class="ltx_figure ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S0.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S0.F1.3.2" class="ltx_text" style="font-size:90%;">Left - Examples of the synthetic data used to train the keypoint detector. Center/Right - Successful folds on towels from the evaluation set. Each image pair shows the initial state with the detected keypoints (yellow dots) and the final state after attempting to grasp and fold. </span></figcaption>
</figure>
<section id="S1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Robotic manipulation of deformable objects is challenging, both in terms of perception, control and modelling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. This is a.o. caused by their high-dimensional state <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Nonetheless, any general-purpose robotic manipulation system would encounter such objects. Cloth, for example, is omnipresent in household settings. Although significant progress has been made, cloth manipulation remains a challenging task in terms of perception and control <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Supervised deep learning (DL) has the potential to tackle these perception tasks and to generalise to a wide range of settings. However, DL is far from data-efficient and requires large numbers of labelled data to learn and generalise. Procedural data generation is an appealing alternative to manually collecting these datasets: Synthetic data is cheap to generate, has perfect annotations and can be used to generate all desired variations. The drawback is that the resulting network often has a reduced performance when transferred to the real world. This is due to the remaining differences between the simulation and the real world, referred to as the reality gap.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we tackle towel folding, a standard task for cloth manipulation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.
We approach this task by using a convolutional neural network (CNN) as keypoint detector to estimate the 2D positions of the towel corners from a single RGB image and executing a scripted, open-loop grasp and quasi-static fold trajectory based on these semantic keypoints. We train the keypoint detector entirely on synthetic data and evaluate the zero-shot sim-to-real performance on a unimanual robotic setup under various conditions. Using the terminology of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, we assume the towels are unfolded but not necessarily perfectly flattened. Recent progress in cloth unfolding has made this a realistic assumption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Our contributions are as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We show that synthetic data can be used to train convolutional neural networks to detect keypoints for cloth folding.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">We extensively evaluate the performance of our keypoint detector and unimanual folding system.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We provide qualitative insights into the failure cases.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.5.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.6.2" class="ltx_text ltx_font_italic">Robotic Cloth Folding</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Cloth manipulation has been extensively studied <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. For cloth folding in particular, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> have devised complete pipelines.
These works rely on segmentation and a combination of polygonal approximations and template matching or corner detectors to localise keypoints (landmarks) of unfolded cloth items and subsequently fold them. Although these systems perform very well in their test settings, the segmentation algorithms assume a known background color distribution, which strongly limits the potential to generalise to diverse environments. Furthermore the perception pipeline can take up to 5 seconds <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> to detect the keypoints.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.5.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.6.2" class="ltx_text ltx_font_italic">Learning Keypoint Representations for Robotic Control</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Recently, CNNs were trained to detect <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and even discover <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> semantic keypoints end-to-end for various robotic tasks. As the number of detected keypoints can often vary, keypoint detectors usually output spatial heatmaps instead of cartesian coordinates <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS3.5.1.1" class="ltx_text">II-C</span> </span><span id="S2.SS3.6.2" class="ltx_text ltx_font_italic">Synthetic Data for Computer Vision</span>
</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Procedural data generation for training perception models has been used to learn state representations for various robotic tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. The main limitation is the induced reality gap, which limits the performance on the real-world target domain. Tobin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> introduced domain randomization as a way to overcome the reality gap by enlarging the distribution from which the data is generated to ensure it entails the real-world data. However, more recent work has stressed the importance of more realistic image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and showed that over-randomizing can result in loss of context and performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.
Most authors find that even with domain randomization, the mismatch between both distributions is still too large and resort to finetuning with real-world data to increase performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. The factors that cause this reality gap are not well-known and one usually attempts to close it with task-specific tuning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Method</span>
</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.5.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.6.2" class="ltx_text ltx_font_italic">Synthetic Dataset Generation</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We use Blender <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> and BlenderProc <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> to generate data samples. For each image, we build a new 3D scene and randomize object geometries, materials, lighting, and camera pose. Examples can be found in Fig. <a href="#S0.F1" title="Figure 1 ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
The towel geometry is modelled as a rectangular mesh. We created a procedural material that combines a random HSV colour and a Perlin noise texture to omit the need for realistic fabric textures, which are difficult to obtain. Additionally, we sample up to 5 distractor objects from a subset of the Thingi10k dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> and add them randomly to the scene, as this was observed to reduce false positives on the manipulator and other objects present in the scene.
The ground plane material combines a random HSV base colour with a texture from PolyHaven<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://polyhaven.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://polyhaven.com/</a></span></span></span> to introduce spatial patterns.
To generate complex and realistic lighting of the scene, we use 360-degree images as environment textures, which are also obtained from PolyHaven. The position of the camera is sampled inside a spherical cap and its orientation is set to point towards the centre of the scene. Finally, the scene is rendered into a 256x256 image with Cycles, Blender’s physically-based path tracer.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.5.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.6.2" class="ltx_text ltx_font_italic">Keypoint Detector</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To detect the desired keypoints on an RGB image, we use a fully convolutional neural network to predict a single spatial heatmap that contains all corners of the cloth. Inspired by Vecerik et Al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, a U-net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> is used to combine spatial resolution conserving paths with downsampling paths to obtain a large receptive field.
In the spatial bottleneck, ResNet-inspired skip connections <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> are used. Bilinear upsampling is used for the upsampling layers. All hidden layers use the ReLU activation function. The final layer uses a sigmoid, of which the outputs are interpreted as the probability function of having a keypoint centred on that location <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
Heatmaps for the synthetic data samples are generated using a pixel-wise maximum over 2D Gaussian blobs that are centred on each keypoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. Pixel-wise Binary Cross Entropy is used as a training objective and keypoints are extracted using a max-filter with a configurable receptive field, as implemented in Scikit-Image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. We use Pytorch Lightning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to train the CNN and Weights and Biases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> for experiment tracking.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.5.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.6.2" class="ltx_text ltx_font_italic">Robotic Folding</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To initiate a fold, a top-down RGB image is passed through the keypoint detector network. If less than four keypoints are detected, the folding is aborted; otherwise, the four keypoints with the highest probability are extracted and reprojected onto the table plane. Based on these keypoints, a local frame is defined and the scripted pregrasp pose, grasp pose and fold trajectory in this local frame are transformed to the robot frame. The sequence is then executed by the robot. The robot grasps the towel in the middle of the side that needs to be folded. The folding trajectory is an arc from that grasp point to the corresponding point on the other side of the towel.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.5.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.6.2" class="ltx_text ltx_font_italic">Evaluation</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To evaluate the keypoint detector, two sets of towels are used.
All towels were collected by asking a number of lab members to bring random towels to reduce bias. The first set contains 11 towels with various colours and material properties but with uniform textures, as modelled in the synthetic data. These are referred to as the in-distribution towels. The second set consists of 9 towels that are not in the distribution of the synthetic data as they have highly non-uniform textures or very different material properties. We refer to this additional set as the out-of-distribution towels and include them to evaluate to what degree the neural network is able to generalise from the synthetic data.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">We use three different environment settings, in which we vary lighting and the presence of 6 random chosen distractor objects. For each setting, the robot attempts to fold each towel twice. The pose of the towel is manually randomised within the workspace of the robot before each attempt. For some larger towels, we already partially fold the towel to reduce their dimensions. The partially-folded towels range from 0.2 m to 0.5 m in size.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">As the most informative metric of a robotic system is the task performance, we evaluate the keypoint detector by measuring the grasp success rate. Grasp success is defined as the ability of the system to enclose the cloth at the grasp pose. Note that this is different from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, where grasps are only considered successful if they are held during the entire manipulation. Fold success is defined, as suggested in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, as the approximate coincidence of the opposing corners after manipulation.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS2.5.1.1" class="ltx_text">IV-B</span> </span><span id="S4.SS2.6.2" class="ltx_text ltx_font_italic">Keypoint Detector</span>
</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Using the procedure described in Section <a href="#S3.SS1" title="III-A Synthetic Dataset Generation ‣ III Method ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, a dataset of 30,000 images is generated, of which samples can be found in Fig. <a href="#S0.F1" title="Figure 1 ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The dataset is generated on a Dell XPS 9570 laptop with a low-range Nvidia GTX 1050Ti mobile GPU. Generating a single sample takes 1.8 s on average, of which 0.6 s is spent on building the scene and 1.2 s on rendering.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">We then train our keypoint detector to predict all visible corner keypoints using sensible hyperparameters based on previous experience. We train for 15 epochs, which takes 55 minutes on an Nvidia V100 GPU and results in an average precision (with 2 pixel threshold) of 80% on a synthetic validation set. We refer to the accompanying Github page for details about the network architecture and other training parameters.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS3.5.1.1" class="ltx_text">IV-C</span> </span><span id="S4.SS3.6.2" class="ltx_text ltx_font_italic">Robotic Setup</span>
</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We use a UR3e robot and Robotiq 2F-85 gripper. We 3D-printed fingertips with a width of 0.08 m using flexfill TPU 98A filament to ensure compliance of the fingers while sliding underneath the cloth for grasping. The camera is a ZED2i that is mounted 1 m above the table and of which the extrinsics have been determined upfront. The images are cropped to the relevant area and then rescaled to the CNN’s input size of 256x256 pixels.</p>
</div>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.06714/assets/figures/failed-keypoints.jpg" id="S4.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="419" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2205.06714/assets/figures/failed-folds.jpg" id="S4.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="417" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.3.2" class="ltx_text" style="font-size:90%;">a) Examples of trials for which the keypoint detection failed, resulting in a failed grasp and fold. b) Examples of trials for which the fold failed after a successful grasp. Each image pair shows the initial state with the detected keypoints (yellow dots) and the final state after attempting to grasp and fold. </span></figcaption>
</figure>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS4.5.1.1" class="ltx_text">IV-D</span> </span><span id="S4.SS4.6.2" class="ltx_text ltx_font_italic">Results</span>
</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">The performance of the system is reported in Table <a href="#S4.T1" title="TABLE I ‣ IV-D Results ‣ IV Experiments ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>. The total grasp success rate for the towels that are similar to those modelled in the synthetic data is about 77%. From the success rates, it is also clear that the presence of distractor items or changes in lighting conditions, both of which are important real-world domain shifts, do not influence the performance of the keypoint detector. This shows that the randomizations in the synthetic data generation were effective in this regard.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">The fold success rate is about 53%. Examples of successful folds (and hence grasps) for various settings can be seen in Fig. <a href="#S0.F1" title="Figure 1 ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The fold success rate is lower than the grasp success rate due to the limitations of scripted trajectories and single-arm manipulation: About 80% of all fold failures are caused by corner misalignment as the corners bend during the execution of the fold motion ( see bottom row of Fig. <a href="#S4.F2.sf2" title="In Figure 2 ‣ IV-C Robotic Setup ‣ IV Experiments ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>). This could be resolved with bimanual manipulation or by enlarging the fingertips to better perform a line fold <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. For towels that are already partially folded, the thickness and material properties can change, which makes the open-loop trajectory no longer suited to fold them (see top row example in Fig. <a href="#S4.F2.sf2" title="In Figure 2 ‣ IV-C Robotic Setup ‣ IV Experiments ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(b)</span></a>), explaining the remaining fold failures.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para">
<p id="S4.SS4.p3.1" class="ltx_p">Most grasp failures are due to inaccurate or incomplete keypoint detection by the transferred neural network, examples of which can be seen in Fig. <a href="#S4.F2.sf1" title="In Figure 2 ‣ IV-C Robotic Setup ‣ IV Experiments ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>. These failures indicate there is still a reality gap that was not covered by the variations introduced in our synthetic dataset, requiring additional tuning to better match the target distribution. For the towels that contained non-uniform textures and were hence not explicitly modelled, the grasp success rate deteriorates to 48% (see Table <a href="#S4.T1" title="TABLE I ‣ IV-D Results ‣ IV Experiments ‣ Learning Keypoints from Synthetic Data for Robotic Cloth Folding This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG)" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>).
The remaining grasp failures are due to some towels being very light, causing the fingers to push the cloth away instead of reaching underneath and grasping it due to a lack of friction.</p>
</div>
<div id="S4.SS4.p4" class="ltx_para">
<p id="S4.SS4.p4.1" class="ltx_p">Finally, as recommended in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, we report the execution time of our robot system: A forward pass of the neural network is, even on CPU, negligibly fast (<math id="S4.SS4.p4.1.m1.1" class="ltx_Math" alttext="\ll" display="inline"><semantics id="S4.SS4.p4.1.m1.1a"><mo id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml">≪</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><csymbol cd="latexml" id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1">much-less-than</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">\ll</annotation></semantics></math> 1 s) compared to the time it takes to execute the grasp and fold, which is about 20 s.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<th id="S4.T1.2.1.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_t">Setting</th>
<th id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">In-distribution Towels</th>
<th id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2">Out-of-distribution Towels</th>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<th id="S4.T1.2.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Grasp</th>
<th id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Fold</th>
<th id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Grasp</th>
<th id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Fold</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.2.3.1" class="ltx_tr">
<th id="S4.T1.2.3.1.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t">Natural Light</th>
<td id="S4.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">17/22</td>
<td id="S4.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">14/22</td>
<td id="S4.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">9/18</td>
<td id="S4.T1.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">8/18</td>
</tr>
<tr id="S4.T1.2.4.2" class="ltx_tr">
<th id="S4.T1.2.4.2.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">LED Light</th>
<td id="S4.T1.2.4.2.2" class="ltx_td ltx_align_center">16/22</td>
<td id="S4.T1.2.4.2.3" class="ltx_td ltx_align_center">10/22</td>
<td id="S4.T1.2.4.2.4" class="ltx_td ltx_align_center">8/18</td>
<td id="S4.T1.2.4.2.5" class="ltx_td ltx_align_center">8/18</td>
</tr>
<tr id="S4.T1.2.5.3" class="ltx_tr">
<th id="S4.T1.2.5.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row">Distractors</th>
<td id="S4.T1.2.5.3.2" class="ltx_td ltx_align_center">18/22</td>
<td id="S4.T1.2.5.3.3" class="ltx_td ltx_align_center">11/22</td>
<td id="S4.T1.2.5.3.4" class="ltx_td ltx_align_center">9/18</td>
<td id="S4.T1.2.5.3.5" class="ltx_td ltx_align_center">4/18</td>
</tr>
<tr id="S4.T1.2.6.4" class="ltx_tr">
<th id="S4.T1.2.6.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b">Total</th>
<td id="S4.T1.2.6.4.2" class="ltx_td ltx_align_center ltx_border_b">51/66 (77%)</td>
<td id="S4.T1.2.6.4.3" class="ltx_td ltx_align_center ltx_border_b">35/66 (53%)</td>
<td id="S4.T1.2.6.4.4" class="ltx_td ltx_align_center ltx_border_b">26/54 (48%)</td>
<td id="S4.T1.2.6.4.5" class="ltx_td ltx_align_center ltx_border_b">20/54 (37%)</td>
</tr>
</tbody>
</table>
<br class="ltx_break ltx_centering">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">TABLE I</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Performance of the system for real-world robotic folding of towels using zero-shot transfer for the keypoint detector.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusion and Future Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this paper, we used synthetic data to train a neural network to detect keypoints as a low-dimensional state representation for cloth folding. We transferred the detector without any finetuning (zero-shot) and measured its performance by using the keypoints for folding towels on a robot setup. The results indicate that using procedural data generation is a viable approach to training keypoint detectors for cloth manipulation. However, more extensive tuning is required to completely overcome the reality gap. In future work, we plan to further explore what factors matter to close this gap. This will enable increased performance and provide more principled guidelines for procedural data generation in general.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Additionally, the gap between the grasp and fold success rates indicates that not only the perception but also the manipulation of cloth remains a challenging task. In future work, we therefore aim to close the control loop to take the cloth properties into account and make the control more robust.</p>
</div>
</section>
<section id="Sx1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_font_smallcaps ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors wish to thank the members of the <span id="Sx1.p1.1.1" class="ltx_text ltx_font_italic">Keypoints Gang</span>, in particular Rembert Daems and Peter De Roovere, for sharing many insights into Computer Vision and academic research in general. This research is supported by the Research Foundation Flanders (FWO) under Grant numbers 1S56022N (TL) and 1SD4421N (VDG).</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography ltx_centering">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H. Yin, A. Varava, and D. Kragic, “Modeling, learning, perception, and control
methods for deformable object manipulation,” <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Science Robotics</em>,
vol. 6, no. 54, p. eabd8803, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Sanchez, J.-A. Corrales, B.-C. Bouzgarrou, and Y. Mezouar, “Robotic
manipulation and sensing of deformable objects in domestic and industrial
applications: a survey,” <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">The International Journal of Robotics
Research</em>, vol. 37, no. 7, pp. 688–716, 2018.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Borràs, G. Alenyà, and C. Torras, “A grasping-centered analysis for cloth
manipulation,” <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Robotics</em>, vol. 36, no. 3, pp.
924–936, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Doumanoglou, J. Stria, G. Peleka, I. Mariolis, V. Petrík, A. Kargakos,
L. Wagner, V. Hlaváč, T.-K. Kim, and S. Malassiotis, “Folding clothes
autonomously: A complete pipeline,” <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Robotics</em>,
vol. 32, no. 6, pp. 1461–1478, 2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
I. Garcia-Camacho, M. Lippi, M. C. Welle, H. Yin, R. Antonova, A. Varava,
J. Borras, C. Torras, A. Marino, G. Alenyà, and D. Kragic, “Benchmarking
bimanual cloth manipulation,” <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and Automation Letters</em>,
vol. 5, no. 2, pp. 1111–1118, 2020.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Maitin-Shepard, M. Cusumano-Towner, J. Lei, and P. Abbeel, “Cloth grasp
point detection based on multiple-view geometric cues with application to
robotic towel folding,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">2010 IEEE International Conference on
Robotics and Automation</em>, 2010, pp. 2308–2315.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Verleysen, M. Biondina, and F. Wyffels, “Video dataset of human
demonstrations of folding clothing for robotic folding,” <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">The
International Journal of Robotics Research</em>, vol. 39, no. 9, pp. 1031–1036,
2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H. Ha and S. Song, “Flingbot: The unreasonable effectiveness of dynamic
manipulation for cloth unfolding,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Conference on Robot
Learning</em>.   PMLR, 2022, pp. 24–33.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
S. Miller, J. van den Berg, M. Fritz, T. Darrell, K. Goldberg, and P. Abbeel,
“A geometric approach to robotic laundry folding,” <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">The International
Journal of Robotics Research</em>, vol. 31, no. 2, pp. 249–267, 2012.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Vecerik, J.-B. Regli, O. Sushkov, D. Barker, R. Pevceviciute,
T. Rothörl, R. Hadsell, L. Agapito, and J. Scholz, “S3k: Self-supervised
semantic keypoints for robotic manipulation via multi-view consistency,” in
<em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Conference on Robot Learning</em>.   PMLR, 2021, pp. 449–460.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Qin, K. Fang, Y. Zhu, L. Fei-Fei, and S. Savarese, “Keto: Learning keypoint
representations for tool manipulation,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2020 IEEE International
Conference on Robotics and Automation (ICRA)</em>, 2020, pp. 7278–7285.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Wang, S. Lin, C. Hu, Y. Zhu, and L. Zhu, “Learning semantic keypoint
representations for door opening manipulation,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">IEEE Robotics and
Automation Letters</em>, vol. 5, no. 4, pp. 6980–6987, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional network for
real-time 6-dof camera relocalization,” in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
international conference on computer vision</em>, 2015, pp. 2938–2946.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
T. D. Kulkarni, A. Gupta, C. Ionescu, S. Borgeaud, M. Reynolds, A. Zisserman,
and V. Mnih, “Unsupervised learning of object keypoints for perception and
control,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 32,
2019.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
B. Chen, P. Abbeel, and D. Pathak, “Unsupervised learning of visual 3d
keypoints for control,” in <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine
Learning</em>.   PMLR, 2021, pp. 1539–1549.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
T. Jakab, A. Gupta, H. Bilen, and A. Vedaldi, “Unsupervised learning of object
landmarks through conditional image generation,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in Neural
Information Processing Systems</em>, vol. 31, 2018.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
X. Zhou, D. Wang, and P. Krähenbühl, “Objects as points,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">arXiv
preprint arXiv:1904.07850</em>, 2019.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
J. Tremblay, T. To, B. Sundaralingam, Y. Xiang, D. Fox, and S. Birchfield,
“Deep object pose estimation for semantic robotic grasping of household
objects,” in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Conference on Robot Learning</em>.   PMLR, 2018, pp. 306–316.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel, “Domain
randomization for transferring deep neural networks from simulation to the
real world,” in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ international conference on intelligent
robots and systems (IROS)</em>.   IEEE,
2017, pp. 23–30.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
A. Tsirikoglou, J. Kronander, M. Wrenninge, and J. Unger, “Procedural modeling
and physically based rendering for synthetic data generation in automotive
applications,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1710.06270</em>, 2017.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Prakash, S. Boochoon, M. Brophy, D. Acuna, E. Cameracci, G. State,
O. Shapira, and S. Birchfield, “Structured domain randomization: Bridging
the reality gap by context-aware synthetic data,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2019
International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2019, pp. 7249–7255.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
J. Tremblay, A. Prakash, D. Acuna, M. Brophy, V. Jampani, C. Anil, T. To,
E. Cameracci, S. Boochoon, and S. Birchfield, “Training deep networks with
synthetic data: Bridging the reality gap by domain randomization,” in
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition workshops</em>, 2018, pp. 969–977.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
B. O. Community, <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Blender - a 3D modelling and rendering package</em>, Blender
Foundation, Stichting Blender Foundation, Amsterdam, 2018. [Online].
Available: <a target="_blank" href="http://www.blender.org" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://www.blender.org</a>

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
M. Denninger, M. Sundermeyer, D. Winkelbauer, Y. Zidan, D. Olefir,
M. Elbadrawy, A. Lodhi, and H. Katam, “Blenderproc,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1911.01911</em>, 2019.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Q. Zhou and A. Jacobson, “Thingi10k: A dataset of 10,000 3d-printing models,”
<em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1605.04797</em>, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for
biomedical image segmentation,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">International Conference on Medical
image computing and computer-assisted intervention</em>.   Springer, 2015, pp. 234–241.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision
and pattern recognition</em>, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
S. Van der Walt, J. L. Schönberger, J. Nunez-Iglesias, F. Boulogne, J. D.
Warner, N. Yager, E. Gouillart, and T. Yu, “scikit-image: image processing
in python,” <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">PeerJ</em>, vol. 2, p. e453, 2014.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
W. Falcon and The PyTorch Lightning team, “PyTorch Lightning,” 3 2019.
[Online]. Available:
<a target="_blank" href="https://github.com/PyTorchLightning/pytorch-lightning" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/PyTorchLightning/pytorch-lightning</a>

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
L. Biewald, “Experiment tracking with weights and biases,” 2020, software
available from wandb.com. [Online]. Available: <a target="_blank" href="https://www.wandb.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.wandb.com/</a>

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2205.06713" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2205.06714" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2205.06714">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2205.06714" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2205.06715" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 12:59:38 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
