<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.14672] Quantifying Character Similarity with Vision Transformers</title><meta property="og:description" content="Record linkage is a bedrock of quantitative social science, as analyses often require linking data from multiple, noisy sources. Off-the-shelf string matching methods are widely used, as they are straightforward and châ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Quantifying Character Similarity with Vision Transformers">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Quantifying Character Similarity with Vision Transformers">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.14672">

<!--Generated on Thu Feb 29 05:50:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Quantifying Character Similarity with Vision Transformers</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinmei Yang<sup id="id9.9.id1" class="ltx_sup"><span id="id9.9.id1.1" class="ltx_text ltx_font_italic">1</span></sup>, Abhishek Arora<sup id="id10.10.id2" class="ltx_sup"><span id="id10.10.id2.1" class="ltx_text ltx_font_italic">2</span></sup>, Shao-Yu Jheng<sup id="id11.11.id3" class="ltx_sup"><span id="id11.11.id3.1" class="ltx_text ltx_font_italic">2</span></sup>, Melissa Dell<math id="id4.4.m4.2" class="ltx_Math" alttext="{}^{2,3^{\ast}}" display="inline"><semantics id="id4.4.m4.2a"><msup id="id4.4.m4.2.2" xref="id4.4.m4.2.2.cmml"><mi id="id4.4.m4.2.2a" xref="id4.4.m4.2.2.cmml"></mi><mrow id="id4.4.m4.2.2.2.2" xref="id4.4.m4.2.2.2.3.cmml"><mn id="id4.4.m4.1.1.1.1" xref="id4.4.m4.1.1.1.1.cmml">2</mn><mo id="id4.4.m4.2.2.2.2.2" xref="id4.4.m4.2.2.2.3.cmml">,</mo><msup id="id4.4.m4.2.2.2.2.1" xref="id4.4.m4.2.2.2.2.1.cmml"><mn id="id4.4.m4.2.2.2.2.1.2" xref="id4.4.m4.2.2.2.2.1.2.cmml">3</mn><mo id="id4.4.m4.2.2.2.2.1.3" xref="id4.4.m4.2.2.2.2.1.3.cmml">âˆ—</mo></msup></mrow></msup><annotation-xml encoding="MathML-Content" id="id4.4.m4.2b"><apply id="id4.4.m4.2.2.cmml" xref="id4.4.m4.2.2"><list id="id4.4.m4.2.2.2.3.cmml" xref="id4.4.m4.2.2.2.2"><cn type="integer" id="id4.4.m4.1.1.1.1.cmml" xref="id4.4.m4.1.1.1.1">2</cn><apply id="id4.4.m4.2.2.2.2.1.cmml" xref="id4.4.m4.2.2.2.2.1"><csymbol cd="ambiguous" id="id4.4.m4.2.2.2.2.1.1.cmml" xref="id4.4.m4.2.2.2.2.1">superscript</csymbol><cn type="integer" id="id4.4.m4.2.2.2.2.1.2.cmml" xref="id4.4.m4.2.2.2.2.1.2">3</cn><ci id="id4.4.m4.2.2.2.2.1.3.cmml" xref="id4.4.m4.2.2.2.2.1.3">âˆ—</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="id4.4.m4.2c">{}^{2,3^{\ast}}</annotation></semantics></math> 
<br class="ltx_break"><sup id="id12.12.id4" class="ltx_sup"><span id="id12.12.id4.1" class="ltx_text ltx_font_italic">1</span></sup>Renmin University; Beijing, China.
<br class="ltx_break"><sup id="id13.13.id5" class="ltx_sup"><span id="id13.13.id5.1" class="ltx_text ltx_font_italic">2</span></sup>Harvard University; Cambridge, MA, USA.
<br class="ltx_break"><sup id="id14.14.id6" class="ltx_sup"><span id="id14.14.id6.1" class="ltx_text ltx_font_italic">3</span></sup>National Bureau of Economic Research; Cambridge, MA, USA.
<br class="ltx_break"><sup id="id15.15.id7" class="ltx_sup">âˆ—</sup>Corresponding author: melissadell@fas.harvard.edu
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id16.id1" class="ltx_p">Record linkage is a bedrock of quantitative social science, as analyses often require linking data from multiple, noisy sources. Off-the-shelf string matching methods are widely used, as they are straightforward and cheap to implement and scale. Not all character substitutions are equally probable, and for some settings there are widely used handcrafted lists denoting which string substitutions are more likely, that improve the accuracy of string matching. However, such lists do not exist for many settings, skewing research with linked datasets towards a few high-resource contexts that are not representative of the diversity of human societies. This study develops an extensible way to measure character substitution costs for OCRâ€™ed documents, by employing large-scale self-supervised training of vision transformers (ViT) with augmented digital fonts. For each language written with the CJK script, we contrastively learn a metric space where different augmentations of the same character are represented nearby. In this space, homoglyphic characters - those with similar appearance such as â€œOâ€ and â€œ0â€ - have similar vector representations. Using the cosine distance between charactersâ€™ representations as the substitution cost in an edit distance matching algorithm significantly improves record linkage compared to other widely used string matching methods, as OCR errors tend to be homoglyphic in nature. Homoglyphs can plausibly capture character visual similarity across <span id="id16.id1.1" class="ltx_text ltx_font_italic">any</span> script, including low-resource settings. We illustrate this by creating homoglyph sets for 3,000 year old ancient Chinese characters, which are highly pictorial. Fascinatingly, a ViT is able to capture relationships in how different abstract concepts were conceptualized by ancient societies, that have been noted in the archaeological literature.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Many quantitative analyses in the social sciences - as well as government and business applications - require linking information from multiple datasets.
For example, researchers and governments link historical censuses, match hand-written records from vaccination campaigns to administrative data, and de-duplicate voter rolls.
The sources to be linked often contain noise, particularly when they were created with optical character recognition (OCR).
String matching methods are widely used to link entities across datasets, as they are straightforward to implement off-the-shelf and can be scaled to massive datasets <cite class="ltx_cite ltx_citemacro_cite">Binette and Steorts (<a href="#bib.bib7" title="" class="ltx_ref">2022</a>); Abramitzky etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.3" class="ltx_p">Most simply, approximate string matching methods count the number of edits (insertions, deletions, and substitutions) to transform one string into another <cite class="ltx_cite ltx_citemacro_cite">Levenshtein etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">1966</a>)</cite>. Another common approach computes the similarity between <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p2.1.m1.1a"><mi id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><ci id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">n</annotation></semantics></math>-gram representations of strings, where <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p2.2.m2.1a"><mi id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><ci id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">n</annotation></semantics></math>-grams are all substrings of length <math id="S1.p2.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p2.3.m3.1a"><mi id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><ci id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">n</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">Okazaki and Tsujii (<a href="#bib.bib33" title="" class="ltx_ref">2010</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In practice, not all string substitutions are equally probable, and efforts to construct lists that vary their costs date back over a century.
For example, in 1918 Russell and Odell patented Soundex, a sound standardization toolkit that accounts for the fact that census enumerators often misspelled names according to their sound. Together with the updated New York State Identification and Intelligence System <cite class="ltx_cite ltx_citemacro_cite">Silbert (<a href="#bib.bib41" title="" class="ltx_ref">1970</a>)</cite>, it remains a bedrock for linking U.S. historical censuses <cite class="ltx_cite ltx_citemacro_cite">Abramitzky etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2021</a>)</cite>.
Similarly, <cite class="ltx_cite ltx_citemacro_citet">Novosad (<a href="#bib.bib32" title="" class="ltx_ref">2018</a>)</cite> adjusts Levenshtein distance to impose smaller penalties for common alternative spellings in Hindi, and the FuzzyChinese package <cite class="ltx_cite ltx_citemacro_cite">znwang25 (<a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite> uses strokes as the unit for <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S1.p3.1.m1.1a"><mi id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">n</annotation></semantics></math>-grams substring representations, where the strokes for a given character are drawn from an external database <cite class="ltx_cite ltx_citemacro_cite">kfcd (<a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite> covering a subset of the CJK script. Characters sharing strokes are more likely to be matched.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Such methods can perform well in the contexts for which they are tailored but are labor-intensive to extend to new settings, due to the use of hand-crafted features. Low extensibility skews research with linked data - necessary to examine intergenerational mobility, the evolution of firm productivity, the persistence of poverty, and many other topics - towards a few higher resource settings that are not representative of the diversity of human societies.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This study aims to preserve the advantages of string matching methods - simple off-the-shelf implementation and high scalability - while developing an extensible, self-supervised method for determining the relative costs of character substitutions in databases created with OCR. OCR often confuses characters with their homoglyphs, which have a similar visual appearance (<span id="S1.p5.1.1" class="ltx_text ltx_font_italic">e.g.</span> â€œ0â€ and â€œOâ€). Incorporating character visual similarity into string matching can thus plausibly improve record linkage.
Homoglyphs can be constructed by hand for small script sets such as Latin, as in a psychology literature on literacy acquisition <cite class="ltx_cite ltx_citemacro_cite">Simpson etÂ al. (<a href="#bib.bib42" title="" class="ltx_ref">2013</a>)</cite>, but for a script such as CJK, containing over 38,000 characters, this is infeasible.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Following a literature on self-supervision through simple data augmentation for image encoders <cite class="ltx_cite ltx_citemacro_cite">Grill etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>); Chen etÂ al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>); Chen and He (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>, this study uses augmented digital fonts to contrastively learn a metric space where different augmentations of a character have similar vector representations. The resulting space can be used, with a reference font, to measure the visual similarity of different characters. Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows representative examples of how the same characters are rendered very differently across fonts. These different representations form positive examples for the contrastively trained <span id="S1.p6.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> model. This purely self-supervised approach can be extended to any character set, but since creating evaluation data for record linkage is costly, the study focuses on languages written with CJK: Simplified and Traditional Chinese, Japanese, and Korean.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.14672/assets/figs/char_font.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="819" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span id="S1.F1.3.1" class="ltx_text ltx_font_bold">Character variation across fonts.</span> This figure illustrates examples of the same character rendered with different fonts. Augmentations of these comprise positives in the <span id="S1.F1.4.2" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> training data.</figcaption>
</figure>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">We train on augmentations of the same character - rather than paired data across characters - because a self-supervised approach is more extensible. Paired character similarity data are limited. Unicode maintains a set of confusables - constructed with rule-based methods - but for CJK the only confusables are structurally identical characters with different Unicode codepoints. Despite a large post-OCR error correction literature <cite class="ltx_cite ltx_citemacro_cite">Lyu etÂ al. (<a href="#bib.bib28" title="" class="ltx_ref">2021</a>); Nguyen etÂ al. (<a href="#bib.bib31" title="" class="ltx_ref">2021</a>); van Strien. etÂ al. (<a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>, there is also limited ground truth data about the types of errors that OCR makes across architectures, languages, scripts, layouts, and document contexts.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">Using the cosine distance between two characters as the substitution cost within a Levenshtein edit distance framework <cite class="ltx_cite ltx_citemacro_cite">Levenshtein etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">1966</a>)</cite> improves record linkage with 1950s firm level data about Japanese supply chains <cite class="ltx_cite ltx_citemacro_cite">Jinji Koshinjo (<a href="#bib.bib19" title="" class="ltx_ref">1954</a>); Teikoku Koshinjo (<a href="#bib.bib44" title="" class="ltx_ref">1957</a>)</cite>, relative to other string matching methods. The study also compares to end-to-end deep learning methods for record linkage. While these methods can outperform string matching, the data required for them are not always available and technical requirements for implementation are higher, explaining why string matching methods predominate in social science applications. Homoglyphic matching is a cheap and extensible way to improve these predominant methods.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Because creating annotated ground truth data is costly, we provide additional evaluations using synthetically generated data. We augment image renders of place and firm names written with different fonts, for Simplified and Traditional Chinese, Japanese, and Korean character sets. We then OCR two different views of each entity with different OCR engines - EasyOCR and PaddleOCR - that use very different architectures. The different augmentations and OCR engines lead to different text string views of the same entity with high frequency. We then link these using string matching methods. Homoglyphic matching outperforms other widely used string matching techniques for all four scripts. Our <span id="S1.p9.1.1" class="ltx_text ltx_font_typewriter">HomoglyphsCJK</span> python package provides a simple, off-the-shelf implementation.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Package available at <a target="_blank" href="https://pypi.org/project/HomoglyphsCJK/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pypi.org/project/HomoglyphsCJK/</a>.</span></span></span></p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p">Homoglyphs can be extended to any script. To explore this, we contrastively train a <span id="S1.p10.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> model for ancient Chinese characters, using a database that provides views of the same character from different archaeological sites and time periods <cite class="ltx_cite ltx_citemacro_cite">AcademiaÂ Sinica etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>. Ancient characters are much more pictorial than their more abstract, modern equivalents. Fascinatingly, homoglyphs constructed with a ViT for the Shang Dynasty (1600 BC-1045 BC) capture ways in which ancient Chinese society related abstract concepts that have been noted in the archaeological literature (<span id="S1.p10.1.2" class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_citet">Wang (<a href="#bib.bib49" title="" class="ltx_ref">2003</a>)</cite>).</p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">The rest of this study is organized as follows:
Section <a href="#S2" title="2 Methods â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> develops methods for learning character similarity and incorporating it into record linkage, and Section <a href="#S3" title="3 Evaluation Datasets â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes the evaluation datasets.
Section <a href="#S4" title="4 Results â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> compares the performance of homoglyphic edit distance to other string matching and neural methods for record linkage. Section <a href="#S5" title="5 Extending Homoglyphs â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> examines extensibility by constructing homoglyphs for ancient Chinese, Section <a href="#S6" title="6 Limitations â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> discusses the limitations of homoglyphs, and Section <a href="#S7" title="7 Conclusion â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> concludes.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>The <span id="S2.SS1.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> model</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> model contrastively learns a mapping between character crops and dense vector representations, such that crops of augmentations of the same character are nearby. <span id="S2.SS1.p1.1.2" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> is trained purely on digital fonts. Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows variations of the same characters rendered with different fonts, which form positive examples for training. Variations across fonts are non-trivial, forcing the model to learn character similarities at varying levels of abstraction.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2305.14672/assets/figs/char_near.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="819" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span><span id="S2.F2.3.1" class="ltx_text ltx_font_bold">Homoglyphs.</span> This figure illustrates the five nearest neighbors in the <span id="S2.F2.4.2" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> embedding space for representative characters.</figcaption>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">We use a DINO (Self-<span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_bold">Di</span>stillation, <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_bold">No</span> Labels) pre-trained ViT as the encoder <cite class="ltx_cite ltx_citemacro_cite">Caron etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite>.
DINO ViT embeddings perform well as a nearest neighbor classifier, making them well-suited for homoglyphic matching.
The model is trained using a Supervised Contrastive loss function <cite class="ltx_cite ltx_citemacro_cite">Khosla etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>, a generalization of the InfoNCE loss <cite class="ltx_cite ltx_citemacro_cite">Oord etÂ al. (<a href="#bib.bib34" title="" class="ltx_ref">2018</a>)</cite> that allows for multiple positive and negative pairs for a given anchor:</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.8" class="ltx_Math" alttext="\sum_{i\in I}\frac{-1}{|P(i)|}\sum_{p\in P(i)}\log\frac{\exp\left(\boldsymbol{z}_{i}\cdot\boldsymbol{z}_{p}/\tau\right)}{\sum_{a\in A(i)}\exp\left(\boldsymbol{z}_{i}\cdot\boldsymbol{z}_{a}/\tau\right)}" display="block"><semantics id="S2.E1.m1.8a"><mrow id="S2.E1.m1.8.9" xref="S2.E1.m1.8.9.cmml"><munder id="S2.E1.m1.8.9.1" xref="S2.E1.m1.8.9.1.cmml"><mo movablelimits="false" id="S2.E1.m1.8.9.1.2" xref="S2.E1.m1.8.9.1.2.cmml">âˆ‘</mo><mrow id="S2.E1.m1.8.9.1.3" xref="S2.E1.m1.8.9.1.3.cmml"><mi id="S2.E1.m1.8.9.1.3.2" xref="S2.E1.m1.8.9.1.3.2.cmml">i</mi><mo id="S2.E1.m1.8.9.1.3.1" xref="S2.E1.m1.8.9.1.3.1.cmml">âˆˆ</mo><mi id="S2.E1.m1.8.9.1.3.3" xref="S2.E1.m1.8.9.1.3.3.cmml">I</mi></mrow></munder><mrow id="S2.E1.m1.8.9.2" xref="S2.E1.m1.8.9.2.cmml"><mfrac id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml"><mrow id="S2.E1.m1.2.2.4" xref="S2.E1.m1.2.2.4.cmml"><mo id="S2.E1.m1.2.2.4a" xref="S2.E1.m1.2.2.4.cmml">âˆ’</mo><mn id="S2.E1.m1.2.2.4.2" xref="S2.E1.m1.2.2.4.2.cmml">1</mn></mrow><mrow id="S2.E1.m1.2.2.2.2" xref="S2.E1.m1.2.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.2.2" xref="S2.E1.m1.2.2.2.3.1.cmml">|</mo><mrow id="S2.E1.m1.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.1.cmml"><mi id="S2.E1.m1.2.2.2.2.1.2" xref="S2.E1.m1.2.2.2.2.1.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml">â€‹</mo><mrow id="S2.E1.m1.2.2.2.2.1.3.2" xref="S2.E1.m1.2.2.2.2.1.cmml"><mo stretchy="false" id="S2.E1.m1.2.2.2.2.1.3.2.1" xref="S2.E1.m1.2.2.2.2.1.cmml">(</mo><mi id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.2.2.2.2.1.3.2.2" xref="S2.E1.m1.2.2.2.2.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.E1.m1.2.2.2.2.3" xref="S2.E1.m1.2.2.2.3.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S2.E1.m1.8.9.2.1" xref="S2.E1.m1.8.9.2.1.cmml">â€‹</mo><mrow id="S2.E1.m1.8.9.2.2" xref="S2.E1.m1.8.9.2.2.cmml"><munder id="S2.E1.m1.8.9.2.2.1" xref="S2.E1.m1.8.9.2.2.1.cmml"><mo movablelimits="false" id="S2.E1.m1.8.9.2.2.1.2" xref="S2.E1.m1.8.9.2.2.1.2.cmml">âˆ‘</mo><mrow id="S2.E1.m1.3.3.1" xref="S2.E1.m1.3.3.1.cmml"><mi id="S2.E1.m1.3.3.1.3" xref="S2.E1.m1.3.3.1.3.cmml">p</mi><mo id="S2.E1.m1.3.3.1.2" xref="S2.E1.m1.3.3.1.2.cmml">âˆˆ</mo><mrow id="S2.E1.m1.3.3.1.4" xref="S2.E1.m1.3.3.1.4.cmml"><mi id="S2.E1.m1.3.3.1.4.2" xref="S2.E1.m1.3.3.1.4.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.1.4.1" xref="S2.E1.m1.3.3.1.4.1.cmml">â€‹</mo><mrow id="S2.E1.m1.3.3.1.4.3.2" xref="S2.E1.m1.3.3.1.4.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.1.4.3.2.1" xref="S2.E1.m1.3.3.1.4.cmml">(</mo><mi id="S2.E1.m1.3.3.1.1" xref="S2.E1.m1.3.3.1.1.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.3.3.1.4.3.2.2" xref="S2.E1.m1.3.3.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mrow id="S2.E1.m1.8.9.2.2.2" xref="S2.E1.m1.8.9.2.2.2.cmml"><mi id="S2.E1.m1.8.9.2.2.2.1" xref="S2.E1.m1.8.9.2.2.2.1.cmml">log</mi><mo lspace="0.167em" id="S2.E1.m1.8.9.2.2.2a" xref="S2.E1.m1.8.9.2.2.2.cmml">â¡</mo><mfrac id="S2.E1.m1.8.8" xref="S2.E1.m1.8.8.cmml"><mrow id="S2.E1.m1.5.5.2.2" xref="S2.E1.m1.5.5.2.3.cmml"><mi id="S2.E1.m1.4.4.1.1" xref="S2.E1.m1.4.4.1.1.cmml">exp</mi><mo id="S2.E1.m1.5.5.2.2a" xref="S2.E1.m1.5.5.2.3.cmml">â¡</mo><mrow id="S2.E1.m1.5.5.2.2.1" xref="S2.E1.m1.5.5.2.3.cmml"><mo id="S2.E1.m1.5.5.2.2.1.2" xref="S2.E1.m1.5.5.2.3.cmml">(</mo><mrow id="S2.E1.m1.5.5.2.2.1.1" xref="S2.E1.m1.5.5.2.2.1.1.cmml"><mrow id="S2.E1.m1.5.5.2.2.1.1.2" xref="S2.E1.m1.5.5.2.2.1.1.2.cmml"><msub id="S2.E1.m1.5.5.2.2.1.1.2.2" xref="S2.E1.m1.5.5.2.2.1.1.2.2.cmml"><mi id="S2.E1.m1.5.5.2.2.1.1.2.2.2" xref="S2.E1.m1.5.5.2.2.1.1.2.2.2.cmml">ğ’›</mi><mi id="S2.E1.m1.5.5.2.2.1.1.2.2.3" xref="S2.E1.m1.5.5.2.2.1.1.2.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.5.5.2.2.1.1.2.1" xref="S2.E1.m1.5.5.2.2.1.1.2.1.cmml">â‹…</mo><msub id="S2.E1.m1.5.5.2.2.1.1.2.3" xref="S2.E1.m1.5.5.2.2.1.1.2.3.cmml"><mi id="S2.E1.m1.5.5.2.2.1.1.2.3.2" xref="S2.E1.m1.5.5.2.2.1.1.2.3.2.cmml">ğ’›</mi><mi id="S2.E1.m1.5.5.2.2.1.1.2.3.3" xref="S2.E1.m1.5.5.2.2.1.1.2.3.3.cmml">p</mi></msub></mrow><mo id="S2.E1.m1.5.5.2.2.1.1.1" xref="S2.E1.m1.5.5.2.2.1.1.1.cmml">/</mo><mi id="S2.E1.m1.5.5.2.2.1.1.3" xref="S2.E1.m1.5.5.2.2.1.1.3.cmml">Ï„</mi></mrow><mo id="S2.E1.m1.5.5.2.2.1.3" xref="S2.E1.m1.5.5.2.3.cmml">)</mo></mrow></mrow><mrow id="S2.E1.m1.8.8.5" xref="S2.E1.m1.8.8.5.cmml"><msub id="S2.E1.m1.8.8.5.4" xref="S2.E1.m1.8.8.5.4.cmml"><mo id="S2.E1.m1.8.8.5.4.2" xref="S2.E1.m1.8.8.5.4.2.cmml">âˆ‘</mo><mrow id="S2.E1.m1.6.6.3.1.1" xref="S2.E1.m1.6.6.3.1.1.cmml"><mi id="S2.E1.m1.6.6.3.1.1.3" xref="S2.E1.m1.6.6.3.1.1.3.cmml">a</mi><mo id="S2.E1.m1.6.6.3.1.1.2" xref="S2.E1.m1.6.6.3.1.1.2.cmml">âˆˆ</mo><mrow id="S2.E1.m1.6.6.3.1.1.4" xref="S2.E1.m1.6.6.3.1.1.4.cmml"><mi id="S2.E1.m1.6.6.3.1.1.4.2" xref="S2.E1.m1.6.6.3.1.1.4.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.3.1.1.4.1" xref="S2.E1.m1.6.6.3.1.1.4.1.cmml">â€‹</mo><mrow id="S2.E1.m1.6.6.3.1.1.4.3.2" xref="S2.E1.m1.6.6.3.1.1.4.cmml"><mo stretchy="false" id="S2.E1.m1.6.6.3.1.1.4.3.2.1" xref="S2.E1.m1.6.6.3.1.1.4.cmml">(</mo><mi id="S2.E1.m1.6.6.3.1.1.1" xref="S2.E1.m1.6.6.3.1.1.1.cmml">i</mi><mo stretchy="false" id="S2.E1.m1.6.6.3.1.1.4.3.2.2" xref="S2.E1.m1.6.6.3.1.1.4.cmml">)</mo></mrow></mrow></mrow></msub><mrow id="S2.E1.m1.8.8.5.3.1" xref="S2.E1.m1.8.8.5.3.2.cmml"><mi id="S2.E1.m1.7.7.4.2" xref="S2.E1.m1.7.7.4.2.cmml">exp</mi><mo id="S2.E1.m1.8.8.5.3.1a" xref="S2.E1.m1.8.8.5.3.2.cmml">â¡</mo><mrow id="S2.E1.m1.8.8.5.3.1.1" xref="S2.E1.m1.8.8.5.3.2.cmml"><mo id="S2.E1.m1.8.8.5.3.1.1.2" xref="S2.E1.m1.8.8.5.3.2.cmml">(</mo><mrow id="S2.E1.m1.8.8.5.3.1.1.1" xref="S2.E1.m1.8.8.5.3.1.1.1.cmml"><mrow id="S2.E1.m1.8.8.5.3.1.1.1.2" xref="S2.E1.m1.8.8.5.3.1.1.1.2.cmml"><msub id="S2.E1.m1.8.8.5.3.1.1.1.2.2" xref="S2.E1.m1.8.8.5.3.1.1.1.2.2.cmml"><mi id="S2.E1.m1.8.8.5.3.1.1.1.2.2.2" xref="S2.E1.m1.8.8.5.3.1.1.1.2.2.2.cmml">ğ’›</mi><mi id="S2.E1.m1.8.8.5.3.1.1.1.2.2.3" xref="S2.E1.m1.8.8.5.3.1.1.1.2.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.E1.m1.8.8.5.3.1.1.1.2.1" xref="S2.E1.m1.8.8.5.3.1.1.1.2.1.cmml">â‹…</mo><msub id="S2.E1.m1.8.8.5.3.1.1.1.2.3" xref="S2.E1.m1.8.8.5.3.1.1.1.2.3.cmml"><mi id="S2.E1.m1.8.8.5.3.1.1.1.2.3.2" xref="S2.E1.m1.8.8.5.3.1.1.1.2.3.2.cmml">ğ’›</mi><mi id="S2.E1.m1.8.8.5.3.1.1.1.2.3.3" xref="S2.E1.m1.8.8.5.3.1.1.1.2.3.3.cmml">a</mi></msub></mrow><mo id="S2.E1.m1.8.8.5.3.1.1.1.1" xref="S2.E1.m1.8.8.5.3.1.1.1.1.cmml">/</mo><mi id="S2.E1.m1.8.8.5.3.1.1.1.3" xref="S2.E1.m1.8.8.5.3.1.1.1.3.cmml">Ï„</mi></mrow><mo id="S2.E1.m1.8.8.5.3.1.1.3" xref="S2.E1.m1.8.8.5.3.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.8b"><apply id="S2.E1.m1.8.9.cmml" xref="S2.E1.m1.8.9"><apply id="S2.E1.m1.8.9.1.cmml" xref="S2.E1.m1.8.9.1"><csymbol cd="ambiguous" id="S2.E1.m1.8.9.1.1.cmml" xref="S2.E1.m1.8.9.1">subscript</csymbol><sum id="S2.E1.m1.8.9.1.2.cmml" xref="S2.E1.m1.8.9.1.2"></sum><apply id="S2.E1.m1.8.9.1.3.cmml" xref="S2.E1.m1.8.9.1.3"><in id="S2.E1.m1.8.9.1.3.1.cmml" xref="S2.E1.m1.8.9.1.3.1"></in><ci id="S2.E1.m1.8.9.1.3.2.cmml" xref="S2.E1.m1.8.9.1.3.2">ğ‘–</ci><ci id="S2.E1.m1.8.9.1.3.3.cmml" xref="S2.E1.m1.8.9.1.3.3">ğ¼</ci></apply></apply><apply id="S2.E1.m1.8.9.2.cmml" xref="S2.E1.m1.8.9.2"><times id="S2.E1.m1.8.9.2.1.cmml" xref="S2.E1.m1.8.9.2.1"></times><apply id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2"><divide id="S2.E1.m1.2.2.3.cmml" xref="S2.E1.m1.2.2"></divide><apply id="S2.E1.m1.2.2.4.cmml" xref="S2.E1.m1.2.2.4"><minus id="S2.E1.m1.2.2.4.1.cmml" xref="S2.E1.m1.2.2.4"></minus><cn type="integer" id="S2.E1.m1.2.2.4.2.cmml" xref="S2.E1.m1.2.2.4.2">1</cn></apply><apply id="S2.E1.m1.2.2.2.3.cmml" xref="S2.E1.m1.2.2.2.2"><abs id="S2.E1.m1.2.2.2.3.1.cmml" xref="S2.E1.m1.2.2.2.2.2"></abs><apply id="S2.E1.m1.2.2.2.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1"><times id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1"></times><ci id="S2.E1.m1.2.2.2.2.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.2">ğ‘ƒ</ci><ci id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1">ğ‘–</ci></apply></apply></apply><apply id="S2.E1.m1.8.9.2.2.cmml" xref="S2.E1.m1.8.9.2.2"><apply id="S2.E1.m1.8.9.2.2.1.cmml" xref="S2.E1.m1.8.9.2.2.1"><csymbol cd="ambiguous" id="S2.E1.m1.8.9.2.2.1.1.cmml" xref="S2.E1.m1.8.9.2.2.1">subscript</csymbol><sum id="S2.E1.m1.8.9.2.2.1.2.cmml" xref="S2.E1.m1.8.9.2.2.1.2"></sum><apply id="S2.E1.m1.3.3.1.cmml" xref="S2.E1.m1.3.3.1"><in id="S2.E1.m1.3.3.1.2.cmml" xref="S2.E1.m1.3.3.1.2"></in><ci id="S2.E1.m1.3.3.1.3.cmml" xref="S2.E1.m1.3.3.1.3">ğ‘</ci><apply id="S2.E1.m1.3.3.1.4.cmml" xref="S2.E1.m1.3.3.1.4"><times id="S2.E1.m1.3.3.1.4.1.cmml" xref="S2.E1.m1.3.3.1.4.1"></times><ci id="S2.E1.m1.3.3.1.4.2.cmml" xref="S2.E1.m1.3.3.1.4.2">ğ‘ƒ</ci><ci id="S2.E1.m1.3.3.1.1.cmml" xref="S2.E1.m1.3.3.1.1">ğ‘–</ci></apply></apply></apply><apply id="S2.E1.m1.8.9.2.2.2.cmml" xref="S2.E1.m1.8.9.2.2.2"><log id="S2.E1.m1.8.9.2.2.2.1.cmml" xref="S2.E1.m1.8.9.2.2.2.1"></log><apply id="S2.E1.m1.8.8.cmml" xref="S2.E1.m1.8.8"><divide id="S2.E1.m1.8.8.6.cmml" xref="S2.E1.m1.8.8"></divide><apply id="S2.E1.m1.5.5.2.3.cmml" xref="S2.E1.m1.5.5.2.2"><exp id="S2.E1.m1.4.4.1.1.cmml" xref="S2.E1.m1.4.4.1.1"></exp><apply id="S2.E1.m1.5.5.2.2.1.1.cmml" xref="S2.E1.m1.5.5.2.2.1.1"><divide id="S2.E1.m1.5.5.2.2.1.1.1.cmml" xref="S2.E1.m1.5.5.2.2.1.1.1"></divide><apply id="S2.E1.m1.5.5.2.2.1.1.2.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2"><ci id="S2.E1.m1.5.5.2.2.1.1.2.1.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.1">â‹…</ci><apply id="S2.E1.m1.5.5.2.2.1.1.2.2.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.2.2.1.1.2.2.1.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.5.5.2.2.1.1.2.2.2.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.2.2">ğ’›</ci><ci id="S2.E1.m1.5.5.2.2.1.1.2.2.3.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.2.3">ğ‘–</ci></apply><apply id="S2.E1.m1.5.5.2.2.1.1.2.3.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.2.2.1.1.2.3.1.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.5.5.2.2.1.1.2.3.2.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.3.2">ğ’›</ci><ci id="S2.E1.m1.5.5.2.2.1.1.2.3.3.cmml" xref="S2.E1.m1.5.5.2.2.1.1.2.3.3">ğ‘</ci></apply></apply><ci id="S2.E1.m1.5.5.2.2.1.1.3.cmml" xref="S2.E1.m1.5.5.2.2.1.1.3">ğœ</ci></apply></apply><apply id="S2.E1.m1.8.8.5.cmml" xref="S2.E1.m1.8.8.5"><apply id="S2.E1.m1.8.8.5.4.cmml" xref="S2.E1.m1.8.8.5.4"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.5.4.1.cmml" xref="S2.E1.m1.8.8.5.4">subscript</csymbol><sum id="S2.E1.m1.8.8.5.4.2.cmml" xref="S2.E1.m1.8.8.5.4.2"></sum><apply id="S2.E1.m1.6.6.3.1.1.cmml" xref="S2.E1.m1.6.6.3.1.1"><in id="S2.E1.m1.6.6.3.1.1.2.cmml" xref="S2.E1.m1.6.6.3.1.1.2"></in><ci id="S2.E1.m1.6.6.3.1.1.3.cmml" xref="S2.E1.m1.6.6.3.1.1.3">ğ‘</ci><apply id="S2.E1.m1.6.6.3.1.1.4.cmml" xref="S2.E1.m1.6.6.3.1.1.4"><times id="S2.E1.m1.6.6.3.1.1.4.1.cmml" xref="S2.E1.m1.6.6.3.1.1.4.1"></times><ci id="S2.E1.m1.6.6.3.1.1.4.2.cmml" xref="S2.E1.m1.6.6.3.1.1.4.2">ğ´</ci><ci id="S2.E1.m1.6.6.3.1.1.1.cmml" xref="S2.E1.m1.6.6.3.1.1.1">ğ‘–</ci></apply></apply></apply><apply id="S2.E1.m1.8.8.5.3.2.cmml" xref="S2.E1.m1.8.8.5.3.1"><exp id="S2.E1.m1.7.7.4.2.cmml" xref="S2.E1.m1.7.7.4.2"></exp><apply id="S2.E1.m1.8.8.5.3.1.1.1.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1"><divide id="S2.E1.m1.8.8.5.3.1.1.1.1.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.1"></divide><apply id="S2.E1.m1.8.8.5.3.1.1.1.2.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2"><ci id="S2.E1.m1.8.8.5.3.1.1.1.2.1.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.1">â‹…</ci><apply id="S2.E1.m1.8.8.5.3.1.1.1.2.2.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.5.3.1.1.1.2.2.1.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.2">subscript</csymbol><ci id="S2.E1.m1.8.8.5.3.1.1.1.2.2.2.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.2.2">ğ’›</ci><ci id="S2.E1.m1.8.8.5.3.1.1.1.2.2.3.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.2.3">ğ‘–</ci></apply><apply id="S2.E1.m1.8.8.5.3.1.1.1.2.3.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.8.8.5.3.1.1.1.2.3.1.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.3">subscript</csymbol><ci id="S2.E1.m1.8.8.5.3.1.1.1.2.3.2.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.3.2">ğ’›</ci><ci id="S2.E1.m1.8.8.5.3.1.1.1.2.3.3.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.2.3.3">ğ‘</ci></apply></apply><ci id="S2.E1.m1.8.8.5.3.1.1.1.3.cmml" xref="S2.E1.m1.8.8.5.3.1.1.1.3">ğœ</ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.8c">\sum_{i\in I}\frac{-1}{|P(i)|}\sum_{p\in P(i)}\log\frac{\exp\left(\boldsymbol{z}_{i}\cdot\boldsymbol{z}_{p}/\tau\right)}{\sum_{a\in A(i)}\exp\left(\boldsymbol{z}_{i}\cdot\boldsymbol{z}_{a}/\tau\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.7" class="ltx_p">where <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mi id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">\tau</annotation></semantics></math> is a temperature parameter (equal to 0.1), <math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">i</annotation></semantics></math> indexes a sample in a â€œmultiviewed" batch (in this case multiple fonts/augmentations of characters with the same identity), <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="P(i)" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><mrow id="S2.SS1.p4.3.m3.1.2" xref="S2.SS1.p4.3.m3.1.2.cmml"><mi id="S2.SS1.p4.3.m3.1.2.2" xref="S2.SS1.p4.3.m3.1.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.3.m3.1.2.1" xref="S2.SS1.p4.3.m3.1.2.1.cmml">â€‹</mo><mrow id="S2.SS1.p4.3.m3.1.2.3.2" xref="S2.SS1.p4.3.m3.1.2.cmml"><mo stretchy="false" id="S2.SS1.p4.3.m3.1.2.3.2.1" xref="S2.SS1.p4.3.m3.1.2.cmml">(</mo><mi id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">i</mi><mo stretchy="false" id="S2.SS1.p4.3.m3.1.2.3.2.2" xref="S2.SS1.p4.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><apply id="S2.SS1.p4.3.m3.1.2.cmml" xref="S2.SS1.p4.3.m3.1.2"><times id="S2.SS1.p4.3.m3.1.2.1.cmml" xref="S2.SS1.p4.3.m3.1.2.1"></times><ci id="S2.SS1.p4.3.m3.1.2.2.cmml" xref="S2.SS1.p4.3.m3.1.2.2">ğ‘ƒ</ci><ci id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">P(i)</annotation></semantics></math> is the set of indices of all positives in the multiviewed batch that are distinct from <math id="S2.SS1.p4.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p4.4.m4.1a"><mi id="S2.SS1.p4.4.m4.1.1" xref="S2.SS1.p4.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.4.m4.1b"><ci id="S2.SS1.p4.4.m4.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.4.m4.1c">i</annotation></semantics></math>, <math id="S2.SS1.p4.5.m5.1" class="ltx_Math" alttext="A(i)" display="inline"><semantics id="S2.SS1.p4.5.m5.1a"><mrow id="S2.SS1.p4.5.m5.1.2" xref="S2.SS1.p4.5.m5.1.2.cmml"><mi id="S2.SS1.p4.5.m5.1.2.2" xref="S2.SS1.p4.5.m5.1.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p4.5.m5.1.2.1" xref="S2.SS1.p4.5.m5.1.2.1.cmml">â€‹</mo><mrow id="S2.SS1.p4.5.m5.1.2.3.2" xref="S2.SS1.p4.5.m5.1.2.cmml"><mo stretchy="false" id="S2.SS1.p4.5.m5.1.2.3.2.1" xref="S2.SS1.p4.5.m5.1.2.cmml">(</mo><mi id="S2.SS1.p4.5.m5.1.1" xref="S2.SS1.p4.5.m5.1.1.cmml">i</mi><mo stretchy="false" id="S2.SS1.p4.5.m5.1.2.3.2.2" xref="S2.SS1.p4.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.5.m5.1b"><apply id="S2.SS1.p4.5.m5.1.2.cmml" xref="S2.SS1.p4.5.m5.1.2"><times id="S2.SS1.p4.5.m5.1.2.1.cmml" xref="S2.SS1.p4.5.m5.1.2.1"></times><ci id="S2.SS1.p4.5.m5.1.2.2.cmml" xref="S2.SS1.p4.5.m5.1.2.2">ğ´</ci><ci id="S2.SS1.p4.5.m5.1.1.cmml" xref="S2.SS1.p4.5.m5.1.1">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.5.m5.1c">A(i)</annotation></semantics></math> is the set of all indices excluding <math id="S2.SS1.p4.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS1.p4.6.m6.1a"><mi id="S2.SS1.p4.6.m6.1.1" xref="S2.SS1.p4.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.6.m6.1b"><ci id="S2.SS1.p4.6.m6.1.1.cmml" xref="S2.SS1.p4.6.m6.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.6.m6.1c">i</annotation></semantics></math>, and <math id="S2.SS1.p4.7.m7.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S2.SS1.p4.7.m7.1a"><mi id="S2.SS1.p4.7.m7.1.1" xref="S2.SS1.p4.7.m7.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.7.m7.1b"><ci id="S2.SS1.p4.7.m7.1.1.cmml" xref="S2.SS1.p4.7.m7.1.1">ğ‘§</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.7.m7.1c">z</annotation></semantics></math> is an embedding of a sample in the batch. Training details are describe in the supplementary materials.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para">
<p id="S2.SS1.p5.1" class="ltx_p">To compute charactersâ€™ similarity, we embed their image crops, created with a reference font (Google Noto), and compute cosine similarity with a Facebook Artificial Intelligence Similarly Search backend <cite class="ltx_cite ltx_citemacro_cite">Johnson etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S2.SS1.p6" class="ltx_para">
<p id="S2.SS1.p6.1" class="ltx_p">Figure <a href="#S2.F2" title="Figure 2 â€£ 2.1 The HOMOGLYPH model â€£ 2 Methods â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows representative examples of characters and their five nearest neighbors. Characters with similar vector representations have qualitatively similar appearances.</p>
</div>
<div id="S2.SS1.p7" class="ltx_para">
<p id="S2.SS1.p7.1" class="ltx_p"><span id="S2.SS1.p7.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> shares common elements with EfficientOCR <cite class="ltx_cite ltx_citemacro_cite">Carlson etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>, an OCR architecture that learns to recognize characters by contrastively training on character crops rendered with augmented digital fonts. Different augmentations of a character provide positive examples. At inference time, localized characters are OCRâ€™ed by retrieving their nearest neighbor from an index of exemplary character embeddings. The OCR application of contrastive learning on character renders aims to retrieve the same character in an offline index, whereas <span id="S2.SS1.p7.1.2" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> measures similarity across characters. While <span id="S2.SS1.p7.1.3" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> shares the architecture of the EfficientOCR character recognizer, it does not use the same model weights or training data as EffOCR (which does not support Chinese or Korean and is also trained on labeled crops from historical documents).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>String Matching Methods</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Dunn (<a href="#bib.bib13" title="" class="ltx_ref">1946</a>)</cite> - in one of the first treatments of record linkage - wrote: â€œEach person in the world creates a Book of Life. This Book starts with birth and ends with death. Its pages are made up of the records of the principal events in life. Record linkage is the name given to the process of assembling the pages of this Book into a volume.â€</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Edit distance metrics are widely used for this task <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_citet">Levenshtein etÂ al. (<a href="#bib.bib24" title="" class="ltx_ref">1966</a>); Jaro (<a href="#bib.bib18" title="" class="ltx_ref">1989</a>); Winkler (<a href="#bib.bib51" title="" class="ltx_ref">1990</a>)</cite>.
Another common approach computes the cosine similarity between <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">n</annotation></semantics></math>-gram representations of strings <cite class="ltx_cite ltx_citemacro_cite">Okazaki and Tsujii (<a href="#bib.bib33" title="" class="ltx_ref">2010</a>)</cite>.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">There are a variety of ways that character-level visual similarity could be incorporated into record linkage. We follow the literature modifying Levenshtein distance, e.g. <cite class="ltx_cite ltx_citemacro_citet">Novosad (<a href="#bib.bib32" title="" class="ltx_ref">2018</a>)</cite>, by using cosine distance in the <span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> space as the substitution cost. Insertion and deletion costs are set to one. It is straightforward to scale the insertion and deletion costs using parameters estimated on a validation set, but we focus on performance without any tuned parameters to maintain a purely off-the-shelf, self-supervised implementation.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">We compare matching with homoglyphic edit distance to a variety of other methods. The first comparison is to classic Levenshtein distance (insertions, deletions, and substitutions are all equally costly), to isolate the effect of varying the substitution cost. We also compare to the popular Simstring package, which uses a variety of similarity metrics (Jaccard, cosine, and Dice similarity), computed with 2-gram substrings <cite class="ltx_cite ltx_citemacro_cite">Okazaki and Tsujii (<a href="#bib.bib33" title="" class="ltx_ref">2010</a>)</cite>. The third comparison is to FuzzyChinese, a widely used package that uses strokes or characters as the fundamental unit for n-gram substring representations (we use the default 3-grams). These are compared using the TF-IDF vectors. The strokes in each character are drawn from an external database <cite class="ltx_cite ltx_citemacro_cite">kfcd (<a href="#bib.bib21" title="" class="ltx_ref">2015</a>)</cite> covering a subset of the CJK script.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation Datasets</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">To our knowledge, there are not widely used benchmarks for evaluating record linkage for the CJK script.
Hence, we develop evaluation data.
First, we link a dataset on the customers and suppliers of major Japanese firms, drawn from a 1956 Japanese firm publication <cite class="ltx_cite ltx_citemacro_cite">Jinji Koshinjo (<a href="#bib.bib19" title="" class="ltx_ref">1954</a>)</cite>, to a firm index of around 7,000 firms. The index is from the same publication but written in a different font. Supply chains are fundamental to the transmission of economic shocks <cite class="ltx_cite ltx_citemacro_cite">Acemoglu etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2016</a>, <a href="#bib.bib4" title="" class="ltx_ref">2012</a>)</cite>, agglomeration <cite class="ltx_cite ltx_citemacro_cite">Ellison etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2010</a>)</cite>, and economic development <cite class="ltx_cite ltx_citemacro_cite">Hirschman (<a href="#bib.bib17" title="" class="ltx_ref">1958</a>); Myrdal and Sitohang (<a href="#bib.bib30" title="" class="ltx_ref">1957</a>); Rasmussen (<a href="#bib.bib36" title="" class="ltx_ref">1956</a>); Bartelme and Gorodnichenko (<a href="#bib.bib6" title="" class="ltx_ref">2015</a>); Lane (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>. Supply chains are challenging to study historically, as they require accurate record linkage. This makes them a particularly relevant test case for downstream applications.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Firm names are localized with LayoutParser <cite class="ltx_cite ltx_citemacro_cite">Shen etÂ al. (<a href="#bib.bib39" title="" class="ltx_ref">2021</a>)</cite> and then OCRâ€™ed twice, to shed light on whether errors tend to be homoglyphic in popular vision-only OCR and vision-language sequence-to-sequence OCR. We employ two widely used, open-source OCR engines: PaddleOCR and EasyOCR.
EasyOCR uses a convolutional recurrent neural network (CRNN) <cite class="ltx_cite ltx_citemacro_cite">Shi etÂ al. (<a href="#bib.bib40" title="" class="ltx_ref">2016</a>)</cite>, with learned embeddings from a vision model serving as inputs to a learned language model. PaddleOCR abandons language modeling, dividing text images into small patches, using mixing blocks to perceive inter- and intra-character patterns, and recognizing text by linear prediction <cite class="ltx_cite ltx_citemacro_cite">Du etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>.
Neither engine localizes individual characters.</p>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.1" class="ltx_p">In a second exercise, we use the dataset examined in <cite class="ltx_cite ltx_citemacro_citet">Arora etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, which links the same customer-supplier list to a firm directory containing over 70,000 firms <cite class="ltx_cite ltx_citemacro_cite">Teikoku Koshinjo (<a href="#bib.bib44" title="" class="ltx_ref">1957</a>)</cite>.
Examining this dataset allows a comparison of string matching methods to the OCR-free vision only methods and multimodal methods from <cite class="ltx_cite ltx_citemacro_citet">Arora etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>.
This dataset was created with EfficientOCR <cite class="ltx_cite ltx_citemacro_cite">Carlson etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite> and cannot be re-created with EasyOCR or PaddleOCR because the directory is written vertically, which these engines do not support. We would expect EfficientOCRâ€™s character retrieval framework to make homoglyphic errors. Performance across datasets created by three highly diverse OCR architectures is important to extensibility, since database collections have also been constructed with diverse OCR architectures.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p">Because creating ground truth data for record linkage is costly, we use synthetically generated data for a third set of evaluations. We render place and firm names using different digital fonts and image augmentations, conducting separate experiments for Traditional Chinese, Simplified Chinese, Japanese, and Korean. For Simplified Chinese, Japanese, and Korean, we draw placenames from the Geonames database <cite class="ltx_cite ltx_citemacro_cite">Geonames (<a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>. Because Traditional Chinese placenames in Geonames are rare, we instead draw from a list of Taiwanese firms, as Taiwan - unlike Mainland China - still uses Traditional Chinese <cite class="ltx_cite ltx_citemacro_cite">Taiwan Ministry of Economic Affairs (<a href="#bib.bib43" title="" class="ltx_ref">2023</a>)</cite>. We randomly select two image crops of each entity name, and OCR them using EasyOCR and PaddleOCR. Anywhere from 40% (Simplified Chinese) to 88% (Traditional Chinese) of OCRâ€™ed string pairs differ between the two OCR engines. We limit the evaluation dataset to pairs where the two string representations differ.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The sample size is 20,162 for Simplified Chinese, 66,943 for Traditional Chinese, 86,470 for Japanese, and 48,809 for Korean.</span></span></span></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:452.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(70.1pt,-73.2pt) scale(1.4781079948028,1.4781079948028) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4">OCR Engines</th>
</tr>
<tr id="S4.T1.1.1.2.2" class="ltx_tr">
<th id="S4.T1.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row">Method</th>
<th id="S4.T1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Paddle</th>
<th id="S4.T1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Easy</th>
<th id="S4.T1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Paddle</th>
<th id="S4.T1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Easy</th>
</tr>
<tr id="S4.T1.1.1.3.3" class="ltx_tr">
<th id="S4.T1.1.1.3.3.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">to Easy</th>
<th id="S4.T1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">to Paddle</th>
<th id="S4.T1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">to Paddle</th>
<th id="S4.T1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">to Easy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.4.1" class="ltx_tr">
<th id="S4.T1.1.1.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Homoglyphic</th>
<td id="S4.T1.1.1.4.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.1.2.1" class="ltx_text ltx_font_bold">0.808</span></td>
<td id="S4.T1.1.1.4.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.1.3.1" class="ltx_text ltx_font_bold">0.753</span></td>
<td id="S4.T1.1.1.4.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.1.4.1" class="ltx_text ltx_font_bold">0.844</span></td>
<td id="S4.T1.1.1.4.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T1.1.1.4.1.5.1" class="ltx_text ltx_font_bold">0.728</span></td>
</tr>
<tr id="S4.T1.1.1.5.2" class="ltx_tr">
<th id="S4.T1.1.1.5.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â distance</th>
<td id="S4.T1.1.1.5.2.2" class="ltx_td"></td>
<td id="S4.T1.1.1.5.2.3" class="ltx_td"></td>
<td id="S4.T1.1.1.5.2.4" class="ltx_td"></td>
<td id="S4.T1.1.1.5.2.5" class="ltx_td"></td>
</tr>
<tr id="S4.T1.1.1.6.3" class="ltx_tr">
<th id="S4.T1.1.1.6.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Levenshtein</th>
<td id="S4.T1.1.1.6.3.2" class="ltx_td ltx_align_center">0.766</td>
<td id="S4.T1.1.1.6.3.3" class="ltx_td ltx_align_center">0.697</td>
<td id="S4.T1.1.1.6.3.4" class="ltx_td ltx_align_center">0.807</td>
<td id="S4.T1.1.1.6.3.5" class="ltx_td ltx_align_center">0.693</td>
</tr>
<tr id="S4.T1.1.1.7.4" class="ltx_tr">
<th id="S4.T1.1.1.7.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â distance</th>
<td id="S4.T1.1.1.7.4.2" class="ltx_td"></td>
<td id="S4.T1.1.1.7.4.3" class="ltx_td"></td>
<td id="S4.T1.1.1.7.4.4" class="ltx_td"></td>
<td id="S4.T1.1.1.7.4.5" class="ltx_td"></td>
</tr>
<tr id="S4.T1.1.1.8.5" class="ltx_tr">
<th id="S4.T1.1.1.8.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring</th>
<td id="S4.T1.1.1.8.5.2" class="ltx_td ltx_align_center">0.762</td>
<td id="S4.T1.1.1.8.5.3" class="ltx_td ltx_align_center">0.662</td>
<td id="S4.T1.1.1.8.5.4" class="ltx_td ltx_align_center">0.787</td>
<td id="S4.T1.1.1.8.5.5" class="ltx_td ltx_align_center">0.673</td>
</tr>
<tr id="S4.T1.1.1.9.6" class="ltx_tr">
<th id="S4.T1.1.1.9.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â  (cosine)</th>
<td id="S4.T1.1.1.9.6.2" class="ltx_td"></td>
<td id="S4.T1.1.1.9.6.3" class="ltx_td"></td>
<td id="S4.T1.1.1.9.6.4" class="ltx_td"></td>
<td id="S4.T1.1.1.9.6.5" class="ltx_td"></td>
</tr>
<tr id="S4.T1.1.1.10.7" class="ltx_tr">
<th id="S4.T1.1.1.10.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring</th>
<td id="S4.T1.1.1.10.7.2" class="ltx_td ltx_align_center">0.763</td>
<td id="S4.T1.1.1.10.7.3" class="ltx_td ltx_align_center">0.663</td>
<td id="S4.T1.1.1.10.7.4" class="ltx_td ltx_align_center">0.788</td>
<td id="S4.T1.1.1.10.7.5" class="ltx_td ltx_align_center">0.673</td>
</tr>
<tr id="S4.T1.1.1.11.8" class="ltx_tr">
<th id="S4.T1.1.1.11.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â (dice)</th>
<td id="S4.T1.1.1.11.8.2" class="ltx_td"></td>
<td id="S4.T1.1.1.11.8.3" class="ltx_td"></td>
<td id="S4.T1.1.1.11.8.4" class="ltx_td"></td>
<td id="S4.T1.1.1.11.8.5" class="ltx_td"></td>
</tr>
<tr id="S4.T1.1.1.12.9" class="ltx_tr">
<th id="S4.T1.1.1.12.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring</th>
<td id="S4.T1.1.1.12.9.2" class="ltx_td ltx_align_center">0.763</td>
<td id="S4.T1.1.1.12.9.3" class="ltx_td ltx_align_center">0.663</td>
<td id="S4.T1.1.1.12.9.4" class="ltx_td ltx_align_center">0.788</td>
<td id="S4.T1.1.1.12.9.5" class="ltx_td ltx_align_center">0.673</td>
</tr>
<tr id="S4.T1.1.1.13.10" class="ltx_tr">
<th id="S4.T1.1.1.13.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â (jaccard)</th>
<td id="S4.T1.1.1.13.10.2" class="ltx_td"></td>
<td id="S4.T1.1.1.13.10.3" class="ltx_td"></td>
<td id="S4.T1.1.1.13.10.4" class="ltx_td"></td>
<td id="S4.T1.1.1.13.10.5" class="ltx_td"></td>
</tr>
<tr id="S4.T1.1.1.14.11" class="ltx_tr">
<th id="S4.T1.1.1.14.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FuzzyChinese</th>
<td id="S4.T1.1.1.14.11.2" class="ltx_td ltx_align_center">0.690</td>
<td id="S4.T1.1.1.14.11.3" class="ltx_td ltx_align_center">0.567</td>
<td id="S4.T1.1.1.14.11.4" class="ltx_td ltx_align_center">0.717</td>
<td id="S4.T1.1.1.14.11.5" class="ltx_td ltx_align_center">0.554</td>
</tr>
<tr id="S4.T1.1.1.15.12" class="ltx_tr">
<th id="S4.T1.1.1.15.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â (stroke)</th>
<td id="S4.T1.1.1.15.12.2" class="ltx_td"></td>
<td id="S4.T1.1.1.15.12.3" class="ltx_td"></td>
<td id="S4.T1.1.1.15.12.4" class="ltx_td"></td>
<td id="S4.T1.1.1.15.12.5" class="ltx_td"></td>
</tr>
<tr id="S4.T1.1.1.16.13" class="ltx_tr">
<th id="S4.T1.1.1.16.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FuzzyChinese</th>
<td id="S4.T1.1.1.16.13.2" class="ltx_td ltx_align_center">0.533</td>
<td id="S4.T1.1.1.16.13.3" class="ltx_td ltx_align_center">0.445</td>
<td id="S4.T1.1.1.16.13.4" class="ltx_td ltx_align_center">0.559</td>
<td id="S4.T1.1.1.16.13.5" class="ltx_td ltx_align_center">0.464</td>
</tr>
<tr id="S4.T1.1.1.17.14" class="ltx_tr">
<th id="S4.T1.1.1.17.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Â Â Â Â (character)</th>
<td id="S4.T1.1.1.17.14.2" class="ltx_td ltx_border_bb"></td>
<td id="S4.T1.1.1.17.14.3" class="ltx_td ltx_border_bb"></td>
<td id="S4.T1.1.1.17.14.4" class="ltx_td ltx_border_bb"></td>
<td id="S4.T1.1.1.17.14.5" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S4.T1.3.1" class="ltx_text ltx_font_bold">Baseline Matching Results: Historical Japanese Data</span>. This table reports accuracy using a variety of different methods for linking Japanese firms from supply chain records to a horizontally written firm directory. The four columns report results when (1) PaddleOCR is used to OCR the firm list and EasyOCR the directory, (2) EasyOCR is used to OCR the firm list and PaddleOCR the directory, (3) PaddleOCR is used to OCR both lists, (4) EasyOCR is used to OCR both lists.</figcaption>
</figure>
<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Homoglyphic edit distance outperforms the other string matching methods in all three evaluation exercises - across different OCR engines and languages - typically by an appreciable margin. This illustrates that homoglyphic errors in OCR are common and can be captured with self-supervised vision transformers.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Our first evaluation exercise - with linked Japanese supply chain data - aims to elucidate whether homoglyphic matching is as helpful for linking datasets created with vision-language OCR as for linking datasets created with vision-only OCR, and whether it can similarly be useful for linking datasets created with different OCR architectures.
We hence separately consider results linking PaddleOCRâ€™ed customers and suppliers to the EasyOCRâ€™ed firm index, vice versa, as well as linking when both are OCRâ€™ed by either PaddleOCR or EasyOCR.
Homoglyphic edit distance outperforms other string matching methods and does so by a similar margin (around 4 percentage points higher accuracy) regardless of the OCR architecture used. FuzzyChinese has the weakest performance, as expected, since many Japanese characters are not covered in their stroke dictionary.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:431.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(99.1pt,-98.6pt) scale(1.84235692791832,1.84235692791832) ;">
<table id="S4.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt">Method</th>
<th id="S4.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Accuracy</th>
</tr>
<tr id="S4.T2.1.1.2.2" class="ltx_tr">
<th id="S4.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><span id="S4.T2.1.1.2.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Panel A: String-Matching</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.3.1" class="ltx_tr">
<th id="S4.T2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Homoglyphic distance</th>
<td id="S4.T2.1.1.3.1.2" class="ltx_td ltx_align_center">0.824</td>
</tr>
<tr id="S4.T2.1.1.4.2" class="ltx_tr">
<th id="S4.T2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Levenshtein distance</th>
<td id="S4.T2.1.1.4.2.2" class="ltx_td ltx_align_center">0.731</td>
</tr>
<tr id="S4.T2.1.1.5.3" class="ltx_tr">
<th id="S4.T2.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring (cosine)</th>
<td id="S4.T2.1.1.5.3.2" class="ltx_td ltx_align_center">0.748</td>
</tr>
<tr id="S4.T2.1.1.6.4" class="ltx_tr">
<th id="S4.T2.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring (dice)</th>
<td id="S4.T2.1.1.6.4.2" class="ltx_td ltx_align_center">0.752</td>
</tr>
<tr id="S4.T2.1.1.7.5" class="ltx_tr">
<th id="S4.T2.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring (jaccard)</th>
<td id="S4.T2.1.1.7.5.2" class="ltx_td ltx_align_center">0.752</td>
</tr>
<tr id="S4.T2.1.1.8.6" class="ltx_tr">
<th id="S4.T2.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FuzzyChinese (stroke)</th>
<td id="S4.T2.1.1.8.6.2" class="ltx_td ltx_align_center">0.735</td>
</tr>
<tr id="S4.T2.1.1.9.7" class="ltx_tr">
<th id="S4.T2.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FuzzyChinese (character)</th>
<td id="S4.T2.1.1.9.7.2" class="ltx_td ltx_align_center">0.618</td>
</tr>
<tr id="S4.T2.1.1.10.8" class="ltx_tr">
<th id="S4.T2.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2"><span id="S4.T2.1.1.10.8.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Panel B: Neural Methods</span></th>
</tr>
<tr id="S4.T2.1.1.11.9" class="ltx_tr">
<th id="S4.T2.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Self-Supervised Multimodal Linking</th>
<td id="S4.T2.1.1.11.9.2" class="ltx_td ltx_align_center">0.849</td>
</tr>
<tr id="S4.T2.1.1.12.10" class="ltx_tr">
<th id="S4.T2.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Supervised Vision-Only Linking</th>
<td id="S4.T2.1.1.12.10.2" class="ltx_td ltx_align_center">0.878</td>
</tr>
<tr id="S4.T2.1.1.13.11" class="ltx_tr">
<th id="S4.T2.1.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Supervised Multimodal Linking</th>
<td id="S4.T2.1.1.13.11.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T2.1.1.13.11.2.1" class="ltx_text ltx_font_bold">0.945</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S4.T2.3.1" class="ltx_text ltx_font_bold">Comparisons to fully neural record linkage methods:</span> This table links Japanese firms from supply chain records to an extensive firm directory. String matching methods are reported in Panel A. End-to-end neural methods are reported in Panel B.</figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">The primary objective of our second evaluation is to compare homoglyphic matching to the OCR-free vision-only and end-to-end multimodal frameworks developed in <cite class="ltx_cite ltx_citemacro_citet">Arora etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite>, using customer-supplier data linked to an extensive index of 70K Japanese firms. Homoglyphic distance outperforms all other string matching methods, with a matching accuracy of 82%. The <cite class="ltx_cite ltx_citemacro_citet">Arora etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> self-supervised multimodal record linkage model - which employs language-image contrastive pre-training on firm image crop-OCR text pairs (following <cite class="ltx_cite ltx_citemacro_citet">Radford etÂ al. (<a href="#bib.bib35" title="" class="ltx_ref">2021</a>)</cite>) - outperforms homoglyphic distance, with 85% matching accuracy. The supervised multimodal model outperforms by a wider margin (95% accuracy). These methods avoid the OCR information bottleneck by using crops from the original document images. Moreover, the language model can understand different ways of writing the same firm name (<span id="S4.p3.1.1" class="ltx_text ltx_font_italic">e.g.</span>, using different terms for corporation).</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">The <cite class="ltx_cite ltx_citemacro_citet">Arora etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> supervised vision-only approach, which contrastively trains different views of the same firmâ€™s image crops to have similar representations, also outperforms homoglyphic matching (88% accuracy). While homoglyphs do not fully eliminate the OCR information bottleneck, they do significantly reduce it relative to widely used string matching methods (<span id="S4.p4.1.1" class="ltx_text ltx_font_italic">e.g.</span> 75% accuracy with the Simstring package), with the advantage of not requiring labeled data or image crops.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:413pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(65.6pt,-62.5pt) scale(1.43397901711472,1.43397901711472) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S4.T3.1.1.1.1.2" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T3.1.1.1.1.3" class="ltx_td ltx_th ltx_th_column ltx_border_tt"></th>
<th id="S4.T3.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Simplified</th>
<th id="S4.T3.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Traditional</th>
</tr>
<tr id="S4.T3.1.1.2.2" class="ltx_tr">
<th id="S4.T3.1.1.2.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<th id="S4.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">Japanese</th>
<th id="S4.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Korean</th>
<th id="S4.T3.1.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">Chinese</th>
<th id="S4.T3.1.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Chinese</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.1.3.1" class="ltx_tr">
<th id="S4.T3.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Homoglyphic</th>
<td id="S4.T3.1.1.3.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.3.1.2.1" class="ltx_text ltx_font_bold">0.456</span></td>
<td id="S4.T3.1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.3.1.3.1" class="ltx_text ltx_font_bold">0.292</span></td>
<td id="S4.T3.1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.3.1.4.1" class="ltx_text ltx_font_bold">0.476</span></td>
<td id="S4.T3.1.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.1.1.3.1.5.1" class="ltx_text ltx_font_bold">0.465</span></td>
</tr>
<tr id="S4.T3.1.1.4.2" class="ltx_tr">
<th id="S4.T3.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â distance</th>
<td id="S4.T3.1.1.4.2.2" class="ltx_td"></td>
<td id="S4.T3.1.1.4.2.3" class="ltx_td"></td>
<td id="S4.T3.1.1.4.2.4" class="ltx_td"></td>
<td id="S4.T3.1.1.4.2.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.1.1.5.3" class="ltx_tr">
<th id="S4.T3.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Levenshtein</th>
<td id="S4.T3.1.1.5.3.2" class="ltx_td ltx_align_center">0.396</td>
<td id="S4.T3.1.1.5.3.3" class="ltx_td ltx_align_center">0.188</td>
<td id="S4.T3.1.1.5.3.4" class="ltx_td ltx_align_center">0.375</td>
<td id="S4.T3.1.1.5.3.5" class="ltx_td ltx_align_center">0.407</td>
</tr>
<tr id="S4.T3.1.1.6.4" class="ltx_tr">
<th id="S4.T3.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â distance</th>
<td id="S4.T3.1.1.6.4.2" class="ltx_td"></td>
<td id="S4.T3.1.1.6.4.3" class="ltx_td"></td>
<td id="S4.T3.1.1.6.4.4" class="ltx_td"></td>
<td id="S4.T3.1.1.6.4.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.1.1.7.5" class="ltx_tr">
<th id="S4.T3.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring</th>
<td id="S4.T3.1.1.7.5.2" class="ltx_td ltx_align_center">0.376</td>
<td id="S4.T3.1.1.7.5.3" class="ltx_td ltx_align_center">0.247</td>
<td id="S4.T3.1.1.7.5.4" class="ltx_td ltx_align_center">0.425</td>
<td id="S4.T3.1.1.7.5.5" class="ltx_td ltx_align_center">0.383</td>
</tr>
<tr id="S4.T3.1.1.8.6" class="ltx_tr">
<th id="S4.T3.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â (cosine)</th>
<td id="S4.T3.1.1.8.6.2" class="ltx_td"></td>
<td id="S4.T3.1.1.8.6.3" class="ltx_td"></td>
<td id="S4.T3.1.1.8.6.4" class="ltx_td"></td>
<td id="S4.T3.1.1.8.6.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.1.1.9.7" class="ltx_tr">
<th id="S4.T3.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring</th>
<td id="S4.T3.1.1.9.7.2" class="ltx_td ltx_align_center">0.380</td>
<td id="S4.T3.1.1.9.7.3" class="ltx_td ltx_align_center">0.248</td>
<td id="S4.T3.1.1.9.7.4" class="ltx_td ltx_align_center">0.426</td>
<td id="S4.T3.1.1.9.7.5" class="ltx_td ltx_align_center">0.385</td>
</tr>
<tr id="S4.T3.1.1.10.8" class="ltx_tr">
<th id="S4.T3.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â (dice)</th>
<td id="S4.T3.1.1.10.8.2" class="ltx_td"></td>
<td id="S4.T3.1.1.10.8.3" class="ltx_td"></td>
<td id="S4.T3.1.1.10.8.4" class="ltx_td"></td>
<td id="S4.T3.1.1.10.8.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.1.1.11.9" class="ltx_tr">
<th id="S4.T3.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simstring</th>
<td id="S4.T3.1.1.11.9.2" class="ltx_td ltx_align_center">0.380</td>
<td id="S4.T3.1.1.11.9.3" class="ltx_td ltx_align_center">0.248</td>
<td id="S4.T3.1.1.11.9.4" class="ltx_td ltx_align_center">0.426</td>
<td id="S4.T3.1.1.11.9.5" class="ltx_td ltx_align_center">0.385</td>
</tr>
<tr id="S4.T3.1.1.12.10" class="ltx_tr">
<th id="S4.T3.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â (jaccard)</th>
<td id="S4.T3.1.1.12.10.2" class="ltx_td"></td>
<td id="S4.T3.1.1.12.10.3" class="ltx_td"></td>
<td id="S4.T3.1.1.12.10.4" class="ltx_td"></td>
<td id="S4.T3.1.1.12.10.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.1.1.13.11" class="ltx_tr">
<th id="S4.T3.1.1.13.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FuzzyChinese</th>
<td id="S4.T3.1.1.13.11.2" class="ltx_td ltx_align_center">0.168</td>
<td id="S4.T3.1.1.13.11.3" class="ltx_td ltx_align_center">0.000</td>
<td id="S4.T3.1.1.13.11.4" class="ltx_td ltx_align_center">0.473</td>
<td id="S4.T3.1.1.13.11.5" class="ltx_td ltx_align_center">0.372</td>
</tr>
<tr id="S4.T3.1.1.14.12" class="ltx_tr">
<th id="S4.T3.1.1.14.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Â Â Â Â (stroke)</th>
<td id="S4.T3.1.1.14.12.2" class="ltx_td"></td>
<td id="S4.T3.1.1.14.12.3" class="ltx_td"></td>
<td id="S4.T3.1.1.14.12.4" class="ltx_td"></td>
<td id="S4.T3.1.1.14.12.5" class="ltx_td"></td>
</tr>
<tr id="S4.T3.1.1.15.13" class="ltx_tr">
<th id="S4.T3.1.1.15.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">FuzzyChinese</th>
<td id="S4.T3.1.1.15.13.2" class="ltx_td ltx_align_center">0.230</td>
<td id="S4.T3.1.1.15.13.3" class="ltx_td ltx_align_center">0.110</td>
<td id="S4.T3.1.1.15.13.4" class="ltx_td ltx_align_center">0.137</td>
<td id="S4.T3.1.1.15.13.5" class="ltx_td ltx_align_center">0.197</td>
</tr>
<tr id="S4.T3.1.1.16.14" class="ltx_tr">
<th id="S4.T3.1.1.16.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Â Â Â Â (character)</th>
<td id="S4.T3.1.1.16.14.2" class="ltx_td ltx_border_bb"></td>
<td id="S4.T3.1.1.16.14.3" class="ltx_td ltx_border_bb"></td>
<td id="S4.T3.1.1.16.14.4" class="ltx_td ltx_border_bb"></td>
<td id="S4.T3.1.1.16.14.5" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span><span id="S4.T3.3.1" class="ltx_text ltx_font_bold">Matching Results: Synthetic Data</span>. This table reports accuracy linking synthetic paired data generated by OCRâ€™ing location and firm names - rendered with augmented digital fonts - with two different OCR engines.</figcaption>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> complements end-to-end deep neural methods. The <cite class="ltx_cite ltx_citemacro_citet">Arora etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> methods cannot be used when researchers lack access to the original document images. Moreover, researchers often lack the compute or technical requirements to work with image data, whereas the vast majority of quantitative social science researchers are comfortable processing strings. On the language side, there are many contexts where using a language model may contribute little. Person or location names - for instance - donâ€™t contain much natural language relative to firm names. String matching methods remain the most widely used because they are simple and cheap to use off-the-shelf, and there are contexts where more sophisticated methods may not be feasible or offer large incremental gains. <span id="S4.p5.1.2" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> is an extensible way to improve string matching when linking OCRâ€™ed datasets.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2305.14672/assets/figs/bold_matches.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="277" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span><span id="S4.F3.2.1" class="ltx_text ltx_font_bold">Error analysis.</span> Panel A shows representative errors from homoglyphic matching. Panel B shows representative cases that homoglyphic matching gets correct. The ground truth string is shown in column (1). PaddleOCR is used to OCR the query images (column (2)) and EasyOCR is used to OCR their corresponding keys (column (3)). Columns (4) through (7) give the selected match to the query using different string matching methods, with the correct match shown in column (3). Bold characters differ from the query.</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Finally, Table <a href="#S4.T3" title="Table 3 â€£ 4 Results â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> reports results with the synthetically generated record linkage dataset, to elucidate the performance of homoglyphic matching across languages using the CJK script. Homoglyphs outperform other string matching methods. The only case where the performance of another method is similar is Simplified Chinese, where the FuzzyChinese package using stroke level <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.p6.1.m1.1a"><mi id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><ci id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">n</annotation></semantics></math>-grams performs similarly. The stroke dictionary that underlies FuzzyChinese was crafted for Simplified Chinese, yet homoglyphs can perform similarly with self-supervised methods. On Traditional Chinese, which proliferates in historical documents, homoglyphic edit distance offers a nine percentage point accuracy advantage over FuzzyChinese, illustrating the extensibility advantages of self-supervised methods. The accuracy rates are rather low, but this must be interpreted in the context of the dataset, which only includes paired records where the OCR differs.</p>
</div>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">Figure <a href="#S4.F3" title="Figure 3 â€£ 4 Results â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides an error analysis for the synthetic record linkage exercise. The ground truth string is shown in the first column, PaddleOCR is used to OCR the query (column 2) and EasyOCR is used to OCR the key and provides the correct match (column 3). The matches selected from the key by different string matching methods are shown in columns (4) through (7).</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">Panel A shows cases where homoglyphic edit distance selects an incorrect match. This typically occurs when the OCRâ€™ed text has a similar visual appearance to another firm in the index, showing the limits of homoglyphs to fully close the OCR information bottleneck. Panel B shows cases where homoglyphic edit distance selects a correct match, avoiding the wrong strings chosen by other methods through exploiting character visual similarity.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Extending Homoglyphs</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">While this study focuses on the modern CJK script, <span id="S5.p1.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> can be extended to any character set. As a proof of concept, we explore its extensibility to ancient Chinese characters. Like other early forms of human writing, ancient Chinese scripts are highly pictorial relative to modern characters.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Using an existing database of grouped ancient characters from different archaeological sites and periods that correspond to the same concept <cite class="ltx_cite ltx_citemacro_cite">AcademiaÂ Sinica etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite>, we contrastively learn a metric space where the representations of different views of ancient characters denoting the same concept are nearby. We train on 25,984 character views, as well as the corresponding modern augmented fonts. The dataset includes characters from the Shang Dynasty (1600 BC-1045 BC), the Western Zhou (1045 BC-771 BC), the Spring and Autumn Warring States Era (770 BBC -221 BC), and the Qin-Han Dynasties (221BC - circa third century).<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>We exclude images from the Shuowen Jiezi - a book on ancient characters - limiting to the most reliable character renders, which were drawn from archaeological sites.</span></span></span> To illustrate homoglyphs, we create a reference set for the Shang Dynasty, randomly choosing one character for each concept.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Figure <a href="#S5.F4" title="Figure 4 â€£ 5 Extending Homoglyphs â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows representative examples of homoglyphs, consisting of a character and its five nearest neighbors. The modern character descendant - taken from the database - as well as a short description of the ancient concept are provided. The description draws upon <cite class="ltx_cite ltx_citemacro_citet">Li (<a href="#bib.bib25" title="" class="ltx_ref">2012</a>)</cite> as well.</p>
</div>
<figure id="S5.F4" class="ltx_figure"><img src="/html/2305.14672/assets/figs/ancient.png" id="S5.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="648" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span id="S5.F4.2.1" class="ltx_text ltx_font_bold">Ancient Homoglyphs.</span> This figure shows homoglyph sets constructed for ancient Chinese, with the descendant modern Chinese character and a description of the characterâ€™s ancient meaning.</figcaption>
</figure>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">The homoglyph sets are able to capture related abstract concepts noted in the archaeological literature. The first line shows that the concepts of writing, law, learning, and morning (â€œrecording the sunâ€) are homoglyphs, and the second line shows that characters for different types of officials are homoglyphs, as are characters denoting â€œjoining.â€ The final line shows that history and government official are homoglyphs - underscoring the central role of the government in constituting history - as are characters denoting conquest, tying up, and city center (denoted by a prisoner to be executed by the government, which occurred in the city center).</p>
</div>
<div id="S5.p5" class="ltx_para">
<p id="S5.p5.1" class="ltx_p">Not all concepts within each set are related, but many of the connections above have been noted in an archaeological literature examining how ancient peoples conceptualized the world (<span id="S5.p5.1.1" class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_citet">Wang (<a href="#bib.bib49" title="" class="ltx_ref">2003</a>)</cite>). That these meanings can be captured using <span id="S5.p5.1.2" class="ltx_text ltx_font_italic">vision</span> transformers is a fascinating illustration of the relationship between images, written language, and meaning in ancient societies.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Using homoglyphs for string matching inherits the well-known limitations of string matching.
In some cases, OCR destroys too much information for record linkage to be feasible with the resulting strings.
Even with clean OCR, sometimes language understanding is necessary to determine the correct match.
Homoglyphs do not address other types of string substitutions, like those that result from enumerator misspellings, although in principle a similar contrastive approach could also be developed to quantify other types of string substitutions.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">More sophisticated methods have been developed as alternatives to string matching. For example, <cite class="ltx_cite ltx_citemacro_citet">Ventura etÂ al. (<a href="#bib.bib47" title="" class="ltx_ref">2015</a>)</cite> use a random forest classifier trained on labeled data to disambiguate authors of U.S. patents, applying clustering to the resulting dissimilarity scores to enforce transitivity.
<cite class="ltx_cite ltx_citemacro_citet">Arora etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2023</a>)</cite> use multimodal methods that combine the image crops of entities and their OCR and also develop a vision-only OCR free linkage method.
Bayesian methods have also been used, <span id="S6.p2.1.1" class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_citet">Sadinle (<a href="#bib.bib37" title="" class="ltx_ref">2014</a>, <a href="#bib.bib38" title="" class="ltx_ref">2017</a>)</cite>. They offer the advantage of uncertainty quantification - another well-known limitation of string matching - but do not scale well.</p>
</div>
<div id="S6.p3" class="ltx_para">
<p id="S6.p3.1" class="ltx_p">While these methods can offer advantages, they are not always applicable. Researchers may lack access to the original document images, or may lack the compute or technical resources to process images, limiting the use of OCR-free or multimodal approaches. Language models are unlikely to be useful in linking individual names, a common application. Labeled data may be infeasibly costly to create at a sufficient scale for training supervised models. Finally, researchers in disciplines like social science often lack familiarity with machine learning methods, but most are familiar with off-the-shelf string matching packages. String matching methods are also cheap to scale to massive datasets. Simple string matching algorithms are often preferred by practitioners and can be the most suitable tool given the constraints.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">Homoglyphic edit distance significantly improves string matching accuracy on OCRâ€™ed documents, by integrating information about character similarity from purely self-supervised vision transformers. It can be implemented using a simple, off-the-shelf string matching package.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Package available at <a target="_blank" href="https://pypi.org/project/HomoglyphsCJK/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pypi.org/project/HomoglyphsCJK/</a>.</span></span></span> Learning homoglyphs through self-supervised vision transformers is hhighly extensible, including to low resource settings and settings with many characters. By improving record linkage in such settings - where handcrafted features used to improve record linkage are not available - research on important questions requiring linked data can become more representative of the diversity of human societies.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S7.1" class="ltx_sectional-block">
<section id="Sx1" class="ltx_section ltx_centering">
<h2 class="ltx_title ltx_title_section">Supplementary Materials</h2>

</section>
</div>
</section>
<section id="S1a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S-1 </span><span id="S1a.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> Model Details</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S-1.1 </span>Encoder</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">For both of our applications, we use a DINO pre-trained <cite class="ltx_cite ltx_citemacro_cite">Caron etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2021</a>)</cite> vision transformer (ViT) as the encoder. Our implementation of the ViT comes from the Pytorch Image Models library (timm) <cite class="ltx_cite ltx_citemacro_cite">Wightman (<a href="#bib.bib50" title="" class="ltx_ref">2019</a>)</cite>. Specifically, we use the <span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_italic">vit_base_patch16_224.dino</span> model that corresponds to the official DINO-pretained ViT-base model with a patch size of 16 and with input resolution of <math id="S1.SS1.p1.1.m1.1" class="ltx_Math" alttext="224^{2}" display="inline"><semantics id="S1.SS1.p1.1.m1.1a"><msup id="S1.SS1.p1.1.m1.1.1" xref="S1.SS1.p1.1.m1.1.1.cmml"><mn id="S1.SS1.p1.1.m1.1.1.2" xref="S1.SS1.p1.1.m1.1.1.2.cmml">224</mn><mn id="S1.SS1.p1.1.m1.1.1.3" xref="S1.SS1.p1.1.m1.1.1.3.cmml">2</mn></msup><annotation-xml encoding="MathML-Content" id="S1.SS1.p1.1.m1.1b"><apply id="S1.SS1.p1.1.m1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.SS1.p1.1.m1.1.1.1.cmml" xref="S1.SS1.p1.1.m1.1.1">superscript</csymbol><cn type="integer" id="S1.SS1.p1.1.m1.1.1.2.cmml" xref="S1.SS1.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S1.SS1.p1.1.m1.1.1.3.cmml" xref="S1.SS1.p1.1.m1.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS1.p1.1.m1.1c">224^{2}</annotation></semantics></math>. The pretrained checkpoint does not have a classification head.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S-1.2 </span>Loss function</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">We use Supervised Contrastive loss <cite class="ltx_cite ltx_citemacro_cite">Khosla etÂ al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite> as our training objective, as implemented in the PyTorch Metric Learning library <cite class="ltx_cite ltx_citemacro_cite">Musgrave etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite>, where the temperature parameter is set to 0.1.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S-1.3 </span>Data Augmentation</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">We deploy several image augmentations, using transformations provided in the Torchvision library <cite class="ltx_cite ltx_citemacro_cite">TorchVision (<a href="#bib.bib45" title="" class="ltx_ref">2016</a>)</cite>. These include Affine transformation (only slight translation and scaling allowed), Random Color Jitter,
Random Autocontrast, Random Gaussian Blurring, and Random Grayscale. Additionally, we pad the character to make the image square while preserving the aspect ratio of the character render.
We do not use common augmentations like Random Cropping or Center Cropping, to avoid destroying too much information.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">For augmenting the skeleton of the rendered character itself, we use a variety of digital fonts to render the images. We use 27 fonts for Simplified Chinese, 17 fonts for Traditional Chinese (for both string matching and ancient Chinese), 62 fonts for Korean, and 14 fonts for Japanese.</p>
</div>
</section>
</section>
<section id="S2a" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">S-2 </span>Application-specific details</h2>

<section id="S2.SS1a" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S-2.1 </span>Record Linkage</h3>

<section id="S2.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.1.1 </span>Data</h4>

<div id="S2.SS1.SSS1.p1" class="ltx_para">
<p id="S2.SS1.SSS1.p1.1" class="ltx_p">For each script, the dataset consists of images of characters from the corresponding script rendered with different fonts and augmented during training. The number of characters for each script seen during training is given in Table <a href="#S2.T1" title="Table S-1 â€£ S-2.1.1 Data â€£ S-2.1 Record Linkage â€£ S-2 Application-specific details â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-1</span></a>. Each character can be considered a "class" to which its digital renders belong. Characters do not need to be seen during training to be considered at inference time, an advantage if users wish to expand the homoglpyph sets (<span id="S2.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">e.g.</span> because an OCR engine uses a different character set). We illustrate this empirically by expanding the character set to characters covered by the three OCR engines we explore that were not included in our character ranges used initially for training.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<th id="S2.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt"><span id="S2.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Script</span></th>
<th id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Training</span></th>
<th id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S2.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Inference</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.1.2.1" class="ltx_tr">
<th id="S2.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Japanese</th>
<td id="S2.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">17,963</td>
<td id="S2.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">17,963</td>
</tr>
<tr id="S2.T1.1.3.2" class="ltx_tr">
<th id="S2.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Simplified Chinese</th>
<td id="S2.T1.1.3.2.2" class="ltx_td ltx_align_center">6,621</td>
<td id="S2.T1.1.3.2.3" class="ltx_td ltx_align_center">7,806</td>
</tr>
<tr id="S2.T1.1.4.3" class="ltx_tr">
<th id="S2.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Traditional Chinese</th>
<td id="S2.T1.1.4.3.2" class="ltx_td ltx_align_center">8,415</td>
<td id="S2.T1.1.4.3.3" class="ltx_td ltx_align_center">8,628</td>
</tr>
<tr id="S2.T1.1.5.4" class="ltx_tr">
<th id="S2.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Korean</th>
<td id="S2.T1.1.5.4.2" class="ltx_td ltx_align_center">3,686</td>
<td id="S2.T1.1.5.4.3" class="ltx_td ltx_align_center">3,729</td>
</tr>
<tr id="S2.T1.1.6.5" class="ltx_tr">
<th id="S2.T1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="S2.T1.1.6.5.1.1" class="ltx_text ltx_font_bold">Total</span></th>
<td id="S2.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T1.1.6.5.2.1" class="ltx_text ltx_font_bold">36,685</span></td>
<td id="S2.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S2.T1.1.6.5.3.1" class="ltx_text ltx_font_bold">38,126</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table S-1: </span><span id="S2.T1.3.1" class="ltx_text ltx_font_bold">Training and Inference Sizes</span>. This table shows the training and inference sizes for different language scripts.</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.1.2 </span>Batching</h4>

<div id="S2.SS1.SSS2.p1" class="ltx_para">
<p id="S2.SS1.SSS2.p1.1" class="ltx_p"><span id="S2.SS1.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Without hard-negative mining</span></p>
</div>
<div id="S2.SS1.SSS2.p2" class="ltx_para">
<p id="S2.SS1.SSS2.p2.3" class="ltx_p">Let <math id="S2.SS1.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S2.SS1.SSS2.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p2.1.m1.1.1" xref="S2.SS1.SSS2.p2.1.m1.1.1.cmml">â„¬</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.1.m1.1b"><ci id="S2.SS1.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p2.1.m1.1.1">â„¬</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.1.m1.1c">\mathcal{B}</annotation></semantics></math> denote the batch size.
A batch consists of <math id="S2.SS1.SSS2.p2.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.SS1.SSS2.p2.2.m2.1a"><mi id="S2.SS1.SSS2.p2.2.m2.1.1" xref="S2.SS1.SSS2.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.2.m2.1b"><ci id="S2.SS1.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p2.2.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.2.m2.1c">m</annotation></semantics></math> views of <math id="S2.SS1.SSS2.p2.3.m3.1" class="ltx_Math" alttext="\dfrac{\mathcal{B}}{m}" display="inline"><semantics id="S2.SS1.SSS2.p2.3.m3.1a"><mstyle displaystyle="true" id="S2.SS1.SSS2.p2.3.m3.1.1" xref="S2.SS1.SSS2.p2.3.m3.1.1.cmml"><mfrac id="S2.SS1.SSS2.p2.3.m3.1.1a" xref="S2.SS1.SSS2.p2.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.SSS2.p2.3.m3.1.1.2" xref="S2.SS1.SSS2.p2.3.m3.1.1.2.cmml">â„¬</mi><mi id="S2.SS1.SSS2.p2.3.m3.1.1.3" xref="S2.SS1.SSS2.p2.3.m3.1.1.3.cmml">m</mi></mfrac></mstyle><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p2.3.m3.1b"><apply id="S2.SS1.SSS2.p2.3.m3.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1"><divide id="S2.SS1.SSS2.p2.3.m3.1.1.1.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1"></divide><ci id="S2.SS1.SSS2.p2.3.m3.1.1.2.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1.2">â„¬</ci><ci id="S2.SS1.SSS2.p2.3.m3.1.1.3.cmml" xref="S2.SS1.SSS2.p2.3.m3.1.1.3">ğ‘š</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p2.3.m3.1c">\dfrac{\mathcal{B}}{m}</annotation></semantics></math> classes sampled without replacement. When all the views for a class are utilized, all images are replaced and the sampling process without replacement starts again. â€œViewsâ€ of a character are augmented digital renders using the fonts and transformations described above.</p>
</div>
<div id="S2.SS1.SSS2.p3" class="ltx_para">
<p id="S2.SS1.SSS2.p3.1" class="ltx_p">One training epoch is defined as seeing all characters and their m views exactly once.</p>
</div>
<div id="S2.SS1.SSS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS1.SSS2.p4.2" class="ltx_p"><span id="S2.SS1.SSS2.p4.2.1" class="ltx_text ltx_font_bold">With hard-negative mining</span> 
<br class="ltx_break">We find <math id="S2.SS1.SSS2.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.SSS2.p4.1.m1.1a"><mi id="S2.SS1.SSS2.p4.1.m1.1.1" xref="S2.SS1.SSS2.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p4.1.m1.1b"><ci id="S2.SS1.SSS2.p4.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p4.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p4.1.m1.1c">k</annotation></semantics></math> nearest neighbors of each character (or class) on a checkpoint trained without hard negatives. We do this by rendering all characters with a â€œreference font - Noto Serif CJK font (Tc/Sc/Jp/Ko)â€ depending upon the script and finding <math id="S2.SS1.SSS2.p4.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.SSS2.p4.2.m2.1a"><mi id="S2.SS1.SSS2.p4.2.m2.1.1" xref="S2.SS1.SSS2.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p4.2.m2.1b"><ci id="S2.SS1.SSS2.p4.2.m2.1.1.cmml" xref="S2.SS1.SSS2.p4.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p4.2.m2.1c">k</annotation></semantics></math> nearest neighbors using the above checkpoint.</p>
</div>
<div id="S2.SS1.SSS2.p5" class="ltx_para">
<p id="S2.SS1.SSS2.p5.1" class="ltx_p">We create batches as before, but this time, randomly intersperse all hard negative sets (of size k) in the batches.</p>
</div>
<div id="S2.SS1.SSS2.p6" class="ltx_para">
<p id="S2.SS1.SSS2.p6.1" class="ltx_p">One training epoch is now defined as seeing all characters and their m views and additionally, all characters and their hard negative sets (composed of <math id="S2.SS1.SSS2.p6.1.m1.1" class="ltx_Math" alttext="k-1" display="inline"><semantics id="S2.SS1.SSS2.p6.1.m1.1a"><mrow id="S2.SS1.SSS2.p6.1.m1.1.1" xref="S2.SS1.SSS2.p6.1.m1.1.1.cmml"><mi id="S2.SS1.SSS2.p6.1.m1.1.1.2" xref="S2.SS1.SSS2.p6.1.m1.1.1.2.cmml">k</mi><mo id="S2.SS1.SSS2.p6.1.m1.1.1.1" xref="S2.SS1.SSS2.p6.1.m1.1.1.1.cmml">âˆ’</mo><mn id="S2.SS1.SSS2.p6.1.m1.1.1.3" xref="S2.SS1.SSS2.p6.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS2.p6.1.m1.1b"><apply id="S2.SS1.SSS2.p6.1.m1.1.1.cmml" xref="S2.SS1.SSS2.p6.1.m1.1.1"><minus id="S2.SS1.SSS2.p6.1.m1.1.1.1.cmml" xref="S2.SS1.SSS2.p6.1.m1.1.1.1"></minus><ci id="S2.SS1.SSS2.p6.1.m1.1.1.2.cmml" xref="S2.SS1.SSS2.p6.1.m1.1.1.2">ğ‘˜</ci><cn type="integer" id="S2.SS1.SSS2.p6.1.m1.1.1.3.cmml" xref="S2.SS1.SSS2.p6.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS2.p6.1.m1.1c">k-1</annotation></semantics></math> neighbors) and their m views exactly once.</p>
</div>
<div id="S2.SS1.SSS2.p7" class="ltx_para">
<p id="S2.SS1.SSS2.p7.1" class="ltx_p">Table <a href="#S2.T2" title="Table S-2 â€£ S-2.1.2 Batching â€£ S-2.1 Record Linkage â€£ S-2 Application-specific details â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-2</span></a> contains the number of epochs we trained each model for.</p>
</div>
<figure id="S2.T2" class="ltx_table">
<div id="S2.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:423.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(23.4pt,-22.9pt) scale(1.12126857294461,1.12126857294461) ;">
<table id="S2.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.1.1.1.1" class="ltx_tr">
<th id="S2.T2.1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">Model</th>
<th id="S2.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">lr</th>
<th id="S2.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">weight decay</th>
<th id="S2.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">T_0</th>
<th id="S2.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">T_mult</th>
<th id="S2.T2.1.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Epochs</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.1.1.2.1" class="ltx_tr">
<td id="S2.T2.1.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">Japanese - (No HN)</td>
<td id="S2.T2.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2e-5</td>
<td id="S2.T2.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">5e-3</td>
<td id="S2.T2.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">1</td>
<td id="S2.T2.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S2.T2.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t">100</td>
</tr>
<tr id="S2.T2.1.1.3.2" class="ltx_tr">
<td id="S2.T2.1.1.3.2.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.3.2.2" class="ltx_td"></td>
<td id="S2.T2.1.1.3.2.3" class="ltx_td"></td>
<td id="S2.T2.1.1.3.2.4" class="ltx_td"></td>
<td id="S2.T2.1.1.3.2.5" class="ltx_td"></td>
<td id="S2.T2.1.1.3.2.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.4.3" class="ltx_tr">
<td id="S2.T2.1.1.4.3.1" class="ltx_td ltx_align_left">Japanese - (HN)</td>
<td id="S2.T2.1.1.4.3.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.4.3.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.4.3.4" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.1.4.3.5" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.1.4.3.6" class="ltx_td ltx_align_center">30</td>
</tr>
<tr id="S2.T2.1.1.5.4" class="ltx_tr">
<td id="S2.T2.1.1.5.4.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.5.4.2" class="ltx_td"></td>
<td id="S2.T2.1.1.5.4.3" class="ltx_td"></td>
<td id="S2.T2.1.1.5.4.4" class="ltx_td"></td>
<td id="S2.T2.1.1.5.4.5" class="ltx_td"></td>
<td id="S2.T2.1.1.5.4.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.6.5" class="ltx_tr">
<td id="S2.T2.1.1.6.5.1" class="ltx_td ltx_align_left">Simplified Chinese - (No HN)</td>
<td id="S2.T2.1.1.6.5.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.6.5.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.6.5.4" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.1.6.5.5" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.1.6.5.6" class="ltx_td ltx_align_center">30</td>
</tr>
<tr id="S2.T2.1.1.7.6" class="ltx_tr">
<td id="S2.T2.1.1.7.6.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.7.6.2" class="ltx_td"></td>
<td id="S2.T2.1.1.7.6.3" class="ltx_td"></td>
<td id="S2.T2.1.1.7.6.4" class="ltx_td"></td>
<td id="S2.T2.1.1.7.6.5" class="ltx_td"></td>
<td id="S2.T2.1.1.7.6.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.8.7" class="ltx_tr">
<td id="S2.T2.1.1.8.7.1" class="ltx_td ltx_align_left">Simplified Chinese - (HN)</td>
<td id="S2.T2.1.1.8.7.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.8.7.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.8.7.4" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.1.8.7.5" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.1.8.7.6" class="ltx_td ltx_align_center">30</td>
</tr>
<tr id="S2.T2.1.1.9.8" class="ltx_tr">
<td id="S2.T2.1.1.9.8.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.9.8.2" class="ltx_td"></td>
<td id="S2.T2.1.1.9.8.3" class="ltx_td"></td>
<td id="S2.T2.1.1.9.8.4" class="ltx_td"></td>
<td id="S2.T2.1.1.9.8.5" class="ltx_td"></td>
<td id="S2.T2.1.1.9.8.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.10.9" class="ltx_tr">
<td id="S2.T2.1.1.10.9.1" class="ltx_td ltx_align_left">Traditional Chinese - (No HN)</td>
<td id="S2.T2.1.1.10.9.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.10.9.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.10.9.4" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.1.10.9.5" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.1.10.9.6" class="ltx_td ltx_align_center">30</td>
</tr>
<tr id="S2.T2.1.1.11.10" class="ltx_tr">
<td id="S2.T2.1.1.11.10.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.11.10.2" class="ltx_td"></td>
<td id="S2.T2.1.1.11.10.3" class="ltx_td"></td>
<td id="S2.T2.1.1.11.10.4" class="ltx_td"></td>
<td id="S2.T2.1.1.11.10.5" class="ltx_td"></td>
<td id="S2.T2.1.1.11.10.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.12.11" class="ltx_tr">
<td id="S2.T2.1.1.12.11.1" class="ltx_td ltx_align_left">Traditional Chinese - (HN)</td>
<td id="S2.T2.1.1.12.11.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.12.11.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.12.11.4" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.1.12.11.5" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.1.12.11.6" class="ltx_td ltx_align_center">30</td>
</tr>
<tr id="S2.T2.1.1.13.12" class="ltx_tr">
<td id="S2.T2.1.1.13.12.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.13.12.2" class="ltx_td"></td>
<td id="S2.T2.1.1.13.12.3" class="ltx_td"></td>
<td id="S2.T2.1.1.13.12.4" class="ltx_td"></td>
<td id="S2.T2.1.1.13.12.5" class="ltx_td"></td>
<td id="S2.T2.1.1.13.12.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.14.13" class="ltx_tr">
<td id="S2.T2.1.1.14.13.1" class="ltx_td ltx_align_left">Korean - (No HN)</td>
<td id="S2.T2.1.1.14.13.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.14.13.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.14.13.4" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.1.14.13.5" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.1.14.13.6" class="ltx_td ltx_align_center">60</td>
</tr>
<tr id="S2.T2.1.1.15.14" class="ltx_tr">
<td id="S2.T2.1.1.15.14.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.15.14.2" class="ltx_td"></td>
<td id="S2.T2.1.1.15.14.3" class="ltx_td"></td>
<td id="S2.T2.1.1.15.14.4" class="ltx_td"></td>
<td id="S2.T2.1.1.15.14.5" class="ltx_td"></td>
<td id="S2.T2.1.1.15.14.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.16.15" class="ltx_tr">
<td id="S2.T2.1.1.16.15.1" class="ltx_td ltx_align_left">Korean - (HN)</td>
<td id="S2.T2.1.1.16.15.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.16.15.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.16.15.4" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.1.16.15.5" class="ltx_td ltx_align_center">2</td>
<td id="S2.T2.1.1.16.15.6" class="ltx_td ltx_align_center">30</td>
</tr>
<tr id="S2.T2.1.1.17.16" class="ltx_tr">
<td id="S2.T2.1.1.17.16.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.17.16.2" class="ltx_td"></td>
<td id="S2.T2.1.1.17.16.3" class="ltx_td"></td>
<td id="S2.T2.1.1.17.16.4" class="ltx_td"></td>
<td id="S2.T2.1.1.17.16.5" class="ltx_td"></td>
<td id="S2.T2.1.1.17.16.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.18.17" class="ltx_tr">
<td id="S2.T2.1.1.18.17.1" class="ltx_td ltx_align_left">Ancient Chinese - (No HN)</td>
<td id="S2.T2.1.1.18.17.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.18.17.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.18.17.4" class="ltx_td ltx_align_center">300</td>
<td id="S2.T2.1.1.18.17.5" class="ltx_td ltx_align_center">1</td>
<td id="S2.T2.1.1.18.17.6" class="ltx_td ltx_align_center">200</td>
</tr>
<tr id="S2.T2.1.1.19.18" class="ltx_tr">
<td id="S2.T2.1.1.19.18.1" class="ltx_td ltx_align_left">Â Â Â Â distance</td>
<td id="S2.T2.1.1.19.18.2" class="ltx_td"></td>
<td id="S2.T2.1.1.19.18.3" class="ltx_td"></td>
<td id="S2.T2.1.1.19.18.4" class="ltx_td"></td>
<td id="S2.T2.1.1.19.18.5" class="ltx_td"></td>
<td id="S2.T2.1.1.19.18.6" class="ltx_td"></td>
</tr>
<tr id="S2.T2.1.1.20.19" class="ltx_tr">
<td id="S2.T2.1.1.20.19.1" class="ltx_td ltx_align_left">Ancient Chinese - (HN)</td>
<td id="S2.T2.1.1.20.19.2" class="ltx_td ltx_align_center">2e-5</td>
<td id="S2.T2.1.1.20.19.3" class="ltx_td ltx_align_center">5e-3</td>
<td id="S2.T2.1.1.20.19.4" class="ltx_td ltx_align_center">300</td>
<td id="S2.T2.1.1.20.19.5" class="ltx_td ltx_align_center">3</td>
<td id="S2.T2.1.1.20.19.6" class="ltx_td ltx_align_center">24</td>
</tr>
<tr id="S2.T2.1.1.21.20" class="ltx_tr">
<td id="S2.T2.1.1.21.20.1" class="ltx_td ltx_align_left ltx_border_bb">Â Â Â Â distance</td>
<td id="S2.T2.1.1.21.20.2" class="ltx_td ltx_border_bb"></td>
<td id="S2.T2.1.1.21.20.3" class="ltx_td ltx_border_bb"></td>
<td id="S2.T2.1.1.21.20.4" class="ltx_td ltx_border_bb"></td>
<td id="S2.T2.1.1.21.20.5" class="ltx_td ltx_border_bb"></td>
<td id="S2.T2.1.1.21.20.6" class="ltx_td ltx_border_bb"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table S-2: </span><span id="S2.T2.3.1" class="ltx_text ltx_font_bold">Training Hyperparameters</span>. This table reports the training hyperparameters used for the models. The lr stands for learning rate, weight decay represents the weight decay factor, T_0 is the number of steps until the first restart of the learning rate scheduler, T_mult denotes the factor by which T_0 is multiplied at each restart, and Epochs indicates the total number of training epochs. Parameters not mentioned here use PyTorch defaults. HN denotes offline hard-negative mining.</figcaption>
</figure>
</section>
<section id="S2.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.1.3 </span>Model Validation</h4>

<div id="S2.SS1.SSS3.p1" class="ltx_para">
<p id="S2.SS1.SSS3.p1.1" class="ltx_p">We split the characters into an 80-10-10 train-val-test set.
We embed the validation images and find the nearest neighbor among the embeddings of digital renders of the universe of characters in the script, rendered with the reference font described above. The top-1 retrieval accuracy is used as the validation metric for the selection of the best checkpoint. We see a peak validation accuracy of 90% for Japanese, 98% for Korean, 91% for Traditional Chinese, and 91% for Simplified Chinese.</p>
</div>
</section>
<section id="S2.SS1.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.1.4 </span>Other training details</h4>

<div id="S2.SS1.SSS4.p1" class="ltx_para">
<p id="S2.SS1.SSS4.p1.1" class="ltx_p">CJK glyphs are similar across the scripts. To converge faster, for the rest of the languages, we initialize the weights of the encoder with the checkpoint used for the <span id="S2.SS1.SSS4.p1.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> encoder for Japanese - the script with the largest number of characters.
We use AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite> as the optimizer and Cosine Annealing with Warm Restarts <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib26" title="" class="ltx_ref">2016</a>)</cite> as the learning rate schedule. We use the standard Pytorch implementation for both. The relevant hyperparameters are listed in Table <a href="#S2.T2" title="Table S-2 â€£ S-2.1.2 Batching â€£ S-2.1 Record Linkage â€£ S-2 Application-specific details â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-2</span></a>. We stop training the models once the validation accuracy stagnates and the checkpoint with the best validation accuracy is chosen as the encoder for each script.</p>
</div>
</section>
</section>
<section id="S2.SS2a" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S-2.2 </span>Homoglyph Sets</h3>

<div id="S2.SS2a.p1" class="ltx_para">
<p id="S2.SS2a.p1.1" class="ltx_p">We allow for the expansion of the character set beyond what is seen in training because different OCR engines use different character dictionaries (a list of characters supported by the engine). We take the union of characters from the character dictionaries of PaddleOCR, EasyOCR, and EfficientOCR.
For each script, we render all its characters using the scriptâ€™s reference font and embed them using the script-specific <span id="S2.SS2a.p1.1.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> encoder. For each character, we then find 800-nearest neighbours (measured by Cosine Similarity between the embeddings) among the set of all renders in the reference set. We store these as a look-up dictionary that contains, for each character in a script, its 800 neighbors and its Cosine Similarity with all of them. This look-up dictionary is used in our modified Levenshtein distance implementation to modify the substitution cost. The dictionaries are available in our GitHub repository <cite class="ltx_cite ltx_citemacro_cite">Yang etÂ al. (<a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS2a.p2" class="ltx_para">
<p id="S2.SS2a.p2.1" class="ltx_p">Table <a href="#S2.T1" title="Table S-1 â€£ S-2.1.1 Data â€£ S-2.1 Record Linkage â€£ S-2 Application-specific details â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-1</span></a> contains the number of characters that were used to prepare these sets for each script.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S-2.3 </span>Implementing the Modified Levenshtein distance</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.3" class="ltx_p">We use a standard algorithm to calculate Levenshtein distance that uses dynamic programming <cite class="ltx_cite ltx_citemacro_cite">Wagner and Fischer (<a href="#bib.bib48" title="" class="ltx_ref">1974</a>)</cite>. The space and time complexity of the algorithm is <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{O}(mn)" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><mrow id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">ğ’ª</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S2.SS3.p1.1.m1.1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS3.p1.1.m1.1.1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS3.p1.1.m1.1.1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p1.1.m1.1.1.1.1.1.1" xref="S2.SS3.p1.1.m1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S2.SS3.p1.1.m1.1.1.1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.1.1.1.3.cmml">n</mi></mrow><mo stretchy="false" id="S2.SS3.p1.1.m1.1.1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><times id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2"></times><ci id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">ğ’ª</ci><apply id="S2.SS3.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1"><times id="S2.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.1"></times><ci id="S2.SS3.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.2">ğ‘š</ci><ci id="S2.SS3.p1.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.1.1.1.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\mathcal{O}(mn)</annotation></semantics></math> where <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><mi id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ğ‘š</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">m</annotation></semantics></math> and <math id="S2.SS3.p1.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS3.p1.3.m3.1a"><mi id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">n</annotation></semantics></math> are the lengths of the two strings that are being compared.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.9" class="ltx_p">We modify this algorithm by switching the standard substitution cost <math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><mi id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><ci id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">\lambda</annotation></semantics></math> between two characters <math id="S2.SS3.p2.2.m2.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS3.p2.2.m2.1a"><mi id="S2.SS3.p2.2.m2.1.1" xref="S2.SS3.p2.2.m2.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.2.m2.1b"><ci id="S2.SS3.p2.2.m2.1.1.cmml" xref="S2.SS3.p2.2.m2.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.2.m2.1c">a</annotation></semantics></math> and <math id="S2.SS3.p2.3.m3.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.SS3.p2.3.m3.1a"><mi id="S2.SS3.p2.3.m3.1.1" xref="S2.SS3.p2.3.m3.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.3.m3.1b"><ci id="S2.SS3.p2.3.m3.1.1.cmml" xref="S2.SS3.p2.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.3.m3.1c">b</annotation></semantics></math> with <math id="S2.SS3.p2.4.m4.2" class="ltx_math_unparsed" alttext="\lambda*(1-CosineSimilarity(u(a),u(b))" display="inline"><semantics id="S2.SS3.p2.4.m4.2a"><mrow id="S2.SS3.p2.4.m4.2b"><mi id="S2.SS3.p2.4.m4.2.3">Î»</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS3.p2.4.m4.2.4">âˆ—</mo><mrow id="S2.SS3.p2.4.m4.2.5"><mo stretchy="false" id="S2.SS3.p2.4.m4.2.5.1">(</mo><mn id="S2.SS3.p2.4.m4.2.5.2">1</mn><mo id="S2.SS3.p2.4.m4.2.5.3">âˆ’</mo><mi id="S2.SS3.p2.4.m4.2.5.4">C</mi><mi id="S2.SS3.p2.4.m4.2.5.5">o</mi><mi id="S2.SS3.p2.4.m4.2.5.6">s</mi><mi id="S2.SS3.p2.4.m4.2.5.7">i</mi><mi id="S2.SS3.p2.4.m4.2.5.8">n</mi><mi id="S2.SS3.p2.4.m4.2.5.9">e</mi><mi id="S2.SS3.p2.4.m4.2.5.10">S</mi><mi id="S2.SS3.p2.4.m4.2.5.11">i</mi><mi id="S2.SS3.p2.4.m4.2.5.12">m</mi><mi id="S2.SS3.p2.4.m4.2.5.13">i</mi><mi id="S2.SS3.p2.4.m4.2.5.14">l</mi><mi id="S2.SS3.p2.4.m4.2.5.15">a</mi><mi id="S2.SS3.p2.4.m4.2.5.16">r</mi><mi id="S2.SS3.p2.4.m4.2.5.17">i</mi><mi id="S2.SS3.p2.4.m4.2.5.18">t</mi><mi id="S2.SS3.p2.4.m4.2.5.19">y</mi><mrow id="S2.SS3.p2.4.m4.2.5.20"><mo stretchy="false" id="S2.SS3.p2.4.m4.2.5.20.1">(</mo><mi id="S2.SS3.p2.4.m4.2.5.20.2">u</mi><mrow id="S2.SS3.p2.4.m4.2.5.20.3"><mo stretchy="false" id="S2.SS3.p2.4.m4.2.5.20.3.1">(</mo><mi id="S2.SS3.p2.4.m4.1.1">a</mi><mo stretchy="false" id="S2.SS3.p2.4.m4.2.5.20.3.2">)</mo></mrow><mo id="S2.SS3.p2.4.m4.2.5.20.4">,</mo><mi id="S2.SS3.p2.4.m4.2.5.20.5">u</mi><mrow id="S2.SS3.p2.4.m4.2.5.20.6"><mo stretchy="false" id="S2.SS3.p2.4.m4.2.5.20.6.1">(</mo><mi id="S2.SS3.p2.4.m4.2.2">b</mi><mo stretchy="false" id="S2.SS3.p2.4.m4.2.5.20.6.2">)</mo></mrow><mo stretchy="false" id="S2.SS3.p2.4.m4.2.5.20.7">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S2.SS3.p2.4.m4.2c">\lambda*(1-CosineSimilarity(u(a),u(b))</annotation></semantics></math>. Here <math id="S2.SS3.p2.5.m5.1" class="ltx_Math" alttext="u(a)" display="inline"><semantics id="S2.SS3.p2.5.m5.1a"><mrow id="S2.SS3.p2.5.m5.1.2" xref="S2.SS3.p2.5.m5.1.2.cmml"><mi id="S2.SS3.p2.5.m5.1.2.2" xref="S2.SS3.p2.5.m5.1.2.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p2.5.m5.1.2.1" xref="S2.SS3.p2.5.m5.1.2.1.cmml">â€‹</mo><mrow id="S2.SS3.p2.5.m5.1.2.3.2" xref="S2.SS3.p2.5.m5.1.2.cmml"><mo stretchy="false" id="S2.SS3.p2.5.m5.1.2.3.2.1" xref="S2.SS3.p2.5.m5.1.2.cmml">(</mo><mi id="S2.SS3.p2.5.m5.1.1" xref="S2.SS3.p2.5.m5.1.1.cmml">a</mi><mo stretchy="false" id="S2.SS3.p2.5.m5.1.2.3.2.2" xref="S2.SS3.p2.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.5.m5.1b"><apply id="S2.SS3.p2.5.m5.1.2.cmml" xref="S2.SS3.p2.5.m5.1.2"><times id="S2.SS3.p2.5.m5.1.2.1.cmml" xref="S2.SS3.p2.5.m5.1.2.1"></times><ci id="S2.SS3.p2.5.m5.1.2.2.cmml" xref="S2.SS3.p2.5.m5.1.2.2">ğ‘¢</ci><ci id="S2.SS3.p2.5.m5.1.1.cmml" xref="S2.SS3.p2.5.m5.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.5.m5.1c">u(a)</annotation></semantics></math> and <math id="S2.SS3.p2.6.m6.1" class="ltx_Math" alttext="u(b)" display="inline"><semantics id="S2.SS3.p2.6.m6.1a"><mrow id="S2.SS3.p2.6.m6.1.2" xref="S2.SS3.p2.6.m6.1.2.cmml"><mi id="S2.SS3.p2.6.m6.1.2.2" xref="S2.SS3.p2.6.m6.1.2.2.cmml">u</mi><mo lspace="0em" rspace="0em" id="S2.SS3.p2.6.m6.1.2.1" xref="S2.SS3.p2.6.m6.1.2.1.cmml">â€‹</mo><mrow id="S2.SS3.p2.6.m6.1.2.3.2" xref="S2.SS3.p2.6.m6.1.2.cmml"><mo stretchy="false" id="S2.SS3.p2.6.m6.1.2.3.2.1" xref="S2.SS3.p2.6.m6.1.2.cmml">(</mo><mi id="S2.SS3.p2.6.m6.1.1" xref="S2.SS3.p2.6.m6.1.1.cmml">b</mi><mo stretchy="false" id="S2.SS3.p2.6.m6.1.2.3.2.2" xref="S2.SS3.p2.6.m6.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.6.m6.1b"><apply id="S2.SS3.p2.6.m6.1.2.cmml" xref="S2.SS3.p2.6.m6.1.2"><times id="S2.SS3.p2.6.m6.1.2.1.cmml" xref="S2.SS3.p2.6.m6.1.2.1"></times><ci id="S2.SS3.p2.6.m6.1.2.2.cmml" xref="S2.SS3.p2.6.m6.1.2.2">ğ‘¢</ci><ci id="S2.SS3.p2.6.m6.1.1.cmml" xref="S2.SS3.p2.6.m6.1.1">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.6.m6.1c">u(b)</annotation></semantics></math> are the embeddings of the <span id="S2.SS3.p2.9.1" class="ltx_text ltx_font_typewriter">HOMOGLYPH</span> encoder for the script to which <math id="S2.SS3.p2.7.m7.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S2.SS3.p2.7.m7.1a"><mi id="S2.SS3.p2.7.m7.1.1" xref="S2.SS3.p2.7.m7.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.7.m7.1b"><ci id="S2.SS3.p2.7.m7.1.1.cmml" xref="S2.SS3.p2.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.7.m7.1c">a</annotation></semantics></math> and <math id="S2.SS3.p2.8.m8.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.SS3.p2.8.m8.1a"><mi id="S2.SS3.p2.8.m8.1.1" xref="S2.SS3.p2.8.m8.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.8.m8.1b"><ci id="S2.SS3.p2.8.m8.1.1.cmml" xref="S2.SS3.p2.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.8.m8.1c">b</annotation></semantics></math> belong. <math id="S2.SS3.p2.9.m9.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S2.SS3.p2.9.m9.1a"><mi id="S2.SS3.p2.9.m9.1.1" xref="S2.SS3.p2.9.m9.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.9.m9.1b"><ci id="S2.SS3.p2.9.m9.1.1.cmml" xref="S2.SS3.p2.9.m9.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.9.m9.1c">\lambda</annotation></semantics></math> is a tunable hyperparameter but for simplicity, we fix it as 1 for the results shown in the paper. We also fixed the addition and deletion cost as 1 but in the implementation provided in our package and our GitHub repository, the costs are tunable hyperparameters.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">S-2.4 </span>Ancient Chinese Homoglyphs</h3>

<section id="S2.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.4.1 </span>Data</h4>

<div id="S2.SS4.SSS1.p1" class="ltx_para">
<p id="S2.SS4.SSS1.p1.1" class="ltx_p">The source database <cite class="ltx_cite ltx_citemacro_cite">AcademiaÂ Sinica etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2023</a>)</cite> from which we collect the ancient Chinese character crops contains 5,024 concepts, comprised of 25,984 character renderings. Each of these concepts is mapped to a modern character. This enables us to insert digital renders of these modern characters using the same fonts as above (for traditional Chinese) to create more variation. A "class" in this case comprises a character cluster - with both ancient crops and modern digital renders forming the positive samples for a class.</p>
</div>
<div id="S2.SS4.SSS1.p2" class="ltx_para">
<p id="S2.SS4.SSS1.p2.2" class="ltx_p">We slightly modify the data augmentation scheme for this application to account for the wide variation in writing styles across centuries. We allow for a slight (<math id="S2.SS4.SSS1.p2.1.m1.1" class="ltx_Math" alttext="-10" display="inline"><semantics id="S2.SS4.SSS1.p2.1.m1.1a"><mrow id="S2.SS4.SSS1.p2.1.m1.1.1" xref="S2.SS4.SSS1.p2.1.m1.1.1.cmml"><mo id="S2.SS4.SSS1.p2.1.m1.1.1a" xref="S2.SS4.SSS1.p2.1.m1.1.1.cmml">âˆ’</mo><mn id="S2.SS4.SSS1.p2.1.m1.1.1.2" xref="S2.SS4.SSS1.p2.1.m1.1.1.2.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.1.m1.1b"><apply id="S2.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS4.SSS1.p2.1.m1.1.1"><minus id="S2.SS4.SSS1.p2.1.m1.1.1.1.cmml" xref="S2.SS4.SSS1.p2.1.m1.1.1"></minus><cn type="integer" id="S2.SS4.SSS1.p2.1.m1.1.1.2.cmml" xref="S2.SS4.SSS1.p2.1.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.1.m1.1c">-10</annotation></semantics></math> to <math id="S2.SS4.SSS1.p2.2.m2.1" class="ltx_Math" alttext="+10" display="inline"><semantics id="S2.SS4.SSS1.p2.2.m2.1a"><mrow id="S2.SS4.SSS1.p2.2.m2.1.1" xref="S2.SS4.SSS1.p2.2.m2.1.1.cmml"><mo id="S2.SS4.SSS1.p2.2.m2.1.1a" xref="S2.SS4.SSS1.p2.2.m2.1.1.cmml">+</mo><mn id="S2.SS4.SSS1.p2.2.m2.1.1.2" xref="S2.SS4.SSS1.p2.2.m2.1.1.2.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS1.p2.2.m2.1b"><apply id="S2.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS4.SSS1.p2.2.m2.1.1"><plus id="S2.SS4.SSS1.p2.2.m2.1.1.1.cmml" xref="S2.SS4.SSS1.p2.2.m2.1.1"></plus><cn type="integer" id="S2.SS4.SSS1.p2.2.m2.1.1.2.cmml" xref="S2.SS4.SSS1.p2.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS1.p2.2.m2.1c">+10</annotation></semantics></math> degree) rotation and also add more transformations tailored to this use case - Random Equalize, Random Posterize, Random Solarize, Random Inversion and Random Erase (randomly erase 0-5% of the image). We apply all augmentations to the digital renders but only apply Random Affine transformation and Random Inversion to the ancient crops.</p>
</div>
</section>
<section id="S2.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.4.2 </span>Batching</h4>

<div id="S2.SS4.SSS2.p1" class="ltx_para">
<p id="S2.SS4.SSS2.p1.1" class="ltx_p">We use the same sampling and batching process as we did for the modern homoglyph models.
The only difference is in how the hard-negative sets are defined. Instead of one nearest neighbor per concept, for each ancient crop within a concept cluster, we find <math id="S2.SS4.SSS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS4.SSS2.p1.1.m1.1a"><mi id="S2.SS4.SSS2.p1.1.m1.1.1" xref="S2.SS4.SSS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.SSS2.p1.1.m1.1b"><ci id="S2.SS4.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS4.SSS2.p1.1.m1.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.SSS2.p1.1.m1.1c">k</annotation></semantics></math> nearest neighbors. This gives us as many nearest neighbor sets (hard-negative sets) as ancient crops in our dataset. This allows us to account for the fact that the homoglyphs of a character may differ across different historical periods, spanning millennia.</p>
</div>
</section>
<section id="S2.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.4.3 </span>Model Validation</h4>

<div id="S2.SS4.SSS3.p1" class="ltx_para">
<p id="S2.SS4.SSS3.p1.1" class="ltx_p">We split the character clusters into train and validation sets (90-10). We then transfer modern renders of the characters from the validation set to the train set.
After this, we randomly transfer 50% of validation images to training.
Only ancient characters remain in the validation set. We then make a reference set by embedding all the modern renders of our character (using the reference font Noto Serif CJK Tc). We use top-1 accuracy as our validation metric which is defined as the proportion of correct retrievals of the corresponding modern render to each ancient image in the validation set. During training, the model reached a peak validation accuracy of 50% demonstrating the difficult nature of this task. We use this metric for selecting the best checkpoint for our encoder.</p>
</div>
</section>
<section id="S2.SS4.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.4.4 </span>Other training details</h4>

<div id="S2.SS4.SSS4.p1" class="ltx_para">
<p id="S2.SS4.SSS4.p1.1" class="ltx_p">We again use the AdamW optimizer and Cosine Annealing with Warm Restarts as the learning rate schedule. Relevant Hyperparameters are listed in Table <a href="#S2.T2" title="Table S-2 â€£ S-2.1.2 Batching â€£ S-2.1 Record Linkage â€£ S-2 Application-specific details â€£ Quantifying Character Similarity with Vision Transformers" class="ltx_ref"><span class="ltx_text ltx_ref_tag">S-2</span></a>.
We stop training when validation accuracy stagnates.</p>
</div>
</section>
<section id="S2.SS4.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">S-2.4.5 </span>Creation of Ancient Chinese Homoglyphs</h4>

<div id="S2.SS4.SSS5.p1" class="ltx_para">
<p id="S2.SS4.SSS5.p1.1" class="ltx_p">The creation of homolgyph sets is analogous to the case of modern characters. Instead of using digital renders from a particular font as the "reference set", we look at the five nearest neighbors of ancient characters within a period. We illustrate homoglyphs using The Shang Dynasty period (1600 BC-1045 BC), the most ancient.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abramitzky etÂ al. (2021)</span>
<span class="ltx_bibblock">
Ran Abramitzky, Leah Boustan, Katherine Eriksson, James Feigenbaum, and
Santiago PÃ©rez. 2021.

</span>
<span class="ltx_bibblock">Automated linking of historical data.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Journal of Economic Literature</em>, 59(3):865â€“918.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AcademiaÂ Sinica etÂ al. (2023)</span>
<span class="ltx_bibblock">
Institute ofÂ History AcademiaÂ Sinica, Philology, and Institute of
InformationÂ Science AcademiaÂ Sinica. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://xiaoxue.iis.sinica.edu.tw/yanbian" title="" class="ltx_ref ltx_href">Xiaoxuetang
yanbian database</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acemoglu etÂ al. (2016)</span>
<span class="ltx_bibblock">
Daron Acemoglu, Ufuk Akcigit, and William Kerr. 2016.

</span>
<span class="ltx_bibblock">Networks and the macroeconomy: An empirical exploration.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Nber macroeconomics annual</em>, 30(1):273â€“335.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acemoglu etÂ al. (2012)</span>
<span class="ltx_bibblock">
Daron Acemoglu, VascoÂ M Carvalho, Asuman Ozdaglar, and Alireza Tahbaz-Salehi.
2012.

</span>
<span class="ltx_bibblock">The network origins of aggregate fluctuations.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Econometrica</em>, 80(5):1977â€“2016.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arora etÂ al. (2023)</span>
<span class="ltx_bibblock">
Abhishek Arora, Xinmei Yang, ShaoÂ Yu Jheng, and Melissa Dell. 2023.

</span>
<span class="ltx_bibblock">Linking representations with multimodal contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.03464</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartelme and Gorodnichenko (2015)</span>
<span class="ltx_bibblock">
Dominick Bartelme and Yuriy Gorodnichenko. 2015.

</span>
<span class="ltx_bibblock">Linkages and economic development.

</span>
<span class="ltx_bibblock">Technical report, National Bureau of Economic Research.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Binette and Steorts (2022)</span>
<span class="ltx_bibblock">
Olivier Binette and RebeccaÂ C Steorts. 2022.

</span>
<span class="ltx_bibblock">(almost) all of entity resolution.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Science Advances</em>, 8(12):eabi8021.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlson etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jacob Carlson, Tom Bryan, and Melissa Dell. 2023.

</span>
<span class="ltx_bibblock">Efficient ocr for building a diverse digital history.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.02737</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caron etÂ al. (2021)</span>
<span class="ltx_bibblock">
Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal,
Piotr Bojanowski, and Armand Joulin. 2021.

</span>
<span class="ltx_bibblock">Emerging properties in self-supervised vision transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.14294</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and He (2021)</span>
<span class="ltx_bibblock">
Xinlei Chen and Kaiming He. 2021.

</span>
<span class="ltx_bibblock">Exploring simple siamese representation learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 15750â€“15758.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Xinlei Chen, Saining Xie, and Kaiming He. 2021.

</span>
<span class="ltx_bibblock">An empirical study of training self-supervised vision transformers.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.02057</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du etÂ al. (2022)</span>
<span class="ltx_bibblock">
Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li,
Yuning Du, and Yu-Gang Jiang. 2022.

</span>
<span class="ltx_bibblock">Svtr: Scene text recognition with a single visual model.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.00159</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dunn (1946)</span>
<span class="ltx_bibblock">
HalbertÂ L Dunn. 1946.

</span>
<span class="ltx_bibblock">Record linkage.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">American Journal of Public Health and the Nations Health</em>,
36(12):1412â€“1416.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ellison etÂ al. (2010)</span>
<span class="ltx_bibblock">
Glenn Ellison, EdwardÂ L Glaeser, and WilliamÂ R Kerr. 2010.

</span>
<span class="ltx_bibblock">What causes industry agglomeration? evidence from coagglomeration
patterns.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">American Economic Review</em>, 100(3):1195â€“1213.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geonames (2023)</span>
<span class="ltx_bibblock">
Geonames. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.geonames.org/" title="" class="ltx_ref ltx_href">Geonames</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grill etÂ al. (2020)</span>
<span class="ltx_bibblock">
Jean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec,
PierreÂ H Richemond, Elena Buchatskaya, Carl Doersch, BernardoÂ Avila Pires,
ZhaohanÂ Daniel Guo, MohammadÂ Gheshlaghi Azar, etÂ al. 2020.

</span>
<span class="ltx_bibblock">Bootstrap your own latent: A new approach to self-supervised
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.07733</em>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hirschman (1958)</span>
<span class="ltx_bibblock">
AlbertÂ O Hirschman. 1958.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">The strategy of economic development.</em>

</span>
<span class="ltx_bibblock">Yale Univ. Press, New Haven, Conn.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaro (1989)</span>
<span class="ltx_bibblock">
MatthewÂ A Jaro. 1989.

</span>
<span class="ltx_bibblock">Advances in record-linkage methodology as applied to matching the
1985 census of tampa, florida.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Journal of the American Statistical Association</em>,
84(406):414â€“420.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jinji Koshinjo (1954)</span>
<span class="ltx_bibblock">
Jinji Koshinjo. 1954.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Nihon shokuinrokj</em>.

</span>
<span class="ltx_bibblock">Jinji Koshinjo.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and HervÃ© JÃ©gou. 2019.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with gpus.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Big Data</em>, 7(3):535â€“547.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">kfcd (2015)</span>
<span class="ltx_bibblock">
kfcd. 2015.

</span>
<span class="ltx_bibblock">chaizi.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/kfcd/chaizi" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kfcd/chaizi</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khosla etÂ al. (2020)</span>
<span class="ltx_bibblock">
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, CeÂ Liu, and Dilip Krishnan. 2020.

</span>
<span class="ltx_bibblock">Supervised contrastive learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2004.11362</em>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lane (2022)</span>
<span class="ltx_bibblock">
Nathan Lane. 2022.

</span>
<span class="ltx_bibblock">Manufacturing revolutions: Industrial policy and industrialization in
south korea.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Available at SSRN 3890311</em>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levenshtein etÂ al. (1966)</span>
<span class="ltx_bibblock">
VladimirÂ I Levenshtein etÂ al. 1966.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and
reversals.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Soviet physics doklady</em>, volumeÂ 10, pages 707â€“710. Soviet
Union.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li (2012)</span>
<span class="ltx_bibblock">
Xueqin Li, editor. 2012.

</span>
<span class="ltx_bibblock"><em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Zi Yun (The Origin of Characters)</em>.

</span>
<span class="ltx_bibblock">Tianjin Ancient Works Publishing House, Tianjin.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2016)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2016.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1608.03983" title="" class="ltx_ref ltx_href">SGDR: stochastic gradient
descent with restarts</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">CoRR</em>, abs/1608.03983.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/1711.05101" title="" class="ltx_ref ltx_href">Decoupled weight decay
regularization</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lyu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Lijun Lyu, Maria Koutraki, Martin Krickl, and Besnik Fetahu. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1162/tacl_a_00379" title="" class="ltx_ref ltx_href">Neural ocr post-hoc
correction of historical corpora</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9:479â€“483.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Musgrave etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kevin Musgrave, Serge Belongie, and Ser-Nam Lim. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="http://arxiv.org/abs/2008.09164" title="" class="ltx_ref ltx_href">Pytorch metric learning</a>.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Myrdal and Sitohang (1957)</span>
<span class="ltx_bibblock">
Gunnar Myrdal and Paul Sitohang. 1957.

</span>
<span class="ltx_bibblock">Economic theory and under-developed regions.

</span>
<span class="ltx_bibblock"><em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Regional Studies</em>.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Thi TuyetÂ Hai Nguyen, Adam Jatowt, Mickael Coustaty, and Antoine Doucet. 2021.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/3453476" title="" class="ltx_ref ltx_href">Survey of post-ocr
processing approaches</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">ACM Comput. Surv.</em>, 54(6).

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Novosad (2018)</span>
<span class="ltx_bibblock">
Paul Novosad. 2018.

</span>
<span class="ltx_bibblock">Masala merge.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/paulnov/masala-merge" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/paulnov/masala-merge</a>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Okazaki and Tsujii (2010)</span>
<span class="ltx_bibblock">
Naoaki Okazaki and Junâ€™ichi Tsujii. 2010.

</span>
<span class="ltx_bibblock">Simple and efficient algorithm for approximate dictionary matching.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010)</em>, pages 851â€“859.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord etÂ al. (2018)</span>
<span class="ltx_bibblock">
Aaron vanÂ den Oord, Yazhe Li, and Oriol Vinyals. 2018.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive coding.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.03748</em>.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etÂ al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
8748â€“8763. PMLR.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rasmussen (1956)</span>
<span class="ltx_bibblock">
PoulÂ NÃ¸rregaard Rasmussen. 1956.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Studies in inter-sectoral relations</em>, volumeÂ 15.

</span>
<span class="ltx_bibblock">E. Harck.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sadinle (2014)</span>
<span class="ltx_bibblock">
Mauricio Sadinle. 2014.

</span>
<span class="ltx_bibblock">Detecting duplicates in a homicide registry using a bayesian
partitioning approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">The Annals of Applied Statistics</em>, 8(4):2404â€“2434.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sadinle (2017)</span>
<span class="ltx_bibblock">
Mauricio Sadinle. 2017.

</span>
<span class="ltx_bibblock">Bayesian estimation of bipartite matchings for record linkage.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Journal of the American Statistical Association</em>,
112(518):600â€“612.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin CharlesÂ Germain Lee, Jacob
Carlson, and Weining Li. 2021.

</span>
<span class="ltx_bibblock">Layoutparser: A unified toolkit for deep learning based document
image analysis.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">International Conference on Document Analysis and Recognition</em>,
12821.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2016)</span>
<span class="ltx_bibblock">
Baoguang Shi, Xiang Bai, and Cong Yao. 2016.

</span>
<span class="ltx_bibblock">An end-to-end trainable neural network for image-based sequence
recognition and its application to scene text recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">IEEE transactions on pattern analysis and machine
intelligence</em>, 39(11):2298â€“2304.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silbert (1970)</span>
<span class="ltx_bibblock">
JeffreyÂ M Silbert. 1970.

</span>
<span class="ltx_bibblock">The worldâ€™s first computerized criminal-justice information-sharing
system-the new york state identification and intelligence system (nysiis).

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Criminology</em>, 8:107.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Simpson etÂ al. (2013)</span>
<span class="ltx_bibblock">
IanÂ C Simpson, Petroula Mousikou, JuanÂ Manuel Montoya, and Sylvia Defior. 2013.

</span>
<span class="ltx_bibblock">A letter visual-similarity matrix for latin-based alphabets.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Behavior research methods</em>, 45:431â€“439.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taiwan Ministry of Economic Affairs (2023)</span>
<span class="ltx_bibblock">
Taiwan Ministry of Economic Affairs. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://data.gov.tw/dataset/6569" title="" class="ltx_ref ltx_href">Registered factory
directory</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teikoku Koshinjo (1957)</span>
<span class="ltx_bibblock">
Teikoku Koshinjo. 1957.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">Teikoku Ginko Kaisha Yoroku</em>.

</span>
<span class="ltx_bibblock">Teikoku Koshinjo.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">TorchVision (2016)</span>
<span class="ltx_bibblock">
TorchVision. 2016.

</span>
<span class="ltx_bibblock">Torchvision: Pytorchâ€™s computer vision library.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/pytorch/vision" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pytorch/vision</a>.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">van Strien. etÂ al. (2020)</span>
<span class="ltx_bibblock">
Daniel van Strien., Kaspar Beelen., MarionaÂ Coll Ardanuy., Kasra Hosseini.,
Barbara McGillivray., and Giovanni Colavizza. 2020.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5220/0009169004840496" title="" class="ltx_ref ltx_href">Assessing the
impact of ocr quality on downstream nlp tasks</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 12th International Conference on Agents
and Artificial Intelligence - Volume 1: ARTIDIGH,</em>, pages 484â€“496. INSTICC,
SciTePress.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ventura etÂ al. (2015)</span>
<span class="ltx_bibblock">
SamuelÂ L Ventura, Rebecca Nugent, and EricaÂ RH Fuchs. 2015.

</span>
<span class="ltx_bibblock">Seeing the non-stars:(some) sources of bias in past disambiguation
approaches and a new public tool leveraging labeled records.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Research Policy</em>, 44(9):1672â€“1701.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wagner and Fischer (1974)</span>
<span class="ltx_bibblock">
RobertÂ A. Wagner and MichaelÂ J. Fischer. 1974.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1145/321796.321811" title="" class="ltx_ref ltx_href">The string-to-string
correction problem</a>.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">J. ACM</em>, 21(1):168â€“173.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang (2003)</span>
<span class="ltx_bibblock">
Guowei Wang. 2003.

</span>
<span class="ltx_bibblock">Shishi (deciphering "history").

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Guantan Gjilin</em>. Hebei Education Publishing House,
Shijiazhuang.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wightman (2019)</span>
<span class="ltx_bibblock">
Ross Wightman. 2019.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.5281/zenodo.4414861" title="" class="ltx_ref ltx_href">Pytorch image
models</a>.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/rwightman/pytorch-image-models" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/rwightman/pytorch-image-models</a>.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winkler (1990)</span>
<span class="ltx_bibblock">
WilliamÂ E Winkler. 1990.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">String comparator metrics and enhanced decision rules in the
Fellegi-Sunter model of record linkage.</em>

</span>
<span class="ltx_bibblock">ERIC.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xinmei Yang, Abhishek Arora, Shao-Yu Jheng, and Melissa Dell. 2023.

</span>
<span class="ltx_bibblock">Codebase for quantifying character similarity with vision
transformers.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/dell-research-harvard/Quantifying-Character-Similarity" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/dell-research-harvard/Quantifying-Character-Similarity</a>.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">znwang25 (2020)</span>
<span class="ltx_bibblock">
znwang25. 2020.

</span>
<span class="ltx_bibblock">fuzzychinese.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/znwang25/fuzzychinese" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/znwang25/fuzzychinese</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.14671" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.14672" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.14672">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.14672" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.14673" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 05:50:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
