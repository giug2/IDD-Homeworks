<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs</title>
<!--Generated on Tue Sep 24 21:39:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.10279v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S1" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S2" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S2.SS1" title="In 2 Background and Related Work ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S2.SS2" title="In 2 Background and Related Work ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Related Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S3" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Research Questions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiment Design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.SS1" title="In 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Prompt Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.SS2" title="In 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Code Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.SS3" title="In 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Detection Methodology and Heuristics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Evaluation Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.SS1" title="In 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Prevalence of Package Hallucinations (RQ1)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.SS2" title="In 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Impact of Model Settings (RQ2)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.SS3" title="In 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Model Behaviors (RQ3)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.SS4" title="In 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Characteristics of Hallucinations (RQ4)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S6" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Mitigation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S6.SS1" title="In 6 Mitigation ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Mitigation Strategies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S6.SS2" title="In 6 Mitigation ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Mitigation Implementation and Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S7" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion and Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A1" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Truncated List of LLM Generated Coding Prompts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A2" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>System Messages and Prompts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A3" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Model Parameters and Testing Environment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A4" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Python vs. JavaScript Hallucination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A5" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Complete Results for Python and JavaScript</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A6" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Additional Background Information</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A6.SS1" title="In Appendix F Additional Background Information ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.1 </span>Large Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A6.SS2" title="In Appendix F Additional Background Information ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.2 </span>Hallucinations</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A7" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Packages and Modules Explanation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A8" title="In We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Stack Overflow Tags</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A8.SS1" title="In Appendix H Stack Overflow Tags ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H.1 </span>Python Tags</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A8.SS2" title="In Appendix H Stack Overflow Tags ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H.2 </span>JavaScript Tags</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Joseph Spracklen
<br class="ltx_break"/>University of Texas at San Antonio
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raveen Wijewickrama
<br class="ltx_break"/>University of Texas at San Antonio
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">A H M Nazmus Sakib
<br class="ltx_break"/>University of Texas at San Antonio
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anindya Maiti
<br class="ltx_break"/>University of Oklahoma
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bimal Viswanath
<br class="ltx_break"/>Virginia Tech
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Murtuza Jadliwala
<br class="ltx_break"/>University of Texas at San Antonio
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The reliance of popular programming languages such as Python and JavaScript on centralized package repositories and open-source software, combined with the emergence of code-generating Large Language Models (LLMs), has created a new type of threat to the software supply chain: <em class="ltx_emph ltx_font_italic" id="id1.id1.1">package hallucinations</em>. These hallucinations, which arise from fact-conflicting errors when generating code using LLMs, represent a novel form of package confusion attack that poses a critical threat to the integrity of the software supply chain. This paper conducts a rigorous and comprehensive evaluation of package hallucinations across different programming languages, settings, and parameters, exploring how a diverse set of models and configurations affect the likelihood of generating erroneous package recommendations and identifying the root causes of this phenomenon. Using 16 popular LLMs for code generation and two unique prompt datasets, we generate 576,000 code samples in two programming languages that we analyze for package hallucinations. Our findings reveal that that the average percentage of hallucinated packages is at least 5.2% for commercial models and 21.7% for open-source models, including a staggering 205,474 unique examples of hallucinated package names, further underscoring the severity and pervasiveness of this threat.
To overcome this problem, we implement several hallucination mitigation strategies and show that they are able to significantly reduce the number of package hallucinations while maintaining code quality.
Our experiments and findings highlight package hallucinations as a persistent and systemic phenomenon while using state-of-the-art LLMs for code generation, and a significant challenge which deserves the research community’s urgent attention.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Modern generative AI models, pre-trained on extensive multi-modal datasets, can create synthetic images (e.g., Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib65" title="">65</a>]</cite>, DALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib64" title="">64</a>]</cite>, MidJourney <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib52" title="">52</a>]</cite>), videos (e.g., OpenAI Sora <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib60" title="">60</a>]</cite>, Microsoft VASA-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib81" title="">81</a>]</cite>), audio (e.g., AudioGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib31" title="">31</a>]</cite>) and text/conversations (e.g., ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib5" title="">5</a>]</cite>, BlenderBot <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib67" title="">67</a>]</cite>). These models have enabled a range of new applications such as creative content generation, personalized virtual assistants, and sophisticated chatbots.
Foundational Large Language Models (LLMs), including GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib1" title="">1</a>]</cite> and LlaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib74" title="">74</a>]</cite>, are trained on enormous and diverse textual datasets, and typically excel in a variety of natural language tasks, such as answering questions, summarizing documents, and translating languages.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">An emerging application of these LLMs is to generate computer code, which is typically accomplished by first training or fine-tuning them using vast amounts of programming/code datasets found on online repositories (e.g., GitHub), technical forums, and documentation. Both commercial/black-box (e.g., GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib1" title="">1</a>]</cite>, Claude <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib3" title="">3</a>]</cite>) and open-source (e.g., CodeLlama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib66" title="">66</a>]</cite>, DeepSeek Coder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib17" title="">17</a>]</cite>) varieties of such code-generating LLMs are readily available and being extensively used by both novice and expert programmers
in their coding workflows to increase productivity.
Recent studies indicate that up to 97% of developers are using generative AI to some degree and that approximately 30% of code written today is AI-generated, reflecting significant perceived gains in efficiency and convenience <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib41" title="">41</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">One serious shortcoming of LLMs is a phenomenon referred to as <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">hallucination</em>. Hallucinations are outputs produced by LLMs that are factually incorrect, nonsensical, or completely unrelated to the input task. Hallucinations present a critical obstacle to the effective and safe deployment of LLMs in public-facing applications due to their potential to generate inaccurate or misleading information. As a result, there has been increased efforts to research the detection and mitigation of hallucinations in LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib26" title="">26</a>]</cite>.
However, most existing research has focused only on hallucinations in classical natural language generation and prediction tasks such as machine translation, summarization, and conversational AI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib9" title="">9</a>]</cite>. The occurrence and impact of hallucinations during code generation, particularly regarding the type of hallucinated content and its implications for code security, are still in the nascent stages of research. Recently, Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib44" title="">44</a>]</cite> have shown that popular LLMs (e.g., ChatGPT, CodeRL, and CodeGen) significantly hallucinate during code generation and have established a taxonomy of hallucinations in LLM-generated code.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we focus on a specific type of hallucination during code generation called <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">package hallucination</em>.
<span class="ltx_text ltx_font_bold" id="S1.p4.1.2">Package hallucination occurs when an LLM generates code that recommends or contains a reference to a package that does not actually exist.</span> An adversary can exploit package hallucinations, especially if they are repeated, by publishing a package to an open-source repository with the same name as the hallucinated or fictitious package and containing some malicious code/functionality.
Then, as other unsuspecting and trusting LLM users are subsequently recommended the same fictitious package in their generated code, they end up downloading the adversary-created malicious package, resulting in a successful compromise.
This compromise can then spread through an entire codebase or software dependency chain, infecting any code that relies on the malicious package. This is a variation of the classical <em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">package confusion attack</em> that has been enabled by code-generating LLMs.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Package confusion attacks, through techniques such as typosquatting (i.e., creating packages with names similar to popular ones to deceive users) and name similarity, have been a longstanding issue in the open-source software community <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib32" title="">32</a>]</cite>. Package hallucinations by code-generating LLMs threaten to exacerbate the problem by exposing an additional threat surface for such attacks. Trivial cross-referencing methods (i.e., comparing a generated package name with a list of known packages) are ineffective for detecting a package hallucination attack, as an adversary may have already published the hallucinated package with malicious code. Open source repositories make no guarantee about the safety of hosted content; the mere presence of a package in an open-source repository does not confirm its credibility.
A recent blog post <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib34" title="">34</a>]</cite> suggests that LLMs are prone to package hallucinations and provides a first approximation of its prevalence, but the extent to which this phenomenon occurs in state-of-the-art (SOTA) commercial and open-source LLMs, the nature of these hallucinations, and the effectiveness of potential mitigation measures have not been comprehensively investigated before.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In this paper we conduct the first systematic study of the frequency and nature of package hallucinations across a variety of code generation LLMs operating under a diverse set of model settings and parameters.
We specifically make the following novel contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Quantifying the frequency and prevalence of package hallucinations by code generating LLMs</span>:
We first comprehensively analyze the prevalence of package hallucinations in Python and JavaScript code generated by popular publicly-available commercial and open-source LLMs.
We also examine the common behaviors in LLMs that lead to package hallucinations, including hallucination repetition, output verbosity, and the ability of models to detect their own hallucinations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Analyzing the effect of model settings and training data on package hallucinations:</span>
We further study how specific model settings, such as training data recency, model temperature, and decoding strategies affect the occurrence and nature of package hallucinations.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Characterizing the common traits of the generated hallucinated packages</span>: We carefully study several key properties of the hallucinated packages, such as their semantic similarity to popular packages, the occurrence of package recommendations from other programming languages, package persistence, and the influence of recently removed packages on hallucination rate.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i4.p1.1.1">Mitigation Strategies</span>: We propose and comprehensively evaluate several techniques to effectively mitigate package hallucinations in LLM-generated code while maintaining the ability to produce effective code, something that has not been done before.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i5.p1.1.1">Data</span> We make publicly available a novel dataset of 19,500 coding prompts for a vast range of coding tasks, 586,000 generated coding samples for fine-tuning/analysis, and a list of 205,474 hallucinated package names that can be used to monitor for potential instances of a package hallucination attack.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section we provide a brief background on open-source software security, code generating LLMs, and the issue of hallucinations in LLMs.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p1.1.1">Package Confusion Attacks in Open-source Software Repositories.</span>
Modern software development has seen an increased reliance on open-source software packages and libraries available on public centralized repositories. Many modern programming languages now rely on such centralized package repositories, with PyPI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib12" title="">12</a>]</cite> (for Python) and npm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib56" title="">56</a>]</cite> (for JavaScript) being the two most popular repositories. The open nature of these repositories, where anyone can upload new code packages/libraries, makes them an attractive platform for malware distribution. For instance, a total of 245,000 malicious code packages were discovered on open-source software repositories in 2023 alone <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib68" title="">68</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Once a malicious package is uploaded, adversaries employ various techniques to trick users into downloading it, thereby integrating it into their codebases and dependency chains. These attacks often involve deliberately naming malicious packages to mimic legitimate ones, a tactic known as a <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">package confusion attack</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib55" title="">55</a>]</cite>. Package confusion attacks can be broadly categorized into <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.2">typosquatting</em>, <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.3">combosquatting</em>, <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.4">brandjacking</em>, and <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.5">similarity</em> attacks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib33" title="">33</a>]</cite>, and are distinct from other types of software supply chain attacks such as corrupting legitimate packages or developing unique malicious packages from scratch as part of a long-term campaign. More than 1,200 package confusion attacks have been documented in the last six years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib55" title="">55</a>]</cite>, including the notable PyTorch compromise <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib69" title="">69</a>]</cite> and the Lazarus Group campaign <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib32" title="">32</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Packages/libraries often rely on other packages to function, thus creating extensive <em class="ltx_emph ltx_font_italic" id="S2.SS1.p3.1.1">dependency trees</em>,
infecting a single package in this dependency chain can be sufficient to compromise an entire software product or ecosystem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib30" title="">30</a>]</cite>.
Public OSS repositories such as PyPI and npm have implemented various measures, including two-factor authentication, namespace protection, and software signing to mitigate the distribution of malicious packages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib82" title="">82</a>]</cite>. However, it remains unclear whether these repositories utilize any scan-based techniques for detecting malicious code, and they often do not disclose the full list of removed packages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p4.1.1">Automated Code Generation using LLMs.</span>
Modern LLMs are continuing to demonstrate advanced source code generation capabilities, with success rates for correctly answering coding prompts surging from 25% in June 2021 to 96% by April 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib8" title="">8</a>]</cite>. With the increasing use of these models for software development, concerns are rising about the likelihood of producing insecure or incorrect code that could create vulnerabilities in deployed applications. Early versions of code generation LLMs were found to generate code containing vulnerabilities listed in the MITRE Top-25 Common Weakness Enumeration (CWE) 40% of the time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib61" title="">61</a>]</cite>. Moreover, recent research has shown that AI-assisted programming not only results in less secure code but also instills a false sense of security among developers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib62" title="">62</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p5.1.1">Hallucinations by LLMs.</span>
It has been well-documented that LLMs can unintentionally produce harmful information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib79" title="">79</a>]</cite>, be manipulated for malicious purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib29" title="">29</a>]</cite>, expose private information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib39" title="">39</a>]</cite>, and carry inherent biases in their training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib14" title="">14</a>]</cite>. A related phenomenon is hallucinations, where LLMs generate misleading or entirely fictitious information. These errors take various forms: the model might misinterpret the intended input (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.2">input-conflicting hallucination</em>), produce inconsistencies with prior outputs (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.3">context-conflicting hallucination</em>), or contradict established facts (<em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.4">fact-conflicting hallucination</em>) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib83" title="">83</a>]</cite>.
Hallucinations can arise from three main root causes: (i) <em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.5">data</em>, (ii) <em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.6">training</em>, and (iii) <em class="ltx_emph ltx_font_italic" id="S2.SS1.p5.1.7">inference</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib21" title="">21</a>]</cite>. Data-related hallucinations occur when the source data itself is flawed with misinformation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib42" title="">42</a>]</cite>, bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib14" title="">14</a>]</cite>, or incomplete records <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib58" title="">58</a>]</cite>. Architecture flaws <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib43" title="">43</a>]</cite> or suboptimal training objectives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib77" title="">77</a>]</cite> during training could also result in downstream hallucinations, while inference time issues such as defective coding strategies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib19" title="">19</a>]</cite> and imperfect decoding representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib47" title="">47</a>]</cite> are further contributors. The probabilistic nature of LLMs presents a challenge in mitigating hallucinations. This nondeterminism, while it fosters creativity and generates diverse and innovative content, also contributes to the generation of hallucinations. Balancing creativity with accuracy remains a central challenge in deploying LLMs, underscoring the complexity of developing effective mitigation strategies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p6.1.1">Package Hallucinations and Security Risks.</span>
Package hallucinations, a special form of fact-conflicting hallucinations, are instances where LLMs generate fictitious (non-existent) or erroneous package names in the generated source code. As outlined earlier, an adversary can quickly create malicious packages (on the appropriate open-source repository) with the same name as these hallucinated packages, thus effecting a very simple, yet effective, form of package confusion attack. Unsuspecting users, who trust the LLM output, may not scrutinize the validity of these hallucinated packages in the generated code and could inadvertently include these malicious packages in their codebase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib62" title="">62</a>]</cite>. This resulting insecure open-source code also has the potential of being included in the dependency chain of other packages and code, leading to a cascading effect where vulnerabilities are propagated across numerous codebases. The simplicity and scale of such LLM-enabled package confusion attacks highlight the critical need for quantifying this existing risk, understanding the nature of this unique type of hallucination, and developing effective mitigation techniques that maintain the utility of the code generated by the LLMs. This is precisely what we aim to accomplish in this work.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="316" id="S2.F1.g1" src="x1.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Exploiting Package Hallucination.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Related Work</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The idea of code generation models recommending malicious or typosquatted packages was first suggested in 2021 as tools such as GPT-3 and Codex were released as viable code generation platforms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib5" title="">5</a>]</cite>. At the time, the risk of code generation tools suggesting vulnerable, malicious, or typosquatted packages was assessed to be low <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib8" title="">8</a>]</cite>.
In particular, the related but distinct concept of package hallucinations was not explicitly considered in this initial risk assessment; either because such an attack had not been considered or because the threat was thought to be negligible. The capabilities of generative AI agents have advanced significantly since that introductory evaluation.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Although a comprehensive study on the prevalence of package hallucinations in LLM-generated code has not been done previously, a recent blog post by Lanyado <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib35" title="">35</a>]</cite> outlines the results of some preliminary tests on commercial LLMs such as GPT, Gemini, and Cohere. Their testing does confirm the presence of hallucinated packages in the code generated by these LLMs but their initial estimate of hallucination rate is 5x higher than our findings and they do not consider popular open-source LLMs or study possible mitigation approaches. They also fail to systematically characterize the generated hallucinated packages and model properties that have an impact on hallucinations.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">In contrast, we conduct a rigorous and comprehensive evaluation across a broader range of models, including the first analysis of package hallucinations in open-source models of any kind, at a scale that has not been done previously.
To this end, we provide thorough testing with a larger custom dataset covering two programming languages (namely, Python and Javascript), followed by a detailed analysis of the significant characteristics of this phenomenon.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Research Questions</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Adversary Model and Assumptions.</span>
We assume an adversary that wants to execute a package confusion attack by leveraging package hallucinations in code generated by both closed-source and open-source code generation LLMs (see <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S2.F1" title="In 2.1 Background ‣ 2 Background and Related Work ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>). The target of the adversary are users who employ such LLMs for generating code. Here we assume that the LLM generates code that requires additional packages for the user to install, which the user does without sufficient verification of the recommended packages. In other words, the target users fully trust the LLMs to include only valid package names in the generated code.
We assume that the adversary has access to the same set of LLMs for code generation as the target users, and is unable to modify or manipulate the model and model parameters of these LLMs (e.g., via re-training or fine-tuning) before they are used by the victims.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The adversary is able to determine a list of hallucinated packages generated by these LLMs (for example, by cross-referencing the package repository), and then is able to realize a package confusion attack by creating a package of the same name on the corresponding package repositories.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">These newly created (and now publicly available) packages by the adversary could contain malicious code or functionality. Research has shown that installing open source Python or JavaScript packages allows arbitrary code execution to an attacker <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib57" title="">57</a>]</cite>. Previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib35" title="">35</a>]</cite> has also established the viability of such an attack by publishing a hallucinated package to an open source repository and demonstrating that the package is actively downloaded and was incorporated into the dependency chains of other packages/code.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">Research Questions.</span> We now organize our investigation
into five broad Research Questions (RQ).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">RQ1: <em class="ltx_emph ltx_font_italic" id="S3.p5.1.1.1">How prevalent are package hallucinations while generating Python and JavaScript code using LLMs?</em></span>
Our aim here is to thoroughly examine how often package hallucinations occur with both widely-used commercial and open-source LLMs when they generate Python and JavaScript code across various programming tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p6">
<p class="ltx_p" id="S3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.p6.1.1">RQ2: <em class="ltx_emph ltx_font_italic" id="S3.p6.1.1.1">How are package hallucinations impacted by select model settings?</em></span>
Here our goal is to comprehensively analyze how training data and decoding strategies impact the package hallucinations produced by these code generation LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p7">
<p class="ltx_p" id="S3.p7.1"><span class="ltx_text ltx_font_bold" id="S3.p7.1.1">RQ3: <em class="ltx_emph ltx_font_italic" id="S3.p7.1.1.1">What are the commonly observed model behaviors related to package hallucinations?</em></span>
This RQ will exhaustively study model behaviors such as hallucination repetition by a single LLM (hallucination persistence) and across multiple LLMs (cross-model hallucinations), output verbosity, and the ability of LLMs to detect their own hallucinations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p8">
<p class="ltx_p" id="S3.p8.1"><span class="ltx_text ltx_font_bold" id="S3.p8.1.1">RQ4: <em class="ltx_emph ltx_font_italic" id="S3.p8.1.1.1">What are some of the defining properties/attributes of the observed package hallucinations?</em></span>
The goal of this RQ is to analyze the properties of the hallucinated packages
such as semantic similarity between hallucinated and popular packages, number of cross-language hallucinations (i.e. non-existent packages from the language requested but valid packages in another programming language), and the number of generated packages that were recently removed from the source repositories.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p9">
<p class="ltx_p" id="S3.p9.1"><span class="ltx_text ltx_font_bold" id="S3.p9.1.1">RQ5: <em class="ltx_emph ltx_font_italic" id="S3.p9.1.1.1">Is it possible to effectively mitigate package hallucinations using best practices in the literature and knowledge gained from earlier results?</em></span>
Through this RQ, we will investigate if code generating LLMs can be designed to reduce hallucinations with minimal compromise to code quality. In this direction, we will study if techniques such as retrieval augmented generation (RAG), self-detected feedback, decoding strategies, and supervised fine-tuning are effective hallucination reduction strategies.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment Design</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To address the RQs outlined above,
we design several experiments to repeatedly prompt LLMs to generate code and then analyze the generated code.
Our experimentation pipeline comprises of three distinct phases: (i) <em class="ltx_emph ltx_font_italic" id="S4.p1.1.1">prompt dataset generation</em>, (ii) <em class="ltx_emph ltx_font_italic" id="S4.p1.1.2">code generation</em>, and (iii) <em class="ltx_emph ltx_font_italic" id="S4.p1.1.3">hallucination detection</em>, each of which is described below:</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Prompt Dataset</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The experiments are designed to exhaustively test each LLM through a complete range of coding tasks.
Existing datasets of benchmark coding prompts contain only a limited number of prompts (e.g. only 164 prompts for both EvalPlus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib45" title="">45</a>]</cite> and HumanEval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib8" title="">8</a>]</cite>) and lack diversity. Therefore, we create a new code prompt dataset for our experiment that contains both breadth and depth in terms of overall number of prompts and range of topics. Our goal was to develop a dataset that accurately and comprehensively represents coding tasks commonly requested by everyday users. To accomplish this, we employed two distinct approaches, as described below.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Stack Overflow Dataset.</span>
To model the input prompts around real programmer questions, our first prompt dataset was created using Stack Overflow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib70" title="">70</a>]</cite> questions across relevant programming topics and subject areas. Stack Overflow is a popular online question-and-answer service for software programmers and developers. To capture a wide range of topics, we utilized the “tag” feature of Stack Overflow, which allows users to label posts according to a subject matter. We included any tag that had over 5,000 questions and was also relevant to Python or JavaScript (the two programming languages we focus in this work, as detailed in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.SS2" title="4.2 Code Generation ‣ 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.2</span></a>). For each of the 240 manually selected tags that met this criteria (full list of tags is listed in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A8" title="Appendix H Stack Overflow Tags ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">H</span></a>, we extracted the 20 most up-voted questions, resulting in 4,800 prompts (i.e., 4,800 prompts for Python and 4,800 prompts for JavaScript).</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">As more recent data is less likely to be included in the pre-training data of LLMs,
we are also interested in investigating the correlation between data recency (i.e., how recently the question was asked on Stack Overflow) and model hallucination rate.
To enable such an analysis, we ran two queries on Stack Overflow; one that only captured the most popular questions in the selected tags from 2023 and another that captured the most popular questions for all years prior to 2023. By including the two different ranges of time, we effectively doubled the original number of prompts, for a total of 9,600 for each of the two languages.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Not all questions asked on Stack Overflow may involve coding or require code to answer the question. Rather than attempting to filter out such prompts during the code generation phase, which is non-trivial and error-prone, the LLM is asked to answer the question and only provide code if necessary.
In the end, this may result in a slightly lesser number of usable LLM-generated code samples, but is more realistic as LLMs are expected to accommodate imperfect user inputs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">LLM-generated Dataset.</span> As a majority of the programming tasks require some library/package, our next idea was to use the package repositories themselves as a good representation of the full spectrum of coding topics. Our goal was to represent as many code generation tasks as possible in one comprehensive dataset. We take the 5,000 most popular Python and JavaScript packages (according to number of downloads) and scrape the official package description as listed on PyPI and npm, respectively. These descriptions are then input individually to the Llama-2 70B model with instructions to generate a coding prompt based on the package description (exact prompt available in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A2.F13" title="In Appendix B System Messages and Prompts ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">13</span></a>). This process worked remarkably well and generated roughly 4,800 prompts for Python and JavaScript each, resulting in two datasets of approximately the same size (some packages with no description or descriptions in a non-English language were discarded).
Similar to the Stack Overflow dataset, we doubled the LLM-generated dataset for temporal analysis by dividing it into two segments: packages most downloaded in 2023 and packages most downloaded prior to 2023. When a package appears in both sets, we remove the package from the latter set to ensure no overlap. Removing duplicates from the latter dataset guarantees that the remaining packages will be those that have increased in popularity during the last year, capturing the desired signal. A truncated list from the LLM-generated dataset can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A1" title="Appendix A Truncated List of LLM Generated Coding Prompts ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Code Generation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Model Selection.</span>
For our experiments, we chose the top-ranked models from the EvalPlus leaderboard (as of January 2024) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib45" title="">45</a>]</cite>. While creating our list, we disregarded the fine-tuned versions that were ranked below their corresponding foundational models, and only selected one fine-tuned version of the same foundational model of the same parameter size <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib45" title="">45</a>]</cite>. EvalPlus maintains a ranking of the top performing LLMs for code correctness according to a rigorous code synthesis evaluation framework.
Our goal was to include a mix of top performing base models and a few of the best performing fine-tuned variants. We also included the GPT series of models (GPT-3.5, GPT-4, and GPT-4 Turbo) in our experiments, which currently hold the top rankings on the leaderboard. GPT models are widely considered as SOTA in terms of code generation models at the time of writing and add value to our experiments as representative commercial models. The models were not modified or altered in any way prior to testing; they are strictly “off-the-shelf.” <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.T1" title="In 4.2 Code Generation ‣ 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> provides a complete list of the models that we tested in our experiments.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_block ltx_pruned_first" id="S4.T1.2">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_block">Table 1: </span>Details of the models that were evaluated.</figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S4.T1.2.p2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T1.2.p2.1" style="width:480.0pt;height:257.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S4.T1.2.p2.1.1"><span class="ltx_text" id="S4.T1.2.p2.1.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.2.p2.1.1.1.1">
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.p2.1.1.1.1.1.1" style="padding-bottom:1.93748pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.p2.1.1.1.1.1.1.1">Model</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.p2.1.1.1.1.1.2" style="padding-bottom:1.93748pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.p2.1.1.1.1.1.2.1">Parameters</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.p2.1.1.1.1.1.3" style="padding-bottom:1.93748pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.2.p2.1.1.1.1.1.3.1">License</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.2.p2.1.1.1.1.1.4" style="padding-bottom:1.93748pt;"><span class="ltx_text" id="S4.T1.2.p2.1.1.1.1.1.4.1"></span> <span class="ltx_text" id="S4.T1.2.p2.1.1.1.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T1.2.p2.1.1.1.1.1.4.2.1">
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.p2.1.1.1.1.1.4.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.p2.1.1.1.1.1.4.2.1.1.1.1">Open</span></span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.1.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T1.2.p2.1.1.1.1.1.4.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S4.T1.2.p2.1.1.1.1.1.4.2.1.2.1.1">Source</span></span></span>
</span></span><span class="ltx_text" id="S4.T1.2.p2.1.1.1.1.1.4.3"></span></span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.2.p2.1.1.1.1.2.1">ChatGPT 4.0 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib1" title="">1</a>]</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.p2.1.1.1.1.2.2">Unknown</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.p2.1.1.1.1.2.3">Commercial</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.p2.1.1.1.1.2.4">✗</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.3">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.3.1">ChatGPT 4.0 Turbo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib1" title="">1</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.3.2">Unknown</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.3.3">Commercial</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.3.4">✗</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.4">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.4.1">ChatGPT 3.5 Turbo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib5" title="">5</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.4.2">Unknown</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.4.3">Commercial</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.4.4">✗</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.5">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.5.1">CodeLlama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib66" title="">66</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.5.2">7B, 13B, 34B</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.5.3">Free</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.5.4">✗</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.6">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.6.1">DeepSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib17" title="">17</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.6.2">1.3B, 6.7B, 33B</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.6.3">Free</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.6.4">✓</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.7">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.7.1">Magicoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib80" title="">80</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.7.2">6.7B</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.7.3">Free</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.7.4">✓</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.8">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.8.1">WizardCoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib49" title="">49</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.8.2">34B</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.8.3">Free</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.8.4">✓</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.9">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.9.1">Mistral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib27" title="">27</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.9.2">7B</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.9.3">Free</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.9.4">✓</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.10">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.10.1">Mixtral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib28" title="">28</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.10.2">8x7B</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.10.3">Free</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.10.4">✓</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.11">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.11.1">OpenChat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib78" title="">78</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.11.2">7B</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.11.3">Free</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.11.4">✓</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.12">
<span class="ltx_td ltx_align_left" id="S4.T1.2.p2.1.1.1.1.12.1">WizardCoder-Python <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib49" title="">49</a>]</cite></span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.12.2">7B</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.12.3">Free</span>
<span class="ltx_td ltx_align_center" id="S4.T1.2.p2.1.1.1.1.12.4">✓</span></span>
<span class="ltx_tr" id="S4.T1.2.p2.1.1.1.1.13">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S4.T1.2.p2.1.1.1.1.13.1">CodeLlama-Python <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib66" title="">66</a>]</cite></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.p2.1.1.1.1.13.2">33B</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.p2.1.1.1.1.13.3">Free</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.p2.1.1.1.1.13.4">✓</span></span>
</span></span></p>
</span></div>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Language Selection.</span> In our experiments, we focus on two of the most popular programming languages, JavaScript and Python. These languages were chosen for their overall popularity (#1 and #2 according to GitHub’s 2023 Octoverse report <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib15" title="">15</a>]</cite>) and their reliance on open-source repositories for package management. Other popular programming languages like Java, C, or C++ do not rely on a centralized, open-source repository as Python and JavaScript do, which is a key component of this vulnerability.
The open-source package repositories for these languages, npm and PyPI, represent ecosystems of 5.1 million and 573 thousand packages, respectively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib72" title="">72</a>]</cite>. Of the 16 total models tested (see <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.T1" title="In 4.2 Code Generation ‣ 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>), fourteen were tested for both Python and JavaScript, while two fine-tuned Python-specific models, WizardCoder-Python and CodeLlama-Python, were only tested for Python.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Testing Environment.</span> To create a fair and uniform testing environment, we utilize the <em class="ltx_emph ltx_font_italic" id="S4.SS2.p3.1.2">text-generation-webui</em> tool for LLMs, which supports most open-source LLMs and offers both a GUI and API for local and offline use <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib59" title="">59</a>]</cite>. It allows precise parameter control, ensuring consistent model evaluation during testing. We employed the GPTQ quantization method on these models, which reduces parameter precision to boost inference speed and lower memory use without significantly impacting performance. The GPTQ quantization method, specifically, uses a one-shot weight quantization method based on approximate second-order information that has a negligible effect on the accuracy of models, making it an ideal choice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib13" title="">13</a>]</cite>. Additionally, quantized models better simulate the performance that a typical user can expect when running models on commercial grade hardware, making them more accessible and practical for everyday use.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">For testing uniformity, we use the same parameters and quantization precision for all open-source models, which are summarized in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A3" title="Appendix C Model Parameters and Testing Environment ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">C</span></a>, along with the computing environment used. To generate code for our analysis, we query each of the LLMs (<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.T1" title="In 4.2 Code Generation ‣ 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>) with prompts from the two datasets using the Python <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p4.1.1">requests</span> packages to interact with the text-generation-webui API. The message to the API contains a system message with specific instructions for the model and the prompt itself. An overview of the process, including system messages used during each step, are detailed in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A2" title="Appendix B System Messages and Prompts ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a>. This results in a total of 19,200 code samples per model (16 Python tests + 14 JavaScript tests * 19,200 = 576,000 total code samples), which are further analyzed to determine which packages are required to execute the generated code.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Detection Methodology and Heuristics</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Detection of hallucinated packages from LLM outputs or code samples is non-trivial. In short, simply parsing the code for “import” or “require” is not useful as the arguments in those statements refer to modules and not packages. There is no way to definitively determine required packages from a code snippet alone. A detailed explanation of this problem can be found in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A7" title="Appendix G Packages and Modules Explanation ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">G</span></a>. To solve this problem, we employ the following three heuristics to determine/identify package names in the generated code:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Heuristic 1.</span> As part of our first heuristic, we parse the generated Python and JavaScript code for “<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.2">pip install</span>” and “<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.3">npm install</span>” commands, respectively. These commands look for the specified package in the PyPI/npm repository, resolve its dependencies, and install everything in the current Python/JavaScript environment to ensure that future module requests will work.
This is the most straightforward heuristic for detecting package names (and thus, hallucinations), as it involves explicit commands from the code generation model for package download/installation. This is significant because if the referenced hallucinated package was indeed used by an adversary to execute a package confusion attack,
it could immediately trigger download/install of the malicious code in the package. Note that we did not directly ask the model to provide these commands, but allowed them to occur naturally during the generation process. As such, we observed that these instances (“<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.4">pip install</span>” and “<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p2.1.5">npm install</span>”) occur for <math alttext="7\%" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.1"><semantics id="S4.SS3.p2.1.m1.1a"><mrow id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml"><mn id="S4.SS3.p2.1.m1.1.1.2" xref="S4.SS3.p2.1.m1.1.1.2.cmml">7</mn><mo id="S4.SS3.p2.1.m1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><apply id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS3.p2.1.m1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1">percent</csymbol><cn id="S4.SS3.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS3.p2.1.m1.1.1.2">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">7\%</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.1d">7 %</annotation></semantics></math> of the total outputs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Heuristic 2.</span> For the second heuristic, each generated code sample is
used as an input into the same model that generated it. The model is then prompted for a list of packages that would be required to run the given code. Our intuition is to mimic an actual user/developer who is using LLMs for code generation. If the user gets an error due to an uninstalled package when attempting to execute the generated code, they could query the model for the correct package to install.
We wanted to replicate this intuitive process to identify package names required by the generated code.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Heuristic 3.</span> As the third heuristic, we re-use the original prompt
used to generate the code sample as an input to the model and ask the model to output package names that would be required to accomplish this coding task. Similar to the previous heuristic, this process of extracting package names simulates another approach users would take to obtain package names from the model that generated the code, if the required packages were not mentioned in explicit “<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p4.1.2">pip install</span>” and “<span class="ltx_text ltx_font_typewriter" id="S4.SS3.p4.1.3">npm install</span>” commands.</p>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1">Once each model provides specific package names (through the three heuristics outlined above), we simply compare each package name to a master list of package names acquired from PyPI and npm, respectively (each list is as of 10 January, 2024). If a package name is not on the master list it is considered a hallucination. We acknowledge the possibility that the master list of packages obtained from the package repositories has already been contaminated with malicious hallucinated packages. It is not possible to guarantee that the master list actually represents the ground truth of valid packages, however, the presence of hallucinated packages already in the master list would actually produce fewer hallucinations, therefore our results represent a lower bound of hallucination rate.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Evaluation Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present the results of our experimental analysis related to <span class="ltx_text ltx_font_bold" id="S5.p1.1.1">RQ1</span> - <span class="ltx_text ltx_font_bold" id="S5.p1.1.2">RQ4</span>. After using both Python and JavaScript for RQ1, for RQs 2 through 4, we focus our analysis only on the Python programming language, a subset of the original models tested, and randomly sampled subsets of our original datasets. Given the consistent results that we were able to obtain across both languages in RQ1, this narrowed scope of discussion does not compromise the generalizability of the conclusions and allows for a deeper analysis of package hallucinations in a controlled setting.
We selected GPT-4 Turbo, GPT-3.5, CodeLlama 7B, and DeepSeek 6.7B for in-depth analysis, representing the best-performing and most popular open-source models.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Prevalence of Package Hallucinations (RQ1)</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In our first experiment, our goal was to quantify the prevalence of package hallucinations across different models by generating and analyzing a large number of code samples.
We conducted 30 tests (using 16 models for Python and 14 models for JavaScript, as outlined in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.T1" title="In 4.2 Code Generation ‣ 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a>) producing a combined 576,000 code samples using both the Stack Overflow and LLM-generated datasets (<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.SS1" title="4.1 Prompt Dataset ‣ 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>). Each code sample was evaluated for hallucinations according to the heuristics defined in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.SS3" title="4.3 Detection Methodology and Heuristics ‣ 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>, which include parsing the generated code and prompting the model for packages twice per code sample, for a total of 1,152,000 package prompts across all tests.
To measure LLMs’ propensity to produce hallucinated packages during code generation we use the <em class="ltx_emph ltx_font_italic" id="S5.SS1.p1.1.1">package hallucination rate</em> metric, which can be expressed as a simple ratio of the number of hallucinated packages to the total number of recommended packages. The full set of hallucination results for all models, covering both Python and JavaScript, are presented in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A5" title="Appendix E Complete Results for Python and JavaScript ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<figure class="ltx_figure" id="S5.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="455" id="S5.F2.g1" src="x2.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Observed hallucination rates of the tested models.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">These 30 tests generated a total of 2.23 million packages in response to our prompts, of which <span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">440,445 (19.7%) were determined to be hallucinations, including 205,474 unique non-existent packages</span> (i.e. packages that do not exist in PyPI or npm repositories and were distinct entries in the hallucination count, irrespective of their multiple occurrences). Our results for GPT-3.5 (5.76%) and GPT-4 (4.05%) differ significantly from previous work on package hallucinations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib35" title="">35</a>]</cite>, which found hallucination rates 4-6 times greater (24.2% and 22.2%, respectively) for those specific models. GPT-series models were found 4 times less likely to generate hallucinated packages compared to open-source models, with a 5.2% hallucination rate compared to 21.7%.
GPT-4 Turbo resulted in the lowest overall hallucination rate at 3.59%, while DeepSeek 1B had the best hallucination rate among the open-source models at 13.63%. Python code resulted in fewer hallucinations than JavaScript (15.8% on average compared to 21.3% for JavaScript). Despite the difference in the hallucination rate between the two languages, there is a linear relationship between the results (as shown in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A4.F14" title="In Appendix D Python vs. JavaScript Hallucination ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a> in the appendix), which demonstrates that the propensity of a model to hallucinate is positively correlated between programming languages.
The total hallucination rates for each evaluated model are presented in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F2" title="In 5.1 Prevalence of Package Hallucinations (RQ1) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
For a comprehensive breakdown of the results for each language, please refer to <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A5" title="Appendix E Complete Results for Python and JavaScript ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">E</span></a>.
These results provide strong evidence that package hallucinations are a pervasive issue across all code generating LLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S5.SS1.p3.1.1" style="width:420.6pt;">
<span class="ltx_p" id="S5.SS1.p3.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1.1.1">RQ1 Summary: </span>Package hallucinations were found to be pervasive phenomenon across all 16 models tested. Commercial models hallucinated 4x less compared to open-source models.
Python code resulted in a lower hallucination rate compared to JavaScript.</span>
</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Impact of Model Settings (RQ2)</h3>
<div class="ltx_para ltx_noindent" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">Effect of Temperature Settings.</span>
The temperature setting in LLMs adjusts the randomness of the generated responses, where a lower temperature results in more predictable and deterministic outputs, and a higher temperature increases creativity and diversity in the responses (<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A6.SS2" title="F.2 Hallucinations ‣ Appendix F Additional Background Information ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">F.2</span></a>). We varied this setting for each model between the minimum and maximum allowed values and observed the change in hallucination rate (the maximum temperature for GPT series models is limited to 2, while the open-source models can be set up to 5).
All models exhibited a <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.2">clear increase in hallucination rate as temperature value increases</span>, with the effect becoming severe at maximum values.
The OpenAI models, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F3" title="In 5.2 Impact of Model Settings (RQ2) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a>, displayed only a slight increase in hallucination rate between temperatures 0 and 1, which then increased sharply between 1 and 2.
Most notably, GPT-4 resulted in a hallucination rate (8.9%) nearly 4x lower than GPT 3.5 (31.8%) at its maximum temperature.
At the highest temperature values, open-source models start to generate more hallucinated packages than valid packages.
Most LLMs operate at a default temperature in the range of 0.7 to 1, however, our results indicate that a lower temperature value can reduce package hallucinations, with the optimal value varying per model. A lower temperature also yields more deterministic responses, presenting a trade-off between risk of hallucination and creativity. Therefore, selecting the appropriate temperature value is not a straightforward decision.
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S5.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S5.F3.g1" src="x3.png" width="822"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Hallucination rate vs. temperature.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.5"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.5.1">Effect of Decoding Strategies.</span> Next, we adjusted several decoding parameters (top-<math alttext="p" class="ltx_Math" display="inline" id="S5.SS2.p2.1.m1.1"><semantics id="S5.SS2.p2.1.m1.1a"><mi id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.1.m1.1d">italic_p</annotation></semantics></math>, top-<math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p2.2.m2.1"><semantics id="S5.SS2.p2.2.m2.1a"><mi id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><ci id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.2.m2.1d">italic_k</annotation></semantics></math>, and min-<math alttext="p" class="ltx_Math" display="inline" id="S5.SS2.p2.3.m3.1"><semantics id="S5.SS2.p2.3.m3.1a"><mi id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><ci id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.3.m3.1d">italic_p</annotation></semantics></math> values) to reduce the chances of a low probability token being selected as a potential package, with the intuition that lower probability tokens correspond to higher probabilities of hallucination in this context. Below is a summary of the parameters and the values we modified.
We evaluated each listed value in isolation, followed by a combined evaluation of the values highlighted in bold, resulting in a total of 10 tests.
Note that top-<math alttext="k" class="ltx_Math" display="inline" id="S5.SS2.p2.4.m4.1"><semantics id="S5.SS2.p2.4.m4.1a"><mi id="S5.SS2.p2.4.m4.1.1" xref="S5.SS2.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.4.m4.1b"><ci id="S5.SS2.p2.4.m4.1.1.cmml" xref="S5.SS2.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.4.m4.1d">italic_k</annotation></semantics></math> and min-<math alttext="p" class="ltx_Math" display="inline" id="S5.SS2.p2.5.m5.1"><semantics id="S5.SS2.p2.5.m5.1a"><mi id="S5.SS2.p2.5.m5.1.1" xref="S5.SS2.p2.5.m5.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.5.m5.1b"><ci id="S5.SS2.p2.5.m5.1.1.cmml" xref="S5.SS2.p2.5.m5.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.5.m5.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p2.5.m5.1d">italic_p</annotation></semantics></math> were only tested for DeepSeek and CodeLlama, as those values are not modifiable through the OpenAI API.</p>
<ul class="ltx_itemize" id="S5.I1">
<li class="ltx_item" id="S5.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i1.p1">
<p class="ltx_p" id="S5.I1.i1.p1.1">Top-<math alttext="p" class="ltx_Math" display="inline" id="S5.I1.i1.p1.1.m1.1"><semantics id="S5.I1.i1.p1.1.m1.1a"><mi id="S5.I1.i1.p1.1.m1.1.1" xref="S5.I1.i1.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i1.p1.1.m1.1b"><ci id="S5.I1.i1.p1.1.m1.1.1.cmml" xref="S5.I1.i1.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i1.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i1.p1.1.m1.1d">italic_p</annotation></semantics></math> (0.4, <span class="ltx_text ltx_font_bold" id="S5.I1.i1.p1.1.1">0.6</span>, 0.8): Tokens with probabilities adding up to less than this number are discarded.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i2.p1">
<p class="ltx_p" id="S5.I1.i2.p1.2">Top-<math alttext="k" class="ltx_Math" display="inline" id="S5.I1.i2.p1.1.m1.1"><semantics id="S5.I1.i2.p1.1.m1.1a"><mi id="S5.I1.i2.p1.1.m1.1.1" xref="S5.I1.i2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.1.m1.1b"><ci id="S5.I1.i2.p1.1.m1.1.1.cmml" xref="S5.I1.i2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.1.m1.1d">italic_k</annotation></semantics></math> (5, <span class="ltx_text ltx_font_bold" id="S5.I1.i2.p1.2.1">10</span>, 15): Select only the top-<math alttext="k" class="ltx_Math" display="inline" id="S5.I1.i2.p1.2.m2.1"><semantics id="S5.I1.i2.p1.2.m2.1a"><mi id="S5.I1.i2.p1.2.m2.1.1" xref="S5.I1.i2.p1.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i2.p1.2.m2.1b"><ci id="S5.I1.i2.p1.2.m2.1.1.cmml" xref="S5.I1.i2.p1.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i2.p1.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i2.p1.2.m2.1d">italic_k</annotation></semantics></math> most likely tokens.</p>
</div>
</li>
<li class="ltx_item" id="S5.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S5.I1.i3.p1">
<p class="ltx_p" id="S5.I1.i3.p1.2">Min-<math alttext="p" class="ltx_Math" display="inline" id="S5.I1.i3.p1.1.m1.1"><semantics id="S5.I1.i3.p1.1.m1.1a"><mi id="S5.I1.i3.p1.1.m1.1.1" xref="S5.I1.i3.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.1.m1.1b"><ci id="S5.I1.i3.p1.1.m1.1.1.cmml" xref="S5.I1.i3.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.p1.1.m1.1d">italic_p</annotation></semantics></math> (0.1, <span class="ltx_text ltx_font_bold" id="S5.I1.i3.p1.2.1">0.2</span>, 0.3): Tokens with probability smaller than (min-<math alttext="p" class="ltx_Math" display="inline" id="S5.I1.i3.p1.2.m2.1"><semantics id="S5.I1.i3.p1.2.m2.1a"><mi id="S5.I1.i3.p1.2.m2.1.1" xref="S5.I1.i3.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S5.I1.i3.p1.2.m2.1b"><ci id="S5.I1.i3.p1.2.m2.1.1.cmml" xref="S5.I1.i3.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.I1.i3.p1.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S5.I1.i3.p1.2.m2.1d">italic_p</annotation></semantics></math> * probability of most likely token) are discarded.</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S5.SS2.p2.6">Varying the decoding values induced a slight <em class="ltx_emph ltx_font_italic" id="S5.SS2.p2.6.1">increase</em> (1.16% on average) in hallucination rate for all four models across all values tested. As we will expand on in RQ3, package hallucinations are often persistently repeated across many iterations.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">This suggests that greedy decoding strategies, which prioritize the most probable tokens (i.e., the most probable token is always selected), would still generate fictitious packages.</span> This differs from other types of hallucinations, which generally occur when low-probability tokens are sampled.
This persistent nature of package hallucinations highlights the inherent complexity of the problem.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="474" id="S5.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Hallucination rates of recent vs. all-time data sets.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Recency of Subject Matter.</span>
As described in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S4.SS1" title="4.1 Prompt Dataset ‣ 4 Experiment Design ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.1</span></a>, we separated our coding prompts into two temporal datasets to evaluate if hallucination rate was correlated with topics/packages that emerged after the model was trained. A lower difference between the rates of recent and all-time prompts indicates better performance in handling questions that fall outside the model’s pre-training data and therefore a more generalizable model. The models we tested were shown to be more likely to generate a package hallucination when responding to prompts that deal with more recent topics. This difference resulted in a 10% higher hallucination rate on average for older data vs. more recent data.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">Overall, all 16 Python models we evaluated <span class="ltx_text ltx_font_bold" id="S5.SS2.p5.1.1">demonstrated a higher hallucination rate when being prompted about questions or packages that were popular within the past year</span> (<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F4" title="In 5.2 Impact of Model Settings (RQ2) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>).
These higher rates are at least partly due to inherent limitations and training costs of modern LLMs. As noted in the OpenAI GPT-4 technical report <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib1" title="">1</a>]</cite>, LLMs cannot update themselves with new information post-release and have no knowledge of the world beyond their training data cut-off date. Although fine-tuning can enhance specific tasks, it does not generally improve the model’s overall knowledge of the world. The massive cost of training LLMs from scratch, evidenced by the 1,400,000 GPU hours (220 years) required to train the 12 CodeLlama models, makes continuously updating pre-training data impractically expensive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib66" title="">66</a>]</cite>. This cost, along with steadily increasing model sizes and training times, poses a significant barrier to reducing package hallucinations for advanced coding prompts and packages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S5.SS2.p6.1.1" style="width:420.6pt;">
<span class="ltx_p" id="S5.SS2.p6.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p6.1.1.1.1">RQ2 Summary: </span> Lower temperatures result in the lowest hallucination rate while hallucination rates increase dramatically with temperature values larger than 1. Altering decoding and sampling parameters in the model does not improve hallucination rates. More recent prompting topics lead to a 10% increase in hallucination rates.</span>
</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Model Behaviors (RQ3)</h3>
<div class="ltx_para ltx_noindent" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">Frequency of Repeated Hallucinations.</span>
To determine whether hallucinations are random error or repeatable phenomena, this test focuses on the persistence of hallucinations within a model. We randomly sampled 500 prompts that generated package hallucinations during our initial testing, then repeated those queries 10 times per prompt. Of those 10 queries we recorded how many times the original hallucinated package was regenerated.
Our analysis reveals an unexpected dichotomy when repeatedly querying a model with the same prompt that generated a hallucination: 43% of hallucinated packages were repeated in all 10 queries, while 39% did not repeat at all across the 10 queries. This is indicated in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F5" title="In 5.3 Model Behaviors (RQ3) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>, which shows prominent spikes at zero repetitions and at 10 repetitions respectively for all models.
Further, 58% of the time, a hallucinated package is repeated more than once in 10 iterations, which shows that a majority of <span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.2">hallucinations are not simply random errors, but a repeatable phenomenon that persists across multiple iterations</span>.
This is significant because a persistent hallucination is more valuable for malicious actors looking to exploit this vulnerability and makes the hallucination attack vector a more viable threat.</p>
</div>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="443" id="S5.F5.g1" src="x5.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Frequency of an identical hallucinated package name generated from the same prompt across 10 trials.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Verbose Models vs. Conservative Models.</span>
LLMs operate with inherent randomness and uncertainty. This behavior enables novel and creative output, a desired feature for many NLP tasks but less welcome for code generation, which requires a high degree of accuracy and must adhere to rigid syntax. We define a verbose model as one that operates with higher degree of uncertainty and randomness by generating a greater number of distinct package names while a conservative model generates a lesser number of distinct packages, generally using only the most popular and well-known packages.
To this end, we investigated whether verbose models correspond to higher rate of package hallucinations. Our results show a <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.2">correlation between the hallucination rate and number of unique packages</span> that were recommended during this experiment (i.e. a more verbose model was associated with a higher hallucination rate).
In light of these findings, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F6" title="In 5.3 Model Behaviors (RQ3) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>, it is reasonable to suggest that coding models, or models performing coding tasks, should operate in a more conservative manner when generating packages to answer a coding prompt.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="S5.F6.g1" src="x6.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Unique packages vs. total hallucination rate.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">The most successful models in our study (i.e., the lowest hallucination rates) adhered to a smaller subset of well-known packages when generating code, and these models (e.g., the GPT series) also scored the highest on the EvalPlus <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib45" title="">45</a>]</cite> code quality benchmarks. This suggests that improving code quality and reducing hallucinations can potentially be achieved simultaneously without a trade-off.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">LLMs’ Ability to Detect Hallucinations.</span>
We then evaluated each model’s ability to identify hallucinations vs. valid packages, both from its own code generation outputs and those generated by other models. To test this, we conducted two binary classification tests: (i) each model’s ability to detect hallucinations vs. valid packages from its own generated code, and (ii) each model’s ability to detect the same from code generated by other models.
Names of valid and hallucinated packages produced by each model were randomly sampled and each model was asked if the provided name was a valid Python package.
The identification accuracy was calculated as the ratio of correct identifications to the total number of provided packages.</p>
</div>
<figure class="ltx_figure" id="S5.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="382" id="S5.F7.g1" src="x7.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The ability of models to correctly identify valid vs. hallucinated packages.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.1">Curiously, 3 of the 4 models (GPT 4 Turbo, GPT 3.5, and DeepSeek) proved to be <span class="ltx_text ltx_font_bold" id="S5.SS3.p5.1.1">highly adept at detecting their own hallucinations</span> (see <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F7" title="In 5.3 Model Behaviors (RQ3) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">7</span></a>) with detection accuracy above 75%. The precision and recall values for this test, which also averaged above 75%, are presented in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.T2" title="In 5.3 Model Behaviors (RQ3) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>.
This phenomenon implies that each model’s specific error patterns are detectable by the same mechanisms that generate them, suggesting an inherent self-regulatory capability.
The indication that these models have an implicit understanding of their own generative patterns which could be leveraged for self-improvement is an important finding for developing mitigation strategies.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Performance of hallucination detection tests.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.3" style="width:191.6pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S5.T2.3.1"><span class="ltx_text" id="S5.T2.3.1.1" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S5.T2.3.1.1.1">
<span class="ltx_tr" id="S5.T2.3.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_tt ltx_rowspan ltx_rowspan_2" id="S5.T2.3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.1.1.1">Model Name</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.1.2.1">Other</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.1.3.1">Same</span></span></span>
<span class="ltx_tr" id="S5.T2.3.1.1.1.2">
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.1.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.2.1.1">Precision</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.1.1.2.2"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.2.2.1">Recall</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.1.1.2.3" style="background-color:#999999;"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.2.3.1" style="background-color:#999999;">Precision</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.3.1.1.1.2.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1.2.4.1">Recall</span></span></span>
<span class="ltx_tr" id="S5.T2.3.1.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.3.1.1.1.3.1">GPT 4 Turbo</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.1.1.3.2">0.91</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.1.1.3.3">0.91</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.1.1.3.4" style="background-color:#999999;"><span class="ltx_text" id="S5.T2.3.1.1.1.3.4.1" style="background-color:#999999;">0.89</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T2.3.1.1.1.3.5">0.89</span></span>
<span class="ltx_tr" id="S5.T2.3.1.1.1.4">
<span class="ltx_td ltx_align_left" id="S5.T2.3.1.1.1.4.1">GPT 3.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.3.1.1.1.4.2">0.78</span>
<span class="ltx_td ltx_align_center" id="S5.T2.3.1.1.1.4.3">0.78</span>
<span class="ltx_td ltx_align_center" id="S5.T2.3.1.1.1.4.4" style="background-color:#999999;"><span class="ltx_text" id="S5.T2.3.1.1.1.4.4.1" style="background-color:#999999;">0.82</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.3.1.1.1.4.5">0.82</span></span>
<span class="ltx_tr" id="S5.T2.3.1.1.1.5">
<span class="ltx_td ltx_align_left" id="S5.T2.3.1.1.1.5.1">CodeLlama</span>
<span class="ltx_td ltx_align_center" id="S5.T2.3.1.1.1.5.2">0.72</span>
<span class="ltx_td ltx_align_center" id="S5.T2.3.1.1.1.5.3">0.66</span>
<span class="ltx_td ltx_align_center" id="S5.T2.3.1.1.1.5.4" style="background-color:#999999;"><span class="ltx_text" id="S5.T2.3.1.1.1.5.4.1" style="background-color:#999999;">0.66</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T2.3.1.1.1.5.5">0.60</span></span>
<span class="ltx_tr" id="S5.T2.3.1.1.1.6">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.3.1.1.1.6.1">DeepSeek</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.1.1.6.2">0.80</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.1.1.6.3">0.80</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.1.1.6.4" style="background-color:#999999;"><span class="ltx_text" id="S5.T2.3.1.1.1.6.4.1" style="background-color:#999999;">0.81</span></span>
<span class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S5.T2.3.1.1.1.6.5">0.78</span></span>
</span></span></p>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS3.p6">
<p class="ltx_p" id="S5.SS3.p6.1">CodeLlama displays unique and interesting behavior during both the tests, as it has an overwhelming propensity to label most packages as valid, resulting in a lower accuracy for hallucinated packages.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p7">
<p class="ltx_p" id="S5.SS3.p7.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S5.SS3.p7.1.1" style="width:420.6pt;">
<span class="ltx_p" id="S5.SS3.p7.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p7.1.1.1.1">RQ3 Summary: </span> Package hallucinations are often persistently generated. Models that generate fewer packages when prompted are correlated with a reduced hallucination rate. Several models were able to accurately detect their own hallucinations.</span>
</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Characteristics of Hallucinations (RQ4)</h3>
<div class="ltx_para ltx_noindent" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p1.1.1">Occurrence of the Same Package Hallucination Across Different Models.</span>
To analyze the possibility of the same hallucinated packages being generated across different models, we measured how many models generated the same package name given a confirmed package hallucination.
<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F8" title="In 5.4 Characteristics of Hallucinations (RQ4) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">8</span></a> shows a clear pattern where <span class="ltx_text ltx_font_bold" id="S5.SS4.p1.1.2">a vast majority (81%) of distinctly generated package names were generated by only one model</span>. In other words, specific package names were usually unique to a single model, where only the most common packages were generated by more than one model. The two populations diverge as the number of models increases, with the number of hallucinated packages decreasing nearly exponentially
and the distribution of valid packages becoming more uniform.
The finding that valid packages are less dependent on the specific model used for generation is attributable to their frequent appearance in training data and applicability to universal coding problems, leading to their widespread use in a broad range of prompts.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">Combining the insights gained during the previously discussed persistence analysis (<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F5" title="In 5.3 Model Behaviors (RQ3) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>) leads to a key observation. As previously shown, hallucinations are often persistent (58% are repeated within 10 iterations) within the same model but are not often repeated between models, as 81% of hallucinated packages are generated by only one model.
This further reinforces the evidence that while hallucinations are a common phenomenon across various models, the exact nature of these hallucinations is generally model-specific. This behavior is particularly surprising given that our testing includes multiple models from the same family (i.e. 3 GPT models, 4 CodeLlama models, and 3 DeepSeek models). These models presumably use the same training data for each version, yet each model generates unique hallucinations not found in other models.</p>
</div>
<figure class="ltx_figure" id="S5.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="486" id="S5.F8.g1" src="x8.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Number of models in which each unique package (valid &amp; hallucinated) appeared, with the y-axis on a log scale.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">Semantic Similarity Between Hallucinated and popular packages.</span>
In order to analyze the semantic similarity between hallucinated and popular real/valid packages, we measured the average <em class="ltx_emph ltx_font_italic" id="S5.SS4.p3.1.2">Levenshtein distance</em> of a package to its nearest neighbor (i.e., the closest valid package). Levenshtein distance is a measure of how many insertions, deletions, and substitutions are required for two strings to match <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib36" title="">36</a>]</cite>.
If the distribution of Levenshtein distances is skewed heavily right, with a peak at or near 0, this would indicate that most hallucinations are very similar to valid package names. In that case, attackers could infer a hallucination target based on more traditional package confusion methods (e.g., typosquatting) rather than analyzing a large volume of model output over time to detect persistent hallucinations that could be used as vessels for malicious code. A higher distance reflects that package hallucinations are more random in nature and difficult to predict, rather than the result of minor grammatical errors.</p>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1">The results of the Levenshtein distance test, as seen in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F9" title="In 5.4 Characteristics of Hallucinations (RQ4) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a>, suggests that most package hallucinations are not simple <em class="ltx_emph ltx_font_italic" id="S5.SS4.p4.1.1">off-by-one errors</em>. An off-by-one error in our case is defined as a difference between the hallucinated package and its closest match of 1-2 numbers, letters, or punctuation marks, with only 13.4% (10,263 of 76,489) belonging to this category.
Another 37.9% (29,025 of 76,489) of packages registered a score between 3-5, which would indicate two words with a common root word or concept that still differ significantly.
Notably, 48.6% (37,207 of 76,489) of hallucinations scored 6 or greater, with 20.2% (15,457 of 76,489) of those scoring 10 or higher, indicating two strings that are very different and likely do not share any common theme.</p>
</div>
<div class="ltx_para" id="S5.SS4.p5">
<p class="ltx_p" id="S5.SS4.p5.1">The presence of such a large proportion of high Levenshtein values suggests that <span class="ltx_text ltx_font_bold" id="S5.SS4.p5.1.1">the majority of hallucinations are not merely trivial typographical errors but are instead substantively different from existing package names</span>.
The observed results provide further evidence that the root cause of hallucinations is likely to be more complex than minor string manipulation, pointing to deeper issues in the model’s generative processes that govern the creation of package names.
The long right tail of the distribution in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.F9" title="In 5.4 Characteristics of Hallucinations (RQ4) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">9</span></a> indicates a wide variety of hallucinations spread across a broad range, revealing a diversity in types of errors and reinforcing that the generation of hallucinations is a complex issue not limited to simple character substitutions, additions, and deletions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p6">
<p class="ltx_p" id="S5.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p6.1.1">Effect of Deleted Packages.</span>
To determine whether packages that existed before the pre-training data cut-off
date (i.e., final day of data included in the model’s training set) but were subsequently removed, contribute significantly to package hallucinations, we conducted an analysis using package download counts obtained via Google BigQuery <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib16" title="">16</a>]</cite>. We searched PyPI download counts from 2022 and earlier to compile a list of packages that existed before 2023 but are no longer available on PyPI. This list was then compared against the master list of packages (obtained from PyPI as of January 10, 2024) of hallucinated packages across all models.
We detected 12,871 packages that were available between 2020 and 2022 that have since been removed from PyPI. Out of these deleted packages, only 133 (0.17%) were generated during our analysis, indicating that <span class="ltx_text ltx_font_bold" id="S5.SS4.p6.1.2">deleted packages are a negligible source of package hallucinations</span>. This finding contradicts our hypothesis, as we expected a sizable percentage of hallucinated packages due to the presence of deleted packages in the training data.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS4.p7">
<p class="ltx_p" id="S5.SS4.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p7.1.1">Effect of Language Confusion.</span>
Another behavior observed during the main experiment is the tendency to confuse programming languages while generating package output (i.e. the model is asked to provide Python packages, but instead provides JavaScript packages). To validate the potential existence of this behavior in code generating LLMs, we obtained master lists of packages (using libraries.io <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib72" title="">72</a>]</cite>) from the nine most popular open-source repositories and compared our list of hallucinated package names generated during Python testing to the respective master lists of valid packages from other programming languages. Any intersection between the two lists indicates a <em class="ltx_emph ltx_font_italic" id="S5.SS4.p7.1.2">cross-language hallucination</em>. <span class="ltx_text ltx_font_bold" id="S5.SS4.p7.1.3">Overall, only JavaScript is a significant source of cross-language hallucinations, as 8.7% (6,705/76,489) of hallucinated Python packages are valid JavaScript packages</span>. All other languages contributed negligible hallucinations, combining for only 0.8% (663 of 76,489) across eight other open-source repositories, including R, Rust, Ruby, PHP, Swift and .NET
(see <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.T3" title="In 5.4 Characteristics of Hallucinations (RQ4) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> for complete results).</p>
</div>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_block ltx_pruned_first" id="S5.T3.2">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_block">Table 3: </span>Confusion by programming language repository.</figcaption>
<div class="ltx_para ltx_noindent ltx_align_center" id="S5.T3.2.p2">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T3.2.p2.1" style="width:184.8pt;height:221.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S5.T3.2.p2.1.1"><span class="ltx_text" id="S5.T3.2.p2.1.1.1" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.2.p2.1.1.1.1">
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.2.p2.1.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.2.p2.1.1.1.1.1.1.1">Programming Language</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.2.p2.1.1.1.1.1.2"><span class="ltx_text" id="S5.T3.2.p2.1.1.1.1.1.2.1"></span> <span class="ltx_text" id="S5.T3.2.p2.1.1.1.1.1.2.2">
<span class="ltx_tabular ltx_align_middle" id="S5.T3.2.p2.1.1.1.1.1.2.2.1">
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.1.1.1">No. of</span></span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.2.1"><span class="ltx_text ltx_font_bold" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.2.1.1">Cross-Language</span></span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.3.1"><span class="ltx_text ltx_font_bold" id="S5.T3.2.p2.1.1.1.1.1.2.2.1.3.1.1">Hallucinations</span></span></span>
</span></span><span class="ltx_text" id="S5.T3.2.p2.1.1.1.1.1.2.3"></span></span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.2" style="background-color:#999999;">
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.2.p2.1.1.1.1.2.1"><span class="ltx_text" id="S5.T3.2.p2.1.1.1.1.2.1.1" style="background-color:#999999;">JavaScript (npm)</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.2.p2.1.1.1.1.2.2"><span class="ltx_text" id="S5.T3.2.p2.1.1.1.1.2.2.1" style="background-color:#999999;">6,705</span></span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.3">
<span class="ltx_td ltx_align_left" id="S5.T3.2.p2.1.1.1.1.3.1">R (CRAN)</span>
<span class="ltx_td ltx_align_center" id="S5.T3.2.p2.1.1.1.1.3.2">293</span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.4">
<span class="ltx_td ltx_align_left" id="S5.T3.2.p2.1.1.1.1.4.1">Rust (Cargo)</span>
<span class="ltx_td ltx_align_center" id="S5.T3.2.p2.1.1.1.1.4.2">181</span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.5">
<span class="ltx_td ltx_align_left" id="S5.T3.2.p2.1.1.1.1.5.1">Ruby (Rubygems)</span>
<span class="ltx_td ltx_align_center" id="S5.T3.2.p2.1.1.1.1.5.2">123</span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.6">
<span class="ltx_td ltx_align_left" id="S5.T3.2.p2.1.1.1.1.6.1">PHP (Packagist)</span>
<span class="ltx_td ltx_align_center" id="S5.T3.2.p2.1.1.1.1.6.2">47</span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.7">
<span class="ltx_td ltx_align_left" id="S5.T3.2.p2.1.1.1.1.7.1">Swift/Objective C (Cocoapods)</span>
<span class="ltx_td ltx_align_center" id="S5.T3.2.p2.1.1.1.1.7.2">10</span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.8">
<span class="ltx_td ltx_align_left" id="S5.T3.2.p2.1.1.1.1.8.1">.NET (Nuget)</span>
<span class="ltx_td ltx_align_center" id="S5.T3.2.p2.1.1.1.1.8.2">9</span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.9">
<span class="ltx_td ltx_align_left" id="S5.T3.2.p2.1.1.1.1.9.1">Go (Go)</span>
<span class="ltx_td ltx_align_center" id="S5.T3.2.p2.1.1.1.1.9.2">0</span></span>
<span class="ltx_tr" id="S5.T3.2.p2.1.1.1.1.10">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.2.p2.1.1.1.1.10.1">Java (Maven)</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.2.p2.1.1.1.1.10.2">0</span></span>
</span></span></p>
</span></div>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S5.SS4.p8">
<p class="ltx_p" id="S5.SS4.p8.1">
<span class="ltx_inline-block ltx_parbox ltx_align_middle ltx_framed ltx_framed_rectangle" id="S5.SS4.p8.1.1" style="width:420.6pt;">
<span class="ltx_p" id="S5.SS4.p8.1.1.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.p8.1.1.1.1">RQ4 Summary: </span> Most unique hallucinated package names were generated by a single model and thus only appeared in 1 out of 16 tests.
Most hallucinated package names were not semantically similar to a valid Python package as measured by Levenshtein distance. Deleted packages were a negligible source of package hallucinations while JavaScript was the only significant source of cross-language hallucinations.</span>
</span></p>
</div>
<figure class="ltx_figure" id="S5.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="476" id="S5.F9.g1" src="x9.png" width="788"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Levenshtein distance of hallucinated packages to nearest valid package.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Mitigation</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Motivated by the finding of systemic package hallucination in all the tested models, we investigate techniques to mitigate the occurrence of such hallucinations (thus addressing RQ5).</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Mitigation Strategies</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">A seemingly obvious mitigation strategy when detecting package hallucinations might involve comparing a master list of valid packages to the model’s code output. It is imperative to understand why this approach is merely a superficial solution rather than addressing the root cause. The fundamental vulnerability inherent in code generation by LLMs regarding package hallucinations is not solely that they produce non-existent packages, but that they generate package names that were not present in their training data. Our study emphasizes the detection of hallucinated packages as indicators of an underlying vulnerability. The genuine threat posed by package hallucinations emerges when such a hallucinated package is officially published and contains malicious code. The mere act of publishing does not render a previously hallucinated package benign; on the contrary, it becomes an active threat seeking to exploit a security vulnerability. Consequently, simply cross-referencing generated packages with a master list would erroneously validate a malicious package after publication. Dedicated mitigation techniques are essential to prevent the generation of hallucinated packages altogether, thereby addressing the root cause of this latent vulnerability.</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">Implementing mitigation strategies specific to code generation LLMs is an unexplored topic at the time of writing, therefore we rely on general hallucination mitigation strategies proposed for standard NLP tasks that can be applied to code generation.
These strategies can generally be grouped into two broad categories: <em class="ltx_emph ltx_font_italic" id="S6.SS1.p2.1.1">prompt engineering</em> and <em class="ltx_emph ltx_font_italic" id="S6.SS1.p2.1.2">model development</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib73" title="">73</a>]</cite>.
Prompt engineering includes methods such as <em class="ltx_emph ltx_font_italic" id="S6.SS1.p2.1.3">Retrieval Augmented Generation (RAG)</em>, <em class="ltx_emph ltx_font_italic" id="S6.SS1.p2.1.4">self-refinement</em>, and <em class="ltx_emph ltx_font_italic" id="S6.SS1.p2.1.5">prompt tuning</em>. RAG approaches involve enriching the original prompt with additional information gathered from an external source, such as the web or a pre-determined database <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib37" title="">37</a>]</cite>. This augmentation can occur at any stage—before, during, or after response generation—and can be iterative, improving over multiple cycles until the response is verified as accurate. Self-refinement strategies, on the other hand, utilize the model itself to detect and refine potential hallucinations.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.3">The second main mitigation strategy involves improving the underlying LLM model itself through improved <em class="ltx_emph ltx_font_italic" id="S6.SS1.p3.3.1">decoding strategies</em> or <em class="ltx_emph ltx_font_italic" id="S6.SS1.p3.3.2">supervised fine-tuning</em>.
Supervised fine-tuning alters model parameters to improve performance on tasks prone to hallucinations, utilizing a labeled dataset for more precise training. Decoding strategies are also considered a viable mitigation strategy from the literature, but based on our findings from RQ2 we know that altering decoding parameters, such as top-<math alttext="k" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><mi id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><ci id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">italic_k</annotation></semantics></math>, top-<math alttext="p" class="ltx_Math" display="inline" id="S6.SS1.p3.2.m2.1"><semantics id="S6.SS1.p3.2.m2.1a"><mi id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><ci id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">p</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.2.m2.1d">italic_p</annotation></semantics></math>, and min-<math alttext="p" class="ltx_Math" display="inline" id="S6.SS1.p3.3.m3.1"><semantics id="S6.SS1.p3.3.m3.1a"><mi id="S6.SS1.p3.3.m3.1.1" xref="S6.SS1.p3.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.3.m3.1b"><ci id="S6.SS1.p3.3.m3.1.1.cmml" xref="S6.SS1.p3.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.3.m3.1c">p</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.3.m3.1d">italic_p</annotation></semantics></math>, do not result in a decreased hallucination rate.
We evaluate each of the three remaining categories to determine their applicability to code generation and our specific use case of reducing hallucinations, ultimately developing a viable mitigation strategy.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Mitigation Implementation and Results</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.1">Retrieval Augmented Generation (RAG).</span> We utilized a method of before generation RAG to supplement the prompt with valid package names to assist the model in generating a response. We made an additional dataset to serve as the supplementary information by taking the top 20,000 most popular packages from PyPI and prompting LlaMA-2 to generate a list of 5 questions that each package could help answer given the description. After removing duplicate responses, this resulted in 65,000 statements in the form “Package [x] could answer questions about [y]”. These 65,000 statements were stored in a vector database, which enables efficient retrieval of semantically similar statements. When a model was asked to recommend packages given a code generation prompt or Stack Overflow question, the vector database is first queried from within the code to return the top 5 most semantically similar statements.
These statements are appended to the prompt to give the model additional information containing established valid packages to generate a non-hallucinated response.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">Self-Refinement.</span> Drawing on insights from our findings in RQ3 (see <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.SS3" title="5.3 Model Behaviors (RQ3) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.3</span></a>), which revealed that LLMs often exhibit proficiency in identifying their own package hallucinations, we implemented a self-refinement method.
Following the generation of package names, the model is queried regarding the validity of these packages.
If the model indicates that the packages are invalid, the response is regenerated with a specific instruction to not use the invalid package. This regeneration process is allowed to iterate up to five times, acknowledging that many package hallucinations are persistent, as demonstrated in RQ3, and may be repeatedly generated. It is possible during this test that a valid package is misclassified as a hallucination by the model, although because this is an iterative process the success rate should outweigh the false positive rate over the course of testing, resulting in a net decrease in hallucination rate.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p3.1.1">Fine-tuning.</span> For our next method, we fine-tuned the models using the code/package list (Heuristic 1) and prompt/package list (Heuristic 2) pairs that were generated during our initial experiments (<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.SS1" title="5.1 Prevalence of Package Hallucinations (RQ1) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>). All hallucinations were filtered out and the models were re-trained using the remaining valid responses (560,000 samples). As fine-tuning affects the underlying model weights, we also need to make sure the fine-tuned model retains the ability to produce functional and effective code, which we will test by comparing the code quality from the original models and the fine-tuned models using the well-known HumanEval benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib8" title="">8</a>]</cite>. The HumalEval benchmark is a set of pre-defined prompts and test cases and the final score is a percentage of problems for which the model generated code is both syntactically correct and passes the test cases, reflecting the model’s ability to produce functional and accurate code.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">We implemented these mitigation techniques using the DeepSeek Coder 6.7B and CodeLlama 7B models. These models were selected because they represent two distinct classes of foundational models, with these specific parameter sizes reflecting diverse performance levels: DeepSeek being among the best-performing, and CodeLlama among the worst-performing during our initial experiments (see <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.SS1" title="5.1 Prevalence of Package Hallucinations (RQ1) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>).
Both models were tested using each of the above methods individually and then using all three methods in an <em class="ltx_emph ltx_font_italic" id="S6.SS2.p4.1.1">ensemble</em> configuration.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance of the mitigation techniques.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T4.3" style="width:281.7pt;height:108pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T4.3.1"><span class="ltx_text" id="S6.T4.3.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S6.T4.3.1.1.1">
<span class="ltx_tr" id="S6.T4.3.1.1.1.1">
<span class="ltx_td ltx_border_tt" id="S6.T4.3.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.1.1.2.1">DeepSeek</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.1.1.3.1">CodeLlama</span></span></span>
<span class="ltx_tr" id="S6.T4.3.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.3.1.1.1.2.1">Baseline (No Mitigations)</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.1.1.1.2.2">16.14%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.1.1.1.2.3">26.28%</span></span>
<span class="ltx_tr" id="S6.T4.3.1.1.1.3">
<span class="ltx_td ltx_align_left" id="S6.T4.3.1.1.1.3.1">Retrieval Augmented Generation (RAG)</span>
<span class="ltx_td ltx_align_center" id="S6.T4.3.1.1.1.3.2">12.24%</span>
<span class="ltx_td ltx_align_center" id="S6.T4.3.1.1.1.3.3">13.40%</span></span>
<span class="ltx_tr" id="S6.T4.3.1.1.1.4">
<span class="ltx_td ltx_align_left" id="S6.T4.3.1.1.1.4.1">Self-Refinement</span>
<span class="ltx_td ltx_align_center" id="S6.T4.3.1.1.1.4.2">13.04%</span>
<span class="ltx_td ltx_align_center" id="S6.T4.3.1.1.1.4.3">25.51%</span></span>
<span class="ltx_tr" id="S6.T4.3.1.1.1.5" style="background-color:#999999;">
<span class="ltx_td ltx_align_left" id="S6.T4.3.1.1.1.5.1"><span class="ltx_text" id="S6.T4.3.1.1.1.5.1.1" style="background-color:#999999;">Fine-tuning</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.3.1.1.1.5.2"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.1.5.2.1" style="background-color:#999999;">2.66%</span></span>
<span class="ltx_td ltx_align_center" id="S6.T4.3.1.1.1.5.3"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.1.5.3.1" style="background-color:#999999;">10.27%</span></span></span>
<span class="ltx_tr" id="S6.T4.3.1.1.1.6" style="background-color:#999999;">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S6.T4.3.1.1.1.6.1"><span class="ltx_text" id="S6.T4.3.1.1.1.6.1.1" style="background-color:#999999;">Ensemble</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.1.1.1.6.2"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.1.6.2.1" style="background-color:#999999;">2.40%</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.1.1.1.6.3"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.1.6.3.1" style="background-color:#999999;">9.32%</span></span></span>
</span></span></p>
</span></div>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS2.p5">
<p class="ltx_p" id="S6.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p5.1.1">Results.</span>
Overall, all the mitigation strategies we implemented resulted in a reduced rate of package hallucination, with <span class="ltx_text ltx_font_bold" id="S6.SS2.p5.1.2">RAG and Supervised Fine-Tuning proving particularly effective</span> (see <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S6.T4" title="In 6.2 Mitigation Implementation and Results ‣ 6 Mitigation ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a>).
Fine-tuning proved to significantly improve results, especially for the DeepSeek model, where hallucinations were reduced by 83%, achieving a total rate of just 2.66%, which is a lower rate than any of the ChatGPT models (observed during RQ1).
Self-refinement feedback was also much more effective for the DeepSeek model (19% reduction) compared to the CodeLlama model (3% reduction). This aligns with our results in RQ3, where the DeepSeek model was proficient at detecting hallucinations while the CodeLlama model had a strong bias towards labeling packages as valid, that limited its ability to reliably detect errors.
The ensemble method of combining all mitigation strategies further improved results, reducing hallucination rates by 85% and 64% from their baseline levels for DeepSeek and CodeLlama, respectively.</p>
</div>
<div class="ltx_para" id="S6.SS2.p6">
<p class="ltx_p" id="S6.SS2.p6.1">The code quality of the fine-tuned models did decrease significantly, -26.1% and -3.1% for DeepSeek and CodeLlama respectively, in exchange for substantial improvements in package hallucination rate. While the code quality was negatively impacted, the fine-tuned scores are still at comparable levels to other high performing models such as Mistral 7B (26.1%), Llama 65B (23.1%), and Llama 2 7B (12.8%).</p>
</div>
<figure class="ltx_table" id="S6.T5">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 5: </span>Analysis of code quality after fine-tuning.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.T5.3" style="width:173.0pt;height:54pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="S6.T5.3.1"><span class="ltx_text" id="S6.T5.3.1.1" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="S6.T5.3.1.1.1">
<span class="ltx_tr" id="S6.T5.3.1.1.1.1">
<span class="ltx_td ltx_border_tt" id="S6.T5.3.1.1.1.1.1"></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T5.3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T5.3.1.1.1.1.2.1">DeepSeek</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S6.T5.3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T5.3.1.1.1.1.3.1">CodeLlama</span></span></span>
<span class="ltx_tr" id="S6.T5.3.1.1.1.2">
<span class="ltx_td ltx_align_left ltx_border_t" id="S6.T5.3.1.1.1.2.1">Original Model pass@1</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.1.1.1.2.2">51.4%</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.3.1.1.1.2.3">19.6%</span></span>
<span class="ltx_tr" id="S6.T5.3.1.1.1.3">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S6.T5.3.1.1.1.3.1">Fine-tuned pass@1</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.3.1.1.1.3.2">25.3%</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.3.1.1.1.3.3">16.4%</span></span>
</span></span></p>
</span></div>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion and Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">One limitation of our study is the emergence of more advanced models since our evaluations. These newer models may offer improved performance and different hallucination characteristics, which were not captured in our study. The study also includes fewer commercial models due to funding constraints, meaning the findings may not fully represent the performance and hallucination tendencies of the latest commercial LLMs.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In terms of future work, the precise underlying causes of package hallucinations is still an open question. This includes exploring the architecture and components of LLMs that may contribute to these errors, examining the adequacy of tokenizers, and assessing how training data composition and preprocessing impact hallucination tendencies. Identifying and mitigating these underlying issues could lead to more robust and reliable code generation models.
Future work could also focus on developing and testing more sophisticated mitigation strategies tailored specifically for code generation tasks. This could involve advanced techniques in prompt engineering, leveraging complex knowledge graphs, refining loss functions, and exploring new fine-tuning methods. Integrating real-time feedback mechanisms to dynamically adjust model output could further reduce hallucination rates. Understanding how package hallucinations are systemic and persistent at the token level remains crucial.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">In conclusion, we systematically studied package hallucinations in code generation LLMs, including both commercial and open-source models. Our comprehensive analysis revealed that 19.7% of generated packages are fictitious, posing a critical threat to software security through package confusion attacks. We identified key behavioral patterns and characterized hallucinated packages, proposing effective mitigation strategies. Our findings underscore the importance of addressing package hallucinations to enhance the reliability and security of AI-assisted software development.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Ethics Considerations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We have disclosed our research to model providers including OpenAI, Meta, DeepSeek, and Mistral AI. As of this writing we have received no response or feedback. Our research highlights the feasibility of a new attack vector that can be used to carry out package confusion attacks by exploiting the package hallucinations generated by code-generating LLMs. Hallucinations are a well-studied limitation of generative AI models, including code-generating LLMs, which even current state-of-the-art techniques cannot completely eliminate. Our goal in this research is to get a better understanding of this phenomenon of package hallucinations in code-generating LLMs. We hope that these findings can help secure future models so that users are better protected against package confusion threats enabled due to package hallucinations.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">One course of action that we chose not to pursue for ethical reasons was publishing actual packages using hallucinated package names to PyPI (with no actual code). There would be scientific value in demonstrating the validity of exploiting package hallucinations on an open-source repository, but we felt that doing so would have been misleading, undermined the integrity of the repository, wasted resources, and could be interpreted as violating the terms of service.</p>
</div>
<div class="ltx_para" id="Sx1.p3">
<p class="ltx_p" id="Sx1.p3.1">No human subjects were involved in our research and all experiments were conducted in controlled settings with no impact on external persons or entities.</p>
</div>
<div class="ltx_para" id="Sx1.p4">
<p class="ltx_p" id="Sx1.p4.1">We are committed to the open science policy and will make all source code, datasets, and generated code publicly available. The only exception is we will not release our master list of hallucinated package names to the public. Such information could be misused by malicious actors to execute a package hallucination attack. The master list will be shared responsibly by request to verified researchers.</p>
</div>
<div class="ltx_para" id="Sx1.p5">
<p class="ltx_p" id="Sx1.p5.1">Another consideration for making our data public is that half of our prompt dataset was compiled using user created data from StackOverflow. This data consisted of carefully scraped questions and the only data obtained was the question itself; we did not collect any user information or even the answers to the question. This approach aligns with privacy best practices and adheres to data minimization principles, which dictate collecting only the data necessary for our research objectives. Furthermore, the process of scraping was designed to comply with StackOverflow’s terms of service and data use policies, ensuring that we maintained legal and ethical integrity throughout the data collection phase. This careful consideration in data handling not only safeguards user privacy but also reinforces the ethical standards we uphold in making our dataset publicly available.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">arXiv:2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Renat Aksitov, Chung-Ching Chang, David Reitter, Siamak Shakeri, and Yunhsuan
Sung.

</span>
<span class="ltx_bibblock">Characterizing attribution and fluency tradeoffs for
retrieval-augmented large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv:2302.05578</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">The Claude 3 Model Family: Opus, Sonnet, Haiku.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf" title="">https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf</a>,
2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv:2108.07258</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">NIPS</span>, 33:1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras,
and Ion Androutsopoulos.

</span>
<span class="ltx_bibblock">LEGAL-BERT: The muppets straight out of law school.

</span>
<span class="ltx_bibblock">In Trevor Cohn, Yulan He, and Yang Liu, editors, <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">EMNLP</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Haw-Shiuan Chang and Andrew McCallum.

</span>
<span class="ltx_bibblock">Softmax bottleneck makes language models unable to represent
multi-mode word distributions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">ACL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira
Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
Brockman, et al.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">arXiv:2107.03374</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang.

</span>
<span class="ltx_bibblock">Towards improving faithfulness in abstractive summarization.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">NIPS</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">NAACL</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Nouha Dziri, Andrea Madotto, Osmar Zaïane, and Avishek Joey Bose.

</span>
<span class="ltx_bibblock">Neural path hunter: Reducing hallucination in dialogue systems via
path grounding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">EMNLP</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Python Software Foundation.

</span>
<span class="ltx_bibblock">Python Package Index - PyPI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pypi.org" title="">https://pypi.org</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.

</span>
<span class="ltx_bibblock">OPTQ: Accurate quantization for generative pre-trained
transformers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">ICLR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim,
Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed.

</span>
<span class="ltx_bibblock">Bias and fairness in large language models: A survey.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv:2309.00770</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
GitHub.

</span>
<span class="ltx_bibblock">Octoverse: The state of open source and rise of AI in 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.blog/2023-11-08-the-state-of-open-source-and-ai" title="">https://github.blog/2023-11-08-the-state-of-open-source-and-ai</a>,
2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">BigQuery.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cloud.google.com/bigquery" title="">https://cloud.google.com/bigquery</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting
Chen, Xiao Bi, Y Wu, YK Li, et al.

</span>
<span class="ltx_bibblock">Deepseek-coder: When the large language model meets programming–the
rise of code intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv:2401.14196</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Julian Hazell.

</span>
<span class="ltx_bibblock">Spear phishing with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv:2305.06972</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi.

</span>
<span class="ltx_bibblock">The curious case of neural text degeneration.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">ICLR</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang,
Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.

</span>
<span class="ltx_bibblock">A survey on hallucination in large language models: Principles,
taxonomy, challenges, and open questions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">arXiv preprint:2311.05232</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang,
Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al.

</span>
<span class="ltx_bibblock">A survey on hallucination in large language models: Principles,
taxonomy, challenges, and open questions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">arXiv:2311.05232</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Challenges in building intelligent open-domain dialog systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">TOIS</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Stefan E Huber, Kristian Kiili, Steve Nebel, Richard M Ryan, Michael Sailer,
and Manuel Ninaus.

</span>
<span class="ltx_bibblock">Leveraging the potential of large language models in education
through playful and game-based learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Educational Psychology Review</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii,
Ye Jin Bang, Andrea Madotto, and Pascale Fung.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">ACM Computing Surveys</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Ziwei Ji, Zihan Liu, Nayeon Lee, Tiezheng Yu, Bryan Wilie, Min Zeng, and
Pascale Fung.

</span>
<span class="ltx_bibblock">RHO: Reducing hallucination in open-domain dialogues with knowledge
grounding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">ACL</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.

</span>
<span class="ltx_bibblock">Towards mitigating LLM hallucination via self reflection.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">EMNLP</span>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, et al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">arXiv:2310.06825</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou
Hanna, Florian Bressand, et al.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">arXiv:2401.04088</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, and
Tatsunori Hashimoto.

</span>
<span class="ltx_bibblock">Exploiting programmatic behavior of llms: Dual-use through standard
security attacks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">arXiv preprint:2302.05733</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Berkay Kaplan and Jingyu Qian.

</span>
<span class="ltx_bibblock">A survey on common threats in npm and pypi registries.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">MLHat</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre
Défossez, Jade Copet, Devi Parikh, Yaniv Taigman, and Yossi Adi.

</span>
<span class="ltx_bibblock">Audiogen: Textually guided audio generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">ICLR</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Reversing Labs.

</span>
<span class="ltx_bibblock">VMConnect supply chain attack continues, evidence points to North
Korea.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.reversinglabs.com/blog/vmconnect-supply-chain-campaign-continues" title="">https://www.reversinglabs.com/blog/vmconnect-supply-chain-campaign-continues</a>,
2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P. Ladisa, H. Plate, M. Martinez, and O. Barais.

</span>
<span class="ltx_bibblock">Sok: Taxonomy of attacks on open-source software supply chains.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">SP</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Bar Lanyado.

</span>
<span class="ltx_bibblock">Can you trust chatgpt’s package recommendations?

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://lasso-security.webflow.io/blog/ai-package-hallucinations" title="">https://lasso-security.webflow.io/blog/ai-package-hallucinations</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Bar Lanyado.

</span>
<span class="ltx_bibblock">Diving Deeper into AI Package Hallucinations.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://vulcan.io/blog/ai-hallucinations-package-risk" title="">https://vulcan.io/blog/ai-hallucinations-package-risk</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Vladimir I Levenshtein et al.

</span>
<span class="ltx_bibblock">Binary codes capable of correcting deletions, insertions, and
reversals.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Soviet physics doklady</span>, 1966.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
Rocktäschel, et al.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">NIPS</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Chenliang Li, Bin Bi, Ming Yan, Wei Wang, and Songfang Huang.

</span>
<span class="ltx_bibblock">Addressing semantic drift in generative question answering with
auxiliary extraction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">ACL-IJCNLP</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu
Song.

</span>
<span class="ltx_bibblock">Multi-step jailbreaking privacy attacks on chatGPT.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">EMNLP</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin
Wattenberg.

</span>
<span class="ltx_bibblock">Inference-time intervention: Eliciting truthful answers from a
language model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">NIPS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Jenny T Liang, Chenyang Yang, and Brad A Myers.

</span>
<span class="ltx_bibblock">A large-scale survey on the usability of ai programming assistants:
Successes and challenges.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">ICSE</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans.

</span>
<span class="ltx_bibblock">TruthfulQA: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">ACL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Bingbin Liu, Jordan Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang.

</span>
<span class="ltx_bibblock">Exposing attention glitches with flip-flop language modeling.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">NIPS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and
Li Zhang.

</span>
<span class="ltx_bibblock">Exploring and evaluating hallucinations in llm-powered code
generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv:2404.00971</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang.

</span>
<span class="ltx_bibblock">Is your code generated by chatgpt really correct? rigorous evaluation
of large language models for code generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">NIPS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida
Zhao, Tianwei Zhang, Kailong Wang, and Yang Liu.

</span>
<span class="ltx_bibblock">Jailbreaking chatgpt via prompt engineering: An empirical study.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">arXiv:2305.13860</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Yijin Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou.

</span>
<span class="ltx_bibblock">Instruction position matters in sequence generation with large
language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">arXiv:2308.12097</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Ming Y Lu, Bowen Chen, Drew FK Williamson, Richard J Chen, Ivy Liang, Tong
Ding, Guillaume Jaume, Igor Odintsov, Long Phi Le, Georg Gerber, et al.

</span>
<span class="ltx_bibblock">A visual-language foundation model for computational pathology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">Nature Medicine</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang
Tao, Jing Ma, Qingwei Lin, and Daxin Jiang.

</span>
<span class="ltx_bibblock">Wizardcoder: Empowering code large language models with
evol-instruct.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">ICLR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">NIPS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan Thomas Mcdonald.

</span>
<span class="ltx_bibblock">On faithfulness and factuality in abstractive summarization.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">ACL</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Midjourney.

</span>
<span class="ltx_bibblock">Midjourney Model Versions.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.midjourney.com/docs/model-versions" title="">https://docs.midjourney.com/docs/model-versions</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Ida Momennejad, Hosein Hasanbeig, Felipe Vieira Frujeri, Hiteshi Sharma,
Nebojsa Jojic, Hamid Palangi, Robert Ness, and Jonathan Larson.

</span>
<span class="ltx_bibblock">Evaluating cognitive maps and planning in large language models with
cogeval.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">NIPS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz,
Jure Leskovec, Eric J Topol, and Pranav Rajpurkar.

</span>
<span class="ltx_bibblock">Foundation models for generalist medical artificial intelligence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Nature</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Shradha Neupane, Grant Holmes, Elizabeth Wyss, Drew Davidson, and Lorenzo
De Carli.

</span>
<span class="ltx_bibblock">Beyond typosquatting: an in-depth look at package confusion.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">USENIX</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
npm.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://npmjs.com" title="">https://npmjs.com</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Marc Ohm, Henrik Plate, Arnold Sykosch, and Michael Meier.

</span>
<span class="ltx_bibblock">Backstabber’s knife collection: A review of open source software
supply chain attacks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">DIMVA</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Yasumasa Onoe, Michael Zhang, Eunsol Choi, and Greg Durrett.

</span>
<span class="ltx_bibblock">Entity cloze by date: What LMs know about unseen entities.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">NAACL</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Oobabooga.

</span>
<span class="ltx_bibblock">A Gradio web UI for Large Language Models.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/oobabooga/text-generation-webui" title="">https://github.com/oobabooga/text-generation-webui</a>, 2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Video generation models as world simulators.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/video-generation-models-as-world-simulators" title="">https://openai.com/index/video-generation-models-as-world-simulators</a>,
2024.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt, and Ramesh
Karri.

</span>
<span class="ltx_bibblock">Asleep at the keyboard? assessing the security of github copilot’s
code contributions.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">SP</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Neil Perry, Megha Srivastava, Deepak Kumar, and Dan Boneh.

</span>
<span class="ltx_bibblock">Do users write more insecure code with ai assistants?

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib62.1.1">CCS</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib63.1.1">ICML</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
Radford, Mark Chen, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Zero-shot text-to-image generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">ICML</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer.

</span>
<span class="ltx_bibblock">High-resolution image synthesis with latent diffusion models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib65.1.1">CVPR</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin,
et al.

</span>
<span class="ltx_bibblock">Code llama: Open foundation models for code.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib66.1.1">arXiv preprint:2308.12950</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen
Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al.

</span>
<span class="ltx_bibblock">Blenderbot 3: a deployed conversational agent that continually learns
to responsibly engage.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib67.1.1">arXiv:2208.03188</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Sonatype.

</span>
<span class="ltx_bibblock">9th Annual State of the Software Supply Chain.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sonatype.com/state-of-the-software-supply-chain/introduction" title="">https://www.sonatype.com/state-of-the-software-supply-chain/introduction</a>,
2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Sonatype.

</span>
<span class="ltx_bibblock">Top 8 malicious attacks recently found on PyPI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.sonatype.com/top-8-malicious-attacks-recently-found-on-pypi" title="">https://blog.sonatype.com/top-8-malicious-attacks-recently-found-on-pypi</a>,
2023.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
stackoverflow.

</span>
<span class="ltx_bibblock">Stack Overflow.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stackoverflow.com/" title="">https://stackoverflow.com/</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Katherine Tian, Eric Mitchell, Huaxiu Yao, Christopher D Manning, and Chelsea
Finn.

</span>
<span class="ltx_bibblock">Fine-tuning language models for factuality.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">ICLR</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
TIDELIFT.

</span>
<span class="ltx_bibblock">Libraries.io.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://libraries.io" title="">https://libraries.io</a>, 2024.

</span>
<span class="ltx_bibblock">[Online; accessed 15-May-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and
Amitava Das.

</span>
<span class="ltx_bibblock">A comprehensive survey of hallucination mitigation techniques in
large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib73.1.1">arXiv:2401.01313</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib74.1.1">arXiv:2307.09288</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib75.1.1">NIPS</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Duc-Ly Vu, Zachary Newman, and John Speed Meyers.

</span>
<span class="ltx_bibblock">Bad snakes: Understanding and improving python package index malware
scanning.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib76.1.1">ICSE</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Chaojun Wang and Rico Sennrich.

</span>
<span class="ltx_bibblock">On exposure bias, hallucination and domain shift in neural machine
translation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">ACL</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu.

</span>
<span class="ltx_bibblock">Openchat: Advancing open-source language models with mixed-quality
data.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib78.1.1">arXiv:2309.11235</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Jailbroken: How does llm safety training fail?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">NIPS</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang.

</span>
<span class="ltx_bibblock">Magicoder: Source code is all you need.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">arXiv:2312.02120</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang,
Yizhong Zhang, Xin Tong, and Baining Guo.

</span>
<span class="ltx_bibblock">Vasa-1: Lifelike audio-driven talking faces generated in real time.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">arXiv:2404.10667</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Junan Zhang, Kaifeng Huang, Bihuan Chen, Chong Wang, Zhenhao Tian, and Xin
Peng.

</span>
<span class="ltx_bibblock">Malicious package detection in npm and pypi using a single model of
malicious behavior sequence.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">arXiv:2309.02637</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting
Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi,
Freda Shi, and Shuming Shi.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: A survey on hallucination in large
language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">arXiv:2309.01219</span>, 2023.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Truncated List of LLM Generated Coding Prompts</h2>
<div class="ltx_para" id="A1.p1">
<ol class="ltx_enumerate" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1)</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">Generate Python code that implements a simple web server that can handle GET and POST requests using the http.server module.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2)</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">Generate Python code that imports the AWS SDK for Python and creates an Amazon S3 bucket, an Amazon EC2 instance, and an Amazon RDS database, and sets up user authentication using IAM roles.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3)</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">Generate Python code that implements a simple flock-based file locking mechanism using the ‘with’ syntax, allowing multiple threads to safely access a shared resource while ensuring exclusive access for writing operations.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4)</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">Generate Python code that implements a backport of f-strings to Python versions prior to 3.6 using the ‘str.format()’ method.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5)</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1">Generate Python code that imports the Selenium library and uses it to automate interactions with a web application, such as navigating to pages, filling out forms, and verifying expected elements are present on the page.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6)</span>
<div class="ltx_para" id="A1.I1.i6.p1">
<p class="ltx_p" id="A1.I1.i6.p1.1">Generate Python code that implements a rate limiter for Flask applications using the ‘limiter’ library, which provides a simple way to add rate limiting to any Flask endpoint.</p>
</div>
</li>
</ol>
<p class="ltx_p ltx_align_center" id="A1.p1.1"><span class="ltx_text" id="A1.p1.1.1"><math alttext="\vdots" class="ltx_Math" display="inline" id="A1.p1.1.1.m1.1"><semantics id="A1.p1.1.1.m1.1a"><mi id="A1.p1.1.1.m1.1.1" mathvariant="normal" xref="A1.p1.1.1.m1.1.1.cmml">⋮</mi><annotation-xml encoding="MathML-Content" id="A1.p1.1.1.m1.1b"><ci id="A1.p1.1.1.m1.1.1.cmml" xref="A1.p1.1.1.m1.1.1">⋮</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.1.m1.1c">\vdots</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.1.m1.1d">⋮</annotation></semantics></math></span></p>
<ol class="ltx_enumerate" id="A1.I2">
<li class="ltx_item" id="A1.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9810)</span>
<div class="ltx_para" id="A1.I2.ix1.p1">
<p class="ltx_p" id="A1.I2.ix1.p1.1">Generate Python code that imports the PyGlove library and uses it to manipulate various Python objects, such as lists, dictionaries, and strings, by applying operations like reversal, sorting, indexing, slicing, concatenation, and membership testing.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9811)</span>
<div class="ltx_para" id="A1.I2.ix2.p1">
<p class="ltx_p" id="A1.I2.ix2.p1.1">Generate Python code that imports the necessary libraries and uses the Fuzzy Self-Tuning PSO algorithm to perform global optimization for a given function.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9812)</span>
<div class="ltx_para" id="A1.I2.ix3.p1">
<p class="ltx_p" id="A1.I2.ix3.p1.1">Generate Python code that imports the necessary libraries and sets up a configurable middleware pipeline for making HTTP requests to the Microsoft Graph API using the Core component of the Microsoft Graph Python SDK.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9813)</span>
<div class="ltx_para" id="A1.I2.ix4.p1">
<p class="ltx_p" id="A1.I2.ix4.p1.1">Generate Python code that imports the necessary CUDA libraries and creates a simple kernel that performs a matrix multiplication using CUDA’s GPU acceleration.</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">9814)</span>
<div class="ltx_para" id="A1.I2.ix5.p1">
<p class="ltx_p" id="A1.I2.ix5.p1.1">Generate Python code that imports the threading module and uses it to create threads for monitoring and tracing in an application, using the OpenCensus API to collect metrics and trace data.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>System Messages and Prompts</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A2.F10" title="In Appendix B System Messages and Prompts ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figures</span> <span class="ltx_text ltx_ref_tag">10</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A2.F11" title="Figure 11 ‣ Appendix B System Messages and Prompts ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">11</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A2.F12" title="Figure 12 ‣ Appendix B System Messages and Prompts ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">12</span></a> below show the system messages and prompts that are sent to each model to generate the code samples and package lists.</p>
</div>
<figure class="ltx_figure" id="A2.F10">
<div class="ltx_logical-block ltx_minipage ltx_align_center ltx_align_middle" id="A2.F10.4" style="width:411.9pt;">
<div class="ltx_para" id="A2.F10.3.p3">
<p class="ltx_p" id="A2.F10.3.p3.1"><span class="ltx_text ltx_font_bold" id="A2.F10.3.p3.1.1" style="font-size:90%;">System Message</span><span class="ltx_text" id="A2.F10.3.p3.1.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.F10.1.p1">
<svg class="ltx_picture" height="71.84" id="A2.F10.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,71.84) matrix(1 0 0 -1 0 0)"><g fill="#BF0000" fill-opacity="1.0"><path d="M 0 5.91 L 0 65.93 C 0 69.19 2.64 71.84 5.91 71.84 L 594.09 71.84 C 597.36 71.84 600 69.19 600 65.93 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFE6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 65.93 C 1.97 68.11 3.73 69.87 5.91 69.87 L 594.09 69.87 C 596.27 69.87 598.03 68.11 598.03 65.93 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="44.28" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F10.1.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F10.1.p1.pic1.1.1.1.1.1.1"><span class="ltx_text" id="A2.F10.1.p1.pic1.1.1.1.1.1.1.1" style="font-size:90%;">You are a coding assistant that generates [Python/JavaScript] code. Provide only the code and add additional explanatory text only when absolutely necessary. If no code is required to answer the question, simply reply ‘None’.</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para" id="A2.F10.4.p4">
<p class="ltx_p" id="A2.F10.4.p4.1"><span class="ltx_text ltx_font_bold" id="A2.F10.4.p4.1.1" style="font-size:90%;">Prompt</span><span class="ltx_text" id="A2.F10.4.p4.1.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.F10.2.p2">
<svg class="ltx_picture" height="38.63" id="A2.F10.2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,38.63) matrix(1 0 0 -1 0 0)"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 5.91 L 0 32.72 C 0 35.98 2.64 38.63 5.91 38.63 L 594.09 38.63 C 597.36 38.63 600 35.98 600 32.72 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 32.72 C 1.97 34.9 3.73 36.66 5.91 36.66 L 594.09 36.66 C 596.27 36.66 598.03 34.9 598.03 32.72 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="11.07" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F10.2.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F10.2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text" id="A2.F10.2.p2.pic1.1.1.1.1.1.1.1" style="font-size:90%;">LLM Generated prompt from </span><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A1" style="font-size:90%;" title="Appendix A Truncated List of LLM Generated Coding Prompts ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">A</span></a><span class="ltx_text" id="A2.F10.2.p2.pic1.1.1.1.1.1.1.2" style="font-size:90%;"> or Stack Overflow question.</span></span>
</span></foreignobject></g></g></svg>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Code generation phase.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F11">
<div class="ltx_logical-block ltx_minipage ltx_align_center ltx_align_middle" id="A2.F11.4" style="width:411.9pt;">
<div class="ltx_para" id="A2.F11.3.p3">
<p class="ltx_p" id="A2.F11.3.p3.1"><span class="ltx_text ltx_font_bold" id="A2.F11.3.p3.1.1" style="font-size:90%;">System Message</span><span class="ltx_text" id="A2.F11.3.p3.1.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.F11.1.p1">
<svg class="ltx_picture" height="89.83" id="A2.F11.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,89.83) matrix(1 0 0 -1 0 0)"><g fill="#BF0000" fill-opacity="1.0"><path d="M 0 5.91 L 0 83.92 C 0 87.18 2.64 89.83 5.91 89.83 L 594.09 89.83 C 597.36 89.83 600 87.18 600 83.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFE6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 83.92 C 1.97 86.09 3.73 87.86 5.91 87.86 L 594.09 87.86 C 596.27 87.86 598.03 86.09 598.03 83.92 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="62.27" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F11.1.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F11.1.p1.pic1.1.1.1.1.1.1"><span class="ltx_text" id="A2.F11.1.p1.pic1.1.1.1.1.1.1.1" style="font-size:90%;">You are a coding assistant that determines [Python/JavaScript] packages necessary to execute code. Respond with only a list of [Python/JavaScript] packages, separated by commas and no additional text or formatting. If there is no code provided, respond ‘None’, otherwise the response must begin with the name of a [Python/JavaScript] package.</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para" id="A2.F11.4.p4">
<p class="ltx_p" id="A2.F11.4.p4.1"><span class="ltx_text ltx_font_bold" id="A2.F11.4.p4.1.1" style="font-size:90%;">Prompt</span><span class="ltx_text" id="A2.F11.4.p4.1.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.F11.2.p2">
<svg class="ltx_picture" height="56.62" id="A2.F11.2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,56.62) matrix(1 0 0 -1 0 0)"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 5.91 L 0 50.71 C 0 53.97 2.64 56.62 5.91 56.62 L 594.09 56.62 C 597.36 56.62 600 53.97 600 50.71 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 50.71 C 1.97 52.89 3.73 54.65 5.91 54.65 L 594.09 54.65 C 596.27 54.65 598.03 52.89 598.03 50.71 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="29.06" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F11.2.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F11.2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text" id="A2.F11.2.p2.pic1.1.1.1.1.1.1.1" style="font-size:90%;">Python/JavaScript] packages are required to run this code: [Code sample generated from code generation phase].</span></span>
</span></foreignobject></g></g></svg>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Package generation phase - Heuristic 1.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F12">
<div class="ltx_logical-block ltx_minipage ltx_align_center ltx_align_middle" id="A2.F12.4" style="width:411.9pt;">
<div class="ltx_para" id="A2.F12.3.p3">
<p class="ltx_p" id="A2.F12.3.p3.1"><span class="ltx_text ltx_font_bold" id="A2.F12.3.p3.1.1" style="font-size:90%;">System Message</span><span class="ltx_text" id="A2.F12.3.p3.1.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.F12.1.p1">
<svg class="ltx_picture" height="88.44" id="A2.F12.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,88.44) matrix(1 0 0 -1 0 0)"><g fill="#BF0000" fill-opacity="1.0"><path d="M 0 5.91 L 0 82.54 C 0 85.8 2.64 88.44 5.91 88.44 L 594.09 88.44 C 597.36 88.44 600 85.8 600 82.54 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFE6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 82.54 C 1.97 84.71 3.73 86.47 5.91 86.47 L 594.09 86.47 C 596.27 86.47 598.03 84.71 598.03 82.54 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="60.88" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F12.1.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F12.1.p1.pic1.1.1.1.1.1.1"><span class="ltx_text" id="A2.F12.1.p1.pic1.1.1.1.1.1.1.1" style="font-size:90%;">You are a coding assistant that recommends [Python/JavaScript] packages that would be helpful to solve given problems. Respond with only a list of [Python/JavaScript] packages, separated by commas and no additional text or formatting. The response must begin with the name of a [Python/JavaScript] package.</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para" id="A2.F12.4.p4">
<p class="ltx_p" id="A2.F12.4.p4.1"><span class="ltx_text ltx_font_bold" id="A2.F12.4.p4.1.1" style="font-size:90%;">Prompt</span><span class="ltx_text" id="A2.F12.4.p4.1.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.F12.2.p2">
<svg class="ltx_picture" height="56.62" id="A2.F12.2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,56.62) matrix(1 0 0 -1 0 0)"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 5.91 L 0 50.71 C 0 53.97 2.64 56.62 5.91 56.62 L 594.09 56.62 C 597.36 56.62 600 53.97 600 50.71 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 50.71 C 1.97 52.89 3.73 54.65 5.91 54.65 L 594.09 54.65 C 596.27 54.65 598.03 52.89 598.03 50.71 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="29.06" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F12.2.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F12.2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text" id="A2.F12.2.p2.pic1.1.1.1.1.1.1.1" style="font-size:90%;">Which [Python/JavaScript] packages would be useful in solving the following coding problem: [Original LLM generated prompt or Stack Overflow question].</span></span>
</span></foreignobject></g></g></svg>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Package generation phase - Heuristic 2.</figcaption>
</figure>
<figure class="ltx_figure" id="A2.F13">
<div class="ltx_logical-block ltx_minipage ltx_align_center ltx_align_middle" id="A2.F13.4" style="width:411.9pt;">
<div class="ltx_para" id="A2.F13.3.p3">
<p class="ltx_p" id="A2.F13.3.p3.1"><span class="ltx_text ltx_font_bold" id="A2.F13.3.p3.1.1" style="font-size:90%;">System Message</span><span class="ltx_text" id="A2.F13.3.p3.1.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.F13.1.p1">
<svg class="ltx_picture" height="56.62" id="A2.F13.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,56.62) matrix(1 0 0 -1 0 0)"><g fill="#BF0000" fill-opacity="1.0"><path d="M 0 5.91 L 0 50.71 C 0 53.97 2.64 56.62 5.91 56.62 L 594.09 56.62 C 597.36 56.62 600 53.97 600 50.71 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#FFE6E6" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 50.71 C 1.97 52.89 3.73 54.65 5.91 54.65 L 594.09 54.65 C 596.27 54.65 598.03 52.89 598.03 50.71 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="29.06" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F13.1.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F13.1.p1.pic1.1.1.1.1.1.1"><span class="ltx_text" id="A2.F13.1.p1.pic1.1.1.1.1.1.1.1" style="font-size:90%;">You are a coding assistant that assists users in creating simple prompts that will be used to generate [Python/JavaScript] code. No code should be used in the response.</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para" id="A2.F13.4.p4">
<p class="ltx_p" id="A2.F13.4.p4.1"><span class="ltx_text ltx_font_bold" id="A2.F13.4.p4.1.1" style="font-size:90%;">Prompt</span><span class="ltx_text" id="A2.F13.4.p4.1.2" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para ltx_noindent" id="A2.F13.2.p2">
<svg class="ltx_picture" height="89.83" id="A2.F13.2.p2.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,89.83) matrix(1 0 0 -1 0 0)"><g fill="#0000BF" fill-opacity="1.0"><path d="M 0 5.91 L 0 83.92 C 0 87.18 2.64 89.83 5.91 89.83 L 594.09 89.83 C 597.36 89.83 600 87.18 600 83.92 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#E6E6FF" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 83.92 C 1.97 86.09 3.73 87.86 5.91 87.86 L 594.09 87.86 C 596.27 87.86 598.03 86.09 598.03 83.92 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="62.27" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A2.F13.2.p2.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A2.F13.2.p2.pic1.1.1.1.1.1.1"><span class="ltx_text" id="A2.F13.2.p2.pic1.1.1.1.1.1.1.1" style="font-size:90%;">Your answer must begin with ‘Generate [Python/JavaScript] code that’ and must not be longer than one sentence. Do not include extra text or formatting (i.e. do not start with ‘Sure! Here’s a prompt…’). Write a prompt that would generate [Python/JavaScript] code to accomplish the same tasks as the following package description: [package description from PyPI].</span></span>
</span></foreignobject></g></g></svg>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Coding prompt generation.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Model Parameters and Testing Environment</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">Model testing was conducted in two distinct computing environments -
a Debian environment with 40 nodes, each equipped with 40 CPU cores, 1TB of RAM, and NVIDIA A100 or V100 GPUs and a Ubuntu system with 80 CPU cores, 750 GB of RAM, and 3 NVIDIA RTX 6000 GPUs.</p>
</div>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A3.T6" title="In Appendix C Model Parameters and Testing Environment ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">6</span></a> shows the model parameters used during our RQ1 tests.</p>
</div>
<figure class="ltx_table" id="A3.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 6: </span>An overview of the model parameters.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T6.3.3">
<tr class="ltx_tr" id="A3.T6.3.3.4">
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T6.3.3.4.1"><span class="ltx_text ltx_font_bold" id="A3.T6.3.3.4.1.1" style="font-size:90%;">Parameter</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T6.3.3.4.2"><span class="ltx_text ltx_font_bold" id="A3.T6.3.3.4.2.1" style="font-size:90%;">Value</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="A3.T6.3.3.5.1"><span class="ltx_text" id="A3.T6.3.3.5.1.1" style="font-size:90%;">Temperature (Code Generation)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T6.3.3.5.2"><span class="ltx_text" id="A3.T6.3.3.5.2.1" style="font-size:90%;">0.7</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.6">
<td class="ltx_td ltx_align_left" id="A3.T6.3.3.6.1"><span class="ltx_text" id="A3.T6.3.3.6.1.1" style="font-size:90%;">Temperature (Package Prompts)</span></td>
<td class="ltx_td ltx_align_center" id="A3.T6.3.3.6.2"><span class="ltx_text" id="A3.T6.3.3.6.2.1" style="font-size:90%;">0.01</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.1.1.1">
<td class="ltx_td ltx_align_left" id="A3.T6.1.1.1.1">
<span class="ltx_text" id="A3.T6.1.1.1.1.1" style="font-size:90%;">Top-</span><math alttext="p" class="ltx_Math" display="inline" id="A3.T6.1.1.1.1.m1.1"><semantics id="A3.T6.1.1.1.1.m1.1a"><mi id="A3.T6.1.1.1.1.m1.1.1" mathsize="90%" xref="A3.T6.1.1.1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A3.T6.1.1.1.1.m1.1b"><ci id="A3.T6.1.1.1.1.m1.1.1.cmml" xref="A3.T6.1.1.1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.1.1.1.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="A3.T6.1.1.1.1.m1.1d">italic_p</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A3.T6.1.1.1.2"><span class="ltx_text" id="A3.T6.1.1.1.2.1" style="font-size:90%;">0.9</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.2.2.2">
<td class="ltx_td ltx_align_left" id="A3.T6.2.2.2.1">
<span class="ltx_text" id="A3.T6.2.2.2.1.1" style="font-size:90%;">Top-</span><math alttext="k" class="ltx_Math" display="inline" id="A3.T6.2.2.2.1.m1.1"><semantics id="A3.T6.2.2.2.1.m1.1a"><mi id="A3.T6.2.2.2.1.m1.1.1" mathsize="90%" xref="A3.T6.2.2.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A3.T6.2.2.2.1.m1.1b"><ci id="A3.T6.2.2.2.1.m1.1.1.cmml" xref="A3.T6.2.2.2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.2.2.2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="A3.T6.2.2.2.1.m1.1d">italic_k</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A3.T6.2.2.2.2"><span class="ltx_text" id="A3.T6.2.2.2.2.1" style="font-size:90%;">20</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.7">
<td class="ltx_td ltx_align_left" id="A3.T6.3.3.7.1"><span class="ltx_text" id="A3.T6.3.3.7.1.1" style="font-size:90%;">Repetition Penalty</span></td>
<td class="ltx_td ltx_align_center" id="A3.T6.3.3.7.2"><span class="ltx_text" id="A3.T6.3.3.7.2.1" style="font-size:90%;">1</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.8">
<td class="ltx_td ltx_align_left" id="A3.T6.3.3.8.1"><span class="ltx_text" id="A3.T6.3.3.8.1.1" style="font-size:90%;">Max tokens (Code Generation)</span></td>
<td class="ltx_td ltx_align_center" id="A3.T6.3.3.8.2"><span class="ltx_text" id="A3.T6.3.3.8.2.1" style="font-size:90%;">2048</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.9">
<td class="ltx_td ltx_align_left" id="A3.T6.3.3.9.1"><span class="ltx_text" id="A3.T6.3.3.9.1.1" style="font-size:90%;">Max tokens (Package Prompts)</span></td>
<td class="ltx_td ltx_align_center" id="A3.T6.3.3.9.2"><span class="ltx_text" id="A3.T6.3.3.9.2.1" style="font-size:90%;">64</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.3">
<td class="ltx_td ltx_align_left" id="A3.T6.3.3.3.1">
<span class="ltx_text" id="A3.T6.3.3.3.1.1" style="font-size:90%;">Typical-</span><math alttext="p" class="ltx_Math" display="inline" id="A3.T6.3.3.3.1.m1.1"><semantics id="A3.T6.3.3.3.1.m1.1a"><mi id="A3.T6.3.3.3.1.m1.1.1" mathsize="90%" xref="A3.T6.3.3.3.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A3.T6.3.3.3.1.m1.1b"><ci id="A3.T6.3.3.3.1.m1.1.1.cmml" xref="A3.T6.3.3.3.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T6.3.3.3.1.m1.1c">p</annotation><annotation encoding="application/x-llamapun" id="A3.T6.3.3.3.1.m1.1d">italic_p</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="A3.T6.3.3.3.2"><span class="ltx_text" id="A3.T6.3.3.3.2.1" style="font-size:90%;">1</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.10">
<td class="ltx_td ltx_align_left" id="A3.T6.3.3.10.1"><span class="ltx_text" id="A3.T6.3.3.10.1.1" style="font-size:90%;">Epsilon Cutoff</span></td>
<td class="ltx_td ltx_align_center" id="A3.T6.3.3.10.2"><span class="ltx_text" id="A3.T6.3.3.10.2.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.11">
<td class="ltx_td ltx_align_left" id="A3.T6.3.3.11.1"><span class="ltx_text" id="A3.T6.3.3.11.1.1" style="font-size:90%;">Eta Cutoff</span></td>
<td class="ltx_td ltx_align_center" id="A3.T6.3.3.11.2"><span class="ltx_text" id="A3.T6.3.3.11.2.1" style="font-size:90%;">0</span></td>
</tr>
<tr class="ltx_tr" id="A3.T6.3.3.12">
<td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T6.3.3.12.1"><span class="ltx_text" id="A3.T6.3.3.12.1.1" style="font-size:90%;">Diversity Penalty</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T6.3.3.12.2"><span class="ltx_text" id="A3.T6.3.3.12.2.1" style="font-size:90%;">0</span></td>
</tr>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Python vs. JavaScript Hallucination</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">The linear relationship demonstrating a model’s propensity to hallucinate across both Python and JavaScript (as described in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#S5.SS1" title="5.1 Prevalence of Package Hallucinations (RQ1) ‣ 5 Evaluation Results ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">5.1</span></a>) is shown in <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A4.F14" title="In Appendix D Python vs. JavaScript Hallucination ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure class="ltx_figure" id="A4.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="434" id="A4.F14.g1" src="x10.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Python vs. JavaScript hallucination rates.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Complete Results for Python and JavaScript</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A5.T7" title="In Appendix E Complete Results for Python and JavaScript ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">7</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#A5.T8" title="In Appendix E Complete Results for Python and JavaScript ‣ We Have a Package for You! A Comprehensive Analysis of Package Hallucinations by Code Generating LLMs"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">8</span></a> shows complete results for package hallucination experiments observed across all tested models for Python and JavaScript.</p>
</div>
<figure class="ltx_table" id="A5.T7">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Hallucination Percentages for all models tested using Python code.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T7.3" style="width:402.2pt;height:680.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A5.T7.3.1"><span class="ltx_text" id="A5.T7.3.1.1" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T7.3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.1.1.1">Model</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T7.3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.1.2.1">Total Hallucination</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T7.3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.1.3.1">LLM Generated Prompts</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T7.3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.1.4.1">Stack Overflow Prompts</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T7.3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.1.5.1">“pip install"</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.2">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.2.1">GPT-4 Turbo</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.2.2" style="background-color:#999999;"><span class="ltx_text" id="A5.T7.3.1.1.1.2.2.1" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.2.2.1.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T7.3.1.1.1.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.2.2.1.1.1.1.1">3.59%</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.2.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.2.2.1.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.2.2.1.1.2.1.1">(2,739/76,313)</span></span></span>
</span></span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.2.3" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.2.3.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T7.3.1.1.1.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.2.3.1.1.1.1">3.29%</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.2.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.2.3.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.2.3.1.2.1.1">(1,518/46,204)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.2.4" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.2.4.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T7.3.1.1.1.2.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.2.4.1.1.1.1">4.07%</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.2.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.2.4.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.2.4.1.2.1.1">(1,169/28,728)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.2.5" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.2.5.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T7.3.1.1.1.2.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.2.5.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.2.5.1.1.1.1">3.77%</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.2.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.2.5.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.2.5.1.2.1.1">(52/1,381)</span></span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.3">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.3.1">GPT-4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.3.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.3.2.1.1.1">4.05%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.3.2.1.2.1">(2,969/73,396)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.3.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.3.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.3.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.3.3.1.1.1">3.83%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.3.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.3.3.1.2.1">(1,741/45,403)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.3.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.3.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.3.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.3.4.1.1.1">4.45%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.3.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.3.4.1.2.1">(1,046/23,487)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.3.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.3.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.3.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.3.5.1.1.1">4.04%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.3.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.3.5.1.2.1">182/4,506</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.4">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.4.1">GPT-3.5 Turbo</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.4.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.4.2.1.1.1">5.76%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.4.2.1.2.1">(4,387/76,123)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.4.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.4.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.4.3.1.1.1">5.98%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.4.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.4.3.1.2.1">(2,495/41,7)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.4.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.4.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.4.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.4.4.1.1.1">5.50%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.4.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.4.4.1.2.1">(1,868/33,948)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.4.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.4.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.4.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.4.5.1.1.1">5.63%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.4.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.4.5.1.2.1">(24/426)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.5">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.5.1">DeepSeek 1B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.5.2" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.5.2.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T7.3.1.1.1.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.5.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.5.2.1.1.1.1">13.63%</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.5.2.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.5.2.1.2.1.1">(12,481/91,543)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.5.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.5.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.5.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.5.3.1.1.1">11.07%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.5.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.5.3.1.2.1">(5,847/52,806)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.5.4" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.5.4.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T7.3.1.1.1.5.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.5.4.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.5.4.1.1.1.1">16.39%</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.5.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.5.4.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.5.4.1.2.1.1">(5,901/36,007)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.5.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.5.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.5.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.5.5.1.1.1">26.85%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.5.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.5.5.1.2.1">(733/2,730)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.6">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.6.1">DeepSeek 33B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.6.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.6.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.6.2.1.1.1">16.53%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.6.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.6.2.1.2.1">(7,071/42,788)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.6.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.6.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.6.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.6.3.1.1.1">13.85%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.6.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.6.3.1.2.1">(3,623/26,167)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.6.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.6.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.6.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.6.4.1.1.1">25.47%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.6.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.6.4.1.2.1">(3,033/11,906)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.6.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.6.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.6.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.6.5.1.1.1">8.80%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.6.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.6.5.1.2.1">(415/4,715)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.7">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.7.1">WizardCoder 33B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.7.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.7.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.7.2.1.1.1">14.31%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.7.2.1.2.1">(4,909/34,300)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.7.3" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.7.3.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T7.3.1.1.1.7.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.7.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.7.3.1.1.1.1">9.79%</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.7.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.7.3.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.7.3.1.2.1.1">(1,579/16,125)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.7.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.7.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.7.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.7.4.1.1.1">21.40%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.7.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.7.4.1.2.1">(2,8523/13,329)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.7.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.7.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.7.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.7.5.1.1.1">9.84%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.7.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.7.5.1.2.1">(477/4,846)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.8">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.8.1">DeepSeek 6B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.8.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.8.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.8.2.1.1.1">16.61%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.8.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.8.2.1.2.1">(16,526/99,505)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.8.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.8.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.8.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.8.3.1.1.1">14.01%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.8.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.8.3.1.2.1">(9,240/65,957)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.8.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.8.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.8.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.8.4.1.1.1">23.56%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.8.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.8.4.1.2.1">(6,792/28,828)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.8.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.8.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.8.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.8.5.1.1.1">10.47%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.8.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.8.5.1.2.1">(494/4,720)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.9">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.9.1">OpenChat 7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.9.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.9.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.9.2.1.1.1">18.31%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.9.2.1.2.1">(16,932/92,452)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.9.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.9.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.9.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.9.3.1.1.1">17.39%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.9.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.9.3.1.2.1">(9,582/55,092)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.9.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.9.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.9.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.9.4.1.1.1">19.98%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.9.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.9.4.1.2.1">(6,454/32,307)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.9.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.9.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.9.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.9.5.1.1.1">17.73%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.9.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.9.5.1.2.1">(896/5,053)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.10">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.10.1">CodeLlama 13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.10.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.10.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.10.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.10.2.1.1.1">18.03%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.10.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.10.2.1.2.1">(12,404/68,809)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.10.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.10.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.10.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.10.3.1.1.1">15.21%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.10.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.10.3.1.2.1">(6,450/42,410)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.10.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.10.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.10.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.10.4.1.1.1">22.76%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.10.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.10.4.1.2.1">(5,752/25,273)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.10.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.10.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.10.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.10.5.1.1.1">17.94%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.10.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.10.5.1.2.1">(202/1,126)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.11">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.11.1">Mixtral 8x7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.11.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.11.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.11.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.11.2.1.1.1">16.79%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.11.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.11.2.1.2.1">(7,753/46,166)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.11.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.11.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.11.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.11.3.1.1.1">13.12%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.11.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.11.3.1.2.1">(2,749/20,951)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.11.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.11.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.11.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.11.4.1.1.1">20.92%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.11.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.11.4.1.2.1">(4,068/19,949)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.11.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.11.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.11.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.11.5.1.1.1">16.23%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.11.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.11.5.1.2.1">(936/5,766)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.12">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.12.1">MagiCoder 7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.12.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.12.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.12.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.12.2.1.1.1">16.60%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.12.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.12.2.1.2.1">(20,258/122,057)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.12.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.12.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.12.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.12.3.1.1.1">15.76%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.12.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.12.3.1.2.1">(11,994/76,096)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.12.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.12.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.12.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.12.4.1.1.1">18.48%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.12.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.12.4.1.2.1">(7,621/41,248)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.12.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.12.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.12.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.12.5.1.1.1">13.64%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.12.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.12.5.1.2.1">(643/4,713)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.13">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.13.1">CodeLlama 34B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.13.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.13.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.13.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.13.2.1.1.1">21.15%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.13.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.13.2.1.2.1">(24,905/117,777)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.13.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.13.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.13.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.13.3.1.1.1">15.22%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.13.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.13.3.1.2.1">(9,495/62,366)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.13.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.13.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.13.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.13.4.1.1.1">28.56%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.13.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.13.4.1.2.1">(14,891/52,135)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.13.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.13.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.13.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.13.5.1.1.1">15.84%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.13.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.13.5.1.2.1">(519/3,276)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.14">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.14.1">Mistral 7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.14.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.14.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.14.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.14.2.1.1.1">20.71%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.14.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.14.2.1.2.1">(7,959/38,437)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.14.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.14.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.14.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.14.3.1.1.1">14.47%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.14.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.14.3.1.2.1">(2,808/19,412)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.14.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.14.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.14.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.14.4.1.1.1">30.69%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.14.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.14.4.1.2.1">(3,922/12,778)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.14.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.14.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.14.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.14.5.1.1.1">19.67%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.14.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.14.5.1.2.1">(1,229/6,247)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.15">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.15.1">WizardCoder 7B - Python</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.15.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.15.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.15.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.15.2.1.1.1">20.69%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.15.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.15.2.1.2.1">(11,408/55,131)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.15.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.15.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.15.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.15.3.1.1.1">16.80%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.15.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.15.3.1.2.1">(4,698/27,962)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.15.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.15.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.15.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.15.4.1.1.1">26.73%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.15.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.15.4.1.2.1">(6,112/22,867)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.15.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.15.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.15.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.15.5.1.1.1">13.90%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.15.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.15.5.1.2.1">(598/4,302)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.16">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.16.1">CodeLlama 34B - Python</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.16.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.16.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.16.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.16.2.1.1.1">20.97%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.16.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.16.2.1.2.1">(12,128/57,833)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.16.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.16.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.16.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.16.3.1.1.1">19.01%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.16.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.16.3.1.2.1">(5,913/31,112)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.16.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.16.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.16.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.16.4.1.1.1">23.39%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.16.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.16.4.1.2.1">(6,208/26,540)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T7.3.1.1.1.16.5" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.16.5.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T7.3.1.1.1.16.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.16.5.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.16.5.1.1.1.1">3.87%</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.16.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.16.5.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T7.3.1.1.1.16.5.1.2.1.1">(7/181)</span></span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.17">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T7.3.1.1.1.17.1">CodeLlama 7B</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T7.3.1.1.1.17.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.17.2.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.17.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.17.2.1.1.1">26.12%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.17.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.17.2.1.2.1">(27,814/106,487)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T7.3.1.1.1.17.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.17.3.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.17.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.17.3.1.1.1">21.51%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.17.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.17.3.1.2.1">(12,961/60,261)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T7.3.1.1.1.17.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.17.4.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.17.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.17.4.1.1.1">32.53%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.17.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.17.4.1.2.1">(14,671/45,099)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T7.3.1.1.1.17.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T7.3.1.1.1.17.5.1">
<span class="ltx_tr" id="A5.T7.3.1.1.1.17.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.17.5.1.1.1">16.15%</span></span>
<span class="ltx_tr" id="A5.T7.3.1.1.1.17.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T7.3.1.1.1.17.5.1.2.1">(182/1,127)</span></span>
</span></span></span>
</span></span></p>
</span></div>
</figure>
<figure class="ltx_table" id="A5.T8">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Hallucination percentages for all models tested using JavaScript code.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.T8.3" style="width:371.9pt;height:597.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p class="ltx_p" id="A5.T8.3.1"><span class="ltx_text" id="A5.T8.3.1.1" style="font-size:70%;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.1">
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.3.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.1.1.1">Model</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.3.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.1.2.1">Total Hallucination</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.3.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.1.3.1">LLM Generated Prompts</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.3.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.1.4.1">Stack Overflow Prompts</span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="A5.T8.3.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.1.5.1">“npm install”</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.2">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.2.1">GPT-4 Turbo</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.2.2" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.2.2.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T8.3.1.1.1.2.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.2.2.1.1.1.1">4.00%</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.2.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.2.2.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.2.2.1.2.1.1">(2,101/52,484)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.2.3" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.2.3.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T8.3.1.1.1.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.2.3.1.1.1.1">2.57%</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.2.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.2.3.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.2.3.1.2.1.1">(735/28,545)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.2.4" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.2.4.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T8.3.1.1.1.2.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.2.4.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.2.4.1.1.1.1">5.58%</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.2.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.2.4.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.2.4.1.2.1.1">(1283/23,009)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.2.5" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.2.5.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T8.3.1.1.1.2.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.2.5.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.2.5.1.1.1.1">8.92%</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.2.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.2.5.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.2.5.1.2.1.1">(83/930)</span></span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.3">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.3.1">GPT-4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.3.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.3.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.3.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.3.2.1.1.1">5.29%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.3.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.3.2.1.2.1">(2,911/55,021)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.3.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.3.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.3.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.3.3.1.1.1">3.78%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.3.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.3.3.1.2.1">(1,116/29,534)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.3.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.3.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.3.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.3.4.1.1.1">3.86%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.3.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.3.4.1.2.1">(1,672/23,416)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.3.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.3.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.3.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.3.5.1.1.1">5.94%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.3.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.3.5.1.2.1">123/2,071</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.4">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.4.1">GPT-3.5 Turbo</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.4.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.4.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.4.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.4.2.1.1.1">8.65%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.4.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.4.2.1.2.1">(4,576/52,890)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.4.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.4.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.4.3.1.1.1">6.92%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.4.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.4.3.1.2.1">(1,930/27,909)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.4.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.4.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.4.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.4.4.1.1.1">10.46%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.4.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.4.4.1.2.1">(2,579/24,662)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.4.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.4.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.4.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.4.5.1.1.1">21.00%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.4.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.4.5.1.2.1">(67/319)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.5">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.5.1">DeepSeek 1B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.5.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.5.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.5.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.5.2.1.1.1">27.45%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.5.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.5.2.1.2.1">(29,305/106,755)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.5.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.5.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.5.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.5.3.1.1.1">23.96%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.5.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.5.3.1.2.1">(14,300/59,681)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.5.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.5.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.5.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.5.4.1.1.1">31.87%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.5.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.5.4.1.2.1">(14,975/46,988)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.5.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.5.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.5.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.5.5.1.1.1">34.88%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.5.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.5.5.1.2.1">(30/86)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.6">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.6.1">DeepSeek 33B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.6.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.6.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.6.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.6.2.1.1.1">17.12%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.6.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.6.2.1.2.1">(10,505/61,373)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.6.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.6.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.6.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.6.3.1.1.1">13.28%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.6.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.6.3.1.2.1">(5,472/41,209)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.6.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.6.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.6.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.6.4.1.1.1">25.65%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.6.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.6.4.1.2.1">(4,940/19,260)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.6.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.6.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.6.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.6.5.1.1.1">10.29%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.6.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.6.5.1.2.1">(93/904)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.7">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.7.1">WizardCoder 33B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.7.2" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.7.2.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T8.3.1.1.1.7.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.7.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.7.2.1.1.1.1">14.93%</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.7.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.7.2.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.7.2.1.2.1.1">(3,876/25,969)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.7.3" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.7.3.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T8.3.1.1.1.7.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.7.3.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.7.3.1.1.1.1">7.83%</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.7.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.7.3.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.7.3.1.2.1.1">(1,038/13,256)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.7.4" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.7.4.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T8.3.1.1.1.7.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.7.4.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.7.4.1.1.1.1">23.31%</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.7.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.7.4.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.7.4.1.2.1.1">(2,772/11,894)</span></span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.7.5" style="background-color:#999999;">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.7.5.1" style="background-color:#999999;">
<span class="ltx_tr" id="A5.T8.3.1.1.1.7.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.7.5.1.1.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.7.5.1.1.1.1">8.06%</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.7.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.7.5.1.2.1"><span class="ltx_text ltx_font_bold" id="A5.T8.3.1.1.1.7.5.1.2.1.1">(66/819)</span></span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.8">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.8.1">DeepSeek 6B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.8.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.8.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.8.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.8.2.1.1.1">24.06%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.8.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.8.2.1.2.1">(25,178/104,628)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.8.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.8.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.8.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.8.3.1.1.1">17.36%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.8.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.8.3.1.2.1">(9,595/55,255)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.8.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.8.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.8.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.8.4.1.1.1">31.82%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.8.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.8.4.1.2.1">(15,493/48,693)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.8.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.8.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.8.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.8.5.1.1.1">13.24%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.8.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.8.5.1.2.1">(90/680)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.9">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.9.1">OpenChat 7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.9.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.9.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.9.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.9.2.1.1.1">23.04%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.9.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.9.2.1.2.1">(24,863/107,903)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.9.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.9.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.9.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.9.3.1.1.1">18.34%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.9.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.9.3.1.2.1">(10,275/56,039)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.9.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.9.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.9.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.9.4.1.1.1">28.18%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.9.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.9.4.1.2.1">(14,557/51,657)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.9.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.9.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.9.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.9.5.1.1.1">14.98%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.9.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.9.5.1.2.1">(31/207)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.10">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.10.1">CodeLlama 13B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.10.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.10.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.10.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.10.2.1.1.1">28.62%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.10.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.10.2.1.2.1">(11,984/41,866)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.10.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.10.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.10.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.10.3.1.1.1">19.10%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.10.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.10.3.1.2.1">(3,774/19,757)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.10.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.10.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.10.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.10.4.1.1.1">37.15%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.10.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.10.4.1.2.1">(8,200/22,071)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.10.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.10.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.10.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.10.5.1.1.1">26.32%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.10.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.10.5.1.2.1">(10/38)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.11">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.11.1">Mixtral 8x7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.11.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.11.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.11.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.11.2.1.1.1">21.22%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.11.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.11.2.1.2.1">(9,429/44,435)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.11.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.11.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.11.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.11.3.1.1.1">14.83%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.11.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.11.3.1.2.1">(2,882/19,436)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.11.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.11.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.11.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.11.4.1.1.1">27.98%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.11.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.11.4.1.2.1">(6,257/22,362)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.11.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.11.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.11.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.11.5.1.1.1">11.00%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.11.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.11.5.1.2.1">(290/2,637)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.12">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.12.1">MagiCoder 7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.12.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.12.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.12.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.12.2.1.1.1">29.85%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.12.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.12.2.1.2.1">(40,085/134,276)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.12.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.12.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.12.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.12.3.1.1.1">26.27%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.12.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.12.3.1.2.1">(20,703/78817)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.12.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.12.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.12.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.12.4.1.1.1">35.10%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.12.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.12.4.1.2.1">(19,301/54,982)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.12.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.12.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.12.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.12.5.1.1.1">16.98%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.12.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.12.5.1.2.1">(81/477)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.13">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.13.1">CodeLlama 34B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.13.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.13.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.13.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.13.2.1.1.1">34.57%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.13.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.13.2.1.2.1">(38,607/111,668)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.13.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.13.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.13.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.13.3.1.1.1">25.18%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.13.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.13.3.1.2.1">(13,090/51,995)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.13.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.13.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.13.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.13.4.1.1.1">42.77%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.13.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.13.4.1.2.1">(25,489/59,590)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.13.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.13.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.13.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.13.5.1.1.1">33.73%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.13.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.13.5.1.2.1">(28/83)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.14">
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.14.1">Mistral 7B</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.14.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.14.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.14.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.14.2.1.1.1">24.79%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.14.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.14.2.1.2.1">(10,505/42,381)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.14.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.14.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.14.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.14.3.1.1.1">20.60%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.14.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.14.3.1.2.1">(4,961/24,083)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.14.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.14.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.14.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.14.4.1.1.1">34.59%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.14.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.14.4.1.2.1">(5,252/15,183)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="A5.T8.3.1.1.1.14.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.14.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.14.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.14.5.1.1.1">9.37%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.14.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.14.5.1.2.1">(292/3,115)</span></span>
</span></span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.15">
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.3.1.1.1.15.1">CodeLlama 7B</span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.3.1.1.1.15.2">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.15.2.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.15.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.15.2.1.1.1">35.71%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.15.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.15.2.1.2.1">(33,877/94,876)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.3.1.1.1.15.3">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.15.3.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.15.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.15.3.1.1.1">27.32%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.15.3.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.15.3.1.2.1">(12,103/44,298)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.3.1.1.1.15.4">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.15.4.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.15.4.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.15.4.1.1.1">43.07%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.15.4.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.15.4.1.2.1">(21,751/50,507)</span></span>
</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A5.T8.3.1.1.1.15.5">
<span class="ltx_tabular ltx_align_middle" id="A5.T8.3.1.1.1.15.5.1">
<span class="ltx_tr" id="A5.T8.3.1.1.1.15.5.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.15.5.1.1.1">32.39%</span></span>
<span class="ltx_tr" id="A5.T8.3.1.1.1.15.5.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="A5.T8.3.1.1.1.15.5.1.2.1">(23/71)</span></span>
</span></span></span>
</span></span></p>
</span></div>
</figure>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Additional Background Information</h2>
<section class="ltx_subsection" id="A6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Large Language Models</h3>
<div class="ltx_para" id="A6.SS1.p1">
<p class="ltx_p" id="A6.SS1.p1.1">The advent of extremely versatile and high-performing attention-based transformer models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib75" title="">75</a>]</cite> have catalyzed the emergence of
<em class="ltx_emph ltx_font_italic" id="A6.SS1.p1.1.1">foundational language models</em>, which are large-scale models described by billions of parameters and pre-trained on vast datasets. These foundational LLMs can be tailored to perform a wide range of downstream tasks without the need for task-specific architecture modifications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib4" title="">4</a>]</cite>. Models such as BERT (Bidirectional Encoder Representations from Transformers) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib10" title="">10</a>]</cite>, CLIP (Contrastive Language-Image Pretraining) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib63" title="">63</a>]</cite>, and GPT (Generative Pre-Trained Transformer) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib5" title="">5</a>]</cite> are examples of foundational language models that have emerged as new benchmarks for a wide range of tasks.</p>
</div>
<div class="ltx_para" id="A6.SS1.p2">
<p class="ltx_p" id="A6.SS1.p2.1">The primary method to prepare these foundational models for a specific downstream application is <em class="ltx_emph ltx_font_italic" id="A6.SS1.p2.1.1">fine-tuning</em>, which involves providing the base model with a smaller, task-specific dataset. The model trains on the new data, making slight adjustments to its internal parameter weights in order to optimize its performance for the new application. Nearly all SOTA models in natural language processing (NLP) and computer vision are derivatives of such large foundational language models, excelling in tasks such as sentiment analysis, object recognition, image captioning, and information extraction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib4" title="">4</a>]</cite>. Foundational models have also been effectively adapted to domain-specific applications across various fields, most notably healthcare, law, and education <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib54" title="">54</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib23" title="">23</a>]</cite>.</p>
</div>
<div class="ltx_para" id="A6.SS1.p3">
<p class="ltx_p" id="A6.SS1.p3.1">One such emergent application of fine-tuned foundational language models has been <em class="ltx_emph ltx_font_italic" id="A6.SS1.p3.1.1">code generation</em>. Commercial products such as Codex <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib8" title="">8</a>]</cite> and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib1" title="">1</a>]</cite> from Open AI and open-source models such as CodeLlama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib66" title="">66</a>]</cite> and DeepSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib17" title="">17</a>]</cite> are able to produce functional code snippets, debug existing code, and translate code between programming languages. These tools have rapidly gained popularity and earned trust amongst developers, with 97% of DevOps and SecOps programmers having integrated such generative AI models into their workflows <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib68" title="">68</a>]</cite>.</p>
</div>
<div class="ltx_para" id="A6.SS1.p4">
<p class="ltx_p" id="A6.SS1.p4.1">The quality of code generated by these tools has quickly increased, as success rates for correctly answering coding prompts have skyrocketed from a mere 25% in June 2021 to 96% by April 2024 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib8" title="">8</a>]</cite>.
As code generation models become more ubiquitous in software development, there is a growing concern about the potential for generating insecure or flawed code that could lead to vulnerabilities in deployed applications. Early versions of code generation tools were shown to write code that contained a vulnerability in the MITRE Top-25 Common Weakness Enumeration (CWE) list 40% of the time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib61" title="">61</a>]</cite>. More recent research has shown that AI-assisted programming not only results in less secure code but also instills a false sense of security <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib62" title="">62</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="A6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>Hallucinations</h3>
<div class="ltx_para" id="A6.SS2.p1">
<p class="ltx_p" id="A6.SS2.p1.1">Hallucinations can also be categorized based on whether they can be directly verified from the source content; if so, it is termed an <em class="ltx_emph ltx_font_italic" id="A6.SS2.p1.1.1">intrinsic hallucination</em>, otherwise, it is considered an <em class="ltx_emph ltx_font_italic" id="A6.SS2.p1.1.2">extrinsic hallucination</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib24" title="">24</a>]</cite>. The structured nature of code leads to intrinsic hallucinations that are directly traceable to syntactic errors, while extrinsic hallucinations arise from complex interactions or gaps in the model’s training data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib44" title="">44</a>]</cite>. Code generation hallucinations manifest in several ways, including functional bugs that impair the intended operation, code that performs the wrong task, dead code that never gets executed, and, perhaps most critically, security vulnerabilities that can be exploited.</p>
</div>
<div class="ltx_para" id="A6.SS2.p2">
<p class="ltx_p" id="A6.SS2.p2.1">The persistent issue of hallucinations in LLMs has spurred extensive research into various mitigation strategies for standard natural language generation tasks, broadly categorized into <em class="ltx_emph ltx_font_italic" id="A6.SS2.p2.1.1">prompt engineering</em> and <em class="ltx_emph ltx_font_italic" id="A6.SS2.p2.1.2">model architecture enhancements</em> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib73" title="">73</a>]</cite>. Prompt engineering techniques such as Retrieval Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib37" title="">37</a>]</cite> and self-refinement methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib50" title="">50</a>]</cite> aim to refine the input provided to the model to produce more accurate outputs. Alternatively, developing more robust models involves approaches such as supervised fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib71" title="">71</a>]</cite>, inference time intervention <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib40" title="">40</a>]</cite>, and incorporating knowledge graphs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib25" title="">25</a>]</cite> to improve model understanding and reduce errors/hallucinations in the model output.</p>
</div>
<div class="ltx_para" id="A6.SS2.p3">
<p class="ltx_p" id="A6.SS2.p3.1">The manipulation of the <em class="ltx_emph ltx_font_italic" id="A6.SS2.p3.1.1">temperature</em> parameter within each model has also been shown to significantly influence the prevalence of hallucinations in LLM output <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib2" title="">2</a>]</cite>. The temperature parameter modulates the probability distribution over potential output tokens <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib53" title="">53</a>]</cite>. Lower temperatures produce less random, more predictable outputs, while higher temperatures increase the likelihood of sampling low-frequency tokens, raising the risk of hallucinations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2406.10279v2#bib.bib11" title="">11</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Packages and Modules Explanation</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">Here is a quick primer on the purpose of packages and modules in coding languages and how that relates to the topic of package hallucinations. To enable code modularity and reusability, interpreted programming languages such as Python and JavaScript allow for entities called <em class="ltx_emph ltx_font_italic" id="A7.p1.1.1">modules</em>. A module is a chunk of code, often in an external file, that performs a specific task or function. By encapsulating related code into modules, developers can organize their programs more efficiently and make them easier to maintain and share.
A <em class="ltx_emph ltx_font_italic" id="A7.p1.1.2">package</em> is a collection of related modules that work together to provide certain functionality.
To use a particular module in their source code, a developer must install the appropriate package into its development environment by first downloading the package from an online package manager or repository such as PyPI or npm, if it is not locally available, and then import the desired module into the code using appropriate import functions, e.g., <span class="ltx_text ltx_font_typewriter" id="A7.p1.1.3">import</span> in Python or <span class="ltx_text ltx_font_typewriter" id="A7.p1.1.4">require</span> in JavaScript.</p>
</div>
<div class="ltx_para" id="A7.p2">
<p class="ltx_p" id="A7.p2.1">These module names do not necessarily need to match the package names and the namespace for modules is not protected, i.e., different packages may include modules of the same name.
This discrepancy poses a significant challenge for detecting package dependencies from the raw Python and JavaScript code, as the <span class="ltx_text ltx_font_typewriter" id="A7.p2.1.1">import</span>/<span class="ltx_text ltx_font_typewriter" id="A7.p2.1.2">require</span> statements that are typically included in code samples for importing modules do not have a unique mapping to package names. Thus, it would not be accurate to simply parse the generated code for <span class="ltx_text ltx_font_typewriter" id="A7.p2.1.3">import</span> or <span class="ltx_text ltx_font_typewriter" id="A7.p2.1.4">require</span> statements, as those statements refer to module names and not package names.</p>
</div>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Stack Overflow Tags</h2>
<div class="ltx_para" id="A8.p1">
<p class="ltx_p" id="A8.p1.1">These are the "tags" used to filter questions on StackOverflow to ensure that all questions contained relevant subject matter for the respective programming language. Some tags are relevant to all programming and thus appear in both sets of tags, while others are language specific. We considered any tag with more than 5,000 questions and manually selected the applicable topics, resulting in 233 tags for Python and 274 for JavaScript</p>
</div>
<section class="ltx_subsection" id="A8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.1 </span>Python Tags</h3>
<div class="ltx_para" id="A8.SS1.p1">
<p class="ltx_p" id="A8.SS1.p1.1">python, arrays, django, pandas, string, dataframe, multithreading, list, algorithm, image, forms, numpy, function, loops, rest, apache, csv, dictionary, unit-testing, tensorflow, file, class, sorting, date, authentication, opencv, matplotlib, http, validation, sockets, object, xaml, oop, if-statement, web-services, email, kubernetes, github, parsing, user-interface, pointers, security, machine-learning, flask, exception, debugging, tkinter, ssl, web-scraping, testing, recursion, animation, session, math, django-models, keras, image-processing, dom, logging, matrix, post, pyspark, optimization, networking, opengl, events, encryption, audio, canvas, indexing, multidimensional-array, random, vector, video, data-structures, serialization, model-view-controller, iframe, beautifulsoup, design-patterns, text, mobile, lambda, graph-theory, websocket, scikit-learn, file-upload, deep-learning, error-handling, charts, server, https, merge, jupyter-notebook, encoding, django-views, hash, automation, pytorch, utf-8, python-requests, scipy, file-io, neural-network, interface, linked-list, django-templates, functional-programming, pygame, scrapy, io, bluetooth, 3d, dns, pyqt, package, tree, statistics, computer-vision, dockerfile, hashmap, character-encoding, cryptography, pyqt5, iterator, multiprocessing, cuda, ftp, plotly, operating-system, kivy, type-conversion, dependencies, geometry, format, subprocess, swagger, mapreduce, widget, set, tuples, smtp, ldap, return, dataset, network-programming, base64, stack, iteration, embedded, udp, gmail, conv-neural-network, bots, django-admin, time-complexity, pytest, python-imaging-library, web-crawler, hex, substring, filesystems, export, jinja2, seaborn, regression, thread-safety, logic, cloud, compression, data-science, celery, gpu, pandas-groupby, artificial-intelligence, video-streaming, backend, client, conda, jupyter, refactoring, bigdata, bit-manipulation, classification, boto3, schema, 2d, data-visualization, python-asyncio, wxpython, ipython, nltk, shader, rendering, pymongo, django-queryset, binary-tree, storage, blockchain, ethereum, vectorization, solidity, heap-memory, pyinstaller, linear-regression, metadata, kibana, cluster-analysis, networkx, driver, html-parsing, python-import, sympy, dynamic-programming, listener, pyqt4, user-input, cluster-computing, openpyxl, lxml, cython, simulation, spyder, decorator, pickle, python-multiprocessing, python-sphinx, python-multithreading, ironpython, python-tesseract, python-telegram-bot, python-polars, python-xarray</p>
</div>
</section>
<section class="ltx_subsection" id="A8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.2 </span>JavaScript Tags</h3>
<div class="ltx_para" id="A8.SS2.p1">
<p class="ltx_p" id="A8.SS2.p1.1">javascript, arrays, jquery, reactjs, node.js, angularjs, typescript, string, database, react-native, string, multithreading, list, algorithm, image, function, vue.js, loops, rest, apache, csv, dictionary, unit-testing, file, class, sorting, date, authentication, opencv, http, validation, sockets, object, xaml, oop, if-statement, web-services, email, kubernetes, github, parsing, user-interface, selenium-webdriver, pointers, security, machine-learning, listview, exception, debugging, ssl, web-scraping, testing, npm, mongoose, recursion, animation, session, math, webpack, heroku, redirect, dom, image-processing, logging, matrix, post, button, next.js, jquery-ui, optimization, d3.js, caching, networking, opengl, events, encryption, audio, canvas, indexing, multidimensional-array, redux, cookies, random, vector, video, data-structures, serialization, model-view-controller, iframe, design-patterns, text, mobile, react-hooks, google-chrome-extension, meteor, bootstrap-4, graph-theory, websocket, file-upload, deep-learning, error-handling, browser, vuejs2, charts, view, highcharts, server, https, merge, google-maps-api-3, extjs, axios, encoding, ember.js, hash, automation, react-redux, utf-8, jestjs, neo4j, jquery-mobile, material-ui, file-io, interface, linked-list, functional-programming, three.js, rxjs, datatables, backbone.js, react-router, knockout.js, angular-material, jwt, clojure, angularjs-directive, io, discord.js, kendo-ui, bluetooth, 3d, dns, package, tree, electron, frontend, dockerfile, hashmap, character-encoding, cryptography, iterator, multiprocessing, ftp, plotly, operating-system, jasmine, vuejs3, gulp, nuxt.js, dom-events, sequelize.js, nestjs, type-conversion, dependencies, geometry, webdriver, chart.js, format, swagger, widget, set, xmlhttprequest, tuples, smtp, ldap, return, dataset, network-programming, base64, protractor, arguments, stack, iteration, embedded, udp, gmail, bots, time-complexity, fetch-api, local-storage, angular-ui-router, cypress, coffeescript, babeljs, web-crawler, hex, log4j, substring, filesystems, export, regression, thread-safety, logic, cloud, dojo, jsx, gruntjs, vuetify.js, angularjs-ng-repeat, angularjs-scope, compression, video-streaming, backend, client, refactoring, bigdata, bit-manipulation, classification, mocha.js, momentjs, vuex, requirejs, bootstrap-modal, handlebars.js, phantomjs, angular-cli, passport.js, javascript-objects, eslint, lodash, phpstorm, puppeteer, schema, 2d, data-visualization, shader, rendering, binary-tree, storage, blockchain, visualization, ethereum, vectorization, ejs, sails.js, extjs4, vue-router, this, pug, node-modules, solidity, heap-memory, metadata, svelte, kibana, driver, underscore.js, office-js, ember-data, webstorm, npm-install, html-parsing, dynamic-programming, settimeout, facebook-javascript-sdk, listener, user-input, cluster-computing, lxml, openlayers, simulation, pickle, google-tag-manager, javascript-objects</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 24 21:39:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
