<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task</title>
<!--Generated on Sun Oct  6 23:00:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="human-AI team,  AI-assisted decision making,  precision and recall,  real-world application,  empirical study,  computer vision,  face detection,  video annotation" lang="en" name="keywords"/>
<base href="/html/2410.11860v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1" title="In Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S2" title="In Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3" title="In Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Algorithm choices and pilot studies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.SS1" title="In 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Precision and recall in multi-object tracking</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.SS2" title="In 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Pilot studies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.SS3" title="In 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>The false-positive-robust (FPR) tracker</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S4" title="In Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S4.SS1" title="In 4. Experiments ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>The task and data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S4.SS2" title="In 4. Experiments ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Participants and three treatment groups.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S4.SS3" title="In 4. Experiments ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Experiment procedure of the two-part study.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5" title="In Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.SS1" title="In 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Q1: Can the human-AI teams achieve ”complementary team performance” in this task?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.SS2" title="In 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Q2: Which AI helps annotators be more efficient, i.e. save time?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.SS3" title="In 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Q3: Which AI helps annotators achieve higher recall?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.SS4" title="In 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Q4: Will collaborating with an AI improve or hurt user skills?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6" title="In Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.SS1" title="In 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>The key to forming a strong human-AI team</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.SS2" title="In 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Can AI teammates set the quality lower bound in a crowdsourcing setting?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.SS3" title="In 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Seemingly contradictory survey results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.SS4" title="In 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.4 </span>Limitations and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S7" title="In Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chengyuan Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-9981-8889" title="ORCID identifier">0000-0001-9981-8889</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of California, Santa Barbara</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Santa Barbara</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:cxu@ucsb.edu">cxu@ucsb.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kuo-Chin Lien
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Appen</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Sunnyvale</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:klien@appen.com">klien@appen.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tobias Höllerer
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">University of California, Santa Barbara</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Santa Barbara</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:holl@cs.ucsb.edu">holl@cs.ucsb.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2023)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">When designing an AI-assisted decision-making system, there is often a tradeoff between precision and recall in the AI’s recommendations. We argue that careful exploitation of this tradeoff can harness the complementary strengths in the human-AI collaboration to significantly improve team performance. We investigate a real-world video anonymization task for which recall is paramount and more costly to improve. We analyze the performance of 78 professional annotators working with a) no AI assistance, b) a high-precision ”restrained” AI, and c) a high-recall ”zealous” AI in over 3,466 person-hours of annotation work. In comparison, the zealous AI helps human teammates achieve significantly shorter task completion time and higher recall. In a follow-up study, we remove AI assistance for everyone and find negative training effects on annotators trained with the restrained AI. These findings and our analysis point to important implications for the design of AI assistance in recall-demanding scenarios.</p>
</div>
<div class="ltx_keywords">human-AI team, AI-assisted decision making, precision and recall, real-world application, empirical study, computer vision, face detection, video annotation
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>rightsretained</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems; April 23–28, 2023; Hamburg, Germany</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_booktitle" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">booktitle: </span>Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23), April 23–28, 2023, Hamburg, Germany</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>10.1145/3544548.3581282</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-9421-5/23/04</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Interactive systems and tools</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id9"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Computer vision</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id10"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Collaborative interaction</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id11"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Machine learning</span></span></span>
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="157" id="S0.F1.g1" src="extracted/5905702/fig_teaser.png" width="598"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.2.1.1" style="font-size:90%;">Figure 1</span>. </span><span class="ltx_text" id="S0.F1.3.2" style="font-size:90%;">In video anonymization, face annotation and blurring is a high-stakes task that requires humans to check every frame. It demands high recall because one missed face can reveal a person’s identity in the entire video. We can improve recall and reduce task completion time by forming a human-AI team. We may have two AIs with the same (F1) performance as shown in (c) but provide different sets of recommendations (a &amp; b). A ”zealous” AI would prioritize recall by suggesting more detections, even low-confidence ones. A ”restrained” AI would only provide high-precision recommendations. Which AI teammate can help the human annotators finish in less time and with higher recall?</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S0.F1.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S0.F1.5">A teaser figure that shows two different methods for face annotation used for comparison in this paper. The left image shows an AI that tends to over annotate, the middle image shows an AI that tends to under annotate, on the right side a precision-recall figure shows both AI in fact have similar performance in F1 score.</p>
</div>
</div>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Machine-learning-based artificial intelligence (AI) systems have exceeded human performance in certain applications. But in high-stakes domains where fully-autonomous AI is not at peak performance or not permitted, such as in clinical decision-making <cite class="ltx_cite ltx_citemacro_citep">(Bussone et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib8" title="">2015</a>; Caruana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib10" title="">2015</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib54" title="">2016</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib57" title="">2020</a>; Van Berkel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib50" title="">2022</a>)</cite> or driver assistance <cite class="ltx_cite ltx_citemacro_citep">(Colley et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib14" title="">2021</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib12" title="">2021</a>; Gopinath et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib23" title="">2022</a>)</cite>, forming a human-AI team is a viable strategy to improve both efficiency and accuracy. AI can provide recommendations while human users maintain agency and control over the final decisions. Studies have shown the human-AI team is expected to achieve ”complementary team performance” – the team performance being better than either one alone <cite class="ltx_cite ltx_citemacro_citep">(Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib3" title="">2021a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib6" title="">b</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib62" title="">2022a</a>)</cite>. But there are more questions than answers on which exact factors in the AI system affect the team performance and how.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Bansal et al. recently showed in simplified binary classification problems that <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">the most accurate AI is not necessarily the best teammate unless it helps to improve the team utility</span> <cite class="ltx_cite ltx_citemacro_citep">(Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib3" title="">2021a</a>)</cite>. But how about in more complex problems where the AI teammate is not simply better or worse for its accuracy? For example, in many computer vision problems, people determine the best-performing algorithms based on combination metrics such as the F1 score <cite class="ltx_cite ltx_citemacro_citep">(Sasaki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib48" title="">2007</a>; Csurka et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib15" title="">2013</a>)</cite>, which can be broken down into two metrics – precision and recall <cite class="ltx_cite ltx_citemacro_citep">(Davis and Goadrich, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib17" title="">2006</a>; Lipton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib35" title="">2014</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib34" title="">2014</a>)</cite>. Researchers can either balance the two metrics or prioritize one over the other to identify the best model for their application <cite class="ltx_cite ltx_citemacro_citep">(Erenel and Altınçay, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib21" title="">2013</a>; Morstatter et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib41" title="">2016</a>)</cite>. Two AI systems can have the same F1 score but provide very different recommendations with different measures of recall (see a, b in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S0.F1" title="Figure 1 ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">1</span></a>). The tradeoff between precision and recall puts them on different parts of the same F1 isoline (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S0.F1" title="Figure 1 ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">1</span></a> c). Without additional context, one might argue that there is no better or worse between these two AIs.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In order to capitalize on complementary strengths of humans and AI when presented with tradeoffs in AI precision and recall, we need to be able to answer two questions: <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">1) for a given task, can we clearly identify if either precision or recall is more important than the other</span>, and <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">2) independent of importance, is it vastly easier or harder for humans to improve either precision or recall.</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Consider for example a pedestrian detection task in a driver assistance system: prioritizing the detection model towards either precision or recall will hurt the other. Human instinct tells us the risk of a missing detection could be lethal, so we should tune the AI system to prioritize recall, i.e., towards a ”zealous” AI that provides more detections (recommendations), even the low-confidence ones, at the risk of more false positive errors. In this context, the opposite ”restrained” AI would only provide high-confidence detections and prioritize precision, but at the risk of more false negative errors.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In this work, we investigate how a high-recall zealous AI and a high-precision restrained AI can affect human-AI team performance in a real-world scenario. Compared to, say testing pedestrian detectors on the road, video anonymization is a similar but easier-to-test recall-demanding task. We set up a face annotation task for personally identifiable information (PII) protection that blurs human faces in a real-world video dataset <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib24" title="">2022</a>)</cite>. PII protection is a critical task with increasing demand for both ethical research and abiding by regulatory requirements<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>E.g., The General Data Protection Regulation (EU) or The California Consumer Privacy Act of 2018 (CCPA)</span></span></span>. Similar to pedestrian detection, where the cost of a missing detection is very high, one unlabeled face in a single frame can reveal a person’s identity in the entire video, if not the entire dataset.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">This paper focuses on the common yet critical human-AI collaboration setting, in which recall is more important than precision. As for our second question, ”is it vastly easier or harder for humans to improve either precision or recall?”, an in-depth analysis of the video annotation workflow shows that improving recall is more costly than precision in this task since it is much harder for human annotators to draw a bounding box accurately than rejecting an incorrect one (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.SS2" title="3.2. Pilot studies ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">3.2</span></a> &amp; <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.SS3" title="3.3. The false-positive-robust (FPR) tracker ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">3.3</span></a> for a full discussion).</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The answers to our two questions for our task reveal <span class="ltx_text ltx_font_bold" id="S1.p7.1.1">an optimization opportunity: the AI recommendation tradeoff between precision and recall can be used to exploit complementary strengths of the human and the AI in such collaborative tasks</span>. We posit that similar optimization opportunities exist for many other human-AI collaboration tasks. In addition, locating faces is a human instinct<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Here we refer to the ability to find human faces in a given image. We do not refer to recognizing people by face, which can be affected by Prosopagnosia (face blindness).</span></span></span> that requires no specific training or domain expertise to get started, making face detection a good candidate task to study the effects of different AI recommendations. The relatively small inter-personal differences also make the task a good representative of recall-demanding human-AI collaboration tasks.</p>
</div>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="518" id="S1.F2.g1" src="extracted/5905702/fig_data_stream.jpg" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>. </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">Data processing workflow for Part 1 of the study and the annotation tool user interface. The two AI teammates share the same face detector, which generates bounding box face detections for each frame independently. The ByteTrack tracker <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib65" title="">2022b</a>)</cite> and our proposed false-positive-robust (FPR) tracker define the restrained or zealous AI recommendations – they track the per-frame detections temporally to pre-annotate the videos as shown above. For the human-only workflow, annotators must manually draw a box and adjust its size and location across many frames.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S1.F2.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S1.F2.5">A flow chart image that shows how the three treatment groups receive the video annotation and a screen shot of the annotation tool interface.</p>
</div>
</div>
</figure>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Our large-scale empirical study had 78 professional data annotators spend over 3,466 person-hours<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Our system logged 3,466 person-hours of annotation work, which does not include pilot studies, training sessions, and answering multiple questionnaires. On average it took the 78 annotators three to four weeks to finish the entire study.</span></span></span> to submit a total of 2780 annotated 30-second videos. The between-subjects study split the annotators into three 26-people treatment groups. Detailed worker profiles ensured similar average experience between the groups (details in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S4.SS2" title="4.2. Participants and three treatment groups. ‣ 4. Experiments ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">4.2</span></a>). Each participant annotated human faces in 36 real-world videos of a variety of activities (see examples in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.F4" title="Figure 4 ‣ 3.3. The false-positive-robust (FPR) tracker ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">4</span></a>). We measure each group’s annotation quality and task completion time. Any improvement in time is very meaningful for annotation tasks not only because of the cost. Fatigue induced by long working hours may also cause a decline in quality.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">In Part 1 of the <span class="ltx_text ltx_font_bold" id="S1.p9.1.1">two-part study</span>, the three groups of annotators processed the same 24 videos, each with a) no AI assistance, b) pre-annotated bounding boxes recommended by the restrained AI, or c) the zealous AI. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">2</span></a> summarizes the treatment groups and shows the annotation tool’s interface. In Part 2, the three groups annotated another 12 videos but all without the AI’s help. This design <span class="ltx_text ltx_font_bold" id="S1.p9.1.2">allows us to learn how prior human-AI collaboration experience can affect user skills</span>, should they lose access to AI recommendations in the future. The two-part experiment aims to answer the following research questions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Q1</span>
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1">Can the human-AI teams achieve ”complementary team performance” in this task?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Q2</span>
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1">Which AI helps annotators be more efficient, i.e. save time?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Q3</span>
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1">Which AI helps annotators achieve higher recall?</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">Q4</span>
<div class="ltx_para" id="S1.I1.ix4.p1">
<p class="ltx_p" id="S1.I1.ix4.p1.1">Will collaborating with an AI improve or hurt user skills?</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">We will answer each of the research questions in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5" title="5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5</span></a>. Here we summarize this work’s contributions:</p>
<ul class="ltx_itemize" id="S1.I2">
<li class="ltx_item" id="S1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i1.p1">
<p class="ltx_p" id="S1.I2.i1.p1.1">We propose the concept of restrained and zealous AI recommendations to compare the tradeoff between precision and recall in tuning AI-assisted decision-making systems and investigate how they affect human-AI team performance in high-stakes recall-demanding tasks.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i2.p1">
<p class="ltx_p" id="S1.I2.i2.p1.1">We design a large empirical study to compare the restrained and zealous AI on a face annotation task for video anonymization with 78 professional data annotators. The two-part experiment yielded significant findings to inform future AI assistance design for recall-demanding tasks.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I2.i3.p1">
<p class="ltx_p" id="S1.I2.i3.p1.1">The analysis of 3,466 person-hours of annotation work reveals significant findings:</p>
<ul class="ltx_itemize" id="S1.I2.i3.I1">
<li class="ltx_item" id="S1.I2.i3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I2.i3.I1.i1.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I2.i3.I1.i1.p1">
<p class="ltx_p" id="S1.I2.i3.I1.i1.p1.1">Our study serves as a real-world case study of complementary team performance (cf. <cite class="ltx_cite ltx_citemacro_citep">(Nagar and Malone, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib43" title="">2011</a>; Kamar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib26" title="">2012</a>; Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib45" title="">2019</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib32" title="">2022</a>)</cite>).</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I2.i3.I1.i2.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I2.i3.I1.i2.p1">
<p class="ltx_p" id="S1.I2.i3.I1.i2.p1.1">Identifying the complementary strengths of both human and AI teammates for a task is key to better team performance. The recall-demanding task and the higher cost of improving recall motivated us to propose the zealous AI, which provides high-recall recommendations and leads to significantly better task completion time and recall.</p>
</div>
</li>
<li class="ltx_item" id="S1.I2.i3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S1.I2.i3.I1.i3.1.1.1">–</span></span>
<div class="ltx_para" id="S1.I2.i3.I1.i3.p1">
<p class="ltx_p" id="S1.I2.i3.I1.i3.p1.1">The follow-up study demonstrates that naively pairing humans with an AI system designed for autonomous settings without optimizing it for the task at hand or for the human-AI workflow could potentially have a negative training effect on the users.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related work</h2>
<section class="ltx_subsubsection" id="S2.SS0.SSSx1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Factors affecting human-AI team performance.</h4>
<div class="ltx_para" id="S2.SS0.SSSx1.p1">
<p class="ltx_p" id="S2.SS0.SSSx1.p1.1">While human-AI teams have been studied extensively from various perspectives like in crowdsourcing settings <cite class="ltx_cite ltx_citemacro_citep">(Kamar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib26" title="">2012</a>; Lundgard et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib38" title="">2018</a>)</cite>, computer vision tasks <cite class="ltx_cite ltx_citemacro_citep">(Kamar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib26" title="">2012</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib54" title="">2016</a>; Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib45" title="">2019</a>)</cite>, high-stakes tasks <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib45" title="">2019</a>; Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib4" title="">2019a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib5" title="">b</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib64" title="">2020</a>)</cite>, and real-world tasks <cite class="ltx_cite ltx_citemacro_citep">(Kamar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib26" title="">2012</a>; Nagar and Malone, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib43" title="">2011</a>; Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib45" title="">2019</a>; Amershi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib2" title="">2019</a>; Wilder et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib56" title="">2020</a>; Lai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib32" title="">2022</a>; Van Berkel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib50" title="">2022</a>)</cite>, we still have more questions than answers on exactly which factors affect team performance and how. Researchers have looked into factors like users’ mental models <cite class="ltx_cite ltx_citemacro_citep">(Chakraborti and Kambhampati, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib11" title="">2018</a>; Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib4" title="">2019a</a>)</cite>, user expectations <cite class="ltx_cite ltx_citemacro_citep">(Kay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib29" title="">2015</a>; Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib59" title="">2019</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib63" title="">2021</a>)</cite>, cognitive biases <cite class="ltx_cite ltx_citemacro_citep">(Rastogi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib46" title="">2022</a>)</cite>, model updates during collaboration <cite class="ltx_cite ltx_citemacro_citep">(Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib5" title="">2019b</a>)</cite>, model accuracy <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib59" title="">2019</a>; Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib3" title="">2021a</a>)</cite>, model interpretability or explanations <cite class="ltx_cite ltx_citemacro_citep">(Bilgic and Mooney, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib7" title="">2005</a>; Ribeiro et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib47" title="">2016</a>; Lundberg and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib37" title="">2017</a>; Feng and Boyd-Graber, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib22" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib55" title="">2019</a>; Kaur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib27" title="">2020</a>; Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib6" title="">2021b</a>)</cite>, as well as the tradeoff between accuracy and interpretability <cite class="ltx_cite ltx_citemacro_citep">(Caruana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib10" title="">2015</a>)</cite>. Studying user’s trust and appropriate or inappropriate reliance on AI <cite class="ltx_cite ltx_citemacro_citep">(Muir, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib42" title="">1987</a>; Bussone et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib8" title="">2015</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib60" title="">2017</a>; Kunkel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib31" title="">2019</a>; Lu and Yin, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib36" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib64" title="">2020</a>)</cite> is another important direction.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSSx1.p2">
<p class="ltx_p" id="S2.SS0.SSSx1.p2.1">This paper is aligned with works that focused on the tradeoff between precision and recall in AI recommendations and its effect on team performance. Kay et al. <cite class="ltx_cite ltx_citemacro_citep">(Kay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib29" title="">2015</a>)</cite> introduced the acceptability of accuracy as a new measure and survey instrument to connect classifier evaluation to users’ subjective perception of accuracy. Kocielnik et al. <cite class="ltx_cite ltx_citemacro_citep">(Kocielnik et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib30" title="">2019</a>)</cite> compared two 50%-accurate AI-powered scheduling assistants – one avoids false positive errors, and one avoids false negative. This is a similar design as for our restrained and zealous AIs – their study found that false positive errors are more acceptable by participants, which corroborates the overall better performance we observed in the zealous AI group, who also dealt with more false positive errors.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSSx1.p3">
<p class="ltx_p" id="S2.SS0.SSSx1.p3.1">Balancing precision and recall to compare two real-world AI systems in a human-AI collaboration task is not easy, previous works derived insight from hypothetical systems or manually balanced recommendations <cite class="ltx_cite ltx_citemacro_citep">(Kay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib29" title="">2015</a>; Kocielnik et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib30" title="">2019</a>)</cite>. In this work, we provide a real-world user study by observing how 78 professional users would interact with two high-performance face tracking AI systems that are tuned to truthfully portray the realistic tradeoff between high-precision and high-recall on a recent egocentric video dataset.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSSx2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Face detection.</h4>
<div class="ltx_para" id="S2.SS0.SSSx2.p1">
<p class="ltx_p" id="S2.SS0.SSSx2.p1.1">The annotation platform we used has a built-in face detector, RetinaFace <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib19" title="">2019</a>)</cite>, integrated for autonomous workflows. Our literature search found RetinaFace remains a top-ranking method on the WIDER FACE benchmark <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib58" title="">2016</a>)</cite>. Because more recent methods do not provide significant performance improvement, we continue to use RetinaFace as a consistent baseline to compare with our algorithmic improvements in tracking.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSSx3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Multi-object tracking.</h4>
<div class="ltx_para" id="S2.SS0.SSSx3.p1">
<p class="ltx_p" id="S2.SS0.SSSx3.p1.1">In the AI-assisted face annotation task, the AI teammate provides annotation recommendations for users to review. Conventionally a face detector provides per-frame face bounding boxes and a multi-object tracking (MOT) algorithm produces continuous tracks of the same object across frames. This is known as tracking-by-detection. Recent MOT methods like TransTrack <cite class="ltx_cite ltx_citemacro_citep">(Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib49" title="">2020</a>)</cite>, DETR <cite class="ltx_cite ltx_citemacro_citep">(Carion et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib9" title="">2020</a>)</cite>, Deformable DETR <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib66" title="">2020</a>)</cite>, TrackFormer <cite class="ltx_cite ltx_citemacro_citep">(Meinhardt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib39" title="">2022</a>)</cite>, and TransMOT <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib13" title="">2021</a>)</cite> etc. all move toward the end-to-end Transformer-based <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib51" title="">2017</a>)</cite> architecture. However, these black-box MOTs share the same drawback as they are designed for fully-autonomous settings. Similar to Caruana et al.’s observation that modular system provides better transparency <cite class="ltx_cite ltx_citemacro_citep">(Caruana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib10" title="">2015</a>)</cite>, the two-part tracking-by-detection frameworks actually provide us the interpretability and flexibility to steer the output recommendations as needed, so we can produce restrained and zealous AI recommendations for comparison. We reviewed state-of-the-art methods in related multi-object tracking benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Milan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib40" title="">2016</a>; Voigtlaender et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib52" title="">2019</a>; Dave et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib16" title="">2020</a>)</cite> in search of a multi-object tracker suitable for a human-in-the-loop annotation workflow. ByteTrack <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib65" title="">2022b</a>)</cite> is a conventional tracker that outperforms numerous Transformer-based trackers mentioned earlier.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS0.SSSx4">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Video annotation.</h4>
<div class="ltx_para" id="S2.SS0.SSSx4.p1">
<p class="ltx_p" id="S2.SS0.SSSx4.p1.1">While there are various public video annotation platforms or tools to choose from <cite class="ltx_cite ltx_citemacro_citep">(Dutta and Zisserman, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib20" title="">2019</a>; Vondrick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib53" title="">2013</a>; Yuen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib61" title="">2009</a>; Kavasidis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib28" title="">2014</a>)</cite>, we use a proprietary video annotation tool to gain access to professional data annotators who are already familiar with the specific tool from their past project experience. This tool has Linear Interpolation <cite class="ltx_cite ltx_citemacro_citep">(Vondrick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib53" title="">2013</a>)</cite> activated by default, which provides semi-automatic assistance by linearly interpolating a box between two manually annotated key frames. In this study, all participants, including annotators who review AI’s annotation recommendations have access to this functionality. Linear Interpolation is also an ideal baseline as all participants have sufficient experience using it. We will refer to this basic setup as human only, the baseline method, or the manual method in the rest of the paper.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Algorithm choices and pilot studies</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Precision and recall in multi-object tracking</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Precision, recall, and F1 are important performance metrics that can describe the characteristics of a model and are central concepts in this work and other human-AI research <cite class="ltx_cite ltx_citemacro_citep">(Kay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib29" title="">2015</a>; Kocielnik et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib30" title="">2019</a>)</cite>. Specifically, in the context of annotating and tracking faces with bounding boxes in videos:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A0.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E1.2.1.1.1" style="font-size:90%;">Precision</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\text{TP}}{\text{TP + FP}}=\frac{\text{Face boxes %
correctly drawn}}{\text{All boxes drawn by the user (or the AI)}}" class="ltx_Math" display="inline" id="S3.E1.m2.1"><semantics id="S3.E1.m2.1a"><mrow id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml"><mi id="S3.E1.m2.1.1.2" xref="S3.E1.m2.1.1.2.cmml"></mi><mo id="S3.E1.m2.1.1.3" mathsize="90%" xref="S3.E1.m2.1.1.3.cmml">=</mo><mstyle displaystyle="true" id="S3.E1.m2.1.1.4" xref="S3.E1.m2.1.1.4.cmml"><mfrac id="S3.E1.m2.1.1.4a" xref="S3.E1.m2.1.1.4.cmml"><mtext id="S3.E1.m2.1.1.4.2" mathsize="90%" xref="S3.E1.m2.1.1.4.2a.cmml">TP</mtext><mtext id="S3.E1.m2.1.1.4.3" mathsize="90%" xref="S3.E1.m2.1.1.4.3a.cmml">TP + FP</mtext></mfrac></mstyle><mo id="S3.E1.m2.1.1.5" mathsize="90%" xref="S3.E1.m2.1.1.5.cmml">=</mo><mstyle displaystyle="true" id="S3.E1.m2.1.1.6" xref="S3.E1.m2.1.1.6.cmml"><mfrac id="S3.E1.m2.1.1.6a" xref="S3.E1.m2.1.1.6.cmml"><mtext id="S3.E1.m2.1.1.6.2" mathsize="90%" xref="S3.E1.m2.1.1.6.2a.cmml">Face boxes correctly drawn</mtext><mtext id="S3.E1.m2.1.1.6.3" mathsize="90%" xref="S3.E1.m2.1.1.6.3a.cmml">All boxes drawn by the user (or the AI)</mtext></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.1b"><apply id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1"><and id="S3.E1.m2.1.1a.cmml" xref="S3.E1.m2.1.1"></and><apply id="S3.E1.m2.1.1b.cmml" xref="S3.E1.m2.1.1"><eq id="S3.E1.m2.1.1.3.cmml" xref="S3.E1.m2.1.1.3"></eq><csymbol cd="latexml" id="S3.E1.m2.1.1.2.cmml" xref="S3.E1.m2.1.1.2">absent</csymbol><apply id="S3.E1.m2.1.1.4.cmml" xref="S3.E1.m2.1.1.4"><divide id="S3.E1.m2.1.1.4.1.cmml" xref="S3.E1.m2.1.1.4"></divide><ci id="S3.E1.m2.1.1.4.2a.cmml" xref="S3.E1.m2.1.1.4.2"><mtext id="S3.E1.m2.1.1.4.2.cmml" mathsize="90%" xref="S3.E1.m2.1.1.4.2">TP</mtext></ci><ci id="S3.E1.m2.1.1.4.3a.cmml" xref="S3.E1.m2.1.1.4.3"><mtext id="S3.E1.m2.1.1.4.3.cmml" mathsize="90%" xref="S3.E1.m2.1.1.4.3">TP + FP</mtext></ci></apply></apply><apply id="S3.E1.m2.1.1c.cmml" xref="S3.E1.m2.1.1"><eq id="S3.E1.m2.1.1.5.cmml" xref="S3.E1.m2.1.1.5"></eq><share href="https://arxiv.org/html/2410.11860v1#S3.E1.m2.1.1.4.cmml" id="S3.E1.m2.1.1d.cmml" xref="S3.E1.m2.1.1"></share><apply id="S3.E1.m2.1.1.6.cmml" xref="S3.E1.m2.1.1.6"><divide id="S3.E1.m2.1.1.6.1.cmml" xref="S3.E1.m2.1.1.6"></divide><ci id="S3.E1.m2.1.1.6.2a.cmml" xref="S3.E1.m2.1.1.6.2"><mtext id="S3.E1.m2.1.1.6.2.cmml" mathsize="90%" xref="S3.E1.m2.1.1.6.2">Face boxes correctly drawn</mtext></ci><ci id="S3.E1.m2.1.1.6.3a.cmml" xref="S3.E1.m2.1.1.6.3"><mtext id="S3.E1.m2.1.1.6.3.cmml" mathsize="90%" xref="S3.E1.m2.1.1.6.3">All boxes drawn by the user (or the AI)</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.1c">\displaystyle=\frac{\text{TP}}{\text{TP + FP}}=\frac{\text{Face boxes %
correctly drawn}}{\text{All boxes drawn by the user (or the AI)}}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m2.1d">= divide start_ARG TP end_ARG start_ARG TP + FP end_ARG = divide start_ARG Face boxes correctly drawn end_ARG start_ARG All boxes drawn by the user (or the AI) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E2.2.1.1.1" style="font-size:90%;">Recall</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{\text{TP}}{\text{TP + FN}}=\frac{\text{Face boxes %
correctly drawn}}{\text{All ground truth face boxes}}" class="ltx_Math" display="inline" id="S3.E2.m2.1"><semantics id="S3.E2.m2.1a"><mrow id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml"><mi id="S3.E2.m2.1.1.2" xref="S3.E2.m2.1.1.2.cmml"></mi><mo id="S3.E2.m2.1.1.3" mathsize="90%" xref="S3.E2.m2.1.1.3.cmml">=</mo><mstyle displaystyle="true" id="S3.E2.m2.1.1.4" xref="S3.E2.m2.1.1.4.cmml"><mfrac id="S3.E2.m2.1.1.4a" xref="S3.E2.m2.1.1.4.cmml"><mtext id="S3.E2.m2.1.1.4.2" mathsize="90%" xref="S3.E2.m2.1.1.4.2a.cmml">TP</mtext><mtext id="S3.E2.m2.1.1.4.3" mathsize="90%" xref="S3.E2.m2.1.1.4.3a.cmml">TP + FN</mtext></mfrac></mstyle><mo id="S3.E2.m2.1.1.5" mathsize="90%" xref="S3.E2.m2.1.1.5.cmml">=</mo><mstyle displaystyle="true" id="S3.E2.m2.1.1.6" xref="S3.E2.m2.1.1.6.cmml"><mfrac id="S3.E2.m2.1.1.6a" xref="S3.E2.m2.1.1.6.cmml"><mtext id="S3.E2.m2.1.1.6.2" mathsize="90%" xref="S3.E2.m2.1.1.6.2a.cmml">Face boxes correctly drawn</mtext><mtext id="S3.E2.m2.1.1.6.3" mathsize="90%" xref="S3.E2.m2.1.1.6.3a.cmml">All ground truth face boxes</mtext></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.1b"><apply id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1"><and id="S3.E2.m2.1.1a.cmml" xref="S3.E2.m2.1.1"></and><apply id="S3.E2.m2.1.1b.cmml" xref="S3.E2.m2.1.1"><eq id="S3.E2.m2.1.1.3.cmml" xref="S3.E2.m2.1.1.3"></eq><csymbol cd="latexml" id="S3.E2.m2.1.1.2.cmml" xref="S3.E2.m2.1.1.2">absent</csymbol><apply id="S3.E2.m2.1.1.4.cmml" xref="S3.E2.m2.1.1.4"><divide id="S3.E2.m2.1.1.4.1.cmml" xref="S3.E2.m2.1.1.4"></divide><ci id="S3.E2.m2.1.1.4.2a.cmml" xref="S3.E2.m2.1.1.4.2"><mtext id="S3.E2.m2.1.1.4.2.cmml" mathsize="90%" xref="S3.E2.m2.1.1.4.2">TP</mtext></ci><ci id="S3.E2.m2.1.1.4.3a.cmml" xref="S3.E2.m2.1.1.4.3"><mtext id="S3.E2.m2.1.1.4.3.cmml" mathsize="90%" xref="S3.E2.m2.1.1.4.3">TP + FN</mtext></ci></apply></apply><apply id="S3.E2.m2.1.1c.cmml" xref="S3.E2.m2.1.1"><eq id="S3.E2.m2.1.1.5.cmml" xref="S3.E2.m2.1.1.5"></eq><share href="https://arxiv.org/html/2410.11860v1#S3.E2.m2.1.1.4.cmml" id="S3.E2.m2.1.1d.cmml" xref="S3.E2.m2.1.1"></share><apply id="S3.E2.m2.1.1.6.cmml" xref="S3.E2.m2.1.1.6"><divide id="S3.E2.m2.1.1.6.1.cmml" xref="S3.E2.m2.1.1.6"></divide><ci id="S3.E2.m2.1.1.6.2a.cmml" xref="S3.E2.m2.1.1.6.2"><mtext id="S3.E2.m2.1.1.6.2.cmml" mathsize="90%" xref="S3.E2.m2.1.1.6.2">Face boxes correctly drawn</mtext></ci><ci id="S3.E2.m2.1.1.6.3a.cmml" xref="S3.E2.m2.1.1.6.3"><mtext id="S3.E2.m2.1.1.6.3.cmml" mathsize="90%" xref="S3.E2.m2.1.1.6.3">All ground truth face boxes</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.1c">\displaystyle=\frac{\text{TP}}{\text{TP + FN}}=\frac{\text{Face boxes %
correctly drawn}}{\text{All ground truth face boxes}}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m2.1d">= divide start_ARG TP end_ARG start_ARG TP + FN end_ARG = divide start_ARG Face boxes correctly drawn end_ARG start_ARG All ground truth face boxes end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S3.E3.2.1.1.1" style="font-size:90%;">F1</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\frac{2\cdot\text{precision}\cdot\text{recall}}{\text{precision}%
+\text{recall}}" class="ltx_Math" display="inline" id="S3.E3.m2.1"><semantics id="S3.E3.m2.1a"><mrow id="S3.E3.m2.1.1" xref="S3.E3.m2.1.1.cmml"><mi id="S3.E3.m2.1.1.2" xref="S3.E3.m2.1.1.2.cmml"></mi><mo id="S3.E3.m2.1.1.1" mathsize="90%" xref="S3.E3.m2.1.1.1.cmml">=</mo><mstyle displaystyle="true" id="S3.E3.m2.1.1.3" xref="S3.E3.m2.1.1.3.cmml"><mfrac id="S3.E3.m2.1.1.3a" xref="S3.E3.m2.1.1.3.cmml"><mrow id="S3.E3.m2.1.1.3.2" xref="S3.E3.m2.1.1.3.2.cmml"><mn id="S3.E3.m2.1.1.3.2.2" mathsize="90%" xref="S3.E3.m2.1.1.3.2.2.cmml">2</mn><mo id="S3.E3.m2.1.1.3.2.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.E3.m2.1.1.3.2.1.cmml">⋅</mo><mtext id="S3.E3.m2.1.1.3.2.3" mathsize="90%" xref="S3.E3.m2.1.1.3.2.3a.cmml">precision</mtext><mo id="S3.E3.m2.1.1.3.2.1a" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.E3.m2.1.1.3.2.1.cmml">⋅</mo><mtext id="S3.E3.m2.1.1.3.2.4" mathsize="90%" xref="S3.E3.m2.1.1.3.2.4a.cmml">recall</mtext></mrow><mrow id="S3.E3.m2.1.1.3.3" xref="S3.E3.m2.1.1.3.3.cmml"><mtext id="S3.E3.m2.1.1.3.3.2" mathsize="90%" xref="S3.E3.m2.1.1.3.3.2a.cmml">precision</mtext><mo id="S3.E3.m2.1.1.3.3.1" mathsize="90%" xref="S3.E3.m2.1.1.3.3.1.cmml">+</mo><mtext id="S3.E3.m2.1.1.3.3.3" mathsize="90%" xref="S3.E3.m2.1.1.3.3.3a.cmml">recall</mtext></mrow></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m2.1b"><apply id="S3.E3.m2.1.1.cmml" xref="S3.E3.m2.1.1"><eq id="S3.E3.m2.1.1.1.cmml" xref="S3.E3.m2.1.1.1"></eq><csymbol cd="latexml" id="S3.E3.m2.1.1.2.cmml" xref="S3.E3.m2.1.1.2">absent</csymbol><apply id="S3.E3.m2.1.1.3.cmml" xref="S3.E3.m2.1.1.3"><divide id="S3.E3.m2.1.1.3.1.cmml" xref="S3.E3.m2.1.1.3"></divide><apply id="S3.E3.m2.1.1.3.2.cmml" xref="S3.E3.m2.1.1.3.2"><ci id="S3.E3.m2.1.1.3.2.1.cmml" xref="S3.E3.m2.1.1.3.2.1">⋅</ci><cn id="S3.E3.m2.1.1.3.2.2.cmml" type="integer" xref="S3.E3.m2.1.1.3.2.2">2</cn><ci id="S3.E3.m2.1.1.3.2.3a.cmml" xref="S3.E3.m2.1.1.3.2.3"><mtext id="S3.E3.m2.1.1.3.2.3.cmml" mathsize="90%" xref="S3.E3.m2.1.1.3.2.3">precision</mtext></ci><ci id="S3.E3.m2.1.1.3.2.4a.cmml" xref="S3.E3.m2.1.1.3.2.4"><mtext id="S3.E3.m2.1.1.3.2.4.cmml" mathsize="90%" xref="S3.E3.m2.1.1.3.2.4">recall</mtext></ci></apply><apply id="S3.E3.m2.1.1.3.3.cmml" xref="S3.E3.m2.1.1.3.3"><plus id="S3.E3.m2.1.1.3.3.1.cmml" xref="S3.E3.m2.1.1.3.3.1"></plus><ci id="S3.E3.m2.1.1.3.3.2a.cmml" xref="S3.E3.m2.1.1.3.3.2"><mtext id="S3.E3.m2.1.1.3.3.2.cmml" mathsize="90%" xref="S3.E3.m2.1.1.3.3.2">precision</mtext></ci><ci id="S3.E3.m2.1.1.3.3.3a.cmml" xref="S3.E3.m2.1.1.3.3.3"><mtext id="S3.E3.m2.1.1.3.3.3.cmml" mathsize="90%" xref="S3.E3.m2.1.1.3.3.3">recall</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m2.1c">\displaystyle=\frac{2\cdot\text{precision}\cdot\text{recall}}{\text{precision}%
+\text{recall}}</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m2.1d">= divide start_ARG 2 ⋅ precision ⋅ recall end_ARG start_ARG precision + recall end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.2">where the TPs are true positives, face boxes that were correctly drawn. The FPs are false positives, boxes drawn by the AI or user which did not match real faces properly. The FNs are false negatives, where there is a real face, but the box is missing.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The F1 score is the harmonic mean of the precision and recall (Equation <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.E3" title="In 3.1. Precision and recall in multi-object tracking ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">3</span></a>). We visually introduced the concept of this function using three methods that have the same F1 score in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S0.F1" title="Figure 1 ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">1</span></a> (c). Put simply, a video pre-annotated by a high-recall method (zealous AI) would have more false-positive boxes – the user will make more rejections but add fewer missing boxes. A video pre-annotated by the high-precision method (restrained AI) would provide mostly correct boxes but the user will need to add more missing boxes.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">We are interested in how users will perform differently given restrained or zealous AI recommendations in an AI-assisted face annotation task. While it is easy to generate high-precision annotations by simply avoiding low-confidence detections, it is hard for trackers to produce high-recall results while maintaining a similarly high F1 score at the same time. This motivates us to propose a tracking algorithm that pushes recall to the limit, but aims to maintain a similar level of F1 score. We take advantage of the fact that <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">our tracking results will be reviewed by human annotators, allowing us to make targeted optimizations</span>. We test our ideas of a user-friendly tracker with professional annotators through pilot studies. Observing how users work with trackers allows us to further improve the algorithm.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Pilot studies</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We conducted two pilot studies to observe how professional data annotators work with AI recommendations. Annotators were tasked to draw bounding boxes around potentially moving or blurred faces of any size in a 1,200-frame video sequence of a busy shopping scene in both sessions (similar to hard videos in the formal study). We provided training material on how to review recommendations from the AI for the face annotation job. The annotation tool user interface is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">2</span></a>. With their consent, we recorded their screens to keep track of mouse movements and other user habits. Each session included ten different users with above-average experience. Both pilot studies concluded with a survey about experiment design and their experience. The two pilot sessions were spaced two weeks apart to test algorithm and design improvements.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Users’ screen recordings helped us observe the following user habits and behaviors that are not possible to be identified solely from the results:</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i1.p1.1.1">Certain bad recommendations cost most of the human review efforts.</span> Following the Pareto Principle <cite class="ltx_cite ltx_citemacro_citep">(Juran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib25" title="">2005</a>)</cite>, annotators in fact spent most of their time and effort amending a small fraction of AI recommendations. The tiny bounding boxes (see examples of three tiny faces in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">2</span></a>), duplicate detections (often clustered), and temporally sparse detections (short tracks) are the most costly recommendations. Addressing these issues allows annotators to have better continuity in their workflow.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.1">Model explanation should not increase task complexity.</span> Initially, we offered model explanations using ”Certain” and ”Uncertain” labels based on the face detector’s confidence, hoping this can assist users’ decision-making. But video recordings and user feedback revealed that the extra information in fact increased the task complexity and caused unnecessary confusion. This design was eventually not considered in the formal experiment.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Observing how human annotators review AI recommendations (bounding box pre-annotations) in multi-object detection and tracking tasks inspired us to <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">break the complex workflow into three fundamental user actions:</span> <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p3.1.2">accept</span>, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p3.1.3">reject</span>, or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.SS2.p3.1.4">solve</span>, each coming with a higher cost in time. Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.F3" title="Figure 3 ‣ 3.2. Pilot studies ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">3</span></a> explains each action’s time complexity. We can connect these three actions with our two main objectives (time and recall) to make <span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.5">a simple deduction to identify the human-AI complementary strengths</span> in this task:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1</span>
<div class="ltx_para" id="S3.I2.ix1.p1">
<p class="ltx_p" id="S3.I2.ix1.p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.I2.ix1.p1.1.1">reject</span> improves precision and <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.I2.ix1.p1.1.2">solve</span> improves recall. A correct <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.I2.ix1.p1.1.3">accept</span> improves both.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2</span>
<div class="ltx_para" id="S3.I2.ix2.p1">
<p class="ltx_p" id="S3.I2.ix2.p1.1">It takes the AI constant time to <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.I2.ix2.p1.1.1">solve</span> additional cases (give more recommendations) with a downside of more false-positive boxes for humans to reject.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3</span>
<div class="ltx_para" id="S3.I2.ix3.p1">
<p class="ltx_p" id="S3.I2.ix3.p1.1">Humans are faster at <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.I2.ix3.p1.1.1">reject</span>ing a false-positive (incorrect) box than to <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.I2.ix3.p1.1.2">solve</span> a false-negative (missing) box.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4</span>
<div class="ltx_para" id="S3.I2.ix4.p1">
<p class="ltx_p" id="S3.I2.ix4.p1.1">We also know recall is more important than precision in video anonymization tasks.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5</span>
<div class="ltx_para" id="S3.I2.ix5.p1">
<p class="ltx_p" id="S3.I2.ix5.p1.1">Thus, <span class="ltx_text ltx_font_bold" id="S3.I2.ix5.p1.1.1">a clear path to better human-AI team performance is to delegate more <span class="ltx_text ltx_font_italic" id="S3.I2.ix5.p1.1.1.1">solve</span> actions to the AI, so the human’s overall effort is reduced by doing more easy <span class="ltx_text ltx_font_italic" id="S3.I2.ix5.p1.1.1.2">rejecting</span> and only <span class="ltx_text ltx_font_italic" id="S3.I2.ix5.p1.1.1.3">solving</span> the most challenging faces</span>.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="193" id="S3.F3.g1" src="extracted/5905702/fig_user_actions_timeline.jpg" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.24.7.1" style="font-size:90%;">Figure 3</span>. </span><span class="ltx_text" id="S3.F3.12.6" style="font-size:90%;">When reviewing the AI teammate’s recommendations (green bounding boxes), a user takes one of the three actions for each box: <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.F3.12.6.1">accept</span>, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.F3.12.6.2">reject</span>, or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.F3.12.6.3">solve</span>. In video annotation, because <span class="ltx_text ltx_font_bold" id="S3.F3.12.6.4">the boxes are temporally tracked across many frames, each action’s time complexity is drastically different</span>, note the two types of <span class="ltx_text ltx_font_italic" id="S3.F3.12.6.5">Solve</span> in frame 0 can come at different cost, too. 
<br class="ltx_break"/>    ID:1 – A user can <span class="ltx_text ltx_font_italic" id="S3.F3.12.6.6">accept</span> the true-positive track ID:1 boxes without any action. 
<br class="ltx_break"/>    ID:2 – The entire false-positive ID:2 track can be rejected with two mouse clicks by deleting the ID in any of the frames, which is <math alttext="O(1)" class="ltx_Math" display="inline" id="S3.F3.7.1.m1.1"><semantics id="S3.F3.7.1.m1.1b"><mrow id="S3.F3.7.1.m1.1.2" xref="S3.F3.7.1.m1.1.2.cmml"><mi id="S3.F3.7.1.m1.1.2.2" xref="S3.F3.7.1.m1.1.2.2.cmml">O</mi><mo id="S3.F3.7.1.m1.1.2.1" xref="S3.F3.7.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.F3.7.1.m1.1.2.3.2" xref="S3.F3.7.1.m1.1.2.cmml"><mo id="S3.F3.7.1.m1.1.2.3.2.1" stretchy="false" xref="S3.F3.7.1.m1.1.2.cmml">(</mo><mn id="S3.F3.7.1.m1.1.1" xref="S3.F3.7.1.m1.1.1.cmml">1</mn><mo id="S3.F3.7.1.m1.1.2.3.2.2" stretchy="false" xref="S3.F3.7.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.7.1.m1.1c"><apply id="S3.F3.7.1.m1.1.2.cmml" xref="S3.F3.7.1.m1.1.2"><times id="S3.F3.7.1.m1.1.2.1.cmml" xref="S3.F3.7.1.m1.1.2.1"></times><ci id="S3.F3.7.1.m1.1.2.2.cmml" xref="S3.F3.7.1.m1.1.2.2">𝑂</ci><cn id="S3.F3.7.1.m1.1.1.cmml" type="integer" xref="S3.F3.7.1.m1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.7.1.m1.1d">O(1)</annotation><annotation encoding="application/x-llamapun" id="S3.F3.7.1.m1.1e">italic_O ( 1 )</annotation></semantics></math> in time complexity. 
<br class="ltx_break"/>    ID:3 – False-positive recommendations, like track <span class="ltx_text ltx_font_bold" id="S3.F3.12.6.7">ID:3</span>, are the most time-consuming to <span class="ltx_text ltx_font_bold ltx_font_italic" id="S3.F3.12.6.8">solve</span>: the user can delete and redo this face, or manually adjust every frame until the AI’s pre-annotation becomes acceptable with <math alttext="\geq n" class="ltx_Math" display="inline" id="S3.F3.8.2.m2.1"><semantics id="S3.F3.8.2.m2.1b"><mrow id="S3.F3.8.2.m2.1.1" xref="S3.F3.8.2.m2.1.1.cmml"><mi id="S3.F3.8.2.m2.1.1.2" xref="S3.F3.8.2.m2.1.1.2.cmml"></mi><mo id="S3.F3.8.2.m2.1.1.1" xref="S3.F3.8.2.m2.1.1.1.cmml">≥</mo><mi id="S3.F3.8.2.m2.1.1.3" xref="S3.F3.8.2.m2.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.8.2.m2.1c"><apply id="S3.F3.8.2.m2.1.1.cmml" xref="S3.F3.8.2.m2.1.1"><geq id="S3.F3.8.2.m2.1.1.1.cmml" xref="S3.F3.8.2.m2.1.1.1"></geq><csymbol cd="latexml" id="S3.F3.8.2.m2.1.1.2.cmml" xref="S3.F3.8.2.m2.1.1.2">absent</csymbol><ci id="S3.F3.8.2.m2.1.1.3.cmml" xref="S3.F3.8.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.8.2.m2.1d">\geq n</annotation><annotation encoding="application/x-llamapun" id="S3.F3.8.2.m2.1e">≥ italic_n</annotation></semantics></math> mouse clicks, <math alttext="O(n)" class="ltx_Math" display="inline" id="S3.F3.9.3.m3.1"><semantics id="S3.F3.9.3.m3.1b"><mrow id="S3.F3.9.3.m3.1.2" xref="S3.F3.9.3.m3.1.2.cmml"><mi id="S3.F3.9.3.m3.1.2.2" xref="S3.F3.9.3.m3.1.2.2.cmml">O</mi><mo id="S3.F3.9.3.m3.1.2.1" xref="S3.F3.9.3.m3.1.2.1.cmml">⁢</mo><mrow id="S3.F3.9.3.m3.1.2.3.2" xref="S3.F3.9.3.m3.1.2.cmml"><mo id="S3.F3.9.3.m3.1.2.3.2.1" stretchy="false" xref="S3.F3.9.3.m3.1.2.cmml">(</mo><mi id="S3.F3.9.3.m3.1.1" xref="S3.F3.9.3.m3.1.1.cmml">n</mi><mo id="S3.F3.9.3.m3.1.2.3.2.2" stretchy="false" xref="S3.F3.9.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.9.3.m3.1c"><apply id="S3.F3.9.3.m3.1.2.cmml" xref="S3.F3.9.3.m3.1.2"><times id="S3.F3.9.3.m3.1.2.1.cmml" xref="S3.F3.9.3.m3.1.2.1"></times><ci id="S3.F3.9.3.m3.1.2.2.cmml" xref="S3.F3.9.3.m3.1.2.2">𝑂</ci><ci id="S3.F3.9.3.m3.1.1.cmml" xref="S3.F3.9.3.m3.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.9.3.m3.1d">O(n)</annotation><annotation encoding="application/x-llamapun" id="S3.F3.9.3.m3.1e">italic_O ( italic_n )</annotation></semantics></math>. 
<br class="ltx_break"/>    ID:4 – In frame 0, to <span class="ltx_text ltx_font_italic" id="S3.F3.12.6.9">solve</span> the false-negative missing box for the left-most person, a user needs to manually draw a box and adjust its location and size until the AI-suggested box <span class="ltx_text ltx_font_bold" id="S3.F3.12.6.10">ID:4</span> comes in with <math alttext="\leq n" class="ltx_Math" display="inline" id="S3.F3.10.4.m4.1"><semantics id="S3.F3.10.4.m4.1b"><mrow id="S3.F3.10.4.m4.1.1" xref="S3.F3.10.4.m4.1.1.cmml"><mi id="S3.F3.10.4.m4.1.1.2" xref="S3.F3.10.4.m4.1.1.2.cmml"></mi><mo id="S3.F3.10.4.m4.1.1.1" xref="S3.F3.10.4.m4.1.1.1.cmml">≤</mo><mi id="S3.F3.10.4.m4.1.1.3" xref="S3.F3.10.4.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.10.4.m4.1c"><apply id="S3.F3.10.4.m4.1.1.cmml" xref="S3.F3.10.4.m4.1.1"><leq id="S3.F3.10.4.m4.1.1.1.cmml" xref="S3.F3.10.4.m4.1.1.1"></leq><csymbol cd="latexml" id="S3.F3.10.4.m4.1.1.2.cmml" xref="S3.F3.10.4.m4.1.1.2">absent</csymbol><ci id="S3.F3.10.4.m4.1.1.3.cmml" xref="S3.F3.10.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.10.4.m4.1d">\leq n</annotation><annotation encoding="application/x-llamapun" id="S3.F3.10.4.m4.1e">≤ italic_n</annotation></semantics></math> mouse clicks, <math alttext="O(n)" class="ltx_Math" display="inline" id="S3.F3.11.5.m5.1"><semantics id="S3.F3.11.5.m5.1b"><mrow id="S3.F3.11.5.m5.1.2" xref="S3.F3.11.5.m5.1.2.cmml"><mi id="S3.F3.11.5.m5.1.2.2" xref="S3.F3.11.5.m5.1.2.2.cmml">O</mi><mo id="S3.F3.11.5.m5.1.2.1" xref="S3.F3.11.5.m5.1.2.1.cmml">⁢</mo><mrow id="S3.F3.11.5.m5.1.2.3.2" xref="S3.F3.11.5.m5.1.2.cmml"><mo id="S3.F3.11.5.m5.1.2.3.2.1" stretchy="false" xref="S3.F3.11.5.m5.1.2.cmml">(</mo><mi id="S3.F3.11.5.m5.1.1" xref="S3.F3.11.5.m5.1.1.cmml">n</mi><mo id="S3.F3.11.5.m5.1.2.3.2.2" stretchy="false" xref="S3.F3.11.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F3.11.5.m5.1c"><apply id="S3.F3.11.5.m5.1.2.cmml" xref="S3.F3.11.5.m5.1.2"><times id="S3.F3.11.5.m5.1.2.1.cmml" xref="S3.F3.11.5.m5.1.2.1"></times><ci id="S3.F3.11.5.m5.1.2.2.cmml" xref="S3.F3.11.5.m5.1.2.2">𝑂</ci><ci id="S3.F3.11.5.m5.1.1.cmml" xref="S3.F3.11.5.m5.1.1">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.11.5.m5.1d">O(n)</annotation><annotation encoding="application/x-llamapun" id="S3.F3.11.5.m5.1e">italic_O ( italic_n )</annotation></semantics></math> where <math alttext="n" class="ltx_Math" display="inline" id="S3.F3.12.6.m6.1"><semantics id="S3.F3.12.6.m6.1b"><mi id="S3.F3.12.6.m6.1.1" xref="S3.F3.12.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.F3.12.6.m6.1c"><ci id="S3.F3.12.6.m6.1.1.cmml" xref="S3.F3.12.6.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.12.6.m6.1d">n</annotation><annotation encoding="application/x-llamapun" id="S3.F3.12.6.m6.1e">italic_n</annotation></semantics></math> is the number of frames.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F3.25">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F3.26">Three frames of hand-drawn images show the three actions of accept, reject, or solve on different types of AI-recommended boxes. A missing or incorrect face detection requires to solve, a false-positive detection requires to reject, and true-positive detections require no action.</p>
</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>The false-positive-robust (FPR) tracker</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We adopted a tracking-by-detection system to produce face pre-annotations (Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S2" title="2. Related work ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">2</span></a>), the two-part system design allows us to feed the same per-frame face detection from RetinaFace <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib19" title="">2019</a>)</cite> to different downstream multi-object trackers like the ByteTrack <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib65" title="">2022b</a>)</cite> or our own designs for a fair comparison. Learning from our pilot studies observations, we propose the false-positive-robust (FPR) tracker that specifically provides user-friendly annotation recommendations. We use <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">the following unconventional strategies</span> to design the FPR tracker that can take overwhelmingly noisy detections with a high false positive rate as input but outputs ”clean” tracks for a human-in-the-loop workflow:</p>
<ul class="ltx_itemize" id="S3.I3">
<li class="ltx_item" id="S3.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i1.p1">
<p class="ltx_p" id="S3.I3.i1.p1.1">To improve the AI’s recall, we apply an <span class="ltx_text ltx_font_bold" id="S3.I3.i1.p1.1.1">extremely low threshold (<math alttext="t\geq 0.01,~{}t\in[0,1]" class="ltx_Math" display="inline" id="S3.I3.i1.p1.1.1.m1.4"><semantics id="S3.I3.i1.p1.1.1.m1.4a"><mrow id="S3.I3.i1.p1.1.1.m1.4.4.2" xref="S3.I3.i1.p1.1.1.m1.4.4.3.cmml"><mrow id="S3.I3.i1.p1.1.1.m1.3.3.1.1" xref="S3.I3.i1.p1.1.1.m1.3.3.1.1.cmml"><mi id="S3.I3.i1.p1.1.1.m1.3.3.1.1.2" xref="S3.I3.i1.p1.1.1.m1.3.3.1.1.2.cmml">t</mi><mo id="S3.I3.i1.p1.1.1.m1.3.3.1.1.1" xref="S3.I3.i1.p1.1.1.m1.3.3.1.1.1.cmml">≥</mo><mn id="S3.I3.i1.p1.1.1.m1.3.3.1.1.3" xref="S3.I3.i1.p1.1.1.m1.3.3.1.1.3.cmml">0.01</mn></mrow><mo id="S3.I3.i1.p1.1.1.m1.4.4.2.3" rspace="0.497em" xref="S3.I3.i1.p1.1.1.m1.4.4.3a.cmml">,</mo><mrow id="S3.I3.i1.p1.1.1.m1.4.4.2.2" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.cmml"><mi id="S3.I3.i1.p1.1.1.m1.4.4.2.2.2" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.2.cmml">t</mi><mo id="S3.I3.i1.p1.1.1.m1.4.4.2.2.1" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.1.cmml">∈</mo><mrow id="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.2" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.1.cmml"><mo id="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.2.1" stretchy="false" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.1.cmml">[</mo><mn id="S3.I3.i1.p1.1.1.m1.1.1" xref="S3.I3.i1.p1.1.1.m1.1.1.cmml">0</mn><mo id="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.2.2" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.1.cmml">,</mo><mn id="S3.I3.i1.p1.1.1.m1.2.2" xref="S3.I3.i1.p1.1.1.m1.2.2.cmml">1</mn><mo id="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.2.3" stretchy="false" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.I3.i1.p1.1.1.m1.4b"><apply id="S3.I3.i1.p1.1.1.m1.4.4.3.cmml" xref="S3.I3.i1.p1.1.1.m1.4.4.2"><csymbol cd="ambiguous" id="S3.I3.i1.p1.1.1.m1.4.4.3a.cmml" xref="S3.I3.i1.p1.1.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S3.I3.i1.p1.1.1.m1.3.3.1.1.cmml" xref="S3.I3.i1.p1.1.1.m1.3.3.1.1"><geq id="S3.I3.i1.p1.1.1.m1.3.3.1.1.1.cmml" xref="S3.I3.i1.p1.1.1.m1.3.3.1.1.1"></geq><ci id="S3.I3.i1.p1.1.1.m1.3.3.1.1.2.cmml" xref="S3.I3.i1.p1.1.1.m1.3.3.1.1.2">𝑡</ci><cn id="S3.I3.i1.p1.1.1.m1.3.3.1.1.3.cmml" type="float" xref="S3.I3.i1.p1.1.1.m1.3.3.1.1.3">0.01</cn></apply><apply id="S3.I3.i1.p1.1.1.m1.4.4.2.2.cmml" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2"><in id="S3.I3.i1.p1.1.1.m1.4.4.2.2.1.cmml" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.1"></in><ci id="S3.I3.i1.p1.1.1.m1.4.4.2.2.2.cmml" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.2">𝑡</ci><interval closure="closed" id="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.1.cmml" xref="S3.I3.i1.p1.1.1.m1.4.4.2.2.3.2"><cn id="S3.I3.i1.p1.1.1.m1.1.1.cmml" type="integer" xref="S3.I3.i1.p1.1.1.m1.1.1">0</cn><cn id="S3.I3.i1.p1.1.1.m1.2.2.cmml" type="integer" xref="S3.I3.i1.p1.1.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i1.p1.1.1.m1.4c">t\geq 0.01,~{}t\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i1.p1.1.1.m1.4d">italic_t ≥ 0.01 , italic_t ∈ [ 0 , 1 ]</annotation></semantics></math>) on the face detector’s confidence score</span> to keep any potentially useful detected boxes. This is not a viable solution for Autonomous AI systems but we are working in conjunction with a human.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i2.p1">
<p class="ltx_p" id="S3.I3.i2.p1.1">The consequence of such a low face detector threshold is <span class="ltx_text ltx_font_bold" id="S3.I3.i2.p1.1.1">clusters of overlapping boxes</span> on small faces. Our solution: for each cluster, we perform non-maximum suppression <cite class="ltx_cite ltx_citemacro_citep">(Neubeck and Van Gool, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib44" title="">2006</a>)</cite> by only keeping the single bounding box with the highest confidence score because in most cases they are duplicate detections on one true face. This step also improves the AI recommendations’ precision.</p>
</div>
</li>
<li class="ltx_item" id="S3.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I3.i3.p1">
<p class="ltx_p" id="S3.I3.i3.p1.2">Finally, based on our observation that <span class="ltx_text ltx_font_bold" id="S3.I3.i3.p1.2.1">the majority of temporally sparse detections are false positives</span> induced by the low threshold, we remove any tracks that are shorter than <math alttext="m" class="ltx_Math" display="inline" id="S3.I3.i3.p1.1.m1.1"><semantics id="S3.I3.i3.p1.1.m1.1a"><mi id="S3.I3.i3.p1.1.m1.1.1" xref="S3.I3.i3.p1.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S3.I3.i3.p1.1.m1.1b"><ci id="S3.I3.i3.p1.1.m1.1.1.cmml" xref="S3.I3.i3.p1.1.m1.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i3.p1.1.m1.1c">m</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i3.p1.1.m1.1d">italic_m</annotation></semantics></math> consecutive frames so they do not interrupt users’ continuity. We used <math alttext="m=10" class="ltx_Math" display="inline" id="S3.I3.i3.p1.2.m2.1"><semantics id="S3.I3.i3.p1.2.m2.1a"><mrow id="S3.I3.i3.p1.2.m2.1.1" xref="S3.I3.i3.p1.2.m2.1.1.cmml"><mi id="S3.I3.i3.p1.2.m2.1.1.2" xref="S3.I3.i3.p1.2.m2.1.1.2.cmml">m</mi><mo id="S3.I3.i3.p1.2.m2.1.1.1" xref="S3.I3.i3.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.I3.i3.p1.2.m2.1.1.3" xref="S3.I3.i3.p1.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I3.i3.p1.2.m2.1b"><apply id="S3.I3.i3.p1.2.m2.1.1.cmml" xref="S3.I3.i3.p1.2.m2.1.1"><eq id="S3.I3.i3.p1.2.m2.1.1.1.cmml" xref="S3.I3.i3.p1.2.m2.1.1.1"></eq><ci id="S3.I3.i3.p1.2.m2.1.1.2.cmml" xref="S3.I3.i3.p1.2.m2.1.1.2">𝑚</ci><cn id="S3.I3.i3.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.I3.i3.p1.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I3.i3.p1.2.m2.1c">m=10</annotation><annotation encoding="application/x-llamapun" id="S3.I3.i3.p1.2.m2.1d">italic_m = 10</annotation></semantics></math> in the FPR tracker. Although some true-positive faces are also removed, users are much faster at solving an unlabeled face from scratch than filling the gaps between temporally sparse detections.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To design the experiment, we also need a restrained AI that generates recommendations of similar performance (F1 score) but with high precision. This is done by using only the high-confidence (<math alttext="t\geq 0.8,t\in[0,1]" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.4"><semantics id="S3.SS3.p2.1.m1.4a"><mrow id="S3.SS3.p2.1.m1.4.4.2" xref="S3.SS3.p2.1.m1.4.4.3.cmml"><mrow id="S3.SS3.p2.1.m1.3.3.1.1" xref="S3.SS3.p2.1.m1.3.3.1.1.cmml"><mi id="S3.SS3.p2.1.m1.3.3.1.1.2" xref="S3.SS3.p2.1.m1.3.3.1.1.2.cmml">t</mi><mo id="S3.SS3.p2.1.m1.3.3.1.1.1" xref="S3.SS3.p2.1.m1.3.3.1.1.1.cmml">≥</mo><mn id="S3.SS3.p2.1.m1.3.3.1.1.3" xref="S3.SS3.p2.1.m1.3.3.1.1.3.cmml">0.8</mn></mrow><mo id="S3.SS3.p2.1.m1.4.4.2.3" xref="S3.SS3.p2.1.m1.4.4.3a.cmml">,</mo><mrow id="S3.SS3.p2.1.m1.4.4.2.2" xref="S3.SS3.p2.1.m1.4.4.2.2.cmml"><mi id="S3.SS3.p2.1.m1.4.4.2.2.2" xref="S3.SS3.p2.1.m1.4.4.2.2.2.cmml">t</mi><mo id="S3.SS3.p2.1.m1.4.4.2.2.1" xref="S3.SS3.p2.1.m1.4.4.2.2.1.cmml">∈</mo><mrow id="S3.SS3.p2.1.m1.4.4.2.2.3.2" xref="S3.SS3.p2.1.m1.4.4.2.2.3.1.cmml"><mo id="S3.SS3.p2.1.m1.4.4.2.2.3.2.1" stretchy="false" xref="S3.SS3.p2.1.m1.4.4.2.2.3.1.cmml">[</mo><mn id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml">0</mn><mo id="S3.SS3.p2.1.m1.4.4.2.2.3.2.2" xref="S3.SS3.p2.1.m1.4.4.2.2.3.1.cmml">,</mo><mn id="S3.SS3.p2.1.m1.2.2" xref="S3.SS3.p2.1.m1.2.2.cmml">1</mn><mo id="S3.SS3.p2.1.m1.4.4.2.2.3.2.3" stretchy="false" xref="S3.SS3.p2.1.m1.4.4.2.2.3.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.4b"><apply id="S3.SS3.p2.1.m1.4.4.3.cmml" xref="S3.SS3.p2.1.m1.4.4.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.4.4.3a.cmml" xref="S3.SS3.p2.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S3.SS3.p2.1.m1.3.3.1.1.cmml" xref="S3.SS3.p2.1.m1.3.3.1.1"><geq id="S3.SS3.p2.1.m1.3.3.1.1.1.cmml" xref="S3.SS3.p2.1.m1.3.3.1.1.1"></geq><ci id="S3.SS3.p2.1.m1.3.3.1.1.2.cmml" xref="S3.SS3.p2.1.m1.3.3.1.1.2">𝑡</ci><cn id="S3.SS3.p2.1.m1.3.3.1.1.3.cmml" type="float" xref="S3.SS3.p2.1.m1.3.3.1.1.3">0.8</cn></apply><apply id="S3.SS3.p2.1.m1.4.4.2.2.cmml" xref="S3.SS3.p2.1.m1.4.4.2.2"><in id="S3.SS3.p2.1.m1.4.4.2.2.1.cmml" xref="S3.SS3.p2.1.m1.4.4.2.2.1"></in><ci id="S3.SS3.p2.1.m1.4.4.2.2.2.cmml" xref="S3.SS3.p2.1.m1.4.4.2.2.2">𝑡</ci><interval closure="closed" id="S3.SS3.p2.1.m1.4.4.2.2.3.1.cmml" xref="S3.SS3.p2.1.m1.4.4.2.2.3.2"><cn id="S3.SS3.p2.1.m1.1.1.cmml" type="integer" xref="S3.SS3.p2.1.m1.1.1">0</cn><cn id="S3.SS3.p2.1.m1.2.2.cmml" type="integer" xref="S3.SS3.p2.1.m1.2.2">1</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.4c">t\geq 0.8,t\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.4d">italic_t ≥ 0.8 , italic_t ∈ [ 0 , 1 ]</annotation></semantics></math>) face detections with ByteTrack. To ensure fair comparison and reduce moving parts in our systems, we use the same face detection model RetinaFace <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib19" title="">2019</a>)</cite> for both AI teammates. It is the two different (fully transparent) trackers we apply that push the AI recommendations towards either high-precision or high-recall (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.2">Note that we were only able to optimize the FPR tracker and ByteTrack through pilot studies because the ground truth data was not available for the 36 videos used in the user study. After the study, we aggregated the annotations from all 78 participants (2,780 submissions in total) to form an expert-reviewed consensus to serve as the ground truth. It turns out the zealous AI recommendations (FPR tracker) yielded an F1 score of <math alttext="90.9\%" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mrow id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mn id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">90.9</mn><mo id="S3.SS3.p3.1.m1.1.1.1" xref="S3.SS3.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1.1">percent</csymbol><cn id="S3.SS3.p3.1.m1.1.1.2.cmml" type="float" xref="S3.SS3.p3.1.m1.1.1.2">90.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">90.9\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">90.9 %</annotation></semantics></math> and the restrained AI (ByteTrack) had an F1 score of <math alttext="93.4\%" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml"><mn id="S3.SS3.p3.2.m2.1.1.2" xref="S3.SS3.p3.2.m2.1.1.2.cmml">93.4</mn><mo id="S3.SS3.p3.2.m2.1.1.1" xref="S3.SS3.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1"><csymbol cd="latexml" id="S3.SS3.p3.2.m2.1.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1.1">percent</csymbol><cn id="S3.SS3.p3.2.m2.1.1.2.cmml" type="float" xref="S3.SS3.p3.2.m2.1.1.2">93.4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">93.4\%</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">93.4 %</annotation></semantics></math>. While the two AIs did not provide identical initial performance for their human teammates, we achieved the goal of two distinctive high-recall and high-precision AIs (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F5" title="Figure 5 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5</span></a>). The performance gap also provided us additional evidence to support our previous deduction on the zealous AI being the superior choice for this task, which we will discuss in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.SS1" title="5.1. Q1: Can the human-AI teams achieve ”complementary team performance” in this task? ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S3.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="137" id="S3.F4.g1" src="extracted/5905702/fig_ego4d_examples.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>. </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Screenshot examples of Ego4D videos <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib24" title="">2022</a>)</cite> used in our face annotation experiment. Easy videos include about one face to annotate in each non-empty frame. Medium videos include about two faces. Hard videos include three or more faces. Videos with more faces are expected to take longer time to finish. The study results show shorter to longer completion times for Easy, Medium, and Hard videos in both parts (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F6" title="Figure 6 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F7" title="Figure 7 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">7</span></a>), demonstrating that our video difficulty categorization is reasonable and performed as expected. We also considered scene diversity, box size (smaller faces are harder), and camera movement intensity (more movement is harder) to ensure a balanced difficulty distribution in selecting the specific videos.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F4.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F4.5">Three clusters of images that show examples of videos in different difficulty levels.</p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this work, we aim to investigate how restrained and zealous AI recommendations will affect human-AI team performance. We are also curious if the collaboration experience with an AI teammate can affect users’ skills, should they lose access to AI assistance in the future. We design a two-part empirical study to test the restrained and zealous AIs in a recall-demanding high-stakes task.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>The task and data.</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Face annotation for video anonymization is a perfect example of recall-demanding tasks – a missing face in a single frame can reveal a person’s identity in the entire video. The high-stakes nature requires humans to annotate or verify every frame, yet the manual process will become the throughput bottleneck. The tedious process and long hours may also fatigue annotators and cause a decline in quality. In addition, <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">because the task of locating faces requires no specific training or domain expertise, it should help the generalizability of our observations</span> to other AI-assisted annotation tasks or even to other recall-demanding human-AI collaboration tasks.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In our human-AI collaboration setting, the AI teammate provides recommendations in the form of bounding boxes (see examples in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">2</span></a>), and a user reviews each of the AI’s pre-annotations to make one of the three decisions shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.F3" title="Figure 3 ‣ 3.2. Pilot studies ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">3</span></a>. We evaluate users’ performance on the two most important metrics for face anonymization: <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">task completion time</span> and <span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.2">recall</span>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">To test different AI recommendations in a real-world setting, we curate 36 first-person videos from a large-scale egocentric video dataset Ego4D <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib24" title="">2022</a>)</cite>. Privacy has always been a major concern for datasets collecting human activities so first-person videos are ideal for this study. The videos we selected include various indoor social activities that are suitable for benchmarking face detection and annotation tasks. Each video clip is 30 seconds long, or 900 frames. We estimate each video takes about 30 minutes to one hour to fully annotate, depending on its difficulty.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">The different annotation methods (without or with different AI recommendations) adopted by the three treatment groups are the first level of independent variables that we will discuss in the next section. The second level of independent variables that can affect users’ performance is the difficulty of the videos. We divide the videos into Easy, Medium, and Hard categories based on the average number of people one needs to track simultaneously in non-empty frames (see examples in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.F4" title="Figure 4 ‣ 3.3. The false-positive-robust (FPR) tracker ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">4</span></a>). We also considered factors like scene diversity, bounding box size, and camera movement intensity that affect the annotation difficulty in a more subtle way. Based on this overall difficulty ranking distribution, we ensure Part 1 and Part 2 videos are not only similar in content but also consistent in annotation difficulty.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">We generate the bounding box ground truth by aggregating the crowd’s annotations to reach a consensus, which is further reviewed and refined by a domain expert. We used an equal number of manual and AI-assisted submissions for each video to generate an unbiased ground truth.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">On task completion time, <span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">annotators are advised to finish each video without taking breaks longer than five minutes</span> but we still need to reject outlier video completion times caused by a known limitation of the annotation tool – the timer continues if an ongoing task window was left idle, or the timer will reset if the annotator continues from previously saved progress. We adopted median absolute deviation (MAD) <cite class="ltx_cite ltx_citemacro_citep">(Leys et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib33" title="">2013</a>)</cite> by comparing each video’s completion time within each group to reject 420 out of 2780 (15.11%) completed videos, including completion times that are less than six minutes (the minimum time needed to verify each frame) or longer than median + 3 * MAD. The rejected videos also include all 36 submissions from one particular problematic user, see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.SS2" title="6.2. Can AI teammates set the quality lower bound in a crowdsourcing setting? ‣ 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6.2</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.1">Group</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.2">Novice</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.3">Veteran</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.4">Part 1 method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T1.2.1.1.5">Submissions</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.6">Part 2 method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.2.1.1.7">Submissions</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.2.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.1">A</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.2">11</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.3">14</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.4">Human only</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.1.5">602</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.6">Human only</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.2.2.1.7">299</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.3.2.1">B</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.2">14</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.3.2.3">12</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.4">Restrained AI + Human</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.2.3.2.5">619</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.6">Human only</td>
<td class="ltx_td ltx_align_center" id="S4.T1.2.3.2.7">304</td>
</tr>
<tr class="ltx_tr" id="S4.T1.2.4.3">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.2.4.3.1">C</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.4.3.2">13</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.2.4.3.3">13</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.4.3.4">Zealous AI + Human</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.2.4.3.5">621</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.4.3.6">Human only</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.2.4.3.7">299</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.3.1.1" style="font-size:90%;">Table 1</span>. </span><span class="ltx_text" id="S4.T1.4.2" style="font-size:90%;">In the two-part study, the three treatment groups use different methods in Part 1, but we remove all AI assistance in Part 2. The novice and veteran workers represent a balance of different user expertise in each group. The submission numbers are the 30-second annotated videos each group finished. Note that Group A is one user short as a particular worker was later rejected because of repeated bad submissions.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Participants and three treatment groups.</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">A total of 78 in-house professional data annotators completed our study. It is important to note that in this project <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">they are paid at their regular hourly rate, so participants are not motivated by compensation to work faster</span>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">In the between-subjects experiment, participants were evenly split into three 26-people treatment groups to annotate identical sets of videos. The annotators’ profiles ensure similar average experience between the groups. The assignments also considered people’s day/night shifts and computer setup to ensure a fair comparison.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The participants have at least two months or up to five years of data annotation experience, with an average experience of 20.9 months. We use the median experience of 17 months to split the user expertise factor so each group has about half novice and half veteran workers (see Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S4.T1" title="Table 1 ‣ 4.1. The task and data. ‣ 4. Experiments ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">1</span></a>). All annotators were aware of participating in a study testing new AI-assisted annotation algorithms and were free to leave the study at any time. The Human Subjects Committee (HSC) approved our procedure and each participant was provided a consent form during the survey session.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Group A servers as the baseline, they use an efficient annotation tool that supports linear interpolation <cite class="ltx_cite ltx_citemacro_citep">(Vondrick et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib53" title="">2013</a>)</cite> but solely relies on manual annotation in both parts of the study. Groups B and C work with their AI teammates in Part 1 of the study. They use the same tool as Group A but the AI will have pre-annotated the videos (see example in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">2</span></a>). Group B reviews the restrained AI recommendations that prioritize precision. Group C reviews the zealous AI recommendations that prioritize recall (see a, b in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S0.F1" title="Figure 1 ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">1</span></a>). The treatment groups are summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S4.T1" title="Table 1 ‣ 4.1. The task and data. ‣ 4. Experiments ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">1</span></a> or Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">2</span></a>. We informed the participants in Groups B and C that they are working with an AI that provides recommendations to assist their annotation work, but they do not know the difference between the two human-AI groups.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Experiment procedure of the two-part study.</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Before beginning the study, we organized a video conference training session with each treatment group to calibrate the task background and requirements. All participants were also asked to review the instruction text and a training video on the landing page. Previous pilot study users become supervisors in each group to ensure all participants have finished the training and the surveys before processing to the next step. We also created three instant messaging (IM) groups to answer questions and send out reminders when necessary. The overall procedure can be summarized as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A0.EGx2">
<tbody id="S4.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S4.Ex1.2.1.1.1">Training</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\rightarrow\text{Survey 0}\rightarrow" class="ltx_Math" display="inline" id="S4.Ex1.m2.1"><semantics id="S4.Ex1.m2.1a"><mrow id="S4.Ex1.m2.1.1" xref="S4.Ex1.m2.1.1.cmml"><mi id="S4.Ex1.m2.1.1.2" xref="S4.Ex1.m2.1.1.2.cmml"></mi><mo id="S4.Ex1.m2.1.1.3" stretchy="false" xref="S4.Ex1.m2.1.1.3.cmml">→</mo><mtext id="S4.Ex1.m2.1.1.4" xref="S4.Ex1.m2.1.1.4a.cmml">Survey 0</mtext><mo id="S4.Ex1.m2.1.1.5" stretchy="false" xref="S4.Ex1.m2.1.1.5.cmml">→</mo><mi id="S4.Ex1.m2.1.1.6" xref="S4.Ex1.m2.1.1.6.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m2.1b"><apply id="S4.Ex1.m2.1.1.cmml" xref="S4.Ex1.m2.1.1"><and id="S4.Ex1.m2.1.1a.cmml" xref="S4.Ex1.m2.1.1"></and><apply id="S4.Ex1.m2.1.1b.cmml" xref="S4.Ex1.m2.1.1"><ci id="S4.Ex1.m2.1.1.3.cmml" xref="S4.Ex1.m2.1.1.3">→</ci><csymbol cd="latexml" id="S4.Ex1.m2.1.1.2.cmml" xref="S4.Ex1.m2.1.1.2">absent</csymbol><ci id="S4.Ex1.m2.1.1.4a.cmml" xref="S4.Ex1.m2.1.1.4"><mtext id="S4.Ex1.m2.1.1.4.cmml" xref="S4.Ex1.m2.1.1.4">Survey 0</mtext></ci></apply><apply id="S4.Ex1.m2.1.1c.cmml" xref="S4.Ex1.m2.1.1"><ci id="S4.Ex1.m2.1.1.5.cmml" xref="S4.Ex1.m2.1.1.5">→</ci><share href="https://arxiv.org/html/2410.11860v1#S4.Ex1.m2.1.1.4.cmml" id="S4.Ex1.m2.1.1d.cmml" xref="S4.Ex1.m2.1.1"></share><csymbol cd="latexml" id="S4.Ex1.m2.1.1.6.cmml" xref="S4.Ex1.m2.1.1.6">absent</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m2.1c">\displaystyle\rightarrow\text{Survey 0}\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m2.1d">→ Survey 0 →</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S4.Ex2.2.1.1.1">Part 1 (24 videos, different methods)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\rightarrow\text{Survey 1}\rightarrow" class="ltx_Math" display="inline" id="S4.Ex2.m2.1"><semantics id="S4.Ex2.m2.1a"><mrow id="S4.Ex2.m2.1.1" xref="S4.Ex2.m2.1.1.cmml"><mi id="S4.Ex2.m2.1.1.2" xref="S4.Ex2.m2.1.1.2.cmml"></mi><mo id="S4.Ex2.m2.1.1.3" stretchy="false" xref="S4.Ex2.m2.1.1.3.cmml">→</mo><mtext id="S4.Ex2.m2.1.1.4" xref="S4.Ex2.m2.1.1.4a.cmml">Survey 1</mtext><mo id="S4.Ex2.m2.1.1.5" stretchy="false" xref="S4.Ex2.m2.1.1.5.cmml">→</mo><mi id="S4.Ex2.m2.1.1.6" xref="S4.Ex2.m2.1.1.6.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m2.1b"><apply id="S4.Ex2.m2.1.1.cmml" xref="S4.Ex2.m2.1.1"><and id="S4.Ex2.m2.1.1a.cmml" xref="S4.Ex2.m2.1.1"></and><apply id="S4.Ex2.m2.1.1b.cmml" xref="S4.Ex2.m2.1.1"><ci id="S4.Ex2.m2.1.1.3.cmml" xref="S4.Ex2.m2.1.1.3">→</ci><csymbol cd="latexml" id="S4.Ex2.m2.1.1.2.cmml" xref="S4.Ex2.m2.1.1.2">absent</csymbol><ci id="S4.Ex2.m2.1.1.4a.cmml" xref="S4.Ex2.m2.1.1.4"><mtext id="S4.Ex2.m2.1.1.4.cmml" xref="S4.Ex2.m2.1.1.4">Survey 1</mtext></ci></apply><apply id="S4.Ex2.m2.1.1c.cmml" xref="S4.Ex2.m2.1.1"><ci id="S4.Ex2.m2.1.1.5.cmml" xref="S4.Ex2.m2.1.1.5">→</ci><share href="https://arxiv.org/html/2410.11860v1#S4.Ex2.m2.1.1.4.cmml" id="S4.Ex2.m2.1.1d.cmml" xref="S4.Ex2.m2.1.1"></share><csymbol cd="latexml" id="S4.Ex2.m2.1.1.6.cmml" xref="S4.Ex2.m2.1.1.6">absent</csymbol></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m2.1c">\displaystyle\rightarrow\text{Survey 1}\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S4.Ex2.m2.1d">→ Survey 1 →</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S4.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><span class="ltx_text ltx_markedasmath" id="S4.Ex3.2.1.1.1">Part 2 (12 videos, same method)</span></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\rightarrow\text{Survey 2}" class="ltx_Math" display="inline" id="S4.Ex3.m2.1"><semantics id="S4.Ex3.m2.1a"><mrow id="S4.Ex3.m2.1.1" xref="S4.Ex3.m2.1.1.cmml"><mi id="S4.Ex3.m2.1.1.2" xref="S4.Ex3.m2.1.1.2.cmml"></mi><mo id="S4.Ex3.m2.1.1.1" stretchy="false" xref="S4.Ex3.m2.1.1.1.cmml">→</mo><mtext id="S4.Ex3.m2.1.1.3" xref="S4.Ex3.m2.1.1.3a.cmml">Survey 2</mtext></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex3.m2.1b"><apply id="S4.Ex3.m2.1.1.cmml" xref="S4.Ex3.m2.1.1"><ci id="S4.Ex3.m2.1.1.1.cmml" xref="S4.Ex3.m2.1.1.1">→</ci><csymbol cd="latexml" id="S4.Ex3.m2.1.1.2.cmml" xref="S4.Ex3.m2.1.1.2">absent</csymbol><ci id="S4.Ex3.m2.1.1.3a.cmml" xref="S4.Ex3.m2.1.1.3"><mtext id="S4.Ex3.m2.1.1.3.cmml" xref="S4.Ex3.m2.1.1.3">Survey 2</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex3.m2.1c">\displaystyle\rightarrow\text{Survey 2}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex3.m2.1d">→ Survey 2</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">Part 1</span>, all participants from Groups A, B, and C each annotated 24 videos using different methods. For each annotator, the videos were assigned in random order by the annotation platform. We also reminded all participants to avoid taking breaks longer than five minutes before finishing a video, so the timing is more accurate. Depending on the method and individual pace, it took all groups on the order of two to three weeks to finish Part 1. In <span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.2">Part 2</span>, all participants annotated another 12 videos from similar scenes. But we took away the AI assistance from the two human-AI teams B and C in order to find out if their previous human-AI collaboration experiences trained them in any way so that they would perform differently on manual annotations from here on out.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">A post-task survey was administered after each part of the study. <span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Survey 0</span> was set to ”repeat until perfect”, this was to verify that the participants were clear about the task requirements before they could start the actual annotation. <span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.2">Survey 1</span> focused on getting people’s immediate feedback on their experience working with the AI they were paired with. Questions include the correctness and consistency of the AI recommendations, and if the AI made their job easier. This allows us to compare if participants’ subjective feelings match the different AI recommendations’ underlying personae (high-precision vs. high-recall). <span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.3">Survey 2</span> focused on comparing the annotators’ preference between AI-assisted and human-only methods after they had experienced both workflows on the same task.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present our study results and analysis by answering each research question presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S1" title="1. Introduction ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">1</span></a>. For statistical analysis, we ran one-way ANOVA or one-way Welch ANOVA tests, depending on the underlying assumptions being satisfied, followed by Pairwise Tukey-HSD or Games-Howell post-hoc tests, respectively. To examine interactions between factors, we conducted two-way ANOVAs followed by Pairwise Tukey-HSD or Bonferroni-corrected post-hoc tests. We adopted Type III sums of squares in ANOVA to address unbalanced data.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Research questions <span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Q1</span>, <span class="ltx_text ltx_font_bold" id="S5.p2.1.2">Q2</span>, and <span class="ltx_text ltx_font_bold" id="S5.p2.1.3">Q3</span> focus on results from Part 1 of the study (Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F5" title="Figure 5 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F6" title="Figure 6 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F8" title="Figure 8 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">8</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F10" title="Figure 10 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">10</span></a>a), in which Groups B and C collaborated with restrained and zealous AIs. Question <span class="ltx_text ltx_font_bold" id="S5.p2.1.4">Q4</span> focuses on results from Part 2 (Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F7" title="Figure 7 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">7</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F9" title="Figure 9 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">9</span></a>, and <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F10" title="Figure 10 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">10</span></a>b) to examine how the prior human-AI collaboration experience could affect the users.</p>
</div>
<figure class="ltx_figure" id="S5.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="372" id="S5.F5.g1" src="extracted/5905702/fig_part1_precision_recall_changes_overall.jpg" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.5.1.1" style="font-size:90%;">Figure 5</span>. </span><span class="ltx_text" id="S5.F5.6.2" style="font-size:90%;">Visualizing each group’s overall annotation quality on the precision-recall plot with F1 scores (Part 1). Group A manually annotates all videos and without surprise, they are the slowest (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F6" title="Figure 6 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6</span></a>) with a quality better than Autonomous AI alone but worse than the two human-AI groups’ team effort. Annotators in Groups B &amp; C had to <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.F5.6.2.1">accept</span>, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.F5.6.2.2">reject</span>, or <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.F5.6.2.3">solve</span> the face boxes pre-annotated by the restrained or zealous AIs to improve the human-AI team performance. The arrows show how much humans improved from the AIs’ initial annotation.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F5.7">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F5.8">A figure plotting different treatment groups’ annotation performance in precision and recall. The restrained AI started with high precision, and the zealous AI started with high recall.</p>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S5.4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.1.1" style="width:208.1pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_figure_panel ltx_img_square" height="499" id="S5.1.1.g1" src="extracted/5905702/fig_part1_task_time_overall.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S5.1.1.1">
<figure class="ltx_figure ltx_figure_panel" id="S5.F6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F6.3.1.1" style="font-size:90%;">Figure 6</span>. </span><span class="ltx_text" id="S5.F6.4.2" style="font-size:90%;">Average annotation time for a single video in <span class="ltx_text ltx_font_bold" id="S5.F6.4.2.1">Part 1</span>. Lower is better. Error bars represent the 95% confidence interval. Treatment Group A used a baseline manual method and the annotators in Groups B and C reviewed restrained and zealous AI recommendations in Part 1. Groups B &amp; C included the GPU time used to calculate the AI recommendations.</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S5.1.1.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.1.1.2">Part 1 task completion time of the three methods, details described in the main text.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.2.2" style="width:208.1pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_figure_panel ltx_img_square" height="499" id="S5.2.2.g1" src="extracted/5905702/fig_part2_task_time_overall.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S5.2.2.1">
<figure class="ltx_figure ltx_figure_panel" id="S5.F7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F7.3.1.1" style="font-size:90%;">Figure 7</span>. </span><span class="ltx_text" id="S5.F7.4.2" style="font-size:90%;">Average annotation time for a single video in <span class="ltx_text ltx_font_bold" id="S5.F7.4.2.1">Part 2</span>. After working 2-3 weeks on Part 1, every worker annotated another 12 videos in Part 2 but all used the same manual tool without AI recommendations. We no longer see a significant difference between Groups A &amp; C but Group B is now slower in hard videos, mainly caused by novice workers.</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S5.2.2.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.2.2.2">Part 2 task completion time of the three methods, details described in the main text.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.3.3" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="128" id="S5.3.3.g1" src="extracted/5905702/fig_part1_judgement_recalls.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S5.3.3.1">
<figure class="ltx_figure ltx_figure_panel" id="S5.F8">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F8.4.1.1" style="font-size:90%;">Figure 8</span>. </span><span class="ltx_text" id="S5.F8.5.2" style="font-size:90%;">The recall distribution of annotated videos in <span class="ltx_text ltx_font_bold" id="S5.F8.5.2.1">Part 1</span>. For the purpose of visualization clarity, we plot the 75-100% range in all recall distributions, which omits maximally 2% of outlier cases. Higher recalls and a ”shorter tail” are better. The average recall is marked with a darker diamond. The recall distribution reveals <span class="ltx_text ltx_font_bold" id="S5.F8.5.2.2">the likelihood of having a higher quality result</span>, an insight needed to analyze results from crowdworkers. E.g., in hard videos (right), annotations from ”zealous AI + Group C” have a shorter tail than other methods, as expected, the high-recall zealous AI recommendations make it easier for more people to achieve higher recalls especially when people’s attention are pushed to the limit when there are three or more faces to track across many frames simultaneously.</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S5.3.3.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.3.3.2">Part 1 recall distribution of the three methods, details described in the main text.</p>
</div>
</div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_top" id="S5.4.4" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="128" id="S5.4.4.g1" src="extracted/5905702/fig_part2_judgement_recalls.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S5.4.4.1">
<figure class="ltx_figure ltx_figure_panel" id="S5.F9">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F9.3.1.1" style="font-size:90%;">Figure 9</span>. </span><span class="ltx_text" id="S5.F9.4.2" style="font-size:90%;">The recall distribution of annotated videos in <span class="ltx_text ltx_font_bold" id="S5.F9.4.2.1">Part 2</span>. The previously human-AI collaborative Groups B &amp; C no longer have access to the AI recommendations so they used the same manual method that Group A have been using. The overall subplot (left) shows visible longer tails from these two groups, especially Group C in hard videos (right), indicating a discrepancy in individuals’ performance now without the help from AIs.</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S5.4.4.1.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S5.4.4.2">Part 2 recall distribution of the three methods, details described in the main text.</p>
</div>
</div>
</figure>
</div>
</div>
</figure>
<figure class="ltx_figure" id="S5.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F10.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="S5.F10.sf1.g1" src="extracted/5905702/fig_part1_recall_tenure.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="S5.F10.sf1.3.2" style="font-size:90%;">Part 1 (different methods between groups)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S5.F10.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="278" id="S5.F10.sf2.g1" src="extracted/5905702/fig_part2_recall_tenure.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="S5.F10.sf2.3.2" style="font-size:90%;">Part 2 (same method: human only)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F10.2.1.1" style="font-size:90%;">Figure 10</span>. </span><span class="ltx_text" id="S5.F10.3.2" style="font-size:90%;">Recall distribution of annotated videos split by user expertise. Figure (a) shows both human-AI Groups B &amp; C gained advantage over the manual method Group A mainly through veteran workers. The longer tails in Figure (b, novice) provide a new perspective to interpret Group C’s long tails in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F9" title="Figure 9 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">9</span></a> (Overall) that the performance discrepancy is mostly caused by novice workers after they lost access to AI recommendations.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S5.F10.4">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S5.F10.5">Part 1 and 2 recall distribution of annotated videos split by user expertise, details described in the main text.</p>
</div>
</div>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Q1: Can the human-AI teams achieve ”complementary team performance” in this task?</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Bansal et al.<cite class="ltx_cite ltx_citemacro_citep">(Bansal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib6" title="">2021b</a>)</cite> defines <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">complementary team performance</span> as the human-AI team performance exceeding both the human-only and AI-only performance.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.2">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F5" title="Figure 5 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5</span></a> shows the two human-AI teams B &amp; C reached comparable F1 scores of 96.9% &amp; 96.8%, respectively, significantly better than the human-only Group A that reached 94.5% (Welch <math alttext="F_{2,1151}=18.2" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.2"><semantics id="S5.SS1.p2.1.m1.2a"><mrow id="S5.SS1.p2.1.m1.2.3" xref="S5.SS1.p2.1.m1.2.3.cmml"><msub id="S5.SS1.p2.1.m1.2.3.2" xref="S5.SS1.p2.1.m1.2.3.2.cmml"><mi id="S5.SS1.p2.1.m1.2.3.2.2" xref="S5.SS1.p2.1.m1.2.3.2.2.cmml">F</mi><mrow id="S5.SS1.p2.1.m1.2.2.2.4" xref="S5.SS1.p2.1.m1.2.2.2.3.cmml"><mn id="S5.SS1.p2.1.m1.1.1.1.1" xref="S5.SS1.p2.1.m1.1.1.1.1.cmml">2</mn><mo id="S5.SS1.p2.1.m1.2.2.2.4.1" xref="S5.SS1.p2.1.m1.2.2.2.3.cmml">,</mo><mn id="S5.SS1.p2.1.m1.2.2.2.2" xref="S5.SS1.p2.1.m1.2.2.2.2.cmml">1151</mn></mrow></msub><mo id="S5.SS1.p2.1.m1.2.3.1" xref="S5.SS1.p2.1.m1.2.3.1.cmml">=</mo><mn id="S5.SS1.p2.1.m1.2.3.3" xref="S5.SS1.p2.1.m1.2.3.3.cmml">18.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.2b"><apply id="S5.SS1.p2.1.m1.2.3.cmml" xref="S5.SS1.p2.1.m1.2.3"><eq id="S5.SS1.p2.1.m1.2.3.1.cmml" xref="S5.SS1.p2.1.m1.2.3.1"></eq><apply id="S5.SS1.p2.1.m1.2.3.2.cmml" xref="S5.SS1.p2.1.m1.2.3.2"><csymbol cd="ambiguous" id="S5.SS1.p2.1.m1.2.3.2.1.cmml" xref="S5.SS1.p2.1.m1.2.3.2">subscript</csymbol><ci id="S5.SS1.p2.1.m1.2.3.2.2.cmml" xref="S5.SS1.p2.1.m1.2.3.2.2">𝐹</ci><list id="S5.SS1.p2.1.m1.2.2.2.3.cmml" xref="S5.SS1.p2.1.m1.2.2.2.4"><cn id="S5.SS1.p2.1.m1.1.1.1.1.cmml" type="integer" xref="S5.SS1.p2.1.m1.1.1.1.1">2</cn><cn id="S5.SS1.p2.1.m1.2.2.2.2.cmml" type="integer" xref="S5.SS1.p2.1.m1.2.2.2.2">1151</cn></list></apply><cn id="S5.SS1.p2.1.m1.2.3.3.cmml" type="float" xref="S5.SS1.p2.1.m1.2.3.3">18.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.2c">F_{2,1151}=18.2</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.1.m1.2d">italic_F start_POSTSUBSCRIPT 2 , 1151 end_POSTSUBSCRIPT = 18.2</annotation></semantics></math>, <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS1.p2.2.m2.1"><semantics id="S5.SS1.p2.2.m2.1a"><mrow id="S5.SS1.p2.2.m2.1.1" xref="S5.SS1.p2.2.m2.1.1.cmml"><mi id="S5.SS1.p2.2.m2.1.1.2" xref="S5.SS1.p2.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS1.p2.2.m2.1.1.1" xref="S5.SS1.p2.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p2.2.m2.1.1.3" xref="S5.SS1.p2.2.m2.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p2.2.m2.1b"><apply id="S5.SS1.p2.2.m2.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1"><lt id="S5.SS1.p2.2.m2.1.1.1.cmml" xref="S5.SS1.p2.2.m2.1.1.1"></lt><ci id="S5.SS1.p2.2.m2.1.1.2.cmml" xref="S5.SS1.p2.2.m2.1.1.2">𝑝</ci><cn id="S5.SS1.p2.2.m2.1.1.3.cmml" type="float" xref="S5.SS1.p2.2.m2.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p2.2.m2.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p2.2.m2.1d">italic_p &lt; 0.0001</annotation></semantics></math>). Both human-AI teams improved F1 accuracy and recall significantly compared to their human-only counterpart.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Because the high-stakes nature of this task rules out autonomous AI as a viable option, we really only need to compare the human-AI team performance with human-only performance in Part 1 of our study. However, to verify complementary team performance, we also verify that the two human-AI teams achieved higher performance in terms of F1 scores and recall than their respective AI’s initial standalone performance.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.4">Comparing each human-AI team with their perspective AI teammates’ initial performance – Group B annotators improved the restrained AI from 93.4% to 96.9% (Welch <math alttext="F_{1,1228}=178" class="ltx_Math" display="inline" id="S5.SS1.p4.1.m1.2"><semantics id="S5.SS1.p4.1.m1.2a"><mrow id="S5.SS1.p4.1.m1.2.3" xref="S5.SS1.p4.1.m1.2.3.cmml"><msub id="S5.SS1.p4.1.m1.2.3.2" xref="S5.SS1.p4.1.m1.2.3.2.cmml"><mi id="S5.SS1.p4.1.m1.2.3.2.2" xref="S5.SS1.p4.1.m1.2.3.2.2.cmml">F</mi><mrow id="S5.SS1.p4.1.m1.2.2.2.4" xref="S5.SS1.p4.1.m1.2.2.2.3.cmml"><mn id="S5.SS1.p4.1.m1.1.1.1.1" xref="S5.SS1.p4.1.m1.1.1.1.1.cmml">1</mn><mo id="S5.SS1.p4.1.m1.2.2.2.4.1" xref="S5.SS1.p4.1.m1.2.2.2.3.cmml">,</mo><mn id="S5.SS1.p4.1.m1.2.2.2.2" xref="S5.SS1.p4.1.m1.2.2.2.2.cmml">1228</mn></mrow></msub><mo id="S5.SS1.p4.1.m1.2.3.1" xref="S5.SS1.p4.1.m1.2.3.1.cmml">=</mo><mn id="S5.SS1.p4.1.m1.2.3.3" xref="S5.SS1.p4.1.m1.2.3.3.cmml">178</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.2b"><apply id="S5.SS1.p4.1.m1.2.3.cmml" xref="S5.SS1.p4.1.m1.2.3"><eq id="S5.SS1.p4.1.m1.2.3.1.cmml" xref="S5.SS1.p4.1.m1.2.3.1"></eq><apply id="S5.SS1.p4.1.m1.2.3.2.cmml" xref="S5.SS1.p4.1.m1.2.3.2"><csymbol cd="ambiguous" id="S5.SS1.p4.1.m1.2.3.2.1.cmml" xref="S5.SS1.p4.1.m1.2.3.2">subscript</csymbol><ci id="S5.SS1.p4.1.m1.2.3.2.2.cmml" xref="S5.SS1.p4.1.m1.2.3.2.2">𝐹</ci><list id="S5.SS1.p4.1.m1.2.2.2.3.cmml" xref="S5.SS1.p4.1.m1.2.2.2.4"><cn id="S5.SS1.p4.1.m1.1.1.1.1.cmml" type="integer" xref="S5.SS1.p4.1.m1.1.1.1.1">1</cn><cn id="S5.SS1.p4.1.m1.2.2.2.2.cmml" type="integer" xref="S5.SS1.p4.1.m1.2.2.2.2">1228</cn></list></apply><cn id="S5.SS1.p4.1.m1.2.3.3.cmml" type="integer" xref="S5.SS1.p4.1.m1.2.3.3">178</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.2c">F_{1,1228}=178</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.1.m1.2d">italic_F start_POSTSUBSCRIPT 1 , 1228 end_POSTSUBSCRIPT = 178</annotation></semantics></math>, <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS1.p4.2.m2.1"><semantics id="S5.SS1.p4.2.m2.1a"><mrow id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml"><mi id="S5.SS1.p4.2.m2.1.1.2" xref="S5.SS1.p4.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS1.p4.2.m2.1.1.1" xref="S5.SS1.p4.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p4.2.m2.1.1.3" xref="S5.SS1.p4.2.m2.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><apply id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1"><lt id="S5.SS1.p4.2.m2.1.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1.1"></lt><ci id="S5.SS1.p4.2.m2.1.1.2.cmml" xref="S5.SS1.p4.2.m2.1.1.2">𝑝</ci><cn id="S5.SS1.p4.2.m2.1.1.3.cmml" type="float" xref="S5.SS1.p4.2.m2.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.2.m2.1d">italic_p &lt; 0.0001</annotation></semantics></math>), Group C annotators improved the zealous AI from 90.9% to 96.8% (Welch <math alttext="F_{1,837}=169" class="ltx_Math" display="inline" id="S5.SS1.p4.3.m3.2"><semantics id="S5.SS1.p4.3.m3.2a"><mrow id="S5.SS1.p4.3.m3.2.3" xref="S5.SS1.p4.3.m3.2.3.cmml"><msub id="S5.SS1.p4.3.m3.2.3.2" xref="S5.SS1.p4.3.m3.2.3.2.cmml"><mi id="S5.SS1.p4.3.m3.2.3.2.2" xref="S5.SS1.p4.3.m3.2.3.2.2.cmml">F</mi><mrow id="S5.SS1.p4.3.m3.2.2.2.4" xref="S5.SS1.p4.3.m3.2.2.2.3.cmml"><mn id="S5.SS1.p4.3.m3.1.1.1.1" xref="S5.SS1.p4.3.m3.1.1.1.1.cmml">1</mn><mo id="S5.SS1.p4.3.m3.2.2.2.4.1" xref="S5.SS1.p4.3.m3.2.2.2.3.cmml">,</mo><mn id="S5.SS1.p4.3.m3.2.2.2.2" xref="S5.SS1.p4.3.m3.2.2.2.2.cmml">837</mn></mrow></msub><mo id="S5.SS1.p4.3.m3.2.3.1" xref="S5.SS1.p4.3.m3.2.3.1.cmml">=</mo><mn id="S5.SS1.p4.3.m3.2.3.3" xref="S5.SS1.p4.3.m3.2.3.3.cmml">169</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.2b"><apply id="S5.SS1.p4.3.m3.2.3.cmml" xref="S5.SS1.p4.3.m3.2.3"><eq id="S5.SS1.p4.3.m3.2.3.1.cmml" xref="S5.SS1.p4.3.m3.2.3.1"></eq><apply id="S5.SS1.p4.3.m3.2.3.2.cmml" xref="S5.SS1.p4.3.m3.2.3.2"><csymbol cd="ambiguous" id="S5.SS1.p4.3.m3.2.3.2.1.cmml" xref="S5.SS1.p4.3.m3.2.3.2">subscript</csymbol><ci id="S5.SS1.p4.3.m3.2.3.2.2.cmml" xref="S5.SS1.p4.3.m3.2.3.2.2">𝐹</ci><list id="S5.SS1.p4.3.m3.2.2.2.3.cmml" xref="S5.SS1.p4.3.m3.2.2.2.4"><cn id="S5.SS1.p4.3.m3.1.1.1.1.cmml" type="integer" xref="S5.SS1.p4.3.m3.1.1.1.1">1</cn><cn id="S5.SS1.p4.3.m3.2.2.2.2.cmml" type="integer" xref="S5.SS1.p4.3.m3.2.2.2.2">837</cn></list></apply><cn id="S5.SS1.p4.3.m3.2.3.3.cmml" type="integer" xref="S5.SS1.p4.3.m3.2.3.3">169</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.2c">F_{1,837}=169</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.3.m3.2d">italic_F start_POSTSUBSCRIPT 1 , 837 end_POSTSUBSCRIPT = 169</annotation></semantics></math>, <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS1.p4.4.m4.1"><semantics id="S5.SS1.p4.4.m4.1a"><mrow id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml"><mi id="S5.SS1.p4.4.m4.1.1.2" xref="S5.SS1.p4.4.m4.1.1.2.cmml">p</mi><mo id="S5.SS1.p4.4.m4.1.1.1" xref="S5.SS1.p4.4.m4.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p4.4.m4.1.1.3" xref="S5.SS1.p4.4.m4.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.1b"><apply id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1"><lt id="S5.SS1.p4.4.m4.1.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1.1"></lt><ci id="S5.SS1.p4.4.m4.1.1.2.cmml" xref="S5.SS1.p4.4.m4.1.1.2">𝑝</ci><cn id="S5.SS1.p4.4.m4.1.1.3.cmml" type="float" xref="S5.SS1.p4.4.m4.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p4.4.m4.1d">italic_p &lt; 0.0001</annotation></semantics></math>). Both human-AI teams improved significantly from their respective AI teammate’s solo performance.</p>
</div>
<div class="ltx_para" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1">It is understandable that Bansal et al. only considered accuracy and did not compare task completion time in complementary performance, since the human-AI teamwork will undoubtedly add more time than AI alone. As we discussed, task completion times directly affect the operation cost as people are paid at an hourly rate, making it a critical metric for annotation tasks, so we additionally compare the human-AI teams’ task completion times with the human-only team.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.4">We saw overall significant differences between all three groups on task completion time (Welch <math alttext="F_{2,1039}=48.6" class="ltx_Math" display="inline" id="S5.SS1.p6.1.m1.2"><semantics id="S5.SS1.p6.1.m1.2a"><mrow id="S5.SS1.p6.1.m1.2.3" xref="S5.SS1.p6.1.m1.2.3.cmml"><msub id="S5.SS1.p6.1.m1.2.3.2" xref="S5.SS1.p6.1.m1.2.3.2.cmml"><mi id="S5.SS1.p6.1.m1.2.3.2.2" xref="S5.SS1.p6.1.m1.2.3.2.2.cmml">F</mi><mrow id="S5.SS1.p6.1.m1.2.2.2.4" xref="S5.SS1.p6.1.m1.2.2.2.3.cmml"><mn id="S5.SS1.p6.1.m1.1.1.1.1" xref="S5.SS1.p6.1.m1.1.1.1.1.cmml">2</mn><mo id="S5.SS1.p6.1.m1.2.2.2.4.1" xref="S5.SS1.p6.1.m1.2.2.2.3.cmml">,</mo><mn id="S5.SS1.p6.1.m1.2.2.2.2" xref="S5.SS1.p6.1.m1.2.2.2.2.cmml">1039</mn></mrow></msub><mo id="S5.SS1.p6.1.m1.2.3.1" xref="S5.SS1.p6.1.m1.2.3.1.cmml">=</mo><mn id="S5.SS1.p6.1.m1.2.3.3" xref="S5.SS1.p6.1.m1.2.3.3.cmml">48.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.2b"><apply id="S5.SS1.p6.1.m1.2.3.cmml" xref="S5.SS1.p6.1.m1.2.3"><eq id="S5.SS1.p6.1.m1.2.3.1.cmml" xref="S5.SS1.p6.1.m1.2.3.1"></eq><apply id="S5.SS1.p6.1.m1.2.3.2.cmml" xref="S5.SS1.p6.1.m1.2.3.2"><csymbol cd="ambiguous" id="S5.SS1.p6.1.m1.2.3.2.1.cmml" xref="S5.SS1.p6.1.m1.2.3.2">subscript</csymbol><ci id="S5.SS1.p6.1.m1.2.3.2.2.cmml" xref="S5.SS1.p6.1.m1.2.3.2.2">𝐹</ci><list id="S5.SS1.p6.1.m1.2.2.2.3.cmml" xref="S5.SS1.p6.1.m1.2.2.2.4"><cn id="S5.SS1.p6.1.m1.1.1.1.1.cmml" type="integer" xref="S5.SS1.p6.1.m1.1.1.1.1">2</cn><cn id="S5.SS1.p6.1.m1.2.2.2.2.cmml" type="integer" xref="S5.SS1.p6.1.m1.2.2.2.2">1039</cn></list></apply><cn id="S5.SS1.p6.1.m1.2.3.3.cmml" type="float" xref="S5.SS1.p6.1.m1.2.3.3">48.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.2c">F_{2,1039}=48.6</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.1.m1.2d">italic_F start_POSTSUBSCRIPT 2 , 1039 end_POSTSUBSCRIPT = 48.6</annotation></semantics></math>, <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS1.p6.2.m2.1"><semantics id="S5.SS1.p6.2.m2.1a"><mrow id="S5.SS1.p6.2.m2.1.1" xref="S5.SS1.p6.2.m2.1.1.cmml"><mi id="S5.SS1.p6.2.m2.1.1.2" xref="S5.SS1.p6.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS1.p6.2.m2.1.1.1" xref="S5.SS1.p6.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p6.2.m2.1.1.3" xref="S5.SS1.p6.2.m2.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.2.m2.1b"><apply id="S5.SS1.p6.2.m2.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1"><lt id="S5.SS1.p6.2.m2.1.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1.1"></lt><ci id="S5.SS1.p6.2.m2.1.1.2.cmml" xref="S5.SS1.p6.2.m2.1.1.2">𝑝</ci><cn id="S5.SS1.p6.2.m2.1.1.3.cmml" type="float" xref="S5.SS1.p6.2.m2.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.2.m2.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.2.m2.1d">italic_p &lt; 0.0001</annotation></semantics></math>), as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F6" title="Figure 6 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6</span></a>, left. As a baseline, on average it took 1.05 hours for Group A to manually annotate a 30-second video of 900 frames. Group B took a significantly shorter time of 0.91 hours (Games-Howell <math alttext="p&lt;0.001" class="ltx_Math" display="inline" id="S5.SS1.p6.3.m3.1"><semantics id="S5.SS1.p6.3.m3.1a"><mrow id="S5.SS1.p6.3.m3.1.1" xref="S5.SS1.p6.3.m3.1.1.cmml"><mi id="S5.SS1.p6.3.m3.1.1.2" xref="S5.SS1.p6.3.m3.1.1.2.cmml">p</mi><mo id="S5.SS1.p6.3.m3.1.1.1" xref="S5.SS1.p6.3.m3.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p6.3.m3.1.1.3" xref="S5.SS1.p6.3.m3.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.3.m3.1b"><apply id="S5.SS1.p6.3.m3.1.1.cmml" xref="S5.SS1.p6.3.m3.1.1"><lt id="S5.SS1.p6.3.m3.1.1.1.cmml" xref="S5.SS1.p6.3.m3.1.1.1"></lt><ci id="S5.SS1.p6.3.m3.1.1.2.cmml" xref="S5.SS1.p6.3.m3.1.1.2">𝑝</ci><cn id="S5.SS1.p6.3.m3.1.1.3.cmml" type="float" xref="S5.SS1.p6.3.m3.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.3.m3.1c">p&lt;0.001</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.3.m3.1d">italic_p &lt; 0.001</annotation></semantics></math>) to review the restrained AI recommendations. Group C only used 0.73 hours to review zealous AI’s recommendations, also significantly shorter than the human-only Group A (Games-Howell <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS1.p6.4.m4.1"><semantics id="S5.SS1.p6.4.m4.1a"><mrow id="S5.SS1.p6.4.m4.1.1" xref="S5.SS1.p6.4.m4.1.1.cmml"><mi id="S5.SS1.p6.4.m4.1.1.2" xref="S5.SS1.p6.4.m4.1.1.2.cmml">p</mi><mo id="S5.SS1.p6.4.m4.1.1.1" xref="S5.SS1.p6.4.m4.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p6.4.m4.1.1.3" xref="S5.SS1.p6.4.m4.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.4.m4.1b"><apply id="S5.SS1.p6.4.m4.1.1.cmml" xref="S5.SS1.p6.4.m4.1.1"><lt id="S5.SS1.p6.4.m4.1.1.1.cmml" xref="S5.SS1.p6.4.m4.1.1.1"></lt><ci id="S5.SS1.p6.4.m4.1.1.2.cmml" xref="S5.SS1.p6.4.m4.1.1.2">𝑝</ci><cn id="S5.SS1.p6.4.m4.1.1.3.cmml" type="float" xref="S5.SS1.p6.4.m4.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.4.m4.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p6.4.m4.1d">italic_p &lt; 0.0001</annotation></semantics></math>).</p>
</div>
<div class="ltx_para" id="S5.SS1.p7">
<p class="ltx_p" id="S5.SS1.p7.4">It is also worth noting that Group C, the zealous human-AI team, had an overall significantly worse starting point than Group B in terms of F1 score: 90.9% vs. 93.4% (Welch <math alttext="F_{1,854}=35.32" class="ltx_Math" display="inline" id="S5.SS1.p7.1.m1.2"><semantics id="S5.SS1.p7.1.m1.2a"><mrow id="S5.SS1.p7.1.m1.2.3" xref="S5.SS1.p7.1.m1.2.3.cmml"><msub id="S5.SS1.p7.1.m1.2.3.2" xref="S5.SS1.p7.1.m1.2.3.2.cmml"><mi id="S5.SS1.p7.1.m1.2.3.2.2" xref="S5.SS1.p7.1.m1.2.3.2.2.cmml">F</mi><mrow id="S5.SS1.p7.1.m1.2.2.2.4" xref="S5.SS1.p7.1.m1.2.2.2.3.cmml"><mn id="S5.SS1.p7.1.m1.1.1.1.1" xref="S5.SS1.p7.1.m1.1.1.1.1.cmml">1</mn><mo id="S5.SS1.p7.1.m1.2.2.2.4.1" xref="S5.SS1.p7.1.m1.2.2.2.3.cmml">,</mo><mn id="S5.SS1.p7.1.m1.2.2.2.2" xref="S5.SS1.p7.1.m1.2.2.2.2.cmml">854</mn></mrow></msub><mo id="S5.SS1.p7.1.m1.2.3.1" xref="S5.SS1.p7.1.m1.2.3.1.cmml">=</mo><mn id="S5.SS1.p7.1.m1.2.3.3" xref="S5.SS1.p7.1.m1.2.3.3.cmml">35.32</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.1.m1.2b"><apply id="S5.SS1.p7.1.m1.2.3.cmml" xref="S5.SS1.p7.1.m1.2.3"><eq id="S5.SS1.p7.1.m1.2.3.1.cmml" xref="S5.SS1.p7.1.m1.2.3.1"></eq><apply id="S5.SS1.p7.1.m1.2.3.2.cmml" xref="S5.SS1.p7.1.m1.2.3.2"><csymbol cd="ambiguous" id="S5.SS1.p7.1.m1.2.3.2.1.cmml" xref="S5.SS1.p7.1.m1.2.3.2">subscript</csymbol><ci id="S5.SS1.p7.1.m1.2.3.2.2.cmml" xref="S5.SS1.p7.1.m1.2.3.2.2">𝐹</ci><list id="S5.SS1.p7.1.m1.2.2.2.3.cmml" xref="S5.SS1.p7.1.m1.2.2.2.4"><cn id="S5.SS1.p7.1.m1.1.1.1.1.cmml" type="integer" xref="S5.SS1.p7.1.m1.1.1.1.1">1</cn><cn id="S5.SS1.p7.1.m1.2.2.2.2.cmml" type="integer" xref="S5.SS1.p7.1.m1.2.2.2.2">854</cn></list></apply><cn id="S5.SS1.p7.1.m1.2.3.3.cmml" type="float" xref="S5.SS1.p7.1.m1.2.3.3">35.32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.1.m1.2c">F_{1,854}=35.32</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p7.1.m1.2d">italic_F start_POSTSUBSCRIPT 1 , 854 end_POSTSUBSCRIPT = 35.32</annotation></semantics></math>, <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS1.p7.2.m2.1"><semantics id="S5.SS1.p7.2.m2.1a"><mrow id="S5.SS1.p7.2.m2.1.1" xref="S5.SS1.p7.2.m2.1.1.cmml"><mi id="S5.SS1.p7.2.m2.1.1.2" xref="S5.SS1.p7.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS1.p7.2.m2.1.1.1" xref="S5.SS1.p7.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p7.2.m2.1.1.3" xref="S5.SS1.p7.2.m2.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.2.m2.1b"><apply id="S5.SS1.p7.2.m2.1.1.cmml" xref="S5.SS1.p7.2.m2.1.1"><lt id="S5.SS1.p7.2.m2.1.1.1.cmml" xref="S5.SS1.p7.2.m2.1.1.1"></lt><ci id="S5.SS1.p7.2.m2.1.1.2.cmml" xref="S5.SS1.p7.2.m2.1.1.2">𝑝</ci><cn id="S5.SS1.p7.2.m2.1.1.3.cmml" type="float" xref="S5.SS1.p7.2.m2.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.2.m2.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p7.2.m2.1d">italic_p &lt; 0.0001</annotation></semantics></math>) as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F5" title="Figure 5 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5</span></a>. However, annotators working with the zealous AI managed to achieve a significantly higher improvement in F1 score of +5.9% vs. +3.5% (Welch <math alttext="F_{1,934}=45.02" class="ltx_Math" display="inline" id="S5.SS1.p7.3.m3.2"><semantics id="S5.SS1.p7.3.m3.2a"><mrow id="S5.SS1.p7.3.m3.2.3" xref="S5.SS1.p7.3.m3.2.3.cmml"><msub id="S5.SS1.p7.3.m3.2.3.2" xref="S5.SS1.p7.3.m3.2.3.2.cmml"><mi id="S5.SS1.p7.3.m3.2.3.2.2" xref="S5.SS1.p7.3.m3.2.3.2.2.cmml">F</mi><mrow id="S5.SS1.p7.3.m3.2.2.2.4" xref="S5.SS1.p7.3.m3.2.2.2.3.cmml"><mn id="S5.SS1.p7.3.m3.1.1.1.1" xref="S5.SS1.p7.3.m3.1.1.1.1.cmml">1</mn><mo id="S5.SS1.p7.3.m3.2.2.2.4.1" xref="S5.SS1.p7.3.m3.2.2.2.3.cmml">,</mo><mn id="S5.SS1.p7.3.m3.2.2.2.2" xref="S5.SS1.p7.3.m3.2.2.2.2.cmml">934</mn></mrow></msub><mo id="S5.SS1.p7.3.m3.2.3.1" xref="S5.SS1.p7.3.m3.2.3.1.cmml">=</mo><mn id="S5.SS1.p7.3.m3.2.3.3" xref="S5.SS1.p7.3.m3.2.3.3.cmml">45.02</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.3.m3.2b"><apply id="S5.SS1.p7.3.m3.2.3.cmml" xref="S5.SS1.p7.3.m3.2.3"><eq id="S5.SS1.p7.3.m3.2.3.1.cmml" xref="S5.SS1.p7.3.m3.2.3.1"></eq><apply id="S5.SS1.p7.3.m3.2.3.2.cmml" xref="S5.SS1.p7.3.m3.2.3.2"><csymbol cd="ambiguous" id="S5.SS1.p7.3.m3.2.3.2.1.cmml" xref="S5.SS1.p7.3.m3.2.3.2">subscript</csymbol><ci id="S5.SS1.p7.3.m3.2.3.2.2.cmml" xref="S5.SS1.p7.3.m3.2.3.2.2">𝐹</ci><list id="S5.SS1.p7.3.m3.2.2.2.3.cmml" xref="S5.SS1.p7.3.m3.2.2.2.4"><cn id="S5.SS1.p7.3.m3.1.1.1.1.cmml" type="integer" xref="S5.SS1.p7.3.m3.1.1.1.1">1</cn><cn id="S5.SS1.p7.3.m3.2.2.2.2.cmml" type="integer" xref="S5.SS1.p7.3.m3.2.2.2.2">934</cn></list></apply><cn id="S5.SS1.p7.3.m3.2.3.3.cmml" type="float" xref="S5.SS1.p7.3.m3.2.3.3">45.02</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.3.m3.2c">F_{1,934}=45.02</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p7.3.m3.2d">italic_F start_POSTSUBSCRIPT 1 , 934 end_POSTSUBSCRIPT = 45.02</annotation></semantics></math>, <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS1.p7.4.m4.1"><semantics id="S5.SS1.p7.4.m4.1a"><mrow id="S5.SS1.p7.4.m4.1.1" xref="S5.SS1.p7.4.m4.1.1.cmml"><mi id="S5.SS1.p7.4.m4.1.1.2" xref="S5.SS1.p7.4.m4.1.1.2.cmml">p</mi><mo id="S5.SS1.p7.4.m4.1.1.1" xref="S5.SS1.p7.4.m4.1.1.1.cmml">&lt;</mo><mn id="S5.SS1.p7.4.m4.1.1.3" xref="S5.SS1.p7.4.m4.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p7.4.m4.1b"><apply id="S5.SS1.p7.4.m4.1.1.cmml" xref="S5.SS1.p7.4.m4.1.1"><lt id="S5.SS1.p7.4.m4.1.1.1.cmml" xref="S5.SS1.p7.4.m4.1.1.1"></lt><ci id="S5.SS1.p7.4.m4.1.1.2.cmml" xref="S5.SS1.p7.4.m4.1.1.2">𝑝</ci><cn id="S5.SS1.p7.4.m4.1.1.3.cmml" type="float" xref="S5.SS1.p7.4.m4.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p7.4.m4.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p7.4.m4.1d">italic_p &lt; 0.0001</annotation></semantics></math>) in significantly less time! This disadvantage for Group C provided the opportunity to demonstrate that our deduction in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.SS3" title="3.3. The false-positive-robust (FPR) tracker ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">3.3</span></a> was correct – a human-AI team can do better in both time and quality (in terms of F1 improvement) by asking the human to <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p7.4.1">reject</span> more false positives and only <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS1.p7.4.2">solve</span> the most challenging faces, i.e., the high-recall zealous AI.</p>
</div>
<div class="ltx_para" id="S5.SS1.p8">
<p class="ltx_p" id="S5.SS1.p8.1">In summary, we have not only verified complementary team performance on accuracy, but also showed human-AI teams could achieve significantly shorter task completions time in a real-world case study.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Q2: Which AI helps annotators be more efficient, i.e. save time?</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We mentioned that the professional <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">annotators are paid at their fixed hourly rate in this task</span>, which means 1) they are not necessarily motivated to work faster, and 2) from the business perspective, their task completion time directly impacts operation costs. We discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.SS1" title="5.1. Q1: Can the human-AI teams achieve ”complementary team performance” in this task? ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5.1</span></a> that overall, both human-AI teams have significantly shortened task completion time compared to the baseline Group A (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F6" title="Figure 6 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6</span></a> left). Specifically, the zealous AI recommendations help annotators use 20% less time than the restrained AI recommendations with statistical significance (0.73 hours vs. 0.91 hours, Games-Howell <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.p1.1.m1.1.1.3" xref="S5.SS2.p1.1.m1.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><lt id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1"></lt><ci id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">𝑝</ci><cn id="S5.SS2.p1.1.m1.1.1.3.cmml" type="float" xref="S5.SS2.p1.1.m1.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">italic_p &lt; 0.0001</annotation></semantics></math>).</p>
</div>
<section class="ltx_subsubsection" id="S5.SS2.SSSx1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Video difficulty.</h4>
<div class="ltx_para" id="S5.SS2.SSSx1.p1">
<p class="ltx_p" id="S5.SS2.SSSx1.p1.7">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F6" title="Figure 6 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6</span></a> (middle) plots task time by video difficulty and saw a significant interaction between group and video difficulty on task completion time (ANOVA <math alttext="F_{4,1577}=5.37" class="ltx_Math" display="inline" id="S5.SS2.SSSx1.p1.1.m1.2"><semantics id="S5.SS2.SSSx1.p1.1.m1.2a"><mrow id="S5.SS2.SSSx1.p1.1.m1.2.3" xref="S5.SS2.SSSx1.p1.1.m1.2.3.cmml"><msub id="S5.SS2.SSSx1.p1.1.m1.2.3.2" xref="S5.SS2.SSSx1.p1.1.m1.2.3.2.cmml"><mi id="S5.SS2.SSSx1.p1.1.m1.2.3.2.2" xref="S5.SS2.SSSx1.p1.1.m1.2.3.2.2.cmml">F</mi><mrow id="S5.SS2.SSSx1.p1.1.m1.2.2.2.4" xref="S5.SS2.SSSx1.p1.1.m1.2.2.2.3.cmml"><mn id="S5.SS2.SSSx1.p1.1.m1.1.1.1.1" xref="S5.SS2.SSSx1.p1.1.m1.1.1.1.1.cmml">4</mn><mo id="S5.SS2.SSSx1.p1.1.m1.2.2.2.4.1" xref="S5.SS2.SSSx1.p1.1.m1.2.2.2.3.cmml">,</mo><mn id="S5.SS2.SSSx1.p1.1.m1.2.2.2.2" xref="S5.SS2.SSSx1.p1.1.m1.2.2.2.2.cmml">1577</mn></mrow></msub><mo id="S5.SS2.SSSx1.p1.1.m1.2.3.1" xref="S5.SS2.SSSx1.p1.1.m1.2.3.1.cmml">=</mo><mn id="S5.SS2.SSSx1.p1.1.m1.2.3.3" xref="S5.SS2.SSSx1.p1.1.m1.2.3.3.cmml">5.37</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx1.p1.1.m1.2b"><apply id="S5.SS2.SSSx1.p1.1.m1.2.3.cmml" xref="S5.SS2.SSSx1.p1.1.m1.2.3"><eq id="S5.SS2.SSSx1.p1.1.m1.2.3.1.cmml" xref="S5.SS2.SSSx1.p1.1.m1.2.3.1"></eq><apply id="S5.SS2.SSSx1.p1.1.m1.2.3.2.cmml" xref="S5.SS2.SSSx1.p1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S5.SS2.SSSx1.p1.1.m1.2.3.2.1.cmml" xref="S5.SS2.SSSx1.p1.1.m1.2.3.2">subscript</csymbol><ci id="S5.SS2.SSSx1.p1.1.m1.2.3.2.2.cmml" xref="S5.SS2.SSSx1.p1.1.m1.2.3.2.2">𝐹</ci><list id="S5.SS2.SSSx1.p1.1.m1.2.2.2.3.cmml" xref="S5.SS2.SSSx1.p1.1.m1.2.2.2.4"><cn id="S5.SS2.SSSx1.p1.1.m1.1.1.1.1.cmml" type="integer" xref="S5.SS2.SSSx1.p1.1.m1.1.1.1.1">4</cn><cn id="S5.SS2.SSSx1.p1.1.m1.2.2.2.2.cmml" type="integer" xref="S5.SS2.SSSx1.p1.1.m1.2.2.2.2">1577</cn></list></apply><cn id="S5.SS2.SSSx1.p1.1.m1.2.3.3.cmml" type="float" xref="S5.SS2.SSSx1.p1.1.m1.2.3.3">5.37</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx1.p1.1.m1.2c">F_{4,1577}=5.37</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx1.p1.1.m1.2d">italic_F start_POSTSUBSCRIPT 4 , 1577 end_POSTSUBSCRIPT = 5.37</annotation></semantics></math>, <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx1.p1.2.m2.1"><semantics id="S5.SS2.SSSx1.p1.2.m2.1a"><mrow id="S5.SS2.SSSx1.p1.2.m2.1.1" xref="S5.SS2.SSSx1.p1.2.m2.1.1.cmml"><mi id="S5.SS2.SSSx1.p1.2.m2.1.1.2" xref="S5.SS2.SSSx1.p1.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS2.SSSx1.p1.2.m2.1.1.1" xref="S5.SS2.SSSx1.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx1.p1.2.m2.1.1.3" xref="S5.SS2.SSSx1.p1.2.m2.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx1.p1.2.m2.1b"><apply id="S5.SS2.SSSx1.p1.2.m2.1.1.cmml" xref="S5.SS2.SSSx1.p1.2.m2.1.1"><lt id="S5.SS2.SSSx1.p1.2.m2.1.1.1.cmml" xref="S5.SS2.SSSx1.p1.2.m2.1.1.1"></lt><ci id="S5.SS2.SSSx1.p1.2.m2.1.1.2.cmml" xref="S5.SS2.SSSx1.p1.2.m2.1.1.2">𝑝</ci><cn id="S5.SS2.SSSx1.p1.2.m2.1.1.3.cmml" type="float" xref="S5.SS2.SSSx1.p1.2.m2.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx1.p1.2.m2.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx1.p1.2.m2.1d">italic_p &lt; 0.0001</annotation></semantics></math>, <math alttext="\eta_{p}^{2}=0.016" class="ltx_Math" display="inline" id="S5.SS2.SSSx1.p1.3.m3.1"><semantics id="S5.SS2.SSSx1.p1.3.m3.1a"><mrow id="S5.SS2.SSSx1.p1.3.m3.1.1" xref="S5.SS2.SSSx1.p1.3.m3.1.1.cmml"><msubsup id="S5.SS2.SSSx1.p1.3.m3.1.1.2" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2.cmml"><mi id="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.2" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.2.cmml">η</mi><mi id="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.3" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.3.cmml">p</mi><mn id="S5.SS2.SSSx1.p1.3.m3.1.1.2.3" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2.3.cmml">2</mn></msubsup><mo id="S5.SS2.SSSx1.p1.3.m3.1.1.1" xref="S5.SS2.SSSx1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S5.SS2.SSSx1.p1.3.m3.1.1.3" xref="S5.SS2.SSSx1.p1.3.m3.1.1.3.cmml">0.016</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx1.p1.3.m3.1b"><apply id="S5.SS2.SSSx1.p1.3.m3.1.1.cmml" xref="S5.SS2.SSSx1.p1.3.m3.1.1"><eq id="S5.SS2.SSSx1.p1.3.m3.1.1.1.cmml" xref="S5.SS2.SSSx1.p1.3.m3.1.1.1"></eq><apply id="S5.SS2.SSSx1.p1.3.m3.1.1.2.cmml" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.SSSx1.p1.3.m3.1.1.2.1.cmml" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2">superscript</csymbol><apply id="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.cmml" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.1.cmml" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.2.cmml" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.2">𝜂</ci><ci id="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.3.cmml" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2.2.3">𝑝</ci></apply><cn id="S5.SS2.SSSx1.p1.3.m3.1.1.2.3.cmml" type="integer" xref="S5.SS2.SSSx1.p1.3.m3.1.1.2.3">2</cn></apply><cn id="S5.SS2.SSSx1.p1.3.m3.1.1.3.cmml" type="float" xref="S5.SS2.SSSx1.p1.3.m3.1.1.3">0.016</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx1.p1.3.m3.1c">\eta_{p}^{2}=0.016</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx1.p1.3.m3.1d">italic_η start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT = 0.016</annotation></semantics></math>, small). Specifically, Group C which reviewed zealous AI recommendations used significantly less time than both Group A and B in medium videos (Bonferroni <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx1.p1.4.m4.1"><semantics id="S5.SS2.SSSx1.p1.4.m4.1a"><mrow id="S5.SS2.SSSx1.p1.4.m4.1.1" xref="S5.SS2.SSSx1.p1.4.m4.1.1.cmml"><mi id="S5.SS2.SSSx1.p1.4.m4.1.1.2" xref="S5.SS2.SSSx1.p1.4.m4.1.1.2.cmml">p</mi><mo id="S5.SS2.SSSx1.p1.4.m4.1.1.1" xref="S5.SS2.SSSx1.p1.4.m4.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx1.p1.4.m4.1.1.3" xref="S5.SS2.SSSx1.p1.4.m4.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx1.p1.4.m4.1b"><apply id="S5.SS2.SSSx1.p1.4.m4.1.1.cmml" xref="S5.SS2.SSSx1.p1.4.m4.1.1"><lt id="S5.SS2.SSSx1.p1.4.m4.1.1.1.cmml" xref="S5.SS2.SSSx1.p1.4.m4.1.1.1"></lt><ci id="S5.SS2.SSSx1.p1.4.m4.1.1.2.cmml" xref="S5.SS2.SSSx1.p1.4.m4.1.1.2">𝑝</ci><cn id="S5.SS2.SSSx1.p1.4.m4.1.1.3.cmml" type="float" xref="S5.SS2.SSSx1.p1.4.m4.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx1.p1.4.m4.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx1.p1.4.m4.1d">italic_p &lt; 0.0001</annotation></semantics></math> &amp; <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx1.p1.5.m5.1"><semantics id="S5.SS2.SSSx1.p1.5.m5.1a"><mrow id="S5.SS2.SSSx1.p1.5.m5.1.1" xref="S5.SS2.SSSx1.p1.5.m5.1.1.cmml"><mi id="S5.SS2.SSSx1.p1.5.m5.1.1.2" xref="S5.SS2.SSSx1.p1.5.m5.1.1.2.cmml">p</mi><mo id="S5.SS2.SSSx1.p1.5.m5.1.1.1" xref="S5.SS2.SSSx1.p1.5.m5.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx1.p1.5.m5.1.1.3" xref="S5.SS2.SSSx1.p1.5.m5.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx1.p1.5.m5.1b"><apply id="S5.SS2.SSSx1.p1.5.m5.1.1.cmml" xref="S5.SS2.SSSx1.p1.5.m5.1.1"><lt id="S5.SS2.SSSx1.p1.5.m5.1.1.1.cmml" xref="S5.SS2.SSSx1.p1.5.m5.1.1.1"></lt><ci id="S5.SS2.SSSx1.p1.5.m5.1.1.2.cmml" xref="S5.SS2.SSSx1.p1.5.m5.1.1.2">𝑝</ci><cn id="S5.SS2.SSSx1.p1.5.m5.1.1.3.cmml" type="float" xref="S5.SS2.SSSx1.p1.5.m5.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx1.p1.5.m5.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx1.p1.5.m5.1d">italic_p &lt; 0.0001</annotation></semantics></math>), as well as in hard videos (Bonferroni <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx1.p1.6.m6.1"><semantics id="S5.SS2.SSSx1.p1.6.m6.1a"><mrow id="S5.SS2.SSSx1.p1.6.m6.1.1" xref="S5.SS2.SSSx1.p1.6.m6.1.1.cmml"><mi id="S5.SS2.SSSx1.p1.6.m6.1.1.2" xref="S5.SS2.SSSx1.p1.6.m6.1.1.2.cmml">p</mi><mo id="S5.SS2.SSSx1.p1.6.m6.1.1.1" xref="S5.SS2.SSSx1.p1.6.m6.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx1.p1.6.m6.1.1.3" xref="S5.SS2.SSSx1.p1.6.m6.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx1.p1.6.m6.1b"><apply id="S5.SS2.SSSx1.p1.6.m6.1.1.cmml" xref="S5.SS2.SSSx1.p1.6.m6.1.1"><lt id="S5.SS2.SSSx1.p1.6.m6.1.1.1.cmml" xref="S5.SS2.SSSx1.p1.6.m6.1.1.1"></lt><ci id="S5.SS2.SSSx1.p1.6.m6.1.1.2.cmml" xref="S5.SS2.SSSx1.p1.6.m6.1.1.2">𝑝</ci><cn id="S5.SS2.SSSx1.p1.6.m6.1.1.3.cmml" type="float" xref="S5.SS2.SSSx1.p1.6.m6.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx1.p1.6.m6.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx1.p1.6.m6.1d">italic_p &lt; 0.0001</annotation></semantics></math> &amp; <math alttext="p&lt;0.01" class="ltx_Math" display="inline" id="S5.SS2.SSSx1.p1.7.m7.1"><semantics id="S5.SS2.SSSx1.p1.7.m7.1a"><mrow id="S5.SS2.SSSx1.p1.7.m7.1.1" xref="S5.SS2.SSSx1.p1.7.m7.1.1.cmml"><mi id="S5.SS2.SSSx1.p1.7.m7.1.1.2" xref="S5.SS2.SSSx1.p1.7.m7.1.1.2.cmml">p</mi><mo id="S5.SS2.SSSx1.p1.7.m7.1.1.1" xref="S5.SS2.SSSx1.p1.7.m7.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx1.p1.7.m7.1.1.3" xref="S5.SS2.SSSx1.p1.7.m7.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx1.p1.7.m7.1b"><apply id="S5.SS2.SSSx1.p1.7.m7.1.1.cmml" xref="S5.SS2.SSSx1.p1.7.m7.1.1"><lt id="S5.SS2.SSSx1.p1.7.m7.1.1.1.cmml" xref="S5.SS2.SSSx1.p1.7.m7.1.1.1"></lt><ci id="S5.SS2.SSSx1.p1.7.m7.1.1.2.cmml" xref="S5.SS2.SSSx1.p1.7.m7.1.1.2">𝑝</ci><cn id="S5.SS2.SSSx1.p1.7.m7.1.1.3.cmml" type="float" xref="S5.SS2.SSSx1.p1.7.m7.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx1.p1.7.m7.1c">p&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx1.p1.7.m7.1d">italic_p &lt; 0.01</annotation></semantics></math>). But no significant difference was found for easy videos among the three groups.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSSx1.p2">
<p class="ltx_p" id="S5.SS2.SSSx1.p2.1">This observation matches very well with our expectations to different video difficulties: the built-in linear interpolation tool for manual annotation is very efficient in tracking a single face continuously, but <span class="ltx_text ltx_font_bold" id="S5.SS2.SSSx1.p2.1.1">AI recommendations can dramatically reduce task time when tracking multiple faces simultaneously in medium and hard videos</span>. This finding allows the system designer to optimize efficiency further: if we know a certain portion of the data has one or fewer people in each frame, it would be reasonable to bypass the AI pre-annotation to save on the GPU budget.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSSx2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">User expertise.</h4>
<div class="ltx_para" id="S5.SS2.SSSx2.p1">
<p class="ltx_p" id="S5.SS2.SSSx2.p1.2">When solely considering the user expertise factor, we were surprised that veteran workers are overall significantly slower than novice workers in both parts of the study (Welch, Part 1: <math alttext="F_{1,1380}=85.6,p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx2.p1.1.m1.4"><semantics id="S5.SS2.SSSx2.p1.1.m1.4a"><mrow id="S5.SS2.SSSx2.p1.1.m1.4.4.2" xref="S5.SS2.SSSx2.p1.1.m1.4.4.3.cmml"><mrow id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.cmml"><msub id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2.cmml"><mi id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2.2" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2.2.cmml">F</mi><mrow id="S5.SS2.SSSx2.p1.1.m1.2.2.2.4" xref="S5.SS2.SSSx2.p1.1.m1.2.2.2.3.cmml"><mn id="S5.SS2.SSSx2.p1.1.m1.1.1.1.1" xref="S5.SS2.SSSx2.p1.1.m1.1.1.1.1.cmml">1</mn><mo id="S5.SS2.SSSx2.p1.1.m1.2.2.2.4.1" xref="S5.SS2.SSSx2.p1.1.m1.2.2.2.3.cmml">,</mo><mn id="S5.SS2.SSSx2.p1.1.m1.2.2.2.2" xref="S5.SS2.SSSx2.p1.1.m1.2.2.2.2.cmml">1380</mn></mrow></msub><mo id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.1" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.1.cmml">=</mo><mn id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.3" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.3.cmml">85.6</mn></mrow><mo id="S5.SS2.SSSx2.p1.1.m1.4.4.2.3" xref="S5.SS2.SSSx2.p1.1.m1.4.4.3a.cmml">,</mo><mrow id="S5.SS2.SSSx2.p1.1.m1.4.4.2.2" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.cmml"><mi id="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.2" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.2.cmml">p</mi><mo id="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.1" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.3" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.3.cmml">0.0001</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx2.p1.1.m1.4b"><apply id="S5.SS2.SSSx2.p1.1.m1.4.4.3.cmml" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2"><csymbol cd="ambiguous" id="S5.SS2.SSSx2.p1.1.m1.4.4.3a.cmml" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.cmml" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1"><eq id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.1.cmml" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.1"></eq><apply id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2.cmml" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2.1.cmml" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2">subscript</csymbol><ci id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2.2.cmml" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.2.2">𝐹</ci><list id="S5.SS2.SSSx2.p1.1.m1.2.2.2.3.cmml" xref="S5.SS2.SSSx2.p1.1.m1.2.2.2.4"><cn id="S5.SS2.SSSx2.p1.1.m1.1.1.1.1.cmml" type="integer" xref="S5.SS2.SSSx2.p1.1.m1.1.1.1.1">1</cn><cn id="S5.SS2.SSSx2.p1.1.m1.2.2.2.2.cmml" type="integer" xref="S5.SS2.SSSx2.p1.1.m1.2.2.2.2">1380</cn></list></apply><cn id="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.3.cmml" type="float" xref="S5.SS2.SSSx2.p1.1.m1.3.3.1.1.3">85.6</cn></apply><apply id="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.cmml" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.2"><lt id="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.1.cmml" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.1"></lt><ci id="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.2.cmml" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.2">𝑝</ci><cn id="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.3.cmml" type="float" xref="S5.SS2.SSSx2.p1.1.m1.4.4.2.2.3">0.0001</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx2.p1.1.m1.4c">F_{1,1380}=85.6,p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx2.p1.1.m1.4d">italic_F start_POSTSUBSCRIPT 1 , 1380 end_POSTSUBSCRIPT = 85.6 , italic_p &lt; 0.0001</annotation></semantics></math>, Part 2: <math alttext="F_{1,665}=22.2,p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx2.p1.2.m2.4"><semantics id="S5.SS2.SSSx2.p1.2.m2.4a"><mrow id="S5.SS2.SSSx2.p1.2.m2.4.4.2" xref="S5.SS2.SSSx2.p1.2.m2.4.4.3.cmml"><mrow id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.cmml"><msub id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2.cmml"><mi id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2.2" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2.2.cmml">F</mi><mrow id="S5.SS2.SSSx2.p1.2.m2.2.2.2.4" xref="S5.SS2.SSSx2.p1.2.m2.2.2.2.3.cmml"><mn id="S5.SS2.SSSx2.p1.2.m2.1.1.1.1" xref="S5.SS2.SSSx2.p1.2.m2.1.1.1.1.cmml">1</mn><mo id="S5.SS2.SSSx2.p1.2.m2.2.2.2.4.1" xref="S5.SS2.SSSx2.p1.2.m2.2.2.2.3.cmml">,</mo><mn id="S5.SS2.SSSx2.p1.2.m2.2.2.2.2" xref="S5.SS2.SSSx2.p1.2.m2.2.2.2.2.cmml">665</mn></mrow></msub><mo id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.1" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.1.cmml">=</mo><mn id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.3" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.3.cmml">22.2</mn></mrow><mo id="S5.SS2.SSSx2.p1.2.m2.4.4.2.3" xref="S5.SS2.SSSx2.p1.2.m2.4.4.3a.cmml">,</mo><mrow id="S5.SS2.SSSx2.p1.2.m2.4.4.2.2" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.cmml"><mi id="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.2" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.2.cmml">p</mi><mo id="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.1" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.3" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.3.cmml">0.0001</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx2.p1.2.m2.4b"><apply id="S5.SS2.SSSx2.p1.2.m2.4.4.3.cmml" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2"><csymbol cd="ambiguous" id="S5.SS2.SSSx2.p1.2.m2.4.4.3a.cmml" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.3">formulae-sequence</csymbol><apply id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.cmml" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1"><eq id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.1.cmml" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.1"></eq><apply id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2.cmml" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2"><csymbol cd="ambiguous" id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2.1.cmml" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2">subscript</csymbol><ci id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2.2.cmml" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.2.2">𝐹</ci><list id="S5.SS2.SSSx2.p1.2.m2.2.2.2.3.cmml" xref="S5.SS2.SSSx2.p1.2.m2.2.2.2.4"><cn id="S5.SS2.SSSx2.p1.2.m2.1.1.1.1.cmml" type="integer" xref="S5.SS2.SSSx2.p1.2.m2.1.1.1.1">1</cn><cn id="S5.SS2.SSSx2.p1.2.m2.2.2.2.2.cmml" type="integer" xref="S5.SS2.SSSx2.p1.2.m2.2.2.2.2">665</cn></list></apply><cn id="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.3.cmml" type="float" xref="S5.SS2.SSSx2.p1.2.m2.3.3.1.1.3">22.2</cn></apply><apply id="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.cmml" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.2"><lt id="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.1.cmml" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.1"></lt><ci id="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.2.cmml" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.2">𝑝</ci><cn id="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.3.cmml" type="float" xref="S5.SS2.SSSx2.p1.2.m2.4.4.2.2.3">0.0001</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx2.p1.2.m2.4c">F_{1,665}=22.2,p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx2.p1.2.m2.4d">italic_F start_POSTSUBSCRIPT 1 , 665 end_POSTSUBSCRIPT = 22.2 , italic_p &lt; 0.0001</annotation></semantics></math>)! However, if we consider how people are paid, this result would be a reasonable optimization given the incentives – veteran workers know the acceptable work pace, so they do not need to work faster than necessary. We further discussed worker’s incentives in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.SS2" title="6.2. Can AI teammates set the quality lower bound in a crowdsourcing setting? ‣ 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6.2</span></a>.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSSx2.p2">
<p class="ltx_p" id="S5.SS2.SSSx2.p2.3">When we consider the group and user expertise factors at the same time, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F6" title="Figure 6 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6</span></a> (right), both novice and veteran workers in Group C who reviewed the zealous AI recommendations were significantly faster than the baseline (Bonferroni <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx2.p2.1.m1.1"><semantics id="S5.SS2.SSSx2.p2.1.m1.1a"><mrow id="S5.SS2.SSSx2.p2.1.m1.1.1" xref="S5.SS2.SSSx2.p2.1.m1.1.1.cmml"><mi id="S5.SS2.SSSx2.p2.1.m1.1.1.2" xref="S5.SS2.SSSx2.p2.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS2.SSSx2.p2.1.m1.1.1.1" xref="S5.SS2.SSSx2.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx2.p2.1.m1.1.1.3" xref="S5.SS2.SSSx2.p2.1.m1.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx2.p2.1.m1.1b"><apply id="S5.SS2.SSSx2.p2.1.m1.1.1.cmml" xref="S5.SS2.SSSx2.p2.1.m1.1.1"><lt id="S5.SS2.SSSx2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.SSSx2.p2.1.m1.1.1.1"></lt><ci id="S5.SS2.SSSx2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.SSSx2.p2.1.m1.1.1.2">𝑝</ci><cn id="S5.SS2.SSSx2.p2.1.m1.1.1.3.cmml" type="float" xref="S5.SS2.SSSx2.p2.1.m1.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx2.p2.1.m1.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx2.p2.1.m1.1d">italic_p &lt; 0.0001</annotation></semantics></math> &amp; <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx2.p2.2.m2.1"><semantics id="S5.SS2.SSSx2.p2.2.m2.1a"><mrow id="S5.SS2.SSSx2.p2.2.m2.1.1" xref="S5.SS2.SSSx2.p2.2.m2.1.1.cmml"><mi id="S5.SS2.SSSx2.p2.2.m2.1.1.2" xref="S5.SS2.SSSx2.p2.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS2.SSSx2.p2.2.m2.1.1.1" xref="S5.SS2.SSSx2.p2.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx2.p2.2.m2.1.1.3" xref="S5.SS2.SSSx2.p2.2.m2.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx2.p2.2.m2.1b"><apply id="S5.SS2.SSSx2.p2.2.m2.1.1.cmml" xref="S5.SS2.SSSx2.p2.2.m2.1.1"><lt id="S5.SS2.SSSx2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.SSSx2.p2.2.m2.1.1.1"></lt><ci id="S5.SS2.SSSx2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.SSSx2.p2.2.m2.1.1.2">𝑝</ci><cn id="S5.SS2.SSSx2.p2.2.m2.1.1.3.cmml" type="float" xref="S5.SS2.SSSx2.p2.2.m2.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx2.p2.2.m2.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx2.p2.2.m2.1d">italic_p &lt; 0.0001</annotation></semantics></math>), while only the veterans in Group B finished faster (Bonferroni <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS2.SSSx2.p2.3.m3.1"><semantics id="S5.SS2.SSSx2.p2.3.m3.1a"><mrow id="S5.SS2.SSSx2.p2.3.m3.1.1" xref="S5.SS2.SSSx2.p2.3.m3.1.1.cmml"><mi id="S5.SS2.SSSx2.p2.3.m3.1.1.2" xref="S5.SS2.SSSx2.p2.3.m3.1.1.2.cmml">p</mi><mo id="S5.SS2.SSSx2.p2.3.m3.1.1.1" xref="S5.SS2.SSSx2.p2.3.m3.1.1.1.cmml">&lt;</mo><mn id="S5.SS2.SSSx2.p2.3.m3.1.1.3" xref="S5.SS2.SSSx2.p2.3.m3.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSSx2.p2.3.m3.1b"><apply id="S5.SS2.SSSx2.p2.3.m3.1.1.cmml" xref="S5.SS2.SSSx2.p2.3.m3.1.1"><lt id="S5.SS2.SSSx2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.SSSx2.p2.3.m3.1.1.1"></lt><ci id="S5.SS2.SSSx2.p2.3.m3.1.1.2.cmml" xref="S5.SS2.SSSx2.p2.3.m3.1.1.2">𝑝</ci><cn id="S5.SS2.SSSx2.p2.3.m3.1.1.3.cmml" type="float" xref="S5.SS2.SSSx2.p2.3.m3.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSSx2.p2.3.m3.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSSx2.p2.3.m3.1d">italic_p &lt; 0.0001</annotation></semantics></math>). This allows us to infer that, unlike the restrained AI that helps veterans more, <span class="ltx_text ltx_font_bold" id="S5.SS2.SSSx2.p2.3.1">the zealous AI can consistently improve user completion time for both novice and veteran annotators</span>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Q3: Which AI helps annotators achieve higher recall?</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">From the F1 scores in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F5" title="Figure 5 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5</span></a> we know that both AI-assisted methods yield significantly higher-quality annotations than the baseline method (compared in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.SS1" title="5.1. Q1: Can the human-AI teams achieve ”complementary team performance” in this task? ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5.1</span></a>), yet we saw no clear winner between the two human-AI teams. Because recall is paramount in video anonymization tasks, we analyze Group B and C’s recall performance in detail.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F8" title="Figure 8 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">8</span></a> shows that Group C, the annotators who reviewed zealous AI recommendations, have an overall significant advantage over Group B, which reviewed restrained AI recommendations (Games-Howell <math alttext="p&lt;0.01" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><mrow id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS3.p2.1.m1.1.1.1" xref="S5.SS3.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><lt id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1.1"></lt><ci id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">𝑝</ci><cn id="S5.SS3.p2.1.m1.1.1.3.cmml" type="float" xref="S5.SS3.p2.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">p&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">italic_p &lt; 0.01</annotation></semantics></math>). Interestingly, we noticed <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">a visible shorter tail</span> in Group C’s recall distribution in hard videos (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F8" title="Figure 8 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">8</span></a>, right). This observation matches the very nature of zealous AI – giving more recommendations, even low-confidence ones, so the human teammate is less likely to miss a face. This strategy is especially effective in hard videos because tracking too many faces simultaneously pushes the user’s attention to its limit. <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.2">Zealous AI’s superfluous recommendations allow the user to focus on the action of <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2.1">reject</span>, rather than searching for missing faces and then <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2.2">solve</span></span>.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.3">Taking user expertise into account, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F10" title="Figure 10 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">10</span></a> (a) reveals that while both AIs improved the veterans’ recall performance compared to the baseline Group A (Bonferroni A/B: <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><mrow id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml"><mi id="S5.SS3.p3.1.m1.1.1.2" xref="S5.SS3.p3.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS3.p3.1.m1.1.1.1" xref="S5.SS3.p3.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS3.p3.1.m1.1.1.3" xref="S5.SS3.p3.1.m1.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><apply id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"><lt id="S5.SS3.p3.1.m1.1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1.1"></lt><ci id="S5.SS3.p3.1.m1.1.1.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2">𝑝</ci><cn id="S5.SS3.p3.1.m1.1.1.3.cmml" type="float" xref="S5.SS3.p3.1.m1.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">italic_p &lt; 0.0001</annotation></semantics></math>, A/C <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS3.p3.2.m2.1"><semantics id="S5.SS3.p3.2.m2.1a"><mrow id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml"><mi id="S5.SS3.p3.2.m2.1.1.2" xref="S5.SS3.p3.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS3.p3.2.m2.1.1.1" xref="S5.SS3.p3.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS3.p3.2.m2.1.1.3" xref="S5.SS3.p3.2.m2.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><apply id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"><lt id="S5.SS3.p3.2.m2.1.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1.1"></lt><ci id="S5.SS3.p3.2.m2.1.1.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2">𝑝</ci><cn id="S5.SS3.p3.2.m2.1.1.3.cmml" type="float" xref="S5.SS3.p3.2.m2.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.2.m2.1d">italic_p &lt; 0.0001</annotation></semantics></math>), for novice workers, we only saw a significant advantage of Group C over Group A (Bonferroni <math alttext="p&lt;0.048" class="ltx_Math" display="inline" id="S5.SS3.p3.3.m3.1"><semantics id="S5.SS3.p3.3.m3.1a"><mrow id="S5.SS3.p3.3.m3.1.1" xref="S5.SS3.p3.3.m3.1.1.cmml"><mi id="S5.SS3.p3.3.m3.1.1.2" xref="S5.SS3.p3.3.m3.1.1.2.cmml">p</mi><mo id="S5.SS3.p3.3.m3.1.1.1" xref="S5.SS3.p3.3.m3.1.1.1.cmml">&lt;</mo><mn id="S5.SS3.p3.3.m3.1.1.3" xref="S5.SS3.p3.3.m3.1.1.3.cmml">0.048</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.1b"><apply id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1"><lt id="S5.SS3.p3.3.m3.1.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1.1"></lt><ci id="S5.SS3.p3.3.m3.1.1.2.cmml" xref="S5.SS3.p3.3.m3.1.1.2">𝑝</ci><cn id="S5.SS3.p3.3.m3.1.1.3.cmml" type="float" xref="S5.SS3.p3.3.m3.1.1.3">0.048</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.1c">p&lt;0.048</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.3.m3.1d">italic_p &lt; 0.048</annotation></semantics></math>). It corroborates our previous finding on completion time that ”the zealous AI can consistently improve both novice and veteran annotators” and extends the statement to higher recalls percentages as well.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Q4: Will collaborating with an AI improve or hurt user skills?</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Should the annotators lose access to their AI teammates in the future, how will they perform? While we are interested in improving human-AI team performance, we should also seriously consider how the prior human-AI collaboration experience would affect people’s skills in the long run before deploying a new system.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">To find out, we removed AI recommendations from Groups B and C in Part 2, so all groups now work with the manual tool that they have always been using for other projects. It took most annotators two to three weeks to complete Part 1 of the study. For the sake of interpreting the results of Part 2, we can consider this period a training period and their performance in Part 2 showcasing the effect of this medium-term training effort.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">Both Groups B &amp; C collaborated with their perspective AI teammates for 2-3 weeks, <span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">but the restrained-AI-trained annotators in Group B performed worse than their peers in different ways</span> – the novice workers were significantly slower than both A &amp; C, especially in hard videos. The veteran workers’ annotations had lower recall percentages than the zealous-AI-trained workers in Group C.</p>
</div>
<section class="ltx_subsubsection" id="S5.SS4.SSSx1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Completion time.</h4>
<div class="ltx_para" id="S5.SS4.SSSx1.p1">
<p class="ltx_p" id="S5.SS4.SSSx1.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F7" title="Figure 7 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">7</span></a> shows the task completion time of Part 2’s 12 new videos without AI recommendations. In all video difficulties, Group C, annotators who previously worked with the zealous AI in Part 1, managed to finish as quickly as Group A, the annotators who were trained using the very manual method now in deployment for all groups. It shows that training with zealous AI recommendations does not negatively affect users’ task completion time on subsequent manual tasks.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSSx1.p2">
<p class="ltx_p" id="S5.SS4.SSSx1.p2.6">However, we were surprised to see that Group B annotators trained with the restrained AI became overall significantly slower than Groups A &amp; C (Tukey-HSD A/B: <math alttext="p~{}&lt;0.021" class="ltx_Math" display="inline" id="S5.SS4.SSSx1.p2.1.m1.1"><semantics id="S5.SS4.SSSx1.p2.1.m1.1a"><mrow id="S5.SS4.SSSx1.p2.1.m1.1.1" xref="S5.SS4.SSSx1.p2.1.m1.1.1.cmml"><mi id="S5.SS4.SSSx1.p2.1.m1.1.1.2" xref="S5.SS4.SSSx1.p2.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS4.SSSx1.p2.1.m1.1.1.1" lspace="0.608em" xref="S5.SS4.SSSx1.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.SSSx1.p2.1.m1.1.1.3" xref="S5.SS4.SSSx1.p2.1.m1.1.1.3.cmml">0.021</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSSx1.p2.1.m1.1b"><apply id="S5.SS4.SSSx1.p2.1.m1.1.1.cmml" xref="S5.SS4.SSSx1.p2.1.m1.1.1"><lt id="S5.SS4.SSSx1.p2.1.m1.1.1.1.cmml" xref="S5.SS4.SSSx1.p2.1.m1.1.1.1"></lt><ci id="S5.SS4.SSSx1.p2.1.m1.1.1.2.cmml" xref="S5.SS4.SSSx1.p2.1.m1.1.1.2">𝑝</ci><cn id="S5.SS4.SSSx1.p2.1.m1.1.1.3.cmml" type="float" xref="S5.SS4.SSSx1.p2.1.m1.1.1.3">0.021</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSSx1.p2.1.m1.1c">p~{}&lt;0.021</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSSx1.p2.1.m1.1d">italic_p &lt; 0.021</annotation></semantics></math>, B/C: <math alttext="p&lt;0.01" class="ltx_Math" display="inline" id="S5.SS4.SSSx1.p2.2.m2.1"><semantics id="S5.SS4.SSSx1.p2.2.m2.1a"><mrow id="S5.SS4.SSSx1.p2.2.m2.1.1" xref="S5.SS4.SSSx1.p2.2.m2.1.1.cmml"><mi id="S5.SS4.SSSx1.p2.2.m2.1.1.2" xref="S5.SS4.SSSx1.p2.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS4.SSSx1.p2.2.m2.1.1.1" xref="S5.SS4.SSSx1.p2.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.SSSx1.p2.2.m2.1.1.3" xref="S5.SS4.SSSx1.p2.2.m2.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSSx1.p2.2.m2.1b"><apply id="S5.SS4.SSSx1.p2.2.m2.1.1.cmml" xref="S5.SS4.SSSx1.p2.2.m2.1.1"><lt id="S5.SS4.SSSx1.p2.2.m2.1.1.1.cmml" xref="S5.SS4.SSSx1.p2.2.m2.1.1.1"></lt><ci id="S5.SS4.SSSx1.p2.2.m2.1.1.2.cmml" xref="S5.SS4.SSSx1.p2.2.m2.1.1.2">𝑝</ci><cn id="S5.SS4.SSSx1.p2.2.m2.1.1.3.cmml" type="float" xref="S5.SS4.SSSx1.p2.2.m2.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSSx1.p2.2.m2.1c">p&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSSx1.p2.2.m2.1d">italic_p &lt; 0.01</annotation></semantics></math>), and more specifically in hard videos (Bonferroni A/B: <math alttext="p&lt;0.044" class="ltx_Math" display="inline" id="S5.SS4.SSSx1.p2.3.m3.1"><semantics id="S5.SS4.SSSx1.p2.3.m3.1a"><mrow id="S5.SS4.SSSx1.p2.3.m3.1.1" xref="S5.SS4.SSSx1.p2.3.m3.1.1.cmml"><mi id="S5.SS4.SSSx1.p2.3.m3.1.1.2" xref="S5.SS4.SSSx1.p2.3.m3.1.1.2.cmml">p</mi><mo id="S5.SS4.SSSx1.p2.3.m3.1.1.1" xref="S5.SS4.SSSx1.p2.3.m3.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.SSSx1.p2.3.m3.1.1.3" xref="S5.SS4.SSSx1.p2.3.m3.1.1.3.cmml">0.044</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSSx1.p2.3.m3.1b"><apply id="S5.SS4.SSSx1.p2.3.m3.1.1.cmml" xref="S5.SS4.SSSx1.p2.3.m3.1.1"><lt id="S5.SS4.SSSx1.p2.3.m3.1.1.1.cmml" xref="S5.SS4.SSSx1.p2.3.m3.1.1.1"></lt><ci id="S5.SS4.SSSx1.p2.3.m3.1.1.2.cmml" xref="S5.SS4.SSSx1.p2.3.m3.1.1.2">𝑝</ci><cn id="S5.SS4.SSSx1.p2.3.m3.1.1.3.cmml" type="float" xref="S5.SS4.SSSx1.p2.3.m3.1.1.3">0.044</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSSx1.p2.3.m3.1c">p&lt;0.044</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSSx1.p2.3.m3.1d">italic_p &lt; 0.044</annotation></semantics></math>, B/C: <math alttext="p&lt;0.013" class="ltx_Math" display="inline" id="S5.SS4.SSSx1.p2.4.m4.1"><semantics id="S5.SS4.SSSx1.p2.4.m4.1a"><mrow id="S5.SS4.SSSx1.p2.4.m4.1.1" xref="S5.SS4.SSSx1.p2.4.m4.1.1.cmml"><mi id="S5.SS4.SSSx1.p2.4.m4.1.1.2" xref="S5.SS4.SSSx1.p2.4.m4.1.1.2.cmml">p</mi><mo id="S5.SS4.SSSx1.p2.4.m4.1.1.1" xref="S5.SS4.SSSx1.p2.4.m4.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.SSSx1.p2.4.m4.1.1.3" xref="S5.SS4.SSSx1.p2.4.m4.1.1.3.cmml">0.013</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSSx1.p2.4.m4.1b"><apply id="S5.SS4.SSSx1.p2.4.m4.1.1.cmml" xref="S5.SS4.SSSx1.p2.4.m4.1.1"><lt id="S5.SS4.SSSx1.p2.4.m4.1.1.1.cmml" xref="S5.SS4.SSSx1.p2.4.m4.1.1.1"></lt><ci id="S5.SS4.SSSx1.p2.4.m4.1.1.2.cmml" xref="S5.SS4.SSSx1.p2.4.m4.1.1.2">𝑝</ci><cn id="S5.SS4.SSSx1.p2.4.m4.1.1.3.cmml" type="float" xref="S5.SS4.SSSx1.p2.4.m4.1.1.3">0.013</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSSx1.p2.4.m4.1c">p&lt;0.013</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSSx1.p2.4.m4.1d">italic_p &lt; 0.013</annotation></semantics></math>). Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F7" title="Figure 7 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">7</span></a> (right) shows that the effects stem mainly from the novice users (Bonferroni A/B: <math alttext="p&lt;0.0001" class="ltx_Math" display="inline" id="S5.SS4.SSSx1.p2.5.m5.1"><semantics id="S5.SS4.SSSx1.p2.5.m5.1a"><mrow id="S5.SS4.SSSx1.p2.5.m5.1.1" xref="S5.SS4.SSSx1.p2.5.m5.1.1.cmml"><mi id="S5.SS4.SSSx1.p2.5.m5.1.1.2" xref="S5.SS4.SSSx1.p2.5.m5.1.1.2.cmml">p</mi><mo id="S5.SS4.SSSx1.p2.5.m5.1.1.1" xref="S5.SS4.SSSx1.p2.5.m5.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.SSSx1.p2.5.m5.1.1.3" xref="S5.SS4.SSSx1.p2.5.m5.1.1.3.cmml">0.0001</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSSx1.p2.5.m5.1b"><apply id="S5.SS4.SSSx1.p2.5.m5.1.1.cmml" xref="S5.SS4.SSSx1.p2.5.m5.1.1"><lt id="S5.SS4.SSSx1.p2.5.m5.1.1.1.cmml" xref="S5.SS4.SSSx1.p2.5.m5.1.1.1"></lt><ci id="S5.SS4.SSSx1.p2.5.m5.1.1.2.cmml" xref="S5.SS4.SSSx1.p2.5.m5.1.1.2">𝑝</ci><cn id="S5.SS4.SSSx1.p2.5.m5.1.1.3.cmml" type="float" xref="S5.SS4.SSSx1.p2.5.m5.1.1.3">0.0001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSSx1.p2.5.m5.1c">p&lt;0.0001</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSSx1.p2.5.m5.1d">italic_p &lt; 0.0001</annotation></semantics></math>, B/C: <math alttext="p&lt;0.01" class="ltx_Math" display="inline" id="S5.SS4.SSSx1.p2.6.m6.1"><semantics id="S5.SS4.SSSx1.p2.6.m6.1a"><mrow id="S5.SS4.SSSx1.p2.6.m6.1.1" xref="S5.SS4.SSSx1.p2.6.m6.1.1.cmml"><mi id="S5.SS4.SSSx1.p2.6.m6.1.1.2" xref="S5.SS4.SSSx1.p2.6.m6.1.1.2.cmml">p</mi><mo id="S5.SS4.SSSx1.p2.6.m6.1.1.1" xref="S5.SS4.SSSx1.p2.6.m6.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.SSSx1.p2.6.m6.1.1.3" xref="S5.SS4.SSSx1.p2.6.m6.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSSx1.p2.6.m6.1b"><apply id="S5.SS4.SSSx1.p2.6.m6.1.1.cmml" xref="S5.SS4.SSSx1.p2.6.m6.1.1"><lt id="S5.SS4.SSSx1.p2.6.m6.1.1.1.cmml" xref="S5.SS4.SSSx1.p2.6.m6.1.1.1"></lt><ci id="S5.SS4.SSSx1.p2.6.m6.1.1.2.cmml" xref="S5.SS4.SSSx1.p2.6.m6.1.1.2">𝑝</ci><cn id="S5.SS4.SSSx1.p2.6.m6.1.1.3.cmml" type="float" xref="S5.SS4.SSSx1.p2.6.m6.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSSx1.p2.6.m6.1c">p&lt;0.01</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSSx1.p2.6.m6.1d">italic_p &lt; 0.01</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSSx2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Recall.</h4>
<div class="ltx_para" id="S5.SS4.SSSx2.p1">
<p class="ltx_p" id="S5.SS4.SSSx2.p1.2">On annotation quality, Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F9" title="Figure 9 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">9</span></a> shows the Groups B annotators, trained by the high-precision restrained AI now produce lower-recall annotations (Games-Howell <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S5.SS4.SSSx2.p1.1.m1.1"><semantics id="S5.SS4.SSSx2.p1.1.m1.1a"><mrow id="S5.SS4.SSSx2.p1.1.m1.1.1" xref="S5.SS4.SSSx2.p1.1.m1.1.1.cmml"><mi id="S5.SS4.SSSx2.p1.1.m1.1.1.2" xref="S5.SS4.SSSx2.p1.1.m1.1.1.2.cmml">p</mi><mo id="S5.SS4.SSSx2.p1.1.m1.1.1.1" xref="S5.SS4.SSSx2.p1.1.m1.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.SSSx2.p1.1.m1.1.1.3" xref="S5.SS4.SSSx2.p1.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSSx2.p1.1.m1.1b"><apply id="S5.SS4.SSSx2.p1.1.m1.1.1.cmml" xref="S5.SS4.SSSx2.p1.1.m1.1.1"><lt id="S5.SS4.SSSx2.p1.1.m1.1.1.1.cmml" xref="S5.SS4.SSSx2.p1.1.m1.1.1.1"></lt><ci id="S5.SS4.SSSx2.p1.1.m1.1.1.2.cmml" xref="S5.SS4.SSSx2.p1.1.m1.1.1.2">𝑝</ci><cn id="S5.SS4.SSSx2.p1.1.m1.1.1.3.cmml" type="float" xref="S5.SS4.SSSx2.p1.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSSx2.p1.1.m1.1c">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSSx2.p1.1.m1.1d">italic_p &lt; 0.05</annotation></semantics></math>) than Group C which was trained with the high-recall zealous AI. The user expertise breakdown shows the effect mostly comes from the veteran workers (Bonferroni <math alttext="p&lt;0.028" class="ltx_Math" display="inline" id="S5.SS4.SSSx2.p1.2.m2.1"><semantics id="S5.SS4.SSSx2.p1.2.m2.1a"><mrow id="S5.SS4.SSSx2.p1.2.m2.1.1" xref="S5.SS4.SSSx2.p1.2.m2.1.1.cmml"><mi id="S5.SS4.SSSx2.p1.2.m2.1.1.2" xref="S5.SS4.SSSx2.p1.2.m2.1.1.2.cmml">p</mi><mo id="S5.SS4.SSSx2.p1.2.m2.1.1.1" xref="S5.SS4.SSSx2.p1.2.m2.1.1.1.cmml">&lt;</mo><mn id="S5.SS4.SSSx2.p1.2.m2.1.1.3" xref="S5.SS4.SSSx2.p1.2.m2.1.1.3.cmml">0.028</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.SSSx2.p1.2.m2.1b"><apply id="S5.SS4.SSSx2.p1.2.m2.1.1.cmml" xref="S5.SS4.SSSx2.p1.2.m2.1.1"><lt id="S5.SS4.SSSx2.p1.2.m2.1.1.1.cmml" xref="S5.SS4.SSSx2.p1.2.m2.1.1.1"></lt><ci id="S5.SS4.SSSx2.p1.2.m2.1.1.2.cmml" xref="S5.SS4.SSSx2.p1.2.m2.1.1.2">𝑝</ci><cn id="S5.SS4.SSSx2.p1.2.m2.1.1.3.cmml" type="float" xref="S5.SS4.SSSx2.p1.2.m2.1.1.3">0.028</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSSx2.p1.2.m2.1c">p&lt;0.028</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSSx2.p1.2.m2.1d">italic_p &lt; 0.028</annotation></semantics></math>).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSSx3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">What caused the negative training effect from the restrained AI?</h4>
<div class="ltx_para" id="S5.SS4.SSSx3.p1">
<p class="ltx_p" id="S5.SS4.SSSx3.p1.1">We would think that annotators in Group B should perform better in Part 2 of the study now that they have to manually annotate – they practiced more on manually adding missing faces (<span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS4.SSSx3.p1.1.1">solve</span>) working with the restrained AI recommendations. In contrast, Group C which trained with the zealous AI focused on <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.SS4.SSSx3.p1.1.2">rejects</span>. However, the experiment results show otherwise. Why was only Group B negatively affected? We believe there are two main factors in play:</p>
</div>
<div class="ltx_para" id="S5.SS4.SSSx3.p2">
<p class="ltx_p" id="S5.SS4.SSSx3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.SSSx3.p2.1.1">1) Not optimizing the AI teammate for the human-in-the-loop workflow.</span> Despite the fact that both AIs used the same face-detection model to generate the untracked bounding boxes in each frame for the tracker to process, the restrained AI recommendations were produced by ByteTrack <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib65" title="">2022b</a>)</cite> which is designed for autonomous tasks rather than for human-AI collaboration. We observed various issues using that tracker directly in pilot studies, so we proposed the FPR tracker specifically for a human-in-the-loop workflow with many optimizations with human users in mind (discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S3.SS3" title="3.3. The false-positive-robust (FPR) tracker ‣ 3. Algorithm choices and pilot studies ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">3.3</span></a>). Given the fact that only novice users became much slower in Part 2 of our study while veterans, who are more familiar with the annotation tool, were unaffected, we strongly believe that the negative transfer effect can be linked back directly to training with the restrained AI.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSSx3.p3">
<p class="ltx_p" id="S5.SS4.SSSx3.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS4.SSSx3.p3.1.1">2) Not optimizing the AI teammate for the task.</span> Recommendations from the high-precision restrained AI are naturally lower in recall than the zealous AI, i.e., the restrained AI missed more faces. Users who worked with such an AI for 2-3 weeks might actually have gotten used to the AI’s pre-annotated videos (in Part 1) as ”acceptable quality”, thus matching their annotation effort with the less optimal recall when working on their own in Part 2. On the other hand, the zealous AI recommendations – the high-recall AI more exhaustively demonstrated all faces that should be annotated, potentially raising the quality standard for the task.</p>
</div>
<div class="ltx_para" id="S5.SS4.SSSx3.p4">
<p class="ltx_p" id="S5.SS4.SSSx3.p4.1">In conclusion, various pieces of evidence from Part 2 of our study showed that despite decent human-AI team performance when working with the AI, naively deploying an AI system into a human-AI setting without considering the nature of the task or without optimizing it for the human teammates could lead to negative effects and potential deskilling of the users.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>The key to forming a strong human-AI team</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">We propose the restrained AI and the zealous AI to depict the tradeoff between precision and recall as two characteristics that have the potential of becoming advantages in human-AI teams if used properly. By actually using the annotation tools and watching annotators’ screens for many hours, we observed that annotators need much less effort in improving precision than recall in a model-assisted annotation task, i.e., rejecting an incorrect box is much easier than adding a missing box, thus we should delegate more effort in improving recall to the AI so human only handles the most difficult boxes that the AI missed (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S0.F1" title="Figure 1 ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">1</span></a>c).</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">We think <span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">an important insight from this study is that it is worthwhile to identify the complementary strengths of both human and AI teammates through an in-depth analysis of the task at hand</span>. While our observations can improve real-world object detection and tracking annotation tasks, in which correcting false-positive errors are easier for human, another task with a higher cost in correcting such errors could lead to different or even opposite optimizations. Working closely with end users can inspire us to decompose the AI’s different properties (in our case precision and recall) and turn them into advantages to complement human skills. We hope this study can motivate fellow researchers to rethink existing AI assistance designs or at least the design for other video annotation tasks.</p>
</div>
<figure class="ltx_figure" id="S6.SS1.4">
<div class="ltx_block" id="S6.SS1.4.5">
<figure class="ltx_figure ltx_figure_panel ltx_align_middle" id="S6.F12" style="width:260.2pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_portrait" height="761" id="S6.SS1.1.1.g1" src="extracted/5905702/fig_Part1_BC_subjective_questions.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S6.F12.2">
<figure class="ltx_figure ltx_figure_panel" id="S6.F11">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F11.2.1.1" style="font-size:90%;">Figure 11</span>. </span><span class="ltx_text" id="S6.F11.3.2" style="font-size:90%;">Survey 1 (post-Part 1). We normalize each group’s five-point Likert scale responses to 100%. 0% indicates no preference. In Part 1’s between-subject study, annotators from Groups B &amp; C only worked with a single AI they were assigned to, so we do not compare the responses between B with C.</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S6.F12.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S6.SS1.2.2.1">Survey 1 questions and responses.

<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="114" id="S6.SS1.2.2.1.g1" src="extracted/5905702/fig_Part1_BC_survey_q6.png" width="598"/></p>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F12.4.1.1" style="font-size:90%;">Figure 12</span>. </span><span class="ltx_text" id="S6.F12.5.2" style="font-size:90%;">Question S1-6 in Survey 1 indicates significant result. The five-point Likert scale responses are converted to [-2, 2] with mean and 95% CI plotted.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S6.F12.6">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S6.F12.7">Survey 1 question 6 shows both group B and C think the AI made their annotation task easier.</p>
</div>
</div>
</figure>
<figure class="ltx_figure ltx_figure_panel ltx_align_middle" id="S6.F14" style="width:164.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="457" id="S6.SS1.3.3.g1" src="extracted/5905702/fig_SUS_pointplot.png" width="598"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel" id="S6.F14.2">
<figure class="ltx_figure ltx_figure_panel" id="S6.F13">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F13.2.1.1" style="font-size:90%;">Figure 13</span>. </span><span class="ltx_text" id="S6.F13.3.2" style="font-size:90%;">A System Usability Scale (SUS) survey was administered at the conclusion of Part 1 of the study. But we saw no significant difference between the groups. Similar to Survey 1 in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.F11" title="Figure 11 ‣ Figure 12 ‣ 6.1. The key to forming a strong human-AI team ‣ 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">11</span></a>, participants tend to provide neutral feedback.</span></figcaption>
</figure><span class="ltx_ERROR undefined" id="S6.F14.2.1">\Description</span>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S6.SS1.4.4.1">SUS score of the three methods.

<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="556" id="S6.SS1.4.4.1.g1" src="extracted/5905702/fig_part2_survey.png" width="598"/></p>
</div>
<div class="ltx_flex_break"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F14.4.1.1" style="font-size:90%;">Figure 14</span>. </span><span class="ltx_text" id="S6.F14.5.2" style="font-size:90%;">Survey 2. Unlike Survey 1 in which annotators answered questions without comparison, Groups B &amp; C have used both AI-assisted and Manual methods at the end of Part 2. Thus this part of the study is close to a within-subject design where the independent variables are the AI-assisted and Manual method.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S6.F14.6">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S6.F14.7">Survey 2 questions and responses.</p>
</div>
</div>
</figure>
</div>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Can AI teammates set the quality lower bound in a crowdsourcing setting?</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We identified and rejected a single veteran user who submitted the majority of the low-quality annotations. This is an unexpected yet not surprising finding in a crowdsourcing setting: when paid at a flat hourly rate, people are not necessarily motivated to work faster. When lacking a quality-based performance evaluation mechanism, people are not necessarily motivated to push for ”better-than-sufficient” quality.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">However, could there be other users not making an effort in Groups B or C as well but not being identified? Because the two AIs have pre-annotated the videos in decent quality (<math alttext="F1&gt;90\%" class="ltx_Math" display="inline" id="S6.SS2.p2.1.m1.1"><semantics id="S6.SS2.p2.1.m1.1a"><mrow id="S6.SS2.p2.1.m1.1.1" xref="S6.SS2.p2.1.m1.1.1.cmml"><mrow id="S6.SS2.p2.1.m1.1.1.2" xref="S6.SS2.p2.1.m1.1.1.2.cmml"><mi id="S6.SS2.p2.1.m1.1.1.2.2" xref="S6.SS2.p2.1.m1.1.1.2.2.cmml">F</mi><mo id="S6.SS2.p2.1.m1.1.1.2.1" xref="S6.SS2.p2.1.m1.1.1.2.1.cmml">⁢</mo><mn id="S6.SS2.p2.1.m1.1.1.2.3" xref="S6.SS2.p2.1.m1.1.1.2.3.cmml">1</mn></mrow><mo id="S6.SS2.p2.1.m1.1.1.1" xref="S6.SS2.p2.1.m1.1.1.1.cmml">&gt;</mo><mrow id="S6.SS2.p2.1.m1.1.1.3" xref="S6.SS2.p2.1.m1.1.1.3.cmml"><mn id="S6.SS2.p2.1.m1.1.1.3.2" xref="S6.SS2.p2.1.m1.1.1.3.2.cmml">90</mn><mo id="S6.SS2.p2.1.m1.1.1.3.1" xref="S6.SS2.p2.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.p2.1.m1.1b"><apply id="S6.SS2.p2.1.m1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1"><gt id="S6.SS2.p2.1.m1.1.1.1.cmml" xref="S6.SS2.p2.1.m1.1.1.1"></gt><apply id="S6.SS2.p2.1.m1.1.1.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2"><times id="S6.SS2.p2.1.m1.1.1.2.1.cmml" xref="S6.SS2.p2.1.m1.1.1.2.1"></times><ci id="S6.SS2.p2.1.m1.1.1.2.2.cmml" xref="S6.SS2.p2.1.m1.1.1.2.2">𝐹</ci><cn id="S6.SS2.p2.1.m1.1.1.2.3.cmml" type="integer" xref="S6.SS2.p2.1.m1.1.1.2.3">1</cn></apply><apply id="S6.SS2.p2.1.m1.1.1.3.cmml" xref="S6.SS2.p2.1.m1.1.1.3"><csymbol cd="latexml" id="S6.SS2.p2.1.m1.1.1.3.1.cmml" xref="S6.SS2.p2.1.m1.1.1.3.1">percent</csymbol><cn id="S6.SS2.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S6.SS2.p2.1.m1.1.1.3.2">90</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.p2.1.m1.1c">F1&gt;90\%</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.p2.1.m1.1d">italic_F 1 &gt; 90 %</annotation></semantics></math>), it’s hard to tell if someone is actually happy with the AI’s recommendations or is not pushing for even better quality.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">What we know for sure is that such low-quality submissions, intentional or unintentional, will certainly appear in other real-world crowdsourcing tasks. However, in absence of ground truth, we won’t be able to identify them in a real-world setting. It is also very costly to identify bad submissions – ImageNet asks 10 votes for each image <cite class="ltx_cite ltx_citemacro_citep">(Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib18" title="">2009</a>)</cite>, and Microsoft COCO asks 3-5 workers to judge each segmentation <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib34" title="">2014</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">Could the AI recommendations have played a critical role in preventing low-quality submissions, i.e., setting a lower bound for the annotation quality? While not verified in our study, this observation could provide yet another strong motivation for human-AI collaboration in a crowdsourcing setting. We encourage fellow researchers to consider this in future experiment designs.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Seemingly contradictory survey results</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.F11" title="Figure 11 ‣ Figure 12 ‣ 6.1. The key to forming a strong human-AI team ‣ 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">11</span></a> shows user responses to the Survey 1 questions, with each group’s five-point Likert scale responses normalized to 100%. 0% indicates no preference. Specifically, question <span class="ltx_text ltx_font_italic" id="S6.SS3.p1.1.1">S1-6</span> (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.F12" title="Figure 12 ‣ 6.1. The key to forming a strong human-AI team ‣ 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">12</span></a>) indicates that users from both human-AI teams, B and C, think that working with the AI makes the task easier than annotating manually. However, in Survey 2 (Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.F14" title="Figure 14 ‣ 6.1. The key to forming a strong human-AI team ‣ 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">14</span></a>), after users have tried both the AI-assisted and the Manual methods on the same task of similar videos, they express higher preference towards the Manual method regarding multiple aspects. As users took each survey immediately after Part 1 and Part 2 respectively, they might prefer the method they just used, but these responses from Groups B &amp; C are in conflict with their continued higher recall in Part 2.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">Comparing Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F8" title="Figure 8 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">8</span></a> (left) with Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F9" title="Figure 9 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">9</span></a> (left), we observe that the Group B &amp; C annotators who had shorter tails in recall distribution than Group A in Part 1 ended up with longer tails in Part 2 after they lost the AI’s assistance. It shows that a fraction of low-performing users were apparently held at a higher standard by the AI recommendations, and when the AI teammate was gone, they returned to their preferred standard.</p>
</div>
<div class="ltx_para" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">This observation might help explain the higher performance with the AI-assisted method but higher user preference for the Manual method. It also reminds us to take users’ incentives into account when designing user preference questions in empirical studies – It is well-known that the most favorable method is not necessarily the best performing method. We administered the System Usability Scale (SUS) survey and saw a trend to support this point in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.F13" title="Figure 13 ‣ Figure 14 ‣ 6.1. The key to forming a strong human-AI team ‣ 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">13</span></a>, but the results are not significant.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Limitations and Future Work</h3>
<section class="ltx_subsubsection" id="S6.SS4.SSSx1">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">What are the conditions for which our findings hold?</h4>
<div class="ltx_para" id="S6.SS4.SSSx1.p1">
<p class="ltx_p" id="S6.SS4.SSSx1.p1.1">This study investigated a single high-stakes task that met the two aforementioned conditions: 1) either recall or precision is far more important than the other, and 2) the complementary strengths of human and AI can be identified and the precision-recall tradeoff can be exploited to improve the important metric for the given task. We proposed and observed that delegating more recall effort to the zealous AI can significantly improve team performance, which was mainly motivated by our observation that <span class="ltx_text ltx_font_bold ltx_font_italic" id="S6.SS4.SSSx1.p1.1.1">reject</span> is much easier than <span class="ltx_text ltx_font_bold ltx_font_italic" id="S6.SS4.SSSx1.p1.1.2">solve</span> for humans in AI-assisted annotation. Will our findings still hold if <span class="ltx_text ltx_font_bold ltx_font_italic" id="S6.SS4.SSSx1.p1.1.3">reject</span> is easier than <span class="ltx_text ltx_font_bold ltx_font_italic" id="S6.SS4.SSSx1.p1.1.4">solve</span> in a different task? What about precision-demanding tasks? We would love to see more HCI and AI researchers conduct latitudinal studies in multiple recall- or precision-demanding tasks to test and refine our findings.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS4.SSSx2">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Tasks without high-performance models.</h4>
<div class="ltx_para" id="S6.SS4.SSSx2.p1">
<p class="ltx_p" id="S6.SS4.SSSx2.p1.1">Face detection is a well-studied problem with high-performance AI models. While we showed in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S5.F5" title="Figure 5 ‣ 5. Results ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">5</span></a> that the AI and human can reach similar performance in this task to achieve complementary team performance, will our findings stand if either the human’s performance or the AI’s recommendations are much worse than the other? What is the lower bound F1 score limit for either the human or the AI to maintain complementary team performance? What are the F1 or precision/recall conditions for other researchers to reproduce our findings?</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS4.SSSx3">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Limitation from data and participants.</h4>
<div class="ltx_para" id="S6.SS4.SSSx3.p1">
<p class="ltx_p" id="S6.SS4.SSSx3.p1.1">We used a subset of realistic, egocentric video dataset <cite class="ltx_cite ltx_citemacro_citep">(Grauman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#bib.bib24" title="">2022</a>)</cite> in this study to measure with the skill of locating faces – a human instinct that comes with relatively small inter-personal differences. However, could our findings still play a major role if the task was to identify and track other objects that could have larger inter-personal differences? Furthermore, working with amateurs via crowdsourcing platforms would introduce larger variances between individuals than with the professional workers employed in this study. Researchers would need to put more effort into benchmarking or measuring the human factor in such follow-up studies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS4.SSSx4">
<h4 class="ltx_title ltx_font_bold ltx_title_subsubsection">Incentives for users to actively perform better.</h4>
<div class="ltx_para" id="S6.SS4.SSSx4.p1">
<p class="ltx_p" id="S6.SS4.SSSx4.p1.1">We discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.11860v1#S6.SS3" title="6.3. Seemingly contradictory survey results ‣ 6. Discussion ‣ Comparing Zealous and Restrained AI Recommendations in a Real-World Human-AI Collaboration Task"><span class="ltx_text ltx_ref_tag">6.3</span></a> observations that methods with better performances are not necessarily favored by the users. I.e., the users were involuntarily pushed to have higher performance by their AI teammates. From a system designer’s perspective, the AI teammate should help users to voluntarily perform better given the right incentives.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work, we look beyond the accuracy of AI recommendations to explore a new direction to improve human-AI team performance – the tradeoff between precision and recall in model tuning. We propose the concept of restrained and zealous AIs for high-precision and high-recall recommendations and conduct an experiment with 78 professional annotators to compare if and how the different AI recommendations can affect team performance in high-stakes human-AI collaboration. This work serves as a new example of complementary team performance in a large-scale realistic setting.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">An in-depth analysis of the task helped us identify an optimization opportunity to harness complementary human and AI strengths utilizing the tradeoff between precision and recall in the AI model tuning – given the importance of recall in face anonymization and the higher cost for humans to improve the recall in video annotation. We showed that the proposed high-recall zealous AI helps annotators achieve significantly better performance than the high-precision restrained AI in the video annotation task. Our follow-up study removed AI assistance and observed potentially negative training effects to the users – if an AI is naively paired with humans without optimizing it for the task at hand or for the human-AI workflow. We feel these findings have important implications for the design of AI assistance in recall-demanding scenarios. We hope this work can also inspire researchers to look for additional directions in model tuning to improve human-AI team performance.</p>
</div>
<div class="ltx_acknowledgements">
<h6 class="ltx_title ltx_title_acknowledgements">Acknowledgements.</h6>
This work was partially done during the first author’s research internship at Appen. We thank all anonymous reviewers for their insightful comments and suggestions. We thank Huan Liu for her support and hand-drawn figures, Yue He and Yuedong Wang for their time and discussion, and members of the UCSB Four Eyes and Expressive Computation Laboratories for their helpful feedback. This work was partially supported by ONR awards N00014-19-1-2553 and N00014-23-1-2118.

</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Amershi et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Saleema Amershi, Dan
Weld, Mihaela Vorvoreanu, Adam Fourney,
Besmira Nushi, Penny Collisson,
Jina Suh, Shamsi Iqbal,
Paul N. Bennett, Kori Inkpen,
Jaime Teevan, Ruth Kikin-Gil, and
Eric Horvitz. 2019.

</span>
<span class="ltx_bibblock">Guidelines for Human-AI Interaction. In
<em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 2019 CHI Conference on Human
Factors in Computing Systems</em> (Glasgow, Scotland Uk)
<em class="ltx_emph ltx_font_italic" id="bib.bib2.4.2">(CHI ’19)</em>. Association for
Computing Machinery, New York, NY, USA,
1–13.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3290605.3300233" title="">https://doi.org/10.1145/3290605.3300233</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Gagan Bansal, Besmira
Nushi, Ece Kamar, Eric Horvitz, and
Daniel S. Weld. 2021a.

</span>
<span class="ltx_bibblock">Is the Most Accurate AI the Best Teammate?
Optimizing AI for Teamwork.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the AAAI Conference on
Artificial Intelligence</em> 35, 13
(May 2021), 11405–11414.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/17359" title="">https://ojs.aaai.org/index.php/AAAI/article/view/17359</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Gagan Bansal, Besmira
Nushi, Ece Kamar, Walter S. Lasecki,
Daniel S. Weld, and Eric Horvitz.
2019a.

</span>
<span class="ltx_bibblock">Beyond Accuracy: The Role of Mental Models in
Human-AI Team Performance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the AAAI Conference on Human
Computation and Crowdsourcing</em> 7, 1
(Oct. 2019), 2–11.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/HCOMP/article/view/5285" title="">https://ojs.aaai.org/index.php/HCOMP/article/view/5285</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Gagan Bansal, Besmira
Nushi, Ece Kamar, Daniel S. Weld,
Walter S. Lasecki, and Eric Horvitz.
2019b.

</span>
<span class="ltx_bibblock">Updates in Human-AI Teams: Understanding and
Addressing the Performance/Compatibility Tradeoff.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the AAAI Conference on
Artificial Intelligence</em> 33, 01
(Jul. 2019), 2429–2437.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1609/aaai.v33i01.33012429" title="">https://doi.org/10.1609/aaai.v33i01.33012429</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bansal et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Gagan Bansal, Tongshuang
Wu, Joyce Zhou, Raymond Fok,
Besmira Nushi, Ece Kamar,
Marco Tulio Ribeiro, and Daniel Weld.
2021b.

</span>
<span class="ltx_bibblock">Does the Whole Exceed Its Parts? The Effect of AI
Explanations on Complementary Team Performance. In
<em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems</em> (Yokohama, Japan) <em class="ltx_emph ltx_font_italic" id="bib.bib6.4.2">(CHI
’21)</em>. Association for Computing Machinery,
New York, NY, USA, Article 81,
16 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3411764.3445717" title="">https://doi.org/10.1145/3411764.3445717</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bilgic and Mooney (2005)</span>
<span class="ltx_bibblock">
Mustafa Bilgic and
Raymond J Mooney. 2005.

</span>
<span class="ltx_bibblock">Explaining recommendations: Satisfaction vs.
promotion. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Beyond personalization workshop,
IUI</em>, Vol. 5. 153.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bussone et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Adrian Bussone, Simone
Stumpf, and Dympna O’Sullivan.
2015.

</span>
<span class="ltx_bibblock">The Role of Explanations on Trust and Reliance in
Clinical Decision Support Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">2015
International Conference on Healthcare Informatics</em>.
160–169.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICHI.2015.26" title="">https://doi.org/10.1109/ICHI.2015.26</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carion et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nicolas Carion, Francisco
Massa, Gabriel Synnaeve, Nicolas
Usunier, Alexander Kirillov, and Sergey
Zagoruyko. 2020.

</span>
<span class="ltx_bibblock">End-to-End Object Detection with Transformers. In
<em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Computer Vision – ECCV 2020</em>,
Andrea Vedaldi, Horst
Bischof, Thomas Brox, and Jan-Michael
Frahm (Eds.). Springer International Publishing,
Cham, 213–229.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caruana et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Rich Caruana, Yin Lou,
Johannes Gehrke, Paul Koch,
Marc Sturm, and Noemie Elhadad.
2015.

</span>
<span class="ltx_bibblock">Intelligible Models for HealthCare: Predicting
Pneumonia Risk and Hospital 30-Day Readmission. In
<em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 21th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining</em> (Sydney, NSW, Australia)
<em class="ltx_emph ltx_font_italic" id="bib.bib10.4.2">(KDD ’15)</em>. Association for
Computing Machinery, New York, NY, USA,
1721–1730.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2783258.2788613" title="">https://doi.org/10.1145/2783258.2788613</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chakraborti and Kambhampati (2018)</span>
<span class="ltx_bibblock">
Tathagata Chakraborti and
Subbarao Kambhampati. 2018.

</span>
<span class="ltx_bibblock">Algorithms for the Greater Good! On Mental Modeling
and Acceptable Symbiosis in Human-AI Collaboration.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1801.09854 [cs.AI]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yimin Chen, Xinjie Zhang,
and Junmin Wang. 2021.

</span>
<span class="ltx_bibblock">Robust Vehicle Driver Assistance Control for
Handover Scenarios Considering Driving Performances.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">IEEE Transactions on Systems, Man, and
Cybernetics: Systems</em> 51, 7
(2021), 4160–4170.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/TSMC.2019.2931484" title="">https://doi.org/10.1109/TSMC.2019.2931484</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Peng Chu, Jiang Wang,
Quanzeng You, Haibin Ling, and
Zicheng Liu. 2021.

</span>
<span class="ltx_bibblock">TransMOT: Spatial-Temporal Graph Transformer for
Multiple Object Tracking.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2104.00194 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Colley et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mark Colley, Benjamin
Eder, Jan Ole Rixen, and Enrico
Rukzio. 2021.

</span>
<span class="ltx_bibblock">Effects of Semantic Segmentation Visualization on
Trust, Situation Awareness, and Cognitive Load in Highly Automated Vehicles.
In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems</em> (Yokohama, Japan) <em class="ltx_emph ltx_font_italic" id="bib.bib14.4.2">(CHI
’21)</em>. Association for Computing Machinery,
New York, NY, USA, Article 155,
11 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3411764.3445351" title="">https://doi.org/10.1145/3411764.3445351</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Csurka et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Gabriela Csurka, Diane
Larlus, and Florent Perronnin.
2013.

</span>
<span class="ltx_bibblock">What is a good evaluation measure for semantic
segmentation?. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Procedings of the British
Machine Vision Conference 2013</em>. British Machine
Vision Association, Bristol,
32.1–32.11.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.5244/C.27.32" title="">https://doi.org/10.5244/C.27.32</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dave et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Achal Dave, Tarasha
Khurana, Pavel Tokmakov, Cordelia
Schmid, and Deva Ramanan.
2020.

</span>
<span class="ltx_bibblock">TAO: A Large-Scale Benchmark for Tracking Any
Object. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">European Conference on Computer
Vision</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.10356" title="">https://arxiv.org/abs/2005.10356</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davis and Goadrich (2006)</span>
<span class="ltx_bibblock">
Jesse Davis and Mark
Goadrich. 2006.

</span>
<span class="ltx_bibblock">The relationship between Precision-Recall and
ROC curves. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 23rd
international conference on Machine learning</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib17.2.2">(ICML ’06)</em>. Association for
Computing Machinery, New York, NY, USA,
233–240.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1143844.1143874" title="">https://doi.org/10.1145/1143844.1143874</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong,
Richard Socher, Li-Jia Li,
Kai Li, and Li Fei-Fei.
2009.

</span>
<span class="ltx_bibblock">ImageNet: A large-scale hierarchical image
database. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">2009 IEEE Conference on
Computer Vision and Pattern Recognition</em>. IEEE,
Miami, FL, 248–255.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/CVPR.2009.5206848" title="">https://doi.org/10.1109/CVPR.2009.5206848</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Jiankang Deng, Jia Guo,
Yuxiang Zhou, Jinke Yu,
Irene Kotsia, and Stefanos Zafeiriou.
2019.

</span>
<span class="ltx_bibblock">RetinaFace: Single-stage Dense Face Localisation in
the Wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">CoRR</em> abs/1905.00641
(2019).

</span>
<span class="ltx_bibblock">arXiv:1905.00641

<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1905.00641" title="">http://arxiv.org/abs/1905.00641</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dutta and Zisserman (2019)</span>
<span class="ltx_bibblock">
Abhishek Dutta and
Andrew Zisserman. 2019.

</span>
<span class="ltx_bibblock">The VIA Annotation Software for Images, Audio and
Video. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 27th ACM
International Conference on Multimedia</em> (Nice, France)
<em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">(MM ’19)</em>. Association for
Computing Machinery, New York, NY, USA,
2276–2279.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3343031.3350535" title="">https://doi.org/10.1145/3343031.3350535</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Erenel and Altınçay (2013)</span>
<span class="ltx_bibblock">
Zafer Erenel and Hakan
Altınçay. 2013.

</span>
<span class="ltx_bibblock">Improving the precision-recall trade-off in
undersampling-based binary text categorization using unanimity rule.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Neural Computing and Applications</em>
22, 1 (May
2013), 83–100.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s00521-012-1056-5" title="">https://doi.org/10.1007/s00521-012-1056-5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Boyd-Graber (2019)</span>
<span class="ltx_bibblock">
Shi Feng and Jordan
Boyd-Graber. 2019.

</span>
<span class="ltx_bibblock">What Can AI Do for Me? Evaluating Machine Learning
Interpretations in Cooperative Play. In
<em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 24th International Conference on
Intelligent User Interfaces</em> (Marina del Ray, California)
<em class="ltx_emph ltx_font_italic" id="bib.bib22.2.2">(IUI ’19)</em>. Association for
Computing Machinery, New York, NY, USA,
229–239.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3301275.3302265" title="">https://doi.org/10.1145/3301275.3302265</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gopinath et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Deepak Gopinath, Jonathan
DeCastro, Guy Rosman, Emily Sumner,
Allison Morgan, Shabnam Hakimi, and
Simon Stent. 2022.

</span>
<span class="ltx_bibblock">HMIway-Env: A Framework for Simulating Behaviors
and Preferences To Support Human-AI Teaming in Driving. In
<em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR) Workshops</em>.
4342–4350.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grauman et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Kristen Grauman, Andrew
Westbury, Eugene Byrne, Zachary Chavis,
Antonino Furnari, Rohit Girdhar,
Jackson Hamburger, Hao Jiang,
Miao Liu, Xingyu Liu,
Miguel Martin, Tushar Nagarajan,
Ilija Radosavovic, Santhosh Kumar
Ramakrishnan, Fiona Ryan, Jayant Sharma,
Michael Wray, Mengmeng Xu,
Eric Zhongcong Xu, Chen Zhao,
Siddhant Bansal, Dhruv Batra,
Vincent Cartillier, Sean Crane,
Tien Do, Morrie Doulaty,
Akshay Erapalli, Christoph Feichtenhofer,
Adriano Fragomeni, Qichen Fu,
Christian Fuegen, Abrham Gebreselasie,
Cristina Gonzalez, James Hillis,
Xuhua Huang, Yifei Huang,
Wenqi Jia, Weslie Khoo,
Jachym Kolar, Satwik Kottur,
Anurag Kumar, Federico Landini,
Chao Li, Yanghao Li,
Zhenqiang Li, Karttikeya Mangalam,
Raghava Modhugu, Jonathan Munro,
Tullie Murrell, Takumi Nishiyasu,
Will Price, Paola Ruiz Puentes,
Merey Ramazanova, Leda Sari,
Kiran Somasundaram, Audrey Southerland,
Yusuke Sugano, Ruijie Tao,
Minh Vo, Yuchen Wang,
Xindi Wu, Takuma Yagi,
Yunyi Zhu, Pablo Arbelaez,
David Crandall, Dima Damen,
Giovanni Maria Farinella, Bernard Ghanem,
Vamsi Krishna Ithapu, C. V. Jawahar,
Hanbyul Joo, Kris Kitani,
Haizhou Li, Richard Newcombe,
Aude Oliva, Hyun Soo Park,
James M. Rehg, Yoichi Sato,
Jianbo Shi, Mike Zheng Shou,
Antonio Torralba, Lorenzo Torresani,
Mingfei Yan, and Jitendra Malik.
2022.

</span>
<span class="ltx_bibblock">Ego4D: Around the World in 3,000 Hours of
Egocentric Video. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">IEEE/CVF Computer Vision
and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Juran et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2005)</span>
<span class="ltx_bibblock">
Joseph Juran, Frederick
Taylor, Walter Shewhart, Edward Deming,
Philip Crosby, Kaoru Ishikawa,
Armand Feigenbaum, Genichi Taguchi, and
Elihu Goldratt. 2005.

</span>
<span class="ltx_bibblock">Quality control.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Joseph M. Juran: Critical Evaluations in
Business and Management</em> 1 (2005),
50.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kamar et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2012)</span>
<span class="ltx_bibblock">
Ece Kamar, Severin
Hacker, and Eric Horvitz.
2012.

</span>
<span class="ltx_bibblock">Combining Human and Machine Intelligence in
Large-Scale Crowdsourcing. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the
11th International Conference on Autonomous Agents and Multiagent Systems -
Volume 1</em> (Valencia, Spain) <em class="ltx_emph ltx_font_italic" id="bib.bib26.4.2">(AAMAS ’12)</em>.
International Foundation for Autonomous Agents and
Multiagent Systems, Richland, SC,
467–474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaur et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Harmanpreet Kaur, Harsha
Nori, Samuel Jenkins, Rich Caruana,
Hanna Wallach, and Jennifer
Wortman Vaughan. 2020.

</span>
<span class="ltx_bibblock">Interpreting Interpretability: Understanding Data
Scientists’ Use of Interpretability Tools for Machine Learning. In
<em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Proceedings of the 2020 CHI Conference on Human
Factors in Computing Systems</em> (Honolulu, HI, USA)
<em class="ltx_emph ltx_font_italic" id="bib.bib27.4.2">(CHI ’20)</em>. Association for
Computing Machinery, New York, NY, USA,
1–14.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3313831.3376219" title="">https://doi.org/10.1145/3313831.3376219</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kavasidis et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Isaak Kavasidis, Simone
Palazzo, Roberto Di Salvo, Daniela
Giordano, and Concetto Spampinato.
2014.

</span>
<span class="ltx_bibblock">An innovative web-based collaborative platform for
video annotation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">Multim. Tools Appl.</em> 70,
1 (2014), 413–432.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11042-013-1419-7" title="">https://doi.org/10.1007/s11042-013-1419-7</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kay et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Matthew Kay, Shwetak N.
Patel, and Julie A. Kientz.
2015.

</span>
<span class="ltx_bibblock">How Good is 85%? A Survey Tool to Connect
Classifier Evaluation to Acceptability of Accuracy. In
<em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 33rd Annual ACM Conference on
Human Factors in Computing Systems</em> (Seoul, Republic of Korea)
<em class="ltx_emph ltx_font_italic" id="bib.bib29.4.2">(CHI ’15)</em>. Association for
Computing Machinery, New York, NY, USA,
347–356.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2702123.2702603" title="">https://doi.org/10.1145/2702123.2702603</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocielnik et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Rafal Kocielnik, Saleema
Amershi, and Paul N. Bennett.
2019.

</span>
<span class="ltx_bibblock">Will You Accept an Imperfect AI? Exploring Designs
for Adjusting End-User Expectations of AI Systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 2019 CHI Conference on Human
Factors in Computing Systems</em> (Glasgow, Scotland Uk)
<em class="ltx_emph ltx_font_italic" id="bib.bib30.4.2">(CHI ’19)</em>. Association for
Computing Machinery, New York, NY, USA,
1–14.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3290605.3300641" title="">https://doi.org/10.1145/3290605.3300641</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kunkel et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Johannes Kunkel, Tim
Donkers, Lisa Michael, Catalin-Mihai
Barbu, and Jürgen Ziegler.
2019.

</span>
<span class="ltx_bibblock">Let Me Explain: Impact of Personal and Impersonal
Explanations on Trust in Recommender Systems. In
<em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the 2019 CHI Conference on Human
Factors in Computing Systems</em> (Glasgow, Scotland Uk)
<em class="ltx_emph ltx_font_italic" id="bib.bib31.4.2">(CHI ’19)</em>. Association for
Computing Machinery, New York, NY, USA,
1–12.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3290605.3300717" title="">https://doi.org/10.1145/3290605.3300717</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Vivian Lai, Samuel
Carton, Rajat Bhatnagar, Q. Vera Liao,
Yunfeng Zhang, and Chenhao Tan.
2022.

</span>
<span class="ltx_bibblock">Human-AI Collaboration via Conditional Delegation:
A Case Study of Content Moderation. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Proceedings
of the 2022 CHI Conference on Human Factors in Computing Systems</em> (New
Orleans, LA, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib32.4.2">(CHI ’22)</em>.
Association for Computing Machinery,
New York, NY, USA, Article 54,
18 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3491102.3501999" title="">https://doi.org/10.1145/3491102.3501999</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leys et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Christophe Leys,
Christophe Ley, Olivier Klein,
Philippe Bernard, and Laurent Licata.
2013.

</span>
<span class="ltx_bibblock">Detecting outliers: Do not use standard deviation
around the mean, use absolute deviation around the median.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Journal of Experimental Social Psychology</em>
49, 4 (2013),
764–766.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.jesp.2013.03.013" title="">https://doi.org/10.1016/j.jesp.2013.03.013</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael
Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan,
Piotr Dollár, and C. Lawrence
Zitnick. 2014.

</span>
<span class="ltx_bibblock">Microsoft COCO: Common Objects in Context.
In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Computer Vision – ECCV 2014</em>
<em class="ltx_emph ltx_font_italic" id="bib.bib34.4.2">(Lecture Notes in Computer Science)</em>,
David Fleet, Tomas
Pajdla, Bernt Schiele, and Tinne
Tuytelaars (Eds.). Springer International Publishing,
Cham, 740–755.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-319-10602-1_48" title="">https://doi.org/10.1007/978-3-319-10602-1_48</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lipton et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Zachary C. Lipton, Charles
Elkan, and Balakrishnan Naryanaswamy.
2014.

</span>
<span class="ltx_bibblock">Optimal Thresholding of Classifiers to
Maximize F1 Measure. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Machine Learning
and Knowledge Discovery in Databases</em> <em class="ltx_emph ltx_font_italic" id="bib.bib35.4.2">(Lecture
Notes in Computer Science)</em>, Toon
Calders, Floriana Esposito, Eyke
Hüllermeier, and Rosa Meo (Eds.).
Springer, Berlin, Heidelberg,
225–239.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/978-3-662-44851-9_15" title="">https://doi.org/10.1007/978-3-662-44851-9_15</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu and Yin (2021)</span>
<span class="ltx_bibblock">
Zhuoran Lu and Ming
Yin. 2021.

</span>
<span class="ltx_bibblock">Human Reliance on Machine Learning Models When
Performance Feedback is Limited: Heuristics and Risks. In
<em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems</em> (Yokohama, Japan) <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">(CHI
’21)</em>. Association for Computing Machinery,
New York, NY, USA, Article 78,
16 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3411764.3445562" title="">https://doi.org/10.1145/3411764.3445562</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lundberg and Lee (2017)</span>
<span class="ltx_bibblock">
Scott M Lundberg and
Su-In Lee. 2017.

</span>
<span class="ltx_bibblock">A Unified Approach to Interpreting Model
Predictions. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Advances in Neural Information
Processing Systems</em>, I. Guyon,
U. Von Luxburg, S. Bengio,
H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett
(Eds.), Vol. 30. Curran Associates,
Inc.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf" title="">https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lundgard et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Alan Lundgard, Yiwei
Yang, Maya L. Foster, and Walter S.
Lasecki. 2018.

</span>
<span class="ltx_bibblock">Bolt: Instantaneous Crowdsourcing via Just-in-Time
Training. In <em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Proceedings of the 2018 CHI
Conference on Human Factors in Computing Systems</em> (Montreal QC, Canada)
<em class="ltx_emph ltx_font_italic" id="bib.bib38.4.2">(CHI ’18)</em>. Association for
Computing Machinery, New York, NY, USA,
1–7.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3173574.3174041" title="">https://doi.org/10.1145/3173574.3174041</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meinhardt et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Tim Meinhardt, Alexander
Kirillov, Laura Leal-Taixe, and
Christoph Feichtenhofer. 2022.

</span>
<span class="ltx_bibblock">TrackFormer: Multi-Object Tracking with
Transformers. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milan et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
A. Milan, L.
Leal-Taixé, I. Reid, S. Roth, and
K. Schindler. 2016.

</span>
<span class="ltx_bibblock">MOT16: A Benchmark for Multi-Object Tracking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv:1603.00831 [cs]</em>
(March 2016).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1603.00831" title="">http://arxiv.org/abs/1603.00831</a>
</span>
<span class="ltx_bibblock">arXiv: 1603.00831.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morstatter et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Fred Morstatter, Liang
Wu, Tahora H. Nazer, Kathleen M. Carley,
and Huan Liu. 2016.

</span>
<span class="ltx_bibblock">A new approach to bot detection: Striking the
balance between precision and recall. In <em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">2016
IEEE/ACM International Conference on Advances in Social Networks Analysis and
Mining (ASONAM)</em>. 533–540.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ASONAM.2016.7752287" title="">https://doi.org/10.1109/ASONAM.2016.7752287</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muir (1987)</span>
<span class="ltx_bibblock">
Bonnie M. Muir.
1987.

</span>
<span class="ltx_bibblock">Trust between humans and machines, and the design
of decision aids.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">International Journal of Man-Machine
Studies</em> 27, 5 (1987),
527–539.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/S0020-7373(87)80013-5" title="">https://doi.org/10.1016/S0020-7373(87)80013-5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagar and Malone (2011)</span>
<span class="ltx_bibblock">
Yiftach Nagar and
Thomas W Malone. 2011.

</span>
<span class="ltx_bibblock">Making business predictions by combining human and
machine intelligence in prediction markets. In
<em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">International Conference on Information Systems</em>.
Association for Information Systems.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Neubeck and Van Gool (2006)</span>
<span class="ltx_bibblock">
A. Neubeck and L.
Van Gool. 2006.

</span>
<span class="ltx_bibblock">Efficient Non-Maximum Suppression. In
<em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">18th International Conference on Pattern
Recognition (ICPR’06)</em>, Vol. 3. 850–855.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICPR.2006.479" title="">https://doi.org/10.1109/ICPR.2006.479</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Bhavik N Patel, Louis
Rosenberg, Gregg Willcox, David Baltaxe,
Mimi Lyons, Jeremy Irvin,
Pranav Rajpurkar, Timothy Amrhein,
Rajan Gupta, Safwan Halabi,
et al<span class="ltx_text" id="bib.bib45.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Human–machine partnership with artificial
intelligence for chest radiograph diagnosis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.4.1">NPJ digital medicine</em> 2,
1 (2019), 1–10.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rastogi et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Charvi Rastogi, Yunfeng
Zhang, Dennis Wei, Kush R. Varshney,
Amit Dhurandhar, and Richard Tomsett.
2022.

</span>
<span class="ltx_bibblock">Deciding Fast and Slow: The Role of Cognitive
Biases in AI-Assisted Decision-Making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Proc. ACM Hum.-Comput. Interact.</em>
6, CSCW1, Article 83
(apr 2022), 22 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3512930" title="">https://doi.org/10.1145/3512930</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ribeiro et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Marco Tulio Ribeiro,
Sameer Singh, and Carlos Guestrin.
2016.

</span>
<span class="ltx_bibblock">”Why Should I Trust You?”: Explaining the
Predictions of Any Classifier. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings of
the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining</em> (San Francisco, California, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib47.4.2">(KDD ’16)</em>.
Association for Computing Machinery,
New York, NY, USA, 1135–1144.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2939672.2939778" title="">https://doi.org/10.1145/2939672.2939778</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sasaki et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Yutaka Sasaki et al<span class="ltx_text" id="bib.bib48.3.1">.</span>
2007.

</span>
<span class="ltx_bibblock">The truth of the F-measure.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.4.1">Teach tutor mater</em> 1,
5 (2007), 1–5.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Peize Sun, Jinkun Cao,
Yi Jiang, Rufeng Zhang,
Enze Xie, Zehuan Yuan,
Changhu Wang, and Ping Luo.
2020.

</span>
<span class="ltx_bibblock">TransTrack: Multiple Object Tracking with
Transformer.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2012.15460 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Berkel et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Niels Van Berkel, Jeremy
Opie, Omer F. Ahmad, Laurence Lovat,
Danail Stoyanov, and Ann Blandford.
2022.

</span>
<span class="ltx_bibblock">Initial Responses to False Positives in
AI-Supported Continuous Interactions: A Colonoscopy Case Study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">ACM Trans. Interact. Intell. Syst.</em>
12, 1, Article 2
(mar 2022), 18 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3480247" title="">https://doi.org/10.1145/3480247</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam
Shazeer, Niki Parmar, Jakob Uszkoreit,
Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin.
2017.

</span>
<span class="ltx_bibblock">Attention is All you Need. In
<em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">Advances in Neural Information Processing
Systems</em>, I. Guyon,
U. Von Luxburg, S. Bengio,
H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett
(Eds.), Vol. 30. Curran Associates,
Inc.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Voigtlaender et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Paul Voigtlaender, Michael
Krause, Aljosa Osep, Jonathon Luiten,
Berin Balachandar Gnana Sekar, Andreas
Geiger, and Bastian Leibe.
2019.

</span>
<span class="ltx_bibblock">MOTS: Multi-Object Tracking and Segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">arXiv:1902.03604[cs]</em>
(2019).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1902.03604" title="">http://arxiv.org/abs/1902.03604</a>
</span>
<span class="ltx_bibblock">arXiv: 1902.03604.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vondrick et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Carl Vondrick, Donald J.
Patterson, and Deva Ramanan.
2013.

</span>
<span class="ltx_bibblock">Efficiently Scaling up Crowdsourced Video
Annotation - A Set of Best Practices for High Quality, Economical Video
Labeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Int. J. Comput. Vis.</em> 101,
1 (2013), 184–204.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s11263-012-0564-1" title="">https://doi.org/10.1007/s11263-012-0564-1</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Dayong Wang, Aditya
Khosla, Rishab Gargeya, Humayun Irshad,
and Andrew H. Beck. 2016.

</span>
<span class="ltx_bibblock">Deep Learning for Identifying Metastatic Breast
Cancer.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1606.05718 [q-bio.QM]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Danding Wang, Qian Yang,
Ashraf Abdul, and Brian Y. Lim.
2019.

</span>
<span class="ltx_bibblock">Designing Theory-Driven User-Centric Explainable
AI. In <em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">Proceedings of the 2019 CHI Conference on
Human Factors in Computing Systems</em> (Glasgow, Scotland Uk)
<em class="ltx_emph ltx_font_italic" id="bib.bib55.4.2">(CHI ’19)</em>. Association for
Computing Machinery, New York, NY, USA,
1–15.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3290605.3300831" title="">https://doi.org/10.1145/3290605.3300831</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilder et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Bryan Wilder, Eric
Horvitz, and Ece Kamar.
2020.

</span>
<span class="ltx_bibblock">Learning to Complement Humans.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">Proceedings of the Twenty-Ninth International
Joint Conference on Artificial Intelligence</em> (Jul
2020).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.24963/ijcai.2020/212" title="">https://doi.org/10.24963/ijcai.2020/212</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yao Xie, Melody Chen,
David Kao, Ge Gao, and
Xiang ’Anthony’ Chen. 2020.

</span>
<span class="ltx_bibblock">CheXplain: Enabling Physicians to Explore and
Understand Data-Driven, AI-Enabled Medical Imaging Analysis. In
<em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Proceedings of the 2020 CHI Conference on Human
Factors in Computing Systems</em> (Honolulu, HI, USA)
<em class="ltx_emph ltx_font_italic" id="bib.bib57.4.2">(CHI ’20)</em>. Association for
Computing Machinery, New York, NY, USA,
1–13.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3313831.3376807" title="">https://doi.org/10.1145/3313831.3376807</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Shuo Yang, Ping Luo,
Chen Change Loy, and Xiaoou Tang.
2016.

</span>
<span class="ltx_bibblock">WIDER FACE: A Face Detection Benchmark. In
<em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ming Yin, Jennifer
Wortman Vaughan, and Hanna Wallach.
2019.

</span>
<span class="ltx_bibblock">Understanding the Effect of Accuracy on Trust in
Machine Learning Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Proceedings of the
2019 CHI Conference on Human Factors in Computing Systems</em> (Glasgow,
Scotland Uk) <em class="ltx_emph ltx_font_italic" id="bib.bib59.4.2">(CHI ’19)</em>.
Association for Computing Machinery,
New York, NY, USA, 1–12.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3290605.3300509" title="">https://doi.org/10.1145/3290605.3300509</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Kun Yu, Shlomo Berkovsky,
Ronnie Taib, Dan Conway,
Jianlong Zhou, and Fang Chen.
2017.

</span>
<span class="ltx_bibblock">User Trust Dynamics: An Investigation Driven by
Differences in System Performance. In <em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Proceedings
of the 22nd International Conference on Intelligent User Interfaces</em>
(Limassol, Cyprus) <em class="ltx_emph ltx_font_italic" id="bib.bib60.4.2">(IUI ’17)</em>.
Association for Computing Machinery,
New York, NY, USA, 307–317.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3025171.3025219" title="">https://doi.org/10.1145/3025171.3025219</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuen et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Jenny Yuen, Bryan
Russell, Ce Liu, and Antonio
Torralba. 2009.

</span>
<span class="ltx_bibblock">LabelMe video: Building a video database with human
annotations. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">2009 IEEE 12th International
Conference on Computer Vision</em>. 1451–1458.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/ICCV.2009.5459289" title="">https://doi.org/10.1109/ICCV.2009.5459289</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Qiaoning Zhang, Matthew L
Lee, and Scott Carter.
2022a.

</span>
<span class="ltx_bibblock">You Complete Me: Human-AI Teams and Complementary
Expertise. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the 2022 CHI
Conference on Human Factors in Computing Systems</em> (New Orleans, LA, USA)
<em class="ltx_emph ltx_font_italic" id="bib.bib62.4.2">(CHI ’22)</em>. Association for
Computing Machinery, New York, NY, USA, Article
114, 28 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3491102.3517791" title="">https://doi.org/10.1145/3491102.3517791</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Rui Zhang, Nathan J.
McNeese, Guo Freeman, and Geoff
Musick. 2021.

</span>
<span class="ltx_bibblock">”An Ideal Human”: Expectations of AI Teammates in
Human-AI Teaming.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Proc. ACM Hum.-Comput. Interact.</em>
4, CSCW3, Article
246 (jan 2021),
25 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3432945" title="">https://doi.org/10.1145/3432945</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yunfeng Zhang, Q. Vera
Liao, and Rachel K. E. Bellamy.
2020.

</span>
<span class="ltx_bibblock">Effect of Confidence and Explanation on Accuracy
and Trust Calibration in AI-Assisted Decision Making. In
<em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">Proceedings of the 2020 Conference on Fairness,
Accountability, and Transparency</em> (Barcelona, Spain)
<em class="ltx_emph ltx_font_italic" id="bib.bib64.4.2">(FAT* ’20)</em>. Association for
Computing Machinery, New York, NY, USA,
295–305.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3351095.3372852" title="">https://doi.org/10.1145/3351095.3372852</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Yifu Zhang, Peize Sun,
Yi Jiang, Dongdong Yu,
Fucheng Weng, Zehuan Yuan,
Ping Luo, Wenyu Liu, and
Xinggang Wang. 2022b.

</span>
<span class="ltx_bibblock">ByteTrack: Multi-Object Tracking by Associating
Every Detection Box.

</span>
<span class="ltx_bibblock">(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xizhou Zhu, Weijie Su,
Lewei Lu, Bin Li,
Xiaogang Wang, and Jifeng Dai.
2020.

</span>
<span class="ltx_bibblock">Deformable DETR: Deformable Transformers for
End-to-End Object Detection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">arXiv preprint arXiv:2010.04159</em>
(2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sun Oct  6 23:00:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
