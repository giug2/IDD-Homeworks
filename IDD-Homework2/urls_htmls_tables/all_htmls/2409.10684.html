<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program.</title>
<!--Generated on Wed Sep 25 17:00:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Music Generation,  Text-To-Music,  Audio Forensics,  DeepFake
" lang="en" name="keywords"/>
<base href="/html/2409.10684v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S1" title="In FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S2" title="In FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Problem Formulation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S3" title="In FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">FakeMusicCaps Dataset</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S3.SS1" title="In III FakeMusicCaps Dataset â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Considered Architectures</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S3.SS2" title="In III FakeMusicCaps Dataset â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Generation strategy</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S4" title="In FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S4.SS1" title="In IV Experimental Analysis â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Dataset</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S4.SS2" title="In IV Experimental Analysis â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Baselines</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S4.SS3" title="In IV Experimental Analysis â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S4.SS4" title="In IV Experimental Analysis â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Classification Techniques</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5" title="In FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Results</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.SS1" title="In V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Closed-Set Performances</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.SS2" title="In V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Open Set Performances</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.SS3" title="In V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Impact of window size</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.SS4" title="In V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-D</span> </span><span class="ltx_text ltx_font_italic">Discussion</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S6" title="In FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models 
<br class="ltx_break"/><span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luca Comanducci
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id2.1.id1">Politecnico di Milano
<br class="ltx_break"/></span>Milan, Italy 
<br class="ltx_break"/>luca.comanducci@polimi.it
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paolo Bestagini
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">Politecnico di Milano
<br class="ltx_break"/></span>Milan, Italy 
<br class="ltx_break"/>paolo.bestagini@polimi.it
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stefano Tubaro
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id4.1.id1">Politecnico di Milano
<br class="ltx_break"/></span>Milan, Italy 
<br class="ltx_break"/>stefano.tubaro@polimi.it
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luca Comanducci, Paolo Bestagini, Stefano Tubaro
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano
<br class="ltx_break"/>Piazza Leonardo Da Vinci 32, 20133 Milan, Italy
<br class="ltx_break"/>Email: name.surname@polimi.it

</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Text-To-Music (TTM) models have recently revolutionized the automatic music generation research field. Specifically, by reaching superior performances to all previous state-of-the-art models and by lowering the technical proficiency needed to use them. Due to these reasons, they have readily started to be adopted for commercial uses and music production practices. This widespread diffusion of TTMs poses several concerns regarding copyright violation and rightful attribution, posing the need of serious consideration of them by the audio forensics community. In this paper, we tackle the problem of detection and attribution of TTM-generated data. We propose a dataset, FakeMusicCaps that contains several versions of the music-caption pairs dataset MusicCaps re-generated via several state-of-the-art TTM techniques. We evaluate the proposed dataset by performing initial experiments regarding the detection and attribution of TTM-generated audio.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Music Generation, Text-To-Music, Audio Forensics, DeepFake

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Deep learning-based music generationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib1" title="">1</a>]</cite> has been recently revolutionized by the introduction of Text-To-Music models.
TTM models are usually either based on a language model that decodes continuous or discrete tokenized embeddings obtained via some neural audio codecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib3" title="">3</a>]</cite>, such as MusicLMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib4" title="">4</a>]</cite>, MusicGENÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib5" title="">5</a>]</cite>, MAGNeTÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib6" title="">6</a>]</cite> and JASCOÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib7" title="">7</a>]</cite> or on latent diffusion models operating on some compressed form of audio, such as AudioLDMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib8" title="">8</a>]</cite>, AudioLDM2Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib9" title="">9</a>]</cite>, MusicLDMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib10" title="">10</a>]</cite>, Noise2MusicÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib11" title="">11</a>]</cite>, MustangoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">These models are characterized by being good in terms of performance and also simple to use, lowering the technical proficiency needed to successfully interact with themÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib13" title="">13</a>]</cite>. This combination of factors has made them extremely appetible to the general public and of interest by private industries.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In Fact several commercial TTMs have been proposed, such as SunoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib14" title="">14</a>]</cite> (reaching the record for the biggest investment ever in an AI music startup, namely $125 million) and UdioÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib15" title="">15</a>]</cite>. Recently, both these companies have been sued by major record companies and have consecutively admitted to copyright infringement, by training their respective models also using unlicensed music. As both the capabilities and the commercial interest of these models grow it is becoming increasingly necessary to try to develop forensic approaches to detect and analyze music generated via TTMsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib16" title="">16</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The multimedia forensics field is well mature in fields related to imageÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib21" title="">21</a>]</cite> and videoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib22" title="">22</a>]</cite> deepfake detection and model attribution, but in the audio domain has been almost exclusively applied to speech signalsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">In the music domain, most efforts have concentrated on Singing voice detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib28" title="">28</a>]</cite> with the development of specific challengesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib29" title="">29</a>]</cite> to tackle the problem. An approach to music deepfake detection has been proposed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib30" title="">30</a>]</cite>, where however no fake music is considered, instead the authors focus on detecting with which neural audio codec real music tracks are processed.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Research in this field is also limited by economic reasons since most models are developed by tech giants that often do not release the code and/or weights. Also, available paired text-music dataset except for MusiCapsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib4" title="">4</a>]</cite>, Song DescriberÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib31" title="">31</a>]</cite> and MusicBenchÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib12" title="">12</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">In this paper, we propose the <span class="ltx_text ltx_font_italic" id="S1.p7.1.1">FakeMusicCaps dataset</span>, with the objective of encouraging research in music deepfake detection. To build FakeMusicCaps, we replicate MusicCaps by using its captions as input to Five state-of-the-art TTM models, namely MusicGenÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib5" title="">5</a>]</cite>, MusicLDMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib10" title="">10</a>]</cite>, AudioLDM2Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib9" title="">9</a>]</cite>, Stable Audio OpenÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib32" title="">32</a>]</cite> and MustangoÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib12" title="">12</a>]</cite>. The nature of FakeMusicCaps makes it easy to incorporate future TTMs by simply generating music examples using the same procedure. We perform a benchmark study, on FakeMusicCaps, by studying if it is possible to perform detection and attribution, i.e. classifying the input music as either real or belonging to one of the chosen TTM models. We analyze how the model performs both in closed set or open set scenarios, where the latter also includes data belonging to generators not seen during training, which in this paper belong to the SunoCaps datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib33" title="">33</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">At the same time of this work, a similar dataset, named SONICS, has been proposedÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib34" title="">34</a>]</cite>, which however considers only the commercially-available models Suno and Udio and performs only real/fake music detection. We instead focus on open-source TTMs and consider commercial ones only in the open set classification. The reasoning behind this is that open-source techniques are possibly available to a wider part of the research community, which could use them to integrate FakeMusicCaps as they see fit.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">The code used to generate FakeMusicaps and perform the experiments<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/polimi-ispl/FakeMusicCaps" title="">https://github.com/polimi-ispl/FakeMusicCaps</a></span></span></span> as well as the full dataset<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/13732524" title="">https://zenodo.org/records/13732524</a></span></span></span> are publicly available.
The rest of the paper is organized as follows. In Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S2" title="II Problem Formulation â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">II</span></a> we introduce the attribution problem for TTM models. In Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S3" title="III FakeMusicCaps Dataset â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">III</span></a> we describe how the FakeMusicCaps dataset was created. Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S4" title="IV Experimental Analysis â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">IV</span></a> presents the experimental setup used to conduct the experiments, while in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5" title="V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">V</span></a> we present the results aimed analyzing the complexity of TTM attribution and the effectiveness of the dataset. Finally, in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S6" title="VI Conclusion â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">VI</span></a> we draw some conclusions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Problem Formulation</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.13">Given some kind of text representation <math alttext="\tau" class="ltx_Math" display="inline" id="S2.p1.1.m1.1"><semantics id="S2.p1.1.m1.1a"><mi id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.1d">italic_Ï„</annotation></semantics></math> and a composite model <math alttext="\mathcal{T}(\cdot)" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mrow id="S2.p1.2.m2.1.2" xref="S2.p1.2.m2.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.2.m2.1.2.2" xref="S2.p1.2.m2.1.2.2.cmml">ğ’¯</mi><mo id="S2.p1.2.m2.1.2.1" xref="S2.p1.2.m2.1.2.1.cmml">â¢</mo><mrow id="S2.p1.2.m2.1.2.3.2" xref="S2.p1.2.m2.1.2.cmml"><mo id="S2.p1.2.m2.1.2.3.2.1" stretchy="false" xref="S2.p1.2.m2.1.2.cmml">(</mo><mo id="S2.p1.2.m2.1.1" lspace="0em" rspace="0em" xref="S2.p1.2.m2.1.1.cmml">â‹…</mo><mo id="S2.p1.2.m2.1.2.3.2.2" stretchy="false" xref="S2.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.2.cmml" xref="S2.p1.2.m2.1.2"><times id="S2.p1.2.m2.1.2.1.cmml" xref="S2.p1.2.m2.1.2.1"></times><ci id="S2.p1.2.m2.1.2.2.cmml" xref="S2.p1.2.m2.1.2.2">ğ’¯</ci><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\mathcal{T}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">caligraphic_T ( â‹… )</annotation></semantics></math>, TTMs techniques model the function <math alttext="\mathbf{x}=\mathcal{T}(\tau)" class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mrow id="S2.p1.3.m3.1.2" xref="S2.p1.3.m3.1.2.cmml"><mi id="S2.p1.3.m3.1.2.2" xref="S2.p1.3.m3.1.2.2.cmml">ğ±</mi><mo id="S2.p1.3.m3.1.2.1" xref="S2.p1.3.m3.1.2.1.cmml">=</mo><mrow id="S2.p1.3.m3.1.2.3" xref="S2.p1.3.m3.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.3.m3.1.2.3.2" xref="S2.p1.3.m3.1.2.3.2.cmml">ğ’¯</mi><mo id="S2.p1.3.m3.1.2.3.1" xref="S2.p1.3.m3.1.2.3.1.cmml">â¢</mo><mrow id="S2.p1.3.m3.1.2.3.3.2" xref="S2.p1.3.m3.1.2.3.cmml"><mo id="S2.p1.3.m3.1.2.3.3.2.1" stretchy="false" xref="S2.p1.3.m3.1.2.3.cmml">(</mo><mi id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">Ï„</mi><mo id="S2.p1.3.m3.1.2.3.3.2.2" stretchy="false" xref="S2.p1.3.m3.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.2.cmml" xref="S2.p1.3.m3.1.2"><eq id="S2.p1.3.m3.1.2.1.cmml" xref="S2.p1.3.m3.1.2.1"></eq><ci id="S2.p1.3.m3.1.2.2.cmml" xref="S2.p1.3.m3.1.2.2">ğ±</ci><apply id="S2.p1.3.m3.1.2.3.cmml" xref="S2.p1.3.m3.1.2.3"><times id="S2.p1.3.m3.1.2.3.1.cmml" xref="S2.p1.3.m3.1.2.3.1"></times><ci id="S2.p1.3.m3.1.2.3.2.cmml" xref="S2.p1.3.m3.1.2.3.2">ğ’¯</ci><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">ğœ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathbf{x}=\mathcal{T}(\tau)</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">bold_x = caligraphic_T ( italic_Ï„ )</annotation></semantics></math>, where <math alttext="\mathbf{x}\in\mathbb{R}^{1\times N}" class="ltx_Math" display="inline" id="S2.p1.4.m4.1"><semantics id="S2.p1.4.m4.1a"><mrow id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">ğ±</mi><mo id="S2.p1.4.m4.1.1.1" xref="S2.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml"><mi id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.3.2.cmml">â„</mi><mrow id="S2.p1.4.m4.1.1.3.3" xref="S2.p1.4.m4.1.1.3.3.cmml"><mn id="S2.p1.4.m4.1.1.3.3.2" xref="S2.p1.4.m4.1.1.3.3.2.cmml">1</mn><mo id="S2.p1.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.p1.4.m4.1.1.3.3.1.cmml">Ã—</mo><mi id="S2.p1.4.m4.1.1.3.3.3" xref="S2.p1.4.m4.1.1.3.3.3.cmml">N</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><in id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1"></in><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">ğ±</ci><apply id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S2.p1.4.m4.1.1.3.2.cmml" xref="S2.p1.4.m4.1.1.3.2">â„</ci><apply id="S2.p1.4.m4.1.1.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3"><times id="S2.p1.4.m4.1.1.3.3.1.cmml" xref="S2.p1.4.m4.1.1.3.3.1"></times><cn id="S2.p1.4.m4.1.1.3.3.2.cmml" type="integer" xref="S2.p1.4.m4.1.1.3.3.2">1</cn><ci id="S2.p1.4.m4.1.1.3.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3.3">ğ‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">\mathbf{x}\in\mathbb{R}^{1\times N}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m4.1d">bold_x âˆˆ blackboard_R start_POSTSUPERSCRIPT 1 Ã— italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> is an audio waveform containing music corresponding to the textual description provided in <math alttext="\tau" class="ltx_Math" display="inline" id="S2.p1.5.m5.1"><semantics id="S2.p1.5.m5.1a"><mi id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">Ï„</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğœ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S2.p1.5.m5.1d">italic_Ï„</annotation></semantics></math>.
Then, the text-to-music attribution problem, schematically shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S2.F1" title="Figure 1 â€£ II Problem Formulation â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">1</span></a>, can be formally defined as follows. Given the time-discrete music signal <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S2.p1.6.m6.1"><semantics id="S2.p1.6.m6.1a"><mi id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.6.m6.1d">bold_x</annotation></semantics></math> and a set of <math alttext="I" class="ltx_Math" display="inline" id="S2.p1.7.m7.1"><semantics id="S2.p1.7.m7.1a"><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">ğ¼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">I</annotation><annotation encoding="application/x-llamapun" id="S2.p1.7.m7.1d">italic_I</annotation></semantics></math> TTM models <math alttext="\{\mathcal{T}_{0},\ldots,\mathcal{T}_{I-1}\}" class="ltx_Math" display="inline" id="S2.p1.8.m8.3"><semantics id="S2.p1.8.m8.3a"><mrow id="S2.p1.8.m8.3.3.2" xref="S2.p1.8.m8.3.3.3.cmml"><mo id="S2.p1.8.m8.3.3.2.3" stretchy="false" xref="S2.p1.8.m8.3.3.3.cmml">{</mo><msub id="S2.p1.8.m8.2.2.1.1" xref="S2.p1.8.m8.2.2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.8.m8.2.2.1.1.2" xref="S2.p1.8.m8.2.2.1.1.2.cmml">ğ’¯</mi><mn id="S2.p1.8.m8.2.2.1.1.3" xref="S2.p1.8.m8.2.2.1.1.3.cmml">0</mn></msub><mo id="S2.p1.8.m8.3.3.2.4" xref="S2.p1.8.m8.3.3.3.cmml">,</mo><mi id="S2.p1.8.m8.1.1" mathvariant="normal" xref="S2.p1.8.m8.1.1.cmml">â€¦</mi><mo id="S2.p1.8.m8.3.3.2.5" xref="S2.p1.8.m8.3.3.3.cmml">,</mo><msub id="S2.p1.8.m8.3.3.2.2" xref="S2.p1.8.m8.3.3.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.8.m8.3.3.2.2.2" xref="S2.p1.8.m8.3.3.2.2.2.cmml">ğ’¯</mi><mrow id="S2.p1.8.m8.3.3.2.2.3" xref="S2.p1.8.m8.3.3.2.2.3.cmml"><mi id="S2.p1.8.m8.3.3.2.2.3.2" xref="S2.p1.8.m8.3.3.2.2.3.2.cmml">I</mi><mo id="S2.p1.8.m8.3.3.2.2.3.1" xref="S2.p1.8.m8.3.3.2.2.3.1.cmml">âˆ’</mo><mn id="S2.p1.8.m8.3.3.2.2.3.3" xref="S2.p1.8.m8.3.3.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.p1.8.m8.3.3.2.6" stretchy="false" xref="S2.p1.8.m8.3.3.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.3b"><set id="S2.p1.8.m8.3.3.3.cmml" xref="S2.p1.8.m8.3.3.2"><apply id="S2.p1.8.m8.2.2.1.1.cmml" xref="S2.p1.8.m8.2.2.1.1"><csymbol cd="ambiguous" id="S2.p1.8.m8.2.2.1.1.1.cmml" xref="S2.p1.8.m8.2.2.1.1">subscript</csymbol><ci id="S2.p1.8.m8.2.2.1.1.2.cmml" xref="S2.p1.8.m8.2.2.1.1.2">ğ’¯</ci><cn id="S2.p1.8.m8.2.2.1.1.3.cmml" type="integer" xref="S2.p1.8.m8.2.2.1.1.3">0</cn></apply><ci id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1">â€¦</ci><apply id="S2.p1.8.m8.3.3.2.2.cmml" xref="S2.p1.8.m8.3.3.2.2"><csymbol cd="ambiguous" id="S2.p1.8.m8.3.3.2.2.1.cmml" xref="S2.p1.8.m8.3.3.2.2">subscript</csymbol><ci id="S2.p1.8.m8.3.3.2.2.2.cmml" xref="S2.p1.8.m8.3.3.2.2.2">ğ’¯</ci><apply id="S2.p1.8.m8.3.3.2.2.3.cmml" xref="S2.p1.8.m8.3.3.2.2.3"><minus id="S2.p1.8.m8.3.3.2.2.3.1.cmml" xref="S2.p1.8.m8.3.3.2.2.3.1"></minus><ci id="S2.p1.8.m8.3.3.2.2.3.2.cmml" xref="S2.p1.8.m8.3.3.2.2.3.2">ğ¼</ci><cn id="S2.p1.8.m8.3.3.2.2.3.3.cmml" type="integer" xref="S2.p1.8.m8.3.3.2.2.3.3">1</cn></apply></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.3c">\{\mathcal{T}_{0},\ldots,\mathcal{T}_{I-1}\}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.8.m8.3d">{ caligraphic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , â€¦ , caligraphic_T start_POSTSUBSCRIPT italic_I - 1 end_POSTSUBSCRIPT }</annotation></semantics></math>, the objective is to retrieve which generator <math alttext="\mathcal{T}_{i}" class="ltx_Math" display="inline" id="S2.p1.9.m9.1"><semantics id="S2.p1.9.m9.1a"><msub id="S2.p1.9.m9.1.1" xref="S2.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.9.m9.1.1.2" xref="S2.p1.9.m9.1.1.2.cmml">ğ’¯</mi><mi id="S2.p1.9.m9.1.1.3" xref="S2.p1.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p1.9.m9.1b"><apply id="S2.p1.9.m9.1.1.cmml" xref="S2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.p1.9.m9.1.1.1.cmml" xref="S2.p1.9.m9.1.1">subscript</csymbol><ci id="S2.p1.9.m9.1.1.2.cmml" xref="S2.p1.9.m9.1.1.2">ğ’¯</ci><ci id="S2.p1.9.m9.1.1.3.cmml" xref="S2.p1.9.m9.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m9.1c">\mathcal{T}_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.9.m9.1d">caligraphic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> has been used to generate <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S2.p1.10.m10.1"><semantics id="S2.p1.10.m10.1a"><mi id="S2.p1.10.m10.1.1" xref="S2.p1.10.m10.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S2.p1.10.m10.1b"><ci id="S2.p1.10.m10.1.1.cmml" xref="S2.p1.10.m10.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.10.m10.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.10.m10.1d">bold_x</annotation></semantics></math>. This is done by training a classifier that takes as input <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S2.p1.11.m11.1"><semantics id="S2.p1.11.m11.1a"><mi id="S2.p1.11.m11.1.1" xref="S2.p1.11.m11.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S2.p1.11.m11.1b"><ci id="S2.p1.11.m11.1.1.cmml" xref="S2.p1.11.m11.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.11.m11.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.11.m11.1d">bold_x</annotation></semantics></math> and outputs the probabilities <math alttext="p_{i},i=0,\ldots,I-1" class="ltx_Math" display="inline" id="S2.p1.12.m12.4"><semantics id="S2.p1.12.m12.4a"><mrow id="S2.p1.12.m12.4.4.2" xref="S2.p1.12.m12.4.4.3.cmml"><mrow id="S2.p1.12.m12.3.3.1.1" xref="S2.p1.12.m12.3.3.1.1.cmml"><mrow id="S2.p1.12.m12.3.3.1.1.1.1" xref="S2.p1.12.m12.3.3.1.1.1.2.cmml"><msub id="S2.p1.12.m12.3.3.1.1.1.1.1" xref="S2.p1.12.m12.3.3.1.1.1.1.1.cmml"><mi id="S2.p1.12.m12.3.3.1.1.1.1.1.2" xref="S2.p1.12.m12.3.3.1.1.1.1.1.2.cmml">p</mi><mi id="S2.p1.12.m12.3.3.1.1.1.1.1.3" xref="S2.p1.12.m12.3.3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.p1.12.m12.3.3.1.1.1.1.2" xref="S2.p1.12.m12.3.3.1.1.1.2.cmml">,</mo><mi id="S2.p1.12.m12.1.1" xref="S2.p1.12.m12.1.1.cmml">i</mi></mrow><mo id="S2.p1.12.m12.3.3.1.1.2" xref="S2.p1.12.m12.3.3.1.1.2.cmml">=</mo><mn id="S2.p1.12.m12.3.3.1.1.3" xref="S2.p1.12.m12.3.3.1.1.3.cmml">0</mn></mrow><mo id="S2.p1.12.m12.4.4.2.3" xref="S2.p1.12.m12.4.4.3a.cmml">,</mo><mrow id="S2.p1.12.m12.4.4.2.2.1" xref="S2.p1.12.m12.4.4.2.2.2.cmml"><mi id="S2.p1.12.m12.2.2" mathvariant="normal" xref="S2.p1.12.m12.2.2.cmml">â€¦</mi><mo id="S2.p1.12.m12.4.4.2.2.1.2" xref="S2.p1.12.m12.4.4.2.2.2.cmml">,</mo><mrow id="S2.p1.12.m12.4.4.2.2.1.1" xref="S2.p1.12.m12.4.4.2.2.1.1.cmml"><mi id="S2.p1.12.m12.4.4.2.2.1.1.2" xref="S2.p1.12.m12.4.4.2.2.1.1.2.cmml">I</mi><mo id="S2.p1.12.m12.4.4.2.2.1.1.1" xref="S2.p1.12.m12.4.4.2.2.1.1.1.cmml">âˆ’</mo><mn id="S2.p1.12.m12.4.4.2.2.1.1.3" xref="S2.p1.12.m12.4.4.2.2.1.1.3.cmml">1</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.12.m12.4b"><apply id="S2.p1.12.m12.4.4.3.cmml" xref="S2.p1.12.m12.4.4.2"><csymbol cd="ambiguous" id="S2.p1.12.m12.4.4.3a.cmml" xref="S2.p1.12.m12.4.4.2.3">formulae-sequence</csymbol><apply id="S2.p1.12.m12.3.3.1.1.cmml" xref="S2.p1.12.m12.3.3.1.1"><eq id="S2.p1.12.m12.3.3.1.1.2.cmml" xref="S2.p1.12.m12.3.3.1.1.2"></eq><list id="S2.p1.12.m12.3.3.1.1.1.2.cmml" xref="S2.p1.12.m12.3.3.1.1.1.1"><apply id="S2.p1.12.m12.3.3.1.1.1.1.1.cmml" xref="S2.p1.12.m12.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p1.12.m12.3.3.1.1.1.1.1.1.cmml" xref="S2.p1.12.m12.3.3.1.1.1.1.1">subscript</csymbol><ci id="S2.p1.12.m12.3.3.1.1.1.1.1.2.cmml" xref="S2.p1.12.m12.3.3.1.1.1.1.1.2">ğ‘</ci><ci id="S2.p1.12.m12.3.3.1.1.1.1.1.3.cmml" xref="S2.p1.12.m12.3.3.1.1.1.1.1.3">ğ‘–</ci></apply><ci id="S2.p1.12.m12.1.1.cmml" xref="S2.p1.12.m12.1.1">ğ‘–</ci></list><cn id="S2.p1.12.m12.3.3.1.1.3.cmml" type="integer" xref="S2.p1.12.m12.3.3.1.1.3">0</cn></apply><list id="S2.p1.12.m12.4.4.2.2.2.cmml" xref="S2.p1.12.m12.4.4.2.2.1"><ci id="S2.p1.12.m12.2.2.cmml" xref="S2.p1.12.m12.2.2">â€¦</ci><apply id="S2.p1.12.m12.4.4.2.2.1.1.cmml" xref="S2.p1.12.m12.4.4.2.2.1.1"><minus id="S2.p1.12.m12.4.4.2.2.1.1.1.cmml" xref="S2.p1.12.m12.4.4.2.2.1.1.1"></minus><ci id="S2.p1.12.m12.4.4.2.2.1.1.2.cmml" xref="S2.p1.12.m12.4.4.2.2.1.1.2">ğ¼</ci><cn id="S2.p1.12.m12.4.4.2.2.1.1.3.cmml" type="integer" xref="S2.p1.12.m12.4.4.2.2.1.1.3">1</cn></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.12.m12.4c">p_{i},i=0,\ldots,I-1</annotation><annotation encoding="application/x-llamapun" id="S2.p1.12.m12.4d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_i = 0 , â€¦ , italic_I - 1</annotation></semantics></math> of <math alttext="\mathbf{x}" class="ltx_Math" display="inline" id="S2.p1.13.m13.1"><semantics id="S2.p1.13.m13.1a"><mi id="S2.p1.13.m13.1.1" xref="S2.p1.13.m13.1.1.cmml">ğ±</mi><annotation-xml encoding="MathML-Content" id="S2.p1.13.m13.1b"><ci id="S2.p1.13.m13.1.1.cmml" xref="S2.p1.13.m13.1.1">ğ±</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.13.m13.1c">\mathbf{x}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.13.m13.1d">bold_x</annotation></semantics></math> being generated using each of the known TTM models.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The attribution problem is often considered in both closed- and open-set scenarios. In the former, all generators are seen both during training and testing, while in the latter, some TTMs are <span class="ltx_text ltx_font_italic" id="S2.p2.1.1">unknown</span> during training and seen only at testing time, posing the need to develop specific classification strategies.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="336" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S2.F1.3.2" style="font-size:90%;">Schematic representation of the text-to-music attribution problem.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">FakeMusicCaps Dataset</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we describe how the FakeMusicCaps dataset was created, first by presenting the chosen TTM models and then describing the generation procedure.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Considered Architectures</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">In this Section, we present an overview of the architectures (TTM01-TTM05) used to create the FakeMusicCaps dataset.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">TTM01-MusicGen</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib5" title="">5</a>]</cite> is an autoregressive language model, based on a single-stage transformer that decodes discrete audio tokens obtained via EncodecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib3" title="">3</a>]</cite>. It was trained over an undisclosed dataset of over 20K hours of music. We use the <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">medium</span> checkpoint consisting of <math alttext="1.5B" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mn id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">1.5</mn><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><times id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></times><cn id="S3.SS1.p2.1.m1.1.1.2.cmml" type="float" xref="S3.SS1.p2.1.m1.1.1.2">1.5</cn><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">ğµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">1.5B</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">1.5 italic_B</annotation></semantics></math> parameters.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">TTM02-MusicLDM</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib10" title="">10</a>]</cite> is a latent diffusion model operating on compressed audio representations extracted via HiFi-GANÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib35" title="">35</a>]</cite>. It adapts AudioLDM to the musical domain, by introducing beat-synchronous audio mixup and beat-synchronous latent mixup strategies to augment the quantity of data used for training.
The text conditioning is provided via CLAPÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib36" title="">36</a>]</cite>, which the authors finetune on music for a total of 20K hours. The MusicLDM model is then trained on the Audiostock datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib36" title="">36</a>]</cite>, containing 455.6 hours of music.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">TTM03-AudioLDM2</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib9" title="">9</a>]</cite>
is a latent diffusion model where the audio is compressed via a Variational AutoEncoder (VAE) and HiFiGAN, similarly to AudioLDM. However, the major difference of AudioLDM2, is that the diffusion model is conditioned through AudioMAEÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib37" title="">37</a>]</cite> that enables the adoption of a â€œLanguage of Audioâ€, that enables to generate a wide variety of typoes of audio. We use the <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">audioldm2-music</span> checkpoint to build FakeMusicCaps, specifically trained for text-to-music generation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">TTM04-Stable Audio Open</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib32" title="">32</a>]</cite> is a latent-diffusion architecture generating stereo data at <math alttext="44.1~{}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.SS1.p5.1.m1.1"><semantics id="S3.SS1.p5.1.m1.1a"><mrow id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml"><mn id="S3.SS1.p5.1.m1.1.1.2" xref="S3.SS1.p5.1.m1.1.1.2.cmml">44.1</mn><mo id="S3.SS1.p5.1.m1.1.1.1" lspace="0.330em" xref="S3.SS1.p5.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.SS1.p5.1.m1.1.1.3" xref="S3.SS1.p5.1.m1.1.1.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><apply id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1"><times id="S3.SS1.p5.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1.1"></times><cn id="S3.SS1.p5.1.m1.1.1.2.cmml" type="float" xref="S3.SS1.p5.1.m1.1.1.2">44.1</cn><ci id="S3.SS1.p5.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.m1.1.1.3">kHz</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">44.1~{}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.m1.1d">44.1 roman_kHz</annotation></semantics></math> based on a variant of Stable AudioÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib38" title="">38</a>]</cite> that uses T5Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib39" title="">39</a>]</cite> as a text encoder. The model is trained only on Creative Commons-licensed audio data for a total of 7.3K hours of audio.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">TTM05-Mustango</span>Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib12" title="">12</a>]</cite> is a diffusion-based TTM model that through a Music-domain-knowledge-informed UNet (MuNet) injects music concepts such as chord, beats, key or tempo in the generated music, during the reverse diffusion process. Through data augmentation, the authors generate the MusicBench dataset, composed of 53.168 tracks, to train the model. The model generates at <math alttext="16~{}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.SS1.p6.1.m1.1"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml"><mn id="S3.SS1.p6.1.m1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.cmml">16</mn><mo id="S3.SS1.p6.1.m1.1.1.1" lspace="0.330em" xref="S3.SS1.p6.1.m1.1.1.1.cmml">â¢</mo><mi id="S3.SS1.p6.1.m1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.3.cmml">kHz</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1"><times id="S3.SS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1"></times><cn id="S3.SS1.p6.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p6.1.m1.1.1.2">16</cn><ci id="S3.SS1.p6.1.m1.1.1.3.cmml" xref="S3.SS1.p6.1.m1.1.1.3">kHz</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">16~{}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.m1.1d">16 roman_kHz</annotation></semantics></math></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Generation strategy</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In this section we describe the strategy used to generate the FakeMusicCaps dataset. We consider the MusicCapsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib4" title="">4</a>]</cite> dataset, consisting of 5.5k 10 seconds of music clips from AudioSetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib40" title="">40</a>]</cite>, each one paired with an annotation by a professional musician. MusicCaps has rapidly become the benchmark dataset for the evaluation of TTM models. To create FakeMusicCaps, we use the caption from MusicCaps and for each one of them we generate a corresponding 10-second audio track using models (TTM01-TTM05) for a total of 27605 music tracks corresponding to almost 77 hours. Since the objective of the dataset is to provide an initial dataset for the analysis of the detection and attribution of music generated via TTM models, we adopt an audio pipeline that ensures that all audios are represented using the same format. Specifically, each track is converted to mono and downsampled to <math alttext="F_{s}=16~{}\mathrm{kHz}" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><msub id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.1.2.2" xref="S3.SS2.p1.1.m1.1.1.2.2.cmml">F</mi><mi id="S3.SS2.p1.1.m1.1.1.2.3" xref="S3.SS2.p1.1.m1.1.1.2.3.cmml">s</mi></msub><mo id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml"><mn id="S3.SS2.p1.1.m1.1.1.3.2" xref="S3.SS2.p1.1.m1.1.1.3.2.cmml">16</mn><mo id="S3.SS2.p1.1.m1.1.1.3.1" lspace="0.330em" xref="S3.SS2.p1.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p1.1.m1.1.1.3.3" xref="S3.SS2.p1.1.m1.1.1.3.3.cmml">kHz</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><eq id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></eq><apply id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.SS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2.2">ğ¹</ci><ci id="S3.SS2.p1.1.m1.1.1.2.3.cmml" xref="S3.SS2.p1.1.m1.1.1.2.3">ğ‘ </ci></apply><apply id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3"><times id="S3.SS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p1.1.m1.1.1.3.1"></times><cn id="S3.SS2.p1.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS2.p1.1.m1.1.1.3.2">16</cn><ci id="S3.SS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3.3">kHz</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">F_{s}=16~{}\mathrm{kHz}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_F start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = 16 roman_kHz</annotation></semantics></math>. Finally, we save each track using the 32-bit float wav format.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experimental Analysis</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section ,we describe the techniques and setup used during the experiment aimed at showing a first validation of the FakeMusicCaps dataset.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Dataset</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We used the FakeMusicCaps dataset during the training and test procedures. In order to build a test set disjointed from the training one, we selected <math alttext="320" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">320</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn id="S4.SS1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.p1.1.m1.1.1">320</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">320</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">320</annotation></semantics></math> tracks from FakeMusicCaps, selecting, for each TTM model, the ones having the same captions of the SunoCapsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib33" title="">33</a>]</cite> dataset. This choice was operated in order to be able to coherently use the Suno-generated music excerpts from SunoCaps to perform the open-set scenario experiments.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Baselines</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We use three classification models as simple benchmarks of the FakeMusicCaps for deepfake music detection and attribution.
We first considered a very simple network operating on raw audio, namely M5Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib41" title="">41</a>]</cite>. This network consists of only 0.5M parameters and is used as an initial experiment to see if the problem is even worth of attention.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Then, we selected a more complicated method operating on raw audio, RawNet2Â <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib42" title="">42</a>]</cite> and on log-spectrograms, denoted ResNet18+SpecÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib43" title="">43</a>]</cite>.
RawNet2 is an end-to-end neural network that has been used as a baseline for several antispoofing challenges such as ASVspoof 2021 and consists of Sinc Filters, followed by residual blocks and a Gated Recurrent Unit (GRU).</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Finally, we made use of ResNet18. This is an 18-layer deep convolutional layer with residual connections, we modify it making it suitable to work with 1-channel log-spectrograms.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">All methods were modified by adding a fully connected layer with a number of neurons at the end of the networks, corresponding to the number of considered classes.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Training</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.7">All models were trained to discriminate between <math alttext="6" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn id="S4.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS3.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">6</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">6</annotation></semantics></math> different classes, comprising the <math alttext="5" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn id="S4.SS3.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS3.p1.2.m2.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">5</annotation></semantics></math> known TTM models and the real music signals belonging to MusicCaps.
We trained all models using the cross-entropy as a loss function and used the Adam optimizer with a learning rate of <math alttext="1e-4" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><mrow id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mrow id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml"><mn id="S4.SS3.p1.3.m3.1.1.2.2" xref="S4.SS3.p1.3.m3.1.1.2.2.cmml">1</mn><mo id="S4.SS3.p1.3.m3.1.1.2.1" xref="S4.SS3.p1.3.m3.1.1.2.1.cmml">â¢</mo><mi id="S4.SS3.p1.3.m3.1.1.2.3" xref="S4.SS3.p1.3.m3.1.1.2.3.cmml">e</mi></mrow><mo id="S4.SS3.p1.3.m3.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.cmml">âˆ’</mo><mn id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">4</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><minus id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1"></minus><apply id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2"><times id="S4.SS3.p1.3.m3.1.1.2.1.cmml" xref="S4.SS3.p1.3.m3.1.1.2.1"></times><cn id="S4.SS3.p1.3.m3.1.1.2.2.cmml" type="integer" xref="S4.SS3.p1.3.m3.1.1.2.2">1</cn><ci id="S4.SS3.p1.3.m3.1.1.2.3.cmml" xref="S4.SS3.p1.3.m3.1.1.2.3">ğ‘’</ci></apply><cn id="S4.SS3.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS3.p1.3.m3.1.1.3">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">1e-4</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">1 italic_e - 4</annotation></semantics></math>.
All networks were traineto set a maximum of <math alttext="100" class="ltx_Math" display="inline" id="S4.SS3.p1.4.m4.1"><semantics id="S4.SS3.p1.4.m4.1a"><mn id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><cn id="S4.SS3.p1.4.m4.1.1.cmml" type="integer" xref="S4.SS3.p1.4.m4.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">100</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.4.m4.1d">100</annotation></semantics></math> epochs, ending the training earlier if the loss did not improve for more than <math alttext="10" class="ltx_Math" display="inline" id="S4.SS3.p1.5.m5.1"><semantics id="S4.SS3.p1.5.m5.1a"><mn id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><cn id="S4.SS3.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS3.p1.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">10</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.5.m5.1d">10</annotation></semantics></math> consecutive epochs. We used batch size of <math alttext="32" class="ltx_Math" display="inline" id="S4.SS3.p1.6.m6.1"><semantics id="S4.SS3.p1.6.m6.1a"><mn id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><cn id="S4.SS3.p1.6.m6.1.1.cmml" type="integer" xref="S4.SS3.p1.6.m6.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">32</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.6.m6.1d">32</annotation></semantics></math> for M5 and <math alttext="16" class="ltx_Math" display="inline" id="S4.SS3.p1.7.m7.1"><semantics id="S4.SS3.p1.7.m7.1a"><mn id="S4.SS3.p1.7.m7.1.1" xref="S4.SS3.p1.7.m7.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m7.1b"><cn id="S4.SS3.p1.7.m7.1.1.cmml" type="integer" xref="S4.SS3.p1.7.m7.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m7.1c">16</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.7.m7.1d">16</annotation></semantics></math> for both RawNet2 and ResNet18 + Spec. In the case of ResNet18 + Spec, we computed the STFT with 512 frequency points, using a hann window of length 512 samples with hop size 128 samples.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Classification Techniques</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.6">In the <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.6.1">closed set</span> classification problem, given a raw audio waveform corresponding to music, we want to identify the generation method from the set of <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.6.2">known</span> (i.e. a set of models included in the training dataset) TTM models, while in the case of the <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.6.3">open set</span> cclassification we want to detect it from a TTM model that it is <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.6.4">unknown</span>, i.e. not included in the training dataset.
If we consider <math alttext="p_{i}" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><msub id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml"><mi id="S4.SS4.p1.1.m1.1.1.2" xref="S4.SS4.p1.1.m1.1.1.2.cmml">p</mi><mi id="S4.SS4.p1.1.m1.1.1.3" xref="S4.SS4.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><apply id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.1.m1.1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p1.1.m1.1.1.2.cmml" xref="S4.SS4.p1.1.m1.1.1.2">ğ‘</ci><ci id="S4.SS4.p1.1.m1.1.1.3.cmml" xref="S4.SS4.p1.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> as the output of the softmax layer of the models, then in the closed set case, class attribution can simply be performed by computing <math alttext="\arg\max_{i}p_{i}" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mrow id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.cmml">arg</mi><mo id="S4.SS4.p1.2.m2.1.1a" lspace="0.167em" xref="S4.SS4.p1.2.m2.1.1.cmml">â¡</mo><mrow id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml"><msub id="S4.SS4.p1.2.m2.1.1.2.1" xref="S4.SS4.p1.2.m2.1.1.2.1.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2.1.2" xref="S4.SS4.p1.2.m2.1.1.2.1.2.cmml">max</mi><mi id="S4.SS4.p1.2.m2.1.1.2.1.3" xref="S4.SS4.p1.2.m2.1.1.2.1.3.cmml">i</mi></msub><mo id="S4.SS4.p1.2.m2.1.1.2a" lspace="0.167em" xref="S4.SS4.p1.2.m2.1.1.2.cmml">â¡</mo><msub id="S4.SS4.p1.2.m2.1.1.2.2" xref="S4.SS4.p1.2.m2.1.1.2.2.cmml"><mi id="S4.SS4.p1.2.m2.1.1.2.2.2" xref="S4.SS4.p1.2.m2.1.1.2.2.2.cmml">p</mi><mi id="S4.SS4.p1.2.m2.1.1.2.2.3" xref="S4.SS4.p1.2.m2.1.1.2.2.3.cmml">i</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><arg id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1"></arg><apply id="S4.SS4.p1.2.m2.1.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2"><apply id="S4.SS4.p1.2.m2.1.1.2.1.cmml" xref="S4.SS4.p1.2.m2.1.1.2.1"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.2.1">subscript</csymbol><max id="S4.SS4.p1.2.m2.1.1.2.1.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2.1.2"></max><ci id="S4.SS4.p1.2.m2.1.1.2.1.3.cmml" xref="S4.SS4.p1.2.m2.1.1.2.1.3">ğ‘–</ci></apply><apply id="S4.SS4.p1.2.m2.1.1.2.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS4.p1.2.m2.1.1.2.2.1.cmml" xref="S4.SS4.p1.2.m2.1.1.2.2">subscript</csymbol><ci id="S4.SS4.p1.2.m2.1.1.2.2.2.cmml" xref="S4.SS4.p1.2.m2.1.1.2.2.2">ğ‘</ci><ci id="S4.SS4.p1.2.m2.1.1.2.2.3.cmml" xref="S4.SS4.p1.2.m2.1.1.2.2.3">ğ‘–</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">\arg\max_{i}p_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">roman_arg roman_max start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.
For open set classification, instead, we follow two different approaches. In the open set <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.6.5">(threshold)</span> techniqueÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib44" title="">44</a>]</cite> we compute the two highest values of <math alttext="p_{i}" class="ltx_Math" display="inline" id="S4.SS4.p1.3.m3.1"><semantics id="S4.SS4.p1.3.m3.1a"><msub id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mi id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">p</mi><mi id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p1.3.m3.1.1.2.cmml" xref="S4.SS4.p1.3.m3.1.1.2">ğ‘</ci><ci id="S4.SS4.p1.3.m3.1.1.3.cmml" xref="S4.SS4.p1.3.m3.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.3.m3.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and then classify the input example as unknown if the ratio between these values is higher than a threshold. The rationale is that if the method was known, only one <math alttext="p_{i}" class="ltx_Math" display="inline" id="S4.SS4.p1.4.m4.1"><semantics id="S4.SS4.p1.4.m4.1a"><msub id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mi id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">p</mi><mi id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS4.p1.4.m4.1.1.2.cmml" xref="S4.SS4.p1.4.m4.1.1.2">ğ‘</ci><ci id="S4.SS4.p1.4.m4.1.1.3.cmml" xref="S4.SS4.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.4.m4.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> value should be high. We choose a threshold of <math alttext="2" class="ltx_Math" display="inline" id="S4.SS4.p1.5.m5.1"><semantics id="S4.SS4.p1.5.m5.1a"><mn id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><cn id="S4.SS4.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS4.p1.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.5.m5.1d">2</annotation></semantics></math> followingÂ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#bib.bib24" title="">24</a>]</cite>.
In the open set <span class="ltx_text ltx_font_italic" id="S4.SS4.p1.6.6">SVM</span> technique, instead, we train a one-class Support Vector Machine (SVM) using radial basis functions kernel over the <math alttext="p_{i}" class="ltx_Math" display="inline" id="S4.SS4.p1.6.m6.1"><semantics id="S4.SS4.p1.6.m6.1a"><msub id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml"><mi id="S4.SS4.p1.6.m6.1.1.2" xref="S4.SS4.p1.6.m6.1.1.2.cmml">p</mi><mi id="S4.SS4.p1.6.m6.1.1.3" xref="S4.SS4.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.1b"><apply id="S4.SS4.p1.6.m6.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS4.p1.6.m6.1.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S4.SS4.p1.6.m6.1.1.2.cmml" xref="S4.SS4.p1.6.m6.1.1.2">ğ‘</ci><ci id="S4.SS4.p1.6.m6.1.1.3.cmml" xref="S4.SS4.p1.6.m6.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.6.m6.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> values computed over the training data. The output of the classification is binary: either the class is known or not.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Results</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we present preliminary results aimed at demonstrating the suitability of FakeMusicCaps as an initial dataset for text-to-music model detection and attribution.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Closed-Set Performances</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Despite closed-set classification on a single dataset is often considered a trivial task in forensic applications, it is worth investigating the performance of the tested methods in this scenario.
TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.T1" title="TABLE I â€£ V-A Closed-Set Performances â€£ V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">I</span></a> reports closed-set classification results in terms of balanced accuracy <math alttext="\mathrm{ACC}_{B}" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><msub id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml"><mi id="S5.SS1.p1.1.m1.1.1.2" xref="S5.SS1.p1.1.m1.1.1.2.cmml">ACC</mi><mi id="S5.SS1.p1.1.m1.1.1.3" xref="S5.SS1.p1.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><apply id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS1.p1.1.m1.1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS1.p1.1.m1.1.1.2.cmml" xref="S5.SS1.p1.1.m1.1.1.2">ACC</ci><ci id="S5.SS1.p1.1.m1.1.1.3.cmml" xref="S5.SS1.p1.1.m1.1.1.3">ğµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\mathrm{ACC}_{B}</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">roman_ACC start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math>, Precision, Recall, and F1 Score. The left column of Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.F2" title="Figure 2 â€£ V-B Open Set Performances â€£ V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">2</span></a> shows the confusion matrix corresponding to M5, RawNet2 and ResNet18+Spec, respectively. In all metrics, the best performances are obtained by ResNet18 + Spec, while RawNet2 obtains slightly worse results than M5. From the inspection of the confusion matrices, we can see that ResNet18 slightly confounds TTM03 (AudioLDM2) with TTM05(Mustango), both diffusion-based models. M5 has slightly lower performances in detecting the ground-truth data, while RawNet2 struggles more to detect model TTM02 (MusicLDM).</p>
</div>
<figure class="ltx_table" id="S5.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T1.7.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S5.T1.8.2" style="font-size:90%;">Closed-set classification performances.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.5" style="width:433.6pt;height:114.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(81.0pt,-21.5pt) scale(1.59634122042719,1.59634122042719) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T1.1.1.1.2">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.1"><math alttext="\mathrm{ACC_{B}}\downarrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.m1.1a"><mrow id="S5.T1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.m1.1.1.cmml"><msub id="S5.T1.1.1.1.1.m1.1.1.2" xref="S5.T1.1.1.1.1.m1.1.1.2.cmml"><mi id="S5.T1.1.1.1.1.m1.1.1.2.2" xref="S5.T1.1.1.1.1.m1.1.1.2.2.cmml">ACC</mi><mi id="S5.T1.1.1.1.1.m1.1.1.2.3" mathvariant="normal" xref="S5.T1.1.1.1.1.m1.1.1.2.3.cmml">B</mi></msub><mo id="S5.T1.1.1.1.1.m1.1.1.1" stretchy="false" xref="S5.T1.1.1.1.1.m1.1.1.1.cmml">â†“</mo><mi id="S5.T1.1.1.1.1.m1.1.1.3" xref="S5.T1.1.1.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1"><ci id="S5.T1.1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1.1">â†“</ci><apply id="S5.T1.1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.T1.1.1.1.1.m1.1.1.2.1.cmml" xref="S5.T1.1.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S5.T1.1.1.1.1.m1.1.1.2.2.cmml" xref="S5.T1.1.1.1.1.m1.1.1.2.2">ACC</ci><ci id="S5.T1.1.1.1.1.m1.1.1.2.3.cmml" xref="S5.T1.1.1.1.1.m1.1.1.2.3">B</ci></apply><csymbol cd="latexml" id="S5.T1.1.1.1.1.m1.1.1.3.cmml" xref="S5.T1.1.1.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.m1.1c">\mathrm{ACC_{B}}\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.m1.1d">roman_ACC start_POSTSUBSCRIPT roman_B end_POSTSUBSCRIPT â†“</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.3">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.4">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T1.1.1.1.5">F1 Score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T1.5.5.6.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T1.5.5.6.1.1">M5</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.5.5.6.1.2">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.5.5.6.1.3">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.5.5.6.1.4">0.90</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T1.5.5.6.1.5">0.90</td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.5.7.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T1.5.5.7.2.1">RawNet2</th>
<td class="ltx_td ltx_align_center" id="S5.T1.5.5.7.2.2">0.88</td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.5.7.2.3">0.89</td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.5.7.2.4">0.88</td>
<td class="ltx_td ltx_align_center" id="S5.T1.5.5.7.2.5">0.88</td>
</tr>
<tr class="ltx_tr" id="S5.T1.5.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T1.5.5.5.5">ResNet18 + Spec</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.2.2.2.1"><math alttext="\mathbf{1.00}" class="ltx_Math" display="inline" id="S5.T1.2.2.2.1.m1.1"><semantics id="S5.T1.2.2.2.1.m1.1a"><mn class="ltx_mathvariant_bold" id="S5.T1.2.2.2.1.m1.1.1" mathvariant="bold" xref="S5.T1.2.2.2.1.m1.1.1.cmml">1.00</mn><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.1.m1.1b"><cn id="S5.T1.2.2.2.1.m1.1.1.cmml" type="float" xref="S5.T1.2.2.2.1.m1.1.1">1.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.1.m1.1c">\mathbf{1.00}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.1.m1.1d">bold_1.00</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.3.3.3.2"><math alttext="\mathbf{1.00}" class="ltx_Math" display="inline" id="S5.T1.3.3.3.2.m1.1"><semantics id="S5.T1.3.3.3.2.m1.1a"><mn class="ltx_mathvariant_bold" id="S5.T1.3.3.3.2.m1.1.1" mathvariant="bold" xref="S5.T1.3.3.3.2.m1.1.1.cmml">1.00</mn><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.2.m1.1b"><cn id="S5.T1.3.3.3.2.m1.1.1.cmml" type="float" xref="S5.T1.3.3.3.2.m1.1.1">1.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.2.m1.1c">\mathbf{1.00}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.2.m1.1d">bold_1.00</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.4.4.4.3"><math alttext="\mathbf{1.00}" class="ltx_Math" display="inline" id="S5.T1.4.4.4.3.m1.1"><semantics id="S5.T1.4.4.4.3.m1.1a"><mn class="ltx_mathvariant_bold" id="S5.T1.4.4.4.3.m1.1.1" mathvariant="bold" xref="S5.T1.4.4.4.3.m1.1.1.cmml">1.00</mn><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.3.m1.1b"><cn id="S5.T1.4.4.4.3.m1.1.1.cmml" type="float" xref="S5.T1.4.4.4.3.m1.1.1">1.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.3.m1.1c">\mathbf{1.00}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.3.m1.1d">bold_1.00</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T1.5.5.5.4"><math alttext="\mathbf{1.00}" class="ltx_Math" display="inline" id="S5.T1.5.5.5.4.m1.1"><semantics id="S5.T1.5.5.5.4.m1.1a"><mn class="ltx_mathvariant_bold" id="S5.T1.5.5.5.4.m1.1.1" mathvariant="bold" xref="S5.T1.5.5.5.4.m1.1.1.cmml">1.00</mn><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.5.4.m1.1b"><cn id="S5.T1.5.5.5.4.m1.1.1.cmml" type="float" xref="S5.T1.5.5.5.4.m1.1.1">1.00</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.5.4.m1.1c">\mathbf{1.00}</annotation><annotation encoding="application/x-llamapun" id="S5.T1.5.5.5.4.m1.1d">bold_1.00</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Open Set Performances</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">We show in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.T2" title="TABLE II â€£ V-B Open Set Performances â€£ V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">II</span></a> the performances when performing open set classification using the threshold approach and the corresponding confusion matrices in the second column of Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.F2" title="Figure 2 â€£ V-B Open Set Performances â€£ V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.3.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S5.T2.4.2" style="font-size:90%;">Open set (Threshold) classification performances.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.1" style="width:433.6pt;height:117.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(83.5pt,-22.5pt) scale(1.62627641752349,1.62627641752349) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.1.1.1.2">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1.1"><math alttext="\mathrm{ACC_{B}}" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.m1.1a"><msub id="S5.T2.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.m1.1.1.cmml"><mi id="S5.T2.1.1.1.1.m1.1.1.2" xref="S5.T2.1.1.1.1.m1.1.1.2.cmml">ACC</mi><mi id="S5.T2.1.1.1.1.m1.1.1.3" mathvariant="normal" xref="S5.T2.1.1.1.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.m1.1b"><apply id="S5.T2.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T2.1.1.1.1.m1.1.1.1.cmml" xref="S5.T2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T2.1.1.1.1.m1.1.1.2.cmml" xref="S5.T2.1.1.1.1.m1.1.1.2">ACC</ci><ci id="S5.T2.1.1.1.1.m1.1.1.3.cmml" xref="S5.T2.1.1.1.1.m1.1.1.3">B</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.m1.1c">\mathrm{ACC_{B}}</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.m1.1d">roman_ACC start_POSTSUBSCRIPT roman_B end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1.3">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1.4">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.1.1.1.5">F1 Score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T2.1.1.2.1.1">M5</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.2.1.2">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.2.1.3">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.2.1.4">0.76</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.1.1.2.1.5">0.75</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.3.2.1">RawNet2</th>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.2">0.75</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.3">0.75</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.4">0.75</td>
<td class="ltx_td ltx_align_center" id="S5.T2.1.1.3.2.5">0.74</td>
</tr>
<tr class="ltx_tr" id="S5.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T2.1.1.4.3.1">ResNet18 + Spec</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.2.1">0.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.3.1">0.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.4.1">0.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T2.1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.4.3.5.1">0.8</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">Results corresponding to the open set classification using the SVM approach are shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.T3" title="TABLE III â€£ V-B Open Set Performances â€£ V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">III</span></a>, with the corresponding confusion matrices in the last column of Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.F2" title="Figure 2 â€£ V-B Open Set Performances â€£ V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.3.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S5.T3.4.2" style="font-size:90%;">Open set (SVM) classification performances.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.1" style="width:433.6pt;height:117.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(83.5pt,-22.5pt) scale(1.62627641752349,1.62627641752349) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T3.1.1.1.2">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.1"><math alttext="\mathrm{ACC_{B}}" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.m1.1a"><msub id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml"><mi id="S5.T3.1.1.1.1.m1.1.1.2" xref="S5.T3.1.1.1.1.m1.1.1.2.cmml">ACC</mi><mi id="S5.T3.1.1.1.1.m1.1.1.3" mathvariant="normal" xref="S5.T3.1.1.1.1.m1.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><apply id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.T3.1.1.1.1.m1.1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S5.T3.1.1.1.1.m1.1.1.2.cmml" xref="S5.T3.1.1.1.1.m1.1.1.2">ACC</ci><ci id="S5.T3.1.1.1.1.m1.1.1.3.cmml" xref="S5.T3.1.1.1.1.m1.1.1.3">B</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\mathrm{ACC_{B}}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.m1.1d">roman_ACC start_POSTSUBSCRIPT roman_B end_POSTSUBSCRIPT</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.3">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.4">Recall</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.1.1.1.5">F1 Score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S5.T3.1.1.2.1.1">M5</th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.2.1.2">0.42</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.2.1.3">0.67</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.2.1.4">0.42</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.1.1.2.1.5">0.48</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T3.1.1.3.2.1">RawNet2</th>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.2">0.47</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.3">0.80</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.4">0.47</td>
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.3.2.5">0.52</td>
</tr>
<tr class="ltx_tr" id="S5.T3.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S5.T3.1.1.4.3.1">ResNet18 + Spec</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.3.2.1">0.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.3.3.1">0.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.3.4.1">0.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T3.1.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.4.3.5.1">0.56</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">As expected, in this case performance is much worse for all considered models. If we look at the results reported in the two tables, we can see that again ResNet18 + Spec obtains the best performance using both methods and that results obtained via the SVM technique are much worse than the ones obtained via the thresholding approach. The perspective however becomes different if we look at the confusion matrices. In the thresholding method, we can see that ResNet18 + Spec obtains the best performance when classifying the known methods, but misclassifies all the audio excerpts belonging to the UNKNWN class. Interestingly enough, these are confounded with the real examples, which is somewhat expected, given that the commercial model Suno is probably the most realistic of the considered TTM models. M5 and RawNet2 obtain somehow similar performance, with the former confounding UNKNWN examples with real and MusicGen-generated ones, while the latter confounds them mostly with MusicGen.</p>
</div>
<figure class="ltx_figure" id="S5.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.1.1" style="width:130.1pt;"><span class="ltx_text" id="S5.F2.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.1.1.1.g1" src="extracted/5879724/figures/cm_closed_set_M5_10_sec.png" width="598"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.2.1" style="width:130.1pt;"><span class="ltx_text" id="S5.F2.2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.2.1.1.g1" src="extracted/5879724/figures/cm_open_set_thresh_M5_10_sec.png" width="598"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.3.1" style="width:130.1pt;"><span class="ltx_text" id="S5.F2.3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.3.1.1.g1" src="extracted/5879724/figures/cm_open_set_svm_M5_10_sec.png" width="598"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.4.1" style="width:130.1pt;"><span class="ltx_text" id="S5.F2.4.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.4.1.1.g1" src="extracted/5879724/figures/cm_closed_set_RawNet2_10_sec.png" width="598"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.5.1" style="width:130.1pt;"><span class="ltx_text" id="S5.F2.5.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.5.1.1.g1" src="extracted/5879724/figures/cm_open_set_thresh_RawNet2_10_sec.png" width="598"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.6.1" style="width:130.1pt;"><span class="ltx_text" id="S5.F2.6.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.6.1.1.g1" src="extracted/5879724/figures/cm_open_set_svm_RawNet2_10_sec.png" width="598"/></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.7" style="width:130.1pt;">
<p class="ltx_p ltx_align_center" id="S5.F2.7.1"><span class="ltx_text" id="S5.F2.7.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.7.1.1.g1" src="extracted/5879724/figures/cm_closed_set_SpecResNet_10_sec.png" width="598"/></span></p>
<p class="ltx_p ltx_align_center ltx_align_center" id="S5.F2.7.2"><span class="ltx_text" id="S5.F2.7.2.1">(a) Closed set</span></p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.8" style="width:130.1pt;">
<p class="ltx_p ltx_align_center" id="S5.F2.8.1"><span class="ltx_text" id="S5.F2.8.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.8.1.1.g1" src="extracted/5879724/figures/cm_open_set_thresh_SpecResNet_10_sec.png" width="598"/></span></p>
<p class="ltx_p ltx_align_center ltx_align_center" id="S5.F2.8.2"><span class="ltx_text" id="S5.F2.8.2.1">(b) Open set - threshold</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F2.9" style="width:130.1pt;">
<p class="ltx_p ltx_align_center" id="S5.F2.9.1"><span class="ltx_text" id="S5.F2.9.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="449" id="S5.F2.9.1.1.g1" src="extracted/5879724/figures/cm_open_set_svm_SpecResNet_10_sec.png" width="598"/></span></p>
<p class="ltx_p ltx_align_center ltx_align_center" id="S5.F2.9.2"><span class="ltx_text" id="S5.F2.9.2.1">(c) Open set - SVM</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F2.11.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S5.F2.12.2" style="font-size:90%;">Confusion matrices of M5 (top), RawNet2 (middle) and ResNet+Spec (bottom) in the three classification scenarios.</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1">In the case of the SVM approach all models have a different behavior, specifically approximately half the time, they mistake the known techniques for the unknown one. Interestingly, RawNet2 obtains the highest accuracy of <math alttext="0.82" class="ltx_Math" display="inline" id="S5.SS2.p4.1.m1.1"><semantics id="S5.SS2.p4.1.m1.1a"><mn id="S5.SS2.p4.1.m1.1.1" xref="S5.SS2.p4.1.m1.1.1.cmml">0.82</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.p4.1.m1.1b"><cn id="S5.SS2.p4.1.m1.1.1.cmml" type="float" xref="S5.SS2.p4.1.m1.1.1">0.82</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p4.1.m1.1c">0.82</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p4.1.m1.1d">0.82</annotation></semantics></math> for what concerns the unknown class, and even in this case ResNet18 mistakes it for the real one.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Impact of window size</span>
</h3>
<figure class="ltx_figure" id="S5.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F3.1" style="width:138.8pt;">
<p class="ltx_p ltx_align_center" id="S5.F3.1.1"><span class="ltx_text" id="S5.F3.1.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="240" id="S5.F3.1.1.1.g1" src="extracted/5879724/figures/closed_set_vs_window.png" width="598"/></span></p>
<p class="ltx_p ltx_align_center ltx_align_center" id="S5.F3.1.2"><span class="ltx_text" id="S5.F3.1.2.1">(a) Closed set</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F3.2" style="width:138.8pt;">
<p class="ltx_p ltx_align_center" id="S5.F3.2.1"><span class="ltx_text" id="S5.F3.2.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="240" id="S5.F3.2.1.1.g1" src="extracted/5879724/figures/open_set_thresh_vs_window.png" width="598"/></span></p>
<p class="ltx_p ltx_align_center ltx_align_center" id="S5.F3.2.2"><span class="ltx_text" id="S5.F3.2.2.1">(b) Open set - Threshold</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S5.F3.3" style="width:138.8pt;">
<p class="ltx_p ltx_align_center" id="S5.F3.3.1"><span class="ltx_text" id="S5.F3.3.1.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="240" id="S5.F3.3.1.1.g1" src="extracted/5879724/figures/open_set_svm_vs_window.png" width="598"/></span></p>
<p class="ltx_p ltx_align_center ltx_align_center" id="S5.F3.3.2"><span class="ltx_text" id="S5.F3.3.2.1">(C) Open set - SVM</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F3.8.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S5.F3.9.2" style="font-size:90%;">Balanced accuracy varying according to the considered window size using M5 (<span class="ltx_text" id="S5.F3.9.2.1" style="color:#1F77B4;"> <span class="ltx_rule" style="width:8.7pt;height:2.8pt;background:black;display:inline-block;">Â </span></span>), RawNet2 (<span class="ltx_text" id="S5.F3.9.2.2" style="color:#AEC7E8;"> <span class="ltx_rule" style="width:8.7pt;height:2.8pt;background:black;display:inline-block;">Â </span></span>) and ResNet + Spec (<span class="ltx_text" id="S5.F3.9.2.3" style="color:#17BECF;"> <span class="ltx_rule" style="width:8.7pt;height:2.8pt;background:black;display:inline-block;">Â </span></span>).</span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.6">We further perform a small experiment to verify how much the impact of the temporal window length used as input to the models changes their performance. This is important, especially for the design of further datasets, i.e. do we need to create longer musical excerpts or not? We consider four window lengths, namely <math alttext="10~{}\mathrm{s}" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mrow id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mn id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">10</mn><mo id="S5.SS3.p1.1.m1.1.1.1" lspace="0.330em" xref="S5.SS3.p1.1.m1.1.1.1.cmml">â¢</mo><mi id="S5.SS3.p1.1.m1.1.1.3" mathvariant="normal" xref="S5.SS3.p1.1.m1.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><times id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1.1"></times><cn id="S5.SS3.p1.1.m1.1.1.2.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1.2">10</cn><ci id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">10~{}\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">10 roman_s</annotation></semantics></math>, <math alttext="7.5~{}\mathrm{s}" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><mrow id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mn id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">7.5</mn><mo id="S5.SS3.p1.2.m2.1.1.1" lspace="0.330em" xref="S5.SS3.p1.2.m2.1.1.1.cmml">â¢</mo><mi id="S5.SS3.p1.2.m2.1.1.3" mathvariant="normal" xref="S5.SS3.p1.2.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><times id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1.1"></times><cn id="S5.SS3.p1.2.m2.1.1.2.cmml" type="float" xref="S5.SS3.p1.2.m2.1.1.2">7.5</cn><ci id="S5.SS3.p1.2.m2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.1.1.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">7.5~{}\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">7.5 roman_s</annotation></semantics></math>, <math alttext="5\mathrm{s}" class="ltx_Math" display="inline" id="S5.SS3.p1.3.m3.1"><semantics id="S5.SS3.p1.3.m3.1a"><mrow id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mn id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">5</mn><mo id="S5.SS3.p1.3.m3.1.1.1" xref="S5.SS3.p1.3.m3.1.1.1.cmml">â¢</mo><mi id="S5.SS3.p1.3.m3.1.1.3" mathvariant="normal" xref="S5.SS3.p1.3.m3.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><times id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1.1"></times><cn id="S5.SS3.p1.3.m3.1.1.2.cmml" type="integer" xref="S5.SS3.p1.3.m3.1.1.2">5</cn><ci id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">5\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.3.m3.1d">5 roman_s</annotation></semantics></math> and <math alttext="2.5~{}\mathrm{s}" class="ltx_Math" display="inline" id="S5.SS3.p1.4.m4.1"><semantics id="S5.SS3.p1.4.m4.1a"><mrow id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml"><mn id="S5.SS3.p1.4.m4.1.1.2" xref="S5.SS3.p1.4.m4.1.1.2.cmml">2.5</mn><mo id="S5.SS3.p1.4.m4.1.1.1" lspace="0.330em" xref="S5.SS3.p1.4.m4.1.1.1.cmml">â¢</mo><mi id="S5.SS3.p1.4.m4.1.1.3" mathvariant="normal" xref="S5.SS3.p1.4.m4.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><apply id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1"><times id="S5.SS3.p1.4.m4.1.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1.1"></times><cn id="S5.SS3.p1.4.m4.1.1.2.cmml" type="float" xref="S5.SS3.p1.4.m4.1.1.2">2.5</cn><ci id="S5.SS3.p1.4.m4.1.1.3.cmml" xref="S5.SS3.p1.4.m4.1.1.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">2.5~{}\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.4.m4.1d">2.5 roman_s</annotation></semantics></math> and report the results in terms of balanced accuracy in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2409.10684v2#S5.F3" title="Figure 3 â€£ V-C Impact of window size â€£ V Results â€£ FakeMusicCaps: a Dataset for Detection and Attribution of Synthetic Music Generated via Text-to-Music Models This material is based on research sponsored by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under agreement number FA8750-20-2-1004. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA and AFRL or the U.S. Government. This work is supported by the European Union under the Italian National Recovery and Resilience Plan (NRRP) of NextGenerationEU (PE00000001 - program â€œRESTARTâ€, PE00000014 - program â€œSERICSâ€). This work is supported by the FOSTERER project, funded by the Italian Ministry of University, and Research within the PRIN 2022 program."><span class="ltx_text ltx_ref_tag">3</span></a>. As we can see, the variations in accuracy are not extreme in all classification scenarios. M5 seems to have an accuracy boost passing from <math alttext="7.5" class="ltx_Math" display="inline" id="S5.SS3.p1.5.m5.1"><semantics id="S5.SS3.p1.5.m5.1a"><mn id="S5.SS3.p1.5.m5.1.1" xref="S5.SS3.p1.5.m5.1.1.cmml">7.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.5.m5.1b"><cn id="S5.SS3.p1.5.m5.1.1.cmml" type="float" xref="S5.SS3.p1.5.m5.1.1">7.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.5.m5.1c">7.5</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.5.m5.1d">7.5</annotation></semantics></math> to <math alttext="10\mathrm{s}" class="ltx_Math" display="inline" id="S5.SS3.p1.6.m6.1"><semantics id="S5.SS3.p1.6.m6.1a"><mrow id="S5.SS3.p1.6.m6.1.1" xref="S5.SS3.p1.6.m6.1.1.cmml"><mn id="S5.SS3.p1.6.m6.1.1.2" xref="S5.SS3.p1.6.m6.1.1.2.cmml">10</mn><mo id="S5.SS3.p1.6.m6.1.1.1" xref="S5.SS3.p1.6.m6.1.1.1.cmml">â¢</mo><mi id="S5.SS3.p1.6.m6.1.1.3" mathvariant="normal" xref="S5.SS3.p1.6.m6.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.6.m6.1b"><apply id="S5.SS3.p1.6.m6.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1"><times id="S5.SS3.p1.6.m6.1.1.1.cmml" xref="S5.SS3.p1.6.m6.1.1.1"></times><cn id="S5.SS3.p1.6.m6.1.1.2.cmml" type="integer" xref="S5.SS3.p1.6.m6.1.1.2">10</cn><ci id="S5.SS3.p1.6.m6.1.1.3.cmml" xref="S5.SS3.p1.6.m6.1.1.3">s</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.6.m6.1c">10\mathrm{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.6.m6.1d">10 roman_s</annotation></semantics></math> window length for both closed set and open set (threshold) methods, ResNet18+Spec does not have major improvements, while a slight increase in accuracy is seen for RawNet2. Results in the case of Open set (SVM) show a less clear trend, but the impact of the window size does not seem to be relevant even in this case.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS4.5.1.1">V-D</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS4.6.2">Discussion</span>
</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">The objective of the results provided in this paper is to present a first approach to the TTM model detection and attribution and do not claim at all to be definitive. Instead, we hope to further motivate research in this direction. New TTM models are proposed almost monthly if not daily, with a continuous increase in quality, especially for what concerns commercial models. For these reasons, while from the results indicated in this paper the problem may seem to be relatively easy, especially in the closed-set scenario, things are not going to stay that way for long and the research community needs to prepare in advance to solve problems related to the detection of fake music. We can already identify a few developments not analyzed in this paper that could be considered in future works related to TTM attribution. For example, <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.1">Do the textual descriptions have an effect on the classification performance?</span> If text and music are effectively mutually dependent, in the scenario of TTM models, we could be able to leverage on that. Also, <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.2">Can we leverage music theory and musicology to detect music deepfakes?</span> The analysis of musical theory could be of interest in a context where generated music is inserted in otherwise â€œrealâ€ music, a problem denoted as <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.3">splicing.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we tackled the problem of detecting and attributing music generated via Text-to-music models. Specifically, we introduced the FakeMusicCaps dataset, created by replicating the MusicCaps dataset via five state-of-the-art TTM models. By applying simple audio forensics techniques, we demonstrate that the dataset could be used as an initial benchmark to tackle TTM detection and attribution. Our results are not to be considered definitive, instead, our objective is to further motivate the research in forensics techniques for the analysis of generated music. In fact, while the problem of fake music detection and attribution is now relatively simple, it is guaranteed to grow more extremely complicated day by day.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
J.-P. Briot, G.Â Hadjeres, and F.-D. Pachet, <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Deep learning techniques for
music generation</em>.Â Â Â Springer, 2020,
vol.Â 1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
R.Â Kumar, P.Â Seetharaman, A.Â Luebs, I.Â Kumar, and K.Â Kumar, â€œHigh-fidelity
audio compression with improved rvqgan,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural
Information Processing Systems</em>, vol.Â 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
A.Â DÃ©fossez, J.Â Copet, G.Â Synnaeve, and Y.Â Adi, â€œHigh fidelity neural
audio compression,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Transactions on Machine Learning Research</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A.Â Agostinelli, T.Â I. Denk, Z.Â Borsos, J.Â Engel, M.Â Verzetti, A.Â Caillon,
Q.Â Huang, A.Â Jansen, A.Â Roberts, M.Â Tagliasacchi <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">etÂ al.</em>, â€œMusiclm:
Generating music from text,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">arXiv preprint arXiv:2301.11325</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
J.Â Copet, F.Â Kreuk, I.Â Gat, T.Â Remez, D.Â Kant, G.Â Synnaeve, Y.Â Adi, and
A.Â DÃ©fossez, â€œSimple and controllable music generation,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances
in Neural Information Processing Systems</em>, vol.Â 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
A.Â Ziv, I.Â Gat, G.Â L. Lan, T.Â Remez, F.Â Kreuk, A.Â DÃ©fossez, J.Â Copet,
G.Â Synnaeve, and Y.Â Adi, â€œMasked audio generation using a single
non-autoregressive transformer,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2401.04577</em>,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
O.Â Tal, A.Â Ziv, I.Â Gat, F.Â Kreuk, and Y.Â Adi, â€œJoint audio and symbolic
conditioning for temporally controlled text-to-music generation,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2406.10970</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H.Â Liu, Z.Â Chen, Y.Â Yuan, X.Â Mei, X.Â Liu, D.Â Mandic, W.Â Wang, and M.Â D.
Plumbley, â€œAudioldm: Text-to-audio generation with latent diffusion
models,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">International Conference on Machine Learning</em>.Â Â Â PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
H.Â Liu, Y.Â Yuan, X.Â Liu, X.Â Mei, Q.Â Kong, Q.Â Tian, Y.Â Wang, W.Â Wang, Y.Â Wang,
and M.Â D. Plumbley, â€œAudioldm 2: Learning holistic audio generation with
self-supervised pretraining,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">IEEE/ACM Trans. Audio Speech Lang.
Process</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
K.Â Chen, Y.Â Wuderak, H.Â Liu, M.Â Nezhurina, T.Â Berg-Kirkpatrick, and S.Â Dubnov,
â€œMusicldm: Enhancing novelty in text-to-music generation using
beat-synchronous mixup strategies,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2024, pp. 1206â€“1210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Q.Â Huang, D.Â S. Park, T.Â Wang, T.Â I. Denk, A.Â Ly, N.Â Chen, Z.Â Zhang, Z.Â Zhang,
J.Â Yu, C.Â Frank <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">etÂ al.</em>, â€œNoise2music: Text-conditioned music
generation with diffusion models,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2">arXiv preprint arXiv:2302.03917</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J.Â Melechovsky, Z.Â Guo, D.Â Ghosal, N.Â Majumder, D.Â Herremans, and S.Â Poria,
â€œMustango: Toward controllable text-to-music generation,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the NAACL)</em>.Â Â Â Association for Computational Linguistics, 2024, pp. 8293â€“8316.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
F.Â Ronchini, L.Â Comanducci, G.Â Perego, and F.Â Antonacci, â€œPaguri: a user
experience study of creative interaction with text-to-music models,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2407.04333</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
â€œSuno â€” suno.com,â€ <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://suno.com/" title="">https://suno.com/</a>, [Accessed 12-09-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
â€œUdio â€” AI Music Generator - Official Website â€” udio.com,â€
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.udio.com/" title="">https://www.udio.com/</a>, [Accessed 12-09-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
M.Â Feffer, Z.Â C. Lipton, and C.Â Donahue, â€œDeepdrake ft. bts-gan and taylorvc:
An exploratory analysis of musical deepfakes and hosting platforms,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">HCMIR@ ISMIR</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Z.Â Sha, Z.Â Li, N.Â Yu, and Y.Â Zhang, â€œDe-fake: Detection and attribution of
fake images generated by text-to-image generation models,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2023 ACM SIGSAC Conference on Computer and
Communications Security</em>, 2023, pp. 3418â€“3432.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
N.Â Yu, L.Â Davis, and M.Â Fritz, â€œAttributing fake images to gans: Learning and
analyzing gan fingerprints,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International Conference on Computer
Vision (ICCV)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
R.Â Corvi, D.Â Cozzolino, G.Â Zingarini, G.Â Poggi, K.Â Nagano, and L.Â Verdoliva,
â€œOn the detection of synthetic images generated by diffusion models,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP)</em>.Â Â Â IEEE, 2023, pp.
1â€“5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L.Â Abady, J.Â Wang, B.Â Tondi, and M.Â Barni, â€œA siamese-based verification
system for open-set architecture attribution of synthetic images,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Pattern Recognition Letters</em>, vol. 180, pp. 75â€“81, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A.Â WiÃŸmann, S.Â Zeiler, R.Â M. Nickel, and D.Â Kolossa, â€œWhodunit: Detection
and attribution of synthetic images by leveraging model-specific
fingerprints,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">ACM International Workshop on Multimedia AI against
Disinformation (MAD)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
S.Â Mandelli, P.Â Bestagini, L.Â Verdoliva, and S.Â Tubaro, â€œFacing device
attribution problem for stabilized video sequences,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">IEEE Transactions
on Information Forensics and Security (TIFS)</em>, vol.Â 15, pp. 14â€“27, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
H.Â Wu, Y.Â Tseng, and H.-y. Lee, â€œCodecfake: Enhancing anti-spoofing models
against deepfake audios from codec-based speech synthesis systems,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Interspeech</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
D.Â Salvi, P.Â Bestagini, and S.Â Tubaro, â€œExploring the synthetic speech
attribution problem through data-driven detectors,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">IEEE
International Workshop on Information Forensics and Security (WIFS)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
K.Â Bhagtani, E.Â R. Bartusiak, A.Â K.Â S. Yadav, P.Â Bestagini, and E.Â J. Delp,
â€œSynthesized speech attribution using the patchout spectrogram attribution
transformer,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">ACM Workshop on Information Hiding and Multimedia
Security (IH&amp;MMSec)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Y.Â Zang, Y.Â Zhang, M.Â Heydari, and Z.Â Duan, â€œSingfake: Singing voice deepfake
detection,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>.Â Â Â IEEE,
2024, pp. 12â€‰156â€“12â€‰160.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Y.Â Xie, J.Â Zhou, X.Â Lu, Z.Â Jiang, Y.Â Yang, H.Â Cheng, and L.Â Ye, â€œFsd: An
initial chinese dataset for fake song detection,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP)</em>.Â Â Â IEEE, 2024, pp. 4605â€“4609.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X.Â Chen, H.Â Wu, J.-S.Â R. Jang, and H.-y. Lee, â€œSinging voice graph modeling
for singfake detection,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Interspeech</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
A.Â Guragain, T.Â Liu, Z.Â Pan, H.Â B. Sailor, and Q.Â Wang, â€œSpeech foundation
model ensembles for the controlled singing voice deepfake detection (ctrsvdd)
challenge 2024,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">2024 IEEE Spoken Language Technology
Workshop</em>.Â Â Â IEEE, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D.Â Afchar, G.Â M. Brocal, and R.Â Hennequin, â€œDetecting music deepfakes is easy
but actually hard,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2405.04181</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
I.Â Manco, B.Â Weck, S.Â Doh, M.Â Won, Y.Â Zhang, D.Â Bogdanov, Y.Â Wu, K.Â Chen,
P.Â Tovstogan, E.Â Benetos <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">etÂ al.</em>, â€œThe song describer dataset: a
corpus of audio captions for music-and-language evaluation,â€ in
<em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2">Machine Learning for Audio Workshop at NeurIPS</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Z.Â Evans, J.Â D. Parker, C.Â Carr, Z.Â Zukowski, J.Â Taylor, and J.Â Pons, â€œStable
audio open,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2407.14358</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
M.Â Civit, V.Â Drai-Zerbib, D.Â Lizcano, and M.Â J. Escalona, â€œSunocaps: A novel
dataset of text-prompt based ai-generated music with emotion annotations,â€
<em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Data in Brief</em>, vol.Â 55, p. 110743, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
M.Â A. Rahman, Z.Â I.Â A. Hakim, N.Â H. Sarker, B.Â Paul, and S.Â A. Fattah,
â€œSonics: Synthetic or notâ€“identifying counterfeit songs,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv
preprint arXiv:2408.14080</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J.Â Kong, J.Â Kim, and J.Â Bae, â€œHifi-gan: Generative adversarial networks for
efficient and high fidelity speech synthesis,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in neural
information processing systems</em>, vol.Â 33, pp. 17â€‰022â€“17â€‰033, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Y.Â Wu, K.Â Chen, T.Â Zhang, Y.Â Hui, T.Â Berg-Kirkpatrick, and S.Â Dubnov,
â€œLarge-scale contrastive language-audio pretraining with feature fusion and
keyword-to-caption augmentation,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2023, pp. 1â€“5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
P.-Y. Huang, H.Â Xu, J.Â Li, A.Â Baevski, M.Â Auli, W.Â Galuba, F.Â Metze, and
C.Â Feichtenhofer, â€œMasked autoencoders that listen,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Advances in
Neural Information Processing Systems</em>, vol.Â 35, pp. 28â€‰708â€“28â€‰720, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Z.Â Evans, J.Â D. Parker, C.Â Carr, Z.Â Zukowski, J.Â Taylor, and J.Â Pons,
â€œLong-form music generation with latent diffusion,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint
arXiv:2404.10301</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
C.Â Raffel, N.Â Shazeer, A.Â Roberts, K.Â Lee, S.Â Narang, M.Â Matena, Y.Â Zhou,
W.Â Li, and P.Â J. Liu, â€œExploring the limits of transfer learning with a
unified text-to-text transformer,â€ <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Journal of machine learning
research</em>, vol.Â 21, no. 140, pp. 1â€“67, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J.Â F. Gemmeke, D.Â P. Ellis, D.Â Freedman, A.Â Jansen, W.Â Lawrence, R.Â C. Moore,
M.Â Plakal, and M.Â Ritter, â€œAudio set: An ontology and human-labeled dataset
for audio events,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2017, pp. 776â€“780.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
W.Â Dai, C.Â Dai, S.Â Qu, J.Â Li, and S.Â Das, â€œVery deep convolutional neural
networks for raw waveforms,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP)</em>.Â Â Â IEEE, 2017, pp. 421â€“425.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
H.Â Tak, J.Â Patino, M.Â Todisco, A.Â Nautsch, N.Â Evans, and A.Â Larcher,
â€œEnd-to-end anti-spoofing with rawnet2,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
K.Â He, X.Â Zhang, S.Â Ren, and J.Â Sun, â€œDeep residual learning for image
recognition,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</em>, 2016, pp. 770â€“778.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
D.Â Hendrycks and K.Â Gimpel, â€œA baseline for detecting misclassified and
out-of-distribution examples in neural networks,â€ in <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">International
Conference on Learning Representations</em>, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Sep 25 17:00:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
