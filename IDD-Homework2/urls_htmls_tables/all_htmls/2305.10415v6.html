<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.10415] PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering</title><meta property="og:description" content="In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA),
which is crucial in efficiently interpreting medical images with vital clinic-relevant information.
Firstly, we reframe the problem …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.10415">

<!--Generated on Thu Feb 29 07:43:39 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">PMC-VQA: Visual Instruction Tuning for 
<br class="ltx_break">Medical Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoman Zhang<sup id="id1.1.id1" class="ltx_sup">1,2,*</sup>,
Chaoyi Wu<sup id="id2.2.id2" class="ltx_sup">1,2,*</sup>,
Ziheng Zhao<sup id="id3.3.id3" class="ltx_sup">1</sup>,
Weixiong Lin<sup id="id4.4.id4" class="ltx_sup">1</sup> 
<br class="ltx_break">
<span id="id5.5.id5" class="ltx_text ltx_font_bold">Ya Zhang<sup id="id5.5.id5.1" class="ltx_sup">1,2</sup></span>,
<span id="id6.6.id6" class="ltx_text ltx_font_bold">Yanfeng Wang<sup id="id6.6.id6.1" class="ltx_sup">1,2,†</sup></span>,
<span id="id7.7.id7" class="ltx_text ltx_font_bold">Weidi Xie<sup id="id7.7.id7.1" class="ltx_sup">1,2,†</sup></span> 
<br class="ltx_break">
<span id="id8.8.id8" class="ltx_text ltx_font_typewriter" style="font-size:90%;">{xm99sjtu, wtzxxxwcy02, weidi}@sjtu.edu.cn
<br class="ltx_break">
<sup id="id8.8.id8.1" class="ltx_sup">1</sup>Shanghai Jiao Tong University  <sup id="id8.8.id8.2" class="ltx_sup">2</sup>Shanghai AI Laboratory 
<br class="ltx_break">
<a target="_blank" href="https://xiaoman-zhang.github.io/PMC-VQA/" title="" class="ltx_ref ltx_url">https://xiaoman-zhang.github.io/PMC-VQA/</a>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA),
which is crucial in efficiently interpreting medical images with vital clinic-relevant information.
Firstly, we reframe the problem of MedVQA as a generation task that naturally follows the human-machine interaction, we propose a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model.
Secondly, we establish a scalable pipeline to construct a large-scale medical visual question-answering dataset, named PMC-VQA, which contains 227k VQA pairs of 149k images that cover various modalities or diseases. Thirdly, we pre-train our proposed model on PMC-VQA and then fine-tune it on multiple public benchmarks, <span id="id9.id1.1" class="ltx_text ltx_font_italic">e.g.,</span> VQA-RAD and SLAKE, outperforming existing work by a large margin.
Additionally, we propose a test set that has undergone manual verification, which is significantly more challenging, even the best models struggle to solve.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>*: These authors contribute equally to this work.</span></span></span><span id="footnotex2" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>†: Corresponding author.</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Large language models (LLMs), such as ChatGPT <cite class="ltx_cite ltx_citemacro_cite">cha (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>, PaLM <cite class="ltx_cite ltx_citemacro_cite">Chowdhery et al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>, LLaMA <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite>,
have recently demonstrated remarkable progress in a wide range of natural language processing (NLP) tasks, such as question answering, text classification, and interactive dialog.
Notably, even in domains where expert knowledge is supposed to play a critical role, like medical diagnosis, these language models have also achieved impressive success, passing the United States Medical Licensing Examination (USMLE) <cite class="ltx_cite ltx_citemacro_cite">Jin et al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>); Kung et al. (<a href="#bib.bib17" title="" class="ltx_ref">2023</a>); Nori et al. (<a href="#bib.bib26" title="" class="ltx_ref">2023</a>); Singhal et al. (<a href="#bib.bib32" title="" class="ltx_ref">2022</a>)</cite>. While recent LLMs excel in language understanding in the medical domain, they are essentially “blind” to visual modalities such as images and videos, hindering the utilization of visual content as a means of communication with these models.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this paper, we focus on the problem of Medical Visual Question Answering (MedVQA),
which aims to develop models that can comprehend text-based queries and produce accurate answers by leveraging medical visual content <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>.
Existing MedVQA methods <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>); Liu et al. (<a href="#bib.bib22" title="" class="ltx_ref">2021a</a>); Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>); Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> typically treat the problem as a retrieval task with a limited answer base and train multi-modal vision-language models with contrastive or classification objectives. Consequently, they are only useful for limited use cases where a finite set of outcomes is provided beforehand.
We propose to develop the <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">first</span> open-ended MedVQA system with a generative model as the backend, capable of handling diverse questions that arise in clinical practice, generating answers in free form without being constrained by the vocabulary. While there has been promising research in visual-language representation learning, such as Flamingo <cite class="ltx_cite ltx_citemacro_cite">Alayrac et al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite> and BLIP <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>, these models have primarily been trained on natural language and images,
with very limited application in medical domain,
due to the complex and nuanced visual concepts often found in medical scenarios.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To this end, we introduce a novel paradigm for MedVQA that harnesses the power of generative learning.
Specifically, our proposed models start from the foundation models in medical domain, and train a bridge to align the pre-trained vision encoder and large language model via visual instruction tuning, we term the model as <span id="S1.p3.1.1" class="ltx_text ltx_font_bold">MedVInT</span> (<span id="S1.p3.1.2" class="ltx_text ltx_font_bold">Med</span>ical <span id="S1.p3.1.3" class="ltx_text ltx_font_bold">V</span>isual <span id="S1.p3.1.4" class="ltx_text ltx_font_bold">In</span>struction <span id="S1.p3.1.5" class="ltx_text ltx_font_bold">T</span>uning).
To accommodate different architectures, we offer two variants, named as MedVInT-TE and MedVInT-TD, that are tailored for encoder-based and decoder-based language models, respectively.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In order to effectively train the generative-based MedVQA models,
our study reveals that existing datasets are limited in size,
making them insufficient for training high-performing models.
To overcome this challenge, we leverage well-established medical visual-language datasets <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> and initiate a scalable, automatic pipeline for constructing a new large-scale medical visual question-answering dataset.
This new dataset, termed as <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">PMC-VQA</span>,
contains 227k VQA pairs of 149k images,
covering various modalities or diseases (Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), surpassing existing datasets in terms of both amount and diversity,
as illustrated in Tab. <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
In our experiments, we pre-trained MedVInT on the collected PMC-VQA dataset and fine-tuned it on the existing MedVQA datasets,
<span id="S1.p4.1.2" class="ltx_text ltx_font_italic">e.g.</span>, VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">Lau et al. (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite> and SLAKE <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021b</a>)</cite>, outperforming existing models by a large margin,
achieving over 80% accuracy on multi-choice selection.
However, while evaluating on our proposed challenging benchmark,
even the state-of-the-art models struggle, showing that there is still ample room for development in this field.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In summary, our contributions are as follows:
<span id="S1.p5.1.1" class="ltx_text ltx_font_bold">(i)</span> We reframe the problem of MedVQA as a generative learning task and propose MedVInT, a model obtained by aligning a pre-trained vision encoder with large language model through visual instruction tuning;
<span id="S1.p5.1.2" class="ltx_text ltx_font_bold">(ii)</span> We introduce a scalable pipeline and construct a large-scale MedVQA dataset, PMC-VQA, which far exceeds the size and diversity of existing datasets, covering various modalities and diseases;
<span id="S1.p5.1.3" class="ltx_text ltx_font_bold">(iii)</span> We pre-train MedVInT on PMC-VQA and fine-tune it on
VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">Lau et al. (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite> and SLAKE <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021b</a>)</cite>,
achieving state-of-the-art performance and significantly outperforming existing models;
<span id="S1.p5.1.4" class="ltx_text ltx_font_bold">(iv)</span> We propose a new test set and present a more challenging benchmark for MedVQA,
to evaluate the performance of VQA methods thoroughly.</p>
</div>
<figure id="S1.T1" class="ltx_table">

<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison of existing medical VQA datasets with PMC-VQA, demonstrating the significant increase in size and diversity achieved by our dataset.
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S1.T1.3" class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.3.4.1" class="ltx_tr">
<th id="S1.T1.3.4.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.3.4.1.1.1" class="ltx_text" style="font-size:80%;">Dataset</span></th>
<th id="S1.T1.3.4.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.3.4.1.2.1" class="ltx_text" style="font-size:80%;">Modality</span></th>
<th id="S1.T1.3.4.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.3.4.1.3.1" class="ltx_text" style="font-size:80%;">Source</span></th>
<th id="S1.T1.3.4.1.4" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.3.4.1.4.1" class="ltx_text" style="font-size:80%;">Images</span></th>
<th id="S1.T1.3.4.1.5" class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.3.4.1.5.1" class="ltx_text" style="font-size:80%;">QA pairs</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1" class="ltx_tr">
<td id="S1.T1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">
<span id="S1.T1.1.1.2.1" class="ltx_text" style="font-size:80%;">VQA-RAD </span><cite class="ltx_cite ltx_citemacro_cite">Lau et al. <span id="S1.T1.1.1.2.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib18" title="" class="ltx_ref">2018</a><span id="S1.T1.1.1.2.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S1.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.1.3.1" class="ltx_text" style="font-size:80%;">Radiology</span></td>
<td id="S1.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_t">
<span id="S1.T1.1.1.1.1" class="ltx_text" style="font-size:80%;">MedPix</span><sup id="S1.T1.1.1.1.2" class="ltx_sup"><span id="S1.T1.1.1.1.2.1" class="ltx_text" style="font-size:80%;">®</span></sup><span id="S1.T1.1.1.1.3" class="ltx_text" style="font-size:80%;"> database</span>
</td>
<td id="S1.T1.1.1.4" class="ltx_td ltx_align_right ltx_border_t"><span id="S1.T1.1.1.4.1" class="ltx_text" style="font-size:80%;">0.3k</span></td>
<td id="S1.T1.1.1.5" class="ltx_td ltx_align_right ltx_border_t"><span id="S1.T1.1.1.5.1" class="ltx_text" style="font-size:80%;">3.5k</span></td>
</tr>
<tr id="S1.T1.3.5.1" class="ltx_tr">
<td id="S1.T1.3.5.1.1" class="ltx_td ltx_align_left">
<span id="S1.T1.3.5.1.1.1" class="ltx_text" style="font-size:80%;">PathVQA </span><cite class="ltx_cite ltx_citemacro_cite">He et al. <span id="S1.T1.3.5.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib12" title="" class="ltx_ref">2020</a><span id="S1.T1.3.5.1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S1.T1.3.5.1.2" class="ltx_td ltx_align_center"><span id="S1.T1.3.5.1.2.1" class="ltx_text" style="font-size:80%;">Pathology</span></td>
<td id="S1.T1.3.5.1.3" class="ltx_td ltx_align_center">
<span id="S1.T1.3.5.1.3.1" class="ltx_text" style="font-size:80%;">PEIR Digital Library </span><cite class="ltx_cite ltx_citemacro_cite">Jones et al. <span id="S1.T1.3.5.1.3.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib14" title="" class="ltx_ref">2001</a><span id="S1.T1.3.5.1.3.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S1.T1.3.5.1.4" class="ltx_td ltx_align_right"><span id="S1.T1.3.5.1.4.1" class="ltx_text" style="font-size:80%;">5k</span></td>
<td id="S1.T1.3.5.1.5" class="ltx_td ltx_align_right"><span id="S1.T1.3.5.1.5.1" class="ltx_text" style="font-size:80%;">32.8k</span></td>
</tr>
<tr id="S1.T1.3.6.2" class="ltx_tr">
<td id="S1.T1.3.6.2.1" class="ltx_td ltx_align_left">
<span id="S1.T1.3.6.2.1.1" class="ltx_text" style="font-size:80%;">SLAKE </span><cite class="ltx_cite ltx_citemacro_cite">Liu et al. <span id="S1.T1.3.6.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib23" title="" class="ltx_ref">2021b</a><span id="S1.T1.3.6.2.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S1.T1.3.6.2.2" class="ltx_td ltx_align_center"><span id="S1.T1.3.6.2.2.1" class="ltx_text" style="font-size:80%;">Radiology</span></td>
<td id="S1.T1.3.6.2.3" class="ltx_td ltx_align_center">
<span id="S1.T1.3.6.2.3.1" class="ltx_text" style="font-size:80%;">MSD </span><cite class="ltx_cite ltx_citemacro_cite">Antonelli et al. <span id="S1.T1.3.6.2.3.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib3" title="" class="ltx_ref">2022</a><span id="S1.T1.3.6.2.3.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S1.T1.3.6.2.3.4" class="ltx_text" style="font-size:80%;">, ChestX-ray8 </span><cite class="ltx_cite ltx_citemacro_cite">Wang et al. <span id="S1.T1.3.6.2.3.5.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib36" title="" class="ltx_ref">2017</a><span id="S1.T1.3.6.2.3.6.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S1.T1.3.6.2.3.7" class="ltx_text" style="font-size:80%;">, CHAOS </span><cite class="ltx_cite ltx_citemacro_cite">Kavur et al. <span id="S1.T1.3.6.2.3.8.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib15" title="" class="ltx_ref">2021</a><span id="S1.T1.3.6.2.3.9.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S1.T1.3.6.2.4" class="ltx_td ltx_align_right"><span id="S1.T1.3.6.2.4.1" class="ltx_text" style="font-size:80%;">0.7k</span></td>
<td id="S1.T1.3.6.2.5" class="ltx_td ltx_align_right"><span id="S1.T1.3.6.2.5.1" class="ltx_text" style="font-size:80%;">14k</span></td>
</tr>
<tr id="S1.T1.2.2" class="ltx_tr">
<td id="S1.T1.2.2.2" class="ltx_td ltx_align_left">
<span id="S1.T1.2.2.2.1" class="ltx_text" style="font-size:80%;">VQA-Med-2021 </span><cite class="ltx_cite ltx_citemacro_cite">Ben Abacha et al. <span id="S1.T1.2.2.2.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib5" title="" class="ltx_ref">2021</a><span id="S1.T1.2.2.2.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</td>
<td id="S1.T1.2.2.3" class="ltx_td ltx_align_center"><span id="S1.T1.2.2.3.1" class="ltx_text" style="font-size:80%;">Radiology</span></td>
<td id="S1.T1.2.2.1" class="ltx_td ltx_align_center">
<span id="S1.T1.2.2.1.1" class="ltx_text" style="font-size:80%;">MedPix</span><sup id="S1.T1.2.2.1.2" class="ltx_sup"><span id="S1.T1.2.2.1.2.1" class="ltx_text" style="font-size:80%;">®</span></sup><span id="S1.T1.2.2.1.3" class="ltx_text" style="font-size:80%;"> database</span>
</td>
<td id="S1.T1.2.2.4" class="ltx_td ltx_align_right"><span id="S1.T1.2.2.4.1" class="ltx_text" style="font-size:80%;">5k</span></td>
<td id="S1.T1.2.2.5" class="ltx_td ltx_align_right"><span id="S1.T1.2.2.5.1" class="ltx_text" style="font-size:80%;">5k</span></td>
</tr>
<tr id="S1.T1.3.3" class="ltx_tr">
<td id="S1.T1.3.3.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t"><span id="S1.T1.3.3.2.1" class="ltx_text" style="font-size:80%;">PMC-VQA</span></td>
<td id="S1.T1.3.3.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S1.T1.3.3.3.1" class="ltx_text" style="font-size:80%;">Mixture</span><sup id="S1.T1.3.3.3.2" class="ltx_sup"><span id="S1.T1.3.3.3.2.1" class="ltx_text" style="font-size:80%;">*</span></sup>
</td>
<td id="S1.T1.3.3.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">
<span id="S1.T1.3.3.1.1" class="ltx_text" style="font-size:80%;">PubMed Central</span><sup id="S1.T1.3.3.1.2" class="ltx_sup"><span id="S1.T1.3.3.1.2.1" class="ltx_text" style="font-size:80%;">®</span></sup>
</td>
<td id="S1.T1.3.3.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S1.T1.3.3.4.1" class="ltx_text" style="font-size:80%;">149k</span></td>
<td id="S1.T1.3.3.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t"><span id="S1.T1.3.3.5.1" class="ltx_text" style="font-size:80%;">227k</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<ul id="S1.I1" class="ltx_itemize ltx_centering ltx_figure_panel">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">*</span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p"><span id="S1.I1.ix1.p1.1.1" class="ltx_text" style="font-size:80%;">Mixture: Radiology, Pathology, Microscopy, Signals, Generic biomedical illlustrations, </span><span id="S1.I1.ix1.p1.1.2" class="ltx_text ltx_font_italic" style="font-size:80%;">etc</span><span id="S1.I1.ix1.p1.1.3" class="ltx_text" style="font-size:80%;">.</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_flex_break"></div>
</div>
</figure>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2305.10415/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="88" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The top 20 figure types in PMC-VQA, cover a wide range of diagnostic procedures.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Here, we start with an introduction to the problem of generative medical visual question answering in Sec. <a href="#S2.SS1" title="2.1 Problem Formulation ‣ 2 Method ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a>;
then we present the architecture detail in Sec. <a href="#S2.SS2" title="2.2 Architecture ‣ 2 Method ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
<figure id="S2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S2.F2.sf1.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:203.8pt;">
<img src="/html/2305.10415/assets/x2.png" id="S2.F2.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="295" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S2.F2.sf1.4.2" class="ltx_text" style="font-size:80%;">Overall architecture of MedVInT</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S2.F2.sf2.1" class="ltx_block ltx_minipage ltx_align_bottom" style="width:203.8pt;">
<img src="/html/2305.10415/assets/x3.png" id="S2.F2.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="295" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S2.F2.sf2.4.2" class="ltx_text" style="font-size:80%;">Pipeline for PMC-VQA generation</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) The proposed architecture of MedVInt, mainly consists of three components: a visual encoder to extract visual features, a language encoder to encode textual context, and a multimodal decoder to generate the answer; (b) The proposed question-answer pairs generation pipeline.</figcaption>
</figure>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Problem Formulation</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.7" class="ltx_p">MedVQA is a task of answering natural language questions about medical visual content,
typically images or videos obtained from medical devices like X-ray, CT, MRI, or microscopy, <span id="S2.SS1.p1.7.1" class="ltx_text ltx_font_italic">etc.</span>
Specifically, our goal is to train a model that can output the corresponding answer for a given question, which can be expressed as:</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.6" class="ltx_Math" alttext="\hat{a}_{i}=\Phi_{\text{MedVQA}}(\mathcal{I}_{i},q_{i};\Theta)=\Phi_{\text{dec}}(\Phi_{\text{vis}}(\mathcal{I}_{i};\theta_{\text{vis}}),\Phi_{\text{text}}(q_{i};\theta_{\text{text}});\theta_{\text{dec}})" display="block"><semantics id="S2.E1.m1.6a"><mrow id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml"><msub id="S2.E1.m1.6.6.7" xref="S2.E1.m1.6.6.7.cmml"><mover accent="true" id="S2.E1.m1.6.6.7.2" xref="S2.E1.m1.6.6.7.2.cmml"><mi id="S2.E1.m1.6.6.7.2.2" xref="S2.E1.m1.6.6.7.2.2.cmml">a</mi><mo id="S2.E1.m1.6.6.7.2.1" xref="S2.E1.m1.6.6.7.2.1.cmml">^</mo></mover><mi id="S2.E1.m1.6.6.7.3" xref="S2.E1.m1.6.6.7.3.cmml">i</mi></msub><mo id="S2.E1.m1.6.6.8" xref="S2.E1.m1.6.6.8.cmml">=</mo><mrow id="S2.E1.m1.3.3.2" xref="S2.E1.m1.3.3.2.cmml"><msub id="S2.E1.m1.3.3.2.4" xref="S2.E1.m1.3.3.2.4.cmml"><mi mathvariant="normal" id="S2.E1.m1.3.3.2.4.2" xref="S2.E1.m1.3.3.2.4.2.cmml">Φ</mi><mtext id="S2.E1.m1.3.3.2.4.3" xref="S2.E1.m1.3.3.2.4.3a.cmml">MedVQA</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.3.3.2.3" xref="S2.E1.m1.3.3.2.3.cmml">​</mo><mrow id="S2.E1.m1.3.3.2.2.2" xref="S2.E1.m1.3.3.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.3.3.2.2.2.3" xref="S2.E1.m1.3.3.2.2.3.cmml">(</mo><msub id="S2.E1.m1.2.2.1.1.1.1" xref="S2.E1.m1.2.2.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.1.1.1.1.2.cmml">ℐ</mi><mi id="S2.E1.m1.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.3.3.2.2.2.4" xref="S2.E1.m1.3.3.2.2.3.cmml">,</mo><msub id="S2.E1.m1.3.3.2.2.2.2" xref="S2.E1.m1.3.3.2.2.2.2.cmml"><mi id="S2.E1.m1.3.3.2.2.2.2.2" xref="S2.E1.m1.3.3.2.2.2.2.2.cmml">q</mi><mi id="S2.E1.m1.3.3.2.2.2.2.3" xref="S2.E1.m1.3.3.2.2.2.2.3.cmml">i</mi></msub><mo id="S2.E1.m1.3.3.2.2.2.5" xref="S2.E1.m1.3.3.2.2.3.cmml">;</mo><mi mathvariant="normal" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">Θ</mi><mo stretchy="false" id="S2.E1.m1.3.3.2.2.2.6" xref="S2.E1.m1.3.3.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.6.6.9" xref="S2.E1.m1.6.6.9.cmml">=</mo><mrow id="S2.E1.m1.6.6.5" xref="S2.E1.m1.6.6.5.cmml"><msub id="S2.E1.m1.6.6.5.5" xref="S2.E1.m1.6.6.5.5.cmml"><mi mathvariant="normal" id="S2.E1.m1.6.6.5.5.2" xref="S2.E1.m1.6.6.5.5.2.cmml">Φ</mi><mtext id="S2.E1.m1.6.6.5.5.3" xref="S2.E1.m1.6.6.5.5.3a.cmml">dec</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.6.6.5.4" xref="S2.E1.m1.6.6.5.4.cmml">​</mo><mrow id="S2.E1.m1.6.6.5.3.3" xref="S2.E1.m1.6.6.5.3.4.cmml"><mo stretchy="false" id="S2.E1.m1.6.6.5.3.3.4" xref="S2.E1.m1.6.6.5.3.4.cmml">(</mo><mrow id="S2.E1.m1.4.4.3.1.1.1" xref="S2.E1.m1.4.4.3.1.1.1.cmml"><msub id="S2.E1.m1.4.4.3.1.1.1.4" xref="S2.E1.m1.4.4.3.1.1.1.4.cmml"><mi mathvariant="normal" id="S2.E1.m1.4.4.3.1.1.1.4.2" xref="S2.E1.m1.4.4.3.1.1.1.4.2.cmml">Φ</mi><mtext id="S2.E1.m1.4.4.3.1.1.1.4.3" xref="S2.E1.m1.4.4.3.1.1.1.4.3a.cmml">vis</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.3.1.1.1.3" xref="S2.E1.m1.4.4.3.1.1.1.3.cmml">​</mo><mrow id="S2.E1.m1.4.4.3.1.1.1.2.2" xref="S2.E1.m1.4.4.3.1.1.1.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.4.4.3.1.1.1.2.2.3" xref="S2.E1.m1.4.4.3.1.1.1.2.3.cmml">(</mo><msub id="S2.E1.m1.4.4.3.1.1.1.1.1.1" xref="S2.E1.m1.4.4.3.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.4.4.3.1.1.1.1.1.1.2" xref="S2.E1.m1.4.4.3.1.1.1.1.1.1.2.cmml">ℐ</mi><mi id="S2.E1.m1.4.4.3.1.1.1.1.1.1.3" xref="S2.E1.m1.4.4.3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.4.4.3.1.1.1.2.2.4" xref="S2.E1.m1.4.4.3.1.1.1.2.3.cmml">;</mo><msub id="S2.E1.m1.4.4.3.1.1.1.2.2.2" xref="S2.E1.m1.4.4.3.1.1.1.2.2.2.cmml"><mi id="S2.E1.m1.4.4.3.1.1.1.2.2.2.2" xref="S2.E1.m1.4.4.3.1.1.1.2.2.2.2.cmml">θ</mi><mtext id="S2.E1.m1.4.4.3.1.1.1.2.2.2.3" xref="S2.E1.m1.4.4.3.1.1.1.2.2.2.3a.cmml">vis</mtext></msub><mo stretchy="false" id="S2.E1.m1.4.4.3.1.1.1.2.2.5" xref="S2.E1.m1.4.4.3.1.1.1.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.6.6.5.3.3.5" xref="S2.E1.m1.6.6.5.3.4.cmml">,</mo><mrow id="S2.E1.m1.5.5.4.2.2.2" xref="S2.E1.m1.5.5.4.2.2.2.cmml"><msub id="S2.E1.m1.5.5.4.2.2.2.4" xref="S2.E1.m1.5.5.4.2.2.2.4.cmml"><mi mathvariant="normal" id="S2.E1.m1.5.5.4.2.2.2.4.2" xref="S2.E1.m1.5.5.4.2.2.2.4.2.cmml">Φ</mi><mtext id="S2.E1.m1.5.5.4.2.2.2.4.3" xref="S2.E1.m1.5.5.4.2.2.2.4.3a.cmml">text</mtext></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.5.5.4.2.2.2.3" xref="S2.E1.m1.5.5.4.2.2.2.3.cmml">​</mo><mrow id="S2.E1.m1.5.5.4.2.2.2.2.2" xref="S2.E1.m1.5.5.4.2.2.2.2.3.cmml"><mo stretchy="false" id="S2.E1.m1.5.5.4.2.2.2.2.2.3" xref="S2.E1.m1.5.5.4.2.2.2.2.3.cmml">(</mo><msub id="S2.E1.m1.5.5.4.2.2.2.1.1.1" xref="S2.E1.m1.5.5.4.2.2.2.1.1.1.cmml"><mi id="S2.E1.m1.5.5.4.2.2.2.1.1.1.2" xref="S2.E1.m1.5.5.4.2.2.2.1.1.1.2.cmml">q</mi><mi id="S2.E1.m1.5.5.4.2.2.2.1.1.1.3" xref="S2.E1.m1.5.5.4.2.2.2.1.1.1.3.cmml">i</mi></msub><mo id="S2.E1.m1.5.5.4.2.2.2.2.2.4" xref="S2.E1.m1.5.5.4.2.2.2.2.3.cmml">;</mo><msub id="S2.E1.m1.5.5.4.2.2.2.2.2.2" xref="S2.E1.m1.5.5.4.2.2.2.2.2.2.cmml"><mi id="S2.E1.m1.5.5.4.2.2.2.2.2.2.2" xref="S2.E1.m1.5.5.4.2.2.2.2.2.2.2.cmml">θ</mi><mtext id="S2.E1.m1.5.5.4.2.2.2.2.2.2.3" xref="S2.E1.m1.5.5.4.2.2.2.2.2.2.3a.cmml">text</mtext></msub><mo stretchy="false" id="S2.E1.m1.5.5.4.2.2.2.2.2.5" xref="S2.E1.m1.5.5.4.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.6.6.5.3.3.6" xref="S2.E1.m1.6.6.5.3.4.cmml">;</mo><msub id="S2.E1.m1.6.6.5.3.3.3" xref="S2.E1.m1.6.6.5.3.3.3.cmml"><mi id="S2.E1.m1.6.6.5.3.3.3.2" xref="S2.E1.m1.6.6.5.3.3.3.2.cmml">θ</mi><mtext id="S2.E1.m1.6.6.5.3.3.3.3" xref="S2.E1.m1.6.6.5.3.3.3.3a.cmml">dec</mtext></msub><mo stretchy="false" id="S2.E1.m1.6.6.5.3.3.7" xref="S2.E1.m1.6.6.5.3.4.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.6b"><apply id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6"><and id="S2.E1.m1.6.6a.cmml" xref="S2.E1.m1.6.6"></and><apply id="S2.E1.m1.6.6b.cmml" xref="S2.E1.m1.6.6"><eq id="S2.E1.m1.6.6.8.cmml" xref="S2.E1.m1.6.6.8"></eq><apply id="S2.E1.m1.6.6.7.cmml" xref="S2.E1.m1.6.6.7"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.7.1.cmml" xref="S2.E1.m1.6.6.7">subscript</csymbol><apply id="S2.E1.m1.6.6.7.2.cmml" xref="S2.E1.m1.6.6.7.2"><ci id="S2.E1.m1.6.6.7.2.1.cmml" xref="S2.E1.m1.6.6.7.2.1">^</ci><ci id="S2.E1.m1.6.6.7.2.2.cmml" xref="S2.E1.m1.6.6.7.2.2">𝑎</ci></apply><ci id="S2.E1.m1.6.6.7.3.cmml" xref="S2.E1.m1.6.6.7.3">𝑖</ci></apply><apply id="S2.E1.m1.3.3.2.cmml" xref="S2.E1.m1.3.3.2"><times id="S2.E1.m1.3.3.2.3.cmml" xref="S2.E1.m1.3.3.2.3"></times><apply id="S2.E1.m1.3.3.2.4.cmml" xref="S2.E1.m1.3.3.2.4"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.2.4.1.cmml" xref="S2.E1.m1.3.3.2.4">subscript</csymbol><ci id="S2.E1.m1.3.3.2.4.2.cmml" xref="S2.E1.m1.3.3.2.4.2">Φ</ci><ci id="S2.E1.m1.3.3.2.4.3a.cmml" xref="S2.E1.m1.3.3.2.4.3"><mtext mathsize="70%" id="S2.E1.m1.3.3.2.4.3.cmml" xref="S2.E1.m1.3.3.2.4.3">MedVQA</mtext></ci></apply><vector id="S2.E1.m1.3.3.2.2.3.cmml" xref="S2.E1.m1.3.3.2.2.2"><apply id="S2.E1.m1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.1.1.1.1.2">ℐ</ci><ci id="S2.E1.m1.2.2.1.1.1.1.3.cmml" xref="S2.E1.m1.2.2.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E1.m1.3.3.2.2.2.2.cmml" xref="S2.E1.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.2.2.2.2.1.cmml" xref="S2.E1.m1.3.3.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.3.3.2.2.2.2.2.cmml" xref="S2.E1.m1.3.3.2.2.2.2.2">𝑞</ci><ci id="S2.E1.m1.3.3.2.2.2.2.3.cmml" xref="S2.E1.m1.3.3.2.2.2.2.3">𝑖</ci></apply><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">Θ</ci></vector></apply></apply><apply id="S2.E1.m1.6.6c.cmml" xref="S2.E1.m1.6.6"><eq id="S2.E1.m1.6.6.9.cmml" xref="S2.E1.m1.6.6.9"></eq><share href="#S2.E1.m1.3.3.2.cmml" id="S2.E1.m1.6.6d.cmml" xref="S2.E1.m1.6.6"></share><apply id="S2.E1.m1.6.6.5.cmml" xref="S2.E1.m1.6.6.5"><times id="S2.E1.m1.6.6.5.4.cmml" xref="S2.E1.m1.6.6.5.4"></times><apply id="S2.E1.m1.6.6.5.5.cmml" xref="S2.E1.m1.6.6.5.5"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.5.5.1.cmml" xref="S2.E1.m1.6.6.5.5">subscript</csymbol><ci id="S2.E1.m1.6.6.5.5.2.cmml" xref="S2.E1.m1.6.6.5.5.2">Φ</ci><ci id="S2.E1.m1.6.6.5.5.3a.cmml" xref="S2.E1.m1.6.6.5.5.3"><mtext mathsize="70%" id="S2.E1.m1.6.6.5.5.3.cmml" xref="S2.E1.m1.6.6.5.5.3">dec</mtext></ci></apply><vector id="S2.E1.m1.6.6.5.3.4.cmml" xref="S2.E1.m1.6.6.5.3.3"><apply id="S2.E1.m1.4.4.3.1.1.1.cmml" xref="S2.E1.m1.4.4.3.1.1.1"><times id="S2.E1.m1.4.4.3.1.1.1.3.cmml" xref="S2.E1.m1.4.4.3.1.1.1.3"></times><apply id="S2.E1.m1.4.4.3.1.1.1.4.cmml" xref="S2.E1.m1.4.4.3.1.1.1.4"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.3.1.1.1.4.1.cmml" xref="S2.E1.m1.4.4.3.1.1.1.4">subscript</csymbol><ci id="S2.E1.m1.4.4.3.1.1.1.4.2.cmml" xref="S2.E1.m1.4.4.3.1.1.1.4.2">Φ</ci><ci id="S2.E1.m1.4.4.3.1.1.1.4.3a.cmml" xref="S2.E1.m1.4.4.3.1.1.1.4.3"><mtext mathsize="70%" id="S2.E1.m1.4.4.3.1.1.1.4.3.cmml" xref="S2.E1.m1.4.4.3.1.1.1.4.3">vis</mtext></ci></apply><list id="S2.E1.m1.4.4.3.1.1.1.2.3.cmml" xref="S2.E1.m1.4.4.3.1.1.1.2.2"><apply id="S2.E1.m1.4.4.3.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.3.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.4.4.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.4.4.3.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.4.4.3.1.1.1.1.1.1.2">ℐ</ci><ci id="S2.E1.m1.4.4.3.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.4.4.3.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S2.E1.m1.4.4.3.1.1.1.2.2.2.cmml" xref="S2.E1.m1.4.4.3.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.3.1.1.1.2.2.2.1.cmml" xref="S2.E1.m1.4.4.3.1.1.1.2.2.2">subscript</csymbol><ci id="S2.E1.m1.4.4.3.1.1.1.2.2.2.2.cmml" xref="S2.E1.m1.4.4.3.1.1.1.2.2.2.2">𝜃</ci><ci id="S2.E1.m1.4.4.3.1.1.1.2.2.2.3a.cmml" xref="S2.E1.m1.4.4.3.1.1.1.2.2.2.3"><mtext mathsize="70%" id="S2.E1.m1.4.4.3.1.1.1.2.2.2.3.cmml" xref="S2.E1.m1.4.4.3.1.1.1.2.2.2.3">vis</mtext></ci></apply></list></apply><apply id="S2.E1.m1.5.5.4.2.2.2.cmml" xref="S2.E1.m1.5.5.4.2.2.2"><times id="S2.E1.m1.5.5.4.2.2.2.3.cmml" xref="S2.E1.m1.5.5.4.2.2.2.3"></times><apply id="S2.E1.m1.5.5.4.2.2.2.4.cmml" xref="S2.E1.m1.5.5.4.2.2.2.4"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.4.2.2.2.4.1.cmml" xref="S2.E1.m1.5.5.4.2.2.2.4">subscript</csymbol><ci id="S2.E1.m1.5.5.4.2.2.2.4.2.cmml" xref="S2.E1.m1.5.5.4.2.2.2.4.2">Φ</ci><ci id="S2.E1.m1.5.5.4.2.2.2.4.3a.cmml" xref="S2.E1.m1.5.5.4.2.2.2.4.3"><mtext mathsize="70%" id="S2.E1.m1.5.5.4.2.2.2.4.3.cmml" xref="S2.E1.m1.5.5.4.2.2.2.4.3">text</mtext></ci></apply><list id="S2.E1.m1.5.5.4.2.2.2.2.3.cmml" xref="S2.E1.m1.5.5.4.2.2.2.2.2"><apply id="S2.E1.m1.5.5.4.2.2.2.1.1.1.cmml" xref="S2.E1.m1.5.5.4.2.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.4.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.5.5.4.2.2.2.1.1.1">subscript</csymbol><ci id="S2.E1.m1.5.5.4.2.2.2.1.1.1.2.cmml" xref="S2.E1.m1.5.5.4.2.2.2.1.1.1.2">𝑞</ci><ci id="S2.E1.m1.5.5.4.2.2.2.1.1.1.3.cmml" xref="S2.E1.m1.5.5.4.2.2.2.1.1.1.3">𝑖</ci></apply><apply id="S2.E1.m1.5.5.4.2.2.2.2.2.2.cmml" xref="S2.E1.m1.5.5.4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.5.5.4.2.2.2.2.2.2.1.cmml" xref="S2.E1.m1.5.5.4.2.2.2.2.2.2">subscript</csymbol><ci id="S2.E1.m1.5.5.4.2.2.2.2.2.2.2.cmml" xref="S2.E1.m1.5.5.4.2.2.2.2.2.2.2">𝜃</ci><ci id="S2.E1.m1.5.5.4.2.2.2.2.2.2.3a.cmml" xref="S2.E1.m1.5.5.4.2.2.2.2.2.2.3"><mtext mathsize="70%" id="S2.E1.m1.5.5.4.2.2.2.2.2.2.3.cmml" xref="S2.E1.m1.5.5.4.2.2.2.2.2.2.3">text</mtext></ci></apply></list></apply><apply id="S2.E1.m1.6.6.5.3.3.3.cmml" xref="S2.E1.m1.6.6.5.3.3.3"><csymbol cd="ambiguous" id="S2.E1.m1.6.6.5.3.3.3.1.cmml" xref="S2.E1.m1.6.6.5.3.3.3">subscript</csymbol><ci id="S2.E1.m1.6.6.5.3.3.3.2.cmml" xref="S2.E1.m1.6.6.5.3.3.3.2">𝜃</ci><ci id="S2.E1.m1.6.6.5.3.3.3.3a.cmml" xref="S2.E1.m1.6.6.5.3.3.3.3"><mtext mathsize="70%" id="S2.E1.m1.6.6.5.3.3.3.3.cmml" xref="S2.E1.m1.6.6.5.3.3.3.3">dec</mtext></ci></apply></vector></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.6c">\hat{a}_{i}=\Phi_{\text{MedVQA}}(\mathcal{I}_{i},q_{i};\Theta)=\Phi_{\text{dec}}(\Phi_{\text{vis}}(\mathcal{I}_{i};\theta_{\text{vis}}),\Phi_{\text{text}}(q_{i};\theta_{\text{text}});\theta_{\text{dec}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p1.6" class="ltx_p">Here, <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="\hat{a}_{i}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msub id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mover accent="true" id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml"><mi id="S2.SS1.p1.1.m1.1.1.2.2" xref="S2.SS1.p1.1.m1.1.1.2.2.cmml">a</mi><mo id="S2.SS1.p1.1.m1.1.1.2.1" xref="S2.SS1.p1.1.m1.1.1.2.1.cmml">^</mo></mover><mi id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">subscript</csymbol><apply id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2"><ci id="S2.SS1.p1.1.m1.1.1.2.1.cmml" xref="S2.SS1.p1.1.m1.1.1.2.1">^</ci><ci id="S2.SS1.p1.1.m1.1.1.2.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2.2">𝑎</ci></apply><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\hat{a}_{i}</annotation></semantics></math> refers to the predicted answer,
<math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="\mathcal{I}_{i}\in\mathbb{R}^{H\times W\times C}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><msub id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1.2.2" xref="S2.SS1.p1.2.m2.1.1.2.2.cmml">ℐ</mi><mi id="S2.SS1.p1.2.m2.1.1.2.3" xref="S2.SS1.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.2" xref="S2.SS1.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p1.2.m2.1.1.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S2.SS1.p1.2.m2.1.1.3.3.2" xref="S2.SS1.p1.2.m2.1.1.3.3.2.cmml">H</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.2.m2.1.1.3.3.1" xref="S2.SS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p1.2.m2.1.1.3.3.3" xref="S2.SS1.p1.2.m2.1.1.3.3.3.cmml">W</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS1.p1.2.m2.1.1.3.3.1a" xref="S2.SS1.p1.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p1.2.m2.1.1.3.3.4" xref="S2.SS1.p1.2.m2.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><in id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></in><apply id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2">ℐ</ci><ci id="S2.SS1.p1.2.m2.1.1.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.2">ℝ</ci><apply id="S2.SS1.p1.2.m2.1.1.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3"><times id="S2.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.1"></times><ci id="S2.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.2">𝐻</ci><ci id="S2.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.3">𝑊</ci><ci id="S2.SS1.p1.2.m2.1.1.3.3.4.cmml" xref="S2.SS1.p1.2.m2.1.1.3.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathcal{I}_{i}\in\mathbb{R}^{H\times W\times C}</annotation></semantics></math> refers to the visual image, <math id="S2.SS1.p1.3.m3.3" class="ltx_Math" alttext="H,W,C" display="inline"><semantics id="S2.SS1.p1.3.m3.3a"><mrow id="S2.SS1.p1.3.m3.3.4.2" xref="S2.SS1.p1.3.m3.3.4.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">H</mi><mo id="S2.SS1.p1.3.m3.3.4.2.1" xref="S2.SS1.p1.3.m3.3.4.1.cmml">,</mo><mi id="S2.SS1.p1.3.m3.2.2" xref="S2.SS1.p1.3.m3.2.2.cmml">W</mi><mo id="S2.SS1.p1.3.m3.3.4.2.2" xref="S2.SS1.p1.3.m3.3.4.1.cmml">,</mo><mi id="S2.SS1.p1.3.m3.3.3" xref="S2.SS1.p1.3.m3.3.3.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.3b"><list id="S2.SS1.p1.3.m3.3.4.1.cmml" xref="S2.SS1.p1.3.m3.3.4.2"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝐻</ci><ci id="S2.SS1.p1.3.m3.2.2.cmml" xref="S2.SS1.p1.3.m3.2.2">𝑊</ci><ci id="S2.SS1.p1.3.m3.3.3.cmml" xref="S2.SS1.p1.3.m3.3.3">𝐶</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.3c">H,W,C</annotation></semantics></math> are height, width, channel respectively.
The posed question and corresponding ground-truth answer in the form of natural language are denoted as <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><msub id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">q</mi><mi id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">𝑞</ci><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">q_{i}</annotation></semantics></math> and <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">a</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝑎</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">a_{i}</annotation></semantics></math>, respectively. <math id="S2.SS1.p1.6.m6.3" class="ltx_Math" alttext="\Theta=\{\theta_{\text{vis}},\theta_{\text{text}},\theta_{\text{dec}}\}" display="inline"><semantics id="S2.SS1.p1.6.m6.3a"><mrow id="S2.SS1.p1.6.m6.3.3" xref="S2.SS1.p1.6.m6.3.3.cmml"><mi mathvariant="normal" id="S2.SS1.p1.6.m6.3.3.5" xref="S2.SS1.p1.6.m6.3.3.5.cmml">Θ</mi><mo id="S2.SS1.p1.6.m6.3.3.4" xref="S2.SS1.p1.6.m6.3.3.4.cmml">=</mo><mrow id="S2.SS1.p1.6.m6.3.3.3.3" xref="S2.SS1.p1.6.m6.3.3.3.4.cmml"><mo stretchy="false" id="S2.SS1.p1.6.m6.3.3.3.3.4" xref="S2.SS1.p1.6.m6.3.3.3.4.cmml">{</mo><msub id="S2.SS1.p1.6.m6.1.1.1.1.1" xref="S2.SS1.p1.6.m6.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.1.1.1.2" xref="S2.SS1.p1.6.m6.1.1.1.1.1.2.cmml">θ</mi><mtext id="S2.SS1.p1.6.m6.1.1.1.1.1.3" xref="S2.SS1.p1.6.m6.1.1.1.1.1.3a.cmml">vis</mtext></msub><mo id="S2.SS1.p1.6.m6.3.3.3.3.5" xref="S2.SS1.p1.6.m6.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.6.m6.2.2.2.2.2" xref="S2.SS1.p1.6.m6.2.2.2.2.2.cmml"><mi id="S2.SS1.p1.6.m6.2.2.2.2.2.2" xref="S2.SS1.p1.6.m6.2.2.2.2.2.2.cmml">θ</mi><mtext id="S2.SS1.p1.6.m6.2.2.2.2.2.3" xref="S2.SS1.p1.6.m6.2.2.2.2.2.3a.cmml">text</mtext></msub><mo id="S2.SS1.p1.6.m6.3.3.3.3.6" xref="S2.SS1.p1.6.m6.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.6.m6.3.3.3.3.3" xref="S2.SS1.p1.6.m6.3.3.3.3.3.cmml"><mi id="S2.SS1.p1.6.m6.3.3.3.3.3.2" xref="S2.SS1.p1.6.m6.3.3.3.3.3.2.cmml">θ</mi><mtext id="S2.SS1.p1.6.m6.3.3.3.3.3.3" xref="S2.SS1.p1.6.m6.3.3.3.3.3.3a.cmml">dec</mtext></msub><mo stretchy="false" id="S2.SS1.p1.6.m6.3.3.3.3.7" xref="S2.SS1.p1.6.m6.3.3.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.3b"><apply id="S2.SS1.p1.6.m6.3.3.cmml" xref="S2.SS1.p1.6.m6.3.3"><eq id="S2.SS1.p1.6.m6.3.3.4.cmml" xref="S2.SS1.p1.6.m6.3.3.4"></eq><ci id="S2.SS1.p1.6.m6.3.3.5.cmml" xref="S2.SS1.p1.6.m6.3.3.5">Θ</ci><set id="S2.SS1.p1.6.m6.3.3.3.4.cmml" xref="S2.SS1.p1.6.m6.3.3.3.3"><apply id="S2.SS1.p1.6.m6.1.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1.1.2">𝜃</ci><ci id="S2.SS1.p1.6.m6.1.1.1.1.1.3a.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1.1.3"><mtext mathsize="70%" id="S2.SS1.p1.6.m6.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1.1.3">vis</mtext></ci></apply><apply id="S2.SS1.p1.6.m6.2.2.2.2.2.cmml" xref="S2.SS1.p1.6.m6.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.2.2.2.2.2.1.cmml" xref="S2.SS1.p1.6.m6.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.6.m6.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.6.m6.2.2.2.2.2.2">𝜃</ci><ci id="S2.SS1.p1.6.m6.2.2.2.2.2.3a.cmml" xref="S2.SS1.p1.6.m6.2.2.2.2.2.3"><mtext mathsize="70%" id="S2.SS1.p1.6.m6.2.2.2.2.2.3.cmml" xref="S2.SS1.p1.6.m6.2.2.2.2.2.3">text</mtext></ci></apply><apply id="S2.SS1.p1.6.m6.3.3.3.3.3.cmml" xref="S2.SS1.p1.6.m6.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.3.3.3.3.3.1.cmml" xref="S2.SS1.p1.6.m6.3.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.6.m6.3.3.3.3.3.2.cmml" xref="S2.SS1.p1.6.m6.3.3.3.3.3.2">𝜃</ci><ci id="S2.SS1.p1.6.m6.3.3.3.3.3.3a.cmml" xref="S2.SS1.p1.6.m6.3.3.3.3.3.3"><mtext mathsize="70%" id="S2.SS1.p1.6.m6.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.6.m6.3.3.3.3.3.3">dec</mtext></ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.3c">\Theta=\{\theta_{\text{vis}},\theta_{\text{text}},\theta_{\text{dec}}\}</annotation></semantics></math> denote the trainable parameters.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.2" class="ltx_p">Existing approaches have primarily treated medical VQA as a classification problem, with the goal of selecting the correct answer from a candidate set, <span id="S2.SS1.p2.2.1" class="ltx_text ltx_font_italic">i.e.</span>, <math id="S2.SS1.p2.1.m1.4" class="ltx_Math" alttext="a_{i}\in\Omega=\{a_{1},a_{2},\dots,a_{N}\}" display="inline"><semantics id="S2.SS1.p2.1.m1.4a"><mrow id="S2.SS1.p2.1.m1.4.4" xref="S2.SS1.p2.1.m1.4.4.cmml"><msub id="S2.SS1.p2.1.m1.4.4.5" xref="S2.SS1.p2.1.m1.4.4.5.cmml"><mi id="S2.SS1.p2.1.m1.4.4.5.2" xref="S2.SS1.p2.1.m1.4.4.5.2.cmml">a</mi><mi id="S2.SS1.p2.1.m1.4.4.5.3" xref="S2.SS1.p2.1.m1.4.4.5.3.cmml">i</mi></msub><mo id="S2.SS1.p2.1.m1.4.4.6" xref="S2.SS1.p2.1.m1.4.4.6.cmml">∈</mo><mi mathvariant="normal" id="S2.SS1.p2.1.m1.4.4.7" xref="S2.SS1.p2.1.m1.4.4.7.cmml">Ω</mi><mo id="S2.SS1.p2.1.m1.4.4.8" xref="S2.SS1.p2.1.m1.4.4.8.cmml">=</mo><mrow id="S2.SS1.p2.1.m1.4.4.3.3" xref="S2.SS1.p2.1.m1.4.4.3.4.cmml"><mo stretchy="false" id="S2.SS1.p2.1.m1.4.4.3.3.4" xref="S2.SS1.p2.1.m1.4.4.3.4.cmml">{</mo><msub id="S2.SS1.p2.1.m1.2.2.1.1.1" xref="S2.SS1.p2.1.m1.2.2.1.1.1.cmml"><mi id="S2.SS1.p2.1.m1.2.2.1.1.1.2" xref="S2.SS1.p2.1.m1.2.2.1.1.1.2.cmml">a</mi><mn id="S2.SS1.p2.1.m1.2.2.1.1.1.3" xref="S2.SS1.p2.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS1.p2.1.m1.4.4.3.3.5" xref="S2.SS1.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p2.1.m1.3.3.2.2.2" xref="S2.SS1.p2.1.m1.3.3.2.2.2.cmml"><mi id="S2.SS1.p2.1.m1.3.3.2.2.2.2" xref="S2.SS1.p2.1.m1.3.3.2.2.2.2.cmml">a</mi><mn id="S2.SS1.p2.1.m1.3.3.2.2.2.3" xref="S2.SS1.p2.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS1.p2.1.m1.4.4.3.3.6" xref="S2.SS1.p2.1.m1.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">…</mi><mo id="S2.SS1.p2.1.m1.4.4.3.3.7" xref="S2.SS1.p2.1.m1.4.4.3.4.cmml">,</mo><msub id="S2.SS1.p2.1.m1.4.4.3.3.3" xref="S2.SS1.p2.1.m1.4.4.3.3.3.cmml"><mi id="S2.SS1.p2.1.m1.4.4.3.3.3.2" xref="S2.SS1.p2.1.m1.4.4.3.3.3.2.cmml">a</mi><mi id="S2.SS1.p2.1.m1.4.4.3.3.3.3" xref="S2.SS1.p2.1.m1.4.4.3.3.3.3.cmml">N</mi></msub><mo stretchy="false" id="S2.SS1.p2.1.m1.4.4.3.3.8" xref="S2.SS1.p2.1.m1.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.4b"><apply id="S2.SS1.p2.1.m1.4.4.cmml" xref="S2.SS1.p2.1.m1.4.4"><and id="S2.SS1.p2.1.m1.4.4a.cmml" xref="S2.SS1.p2.1.m1.4.4"></and><apply id="S2.SS1.p2.1.m1.4.4b.cmml" xref="S2.SS1.p2.1.m1.4.4"><in id="S2.SS1.p2.1.m1.4.4.6.cmml" xref="S2.SS1.p2.1.m1.4.4.6"></in><apply id="S2.SS1.p2.1.m1.4.4.5.cmml" xref="S2.SS1.p2.1.m1.4.4.5"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.4.4.5.1.cmml" xref="S2.SS1.p2.1.m1.4.4.5">subscript</csymbol><ci id="S2.SS1.p2.1.m1.4.4.5.2.cmml" xref="S2.SS1.p2.1.m1.4.4.5.2">𝑎</ci><ci id="S2.SS1.p2.1.m1.4.4.5.3.cmml" xref="S2.SS1.p2.1.m1.4.4.5.3">𝑖</ci></apply><ci id="S2.SS1.p2.1.m1.4.4.7.cmml" xref="S2.SS1.p2.1.m1.4.4.7">Ω</ci></apply><apply id="S2.SS1.p2.1.m1.4.4c.cmml" xref="S2.SS1.p2.1.m1.4.4"><eq id="S2.SS1.p2.1.m1.4.4.8.cmml" xref="S2.SS1.p2.1.m1.4.4.8"></eq><share href="#S2.SS1.p2.1.m1.4.4.7.cmml" id="S2.SS1.p2.1.m1.4.4d.cmml" xref="S2.SS1.p2.1.m1.4.4"></share><set id="S2.SS1.p2.1.m1.4.4.3.4.cmml" xref="S2.SS1.p2.1.m1.4.4.3.3"><apply id="S2.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="S2.SS1.p2.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p2.1.m1.2.2.1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.2.2.1.1.1.2">𝑎</ci><cn type="integer" id="S2.SS1.p2.1.m1.2.2.1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S2.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="S2.SS1.p2.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.3.3.2.2.2.1.cmml" xref="S2.SS1.p2.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p2.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS1.p2.1.m1.3.3.2.2.2.2">𝑎</ci><cn type="integer" id="S2.SS1.p2.1.m1.3.3.2.2.2.3.cmml" xref="S2.SS1.p2.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">…</ci><apply id="S2.SS1.p2.1.m1.4.4.3.3.3.cmml" xref="S2.SS1.p2.1.m1.4.4.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.4.4.3.3.3.1.cmml" xref="S2.SS1.p2.1.m1.4.4.3.3.3">subscript</csymbol><ci id="S2.SS1.p2.1.m1.4.4.3.3.3.2.cmml" xref="S2.SS1.p2.1.m1.4.4.3.3.3.2">𝑎</ci><ci id="S2.SS1.p2.1.m1.4.4.3.3.3.3.cmml" xref="S2.SS1.p2.1.m1.4.4.3.3.3.3">𝑁</ci></apply></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.4c">a_{i}\in\Omega=\{a_{1},a_{2},\dots,a_{N}\}</annotation></semantics></math>, where <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">N</annotation></semantics></math> represents the total number of answers within the dataset. Consequently,
this approach limits the system’s utility to predefined outcomes, hampering its free-form user-machine interaction potential.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.7" class="ltx_p">In this paper, we take an alternative approach,
with the goal to generate an open-ended answer in natural language.
Specifically, we train the system by maximizing the probability of generating the ground-truth answer given the input image and question. The loss function used to train the model is typically the negative log-likelihood of the correct next token in the sequence, summed over all time steps, which can be expressed as :</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.4" class="ltx_Math" alttext="\mathcal{L}(\Theta)=-\sum_{t=1}^{T}\log p(a^{t}|\mathcal{I},q^{1:T},a^{1:t-1};\Theta)" display="block"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.4.4.3.2" xref="S2.E2.m1.4.4.3.2.cmml">ℒ</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.3.1" xref="S2.E2.m1.4.4.3.1.cmml">​</mo><mrow id="S2.E2.m1.4.4.3.3.2" xref="S2.E2.m1.4.4.3.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.3.3.2.1" xref="S2.E2.m1.4.4.3.cmml">(</mo><mi mathvariant="normal" id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">Θ</mi><mo stretchy="false" id="S2.E2.m1.4.4.3.3.2.2" xref="S2.E2.m1.4.4.3.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.2.cmml">=</mo><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.cmml"><mo id="S2.E2.m1.4.4.1a" xref="S2.E2.m1.4.4.1.cmml">−</mo><mrow id="S2.E2.m1.4.4.1.1" xref="S2.E2.m1.4.4.1.1.cmml"><munderover id="S2.E2.m1.4.4.1.1.2" xref="S2.E2.m1.4.4.1.1.2.cmml"><mo movablelimits="false" id="S2.E2.m1.4.4.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.2.2.2.cmml">∑</mo><mrow id="S2.E2.m1.4.4.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.2.2.3.cmml"><mi id="S2.E2.m1.4.4.1.1.2.2.3.2" xref="S2.E2.m1.4.4.1.1.2.2.3.2.cmml">t</mi><mo id="S2.E2.m1.4.4.1.1.2.2.3.1" xref="S2.E2.m1.4.4.1.1.2.2.3.1.cmml">=</mo><mn id="S2.E2.m1.4.4.1.1.2.2.3.3" xref="S2.E2.m1.4.4.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2.m1.4.4.1.1.2.3" xref="S2.E2.m1.4.4.1.1.2.3.cmml">T</mi></munderover><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.1.cmml"><mrow id="S2.E2.m1.4.4.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.3.1" xref="S2.E2.m1.4.4.1.1.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S2.E2.m1.4.4.1.1.1.3a" xref="S2.E2.m1.4.4.1.1.1.3.cmml">⁡</mo><mi id="S2.E2.m1.4.4.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.3.2.cmml">p</mi></mrow><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.2.cmml">​</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml"><msup id="S2.E2.m1.4.4.1.1.1.1.1.1.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.4.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.4.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.4.2.cmml">a</mi><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.4.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.4.3.cmml">t</mi></msup><mo fence="false" id="S2.E2.m1.4.4.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml">|</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">ℐ</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml">,</mo><msup id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml">q</mi><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml"><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.cmml">T</mi></mrow></msup><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.4" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml">,</mo><msup id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.2.cmml">a</mi><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.cmml"><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.1.cmml">:</mo><mrow id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.2" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.2.cmml">t</mi><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.1" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.1.cmml">−</mo><mn id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></mrow></msup><mo id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.5" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml">;</mo><mi mathvariant="normal" id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">Θ</mi></mrow></mrow><mo stretchy="false" id="S2.E2.m1.4.4.1.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.2.cmml" xref="S2.E2.m1.4.4.2"></eq><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"><times id="S2.E2.m1.4.4.3.1.cmml" xref="S2.E2.m1.4.4.3.1"></times><ci id="S2.E2.m1.4.4.3.2.cmml" xref="S2.E2.m1.4.4.3.2">ℒ</ci><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">Θ</ci></apply><apply id="S2.E2.m1.4.4.1.cmml" xref="S2.E2.m1.4.4.1"><minus id="S2.E2.m1.4.4.1.2.cmml" xref="S2.E2.m1.4.4.1"></minus><apply id="S2.E2.m1.4.4.1.1.cmml" xref="S2.E2.m1.4.4.1.1"><apply id="S2.E2.m1.4.4.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.2">superscript</csymbol><apply id="S2.E2.m1.4.4.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.2">subscript</csymbol><sum id="S2.E2.m1.4.4.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.2.2.2"></sum><apply id="S2.E2.m1.4.4.1.1.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.2.2.3"><eq id="S2.E2.m1.4.4.1.1.2.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.2.2.3.1"></eq><ci id="S2.E2.m1.4.4.1.1.2.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.2.2.3.2">𝑡</ci><cn type="integer" id="S2.E2.m1.4.4.1.1.2.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.2.2.3.3">1</cn></apply></apply><ci id="S2.E2.m1.4.4.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.2.3">𝑇</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1"><times id="S2.E2.m1.4.4.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.2"></times><apply id="S2.E2.m1.4.4.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.3"><log id="S2.E2.m1.4.4.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.3.1"></log><ci id="S2.E2.m1.4.4.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.3.2">𝑝</ci></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.3">conditional</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.4.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.4"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.4.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.4">superscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.4.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.4.2">𝑎</ci><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.4.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.4.3">𝑡</ci></apply><list id="S2.E2.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2"><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">ℐ</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.2">𝑞</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3"><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.2">1</cn><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.1.1.1.3.3">𝑇</ci></apply></apply><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.2">𝑎</ci><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3"><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.1">:</ci><cn type="integer" id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.2">1</cn><apply id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3"><minus id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.1"></minus><ci id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.2">𝑡</ci><cn type="integer" id="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1.1.2.2.2.3.3.3">1</cn></apply></apply></apply><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">Θ</ci></list></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\mathcal{L}(\Theta)=-\sum_{t=1}^{T}\log p(a^{t}|\mathcal{I},q^{1:T},a^{1:t-1};\Theta)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.6" class="ltx_p">where <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="T" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">T</annotation></semantics></math> is the length of the ground-truth answer, and <math id="S2.SS1.p3.2.m2.3" class="ltx_Math" alttext="p(a^{t}|\mathcal{I},q^{1:T},a^{1:t-1};\Theta)" display="inline"><semantics id="S2.SS1.p3.2.m2.3a"><mrow id="S2.SS1.p3.2.m2.3.3" xref="S2.SS1.p3.2.m2.3.3.cmml"><mi id="S2.SS1.p3.2.m2.3.3.3" xref="S2.SS1.p3.2.m2.3.3.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p3.2.m2.3.3.2" xref="S2.SS1.p3.2.m2.3.3.2.cmml">​</mo><mrow id="S2.SS1.p3.2.m2.3.3.1.1" xref="S2.SS1.p3.2.m2.3.3.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p3.2.m2.3.3.1.1.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.cmml">(</mo><mrow id="S2.SS1.p3.2.m2.3.3.1.1.1" xref="S2.SS1.p3.2.m2.3.3.1.1.1.cmml"><msup id="S2.SS1.p3.2.m2.3.3.1.1.1.4" xref="S2.SS1.p3.2.m2.3.3.1.1.1.4.cmml"><mi id="S2.SS1.p3.2.m2.3.3.1.1.1.4.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.4.2.cmml">a</mi><mi id="S2.SS1.p3.2.m2.3.3.1.1.1.4.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.4.3.cmml">t</mi></msup><mo fence="false" id="S2.SS1.p3.2.m2.3.3.1.1.1.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.3.cmml">|</mo><mrow id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">ℐ</mi><mo id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.3.cmml">,</mo><msup id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.2.cmml">q</mi><mrow id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.cmml"><mn id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.1" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.1.cmml">:</mo><mi id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.3.cmml">T</mi></mrow></msup><mo id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.4" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.3.cmml">,</mo><msup id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.cmml"><mi id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.2.cmml">a</mi><mrow id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.cmml"><mn id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.1" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.1.cmml">:</mo><mrow id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.cmml"><mi id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.2" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.2.cmml">t</mi><mo id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.1" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.1.cmml">−</mo><mn id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.3.cmml">1</mn></mrow></mrow></msup><mo id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.5" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.3.cmml">;</mo><mi mathvariant="normal" id="S2.SS1.p3.2.m2.2.2" xref="S2.SS1.p3.2.m2.2.2.cmml">Θ</mi></mrow></mrow><mo stretchy="false" id="S2.SS1.p3.2.m2.3.3.1.1.3" xref="S2.SS1.p3.2.m2.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.3b"><apply id="S2.SS1.p3.2.m2.3.3.cmml" xref="S2.SS1.p3.2.m2.3.3"><times id="S2.SS1.p3.2.m2.3.3.2.cmml" xref="S2.SS1.p3.2.m2.3.3.2"></times><ci id="S2.SS1.p3.2.m2.3.3.3.cmml" xref="S2.SS1.p3.2.m2.3.3.3">𝑝</ci><apply id="S2.SS1.p3.2.m2.3.3.1.1.1.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1"><csymbol cd="latexml" id="S2.SS1.p3.2.m2.3.3.1.1.1.3.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.3">conditional</csymbol><apply id="S2.SS1.p3.2.m2.3.3.1.1.1.4.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.4"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.3.3.1.1.1.4.1.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.4">superscript</csymbol><ci id="S2.SS1.p3.2.m2.3.3.1.1.1.4.2.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.4.2">𝑎</ci><ci id="S2.SS1.p3.2.m2.3.3.1.1.1.4.3.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.4.3">𝑡</ci></apply><list id="S2.SS1.p3.2.m2.3.3.1.1.1.2.3.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2"><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">ℐ</ci><apply id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.2">𝑞</ci><apply id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3"><ci id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.1.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.1">:</ci><cn type="integer" id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.2.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.2">1</cn><ci id="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.3.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.1.1.1.3.3">𝑇</ci></apply></apply><apply id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.1.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2">superscript</csymbol><ci id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.2.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.2">𝑎</ci><apply id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3"><ci id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.1.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.1">:</ci><cn type="integer" id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.2.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.2">1</cn><apply id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3"><minus id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.1.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.1"></minus><ci id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.2.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.2">𝑡</ci><cn type="integer" id="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.3.cmml" xref="S2.SS1.p3.2.m2.3.3.1.1.1.2.2.2.3.3.3">1</cn></apply></apply></apply><ci id="S2.SS1.p3.2.m2.2.2.cmml" xref="S2.SS1.p3.2.m2.2.2">Θ</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.3c">p(a^{t}|\mathcal{I},q^{1:T},a^{1:t-1};\Theta)</annotation></semantics></math> is the probability of generating the <math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="t" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mi id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">t</annotation></semantics></math>-th token in the answer sequence given the input image <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><ci id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">\mathcal{I}</annotation></semantics></math>, the question sequence <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="q^{1:T}" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><msup id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.2" xref="S2.SS1.p3.5.m5.1.1.2.cmml">q</mi><mrow id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml"><mn id="S2.SS1.p3.5.m5.1.1.3.2" xref="S2.SS1.p3.5.m5.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p3.5.m5.1.1.3.1" xref="S2.SS1.p3.5.m5.1.1.3.1.cmml">:</mo><mi id="S2.SS1.p3.5.m5.1.1.3.3" xref="S2.SS1.p3.5.m5.1.1.3.3.cmml">T</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">superscript</csymbol><ci id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2">𝑞</ci><apply id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3"><ci id="S2.SS1.p3.5.m5.1.1.3.1.cmml" xref="S2.SS1.p3.5.m5.1.1.3.1">:</ci><cn type="integer" id="S2.SS1.p3.5.m5.1.1.3.2.cmml" xref="S2.SS1.p3.5.m5.1.1.3.2">1</cn><ci id="S2.SS1.p3.5.m5.1.1.3.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3.3">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">q^{1:T}</annotation></semantics></math>, and the previous tokens in the answer sequence <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="a^{1:t-1}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><msup id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.2" xref="S2.SS1.p3.6.m6.1.1.2.cmml">a</mi><mrow id="S2.SS1.p3.6.m6.1.1.3" xref="S2.SS1.p3.6.m6.1.1.3.cmml"><mn id="S2.SS1.p3.6.m6.1.1.3.2" xref="S2.SS1.p3.6.m6.1.1.3.2.cmml">1</mn><mo lspace="0.278em" rspace="0.278em" id="S2.SS1.p3.6.m6.1.1.3.1" xref="S2.SS1.p3.6.m6.1.1.3.1.cmml">:</mo><mrow id="S2.SS1.p3.6.m6.1.1.3.3" xref="S2.SS1.p3.6.m6.1.1.3.3.cmml"><mi id="S2.SS1.p3.6.m6.1.1.3.3.2" xref="S2.SS1.p3.6.m6.1.1.3.3.2.cmml">t</mi><mo id="S2.SS1.p3.6.m6.1.1.3.3.1" xref="S2.SS1.p3.6.m6.1.1.3.3.1.cmml">−</mo><mn id="S2.SS1.p3.6.m6.1.1.3.3.3" xref="S2.SS1.p3.6.m6.1.1.3.3.3.cmml">1</mn></mrow></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">superscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.2">𝑎</ci><apply id="S2.SS1.p3.6.m6.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3"><ci id="S2.SS1.p3.6.m6.1.1.3.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.1">:</ci><cn type="integer" id="S2.SS1.p3.6.m6.1.1.3.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.2">1</cn><apply id="S2.SS1.p3.6.m6.1.1.3.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3"><minus id="S2.SS1.p3.6.m6.1.1.3.3.1.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.1"></minus><ci id="S2.SS1.p3.6.m6.1.1.3.3.2.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.2">𝑡</ci><cn type="integer" id="S2.SS1.p3.6.m6.1.1.3.3.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">a^{1:t-1}</annotation></semantics></math>.
This formulation allows the model to generate diverse and informative answers, which can be useful in a wider range of scenarios than traditional classification-based methods.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Architecture</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In this section, we introduce our proposed architecture for generative MedVQA (Fig. <a href="#S2.F2.sf1" title="In Figure 2 ‣ 2 Method ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(a)</span></a>).
Specifically, we offer two model variants, which are tailored to encoder-based and decoder-based language models, respectively, denoted as MedVInT-TE (Sec. <a href="#S2.SS2.SSS1" title="2.2.1 MedVInT-TE ‣ 2.2 Architecture ‣ 2 Method ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>) and MedVInT-TD (Sec. <a href="#S2.SS2.SSS2" title="2.2.2 MedVInT-TD ‣ 2.2 Architecture ‣ 2 Method ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>).</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>MedVInT-TE</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.4" class="ltx_p"><span id="S2.SS2.SSS1.p1.4.1" class="ltx_text ltx_font_bold">Visual Encoder.</span>
Given one specific image <math id="S2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="\mathcal{I}" display="inline"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">ℐ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><ci id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">ℐ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">\mathcal{I}</annotation></semantics></math>,
we can obtain the image embedding, <span id="S2.SS2.SSS1.p1.4.2" class="ltx_text ltx_font_italic">i.e.</span>, <math id="S2.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="\boldsymbol{v}=\mathrm{\Phi}_{\text{vis}}(\mathcal{I})\in\mathbb{R}^{n\times d}" display="inline"><semantics id="S2.SS2.SSS1.p1.2.m2.1a"><mrow id="S2.SS2.SSS1.p1.2.m2.1.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.cmml"><mi id="S2.SS2.SSS1.p1.2.m2.1.2.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.2.cmml">𝒗</mi><mo id="S2.SS2.SSS1.p1.2.m2.1.2.3" xref="S2.SS2.SSS1.p1.2.m2.1.2.3.cmml">=</mo><mrow id="S2.SS2.SSS1.p1.2.m2.1.2.4" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.cmml"><msub id="S2.SS2.SSS1.p1.2.m2.1.2.4.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.2.cmml"><mi mathvariant="normal" id="S2.SS2.SSS1.p1.2.m2.1.2.4.2.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.2.2.cmml">Φ</mi><mtext id="S2.SS2.SSS1.p1.2.m2.1.2.4.2.3" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.2.3a.cmml">vis</mtext></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.p1.2.m2.1.2.4.1" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.1.cmml">​</mo><mrow id="S2.SS2.SSS1.p1.2.m2.1.2.4.3.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p1.2.m2.1.2.4.3.2.1" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p1.2.m2.1.1" xref="S2.SS2.SSS1.p1.2.m2.1.1.cmml">ℐ</mi><mo stretchy="false" id="S2.SS2.SSS1.p1.2.m2.1.2.4.3.2.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.cmml">)</mo></mrow></mrow><mo id="S2.SS2.SSS1.p1.2.m2.1.2.5" xref="S2.SS2.SSS1.p1.2.m2.1.2.5.cmml">∈</mo><msup id="S2.SS2.SSS1.p1.2.m2.1.2.6" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.cmml"><mi id="S2.SS2.SSS1.p1.2.m2.1.2.6.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.2.cmml">ℝ</mi><mrow id="S2.SS2.SSS1.p1.2.m2.1.2.6.3" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.3.cmml"><mi id="S2.SS2.SSS1.p1.2.m2.1.2.6.3.2" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.p1.2.m2.1.2.6.3.1" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.3.1.cmml">×</mo><mi id="S2.SS2.SSS1.p1.2.m2.1.2.6.3.3" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.2.m2.1b"><apply id="S2.SS2.SSS1.p1.2.m2.1.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2"><and id="S2.SS2.SSS1.p1.2.m2.1.2a.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2"></and><apply id="S2.SS2.SSS1.p1.2.m2.1.2b.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2"><eq id="S2.SS2.SSS1.p1.2.m2.1.2.3.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.3"></eq><ci id="S2.SS2.SSS1.p1.2.m2.1.2.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.2">𝒗</ci><apply id="S2.SS2.SSS1.p1.2.m2.1.2.4.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.4"><times id="S2.SS2.SSS1.p1.2.m2.1.2.4.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.1"></times><apply id="S2.SS2.SSS1.p1.2.m2.1.2.4.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.2.m2.1.2.4.2.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.2">subscript</csymbol><ci id="S2.SS2.SSS1.p1.2.m2.1.2.4.2.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.2.2">Φ</ci><ci id="S2.SS2.SSS1.p1.2.m2.1.2.4.2.3a.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.2.3"><mtext mathsize="70%" id="S2.SS2.SSS1.p1.2.m2.1.2.4.2.3.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.4.2.3">vis</mtext></ci></apply><ci id="S2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.1">ℐ</ci></apply></apply><apply id="S2.SS2.SSS1.p1.2.m2.1.2c.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2"><in id="S2.SS2.SSS1.p1.2.m2.1.2.5.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.5"></in><share href="#S2.SS2.SSS1.p1.2.m2.1.2.4.cmml" id="S2.SS2.SSS1.p1.2.m2.1.2d.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2"></share><apply id="S2.SS2.SSS1.p1.2.m2.1.2.6.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.6"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p1.2.m2.1.2.6.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.6">superscript</csymbol><ci id="S2.SS2.SSS1.p1.2.m2.1.2.6.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.2">ℝ</ci><apply id="S2.SS2.SSS1.p1.2.m2.1.2.6.3.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.3"><times id="S2.SS2.SSS1.p1.2.m2.1.2.6.3.1.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.3.1"></times><ci id="S2.SS2.SSS1.p1.2.m2.1.2.6.3.2.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.3.2">𝑛</ci><ci id="S2.SS2.SSS1.p1.2.m2.1.2.6.3.3.cmml" xref="S2.SS2.SSS1.p1.2.m2.1.2.6.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.2.m2.1c">\boldsymbol{v}=\mathrm{\Phi}_{\text{vis}}(\mathcal{I})\in\mathbb{R}^{n\times d}</annotation></semantics></math>,
where <math id="S2.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="d" display="inline"><semantics id="S2.SS2.SSS1.p1.3.m3.1a"><mi id="S2.SS2.SSS1.p1.3.m3.1.1" xref="S2.SS2.SSS1.p1.3.m3.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.3.m3.1b"><ci id="S2.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p1.3.m3.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.3.m3.1c">d</annotation></semantics></math> denotes the embedding dimension, <math id="S2.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.SSS1.p1.4.m4.1a"><mi id="S2.SS2.SSS1.p1.4.m4.1.1" xref="S2.SS2.SSS1.p1.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.4.m4.1b"><ci id="S2.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS1.p1.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.4.m4.1c">n</annotation></semantics></math> denotes the patch number.
The vision encoder is based on a pre-trained ResNet-50 adopted from PMC-CLIP <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>, with a trainable projection module.
To produce a fixed shape of visual output,
we add a trainable projection module on top of the ResNet-50, with the aim of bridging the gap between the pre-trained visual and language embeddings.
We propose two distinct variants for this projection module.
The first variant, MLP-based, employs a two-layer Multilayer Perceptron (MLP), while the second variant, transformer-based, employs a 12-layer transformer decoder supplemented with several learnable vectors as query input.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.10" class="ltx_p"><span id="S2.SS2.SSS1.p2.10.1" class="ltx_text ltx_font_bold">Language Encoder.</span>
Given one question on the image,
to guide the language model with desirable output,
we append a fixed prompt with the question,
<span id="S2.SS2.SSS1.p2.10.2" class="ltx_text ltx_font_italic">i.e.</span>, “Question: <math id="S2.SS2.SSS1.p2.1.m1.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS2.SSS1.p2.1.m1.1a"><mi id="S2.SS2.SSS1.p2.1.m1.1.1" xref="S2.SS2.SSS1.p2.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.1.m1.1b"><ci id="S2.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p2.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.1.m1.1c">q</annotation></semantics></math>, the answer is:”, and encode it with the language encoder: <math id="S2.SS2.SSS1.p2.2.m2.1" class="ltx_Math" alttext="\boldsymbol{q}=\mathrm{\Phi}_{\text{text}}(q)\in\mathbb{R}^{l\times d}" display="inline"><semantics id="S2.SS2.SSS1.p2.2.m2.1a"><mrow id="S2.SS2.SSS1.p2.2.m2.1.2" xref="S2.SS2.SSS1.p2.2.m2.1.2.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.1.2.2" xref="S2.SS2.SSS1.p2.2.m2.1.2.2.cmml">𝒒</mi><mo id="S2.SS2.SSS1.p2.2.m2.1.2.3" xref="S2.SS2.SSS1.p2.2.m2.1.2.3.cmml">=</mo><mrow id="S2.SS2.SSS1.p2.2.m2.1.2.4" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.cmml"><msub id="S2.SS2.SSS1.p2.2.m2.1.2.4.2" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.2.cmml"><mi mathvariant="normal" id="S2.SS2.SSS1.p2.2.m2.1.2.4.2.2" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.2.2.cmml">Φ</mi><mtext id="S2.SS2.SSS1.p2.2.m2.1.2.4.2.3" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.2.3a.cmml">text</mtext></msub><mo lspace="0em" rspace="0em" id="S2.SS2.SSS1.p2.2.m2.1.2.4.1" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.1.cmml">​</mo><mrow id="S2.SS2.SSS1.p2.2.m2.1.2.4.3.2" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.cmml"><mo stretchy="false" id="S2.SS2.SSS1.p2.2.m2.1.2.4.3.2.1" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.cmml">(</mo><mi id="S2.SS2.SSS1.p2.2.m2.1.1" xref="S2.SS2.SSS1.p2.2.m2.1.1.cmml">q</mi><mo stretchy="false" id="S2.SS2.SSS1.p2.2.m2.1.2.4.3.2.2" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.cmml">)</mo></mrow></mrow><mo id="S2.SS2.SSS1.p2.2.m2.1.2.5" xref="S2.SS2.SSS1.p2.2.m2.1.2.5.cmml">∈</mo><msup id="S2.SS2.SSS1.p2.2.m2.1.2.6" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.1.2.6.2" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.2.cmml">ℝ</mi><mrow id="S2.SS2.SSS1.p2.2.m2.1.2.6.3" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.3.cmml"><mi id="S2.SS2.SSS1.p2.2.m2.1.2.6.3.2" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.3.2.cmml">l</mi><mo lspace="0.222em" rspace="0.222em" id="S2.SS2.SSS1.p2.2.m2.1.2.6.3.1" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.3.1.cmml">×</mo><mi id="S2.SS2.SSS1.p2.2.m2.1.2.6.3.3" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.2.m2.1b"><apply id="S2.SS2.SSS1.p2.2.m2.1.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2"><and id="S2.SS2.SSS1.p2.2.m2.1.2a.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2"></and><apply id="S2.SS2.SSS1.p2.2.m2.1.2b.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2"><eq id="S2.SS2.SSS1.p2.2.m2.1.2.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.3"></eq><ci id="S2.SS2.SSS1.p2.2.m2.1.2.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.2">𝒒</ci><apply id="S2.SS2.SSS1.p2.2.m2.1.2.4.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.4"><times id="S2.SS2.SSS1.p2.2.m2.1.2.4.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.1"></times><apply id="S2.SS2.SSS1.p2.2.m2.1.2.4.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.2.m2.1.2.4.2.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.2">subscript</csymbol><ci id="S2.SS2.SSS1.p2.2.m2.1.2.4.2.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.2.2">Φ</ci><ci id="S2.SS2.SSS1.p2.2.m2.1.2.4.2.3a.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.2.3"><mtext mathsize="70%" id="S2.SS2.SSS1.p2.2.m2.1.2.4.2.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.4.2.3">text</mtext></ci></apply><ci id="S2.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.1">𝑞</ci></apply></apply><apply id="S2.SS2.SSS1.p2.2.m2.1.2c.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2"><in id="S2.SS2.SSS1.p2.2.m2.1.2.5.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.5"></in><share href="#S2.SS2.SSS1.p2.2.m2.1.2.4.cmml" id="S2.SS2.SSS1.p2.2.m2.1.2d.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2"></share><apply id="S2.SS2.SSS1.p2.2.m2.1.2.6.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.6"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.2.m2.1.2.6.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.6">superscript</csymbol><ci id="S2.SS2.SSS1.p2.2.m2.1.2.6.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.2">ℝ</ci><apply id="S2.SS2.SSS1.p2.2.m2.1.2.6.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.3"><times id="S2.SS2.SSS1.p2.2.m2.1.2.6.3.1.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.3.1"></times><ci id="S2.SS2.SSS1.p2.2.m2.1.2.6.3.2.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.3.2">𝑙</ci><ci id="S2.SS2.SSS1.p2.2.m2.1.2.6.3.3.cmml" xref="S2.SS2.SSS1.p2.2.m2.1.2.6.3.3">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.2.m2.1c">\boldsymbol{q}=\mathrm{\Phi}_{\text{text}}(q)\in\mathbb{R}^{l\times d}</annotation></semantics></math>,
where <math id="S2.SS2.SSS1.p2.3.m3.1" class="ltx_Math" alttext="\boldsymbol{q}" display="inline"><semantics id="S2.SS2.SSS1.p2.3.m3.1a"><mi id="S2.SS2.SSS1.p2.3.m3.1.1" xref="S2.SS2.SSS1.p2.3.m3.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.3.m3.1b"><ci id="S2.SS2.SSS1.p2.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p2.3.m3.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.3.m3.1c">\boldsymbol{q}</annotation></semantics></math> refers to the text embedding, <math id="S2.SS2.SSS1.p2.4.m4.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.SS2.SSS1.p2.4.m4.1a"><mi id="S2.SS2.SSS1.p2.4.m4.1.1" xref="S2.SS2.SSS1.p2.4.m4.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.4.m4.1b"><ci id="S2.SS2.SSS1.p2.4.m4.1.1.cmml" xref="S2.SS2.SSS1.p2.4.m4.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.4.m4.1c">l</annotation></semantics></math> represents the sequential length for the question, and <math id="S2.SS2.SSS1.p2.5.m5.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS2.SSS1.p2.5.m5.1a"><mi id="S2.SS2.SSS1.p2.5.m5.1.1" xref="S2.SS2.SSS1.p2.5.m5.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.5.m5.1b"><ci id="S2.SS2.SSS1.p2.5.m5.1.1.cmml" xref="S2.SS2.SSS1.p2.5.m5.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.5.m5.1c">q</annotation></semantics></math> is the prompted question.
<math id="S2.SS2.SSS1.p2.6.m6.1" class="ltx_Math" alttext="\Phi_{\text{text}}" display="inline"><semantics id="S2.SS2.SSS1.p2.6.m6.1a"><msub id="S2.SS2.SSS1.p2.6.m6.1.1" xref="S2.SS2.SSS1.p2.6.m6.1.1.cmml"><mi mathvariant="normal" id="S2.SS2.SSS1.p2.6.m6.1.1.2" xref="S2.SS2.SSS1.p2.6.m6.1.1.2.cmml">Φ</mi><mtext id="S2.SS2.SSS1.p2.6.m6.1.1.3" xref="S2.SS2.SSS1.p2.6.m6.1.1.3a.cmml">text</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.6.m6.1b"><apply id="S2.SS2.SSS1.p2.6.m6.1.1.cmml" xref="S2.SS2.SSS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.6.m6.1.1.1.cmml" xref="S2.SS2.SSS1.p2.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.6.m6.1.1.2.cmml" xref="S2.SS2.SSS1.p2.6.m6.1.1.2">Φ</ci><ci id="S2.SS2.SSS1.p2.6.m6.1.1.3a.cmml" xref="S2.SS2.SSS1.p2.6.m6.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS1.p2.6.m6.1.1.3.cmml" xref="S2.SS2.SSS1.p2.6.m6.1.1.3">text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.6.m6.1c">\Phi_{\text{text}}</annotation></semantics></math> is initialized with the pre-trained language model.
Note that our model can also be applied to multiple-choice tasks,
by providing options and training it to output the right choice as "A/B/C/D".
The prompt is then modified as “Question: <math id="S2.SS2.SSS1.p2.7.m7.1" class="ltx_Math" alttext="q" display="inline"><semantics id="S2.SS2.SSS1.p2.7.m7.1a"><mi id="S2.SS2.SSS1.p2.7.m7.1.1" xref="S2.SS2.SSS1.p2.7.m7.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.7.m7.1b"><ci id="S2.SS2.SSS1.p2.7.m7.1.1.cmml" xref="S2.SS2.SSS1.p2.7.m7.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.7.m7.1c">q</annotation></semantics></math>, the options are: <math id="S2.SS2.SSS1.p2.8.m8.4" class="ltx_Math" alttext="a_{1},a_{2},a_{3},a_{4}" display="inline"><semantics id="S2.SS2.SSS1.p2.8.m8.4a"><mrow id="S2.SS2.SSS1.p2.8.m8.4.4.4" xref="S2.SS2.SSS1.p2.8.m8.4.4.5.cmml"><msub id="S2.SS2.SSS1.p2.8.m8.1.1.1.1" xref="S2.SS2.SSS1.p2.8.m8.1.1.1.1.cmml"><mi id="S2.SS2.SSS1.p2.8.m8.1.1.1.1.2" xref="S2.SS2.SSS1.p2.8.m8.1.1.1.1.2.cmml">a</mi><mn id="S2.SS2.SSS1.p2.8.m8.1.1.1.1.3" xref="S2.SS2.SSS1.p2.8.m8.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS1.p2.8.m8.4.4.4.5" xref="S2.SS2.SSS1.p2.8.m8.4.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.8.m8.2.2.2.2" xref="S2.SS2.SSS1.p2.8.m8.2.2.2.2.cmml"><mi id="S2.SS2.SSS1.p2.8.m8.2.2.2.2.2" xref="S2.SS2.SSS1.p2.8.m8.2.2.2.2.2.cmml">a</mi><mn id="S2.SS2.SSS1.p2.8.m8.2.2.2.2.3" xref="S2.SS2.SSS1.p2.8.m8.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS2.SSS1.p2.8.m8.4.4.4.6" xref="S2.SS2.SSS1.p2.8.m8.4.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.8.m8.3.3.3.3" xref="S2.SS2.SSS1.p2.8.m8.3.3.3.3.cmml"><mi id="S2.SS2.SSS1.p2.8.m8.3.3.3.3.2" xref="S2.SS2.SSS1.p2.8.m8.3.3.3.3.2.cmml">a</mi><mn id="S2.SS2.SSS1.p2.8.m8.3.3.3.3.3" xref="S2.SS2.SSS1.p2.8.m8.3.3.3.3.3.cmml">3</mn></msub><mo id="S2.SS2.SSS1.p2.8.m8.4.4.4.7" xref="S2.SS2.SSS1.p2.8.m8.4.4.5.cmml">,</mo><msub id="S2.SS2.SSS1.p2.8.m8.4.4.4.4" xref="S2.SS2.SSS1.p2.8.m8.4.4.4.4.cmml"><mi id="S2.SS2.SSS1.p2.8.m8.4.4.4.4.2" xref="S2.SS2.SSS1.p2.8.m8.4.4.4.4.2.cmml">a</mi><mn id="S2.SS2.SSS1.p2.8.m8.4.4.4.4.3" xref="S2.SS2.SSS1.p2.8.m8.4.4.4.4.3.cmml">4</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.8.m8.4b"><list id="S2.SS2.SSS1.p2.8.m8.4.4.5.cmml" xref="S2.SS2.SSS1.p2.8.m8.4.4.4"><apply id="S2.SS2.SSS1.p2.8.m8.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.8.m8.1.1.1.1.1.cmml" xref="S2.SS2.SSS1.p2.8.m8.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.8.m8.1.1.1.1.2.cmml" xref="S2.SS2.SSS1.p2.8.m8.1.1.1.1.2">𝑎</ci><cn type="integer" id="S2.SS2.SSS1.p2.8.m8.1.1.1.1.3.cmml" xref="S2.SS2.SSS1.p2.8.m8.1.1.1.1.3">1</cn></apply><apply id="S2.SS2.SSS1.p2.8.m8.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.8.m8.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.8.m8.2.2.2.2.1.cmml" xref="S2.SS2.SSS1.p2.8.m8.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS1.p2.8.m8.2.2.2.2.2.cmml" xref="S2.SS2.SSS1.p2.8.m8.2.2.2.2.2">𝑎</ci><cn type="integer" id="S2.SS2.SSS1.p2.8.m8.2.2.2.2.3.cmml" xref="S2.SS2.SSS1.p2.8.m8.2.2.2.2.3">2</cn></apply><apply id="S2.SS2.SSS1.p2.8.m8.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.8.m8.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.8.m8.3.3.3.3.1.cmml" xref="S2.SS2.SSS1.p2.8.m8.3.3.3.3">subscript</csymbol><ci id="S2.SS2.SSS1.p2.8.m8.3.3.3.3.2.cmml" xref="S2.SS2.SSS1.p2.8.m8.3.3.3.3.2">𝑎</ci><cn type="integer" id="S2.SS2.SSS1.p2.8.m8.3.3.3.3.3.cmml" xref="S2.SS2.SSS1.p2.8.m8.3.3.3.3.3">3</cn></apply><apply id="S2.SS2.SSS1.p2.8.m8.4.4.4.4.cmml" xref="S2.SS2.SSS1.p2.8.m8.4.4.4.4"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.8.m8.4.4.4.4.1.cmml" xref="S2.SS2.SSS1.p2.8.m8.4.4.4.4">subscript</csymbol><ci id="S2.SS2.SSS1.p2.8.m8.4.4.4.4.2.cmml" xref="S2.SS2.SSS1.p2.8.m8.4.4.4.4.2">𝑎</ci><cn type="integer" id="S2.SS2.SSS1.p2.8.m8.4.4.4.4.3.cmml" xref="S2.SS2.SSS1.p2.8.m8.4.4.4.4.3">4</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.8.m8.4c">a_{1},a_{2},a_{3},a_{4}</annotation></semantics></math>, the answer is:”, where <math id="S2.SS2.SSS1.p2.9.m9.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.SS2.SSS1.p2.9.m9.1a"><msub id="S2.SS2.SSS1.p2.9.m9.1.1" xref="S2.SS2.SSS1.p2.9.m9.1.1.cmml"><mi id="S2.SS2.SSS1.p2.9.m9.1.1.2" xref="S2.SS2.SSS1.p2.9.m9.1.1.2.cmml">a</mi><mi id="S2.SS2.SSS1.p2.9.m9.1.1.3" xref="S2.SS2.SSS1.p2.9.m9.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.9.m9.1b"><apply id="S2.SS2.SSS1.p2.9.m9.1.1.cmml" xref="S2.SS2.SSS1.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p2.9.m9.1.1.1.cmml" xref="S2.SS2.SSS1.p2.9.m9.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p2.9.m9.1.1.2.cmml" xref="S2.SS2.SSS1.p2.9.m9.1.1.2">𝑎</ci><ci id="S2.SS2.SSS1.p2.9.m9.1.1.3.cmml" xref="S2.SS2.SSS1.p2.9.m9.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.9.m9.1c">a_{i}</annotation></semantics></math> refers to the <math id="S2.SS2.SSS1.p2.10.m10.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.SSS1.p2.10.m10.1a"><mi id="S2.SS2.SSS1.p2.10.m10.1.1" xref="S2.SS2.SSS1.p2.10.m10.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p2.10.m10.1b"><ci id="S2.SS2.SSS1.p2.10.m10.1.1.cmml" xref="S2.SS2.SSS1.p2.10.m10.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p2.10.m10.1c">i</annotation></semantics></math>-th option.</p>
</div>
<div id="S2.SS2.SSS1.p3" class="ltx_para">
<p id="S2.SS2.SSS1.p3.3" class="ltx_p"><span id="S2.SS2.SSS1.p3.3.1" class="ltx_text ltx_font_bold">Multimodal Decoder.</span>
With encoded visual embeddings (<math id="S2.SS2.SSS1.p3.1.m1.1" class="ltx_Math" alttext="\boldsymbol{v}" display="inline"><semantics id="S2.SS2.SSS1.p3.1.m1.1a"><mi id="S2.SS2.SSS1.p3.1.m1.1.1" xref="S2.SS2.SSS1.p3.1.m1.1.1.cmml">𝒗</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.1.m1.1b"><ci id="S2.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p3.1.m1.1.1">𝒗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.1.m1.1c">\boldsymbol{v}</annotation></semantics></math>) and question embeddings (<math id="S2.SS2.SSS1.p3.2.m2.1" class="ltx_Math" alttext="\boldsymbol{q}" display="inline"><semantics id="S2.SS2.SSS1.p3.2.m2.1a"><mi id="S2.SS2.SSS1.p3.2.m2.1.1" xref="S2.SS2.SSS1.p3.2.m2.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.2.m2.1b"><ci id="S2.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p3.2.m2.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.2.m2.1c">\boldsymbol{q}</annotation></semantics></math>), we concatenate them as the input to the multimodal decoder (<math id="S2.SS2.SSS1.p3.3.m3.1" class="ltx_Math" alttext="\mathrm{\Phi}_{\text{dec}}" display="inline"><semantics id="S2.SS2.SSS1.p3.3.m3.1a"><msub id="S2.SS2.SSS1.p3.3.m3.1.1" xref="S2.SS2.SSS1.p3.3.m3.1.1.cmml"><mi mathvariant="normal" id="S2.SS2.SSS1.p3.3.m3.1.1.2" xref="S2.SS2.SSS1.p3.3.m3.1.1.2.cmml">Φ</mi><mtext id="S2.SS2.SSS1.p3.3.m3.1.1.3" xref="S2.SS2.SSS1.p3.3.m3.1.1.3a.cmml">dec</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.3.m3.1b"><apply id="S2.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p3.3.m3.1.1.1.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS1.p3.3.m3.1.1.2.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1.2">Φ</ci><ci id="S2.SS2.SSS1.p3.3.m3.1.1.3a.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS1.p3.3.m3.1.1.3.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1.3">dec</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.3.m3.1c">\mathrm{\Phi}_{\text{dec}}</annotation></semantics></math>).
The multimodal decoder is initialized from scratch with a 4-layer transformer structure.
Additionally, acknowledging that the encoder-based language models lack casual masking, we reform the generation task as a mask language modeling task,
<span id="S2.SS2.SSS1.p3.3.2" class="ltx_text ltx_font_italic">i.e.</span>, the question input is padded with several ‘[MASK]’ token
and the decoder module learns to generate the prediction for the masked token.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>MedVInT-TD</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p"><span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_bold">Visual Encoder.</span>
The visual encoder is the same as MedVInT-TE.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.2" class="ltx_p"><span id="S2.SS2.SSS2.p2.2.1" class="ltx_text ltx_font_bold">Text Encoder.</span>
We design <math id="S2.SS2.SSS2.p2.1.m1.1" class="ltx_Math" alttext="\Phi_{\text{text}}" display="inline"><semantics id="S2.SS2.SSS2.p2.1.m1.1a"><msub id="S2.SS2.SSS2.p2.1.m1.1.1" xref="S2.SS2.SSS2.p2.1.m1.1.1.cmml"><mi mathvariant="normal" id="S2.SS2.SSS2.p2.1.m1.1.1.2" xref="S2.SS2.SSS2.p2.1.m1.1.1.2.cmml">Φ</mi><mtext id="S2.SS2.SSS2.p2.1.m1.1.1.3" xref="S2.SS2.SSS2.p2.1.m1.1.1.3a.cmml">text</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.1.m1.1b"><apply id="S2.SS2.SSS2.p2.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.2">Φ</ci><ci id="S2.SS2.SSS2.p2.1.m1.1.1.3a.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.3"><mtext mathsize="70%" id="S2.SS2.SSS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.SSS2.p2.1.m1.1.1.3">text</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.1.m1.1c">\Phi_{\text{text}}</annotation></semantics></math> as a simple embedding layer similar to the primary GPT-like LLMs and initialized with their parameters. Same with MedVInT-TE, it also encodes the question input into embedding features <math id="S2.SS2.SSS2.p2.2.m2.1" class="ltx_Math" alttext="\boldsymbol{q}" display="inline"><semantics id="S2.SS2.SSS2.p2.2.m2.1a"><mi id="S2.SS2.SSS2.p2.2.m2.1.1" xref="S2.SS2.SSS2.p2.2.m2.1.1.cmml">𝒒</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p2.2.m2.1b"><ci id="S2.SS2.SSS2.p2.2.m2.1.1.cmml" xref="S2.SS2.SSS2.p2.2.m2.1.1">𝒒</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p2.2.m2.1c">\boldsymbol{q}</annotation></semantics></math> and can perform multi-choice or blank through different prompts.</p>
</div>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p"><span id="S2.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_bold">Multimodal Decoder.</span>
For the Transformer decoder-based language model, with its output format already being free-form text, we directly use its architecture as the multimodal decoder initialized with the pre-train weights.
Specifically, we concatenate the image and text features as the input.
However, directly using the text decoder as a multimodal decoder, may lead to significant mismatching between the image encoding space and the decoder input space. Therefore, to further fill the gap between the image embedding space, here, we pre-train the whole network using the PMC-OA<cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> dataset in a caption-based manner, which is similar to BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>The PMC-VQA Dataset</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our study has identified the lack of large-scale, multi-modal MedVQA datasets as a significant obstacle to the development of effective generative MedVQA models. To address this issue, we present a scalable and automatic pipeline for creating a new large MedVQA dataset.
In this section, we provide a detailed description of our dataset collection process, starting with the source data and continuing with the question-answer generation and data filtering procedures.
Finally, we analyze the collected data from various perspectives to gain insights into its properties and potential applications.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Source Data. </span>
We start from PMC-OA <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>, which is a comprehensive biomedical dataset comprising 1.6 million image-text pairs collected from PubMedCentral (PMC)’s OpenAccess subset <cite class="ltx_cite ltx_citemacro_cite">Roberts (<a href="#bib.bib31" title="" class="ltx_ref">2001</a>)</cite>, which covers 2.4 million papers.
In order to maintain the diversity and complexity of PMC-VQA,
we have used a version of <span id="S3.p2.1.2" class="ltx_text ltx_font_bold">381K image-caption pairs</span> obtained from the first stage of the medical figure collection process without subfigure auto-separation.
We have opted not to use the final released version of the dataset,
which only includes subfigure separation, subcaption separation, and alignment, in order to maintain a certain level of complexity and avoid oversimplifying the dataset.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2305.10415/assets/x4.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="452" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Several examples of challenging questions and answers along with their respective images.
To answer questions related to these images,
the network must acquire sufficient medical knowledge,
for example, for the first two images, it is essential to recognize the anatomy structure and modalities;
for the third image, recognizing the X-ray image pattern of pathologies is necessary;
for the final two images, apart from the basic biomedical knowledge,
the model is also required to discern colors, differentiate subfigures,
and perform Optical Character Recognition (OCR).
</figcaption>
</figure>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Question-Answer Generation. </span>
To automatically generate high-quality question-answer pairs within the constraints of an academic budget, we leverage the power of ChatGPT by inputting the image captions of PMC-OA as the content to the model.
We use the following prompt to generate 5 question-answer pairs for each caption.</p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.3" class="ltx_p"><span id="S3.p4.3.3" class="ltx_text" style="background-color:#E6E6E6;">

<span id="S3.p4.3.3.3.3" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:411.9pt;">
<span id="S3.p4.3.3.3.3.3" class="ltx_p"><span id="S3.p4.3.3.3.3.3.1" class="ltx_text" style="font-size:80%;">Ask 5 questions about the content and generate four options for each question. The questions should be answerable with the information provided in the caption, and the four options should include one correct and three incorrect options, with the position of the correct option randomized. The output should use the following template: i:‘the question index’ question:‘the generate question’ choice: ‘A:option content B:option content C:option content D:option content’ answer: The correct option(A</span><math id="S3.p4.1.1.1.1.1.m1.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.p4.1.1.1.1.1.m1.1a"><mo mathbackground="#E6E6E6" mathsize="80%" id="S3.p4.1.1.1.1.1.m1.1.1" xref="S3.p4.1.1.1.1.1.m1.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.p4.1.1.1.1.1.m1.1b"><ci id="S3.p4.1.1.1.1.1.m1.1.1.cmml" xref="S3.p4.1.1.1.1.1.m1.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.1.1.1.1.m1.1c">\backslash</annotation></semantics></math><span id="S3.p4.3.3.3.3.3.2" class="ltx_text" style="font-size:80%;">B</span><math id="S3.p4.2.2.2.2.2.m2.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.p4.2.2.2.2.2.m2.1a"><mo mathbackground="#E6E6E6" mathsize="80%" id="S3.p4.2.2.2.2.2.m2.1.1" xref="S3.p4.2.2.2.2.2.m2.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.p4.2.2.2.2.2.m2.1b"><ci id="S3.p4.2.2.2.2.2.m2.1.1.cmml" xref="S3.p4.2.2.2.2.2.m2.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.2.2.2.2.m2.1c">\backslash</annotation></semantics></math><span id="S3.p4.3.3.3.3.3.3" class="ltx_text" style="font-size:80%;">C</span><math id="S3.p4.3.3.3.3.3.m3.1" class="ltx_Math" alttext="\backslash" display="inline"><semantics id="S3.p4.3.3.3.3.3.m3.1a"><mo mathbackground="#E6E6E6" mathsize="80%" id="S3.p4.3.3.3.3.3.m3.1.1" xref="S3.p4.3.3.3.3.3.m3.1.1.cmml">\</mo><annotation-xml encoding="MathML-Content" id="S3.p4.3.3.3.3.3.m3.1b"><ci id="S3.p4.3.3.3.3.3.m3.1.1.cmml" xref="S3.p4.3.3.3.3.3.m3.1.1">\</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.3.3.3.3.m3.1c">\backslash</annotation></semantics></math><span id="S3.p4.3.3.3.3.3.4" class="ltx_text" style="font-size:80%;">D).</span></span>
</span><span id="S3.p4.3.3.4" class="ltx_text" style="font-size:80%;">
</span></span></p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p">This approach allows us to generate a large volume of diverse and high-quality questions that cover a wide range of medical topics.
After generating the question-answer pairs using ChatGPT, we applied a rigorous filtering process to ensure that the pairs met our formatting requirements. As a result, we obtained 1,497,808 question-answer pairs, and since the original captions are linked with images, the pairs can naturally find corresponding images, resulting in an average of 3.93 pairs per image.</p>
</div>
<div id="S3.p6" class="ltx_para ltx_noindent">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text ltx_font_bold">Data Filtering. </span>
As the questions are sourced from image captions,
some questions can be answered correctly using biomedical knowledge alone without the need for a specific image, for example, question: “which type of MRI sequence shows high signal in the marrow edema?”.
To address this issue, we trained a question-answer model using LLaMA-7B <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> with text data only and eliminated all questions that could be potentially answerable by the language model. This filtering process resulted in 848,433 high-quality question-answer pairs.</p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p">Furthermore, some questions in our data rely on additional information in the caption that cannot be answered using only the corresponding image, such as “How many patients were classified into the middle stage?"
To identify these questions, we trained a question classification model to determine whether a question is answerable given the image alone. Specifically, we manually annotated 2192 question-answer pairs and randomly split them into a training set of 1752 pairs and a testing set of 440 pairs. We fine-tuned LLaMA-7B <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite> on this training set, and our model achieved an accuracy of 81.77% on the test set.
We then used this model for data cleaning, resulting in a total of 226,946 question-answer pairs corresponding to 149,075 images. From this cleaned dataset, we randomly selected 50,000 image-question pairs to create our test set, namely, PMC-VQA-test.
Additionally, we also provided a small <span id="S3.p7.1.1" class="ltx_text ltx_font_bold">clean</span> test set of 2,000 samples, which were manually verified for quality,
termed as PMC-VQA-test-clean. During this manual verification procedure,
we have estimated that over 80% of PMC-VQA-test can be retained.</p>
</div>
<div id="S3.p8" class="ltx_para ltx_noindent">
<p id="S3.p8.1" class="ltx_p"><span id="S3.p8.1.1" class="ltx_text ltx_font_bold">Data Analysis. </span>
This section provides an analysis on images, questions, and answers in the PMC-VQA dataset. In detail, the dataset comprises 227k image-question pairs, some examples are presented in Fig.<a href="#S3.F3" title="Figure 3 ‣ 3 The PMC-VQA Dataset ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, which demonstrates the wide diversity of images within our dataset.
As indicated in Table<a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, PMC-VQA outperforms existing MedVQA datasets in terms of data size and modality diversity. The questions in our dataset cover a range of difficulties, from simple questions such as identifying image modalities, perspectives, and organs to challenging questions that require specialized knowledge and judgment. Additionally, our dataset includes difficult questions that demand the ability to identify the specific target sub-figure from the compound figure.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2305.10415/assets/x5.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Question distribution of the training set by their first four words. From left to right are all questions, questions started with “What” and questions started with “Which”.
The ordering of the words starts towards the center and radiates outwards. </figcaption>
</figure>
<div id="S3.p9" class="ltx_para">
<p id="S3.p9.4" class="ltx_p">Our analysis of the PMC-VQA dataset can be summarized in three aspects:
(i) <span id="S3.p9.4.1" class="ltx_text ltx_font_bold">Images</span>: We show the top 20 figure types in the PMC-VQA in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The images in the PMC-VQA are extremely diverse, ranging from Radiology to Signals.
(ii) <span id="S3.p9.4.2" class="ltx_text ltx_font_bold">Questions</span>: We clustered the questions into different types based on the words that start the question, as shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3 The PMC-VQA Dataset ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. We found a surprising variety of question types, including "What is the difference…", "What type of imaging…", and "Which image shows…". Most questions range from 5 to 15 words, and detailed information about the distribution of question lengths is shown in the supplementary materials. (iii) <span id="S3.p9.4.3" class="ltx_text ltx_font_bold">Answers</span>: The words in answers primarily encompass positional descriptions, image modalities, and specific anatomical regions. Detailed information about the top 50 words that appeared in the answers is provided in the supplementary materials. Most answers are around 5 words, which is much shorter than the questions.
The correct options were distributed as follows: A (24.07<math id="S3.p9.1.m1.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.p9.1.m1.1a"><mo id="S3.p9.1.m1.1.1" xref="S3.p9.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.p9.1.m1.1b"><csymbol cd="latexml" id="S3.p9.1.m1.1.1.cmml" xref="S3.p9.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.1.m1.1c">\%</annotation></semantics></math>), B (30.87<math id="S3.p9.2.m2.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.p9.2.m2.1a"><mo id="S3.p9.2.m2.1.1" xref="S3.p9.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.p9.2.m2.1b"><csymbol cd="latexml" id="S3.p9.2.m2.1.1.cmml" xref="S3.p9.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.2.m2.1c">\%</annotation></semantics></math>), C (29.09<math id="S3.p9.3.m3.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.p9.3.m3.1a"><mo id="S3.p9.3.m3.1.1" xref="S3.p9.3.m3.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.p9.3.m3.1b"><csymbol cd="latexml" id="S3.p9.3.m3.1.1.cmml" xref="S3.p9.3.m3.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.3.m3.1c">\%</annotation></semantics></math>), D (15.97 <math id="S3.p9.4.m4.1" class="ltx_Math" alttext="\%" display="inline"><semantics id="S3.p9.4.m4.1a"><mo id="S3.p9.4.m4.1.1" xref="S3.p9.4.m4.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.p9.4.m4.1b"><csymbol cd="latexml" id="S3.p9.4.m4.1.1.cmml" xref="S3.p9.4.m4.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p9.4.m4.1c">\%</annotation></semantics></math>).</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we first introduce two existing primary MedVQA datasets, namely VQA-RAD and SLAKE (Sec. <a href="#S4.SS1" title="4.1 Existing MedVQA Datasets ‣ 4 Experiments ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>). We then provide a detailed description of our proposed dataset, PMC-VQA, which can be used for both multiple-choice and open-ended answering tasks (Sec. <a href="#S4.SS2" title="4.2 PMC-VQA Dataset ‣ 4 Experiments ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). Finally, we discuss the primary pre-trained models we use for ablation in Sec. <a href="#S4.SS3" title="4.3 Pre-trained Backbones ‣ 4 Experiments ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a>. The implementation details is provided in the supplementary materials.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Existing MedVQA Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_bold">VQA-RAD</span> <cite class="ltx_cite ltx_citemacro_cite">Lau et al. (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite> is a VQA dataset specifically designed for radiology, consisting of 315 images and 3,515 questions with 517 possible answers. The questions in VQA-RAD are categorized as either close-ended or open-ended, depending on whether answer choices are limited or not. We follow the official dataset split for our evaluation.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text ltx_font_bold">SLAKE</span> <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021b</a>)</cite> is an English-Chinese bilingual VQA dataset composed of 642 images and 14k questions.
The questions are categorized as close-ended if answer choices are limited, otherwise open-ended.
There are <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="224" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">224</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">224</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">224</annotation></semantics></math> possible answers in total.
We only use the “English” part, and follow the official split.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Baselines and Metrics.</span> We compare with various baselines on these two MedVQA datasets, namely, MEVF-BAN <cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>, CPRD-BAN <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib22" title="" class="ltx_ref">2021a</a>)</cite>, M3AE <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a href="#bib.bib6" title="" class="ltx_ref">2022</a>)</cite>, PMC-CLIP <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>. PMC-CLIP <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> is the existing SOTA method on these two datasets. For evaluation, ACC scores are used. <span id="S4.SS1.p3.1.2" class="ltx_text ltx_font_bold">Note that</span>, since our model is generative-based, we calculate ACC by matching the generative output with the options using <span id="S4.SS1.p3.1.3" class="ltx_text ltx_font_typewriter">difflib.SequenceMatcher<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><a target="_blank" href="https://docs.python.org/3/library/difflib.html" title="" class="ltx_ref ltx_url">https://docs.python.org/3/library/difflib.html</a></span></span></span></span> and choosing the most similar one as the choice of the model, which is more difficult than the evaluation for retrieval-based methods due to the larger output space.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>PMC-VQA Dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The PMC-VQA dataset consists of a train set with 177K samples and a test set with 50K samples, which are respectively denoted as PMC-VQA-train and PMC-VQA-test.
Additionally, the smaller clean test set with 2K samples that have been manually verified, is referred to as PMC-VQA-test-clean.
The dataset can be used for both open-ended and multiple-choice tasks.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text ltx_font_bold">Multi-choice MedVQA. </span>
Four candidate answers are provided for each question as the prompt.
The model is then trained to <span id="S4.SS2.p2.1.2" class="ltx_text ltx_font_bold">select the correct option</span> among them. The accuracy (ACC) score can be used to evaluate the performance of the model on this task.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text ltx_font_bold">Open-ended MedVQA. </span>
The total possible answers for PMC-VQA are over <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mn id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><cn type="integer" id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">100</annotation></semantics></math>K,
which makes the traditional retrieval-based approach limited in usefulness for the answer set of such a level. Therefore, we provide another training style, called “blank”,
where the network is not provided with options in input and is required to <span id="S4.SS2.p3.1.2" class="ltx_text ltx_font_bold">directly generate answers</span> based on the questions.
For evaluation, we adopt two metrics. The first is Bleu scores,
which are widely used to evaluate the quality of generated text against a set of references. The second is ACC scores, which can be computed by comparing the generated answer with the ground-truth answer using sentence similarity, as introduced in Sec. <a href="#S4.SS1" title="4.1 Existing MedVQA Datasets ‣ 4 Experiments ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Pre-trained Backbones</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">In this section, we introduce the pre-trained models used in our experiments. We separate them into language and vision backbones. Notably, while all the following models can be used in our architecture, by default,
we use the “PMC-LLaMA” (or “PMC-LLaMA-ENC”) and “PMC-CLIP” as backbones, since they are known to be more suitable for medical data according to previous works.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Language Backbone</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p"><span id="S4.SS3.SSS1.p1.1.1" class="ltx_text ltx_font_bold">LLaMA <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite></span> is a state-of-the-art large-scale language model, pre-trained on trillions of tokens and widely used in the research community.
We adopt the 7B version, which consists of 32 transformer layers, as our language backbone.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p"><span id="S4.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">PMC-LLaMA <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite></span> is an open-source language model that is acquired by fine-tuning LLaMA-7B on a total of 4.8 million biomedical academic papers with auto-regressive loss. Compared to LLaMA, PMC-LLaMA demonstrates stronger fitting capabilities and better performance on medical tasks.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p"><span id="S4.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">PubMedBERT <cite class="ltx_cite ltx_citemacro_cite">Gu et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite></span> is an encoder-based BERT-like model that is trained from scratch using abstracts from PubMed and full-text articles from PubMedCentral in the corpus “The Pile” <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib10" title="" class="ltx_ref">2020</a>)</cite>. It has 12 transformer layers and 100 million parameters. Such domain-specific models proved to yield excellent text embedding capability before the era of large language models.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p4.1" class="ltx_p"><span id="S4.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_bold">LLaMA-ENC and PMC-LLaMA-ENC.</span> While LLaMA and PMC-LLaMA are known for their performance in text generation tasks, we also experiment with them as encoder models by passing a full attention mask and sampling the embedding from the last token. This allows for a direct comparison to be made with the aforementioned BERT-like models, which are also encoder-based.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Vision Backbone</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p"><span id="S4.SS3.SSS2.p1.1.1" class="ltx_text ltx_font_bold">CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a href="#bib.bib30" title="" class="ltx_ref">2021</a>)</cite></span> is a model trained from scratch on a dataset of 400 million image-text pairs collected from the internet with contrastive loss. We use its “ViT-base-patch32” version as our visual encoder with 12 transformer layers, which has been pre-trained on natural images.</p>
</div>
<div id="S4.SS3.SSS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p"><span id="S4.SS3.SSS2.p2.1.1" class="ltx_text ltx_font_bold">PMC-CLIP <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite></span> is a medical-specific visual model based on CLIP architecture, which was trained on a dataset of <math id="S4.SS3.SSS2.p2.1.m1.1" class="ltx_Math" alttext="1.6" display="inline"><semantics id="S4.SS3.SSS2.p2.1.m1.1a"><mn id="S4.SS3.SSS2.p2.1.m1.1.1" xref="S4.SS3.SSS2.p2.1.m1.1.1.cmml">1.6</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS2.p2.1.m1.1b"><cn type="float" id="S4.SS3.SSS2.p2.1.m1.1.1.cmml" xref="S4.SS3.SSS2.p2.1.m1.1.1">1.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS2.p2.1.m1.1c">1.6</annotation></semantics></math> million biomedical image-text pairs collected from PubMed open-access papers using cross-modality contrastive loss. Compared to the pre-trained visual model on natural images, PMC-CLIP is specifically designed to handle medical images and text.</p>
</div>
</section>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we begin by evaluating our model on two publicly-available datasets, VQA-RAD and SLAKE, and compare it with existing MedVQA models, showing state-of-the-art performance. However, these datasets have limited diversity and scope, which led us to propose a more challenging MedVQA benchmark in Sec. <a href="#S5.SS2" title="5.2 Benchmark on PMC-VQA ‣ 5 Results ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>. Our benchmark covers significantly more diverse modalities and diseases, and we demonstrate that even state-of-the-art methods struggle to perform well on it.</p>
</div>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison of ACC to SOTA approaches on VQA-RAD and SLAKE.
We use the blank model for evaluation.
Pre-training data indicates whether the model is pre-trained on the medical multi-modal dataset before training on the target dataset.
The best result is in red, the second-best result is in blue. “Overal” refers to the micro-average ACC of all the Open and Close questions.
</figcaption>
<table id="S5.T2.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.3.1.1" class="ltx_tr">
<th id="S5.T2.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" style="padding-top:-0.4pt;padding-bottom:-0.4pt;" rowspan="2"><span id="S5.T2.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<th id="S5.T2.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" style="padding-top:-0.4pt;padding-bottom:-0.4pt;" rowspan="2"><span id="S5.T2.3.1.1.2.1" class="ltx_text" style="font-size:80%;">Pre-training Data<sup id="S5.T2.3.1.1.2.1.1" class="ltx_sup">*</sup></span></th>
<th id="S5.T2.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:-0.4pt;padding-bottom:-0.4pt;" colspan="3"><span id="S5.T2.3.1.1.3.1" class="ltx_text" style="font-size:80%;">VQA-RAD</span></th>
<th id="S5.T2.3.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" style="padding-top:-0.4pt;padding-bottom:-0.4pt;" colspan="3"><span id="S5.T2.3.1.1.4.1" class="ltx_text" style="font-size:80%;">SLAKE</span></th>
</tr>
<tr id="S5.T2.3.2.2" class="ltx_tr">
<th id="S5.T2.3.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.2.2.1.1" class="ltx_text" style="font-size:80%;">Open</span></th>
<th id="S5.T2.3.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.2.2.2.1" class="ltx_text" style="font-size:80%;">Close</span></th>
<th id="S5.T2.3.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.2.2.3.1" class="ltx_text" style="font-size:80%;">Overall</span></th>
<th id="S5.T2.3.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.2.2.4.1" class="ltx_text" style="font-size:80%;">Open</span></th>
<th id="S5.T2.3.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.2.2.5.1" class="ltx_text" style="font-size:80%;">Close</span></th>
<th id="S5.T2.3.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.2.2.6.1" class="ltx_text" style="font-size:80%;">Overall</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.3.3.1" class="ltx_tr">
<th id="S5.T2.3.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;">
<span id="S5.T2.3.3.1.1.1" class="ltx_text" style="font-size:80%;">MEVF-BAN </span><cite class="ltx_cite ltx_citemacro_cite">Nguyen et al. <span id="S5.T2.3.3.1.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib25" title="" class="ltx_ref">2019</a><span id="S5.T2.3.3.1.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T2.3.3.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.3.1.2.1" class="ltx_text" style="font-size:80%;">–</span></th>
<td id="S5.T2.3.3.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.3.1.3.1" class="ltx_text" style="font-size:80%;">49.2</span></td>
<td id="S5.T2.3.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.3.1.4.1" class="ltx_text" style="font-size:80%;">77.2</span></td>
<td id="S5.T2.3.3.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.3.1.5.1" class="ltx_text" style="font-size:80%;">66.1</span></td>
<td id="S5.T2.3.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.3.1.6.1" class="ltx_text" style="font-size:80%;">77.8</span></td>
<td id="S5.T2.3.3.1.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.3.1.7.1" class="ltx_text" style="font-size:80%;">79.8</span></td>
<td id="S5.T2.3.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.3.1.8.1" class="ltx_text" style="font-size:80%;">78.6</span></td>
</tr>
<tr id="S5.T2.3.4.2" class="ltx_tr">
<th id="S5.T2.3.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.4pt;padding-bottom:-0.4pt;">
<span id="S5.T2.3.4.2.1.1" class="ltx_text" style="font-size:80%;">CPRD-BAN </span><cite class="ltx_cite ltx_citemacro_cite">Liu et al. <span id="S5.T2.3.4.2.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib22" title="" class="ltx_ref">2021a</a><span id="S5.T2.3.4.2.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T2.3.4.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.4.2.2.1" class="ltx_text" style="font-size:80%;">–</span></th>
<td id="S5.T2.3.4.2.3" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.4.2.3.1" class="ltx_text" style="font-size:80%;">52.5</span></td>
<td id="S5.T2.3.4.2.4" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.4.2.4.1" class="ltx_text" style="font-size:80%;">77.9</span></td>
<td id="S5.T2.3.4.2.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.4.2.5.1" class="ltx_text" style="font-size:80%;">67.8</span></td>
<td id="S5.T2.3.4.2.6" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.4.2.6.1" class="ltx_text" style="font-size:80%;">79.5</span></td>
<td id="S5.T2.3.4.2.7" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.4.2.7.1" class="ltx_text" style="font-size:80%;">83.4</span></td>
<td id="S5.T2.3.4.2.8" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.4.2.8.1" class="ltx_text" style="font-size:80%;">81.1</span></td>
</tr>
<tr id="S5.T2.3.5.3" class="ltx_tr">
<th id="S5.T2.3.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.4pt;padding-bottom:-0.4pt;">
<span id="S5.T2.3.5.3.1.1" class="ltx_text" style="font-size:80%;">M3AE </span><cite class="ltx_cite ltx_citemacro_cite">Chen et al. <span id="S5.T2.3.5.3.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib6" title="" class="ltx_ref">2022</a><span id="S5.T2.3.5.3.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T2.3.5.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;">
<span id="S5.T2.3.5.3.2.1" class="ltx_text" style="font-size:80%;">ROCO </span><cite class="ltx_cite ltx_citemacro_cite">Pelka et al. <span id="S5.T2.3.5.3.2.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib29" title="" class="ltx_ref">2018</a><span id="S5.T2.3.5.3.2.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite><span id="S5.T2.3.5.3.2.4" class="ltx_text" style="font-size:80%;">, MedICaT </span><cite class="ltx_cite ltx_citemacro_cite">Subramanian et al. <span id="S5.T2.3.5.3.2.5.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib33" title="" class="ltx_ref">2020</a><span id="S5.T2.3.5.3.2.6.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T2.3.5.3.3" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.5.3.3.1" class="ltx_text" style="font-size:80%;">67.2</span></td>
<td id="S5.T2.3.5.3.4" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.5.3.4.1" class="ltx_text" style="font-size:80%;">83.5</span></td>
<td id="S5.T2.3.5.3.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.5.3.5.1" class="ltx_text" style="font-size:80%;">77.0</span></td>
<td id="S5.T2.3.5.3.6" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.5.3.6.1" class="ltx_text" style="font-size:80%;">80.3</span></td>
<td id="S5.T2.3.5.3.7" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.5.3.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">87.8</span></td>
<td id="S5.T2.3.5.3.8" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.5.3.8.1" class="ltx_text" style="font-size:80%;">83.3</span></td>
</tr>
<tr id="S5.T2.3.6.4" class="ltx_tr">
<th id="S5.T2.3.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.4pt;padding-bottom:-0.4pt;">
<span id="S5.T2.3.6.4.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T2.3.6.4.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T2.3.6.4.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T2.3.6.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;">
<span id="S5.T2.3.6.4.2.1" class="ltx_text" style="font-size:80%;">PMC-OA </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T2.3.6.4.2.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T2.3.6.4.2.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T2.3.6.4.3" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.6.4.3.1" class="ltx_text" style="font-size:80%;">67.0</span></td>
<td id="S5.T2.3.6.4.4" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.6.4.4.1" class="ltx_text" style="font-size:80%;">84.0</span></td>
<td id="S5.T2.3.6.4.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.6.4.5.1" class="ltx_text" style="font-size:80%;">77.6</span></td>
<td id="S5.T2.3.6.4.6" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.6.4.6.1" class="ltx_text" style="font-size:80%;">81.9</span></td>
<td id="S5.T2.3.6.4.7" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.6.4.7.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">88.0</span></td>
<td id="S5.T2.3.6.4.8" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.6.4.8.1" class="ltx_text" style="font-size:80%;">84.3</span></td>
</tr>
<tr id="S5.T2.3.7.5" class="ltx_tr">
<th id="S5.T2.3.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.7.5.1.1" class="ltx_text" style="font-size:80%;">MedVInT-TE-S</span></th>
<th id="S5.T2.3.7.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.7.5.2.1" class="ltx_text" style="font-size:80%;">–</span></th>
<td id="S5.T2.3.7.5.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.7.5.3.1" class="ltx_text" style="font-size:80%;">53.6</span></td>
<td id="S5.T2.3.7.5.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.7.5.4.1" class="ltx_text" style="font-size:80%;">76.5</span></td>
<td id="S5.T2.3.7.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.7.5.5.1" class="ltx_text" style="font-size:80%;">67.4</span></td>
<td id="S5.T2.3.7.5.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.7.5.6.1" class="ltx_text" style="font-size:80%;">84.0</span></td>
<td id="S5.T2.3.7.5.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.7.5.7.1" class="ltx_text" style="font-size:80%;">85.1</span></td>
<td id="S5.T2.3.7.5.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.7.5.8.1" class="ltx_text" style="font-size:80%;">84.4</span></td>
</tr>
<tr id="S5.T2.3.8.6" class="ltx_tr">
<th id="S5.T2.3.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.8.6.1.1" class="ltx_text" style="font-size:80%;">MedVInT-TD-S</span></th>
<th id="S5.T2.3.8.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.8.6.2.1" class="ltx_text" style="font-size:80%;">–</span></th>
<td id="S5.T2.3.8.6.3" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.8.6.3.1" class="ltx_text" style="font-size:80%;">55.3</span></td>
<td id="S5.T2.3.8.6.4" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.8.6.4.1" class="ltx_text" style="font-size:80%;">80.5</span></td>
<td id="S5.T2.3.8.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.8.6.5.1" class="ltx_text" style="font-size:80%;">70.5</span></td>
<td id="S5.T2.3.8.6.6" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.8.6.6.1" class="ltx_text" style="font-size:80%;">79.7</span></td>
<td id="S5.T2.3.8.6.7" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.8.6.7.1" class="ltx_text" style="font-size:80%;">85.1</span></td>
<td id="S5.T2.3.8.6.8" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.8.6.8.1" class="ltx_text" style="font-size:80%;">81.8</span></td>
</tr>
<tr id="S5.T2.3.9.7" class="ltx_tr">
<th id="S5.T2.3.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.9.7.1.1" class="ltx_text" style="font-size:80%;">MedVInT-TE</span></th>
<th id="S5.T2.3.9.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.9.7.2.1" class="ltx_text" style="font-size:80%;">PMC-VQA</span></th>
<td id="S5.T2.3.9.7.3" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.9.7.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">69.3</span></td>
<td id="S5.T2.3.9.7.4" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.9.7.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">84.2</span></td>
<td id="S5.T2.3.9.7.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.9.7.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">78.2</span></td>
<td id="S5.T2.3.9.7.6" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.9.7.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">88.2</span></td>
<td id="S5.T2.3.9.7.7" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.9.7.7.1" class="ltx_text" style="font-size:80%;">87.7</span></td>
<td id="S5.T2.3.9.7.8" class="ltx_td ltx_align_center" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.9.7.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">88.0</span></td>
</tr>
<tr id="S5.T2.3.10.8" class="ltx_tr">
<th id="S5.T2.3.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.10.8.1.1" class="ltx_text" style="font-size:80%;">MedVInT-TD</span></th>
<th id="S5.T2.3.10.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.10.8.2.1" class="ltx_text" style="font-size:80%;">PMC-VQA</span></th>
<td id="S5.T2.3.10.8.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.10.8.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">73.7</span></td>
<td id="S5.T2.3.10.8.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.10.8.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">86.8</span></td>
<td id="S5.T2.3.10.8.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.10.8.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">81.6</span></td>
<td id="S5.T2.3.10.8.6" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.10.8.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">84.5</span></td>
<td id="S5.T2.3.10.8.7" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.10.8.7.1" class="ltx_text" style="font-size:80%;">86.3</span></td>
<td id="S5.T2.3.10.8.8" class="ltx_td ltx_align_center ltx_border_bb" style="padding-top:-0.4pt;padding-bottom:-0.4pt;"><span id="S5.T2.3.10.8.8.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">85.2</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison on Existing Datasets</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">As shown in Tab. <a href="#S5.T2" title="Table 2 ‣ 5 Results ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, comparing our model to existing ones,
we can draw the following observations:</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">State-of-the-art Performance of Generative MedVQA.</span>
As shown in Tab. <a href="#S5.T2" title="Table 2 ‣ 5 Results ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our MedVInT model outperforms the previous state-of-the-art (SOTA) methods on both the VQA-RAD and SLAKE datasets, regardless of whether the “MedVInT-TE” or “MedVInT-TD” variant is used. We improved the overall accuracy (ACC) scores from 77.6% to 81.6% on VQA-RAD and from 84.3% to 88.0% on SLAKE. Notably, since our model generates answers rather than retrieving one from a pre-defined answer basis, the evaluation metric is more challenging, further demonstrating our superiority.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.2" class="ltx_p"><span id="S5.SS1.p3.2.1" class="ltx_text ltx_font_bold">Pre-training on PMC-VQA is Essential for Generative MedVQA.</span>
Comparing results using the same architecture, with and without PMC-VQA, it is clear that pre-training with PMC-VQA significantly outperforms. Specifically, “MedVInT-TE” boosts the final results by approximately <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="11\%" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">11</mn><mo id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">11\%</annotation></semantics></math> on VQA-RAD and <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="4\%" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mrow id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml"><mn id="S5.SS1.p3.2.m2.1.1.2" xref="S5.SS1.p3.2.m2.1.1.2.cmml">4</mn><mo id="S5.SS1.p3.2.m2.1.1.1" xref="S5.SS1.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><apply id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.p3.2.m2.1.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p3.2.m2.1.1.2.cmml" xref="S5.SS1.p3.2.m2.1.1.2">4</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">4\%</annotation></semantics></math> on SLAKE compared to “MedVInT-TE-S” that refers to training the model from scratch without pre-trained on PMC-VQA. Similar improvements are observed with ’MedVInT-TD’.
These results highlight the critical role that our PMC-VQA plays in addressing the major challenges that hinder the development of a generative MedVQA system.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para ltx_noindent">
<p id="S5.SS1.p4.1" class="ltx_p"><span id="S5.SS1.p4.1.1" class="ltx_text ltx_font_bold">Both MedVInT-TE and MedVInT-TD Perform Well.</span> The gap between the two training styles mainly exists in open-ended questions, with “MedVInT-TD” performing better on VQA-RAD and “MedVInT-TE” being more effective on SLAKE. This difference can be attributed to the fact that the VQA-RAD answers are typically longer than those in SLAKE, making the “MedVInT-TD” model more suitable for generating such answers. Conversely, SLAKE questions often require short and concise responses, making the MedVInT-TE” model more appropriate for such retrieve-like tasks.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Benchmark on PMC-VQA</h3>

<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of baseline models using different pre-trained models on both open-ended and multiple-choice tasks. We reported the results on PMC-VQA-test / PMC-VQA-test-clean. “Scratch” means to train the vision model from scratch with the same architecture as “PMC-CLIP”.</figcaption>
<table id="S5.T3.3" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.3.1.1" class="ltx_tr">
<th id="S5.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S5.T3.3.1.1.1.1" class="ltx_text" style="font-size:80%;">Method</span></th>
<th id="S5.T3.3.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S5.T3.3.1.1.2.1" class="ltx_text" style="font-size:80%;">Language Backbone</span></th>
<th id="S5.T3.3.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="2"><span id="S5.T3.3.1.1.3.1" class="ltx_text" style="font-size:80%;">Vision Backbone</span></th>
<td id="S5.T3.3.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="2"><span id="S5.T3.3.1.1.4.1" class="ltx_text" style="font-size:80%;">Blanking</span></td>
<td id="S5.T3.3.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.1.1.5.1" class="ltx_text" style="font-size:80%;">Choice</span></td>
</tr>
<tr id="S5.T3.3.2.2" class="ltx_tr">
<td id="S5.T3.3.2.2.1" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.2.2.1.1" class="ltx_text" style="font-size:80%;">ACC</span></td>
<td id="S5.T3.3.2.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.2.2.2.1" class="ltx_text" style="font-size:80%;">Bleu-1</span></td>
<td id="S5.T3.3.2.2.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.2.2.3.1" class="ltx_text" style="font-size:80%;">ACC</span></td>
</tr>
<tr id="S5.T3.3.3.3" class="ltx_tr">
<th id="S5.T3.3.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="6"><span id="S5.T3.3.3.3.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Zero-shot</span></th>
</tr>
<tr id="S5.T3.3.4.4" class="ltx_tr">
<th id="S5.T3.3.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.4.4.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.4.4.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.4.4.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T3.3.4.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.4.4.2.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.4.4.2.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.4.4.2.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T3.3.4.4.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.4.4.3.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.4.4.3.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.4.4.3.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.4.4.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.4.4.4.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S5.T3.3.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.4.4.5.1" class="ltx_text" style="font-size:80%;">–</span></td>
<td id="S5.T3.3.4.4.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.4.4.6.1" class="ltx_text" style="font-size:80%;">24.0 / 24.7</span></td>
</tr>
<tr id="S5.T3.3.5.5" class="ltx_tr">
<th id="S5.T3.3.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.5.5.1.1" class="ltx_text" style="font-size:80%;">BLIP-2 </span><cite class="ltx_cite ltx_citemacro_cite">Li et al. <span id="S5.T3.3.5.5.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib19" title="" class="ltx_ref">2023</a><span id="S5.T3.3.5.5.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T3.3.5.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.5.5.2.1" class="ltx_text" style="font-size:80%;">OPT-2.7B </span><cite class="ltx_cite ltx_citemacro_cite">Zhang et al. <span id="S5.T3.3.5.5.2.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib38" title="" class="ltx_ref">2022</a><span id="S5.T3.3.5.5.2.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T3.3.5.5.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.5.5.3.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.5.5.3.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.5.5.3.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.5.5.4.1" class="ltx_text" style="font-size:80%;">22.5 / 21.8</span></td>
<td id="S5.T3.3.5.5.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.5.5.5.1" class="ltx_text" style="font-size:80%;">5.2 / 7.6</span></td>
<td id="S5.T3.3.5.5.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.5.5.6.1" class="ltx_text" style="font-size:80%;">24.6 / 24.3</span></td>
</tr>
<tr id="S5.T3.3.6.6" class="ltx_tr">
<th id="S5.T3.3.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.6.6.1.1" class="ltx_text" style="font-size:80%;">Open-Flamingo </span><cite class="ltx_cite ltx_citemacro_cite">Awadalla et al. <span id="S5.T3.3.6.6.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib4" title="" class="ltx_ref">2023</a><span id="S5.T3.3.6.6.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T3.3.6.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.6.6.2.1" class="ltx_text" style="font-size:80%;">LLaMA</span><cite class="ltx_cite ltx_citemacro_cite">Touvron et al. <span id="S5.T3.3.6.6.2.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib35" title="" class="ltx_ref">2023</a><span id="S5.T3.3.6.6.2.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T3.3.6.6.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.6.6.3.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.6.6.3.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.6.6.3.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.6.6.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.6.6.4.1" class="ltx_text" style="font-size:80%;">26.1 / 26.5</span></td>
<td id="S5.T3.3.6.6.5" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.6.6.5.1" class="ltx_text" style="font-size:80%;">4.1 / 4.1</span></td>
<td id="S5.T3.3.6.6.6" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.6.6.6.1" class="ltx_text" style="font-size:80%;">25.0 / 26.4</span></td>
</tr>
<tr id="S5.T3.3.7.7" class="ltx_tr">
<th id="S5.T3.3.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="6"><span id="S5.T3.3.7.7.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Trained on PMC-VQA</span></th>
</tr>
<tr id="S5.T3.3.8.8" class="ltx_tr">
<th id="S5.T3.3.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.8.8.1.1" class="ltx_text" style="font-size:80%;">LLaMA </span><cite class="ltx_cite ltx_citemacro_cite">Touvron et al. <span id="S5.T3.3.8.8.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib35" title="" class="ltx_ref">2023</a><span id="S5.T3.3.8.8.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T3.3.8.8.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.8.8.2.1" class="ltx_text" style="font-size:80%;">LLaMA </span><cite class="ltx_cite ltx_citemacro_cite">Touvron et al. <span id="S5.T3.3.8.8.2.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib35" title="" class="ltx_ref">2023</a><span id="S5.T3.3.8.8.2.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<th id="S5.T3.3.8.8.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.8.8.3.1" class="ltx_text" style="font-size:80%;">–</span></th>
<td id="S5.T3.3.8.8.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.8.8.4.1" class="ltx_text" style="font-size:80%;">26.1 / 27.2</span></td>
<td id="S5.T3.3.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.8.8.5.1" class="ltx_text" style="font-size:80%;">14.2 / 14.6</span></td>
<td id="S5.T3.3.8.8.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.8.8.6.1" class="ltx_text" style="font-size:80%;">30.6 / 30.8</span></td>
</tr>
<tr id="S5.T3.3.9.9" class="ltx_tr">
<th id="S5.T3.3.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="9"><span id="S5.T3.3.9.9.1.1" class="ltx_text" style="font-size:80%;">MedVInT-TE-MLP</span></th>
<th id="S5.T3.3.9.9.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.9.9.2.1" class="ltx_text" style="font-size:80%;">PubMedBERT <cite class="ltx_cite ltx_citemacro_cite">Gu et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite></span></th>
<th id="S5.T3.3.9.9.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.9.9.3.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.9.9.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.9.9.4.1" class="ltx_text" style="font-size:80%;">33.7 / 34.2</span></td>
<td id="S5.T3.3.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.9.9.5.1" class="ltx_text" style="font-size:80%;">20.4 / 20.9</span></td>
<td id="S5.T3.3.9.9.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.9.9.6.1" class="ltx_text" style="font-size:80%;">34.4 / 34.9</span></td>
</tr>
<tr id="S5.T3.3.10.10" class="ltx_tr">
<th id="S5.T3.3.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.10.10.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.10.10.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.10.10.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.10.10.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.10.10.2.1" class="ltx_text" style="font-size:80%;">33.7 / 34.4</span></td>
<td id="S5.T3.3.10.10.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.10.10.3.1" class="ltx_text" style="font-size:80%;">20.4 / 20.8</span></td>
<td id="S5.T3.3.10.10.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.10.10.4.1" class="ltx_text" style="font-size:80%;">34.5 / 34.3</span></td>
</tr>
<tr id="S5.T3.3.11.11" class="ltx_tr">
<th id="S5.T3.3.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.11.11.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.11.11.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.11.11.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.11.11.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.11.11.2.1" class="ltx_text" style="font-size:80%;">35.2 / 36.4</span></td>
<td id="S5.T3.3.11.11.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.11.11.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">22.0</span><span id="S5.T3.3.11.11.3.2" class="ltx_text" style="font-size:80%;"> / </span><span id="S5.T3.3.11.11.3.3" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">23.2</span>
</td>
<td id="S5.T3.3.11.11.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.11.11.4.1" class="ltx_text" style="font-size:80%;">37.1 / 37.6</span></td>
</tr>
<tr id="S5.T3.3.12.12" class="ltx_tr">
<th id="S5.T3.3.12.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.12.12.1.1" class="ltx_text" style="font-size:80%;">LLaMA-ENC <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite></span></th>
<th id="S5.T3.3.12.12.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.12.12.2.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.12.12.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.12.12.3.1" class="ltx_text" style="font-size:80%;">32.5 / 32.5</span></td>
<td id="S5.T3.3.12.12.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.12.12.4.1" class="ltx_text" style="font-size:80%;">15.3 / 15.9</span></td>
<td id="S5.T3.3.12.12.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.12.12.5.1" class="ltx_text" style="font-size:80%;">35.2 / 35.1</span></td>
</tr>
<tr id="S5.T3.3.13.13" class="ltx_tr">
<th id="S5.T3.3.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.13.13.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.13.13.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.13.13.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.13.13.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.13.13.2.1" class="ltx_text" style="font-size:80%;">32.3 / 33.4</span></td>
<td id="S5.T3.3.13.13.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.13.13.3.1" class="ltx_text" style="font-size:80%;">15.6 / 15.1</span></td>
<td id="S5.T3.3.13.13.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.13.13.4.1" class="ltx_text" style="font-size:80%;">35.3 / 36.1</span></td>
</tr>
<tr id="S5.T3.3.14.14" class="ltx_tr">
<th id="S5.T3.3.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.14.14.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.14.14.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.14.14.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.14.14.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.14.14.2.1" class="ltx_text" style="font-size:80%;">35.4 / </span><span id="S5.T3.3.14.14.2.2" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">36.8</span>
</td>
<td id="S5.T3.3.14.14.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.14.14.3.1" class="ltx_text" style="font-size:80%;">18.2 / 18.4</span></td>
<td id="S5.T3.3.14.14.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.14.14.4.1" class="ltx_text" style="font-size:80%;">36.9 / 37.1</span></td>
</tr>
<tr id="S5.T3.3.15.15" class="ltx_tr">
<th id="S5.T3.3.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.15.15.1.1" class="ltx_text" style="font-size:80%;">PMC-LLaMA-ENC <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite></span></th>
<th id="S5.T3.3.15.15.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.15.15.2.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.15.15.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.15.15.3.1" class="ltx_text" style="font-size:80%;">32.6 / 35.0</span></td>
<td id="S5.T3.3.15.15.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.15.15.4.1" class="ltx_text" style="font-size:80%;">16.2 / 17.0</span></td>
<td id="S5.T3.3.15.15.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.15.15.5.1" class="ltx_text" style="font-size:80%;">37.0 / 38.0</span></td>
</tr>
<tr id="S5.T3.3.16.16" class="ltx_tr">
<th id="S5.T3.3.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.16.16.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.16.16.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.16.16.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.16.16.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.16.16.2.1" class="ltx_text" style="font-size:80%;">33.0 / 34.4</span></td>
<td id="S5.T3.3.16.16.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.16.16.3.1" class="ltx_text" style="font-size:80%;">16.6 / 16.5</span></td>
<td id="S5.T3.3.16.16.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.16.16.4.1" class="ltx_text" style="font-size:80%;">37.1 / 38.5</span></td>
</tr>
<tr id="S5.T3.3.17.17" class="ltx_tr">
<th id="S5.T3.3.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.17.17.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.17.17.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.17.17.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.17.17.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.17.17.2.1" class="ltx_text" style="font-size:80%;">34.8 / 35.3</span></td>
<td id="S5.T3.3.17.17.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.17.17.3.1" class="ltx_text" style="font-size:80%;">18.1 / 18.6</span></td>
<td id="S5.T3.3.17.17.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.17.17.4.1" class="ltx_text" style="font-size:80%;">38.2 / 39.2</span></td>
</tr>
<tr id="S5.T3.3.18.18" class="ltx_tr">
<th id="S5.T3.3.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="9"><span id="S5.T3.3.18.18.1.1" class="ltx_text" style="font-size:80%;">MedVInT-TE-Transformer</span></th>
<th id="S5.T3.3.18.18.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.18.18.2.1" class="ltx_text" style="font-size:80%;">PubMedBERT <cite class="ltx_cite ltx_citemacro_cite">Gu et al. (<a href="#bib.bib11" title="" class="ltx_ref">2021</a>)</cite></span></th>
<th id="S5.T3.3.18.18.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.18.18.3.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.18.18.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.18.18.4.1" class="ltx_text" style="font-size:80%;">34.1 / 36.2</span></td>
<td id="S5.T3.3.18.18.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.18.18.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">21.0</span><span id="S5.T3.3.18.18.5.2" class="ltx_text" style="font-size:80%;"> / </span><span id="S5.T3.3.18.18.5.3" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">21.9</span>
</td>
<td id="S5.T3.3.18.18.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.18.18.6.1" class="ltx_text" style="font-size:80%;">39.8 / 40.6</span></td>
</tr>
<tr id="S5.T3.3.19.19" class="ltx_tr">
<th id="S5.T3.3.19.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.19.19.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.19.19.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.19.19.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.19.19.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.19.19.2.1" class="ltx_text" style="font-size:80%;">33.9 / 34.6</span></td>
<td id="S5.T3.3.19.19.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.19.19.3.1" class="ltx_text" style="font-size:80%;">20.6 / 21.8</span></td>
<td id="S5.T3.3.19.19.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.19.19.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">39.9</span><span id="S5.T3.3.19.19.4.2" class="ltx_text" style="font-size:80%;"> / 40.9</span>
</td>
</tr>
<tr id="S5.T3.3.20.20" class="ltx_tr">
<th id="S5.T3.3.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.20.20.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.20.20.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.20.20.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.20.20.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.20.20.2.1" class="ltx_text" style="font-size:80%;">33.7 / 35.4</span></td>
<td id="S5.T3.3.20.20.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.20.20.3.1" class="ltx_text" style="font-size:80%;">20.3 / 21.2</span></td>
<td id="S5.T3.3.20.20.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.20.20.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">40.2</span><span id="S5.T3.3.20.20.4.2" class="ltx_text" style="font-size:80%;"> / 40.9</span>
</td>
</tr>
<tr id="S5.T3.3.21.21" class="ltx_tr">
<th id="S5.T3.3.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.21.21.1.1" class="ltx_text" style="font-size:80%;">LLaMA-ENC <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite></span></th>
<th id="S5.T3.3.21.21.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.21.21.2.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.21.21.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.21.21.3.1" class="ltx_text" style="font-size:80%;">32.0 / 33.5</span></td>
<td id="S5.T3.3.21.21.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.21.21.4.1" class="ltx_text" style="font-size:80%;">15.1 / 15.3</span></td>
<td id="S5.T3.3.21.21.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.21.21.5.1" class="ltx_text" style="font-size:80%;">38.4 / 39.7</span></td>
</tr>
<tr id="S5.T3.3.22.22" class="ltx_tr">
<th id="S5.T3.3.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.22.22.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.22.22.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.22.22.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.22.22.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.22.22.2.1" class="ltx_text" style="font-size:80%;">32.3 / 34.3</span></td>
<td id="S5.T3.3.22.22.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.22.22.3.1" class="ltx_text" style="font-size:80%;">15.5 / 15.7</span></td>
<td id="S5.T3.3.22.22.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.22.22.4.1" class="ltx_text" style="font-size:80%;">38.4 / 38.7</span></td>
</tr>
<tr id="S5.T3.3.23.23" class="ltx_tr">
<th id="S5.T3.3.23.23.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.23.23.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.23.23.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.23.23.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.23.23.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.23.23.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">35.9</span><span id="S5.T3.3.23.23.2.2" class="ltx_text" style="font-size:80%;"> / </span><span id="S5.T3.3.23.23.2.3" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">37.1</span>
</td>
<td id="S5.T3.3.23.23.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.23.23.3.1" class="ltx_text" style="font-size:80%;">19.0 / 19.3</span></td>
<td id="S5.T3.3.23.23.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.23.23.4.1" class="ltx_text" style="font-size:80%;">38.9 / 39.4</span></td>
</tr>
<tr id="S5.T3.3.24.24" class="ltx_tr">
<th id="S5.T3.3.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.24.24.1.1" class="ltx_text" style="font-size:80%;">PMC-LLaMA-ENC <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite></span></th>
<th id="S5.T3.3.24.24.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.24.24.2.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.24.24.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.24.24.3.1" class="ltx_text" style="font-size:80%;">33.2 / 34.7</span></td>
<td id="S5.T3.3.24.24.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.24.24.4.1" class="ltx_text" style="font-size:80%;">16.6 / 16.5</span></td>
<td id="S5.T3.3.24.24.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.24.24.5.1" class="ltx_text" style="font-size:80%;">38.1 /39.8</span></td>
</tr>
<tr id="S5.T3.3.25.25" class="ltx_tr">
<th id="S5.T3.3.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.25.25.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.25.25.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.25.25.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.25.25.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.25.25.2.1" class="ltx_text" style="font-size:80%;">33.6 / 35.1</span></td>
<td id="S5.T3.3.25.25.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.25.25.3.1" class="ltx_text" style="font-size:80%;">16.7 / 17.2</span></td>
<td id="S5.T3.3.25.25.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.25.25.4.1" class="ltx_text" style="font-size:80%;">38.7 / 38.9</span></td>
</tr>
<tr id="S5.T3.3.26.26" class="ltx_tr">
<th id="S5.T3.3.26.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.26.26.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.26.26.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.26.26.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.26.26.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.26.26.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">35.5</span><span id="S5.T3.3.26.26.2.2" class="ltx_text" style="font-size:80%;"> / 36.0</span>
</td>
<td id="S5.T3.3.26.26.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.26.26.3.1" class="ltx_text" style="font-size:80%;">18.4 /18.6</span></td>
<td id="S5.T3.3.26.26.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.26.26.4.1" class="ltx_text" style="font-size:80%;">38.2 / 37.7</span></td>
</tr>
<tr id="S5.T3.3.27.27" class="ltx_tr">
<th id="S5.T3.3.27.27.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="6"><span id="S5.T3.3.27.27.1.1" class="ltx_text" style="font-size:80%;">MedVInT-TD-MLP</span></th>
<th id="S5.T3.3.27.27.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.27.27.2.1" class="ltx_text" style="font-size:80%;">LLaMA<cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite></span></th>
<th id="S5.T3.3.27.27.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.27.27.3.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.27.27.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.27.27.4.1" class="ltx_text" style="font-size:80%;">28.1 / 30.6</span></td>
<td id="S5.T3.3.27.27.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.27.27.5.1" class="ltx_text" style="font-size:80%;">16.5 / 16.9</span></td>
<td id="S5.T3.3.27.27.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.27.27.6.1" class="ltx_text" style="font-size:80%;">35.8 / 37.4</span></td>
</tr>
<tr id="S5.T3.3.28.28" class="ltx_tr">
<th id="S5.T3.3.28.28.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.28.28.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.28.28.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.28.28.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.28.28.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.28.28.2.1" class="ltx_text" style="font-size:80%;">30.2 / 32.7</span></td>
<td id="S5.T3.3.28.28.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.28.28.3.1" class="ltx_text" style="font-size:80%;">18.6 / 18.5</span></td>
<td id="S5.T3.3.28.28.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.28.28.4.1" class="ltx_text" style="font-size:80%;">35.8 / 37.1</span></td>
</tr>
<tr id="S5.T3.3.29.29" class="ltx_tr">
<th id="S5.T3.3.29.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.29.29.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.29.29.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.29.29.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.29.29.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.29.29.2.1" class="ltx_text" style="font-size:80%;">31.3 / 32.6</span></td>
<td id="S5.T3.3.29.29.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.29.29.3.1" class="ltx_text" style="font-size:80%;">19.5 / 19.8</span></td>
<td id="S5.T3.3.29.29.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.29.29.4.1" class="ltx_text" style="font-size:80%;">38.4 / </span><span id="S5.T3.3.29.29.4.2" class="ltx_text ltx_font_bold" style="font-size:80%;color:#0000FF;">41.0</span>
</td>
</tr>
<tr id="S5.T3.3.30.30" class="ltx_tr">
<th id="S5.T3.3.30.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.30.30.1.1" class="ltx_text" style="font-size:80%;">PMC-LLaMA <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite></span></th>
<th id="S5.T3.3.30.30.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.30.30.2.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.30.30.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.30.30.3.1" class="ltx_text" style="font-size:80%;">28.3 / 30.6</span></td>
<td id="S5.T3.3.30.30.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.30.30.4.1" class="ltx_text" style="font-size:80%;">16.4 / 17.3</span></td>
<td id="S5.T3.3.30.30.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.30.30.5.1" class="ltx_text" style="font-size:80%;">35.8 / 37.0</span></td>
</tr>
<tr id="S5.T3.3.31.31" class="ltx_tr">
<th id="S5.T3.3.31.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.31.31.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.31.31.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.31.31.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.31.31.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.31.31.2.1" class="ltx_text" style="font-size:80%;">31.4 / 31.8</span></td>
<td id="S5.T3.3.31.31.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.31.31.3.1" class="ltx_text" style="font-size:80%;">19.2 / 19.5</span></td>
<td id="S5.T3.3.31.31.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.31.31.4.1" class="ltx_text" style="font-size:80%;">36.2 / 37.9</span></td>
</tr>
<tr id="S5.T3.3.32.32" class="ltx_tr">
<th id="S5.T3.3.32.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.32.32.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.32.32.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.32.32.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.32.32.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.32.32.2.1" class="ltx_text" style="font-size:80%;">32.1 / 31.7</span></td>
<td id="S5.T3.3.32.32.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.32.32.3.1" class="ltx_text" style="font-size:80%;">19.7 / 20.2</span></td>
<td id="S5.T3.3.32.32.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.32.32.4.1" class="ltx_text" style="font-size:80%;">38.4 / </span><span id="S5.T3.3.32.32.4.2" class="ltx_text ltx_font_bold" style="font-size:80%;color:#FF0000;">42.3</span>
</td>
</tr>
<tr id="S5.T3.3.33.33" class="ltx_tr">
<th id="S5.T3.3.33.33.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="6"><span id="S5.T3.3.33.33.1.1" class="ltx_text" style="font-size:80%;">MedVInT-TD-Transformer</span></th>
<th id="S5.T3.3.33.33.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.33.33.2.1" class="ltx_text" style="font-size:80%;">LLaMA<cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a href="#bib.bib35" title="" class="ltx_ref">2023</a>)</cite></span></th>
<th id="S5.T3.3.33.33.3" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.33.33.3.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.33.33.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.33.33.4.1" class="ltx_text" style="font-size:80%;">29.1 / 30.2</span></td>
<td id="S5.T3.3.33.33.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.33.33.5.1" class="ltx_text" style="font-size:80%;">17.4 / 18.0</span></td>
<td id="S5.T3.3.33.33.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.33.33.6.1" class="ltx_text" style="font-size:80%;">31.1 / 37.9</span></td>
</tr>
<tr id="S5.T3.3.34.34" class="ltx_tr">
<th id="S5.T3.3.34.34.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.34.34.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.34.34.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.34.34.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.34.34.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.34.34.2.1" class="ltx_text" style="font-size:80%;">31.3 / 32.2</span></td>
<td id="S5.T3.3.34.34.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.34.34.3.1" class="ltx_text" style="font-size:80%;">19.5 / 20.0</span></td>
<td id="S5.T3.3.34.34.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.34.34.4.1" class="ltx_text" style="font-size:80%;">38.2 / 38.3</span></td>
</tr>
<tr id="S5.T3.3.35.35" class="ltx_tr">
<th id="S5.T3.3.35.35.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.35.35.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.35.35.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.35.35.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.35.35.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.35.35.2.1" class="ltx_text" style="font-size:80%;">31.9 / 33.4</span></td>
<td id="S5.T3.3.35.35.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.35.35.3.1" class="ltx_text" style="font-size:80%;">20.0 / 21.3</span></td>
<td id="S5.T3.3.35.35.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.35.35.4.1" class="ltx_text" style="font-size:80%;">37.3 / 39.5</span></td>
</tr>
<tr id="S5.T3.3.36.36" class="ltx_tr">
<th id="S5.T3.3.36.36.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" rowspan="3"><span id="S5.T3.3.36.36.1.1" class="ltx_text" style="font-size:80%;">PMC-LLaMA <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a href="#bib.bib37" title="" class="ltx_ref">2023</a>)</cite></span></th>
<th id="S5.T3.3.36.36.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.36.36.2.1" class="ltx_text" style="font-size:80%;">Scratch</span></th>
<td id="S5.T3.3.36.36.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.36.36.3.1" class="ltx_text" style="font-size:80%;">28.6 / 29.8</span></td>
<td id="S5.T3.3.36.36.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.36.36.4.1" class="ltx_text" style="font-size:80%;">16.8 / 17.4</span></td>
<td id="S5.T3.3.36.36.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.36.36.5.1" class="ltx_text" style="font-size:80%;">36.8 / 36.9</span></td>
</tr>
<tr id="S5.T3.3.37.37" class="ltx_tr">
<th id="S5.T3.3.37.37.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.37.37.1.1" class="ltx_text" style="font-size:80%;">CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Radford et al. <span id="S5.T3.3.37.37.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib30" title="" class="ltx_ref">2021</a><span id="S5.T3.3.37.37.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.37.37.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.37.37.2.1" class="ltx_text" style="font-size:80%;">31.4 / 32.6</span></td>
<td id="S5.T3.3.37.37.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.37.37.3.1" class="ltx_text" style="font-size:80%;">19.5 / 20.4</span></td>
<td id="S5.T3.3.37.37.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.37.37.4.1" class="ltx_text" style="font-size:80%;">36.8 / 36.9</span></td>
</tr>
<tr id="S5.T3.3.38.38" class="ltx_tr">
<th id="S5.T3.3.38.38.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">
<span id="S5.T3.3.38.38.1.1" class="ltx_text" style="font-size:80%;">PMC-CLIP </span><cite class="ltx_cite ltx_citemacro_cite">Lin et al. <span id="S5.T3.3.38.38.1.2.1.1.1" class="ltx_text" style="font-size:80%;">(</span><a href="#bib.bib20" title="" class="ltx_ref">2023</a><span id="S5.T3.3.38.38.1.3.2.2.1" class="ltx_text" style="font-size:80%;">)</span></cite>
</th>
<td id="S5.T3.3.38.38.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.38.38.2.1" class="ltx_text" style="font-size:80%;">32.7 / 33.6</span></td>
<td id="S5.T3.3.38.38.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.38.38.3.1" class="ltx_text" style="font-size:80%;">20.3 / 21.5</span></td>
<td id="S5.T3.3.38.38.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S5.T3.3.38.38.4.1" class="ltx_text" style="font-size:80%;">39.4 / 40.3</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">In this section, we introduce our new MedVQA benchmark on PMC-VQA.
We evaluate different methods for both open-ended and multiple-choice tasks.
The results are summarized in Tab. <a href="#S5.T3" title="Table 3 ‣ 5.2 Benchmark on PMC-VQA ‣ 5 Results ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> (See supplementary for more qualitative comparisons.).We can draw the following observations:</p>
</div>
<div id="S5.SS2.p2" class="ltx_para ltx_noindent">
<p id="S5.SS2.p2.1" class="ltx_p"><span id="S5.SS2.p2.1.1" class="ltx_text ltx_font_bold">Multimodal Understanding is Essential.</span>
As shown in Tab. <a href="#S5.T3" title="Table 3 ‣ 5.2 Benchmark on PMC-VQA ‣ 5 Results ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, when using only language, the model struggles to provide accurate answers and produces nearly random outcomes, with accuracies of only 26.1% in Blanking and 30.6% in Choice.
It is worth noting that around 30% of the questions have “B” answers,
making the 30.6% score nearly equivalent to the highest possible score attainable through guessing. These observations highlight the crucial role of multimodal understanding in our dataset and emphasize the strong relationship between the images and the questions posed.</p>
</div>
<div id="S5.SS2.p3" class="ltx_para ltx_noindent">
<p id="S5.SS2.p3.1" class="ltx_p"><span id="S5.SS2.p3.1.1" class="ltx_text ltx_font_bold">General Visual-language Models Struggle on MedVQA.</span>
We evaluated the zero-shot performance of existing SOTA multimodal models, BLIP-2 and open-source version of Flamingo <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Awadalla et al. (<a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>.
As shown, even the best-performing models in natural images struggle to answer our MedVQA questions, demonstrating the challenging nature of our dataset and its strong biomedical relevance.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para ltx_noindent">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text ltx_font_bold">PMC-VQA-test Presents a Significantly More Challenging Benchmark.</span>
Notably, the previous SOTA multimodal model for MedVQA,
PMC-CLIP <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>, struggles on our dataset.
Not only does it fail to solve the blanking task, but it also significantly underperforms on multi-choice questions, with accuracy close to random.
These findings underline the difficulty of our dataset and its potential to serve as a more robust benchmark for evaluating VQA models.</p>
</div>
<div id="S5.SS2.p5" class="ltx_para ltx_noindent">
<p id="S5.SS2.p5.1" class="ltx_p"><span id="S5.SS2.p5.1.1" class="ltx_text ltx_font_bold">Comparing Generative Model Backbones on PMC-VQA-test.</span>
To further assess the effectiveness of our proposed method, we compared it against various baselines that use different generative model backbones.
Our results show that replacing the general visual backbone with a specialized medical one leads to improved performance, highlighting the importance of visual understanding in MedVQA. Additionally, we observed that replacing the language backbone with a domain-specific model also leads to some improvements, although not as significant as those achieved in the visual domain.</p>
</div>
<div id="S5.SS2.p6" class="ltx_para ltx_noindent">
<p id="S5.SS2.p6.1" class="ltx_p"><span id="S5.SS2.p6.1.1" class="ltx_text ltx_font_bold">Different Projection Modules Demonstrate Comparable Performance.</span>
We provide the comparison of baseline models using different projection modules (MLP or Transformer) on both open-ended and multiple-choice tasks.
As shown, different projection modules demonstrate comparable performance across various evaluation tasks.
Both architectures can effectively reconcile the diversity in the embedding dimensions arising from different pre-trained visual models, making our architecture adaptable to various visual model designs, regardless of whether they are based on ViT or ResNet.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Related Works</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text ltx_font_bold">Instruction Tuning with Large-language Models.</span>
Large Language Models (LLMs) have recently achieved tremendous success <cite class="ltx_cite ltx_citemacro_cite">Ouyang et al. (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>); OpenAI (<a href="#bib.bib27" title="" class="ltx_ref">2023</a>); cha (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite> in generating high-quality text for various tasks such as language translation, summarization, and question answering.
Open-source models, <em id="S6.p1.1.2" class="ltx_emph ltx_font_italic">e.g.</em>, Alpaca <cite class="ltx_cite ltx_citemacro_cite">Taori et al. (<a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>, have proposed instruction tuning to train models using examples generated from ChatGPT<cite class="ltx_cite ltx_citemacro_cite">cha (<a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>, effectively improving the performance of language models.
In the visual-language domain, concurrent work to ours, Mini-GPT4 <cite class="ltx_cite ltx_citemacro_cite">Zhu et al. (<a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite> generates a high-quality image-text dataset by prompting ChatGPT with well-designed inputs. In this paper, we focus on visual instruction tuning for MedVQA, which poses unique challenges due to the complexity of medical texts and the variability of medical images.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p"><span id="S6.p2.1.1" class="ltx_text ltx_font_bold">Medical Visual Question Answering.</span>
The field of MedVQA has gained significant interest in recent years, with a growing number of studies <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a href="#bib.bib21" title="" class="ltx_ref">2022</a>)</cite>. Despite the increasing attention, building a robust and reliable MedVQA system remains challenging due to the complexity and variability of medical images, as well as the lack of large-scale and diverse MedVQA datasets. Existing publicly available MedVQA datasets have limitations on diversity,
or dataset scale, for example, RadVisDial <cite class="ltx_cite ltx_citemacro_cite">Kovaleva et al. (<a href="#bib.bib16" title="" class="ltx_ref">2020</a>)</cite> only contains samples on chest x-ray images, VQA-Med <cite class="ltx_cite ltx_citemacro_cite">Ben Abacha et al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>, VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">Lau et al. (<a href="#bib.bib18" title="" class="ltx_ref">2018</a>)</cite>, and SLAKE <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib23" title="" class="ltx_ref">2021b</a>)</cite> have less than 10K images.
To address these limitations, we propose the PMC-VQA dataset that includes 227k image-question pairs with various image modalities and question types.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In conclusion, this paper addresses the challenge of MedVQA, where even the strongest VQA models trained on natural images yield results that closely resemble random guesses.
To overcome this,
we propose MedVInT, a generative model tailored to
advance this crucial medical task.
MedVInT is trained by aligning visual data from a pre-trained vision encoder with language models.
Additionally, we present a scalable pipeline for constructing PMC-VQA, a comprehensive MedVQA dataset comprising 227k VQA pairs across 149k images, spanning diverse modalities and diseases.
Our proposed model delivers state-of-the-art performance on existing MedVQA datasets, providing a new and reliable benchmark for evaluating different methods in this field.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">cha [2023]</span>
<span class="ltx_bibblock">
Openai. introducing chatgpt.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt/</a>, 2023.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alayrac et al. [2022]</span>
<span class="ltx_bibblock">
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
et al.

</span>
<span class="ltx_bibblock">Flamingo: a visual language model for few-shot learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:23716–23736, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antonelli et al. [2022]</span>
<span class="ltx_bibblock">
Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette
Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf
Ronneberger, Ronald M Summers, et al.

</span>
<span class="ltx_bibblock">The medical segmentation decathlon.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Nature Communications</em>, 13(1):4128, 2022.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Awadalla et al. [2023]</span>
<span class="ltx_bibblock">
Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong
Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, et al.

</span>
<span class="ltx_bibblock">Openflamingo, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://github.com/mlfoundations/open_flamingo" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/mlfoundations/open_flamingo</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ben Abacha et al. [2021]</span>
<span class="ltx_bibblock">
Asma Ben Abacha, Mourad Sarrouti, Dina Demner-Fushman, Sadid A Hasan, and
Henning Müller.

</span>
<span class="ltx_bibblock">Overview of the vqa-med task at imageclef 2021: Visual question
answering and generation in the medical domain.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the CLEF 2021 Conference and Labs of the
Evaluation Forum-working notes</em>. 21-24 September 2021, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2022]</span>
<span class="ltx_bibblock">
Zhihong Chen, Yuhao Du, Jinpeng Hu, Yang Liu, Guanbin Li, Xiang Wan, and
Tsung-Hui Chang.

</span>
<span class="ltx_bibblock">Multi-modal masked autoencoders for medical vision-and-language
pre-training.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention</em>,
pages 679–689. Springer, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al. [2023]</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and
Eric P. Xing.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality, March 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://lmsys.org/blog/2023-03-30-vicuna/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://lmsys.org/blog/2023-03-30-vicuna/</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al. [2022]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2204.02311</em>, 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Huang [2021]</span>
<span class="ltx_bibblock">
Jianwei Feng and Dong Huang.

</span>
<span class="ltx_bibblock">Optimal gradient checkpoint search for arbitrary computation graphs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 11433–11442, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. [2020]</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser,
and Connor Leahy.

</span>
<span class="ltx_bibblock">The Pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00027</em>, 2020.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. [2021]</span>
<span class="ltx_bibblock">
Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu,
Tristan Naumann, Jianfeng Gao, and Hoifung Poon.

</span>
<span class="ltx_bibblock">Domain-specific language model pretraining for biomedical natural
language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">ACM Transactions on Computing for Healthcare (HEALTH)</em>,
3(1):1–23, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. [2020]</span>
<span class="ltx_bibblock">
Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie.

</span>
<span class="ltx_bibblock">Towards visual question answering on pathology images.

</span>
<span class="ltx_bibblock">pages 708–718, 2020.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2021]</span>
<span class="ltx_bibblock">
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter
Szolovits.

</span>
<span class="ltx_bibblock">What disease does this patient have? a large-scale open domain
question answering dataset from medical exams.

</span>
<span class="ltx_bibblock"><em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Applied Sciences</em>, 11(14):6421, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones et al. [2001]</span>
<span class="ltx_bibblock">
Kristopher N Jones, Dwain E Woode, Kristina Panizzi, and Peter G Anderson.

</span>
<span class="ltx_bibblock">Peir digital library: Online resources and authoring system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AMIA Symposium</em>, page 1075. American
Medical Informatics Association, 2001.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kavur et al. [2021]</span>
<span class="ltx_bibblock">
A Emre Kavur, N Sinem Gezer, Mustafa Barış, Sinem Aslan, Pierre-Henri
Conze, Vladimir Groza, Duc Duy Pham, Soumick Chatterjee, Philipp Ernst,
Savaş Özkan, et al.

</span>
<span class="ltx_bibblock">Chaos challenge-combined (ct-mr) healthy abdominal organ
segmentation.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Medical Image Analysis</em>, 69:101950, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kovaleva et al. [2020]</span>
<span class="ltx_bibblock">
Olga Kovaleva, Chaitanya Shivade, Satyananda Kashyap, Karina Kanjaria, Joy Wu,
Deddeh Ballah, Adam Coy, Alexandros Karargyris, Yufan Guo, David Beymer
Beymer, et al.

</span>
<span class="ltx_bibblock">Towards visual dialog for radiology.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th SIGBioMed Workshop on Biomedical
Language Processing</em>, pages 60–69, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kung et al. [2023]</span>
<span class="ltx_bibblock">
Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie
De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel
Diaz-Candido, James Maningo, et al.

</span>
<span class="ltx_bibblock">Performance of chatgpt on usmle: Potential for ai-assisted medical
education using large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">PLoS digital health</em>, 2(2):e0000198, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lau et al. [2018]</span>
<span class="ltx_bibblock">
Jason J Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman.

</span>
<span class="ltx_bibblock">A dataset of clinically generated visual questions and answers about
radiology images.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Scientific data</em>, 5(1):1–10, 2018.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12597</em>, 2023.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2023]</span>
<span class="ltx_bibblock">
Weixiong Lin, Ziheng Zhao, Xiaoman Zhang, Chaoyi Wu, Ya Zhang, Yanfeng Wang,
and Weidi Xie.

</span>
<span class="ltx_bibblock">Pmc-clip: Contrastive language-image pre-training using biomedical
documents.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.07240</em>, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. [2022]</span>
<span class="ltx_bibblock">
Zhihong Lin, Donghao Zhang, Qingyi Tac, Danli Shi, Gholamreza Haffari, Qi Wu,
Mingguang He, and Zongyuan Ge.

</span>
<span class="ltx_bibblock">Medical visual question answering: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2111.10056</em>, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021a]</span>
<span class="ltx_bibblock">
Bo Liu, Li-Ming Zhan, and Xiao-Ming Wu.

</span>
<span class="ltx_bibblock">Contrastive pre-training and representation distillation for medical
visual question answering based on radiology images.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention</em>,
pages 210–220. Springer, 2021a.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2021b]</span>
<span class="ltx_bibblock">
Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu.

</span>
<span class="ltx_bibblock">Slake: A semantically-labeled knowledge-enhanced dataset for medical
visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">2021 IEEE 18th International Symposium on Biomedical Imaging
(ISBI)</em>, pages 1650–1654. IEEE, 2021b.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter [2017]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.05101</em>, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2019]</span>
<span class="ltx_bibblock">
Binh D Nguyen, Thanh-Toan Do, Binh X Nguyen, Tuong Do, Erman Tjiputra, and
Quang D Tran.

</span>
<span class="ltx_bibblock">Overcoming data limitation in medical visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Medical Image Computing and Computer Assisted Intervention</em>,
pages 522–530. Springer, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nori et al. [2023]</span>
<span class="ltx_bibblock">
Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric
Horvitz.

</span>
<span class="ltx_bibblock">Capabilities of gpt-4 on medical challenge problems.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.13375</em>, 2023.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI [2023]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. [2022]</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
35:27730–27744, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pelka et al. [2018]</span>
<span class="ltx_bibblock">
Obioma Pelka, Sven Koitka, Johannes Rückert, Felix Nensa, and Christoph M
Friedrich.

</span>
<span class="ltx_bibblock">Radiology objects in context (roco): a multimodal image dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">MICCAI Workshop on Large-scale Annotation of Biomedical Data
and Expert Label Synthesis (LABELS) 2018</em>, pages 180–189. Springer, 2018.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. [2021]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roberts [2001]</span>
<span class="ltx_bibblock">
Richard J Roberts.

</span>
<span class="ltx_bibblock">Pubmed central: The genbank of the published literature.

</span>
<span class="ltx_bibblock">volume 98, pages 381–382. National Acad Sciences, 2001.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. [2022]</span>
<span class="ltx_bibblock">
Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won
Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.13138</em>, 2022.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subramanian et al. [2020]</span>
<span class="ltx_bibblock">
Sanjay Subramanian et al.

</span>
<span class="ltx_bibblock">Medicat: A dataset of medical images, captions, and textual
references.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Findings of EMNLP</em>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. [2023]</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori B. Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/tatsu-lab/stanford_alpaca" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. [2023]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2017]</span>
<span class="ltx_bibblock">
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and
Ronald M Summers.

</span>
<span class="ltx_bibblock">Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
weakly-supervised classification and localization of common thorax diseases.

</span>
<span class="ltx_bibblock">In <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 2097–2106, 2017.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. [2023]</span>
<span class="ltx_bibblock">
Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie.

</span>
<span class="ltx_bibblock">Pmc-llama: Further finetuning llama on medical papers.

</span>
<span class="ltx_bibblock"><em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.14454</em>, 2023.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. [2022]</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2205.01068</em>, 2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. [2023]</span>
<span class="ltx_bibblock">
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt-4: Enhancing vision-language understanding with advanced
large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2304.10592</em>, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>PMC-VQA Dataset</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Examples</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">In order to provide a more comprehensive understanding of the dataset, we offer additional examples illustrated in Fig. <a href="#A1.F5" title="Figure 5 ‣ A.1 Examples ‣ Appendix A PMC-VQA Dataset ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. This figure showcases random instances of the original image and corresponding captions, along with multiple-choice questions generated from them. Additionally, we present the predictions of MedVInT-TE and MedVInT-TD models, with PMC-CLIP and PMC-LLAMA as their vision and language backbones.</p>
</div>
<figure id="A1.F5" class="ltx_figure"><img src="/html/2305.10415/assets/x6.png" id="A1.F5.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="490" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Examples of image captions, images, the generated question-answer pairs, and model prediction. The wrong predictions are highlighted in red.</figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Data Analysis</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">Fig. <a href="#A1.F6" title="Figure 6 ‣ A.2 Data Analysis ‣ Appendix A PMC-VQA Dataset ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> presents the top 50 words that appeared in the answers. As shown, words in answers primarily encompass positional descriptions such as left and right, image modality such as CT/MRI, and specific anatomical regions.
Fig. <a href="#A1.F7" title="Figure 7 ‣ A.2 Data Analysis ‣ Appendix A PMC-VQA Dataset ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the percentage of questions and answers with different word lengths.
Most questions range from 5 to 15 words, and most answers are around 5 words.</p>
</div>
<figure id="A1.F6" class="ltx_figure"><img src="/html/2305.10415/assets/x7.png" id="A1.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="130" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Answer distribution of training set.</figcaption>
</figure>
<figure id="A1.F7" class="ltx_figure"><img src="/html/2305.10415/assets/x8.png" id="A1.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="415" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Percentage of questions and answers with different word lengths.</figcaption>
</figure>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation Details</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">Our models are trained using the AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter [<a href="#bib.bib24" title="" class="ltx_ref">2017</a>]</cite> with a learning rate 2e-5.
The max context length is set as 512, and the batch size is 128.
To improve the training speed of our models,
we adopt the Deepspeed acceleration strategy, together with Automatic Mixed Precision (AMP) and gradient checkpointing <cite class="ltx_cite ltx_citemacro_cite">Feng and Huang [<a href="#bib.bib9" title="" class="ltx_ref">2021</a>]</cite>.
All models are implemented in PyTorch and trained on NVIDIA A100 GPU with 80 GB memory</p>
</div>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Social Impact</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">In an era where the digitization of healthcare is rapidly advancing, and medical data is proliferating, multimodal tools such as Medical Visual Question Answering (MedVQA) present significant potential to revolutionize patient care, empower clinicians, and bolster research. Our contribution in this field is twofold:
First, we introduce a scalable pipeline for the creation of a MedVQA dataset. This scalability ensures a continuous evolution and expansion of the dataset, maintaining its relevance in the ever-changing landscape of healthcare.
Second, we present the PMC-VQA dataset, crafted to overcome the limitations inherent in existing datasets. By encompassing a larger, more diverse selection of medical images, complemented by sophisticated questions and answers, we aim to significantly enhance the reliability and precision of medical multimodal models. This innovation holds the promise of equipping these models with the necessary tools to effectively navigate real-world scenarios.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Limitation</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.1" class="ltx_p">The proposed PMC-VQA has several limitations:</p>
</div>
<div id="A4.p2" class="ltx_para ltx_noindent">
<p id="A4.p2.1" class="ltx_p"><span id="A4.p2.1.1" class="ltx_text ltx_font_bold">Inherent Biases:</span> Despite efforts to construct a comprehensive MedVQA dataset with PMC-VQA, it is important to acknowledge the potential presence of biases in the dataset. Biases might arise from the data collection process, annotation methodology, or underlying distribution of the medical images and questions. Understanding and addressing these biases is crucial for ensuring fair and unbiased performance evaluation.</p>
</div>
<div id="A4.p3" class="ltx_para ltx_noindent">
<p id="A4.p3.1" class="ltx_p"><span id="A4.p3.1.1" class="ltx_text ltx_font_bold">Potential Annotation Biases:</span> Despite efforts to ensure quality and accuracy during the annotation process of PMC-VQA-test-clean, the dataset may still be susceptible to annotation biases. The subjective nature of question-answer pairs and the involvement of human annotators introduces the possibility of inconsistencies or subjective interpretations, which could impact the dataset’s reliability.</p>
</div>
<div id="A4.p4" class="ltx_para ltx_noindent">
<p id="A4.p4.1" class="ltx_p"><span id="A4.p4.1.1" class="ltx_text ltx_font_bold">Lacking Comprehensive Evaluation Metrics:</span>
Although both the ACC score and Bleu score are utilized in our benchmark for assessing open-ended blanking results, these two metrics fail to capture the fluency of the generated sentence since they measure string similarity irrespective of word order. As exhibited in the third case of Fig.<a href="#A1.F5" title="Figure 5 ‣ A.1 Examples ‣ Appendix A PMC-VQA Dataset ‣ PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, the encoder-based model significantly underperforms compared to the decoder-based model in this regard, a fact not reflected in the quantitative results. Indeed, finding an objective way to evaluate generative results comprehensively poses a significant challenge in the entire generative model community <cite class="ltx_cite ltx_citemacro_cite">Chiang et al. [<a href="#bib.bib7" title="" class="ltx_ref">2023</a>]</cite>. To address this issue, we plan to explore more evaluation metrics in our benchmark in future work.</p>
</div>
<div id="A4.p5" class="ltx_para ltx_noindent">
<p id="A4.p5.1" class="ltx_p"><span id="A4.p5.1.1" class="ltx_text ltx_font_bold">Need for Continual Dataset Expansion and Updates:</span> The medical field is dynamic, with ongoing advancements and new findings. To ensure the dataset’s relevance and coverage of emerging medical knowledge, continual expansion and updates to the PMC-VQA dataset are necessary.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.10414" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.10415" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.10415">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.10415" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.10416" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 07:43:39 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
