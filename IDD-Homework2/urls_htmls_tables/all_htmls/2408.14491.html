<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2408.14491] Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review</title><meta property="og:description" content="Recent technological advancements have enhanced our ability to collect and analyze rich multimodal data (e.g., speech, video, and eye gaze) to better inform learning and training experiences. While previous reviews hav…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2408.14491">

<!--Generated on Thu Sep  5 16:11:14 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="multimodal data,  data analytics,  learning analytics,  multimodal learning analytics,  mmla,  learning environments,  training environments">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Clayton Cohn
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:clayton.a.cohn@vanderbilt.edu">clayton.a.cohn@vanderbilt.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0003-0856-9587" title="ORCID identifier" class="ltx_ref">0000-0003-0856-9587</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_affiliation_institution">Vanderbilt University</span><span id="id2.2.id2" class="ltx_text ltx_affiliation_city">Nashville</span><span id="id3.3.id3" class="ltx_text ltx_affiliation_state">TN</span><span id="id4.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Eduardo Davalos
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:eduardo.davalos.anaya@vanderbilt.edu">eduardo.davalos.anaya@vanderbilt.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-7190-7273" title="ORCID identifier" class="ltx_ref">0000-0001-7190-7273</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_affiliation_institution">Vanderbilt University</span><span id="id6.2.id2" class="ltx_text ltx_affiliation_city">Nashville</span><span id="id7.3.id3" class="ltx_text ltx_affiliation_state">TN</span><span id="id8.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Caleb Vatral
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:cvatral@tnstate.edu">cvatral@tnstate.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5710-2911" title="ORCID identifier" class="ltx_ref">0000-0001-5710-2911</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id9.1.id1" class="ltx_text ltx_affiliation_institution">Tennessee State University</span><span id="id10.2.id2" class="ltx_text ltx_affiliation_city">Nashville</span><span id="id11.3.id3" class="ltx_text ltx_affiliation_state">TN</span><span id="id12.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joyce Horn Fonteles
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:joyce.h.fonteles@vanderbilt.edu">joyce.h.fonteles@vanderbilt.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-9862-8960" title="ORCID identifier" class="ltx_ref">0000-0001-9862-8960</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id13.1.id1" class="ltx_text ltx_affiliation_institution">Vanderbilt University</span><span id="id14.2.id2" class="ltx_text ltx_affiliation_city">Nashville</span><span id="id15.3.id3" class="ltx_text ltx_affiliation_state">TN</span><span id="id16.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hanchen David Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:hanchen.wang.1@vanderbilt.edu">hanchen.wang.1@vanderbilt.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5990-5865" title="ORCID identifier" class="ltx_ref">0000-0001-5990-5865</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id17.1.id1" class="ltx_text ltx_affiliation_institution">Vanderbilt University</span><span id="id18.2.id2" class="ltx_text ltx_affiliation_city">Nashville</span><span id="id19.3.id3" class="ltx_text ltx_affiliation_state">TN</span><span id="id20.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Meiyi Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:meiyi.ma@vanderbilt.edu">meiyi.ma@vanderbilt.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-6916-8774" title="ORCID identifier" class="ltx_ref">0000-0001-6916-8774</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id21.1.id1" class="ltx_text ltx_affiliation_institution">Vanderbilt University</span><span id="id22.2.id2" class="ltx_text ltx_affiliation_city">Nashville</span><span id="id23.3.id3" class="ltx_text ltx_affiliation_state">TN</span><span id="id24.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gautam Biswas
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:gautam.biswas@vanderbilt.edu">gautam.biswas@vanderbilt.edu</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-2752-3878" title="ORCID identifier" class="ltx_ref">0000-0002-2752-3878</a></span>

<span class="ltx_contact ltx_role_affiliation"><span id="id25.1.id1" class="ltx_text ltx_affiliation_institution">Vanderbilt University</span><span id="id26.2.id2" class="ltx_text ltx_affiliation_city">Nashville</span><span id="id27.3.id3" class="ltx_text ltx_affiliation_state">TN</span><span id="id28.4.id4" class="ltx_text ltx_affiliation_country">USA</span>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p id="id29.id1" class="ltx_p">Recent technological advancements have enhanced our ability to collect and analyze rich multimodal data (e.g., speech, video, and eye gaze) to better inform learning and training experiences. While previous reviews have focused on parts of the multimodal pipeline (e.g., conceptual models and data fusion), a comprehensive literature review on the <span id="id29.id1.1" class="ltx_text ltx_font_italic">methods</span> informing multimodal learning and training environments has not been conducted. This literature review provides an in-depth analysis of research methods in these environments, proposing a taxonomy and framework that encapsulates recent methodological advances in this field and characterizes the multimodal domain in terms of five modality groups: Natural Language, Video, Sensors, Human-Centered, and Environment Logs. We introduce a novel data fusion category — <span id="id29.id1.2" class="ltx_text ltx_font_italic">mid fusion</span> — and a graph-based technique for refining literature reviews, termed <span id="id29.id1.3" class="ltx_text ltx_font_italic">citation graph pruning</span>. Our analysis reveals that leveraging multiple modalities offers a more holistic understanding of the behaviors and outcomes of learners and trainees. Even when multimodality does not enhance predictive accuracy, it often uncovers patterns that contextualize and elucidate unimodal data, revealing subtleties that a single modality may miss. However, there remains a need for further research to bridge the divide between multimodal learning and training studies and foundational AI research.</p>
</div>
<div class="ltx_keywords">multimodal data, data analytics, learning analytics, multimodal learning analytics, mmla, learning environments, training environments
</div>
<div class="ltx_acknowledgements">This work is supported under National Science Foundation grants IIS-2327708, DRL-2112635, and IIS-2017000; and US Army CCDC Soldier Center Award #W912CG2220001. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation or United States Government, and no official endorsement by either party should be inferred.
</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing</span></span></span><span id="id2" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing Education</span></span></span><span id="id3" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing Computer-assisted instruction</span></span></span><span id="id4" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing Interactive learning environments</span></span></span><span id="id5" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing Collaborative learning</span></span></span><span id="id6" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing E-learning</span></span></span><span id="id7" class="ltx_note ltx_note_frontmatter ltx_role_ccs"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing Computer-managed instruction</span></span></span>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction and Background</h2>

<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1. </span>A Brief History</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Recent advances in the learning sciences, bolstered by technological progress, are driving the personalization of educational and training curricula to meet the unique needs of learners and trainees. This shift is underpinned by data-driven approaches that are integrated into the field of <span id="S1.SS1.p1.1.1" class="ltx_text ltx_font_italic">learning analytics</span> <cite class="ltx_cite ltx_citemacro_citep">(for Learning Analytics Research, <a href="#bib.bib62" title="" class="ltx_ref">SOLAR</a>)</cite>. Learning analytics focuses on gathering and evaluating data on learners’ and trainees’ behaviors—specifically, their approaches to learning and training tasks <cite class="ltx_cite ltx_citemacro_citep">(Maseleno et al<span class="ltx_text">.</span>, <a href="#bib.bib95" title="" class="ltx_ref">2018</a>; Zilvinskis et al<span class="ltx_text">.</span>, <a href="#bib.bib167" title="" class="ltx_ref">2017</a>)</cite>. For example, intelligent tutoring systems like Practical Algebra Tutor <cite class="ltx_cite ltx_citemacro_citep">(Koedinger et al<span class="ltx_text">.</span>, <a href="#bib.bib79" title="" class="ltx_ref">1997</a>)</cite> focus on diagnosing student errors, open-ended environments like Betty’s Brain <cite class="ltx_cite ltx_citemacro_citep">(Leelawong and Biswas, <a href="#bib.bib85" title="" class="ltx_ref">2008</a>)</cite> adaptively scaffold learning, and teacher-feedback tools (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Rodríguez-Triana et al<span class="ltx_text">.</span>, <a href="#bib.bib125" title="" class="ltx_ref">2018</a>; Hutchins and Biswas, <a href="#bib.bib73" title="" class="ltx_ref">2023</a>)</cite>) assist educators in enhancing instruction through insights into student behaviors.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">A central research question in learning analytics is, <span id="S1.SS1.p2.1.1" class="ltx_text ltx_font_italic">What types of data are necessary to gain insights into learner behaviors and performance, and enable meaningful support that advances student learning and training in different scenarios?</span> <cite class="ltx_cite ltx_citemacro_citep">(Vatral et al<span class="ltx_text">.</span>, <a href="#bib.bib152" title="" class="ltx_ref">2022</a>; Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2017</a>)</cite>. Initially, the scope of data collection and analysis was constrained by available technology and computational methods in educational settings. Early learning analytics predominantly analyzed log data from computer-based environments, establishing correlations between students’ behaviors and their digital interactions, thus forming the foundation for many contemporary theories and methods in the field <cite class="ltx_cite ltx_citemacro_citep">(Hoppe, <a href="#bib.bib72" title="" class="ltx_ref">2017</a>; Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">Advances in sensor and data collection technologies are extending learning analytics beyond traditional log-based analyses <cite class="ltx_cite ltx_citemacro_citep">(Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2017</a>)</cite>. In physical learning spaces, log data is insufficient to capture all learner actions, affective states, and collaborative behaviors. Researchers now integrate additional data collection devices, such as video to capture physical interactions, microphones for conversations, biometric sensors for stress levels, and eye trackers for attention <cite class="ltx_cite ltx_citemacro_citep">(Vatral et al<span class="ltx_text">.</span>, <a href="#bib.bib152" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S1.SS1.p4" class="ltx_para">
<p id="S1.SS1.p4.1" class="ltx_p">This enriched data collection provides a more comprehensive understanding of students’ affective, cognitive, psychomotor, and metacognitive states, advancing multimodal learning analytics (MMLA) <cite class="ltx_cite ltx_citemacro_citep">(Blikstein, <a href="#bib.bib13" title="" class="ltx_ref">2013</a>; Blikstein and Worsley, <a href="#bib.bib14" title="" class="ltx_ref">2016</a>; Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>)</cite>. MMLA has matured over a decade of research, disseminated through journal special issues <cite class="ltx_cite ltx_citemacro_citep">(Drachsler and Schneider, <a href="#bib.bib53" title="" class="ltx_ref">2018</a>; MDPI, <a href="#bib.bib97" title="" class="ltx_ref">2021</a>; of Learning Analytics, <a href="#bib.bib110" title="" class="ltx_ref">2015</a>)</cite>, conferences <cite class="ltx_cite ltx_citemacro_citep">(for Learning Analytics Research, <a href="#bib.bib61" title="" class="ltx_ref">NA</a>)</cite>, an edited volume <cite class="ltx_cite ltx_citemacro_citep">(Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2022</a>)</cite>, and systematic reviews <cite class="ltx_cite ltx_citemacro_citep">(Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022</a>; Alwahaby et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>; Shankar et al<span class="ltx_text">.</span>, <a href="#bib.bib131" title="" class="ltx_ref">2018</a>; Crescenzi-Lanna, <a href="#bib.bib40" title="" class="ltx_ref">2020</a>; Mu et al<span class="ltx_text">.</span>, <a href="#bib.bib101" title="" class="ltx_ref">2020</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2018</a>; Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>)</cite>. This review focuses on <span id="S1.SS1.p4.1.1" class="ltx_text ltx_font_italic">applied research methods</span> in MMLA, building on this substantial foundation.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2. </span>Related Work</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Recent work in MMLA research, surveys, and reviews have explored the MMLA landscape through various lenses: multimodal data fusion <cite class="ltx_cite ltx_citemacro_citep">(Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>, conceptual models and taxonomy <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite>, statistical and qualitative assessments <cite class="ltx_cite ltx_citemacro_citep">(Sharma and Giannakos, <a href="#bib.bib132" title="" class="ltx_ref">2020</a>; Qushem, <a href="#bib.bib122" title="" class="ltx_ref">2020</a>)</cite>, virtual reality <cite class="ltx_cite ltx_citemacro_citep">(Philippe et al<span class="ltx_text">.</span>, <a href="#bib.bib119" title="" class="ltx_ref">2020</a>)</cite>, technology and data engineering <cite class="ltx_cite ltx_citemacro_citep">(Chua et al<span class="ltx_text">.</span>, <a href="#bib.bib27" title="" class="ltx_ref">2019</a>)</cite>, and ethical considerations <cite class="ltx_cite ltx_citemacro_citep">(Alwahaby et al<span class="ltx_text">.</span>, <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>. Our review focuses on applied methods supporting data collection and analysis in multimodal learning and training environments, explicitly centering on methodologies for collecting, fusing, analyzing, and interpreting multimodal data using learning theories. We extend and modify existing taxonomies to reflect recent advances in MMLA.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet">Di Mitri et al<span class="ltx_text">.</span> (<a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite> introduced the Multimodal Learning Analytics Model (MLeAM), a conceptual framework outlining the relationship between behavior, data, machine learning, and feedback in MMLA. This framework provided a taxonomy and introduced the concept of data observability, distinguishing between quantifiable input evidence and inferred annotations (e.g., emotions, cognition). The <span id="S1.SS2.p2.1.1" class="ltx_text ltx_font_italic">observability line</span> demarcates these domains, crucial for AI-mediated transformation from input to hypotheses in MMLA research. <cite class="ltx_cite ltx_citemacro_citet">Chango et al<span class="ltx_text">.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite> surveyed fusion methods in MMLA, categorizing studies by fusion type and application stage within the multimodal pipeline. They proposed three fusion types: <span id="S1.SS2.p2.1.2" class="ltx_text ltx_font_italic">early</span> (feature-level integration), <span id="S1.SS2.p2.1.3" class="ltx_text ltx_font_italic">late</span> (decision-level integration), and <span id="S1.SS2.p2.1.4" class="ltx_text ltx_font_italic">hybrid</span> (a combination of both). This classification clarifies fusion approaches and their relevance to educational data mining.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">Integrating insights from both surveys, we propose a classification focused on <span id="S1.SS2.p3.1.1" class="ltx_text ltx_font_italic">feature observability</span>, distinguishing between sensory data and human-inferred annotations. This adapted scheme refines our understanding of data fusion in MMLA and creates a refined taxonomy, which we present in Section <a href="#S2" title="2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3. </span>Scope of This Review</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">For this paper, we define a <span id="S1.SS3.p1.1.1" class="ltx_text ltx_font_italic">data collection medium</span> as a unique type of raw data stream (e.g., video, audio, photoplethysmography (PPG) sensor). A <span id="S1.SS3.p1.1.2" class="ltx_text ltx_font_italic">modality</span> is a unique attribute derived from data from one or more streams, each conveying different information, even from the same medium <cite class="ltx_cite ltx_citemacro_citep">(Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib109" title="" class="ltx_ref">2017</a>)</cite>. <span id="S1.SS3.p1.1.3" class="ltx_text ltx_font_italic">Modality groups</span> are distinct sets of modalities conveying similar information, derived via inductive coding (see Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1. Framework ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). <span id="S1.SS3.p1.1.4" class="ltx_text ltx_font_italic">Multimodal</span> is a combination of either multiple modalities <span id="S1.SS3.p1.1.5" class="ltx_text ltx_font_bold">or</span> multiple data streams. For example, the same video data stream can be used to derive the affect and pose modalities, and the affect modality can be derived from audio and video streams. Both examples are considered multimodal. We use ”papers” and ”works” interchangeably, including publications outside of conferences and journals (e.g., books and book chapters). Our definitions aim to characterize the scope of our review, not to establish a ”universal” definition of multimodality and multimodal analysis.</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<p id="S1.SS3.p2.1" class="ltx_p">Our review includes all papers from our literature search not excluded by our criteria (see Appendix <a href="#A2.SS2.SSS2" title="B.2.2. Quality Control (Qualitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2.2</span></a>). This includes multimodal learning and training analysis done ”in passing.” For example, a paper focused on multimodal composing environments that performs multimodal learning analysis as a byproduct is included. We are interested in the methods used for multimodal analysis, not just those where it is the primary focus. We examine studies that engage in data collection and analysis across various mediums and modalities, encompassing fully physical settings (e.g., physical therapy), mixed-reality contexts (e.g., manikin-based nursing simulations), and online educational platforms (e.g., computer-based physics instruction). Notably, our review excludes virtual reality environments due to their current scalability challenges in educational settings <cite class="ltx_cite ltx_citemacro_citep">(Cook et al<span class="ltx_text">.</span>, <a href="#bib.bib38" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4. </span>Contributions</h3>

<div id="S1.SS4.p1" class="ltx_para">
<p id="S1.SS4.p1.1" class="ltx_p">This paper presents a systematic literature review on methodologies for multimodal learning and training environments and makes several novel contributions:</p>
</div>
<div id="S1.SS4.p2" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A <span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">comprehensive review</span> of the research methods used in multimodal learning and training environments, the challenges encountered, and relevant results that have been reported in the literature. Simultaneously, we also identify the research gaps in the data collection and analysis methodologies;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">A <span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">congruent framework and taxonomy</span> that reflects the recent advances in multimodal learning and training methodologies;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">An <span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">additional data fusion classification</span> that we call <span id="S1.I1.i3.p1.1.2" class="ltx_text ltx_font_italic">mid fusion</span> (i.e., it is between <span id="S1.I1.i3.p1.1.3" class="ltx_text ltx_font_italic">early fusion</span> and <span id="S1.I1.i3.p1.1.4" class="ltx_text ltx_font_italic">late fusion</span>) that allows for differentiating processed features relative to the observability line.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">A graph-based <span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">corpus reduction procedure</span> using a citation graph, which we refer to as <span id="S1.I1.i4.p1.1.2" class="ltx_text ltx_font_italic">citation graph pruning</span>, that allows for programmatically pruning literature review corpora. This is described in detail in Section <a href="#S3.SS2.SSS1" title="3.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.5. </span>Structure of our Literature Review</h3>

<div id="S1.SS5.p1" class="ltx_para">
<p id="S1.SS5.p1.1" class="ltx_p">The remainder of this literature review is structured as follows. Section <a href="#S2" title="2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents our theoretical framing and taxonomy for multimodal methods in learning and training environments. Section <a href="#S3" title="3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> details the procedures for our literature search, study selection, feature extraction, and analysis. Section <a href="#S4" title="4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents our findings for each component of our framework (each subsection corresponds to a box in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1. Framework ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), including an analysis of each of the 5 modality groups (Section <a href="#S4.SS2" title="4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>). Section <a href="#S5" title="5. Archetypes ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents three research categories (”archetypes”) that best characterize the multimodal learning and training field. Section <a href="#S6" title="6. Discussion ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> highlights current trends, state-of-the-art, results, challenges, and research gaps, addressing limitations and future research directions. Section <a href="#S7" title="7. Conclusions ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> concludes with a recap of this work’s contributions.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Framework and Taxonomy</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we provide a detailed description of the multimodal learning and training analytics process, outlining both the overarching framework and the specific features that constitute our taxonomy.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Framework</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We constructed our theoretical framework by integrating established multimodal learning analytics frameworks and through inductive analysis of the papers in our review corpus. The framework decomposes the multimodal learning and training analytics process into four primary components depicted in Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1. Framework ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: (1) the learning or training environment, (2) multimodal data, (3) learning analytics methods, and (4) feedback.</p>
</div>
<figure id="S2.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><svg version="1.1" fill="none" height="276.7400027674" stroke="none" width="553.4800055348" class="ltx_centering ltx_figure_panel" overflow="visible"><g transform="translate(0,276.7400027674) scale(1,-1)"><g transform="translate(0,0)"><g transform="translate(0,267) scale(1, -1)"><foreignObject width="538" height="267" overflow="visible"><img src="/html/2408.14491/assets/img/20240502_architecture.png" id="S2.F1.pic1.1.g1" class="ltx_graphics ltx_img_landscape" width="538" height="267" alt="Refer to caption"></foreignObject></g></g><g transform="translate(103.78,249.07)"><g class="makebox" transform="translate(-9.8,0)"><text x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">Sec. <g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible"><a href="#S4.SS1" title="4.1. Environments ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a></foreignObject></g></text></g></g><g transform="translate(297.5,249.07)"><g class="makebox" transform="translate(-9.8,0)"><text x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">Sec. <g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible"><a href="#S4.SS2" title="4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a></foreignObject></g></text></g></g><g transform="translate(500.9,232.46)"><g class="makebox" transform="translate(-9.8,0)"><text x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">Sec. <g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible"><a href="#S4.SS3" title="4.3. Data Fusion ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a></foreignObject></g></text></g></g><g transform="translate(500.9,99.63)"><g class="makebox" transform="translate(-9.8,0)"><text x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">Sec. <g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible"><a href="#S4.SS4" title="4.4. Analysis ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.4</span></a></foreignObject></g></text></g></g><g transform="translate(235.23,172.96)"><g class="makebox" transform="translate(-9.8,0)"><text x="0" y="0" transform="scale(1, -1)" fill="black" font-size="50%">Sec. <g transform="translate(0,1.383700013837) scale(1, -1)"><foreignObject width="1.383700013837" height="1.383700013837" overflow="visible"><a href="#S4.SS5" title="4.5. Feedback ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a></foreignObject></g></text></g></g></g></svg></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>. </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Multimodal Learning and Training Environments Literature Review Framework</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S2.F1.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.F1.5" class="ltx_p ltx_figure_panel ltx_align_center">[Multimodal Learning and Training Environments Literature Review Framework]Multimodal Learning and Training Environments Literature Review Framework</p>
</div>
</div>
</figure>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">The <span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">environment</span>, as the context for learner activities, is categorized as either <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_bold">learning</span> or <span id="S2.SS1.p2.1.3" class="ltx_text ltx_font_bold">training</span>, with the former supporting knowledge acquisition and the latter focusing on skill proficiency (Section <a href="#S2.SS2.SSS1" title="2.2.1. Environment Type ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>). Learning environments range from physical classrooms and tutoring centers to online learning centers (e.g., Khan Academy) and individual or group-based computer learning environments. Skill-based training happens through practice and repetition and can include military training, nursing training, physical training, workplace training, etc. We further dissect the environment into sub-components: <span id="S2.SS1.p2.1.4" class="ltx_text ltx_font_italic">human participants</span> (Sections <a href="#S2.SS2.SSS7" title="2.2.7. Domain of Study ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.7</span></a> to <a href="#S2.SS2.SSS10" title="2.2.10. Level of Instruction or Training ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.10</span></a>), <span id="S2.SS1.p2.1.5" class="ltx_text ltx_font_italic">setting</span> (which includes physical, virtual, or blended spaces; Section <a href="#S2.SS2.SSS6" title="2.2.6. Environment Setting ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.6</span></a>), and data collection <span id="S2.SS1.p2.1.6" class="ltx_text ltx_font_italic">sensors</span> (Section <a href="#S2.SS2.SSS2" title="2.2.2. Data Collection Mediums ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.2</span></a>). The framework’s second component is <span id="S2.SS1.p2.1.7" class="ltx_text ltx_font_italic">multimodal data</span> (Section <a href="#S2.SS2.SSS3" title="2.2.3. Modalities ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.3</span></a>), comprising the environmental sensor data streams and the modalities derived from them, which we classify into five modality groups: (1) <span id="S2.SS1.p2.1.8" class="ltx_text ltx_font_italic">natural language</span>, (2) <span id="S2.SS1.p2.1.9" class="ltx_text ltx_font_italic">vision</span>, (3) <span id="S2.SS1.p2.1.10" class="ltx_text ltx_font_italic">sensors</span>, (4) <span id="S2.SS1.p2.1.11" class="ltx_text ltx_font_italic">human-centered</span>, and (5) environment <span id="S2.SS1.p2.1.12" class="ltx_text ltx_font_italic">logs</span> (detailed in Sections <a href="#S4.SS2.SSS1" title="4.2.1. Natural Language ‣ 4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>, <a href="#S4.SS2.SSS2" title="4.2.2. Vision ‣ 4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>, <a href="#S4.SS2.SSS3" title="4.2.3. Sensors ‣ 4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.3</span></a>, <a href="#S4.SS2.SSS4" title="4.2.4. Human-Centered ‣ 4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.4</span></a>, and <a href="#S4.SS2.SSS5" title="4.2.5. Logs ‣ 4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2.5</span></a>, respectively). The next block in our Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1. Framework ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> framework is <span id="S2.SS1.p2.1.13" class="ltx_text ltx_font_italic">learning analytics</span>, which involves the methods for analyzing multimodal data (Section <a href="#S2.SS2.SSS4" title="2.2.4. Analysis Methods ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.4</span></a>), and is divided into <span id="S2.SS1.p2.1.14" class="ltx_text ltx_font_italic">data fusion</span> (<span id="S2.SS1.p2.1.15" class="ltx_text ltx_font_italic">early</span>, <span id="S2.SS1.p2.1.16" class="ltx_text ltx_font_italic">mid</span>, <span id="S2.SS1.p2.1.17" class="ltx_text ltx_font_italic">late</span>, and hybrid; Section <a href="#S2.SS2.SSS5" title="2.2.5. Data Fusion ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.5</span></a>) and <span id="S2.SS1.p2.1.18" class="ltx_text ltx_font_italic">analysis</span> approaches. Analysis approaches can be <span id="S2.SS1.p2.1.19" class="ltx_text ltx_font_italic">model-based</span> or<span id="S2.SS1.p2.1.20" class="ltx_text ltx_font_italic"> model-free</span>, further detailed in Section <a href="#S2.SS2.SSS11" title="2.2.11. Analysis Approach ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.11</span></a>. Finally, <span id="S2.SS1.p2.1.21" class="ltx_text ltx_font_italic">feedback</span> is the output of MMLA, differentiated into (1) <span id="S2.SS1.p2.1.22" class="ltx_text ltx_font_italic">direct</span> feedback for students and instructors, and (2) <span id="S2.SS1.p2.1.23" class="ltx_text ltx_font_italic">indirect</span> feedback for researchers and system designers (Section <a href="#S4.SS5" title="4.5. Feedback ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.5</span></a>).</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Taxonomy</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In this section, we delve deeper into each component of our framework, exploring features extracted from our corpus.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Environment Type</h4>

<figure id="S2.F2" class="ltx_figure"><img src="/html/2408.14491/assets/img/LearningTrainingContinuum.jpg" id="S2.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="240" height="202" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S2.F2.3.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>. </span><span id="S2.F2.4.2" class="ltx_text" style="font-size:90%;">Learning-Training Continuum</span></figcaption>
</figure>
<div id="S2.SS2.SSS1.p1" class="ltx_para">
<p id="S2.SS2.SSS1.p1.1" class="ltx_p">Our paper explores a spectrum of environments on a learning-training continuum (Figure <a href="#S2.F2" title="Figure 2 ‣ 2.2.1. Environment Type ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), from traditional classrooms to online courses, categorized along two dimensions: the learning-training axis <cite class="ltx_cite ltx_citemacro_citep">(Noel et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2018</a>; Vujovic et al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2020</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2017b</a>; Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite> and the physical-virtual space continuum <cite class="ltx_cite ltx_citemacro_citep">(Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Chan et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S2.SS2.SSS1.p2" class="ltx_para">
<p id="S2.SS2.SSS1.p2.1" class="ltx_p">Multimodal methods in learning environments aim to enhance educational outcomes by analyzing student engagement and learning patterns. In contrast, training environments focus on skill acquisition and task proficiency, serving individuals from personal development to professional enhancement in fields like healthcare <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>, athletics <cite class="ltx_cite ltx_citemacro_citep">(Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite>, and the military <cite class="ltx_cite ltx_citemacro_citep">(Henderson et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite>. These settings range from fully virtual simulations to physical training drills, with augmented and mixed realities bridging the gap. MMLA objectives differ between learning and training, necessitating context-specific strategies. While the distinction between learning and training can be ambiguous, as seen in game-based platforms <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2019</a>; Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite>, our review spans this spectrum. We employ a fuzzy qualitative categorization to place each study within this continuum, acknowledging the complexity yet utility of this approach for analyzing MMLA research sub-communities.</p>
</div>
</section>
<section id="S2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Data Collection Mediums</h4>

<div id="S2.SS2.SSS2.p1" class="ltx_para">
<p id="S2.SS2.SSS2.p1.1" class="ltx_p">Current learning and training environments use several computational measures of performance and behaviors such as evaluating learning gains, establishing and progressing toward desired objectives, and employing effective plans of action to achieve these objectives. Multimodal data can provide the basis for computing these measures, ranging from logs and surveys to analyses of student artifacts. A diverse array of <span id="S2.SS2.SSS2.p1.1.1" class="ltx_text ltx_font_italic">data collection mediums</span> plays a pivotal role in gaining a comprehensive understanding of learners’ progress, interactions, strategies, and struggles within these environments. The mediums listed in Table <a href="#S2.T1" title="Table 1 ‣ 2.2.2. Data Collection Mediums ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (and all definitions in Section <a href="#S2.SS2" title="2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>) were identified through our qualitative analysis of the corpus.</p>
</div>
<div id="S2.SS2.SSS2.p2" class="ltx_para">
<p id="S2.SS2.SSS2.p2.1" class="ltx_p">In the context of video data, we distinguish between depth cameras and traditional cameras. Though both fall under the video medium, depth cameras are typically employed with the motion modality to emphasize skeletal features. Furthermore, the scope of the motion medium extends beyond general video data, encompassing technologies such as real-time location systems (e.g., accelerometers, gyroscopes, or magnetometers). These technologies offer diverse approaches to capturing raw motion data, providing granularity in understanding participants’ physical movements.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.1.1.1.1.1" class="ltx_p" style="width:78.0pt;"><span id="S2.T1.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Medium</span></span>
</span>
</th>
<th id="S2.T1.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.1.1.2.1.1" class="ltx_p" style="width:329.5pt;"><span id="S2.T1.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Definition</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.2.1" class="ltx_tr">
<td id="S2.T1.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.2.1.1.1.1" class="ltx_p" style="width:78.0pt;">Video</span>
</span>
</td>
<td id="S2.T1.2.2.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.2.1.2.1.1" class="ltx_p" style="width:329.5pt;">Sequences of image frames captured from a camera source <cite class="ltx_cite ltx_citemacro_citep">(Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>; Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>; Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.3.2" class="ltx_tr">
<td id="S2.T1.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.3.2.1.1.1" class="ltx_p" style="width:78.0pt;">Audio</span>
</span>
</td>
<td id="S2.T1.2.3.2.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.3.2.2.1.1" class="ltx_p" style="width:329.5pt;">Audio signals captured by a microphone <cite class="ltx_cite ltx_citemacro_citep">(Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2017b</a>, <a href="#bib.bib115" title="" class="ltx_ref">a</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.4.3" class="ltx_tr">
<td id="S2.T1.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.4.3.1.1.1" class="ltx_p" style="width:78.0pt;">Screen Recording</span>
</span>
</td>
<td id="S2.T1.2.4.3.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.4.3.2.1.1" class="ltx_p" style="width:329.5pt;">Sequences of image frames displaying a device’s screen contents <cite class="ltx_cite ltx_citemacro_citep">(Alyuz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.5.4" class="ltx_tr">
<td id="S2.T1.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.5.4.1.1.1" class="ltx_p" style="width:78.0pt;">Eye</span>
</span>
</td>
<td id="S2.T1.2.5.4.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.5.4.2.1.1" class="ltx_p" style="width:329.5pt;">Eye movement data and gaze points captured by tracking devices <cite class="ltx_cite ltx_citemacro_citep">(Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>; Tancredi et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2022</a>; Papamitsiou et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2020</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.6.5" class="ltx_tr">
<td id="S2.T1.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.6.5.1.1.1" class="ltx_p" style="width:78.0pt;">Logs</span>
</span>
</td>
<td id="S2.T1.2.6.5.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.6.5.2.1.1" class="ltx_p" style="width:329.5pt;">Participant’s actions within the system and its state data <cite class="ltx_cite ltx_citemacro_citep">(Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>; Azcona et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.7.6" class="ltx_tr">
<td id="S2.T1.2.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.7.6.1.1.1" class="ltx_p" style="width:78.0pt;">Sensor</span>
</span>
</td>
<td id="S2.T1.2.7.6.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.7.6.2.1.1" class="ltx_p" style="width:329.5pt;">Specialized sensors used to gather participants’ physiological data <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2018a</a>; Henderson et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2019</a>; Järvelä et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.8.7" class="ltx_tr">
<td id="S2.T1.2.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.8.7.1.1.1" class="ltx_p" style="width:78.0pt;">Interview</span>
</span>
</td>
<td id="S2.T1.2.8.7.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.8.7.2.1.1" class="ltx_p" style="width:329.5pt;">Structured or unstructured conversations between researchers and participants <cite class="ltx_cite ltx_citemacro_citep">(Birt et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2018</a>; Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>; Noël et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.9.8" class="ltx_tr">
<td id="S2.T1.2.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.9.8.1.1.1" class="ltx_p" style="width:78.0pt;">Survey</span>
</span>
</td>
<td id="S2.T1.2.9.8.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.9.8.2.1.1" class="ltx_p" style="width:329.5pt;">Standardized sets of questions administered to participants <cite class="ltx_cite ltx_citemacro_citep">(Pham and Wang, <a href="#bib.bib117" title="" class="ltx_ref">2017</a>; Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.10.9" class="ltx_tr">
<td id="S2.T1.2.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.10.9.1.1.1" class="ltx_p" style="width:78.0pt;">Participant-Produced Artifacts</span>
</span>
</td>
<td id="S2.T1.2.10.9.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.10.9.2.1.1" class="ltx_p" style="width:329.5pt;">Materials produced by study participants using various mediums, including physical objects created for a task or written responses to formative assessment questions <cite class="ltx_cite ltx_citemacro_citep">(Ashwin and Guddeti, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2021a</a>; Ochoa and Dominguez, <a href="#bib.bib107" title="" class="ltx_ref">2020</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.11.10" class="ltx_tr">
<td id="S2.T1.2.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.11.10.1.1.1" class="ltx_p" style="width:78.0pt;">Researcher-Produced Artifacts</span>
</span>
</td>
<td id="S2.T1.2.11.10.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.11.10.2.1.1" class="ltx_p" style="width:329.5pt;">Materials produced by the researchers that contribute to analysis and findings, such as observational notes <cite class="ltx_cite ltx_citemacro_citep">(Henderson et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2019</a>; Standen et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>; Martinez-Maldonado et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2020</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.12.11" class="ltx_tr">
<td id="S2.T1.2.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.12.11.1.1.1" class="ltx_p" style="width:78.0pt;">Motion</span>
</span>
</td>
<td id="S2.T1.2.12.11.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.12.11.2.1.1" class="ltx_p" style="width:329.5pt;">Raw motion data collected via various different devices/technologies <cite class="ltx_cite ltx_citemacro_citep">(Vujovic et al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2020</a>; Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T1.2.13.12" class="ltx_tr">
<td id="S2.T1.2.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.13.12.1.1.1" class="ltx_p" style="width:78.0pt;">Text</span>
</span>
</td>
<td id="S2.T1.2.13.12.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T1.2.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T1.2.13.12.2.1.1" class="ltx_p" style="width:329.5pt;">Raw textual input <cite class="ltx_cite ltx_citemacro_citep">(Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>. </span><span id="S2.T1.4.2" class="ltx_text" style="font-size:90%;">Data collection mediums.</span></figcaption>
</figure>
<div id="S2.SS2.SSS2.p3" class="ltx_para">
<p id="S2.SS2.SSS2.p3.1" class="ltx_p">Researcher-produced artifacts can range from detailed field observation notes capturing contextual nuances to data labeling. This often requires manual coding that enhances data interpretability and contributes to more nuanced analyses and findings. Similarly, participant-produced artifacts constitute a valuable dimension in capturing participants’ engagement and comprehension. These artifacts include materials such as physical objects crafted by participants or pre/post-test results. We constrain participant-produced artifacts to include artifacts collected during learning and training experiences, which excludes <span id="S2.SS2.SSS2.p3.1.1" class="ltx_text ltx_font_italic">post hoc</span> artifact collection.</p>
</div>
</section>
<section id="S2.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3. </span>Modalities</h4>

<figure id="S2.T2" class="ltx_table">
<table id="S2.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T2.2.1.1" class="ltx_tr">
<th id="S2.T2.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.1.1.1.1.1" class="ltx_p" style="width:65.0pt;"><span id="S2.T2.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Modality</span></span>
</span>
</th>
<th id="S2.T2.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.1.1.2.1.1" class="ltx_p"><span id="S2.T2.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Description</span></span>
</span>
</th>
<th id="S2.T2.2.1.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.1.1.3.1.1" class="ltx_p" style="width:70.2pt;"><span id="S2.T2.2.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Modality Group</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T2.2.2.1" class="ltx_tr">
<td id="S2.T2.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.2.1.1.1.1" class="ltx_p" style="width:65.0pt;">Affect</span>
</span>
</td>
<td id="S2.T2.2.2.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.2.1.2.1.1" class="ltx_p">Participant’s emotional or affective state <cite class="ltx_cite ltx_citemacro_citep">(Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>; Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.2.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.2.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.2.1.3.1.1" class="ltx_p" style="width:70.2pt;">NLP, Vision, Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.3.2" class="ltx_tr">
<td id="S2.T2.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.3.2.1.1.1" class="ltx_p" style="width:65.0pt;">Pose</span>
</span>
</td>
<td id="S2.T2.2.3.2.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.3.2.2.1.1" class="ltx_p">Participant’s physical position, location, or body posture <cite class="ltx_cite ltx_citemacro_citep">(Alyuz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>; Starr et al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2018</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.3.2.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.3.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.3.2.3.1.1" class="ltx_p" style="width:70.2pt;">Vision, Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.4.3" class="ltx_tr">
<td id="S2.T2.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.4.3.1.1.1" class="ltx_p" style="width:65.0pt;">Gesture</span>
</span>
</td>
<td id="S2.T2.2.4.3.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.4.3.2.1.1" class="ltx_p">Participant’s gestures and body language <cite class="ltx_cite ltx_citemacro_citep">(Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2017b</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.4.3.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.4.3.3.1.1" class="ltx_p" style="width:70.2pt;">Vision</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.5.4" class="ltx_tr">
<td id="S2.T2.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.5.4.1.1.1" class="ltx_p" style="width:65.0pt;">Activity</span>
</span>
</td>
<td id="S2.T2.2.5.4.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.5.4.2.1.1" class="ltx_p">Participant’s observable actions or activities <cite class="ltx_cite ltx_citemacro_citep">(Fwa, Hua Leong and Lindsay Marshall, <a href="#bib.bib63" title="" class="ltx_ref">2018</a>; Prieto et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2018</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.5.4.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.5.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.5.4.3.1.1" class="ltx_p" style="width:70.2pt;">Vision, Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.6.5" class="ltx_tr">
<td id="S2.T2.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.6.5.1.1.1" class="ltx_p" style="width:65.0pt;">Prosodic Speech</span>
</span>
</td>
<td id="S2.T2.2.6.5.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.6.5.2.1.1" class="ltx_p">Elements of speech beyond word meaning, e.g. volume, pauses, and intonation <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>, <a href="#bib.bib139" title="" class="ltx_ref">b</a>; Noel et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2018</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.6.5.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.6.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.6.5.3.1.1" class="ltx_p" style="width:70.2pt;">NLP</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.7.6" class="ltx_tr">
<td id="S2.T2.2.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.7.6.1.1.1" class="ltx_p" style="width:65.0pt;">Transcribed Speech</span>
</span>
</td>
<td id="S2.T2.2.7.6.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.7.6.2.1.1" class="ltx_p">Textual speech transcribed from audio <cite class="ltx_cite ltx_citemacro_citep">(Birt et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2018</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>; Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.7.6.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.7.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.7.6.3.1.1" class="ltx_p" style="width:70.2pt;">NLP</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.8.7" class="ltx_tr">
<td id="S2.T2.2.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.8.7.1.1.1" class="ltx_p" style="width:65.0pt;">Qualitative Observations</span>
</span>
</td>
<td id="S2.T2.2.8.7.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.8.7.2.1.1" class="ltx_p">Researcher observations about the participant and study task <cite class="ltx_cite ltx_citemacro_citep">(Worsley, <a href="#bib.bib158" title="" class="ltx_ref">2018</a>; Martin et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2019</a>; Järvelä et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.8.7.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.8.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.8.7.3.1.1" class="ltx_p" style="width:70.2pt;">Human-centered</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.9.8" class="ltx_tr">
<td id="S2.T2.2.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.9.8.1.1.1" class="ltx_p" style="width:65.0pt;">Logs</span>
</span>
</td>
<td id="S2.T2.2.9.8.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.9.8.2.1.1" class="ltx_p">Participant’s environment actions and system state data <cite class="ltx_cite ltx_citemacro_citep">(Azcona et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>; Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>; Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.9.8.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.9.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.9.8.3.1.1" class="ltx_p" style="width:70.2pt;">Logs</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.10.9" class="ltx_tr">
<td id="S2.T2.2.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.10.9.1.1.1" class="ltx_p" style="width:65.0pt;">Gaze</span>
</span>
</td>
<td id="S2.T2.2.10.9.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.10.9.2.1.1" class="ltx_p">Participant’s eye gaze, e.g., movement, direction and focus <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2020</a>; Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>, <a href="#bib.bib55" title="" class="ltx_ref">a</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.10.9.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.10.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.10.9.3.1.1" class="ltx_p" style="width:70.2pt;">Vision, Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.11.10" class="ltx_tr">
<td id="S2.T2.2.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.11.10.1.1.1" class="ltx_p" style="width:65.0pt;">Interview</span>
</span>
</td>
<td id="S2.T2.2.11.10.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.11.10.2.1.1" class="ltx_p">Notes from interviews between researchers and participants <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021</a>; Aslan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.11.10.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.11.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.11.10.3.1.1" class="ltx_p" style="width:70.2pt;">Human-centered</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.12.11" class="ltx_tr">
<td id="S2.T2.2.12.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.12.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.12.11.1.1.1" class="ltx_p" style="width:65.0pt;">Survey</span>
</span>
</td>
<td id="S2.T2.2.12.11.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.12.11.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.12.11.2.1.1" class="ltx_p">Participant’s responses to surveys/questionnaires <cite class="ltx_cite ltx_citemacro_citep">(Pham and Wang, <a href="#bib.bib117" title="" class="ltx_ref">2017</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2017a</a>; Papamitsiou et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2020</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.12.11.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.12.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.12.11.3.1.1" class="ltx_p" style="width:70.2pt;">Human-centered</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.13.12" class="ltx_tr">
<td id="S2.T2.2.13.12.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.13.12.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.13.12.1.1.1" class="ltx_p" style="width:65.0pt;">Pulse</span>
</span>
</td>
<td id="S2.T2.2.13.12.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.13.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.13.12.2.1.1" class="ltx_p">The participant’s pulse, indicating their heart rate <cite class="ltx_cite ltx_citemacro_citep">(Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2021</a>, <a href="#bib.bib83" title="" class="ltx_ref">2022</a>; Tisza et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.13.12.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.13.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.13.12.3.1.1" class="ltx_p" style="width:70.2pt;">Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.14.13" class="ltx_tr">
<td id="S2.T2.2.14.13.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.14.13.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.14.13.1.1.1" class="ltx_p" style="width:65.0pt;">EDA</span>
</span>
</td>
<td id="S2.T2.2.14.13.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.14.13.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.14.13.2.1.1" class="ltx_p">Participant’s electrodermal activity <cite class="ltx_cite ltx_citemacro_citep">(Larmuseau et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020</a>; Mangaroska et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2020</a>; Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.14.13.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.14.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.14.13.3.1.1" class="ltx_p" style="width:70.2pt;">Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.15.14" class="ltx_tr">
<td id="S2.T2.2.15.14.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.15.14.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.15.14.1.1.1" class="ltx_p" style="width:65.0pt;">Temperature</span>
</span>
</td>
<td id="S2.T2.2.15.14.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.15.14.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.15.14.2.1.1" class="ltx_p">Participant’s body temperature <cite class="ltx_cite ltx_citemacro_citep">(Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>; Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2020</a>; Papamitsiou et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2020</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.15.14.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.15.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.15.14.3.1.1" class="ltx_p" style="width:70.2pt;">Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.16.15" class="ltx_tr">
<td id="S2.T2.2.16.15.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.16.15.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.16.15.1.1.1" class="ltx_p" style="width:65.0pt;">Blood Pressure</span>
</span>
</td>
<td id="S2.T2.2.16.15.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.16.15.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.16.15.2.1.1" class="ltx_p">Participant’s blood pressure <cite class="ltx_cite ltx_citemacro_citep">(Papamitsiou et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2020</a>; Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2022</a>; Tisza et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.16.15.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.16.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.16.15.3.1.1" class="ltx_p" style="width:70.2pt;">Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.17.16" class="ltx_tr">
<td id="S2.T2.2.17.16.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.17.16.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.17.16.1.1.1" class="ltx_p" style="width:65.0pt;">EEG</span>
</span>
</td>
<td id="S2.T2.2.17.16.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.17.16.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.17.16.2.1.1" class="ltx_p">Participant’s electroencephalography activity <cite class="ltx_cite ltx_citemacro_citep">(Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>; Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>; Papamitsiou et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2020</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.17.16.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.17.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.17.16.3.1.1" class="ltx_p" style="width:70.2pt;">Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.18.17" class="ltx_tr">
<td id="S2.T2.2.18.17.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.18.17.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.18.17.1.1.1" class="ltx_p" style="width:65.0pt;">Fatigue</span>
</span>
</td>
<td id="S2.T2.2.18.17.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.18.17.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.18.17.2.1.1" class="ltx_p">The level of fatigue experienced during the activity <cite class="ltx_cite ltx_citemacro_citep">(Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2021</a>, <a href="#bib.bib83" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.18.17.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.18.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.18.17.3.1.1" class="ltx_p" style="width:70.2pt;">Vision, Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.19.18" class="ltx_tr">
<td id="S2.T2.2.19.18.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.19.18.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.19.18.1.1.1" class="ltx_p" style="width:65.0pt;">EMG</span>
</span>
</td>
<td id="S2.T2.2.19.18.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.19.18.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.19.18.2.1.1" class="ltx_p">Participant’s electromyography activity <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>, <a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.19.18.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.19.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.19.18.3.1.1" class="ltx_p" style="width:70.2pt;">Sensor</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.20.19" class="ltx_tr">
<td id="S2.T2.2.20.19.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.20.19.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.20.19.1.1.1" class="ltx_p" style="width:65.0pt;">Participant Produced Artifacts</span>
</span>
</td>
<td id="S2.T2.2.20.19.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.20.19.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.20.19.2.1.1" class="ltx_p">Artifacts produced by the participant during the study, e.g., pre/post-tests <cite class="ltx_cite ltx_citemacro_citep">(Ochoa and Dominguez, <a href="#bib.bib107" title="" class="ltx_ref">2020</a>; Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>; Morell et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.20.19.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.20.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.20.19.3.1.1" class="ltx_p" style="width:70.2pt;">Human-centered</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.21.20" class="ltx_tr">
<td id="S2.T2.2.21.20.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.21.20.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.21.20.1.1.1" class="ltx_p" style="width:65.0pt;">Researcher Produced Artifacts</span>
</span>
</td>
<td id="S2.T2.2.21.20.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.21.20.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.21.20.2.1.1" class="ltx_p">Artifacts produced by the researcher about the study and participants, e.g., field notes <cite class="ltx_cite ltx_citemacro_citep">(Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>; Fernandez-Nieto et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>; Noël et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.21.20.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.21.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.21.20.3.1.1" class="ltx_p" style="width:70.2pt;">Human-centered</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.22.21" class="ltx_tr">
<td id="S2.T2.2.22.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.22.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.22.21.1.1.1" class="ltx_p" style="width:65.0pt;">Spectrogram</span>
</span>
</td>
<td id="S2.T2.2.22.21.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.22.21.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.22.21.2.1.1" class="ltx_p">Representation of audio frequencies in the form of a spectrogram <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.22.21.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.22.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.22.21.3.1.1" class="ltx_p" style="width:70.2pt;">NLP</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.23.22" class="ltx_tr">
<td id="S2.T2.2.23.22.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.23.22.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.23.22.1.1.1" class="ltx_p" style="width:65.0pt;">Text</span>
</span>
</td>
<td id="S2.T2.2.23.22.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.23.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.23.22.2.1.1" class="ltx_p">Participant’s raw text data generated in the study environment <cite class="ltx_cite ltx_citemacro_citep">(Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.23.22.3" class="ltx_td ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.23.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.23.22.3.1.1" class="ltx_p" style="width:70.2pt;">NLP</span>
</span>
</td>
</tr>
<tr id="S2.T2.2.24.23" class="ltx_tr">
<td id="S2.T2.2.24.23.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.24.23.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.24.23.1.1.1" class="ltx_p" style="width:65.0pt;">Pixel</span>
</span>
</td>
<td id="S2.T2.2.24.23.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.24.23.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.24.23.2.1.1" class="ltx_p">RGB pixel values from cameras or sensors <cite class="ltx_cite ltx_citemacro_citep">(Prieto et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2018</a>)</cite>.</span>
</span>
</td>
<td id="S2.T2.2.24.23.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T2.2.24.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T2.2.24.23.3.1.1" class="ltx_p" style="width:70.2pt;">Vision</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.3.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>. </span><span id="S2.T2.4.2" class="ltx_text" style="font-size:90%;">Modalities, their definitions, and the modality groups they fall into (detailed in Section <a href="#S4.SS2" title="4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.2</span></a>).</span></figcaption>
</figure>
<div id="S2.SS2.SSS3.p1" class="ltx_para">
<p id="S2.SS2.SSS3.p1.1" class="ltx_p">We previously defined <span id="S2.SS2.SSS3.p1.1.1" class="ltx_text ltx_font_italic">modalities</span> as unique attributes characterized by one or more data streams, where each modality conveys different information. Table <a href="#S2.T2" title="Table 2 ‣ 2.2.3. Modalities ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows several modalities that are used for analyzing and understanding participants’ interactions with and within learning and training environments. In this context, it is important to note that multimodality can arise from a combination of multiple modalities and multiple data streams. For example, the same video data stream could be used to derive both the <span id="S2.SS2.SSS3.p1.1.2" class="ltx_text ltx_font_italic">affect</span> and <span id="S2.SS2.SSS3.p1.1.3" class="ltx_text ltx_font_italic">pose</span> modalities. Similarly, <span id="S2.SS2.SSS3.p1.1.4" class="ltx_text ltx_font_italic">affect</span> can be derived from separate audio and video data streams.</p>
</div>
</section>
<section id="S2.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.4. </span>Analysis Methods</h4>

<div id="S2.SS2.SSS4.p1" class="ltx_para">
<p id="S2.SS2.SSS4.p1.1" class="ltx_p">We use the term <span id="S2.SS2.SSS4.p1.1.1" class="ltx_text ltx_font_italic">analysis method</span> to refer to specific techniques for deriving insights from multimodal data in learning and training contexts, which vary depending on research goals and data characteristics, and are presented in Table <a href="#S2.T3" title="Table 3 ‣ 2.2.4. Analysis Methods ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The methods range from supervised and unsupervised techniques (like classification and clustering) to qualitative analyses. More recently, deep learning algorithms have been developed for analyzing multiple data streams <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a href="#bib.bib64" title="" class="ltx_ref">2020</a>; Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib65" title="" class="ltx_ref">2022</a>)</cite>, and reinforcement learning techniques are being developed for educational recommendations <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2018a</a>)</cite>. Evaluating these methods is essential for understanding current trends in data analysis and informing future research. This review concentrates on the examination and interpretation of the data through these methods and not on the analytical techniques themselves, unless such meta-analysis yields further valuable insights.</p>
</div>
<figure id="S2.T3" class="ltx_table">
<table id="S2.T3.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T3.2.1.1" class="ltx_tr">
<th id="S2.T3.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.1.1.1.1.1" class="ltx_p" style="width:67.2pt;"><span id="S2.T3.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Method</span></span>
</span>
</th>
<th id="S2.T3.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.1.1.2.1.1" class="ltx_p" style="width:338.2pt;"><span id="S2.T3.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Definition</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T3.2.2.1" class="ltx_tr">
<td id="S2.T3.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.2.1.1.1.1" class="ltx_p" style="width:67.2pt;">Classification</span>
</span>
</td>
<td id="S2.T3.2.2.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.2.1.2.1.1" class="ltx_p" style="width:338.2pt;">Assigning pre-defined labels to input data based on feature analysis through supervised learning (often via deep learning approaches) <cite class="ltx_cite ltx_citemacro_citep">(Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2017b</a>; Alyuz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T3.2.3.2" class="ltx_tr">
<td id="S2.T3.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.3.2.1.1.1" class="ltx_p" style="width:67.2pt;">Regression</span>
</span>
</td>
<td id="S2.T3.2.3.2.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.3.2.2.1.1" class="ltx_p" style="width:338.2pt;">Predicting continuous numerical values through supervised learning to understand input-output relationships <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>; Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T3.2.4.3" class="ltx_tr">
<td id="S2.T3.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.4.3.1.1.1" class="ltx_p" style="width:67.2pt;">Clustering</span>
</span>
</td>
<td id="S2.T3.2.4.3.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.4.3.2.1.1" class="ltx_p" style="width:338.2pt;">Grouping data based on patterns or similarities using unsupervised learning <cite class="ltx_cite ltx_citemacro_citep">(Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Chan et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T3.2.5.4" class="ltx_tr">
<td id="S2.T3.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.5.4.1.1.1" class="ltx_p" style="width:67.2pt;">Qualitative</span>
</span>
</td>
<td id="S2.T3.2.5.4.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.5.4.2.1.1" class="ltx_p" style="width:338.2pt;">Manually examining and interpreting data to uncover patterns or themes <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2019</a>; Järvelä et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T3.2.6.5" class="ltx_tr">
<td id="S2.T3.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.6.5.1.1.1" class="ltx_p" style="width:67.2pt;">Statistical</span>
</span>
</td>
<td id="S2.T3.2.6.5.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.6.5.2.1.1" class="ltx_p" style="width:338.2pt;">Using statistical methods (e.g., correlation) to analyze data and draw conclusions <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>; Ochoa and Dominguez, <a href="#bib.bib107" title="" class="ltx_ref">2020</a>; López et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T3.2.7.6" class="ltx_tr">
<td id="S2.T3.2.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.7.6.1.1.1" class="ltx_p" style="width:67.2pt;">Network analysis</span>
</span>
</td>
<td id="S2.T3.2.7.6.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.7.6.2.1.1" class="ltx_p" style="width:338.2pt;">Studying relationships and interactions using graph-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Noel et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2018</a>; Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Chen, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T3.2.8.7" class="ltx_tr">
<td id="S2.T3.2.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.8.7.1.1.1" class="ltx_p" style="width:67.2pt;">Pattern Extraction</span>
</span>
</td>
<td id="S2.T3.2.8.7.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T3.2.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T3.2.8.7.2.1.1" class="ltx_p" style="width:338.2pt;">Identifying meaningful patterns or structures within data, including techniques like Markov analysis and sequence mining <cite class="ltx_cite ltx_citemacro_citep">(Papamitsiou et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2020</a>; Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2023</a>; Tancredi et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2022</a>)</cite>.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T3.3.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>. </span><span id="S2.T3.4.2" class="ltx_text" style="font-size:90%;">Analysis methods.</span></figcaption>
</figure>
</section>
<section id="S2.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.5. </span>Data Fusion</h4>

<div id="S2.SS2.SSS5.p1" class="ltx_para">
<p id="S2.SS2.SSS5.p1.1" class="ltx_p">In multimodal learning and training, data fusion is essential for leveraging multiple data sources to enhance the understanding of learning processes. Data fusion integrates information from diverse sources, creating a unified representation that enables enhanced analysis and understanding relative to unimodal studies. Such integration facilitates deeper insights into learners’ cognitive states, emotions, and behaviors, informing personalized educational interventions and the use of adaptive pedagogical strategies.</p>
</div>
<div id="S2.SS2.SSS5.p2" class="ltx_para">
<p id="S2.SS2.SSS5.p2.1" class="ltx_p">The conventional classification of data fusion methods in MMLA, as reviewed by <cite class="ltx_cite ltx_citemacro_citet">Chango et al<span class="ltx_text">.</span> (<a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>, includes early, late, and hybrid fusion. <span id="S2.SS2.SSS5.p2.1.1" class="ltx_text ltx_font_italic">Early fusion</span> merges raw data from different sources at the initial processing stage and is useful for capturing inter-modal interactions but faces challenges with data heterogeneity and model complexity. <span id="S2.SS2.SSS5.p2.1.2" class="ltx_text ltx_font_italic">Late fusion</span> involves first analyzing each modality separately with outcomes integrated later, allowing for detailed, modality-specific insights but potentially missing inter-modal dynamics. <span id="S2.SS2.SSS5.p2.1.3" class="ltx_text ltx_font_italic">Hybrid fusion</span> combines these approaches, integrating data at various processing stages to harness both inter-modal relationships and in-depth, unimodal analysis, though it increases complexity and necessitates strategic feature selection.</p>
</div>
<div id="S2.SS2.SSS5.p3" class="ltx_para">
<p id="S2.SS2.SSS5.p3.1" class="ltx_p">We contend that the traditional three-state categorization inadequately captures the nuances of multimodal analysis. Our qualitative review reveals difficulties in classifying data fusion practices due to ambiguities in defining <span id="S2.SS2.SSS5.p3.1.1" class="ltx_text ltx_font_italic">raw</span> versus <span id="S2.SS2.SSS5.p3.1.2" class="ltx_text ltx_font_italic">processed</span> features. For example, some researchers might classify the joint position data measured by a Microsoft Kinect camera as a raw feature, and thus permissible in early fusion, since it is available from the camera without any additional processing. However, others might classify this as a processed feature, and thus part of hybrid or late fusion, since the Kinect camera is computing this data from the raw depth data, regardless of whether this computation is obfuscated to the end user. Thus we’ve introduced a new category, <span id="S2.SS2.SSS5.p3.1.3" class="ltx_text ltx_font_italic">mid fusion</span>, which involves moderately processed data integration, as conceptualized by Di Mitri et al. <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite> using the observability line. To elaborate, Di Mitri et al. state, ”<span id="S2.SS2.SSS5.p3.1.4" class="ltx_text ltx_font_italic">The distinction between observable/unobservable is conceptual and can vary in practice.</span>” <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib51" title="" class="ltx_ref">2018</a>)</cite>. Here, <span id="S2.SS2.SSS5.p3.1.5" class="ltx_text ltx_font_italic">early fusion</span> combines unprocessed, observable features; <span id="S2.SS2.SSS5.p3.1.6" class="ltx_text ltx_font_italic">mid fusion</span> combines observable features that have undergone some processing; and <span id="S2.SS2.SSS5.p3.1.7" class="ltx_text ltx_font_italic">late fusion</span> combines processed features that cross into the hypothesis space, becoming inferences rather than direct observations.</p>
</div>
<div id="S2.SS2.SSS5.p4" class="ltx_para">
<p id="S2.SS2.SSS5.p4.1" class="ltx_p">For example, a Kinect sensor’s raw pixel or depth data are suitable for early fusion, while joint position data, processed but observable, fit mid fusion. In contrast, inferred constructs like motivation, derived from joint data, align with late fusion. The <span id="S2.SS2.SSS5.p4.1.1" class="ltx_text ltx_font_italic">mid fusion</span> category, while interpretatively flexible, clarifies ambiguities and aids in identifying MMLA sub-communities by their fusion methods. For a detailed definition of observable modalities, see section <a href="#S2.SS2.SSS3" title="2.2.3. Modalities ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.3</span></a>. Following Chango et al.’s methodology <cite class="ltx_cite ltx_citemacro_citep">(Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib23" title="" class="ltx_ref">2022</a>)</cite>, we also introduce an <span id="S2.SS2.SSS5.p4.1.2" class="ltx_text ltx_font_italic">other</span> category for studies not conforming to the four primary groups or lacking specified fusion points. These categories are summarized in Table <a href="#S2.T4" title="Table 4 ‣ 2.2.5. Data Fusion ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and illustrated in Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2.5. Data Fusion ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure id="S2.T4" class="ltx_table">
<table id="S2.T4.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T4.2.1.1" class="ltx_tr">
<th id="S2.T4.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.1.1.1.1.1" class="ltx_p" style="width:52.0pt;"><span id="S2.T4.2.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></span>
</span>
</th>
<th id="S2.T4.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.1.1.2.1.1" class="ltx_p" style="width:338.2pt;"><span id="S2.T4.2.1.1.2.1.1.1" class="ltx_text ltx_font_bold">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T4.2.2.1" class="ltx_tr">
<td id="S2.T4.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.2.1.1.1.1" class="ltx_p" style="width:52.0pt;">Early Fusion</span>
</span>
</td>
<td id="S2.T4.2.2.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.2.1.2.1.1" class="ltx_p" style="width:338.2pt;">Draws inferences and computes analytics from multiple sources of raw data at the earliest stage of processing before any modality-specific analysis <cite class="ltx_cite ltx_citemacro_citep">(Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>; Larmuseau et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020</a>; Sümer et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2023</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T4.2.3.2" class="ltx_tr">
<td id="S2.T4.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.3.2.1.1.1" class="ltx_p" style="width:52.0pt;">Mid Fusion</span>
</span>
</td>
<td id="S2.T4.2.3.2.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.3.2.2.1.1" class="ltx_p" style="width:338.2pt;">Represents a compromise that mixes early and late fusion for analysis. Combines processed, observable features generated from individual sources with analysis using other sources of data within the input space <cite class="ltx_cite ltx_citemacro_citep">(Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>; Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>, <a href="#bib.bib55" title="" class="ltx_ref">a</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T4.2.4.3" class="ltx_tr">
<td id="S2.T4.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.4.3.1.1.1" class="ltx_p" style="width:52.0pt;">Late Fusion</span>
</span>
</td>
<td id="S2.T4.2.4.3.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.4.3.2.1.1" class="ltx_p" style="width:338.2pt;">Analysis is performed on individual modalities, and the inferences generated are combined to generate outcomes at a later stage, i.e., in the hypothesis space <cite class="ltx_cite ltx_citemacro_citep">(Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>; Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>; Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2018</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T4.2.5.4" class="ltx_tr">
<td id="S2.T4.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.5.4.1.1.1" class="ltx_p" style="width:52.0pt;">Hybrid Fusion</span>
</span>
</td>
<td id="S2.T4.2.5.4.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.5.4.2.1.1" class="ltx_p" style="width:338.2pt;">Combines the strengths of both early and late fusion methods. Data from various sources are combined at multiple stages of processing <cite class="ltx_cite ltx_citemacro_citep">(Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Alyuz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Prieto et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2018</a>)</cite>.</span>
</span>
</td>
</tr>
<tr id="S2.T4.2.6.5" class="ltx_tr">
<td id="S2.T4.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.6.5.1.1.1" class="ltx_p" style="width:52.0pt;">Other</span>
</span>
</td>
<td id="S2.T4.2.6.5.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="S2.T4.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="S2.T4.2.6.5.2.1.1" class="ltx_p" style="width:338.2pt;">Studies that do not fit into the early, mid, late, or hybrid categories, or where the fusion point was not specified or fusion was not performed <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2019</a>; Järvelä et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021</a>)</cite>.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T4.3.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>. </span><span id="S2.T4.4.2" class="ltx_text" style="font-size:90%;">Data fusion approaches.</span></figcaption>
</figure>
<figure id="S2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.14491/assets/img/Fusion_Diagram.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" width="568" height="569" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>. </span><span id="S2.F3.3.2" class="ltx_text" style="font-size:90%;">Multimodal data fusion scheme according to when fusion is performed relative to the observability line.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S2.F3.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S2.F3.5" class="ltx_p ltx_figure_panel ltx_align_center">[Multimodal data fusion scheme]Multimodal data fusion scheme depending on when fusion is performed.</p>
</div>
</div>
</figure>
</section>
<section id="S2.SS2.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.6. </span>Environment Setting</h4>

<div id="S2.SS2.SSS6.p1" class="ltx_para">
<p id="S2.SS2.SSS6.p1.1" class="ltx_p">Analyzing the contextual settings in which these studies occur, we categorize these environments based on the nature of the setting. In a <span id="S2.SS2.SSS6.p1.1.1" class="ltx_text ltx_font_bold">Virtual</span> setting, activities occur entirely within a virtual space <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2017b</a>; Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>; Alyuz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>. A <span id="S2.SS2.SSS6.p1.1.2" class="ltx_text ltx_font_bold">Physical</span> setting is where activities take place in a real-world environment <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>; Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2017b</a>)</cite>. <span id="S2.SS2.SSS6.p1.1.3" class="ltx_text ltx_font_bold">Blended</span> settings combine elements of both virtual and physical environments <cite class="ltx_cite ltx_citemacro_citep">(Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite>. <span id="S2.SS2.SSS6.p1.1.4" class="ltx_text ltx_font_bold">Unspecified</span> settings refer to environments that are not clearly described in the paper <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2018a</a>; Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>. We aim to unveil the contextual relevance of multimodal learning and training by discerning how these approaches manifest in computer-based spaces, traditional classrooms, and blended scenarios combining virtual and physical elements. Additionally, acknowledging instances where sufficient information is not provided directs our attention to research gaps and unexplored areas within the literature.</p>
</div>
</section>
<section id="S2.SS2.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.7. </span>Domain of Study</h4>

<div id="S2.SS2.SSS7.p1" class="ltx_para">
<p id="S2.SS2.SSS7.p1.1" class="ltx_p">We recognized the importance of identifying the subject matter domain that study participants engage in, thus defining five domain categories. <span id="S2.SS2.SSS7.p1.1.1" class="ltx_text ltx_font_bold">STEM+C</span> includes participants engaged in Science, Technology, Engineering, Mathematics, and Computing disciplines, encompassing healthcare and medicine <cite class="ltx_cite ltx_citemacro_citep">(Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>, <a href="#bib.bib139" title="" class="ltx_ref">b</a>)</cite>. <span id="S2.SS2.SSS7.p1.1.2" class="ltx_text ltx_font_bold">Humanities</span> focuses on activities related to literature, debate, and oral presentation <cite class="ltx_cite ltx_citemacro_citep">(Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>; Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2017b</a>)</cite>. <span id="S2.SS2.SSS7.p1.1.3" class="ltx_text ltx_font_bold">Psychomotor Skills</span> emphasizes activities that develop motor skills and coordination <cite class="ltx_cite ltx_citemacro_citep">(Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>; Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>. The <span id="S2.SS2.SSS7.p1.1.4" class="ltx_text ltx_font_bold">Other</span> category covers subjects outside the previously mentioned categories <cite class="ltx_cite ltx_citemacro_citep">(Standen et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>; Morell et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>. <span id="S2.SS2.SSS7.p1.1.5" class="ltx_text ltx_font_bold">Unspecified</span> papers include those that do not provide sufficient information about the subject matter <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2018a</a>; Ashwin and Guddeti, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Capital Normal University, Beijing, China et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>. This categorization helps us better contextualize the use of multimodal analytics, exploring how they apply across diverse domains. These categories are intentionally broad, as we discovered that additional granularity hindered our ability to analyze and interpret current trends in the multimodal design of subject-related environments. Importantly, papers reporting results from multiple studies have labels corresponding to the domain of each separate study <cite class="ltx_cite ltx_citemacro_citep">(Standen et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>; Vujovic et al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2.SSS8" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.8. </span>Participant Interaction Structure</h4>

<div id="S2.SS2.SSS8.p1" class="ltx_para">
<p id="S2.SS2.SSS8.p1.1" class="ltx_p">We categorized papers by how they enabled interactions with participants — i.e., <span id="S2.SS2.SSS8.p1.1.1" class="ltx_text ltx_font_bold">Individual</span> <cite class="ltx_cite ltx_citemacro_citep">(Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2022</a>; Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>; Capital Normal University, Beijing, China et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> or <span id="S2.SS2.SSS8.p1.1.2" class="ltx_text ltx_font_bold">Multi-Person</span>, which often emphasized collaborative or group dynamics <cite class="ltx_cite ltx_citemacro_citep">(Reilly et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2018</a>; Prieto et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2018</a>; Worsley, <a href="#bib.bib158" title="" class="ltx_ref">2018</a>)</cite>. It is noteworthy that some papers analyzed both individual and groups of learners, reflecting the diversity in studies even within individual publications <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Ashwin and Guddeti, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2.SSS9" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.9. </span>Didactic Nature</h4>

<div id="S2.SS2.SSS9.p1" class="ltx_para">
<p id="S2.SS2.SSS9.p1.1" class="ltx_p">This refers to the approach used for delivering the learning and training, resulting in yet another lens through which we can understand, analyze, and differentiate learning and training environments. We define four categories. <span id="S2.SS2.SSS9.p1.1.1" class="ltx_text ltx_font_bold">Formal</span> instruction occurs in traditional classrooms, online courses, or other structured environments with clear objectives <cite class="ltx_cite ltx_citemacro_citep">(Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Chan et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Järvelä et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>)</cite>. <span id="S2.SS2.SSS9.p1.1.2" class="ltx_text ltx_font_bold">Informal</span> learning takes place in unstructured environments without set goals, such as using Minecraft to support diverse learners <cite class="ltx_cite ltx_citemacro_citep">(Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2020a</a>; Papamitsiou et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2020</a>; Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>; Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite>. <span id="S2.SS2.SSS9.p1.1.3" class="ltx_text ltx_font_bold">Training</span> focuses on skill development, practical training, and professional development in specific fields <cite class="ltx_cite ltx_citemacro_citep">(Fernandez-Nieto et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>; Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>; Morell et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>. The <span id="S2.SS2.SSS9.p1.1.4" class="ltx_text ltx_font_bold">Unspecified</span> category includes papers that lack sufficient information about the didactic nature of their studies <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
<section id="S2.SS2.SSS10" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.10. </span>Level of Instruction or Training</h4>

<div id="S2.SS2.SSS10.p1" class="ltx_para">
<p id="S2.SS2.SSS10.p1.1" class="ltx_p">We sought to delineate the level of participants’ instruction or training, defining four categories to provide valuable insights into the educational contexts targeted by the analyses in our corpus. <span id="S2.SS2.SSS10.p1.1.1" class="ltx_text ltx_font_bold">K-12</span> participants are those in kindergarten through 12<sup id="S2.SS2.SSS10.p1.1.2" class="ltx_sup">th</sup> grade <cite class="ltx_cite ltx_citemacro_citep">(Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>; Olsen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2020</a>; Vujovic et al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2020</a>)</cite>. <span id="S2.SS2.SSS10.p1.1.3" class="ltx_text ltx_font_bold">University</span> participants include undergraduate and graduate students <cite class="ltx_cite ltx_citemacro_citep">(Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>; Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>)</cite>. <span id="S2.SS2.SSS10.p1.1.4" class="ltx_text ltx_font_bold">Professional Development</span> participants are involved in professional development training <cite class="ltx_cite ltx_citemacro_citep">(Prieto et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2018</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>, <a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite>. The <span id="S2.SS2.SSS10.p1.1.5" class="ltx_text ltx_font_bold">Unspecified</span> category refers to papers that lack information about the participants’ level of instruction or training <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2019</a>; Chan et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>. It is important to note that studies featuring multiple groups of participants, or those reporting results across various studies, may have been assigned multiple labels.</p>
</div>
</section>
<section id="S2.SS2.SSS11" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.11. </span>Analysis Approach</h4>

<div id="S2.SS2.SSS11.p1" class="ltx_para">
<p id="S2.SS2.SSS11.p1.1" class="ltx_p">Our systematic categorization of analysis methodologies identified two principal approaches: <span id="S2.SS2.SSS11.p1.1.1" class="ltx_text ltx_font_bold">Model-based</span> and <span id="S2.SS2.SSS11.p1.1.2" class="ltx_text ltx_font_bold">Model-free</span>. Model-based analysis employs a formal model to reveal the data’s intrinsic structure and the interrelationships between variables. This approach involves hypothesizing about data structure and variable connections, often using mathematical functions to delineate the relationships in machine learning, or computational models to simulate system dynamics in cyber-physical systems. Conversely, model-free analysis eschews these assumptions, relying instead on empirical statistics (like correlations) to discern patterns and relationships directly from the data. It is important to note that these categorizations are not exclusive; a study may be classified as both model-based and model-free if it incorporates both types of approaches.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methods</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">This section outlines the methodology we employed to compile our literature corpus and ensure comprehensive coverage of pertinent research. We utilized a combination of quantitative (graph-based) and qualitative (quality control) techniques to refine our corpus to a representative yet manageable size. We introduce a novel graph-based method for literature corpus reduction, termed <span id="S3.p1.1.1" class="ltx_text ltx_font_italic">citation graph pruning</span> (CGP) that is detailed in Section <a href="#S3.SS2.SSS1" title="3.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>. CGP employs a directed citation graph that considers each paper’s citation network to identify and exclude outlier papers with minimal connections to the corpus, thus deemed beyond the review’s scope. This graph-based pruning method is a unique contribution to literature review methodologies and has not been previously reported. Additionally, our quality control process, elaborated in Section <a href="#S3.SS2.SSS2" title="3.2.2. Quality Control (Qualitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>, is derived from Kitchenham’s systematic review procedures <cite class="ltx_cite ltx_citemacro_citep">(Kitchenham, <a href="#bib.bib78" title="" class="ltx_ref">2004</a>)</cite>. For an exhaustive description of our search strategy, corpus distillation, and feature extraction methods, refer to Appendix <a href="#A2" title="Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Literature Search</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our literature search employed 42 search strings, collaboratively developed by the authors to encapsulate the relevant work for this review. We generated 14 search phrases, each queried thrice with variations of <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_italic">multimodal</span> (multimodal, multi-modal, multi modal), detailed in Appendix <a href="#A2" title="Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B</span></a>. Searches were conducted programmatically using Google Scholar via SerpAPI <cite class="ltx_cite ltx_citemacro_citep">(SerpApi, <a href="#bib.bib130" title="" class="ltx_ref">NA</a>)</cite>, chosen for its accurate retrieval of organic search results. For each search string, we selected the top five pages (100 publications) as ranked by Google Scholar, resulting in 4,200 papers. After removing 2,079 duplicates through hashing, and excluding 1 non-English paper, we obtained 2,120 unique papers.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Study Selection</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">After the initial search, we distilled the corpus quantitatively via citation graph pruning, which we discuss in Section <a href="#S3.SS2.SSS1" title="3.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>. Subsequent distillation, performed qualitatively, is discussed in Section <a href="#S3.SS2.SSS2" title="3.2.2. Quality Control (Qualitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Citation Graph Pruning (Quantitative Corpus Reduction).</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.2" class="ltx_p">For visualization and analysis, we used <a target="_blank" href="https://networkx.org/" title="" class="ltx_ref ltx_href">NetworkX</a> to construct a <span id="S3.SS2.SSS1.p1.2.1" class="ltx_text ltx_font_italic">citation graph</span> from the initial 2,120 papers. This graph, a directed acyclic graph (DAG), features nodes representing papers identified by their Google Scholar UUID and directed edges denoting citations, i.e., paper A cites paper B. The degree of a node (paper) <math id="S3.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mi id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><ci id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">p</annotation></semantics></math> is defined as the sum of incoming and outgoing edges, representing papers citing and cited by <math id="S3.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mi id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><ci id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">p</annotation></semantics></math>, respectively. SerpAPI was utilized to retrieve the citation lists.</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">We first eliminated all 0-degree nodes, assuming their irrelevance to the field or lack of influence on subsequent research. Further analysis of the DAG’s structure revealed one major component with 1,531 papers and 44 smaller, disconnected components (sizes 2-5), detailed in Appendix <a href="#A2.SS2.SSS1" title="B.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2.1</span></a>. The disconnected components were then removed. Subsequent pruning involved iteratively removing 1-degree nodes until no new 1-degree nodes emerged, a process we term <span id="S3.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">citation graph pruning</span>, outlined in Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. This pruning reduced the corpus to 1,063 papers.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Citation Graph Pruning Algorithm</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span>Acyclic directed graph <math id="alg1.l1.m1.2" class="ltx_Math" alttext="G=(V,E)" display="inline"><semantics id="alg1.l1.m1.2a"><mrow id="alg1.l1.m1.2.3" xref="alg1.l1.m1.2.3.cmml"><mi id="alg1.l1.m1.2.3.2" xref="alg1.l1.m1.2.3.2.cmml">G</mi><mo id="alg1.l1.m1.2.3.1" xref="alg1.l1.m1.2.3.1.cmml">=</mo><mrow id="alg1.l1.m1.2.3.3.2" xref="alg1.l1.m1.2.3.3.1.cmml"><mo stretchy="false" id="alg1.l1.m1.2.3.3.2.1" xref="alg1.l1.m1.2.3.3.1.cmml">(</mo><mi id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml">V</mi><mo id="alg1.l1.m1.2.3.3.2.2" xref="alg1.l1.m1.2.3.3.1.cmml">,</mo><mi id="alg1.l1.m1.2.2" xref="alg1.l1.m1.2.2.cmml">E</mi><mo stretchy="false" id="alg1.l1.m1.2.3.3.2.3" xref="alg1.l1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.2b"><apply id="alg1.l1.m1.2.3.cmml" xref="alg1.l1.m1.2.3"><eq id="alg1.l1.m1.2.3.1.cmml" xref="alg1.l1.m1.2.3.1"></eq><ci id="alg1.l1.m1.2.3.2.cmml" xref="alg1.l1.m1.2.3.2">𝐺</ci><interval closure="open" id="alg1.l1.m1.2.3.3.1.cmml" xref="alg1.l1.m1.2.3.3.2"><ci id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1">𝑉</ci><ci id="alg1.l1.m1.2.2.cmml" xref="alg1.l1.m1.2.2">𝐸</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.2c">G=(V,E)</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><span id="alg1.l2.2" class="ltx_text ltx_font_bold">procedure</span> <span id="alg1.l2.3" class="ltx_text ltx_font_smallcaps">Degree Trimming</span>(<math id="alg1.l2.m1.2" class="ltx_Math" alttext="G,n" display="inline"><semantics id="alg1.l2.m1.2a"><mrow id="alg1.l2.m1.2.3.2" xref="alg1.l2.m1.2.3.1.cmml"><mi id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml">G</mi><mo id="alg1.l2.m1.2.3.2.1" xref="alg1.l2.m1.2.3.1.cmml">,</mo><mi id="alg1.l2.m1.2.2" xref="alg1.l2.m1.2.2.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.2b"><list id="alg1.l2.m1.2.3.1.cmml" xref="alg1.l2.m1.2.3.2"><ci id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1">𝐺</ci><ci id="alg1.l2.m1.2.2.cmml" xref="alg1.l2.m1.2.2">𝑛</ci></list></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.2c">G,n</annotation></semantics></math>)

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span>     <math id="alg1.l3.m1.4" class="ltx_Math" alttext="S,D\leftarrow\{\},\{\}" display="inline"><semantics id="alg1.l3.m1.4a"><mrow id="alg1.l3.m1.4.4.2" xref="alg1.l3.m1.4.4.3.cmml"><mrow id="alg1.l3.m1.3.3.1.1" xref="alg1.l3.m1.3.3.1.1.cmml"><mrow id="alg1.l3.m1.3.3.1.1.2.2" xref="alg1.l3.m1.3.3.1.1.2.1.cmml"><mi id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml">S</mi><mo id="alg1.l3.m1.3.3.1.1.2.2.1" xref="alg1.l3.m1.3.3.1.1.2.1.cmml">,</mo><mi id="alg1.l3.m1.2.2" xref="alg1.l3.m1.2.2.cmml">D</mi></mrow><mo stretchy="false" id="alg1.l3.m1.3.3.1.1.1" xref="alg1.l3.m1.3.3.1.1.1.cmml">←</mo><mrow id="alg1.l3.m1.3.3.1.1.3.2" xref="alg1.l3.m1.3.3.1.1.cmml"><mo stretchy="false" id="alg1.l3.m1.3.3.1.1.3.2.1" xref="alg1.l3.m1.3.3.1.1.3.1.cmml">{</mo><mo stretchy="false" id="alg1.l3.m1.3.3.1.1.3.2.2" xref="alg1.l3.m1.3.3.1.1.3.1.cmml">}</mo></mrow></mrow><mo id="alg1.l3.m1.4.4.2.3" xref="alg1.l3.m1.4.4.3a.cmml">,</mo><mrow id="alg1.l3.m1.4.4.2.2.2" xref="alg1.l3.m1.4.4.3.cmml"><mo stretchy="false" id="alg1.l3.m1.4.4.2.2.2.1" xref="alg1.l3.m1.4.4.2.2.1.cmml">{</mo><mo stretchy="false" id="alg1.l3.m1.4.4.2.2.2.2" xref="alg1.l3.m1.4.4.2.2.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.4b"><apply id="alg1.l3.m1.4.4.3.cmml" xref="alg1.l3.m1.4.4.2"><csymbol cd="ambiguous" id="alg1.l3.m1.4.4.3a.cmml" xref="alg1.l3.m1.4.4.2.3">formulae-sequence</csymbol><apply id="alg1.l3.m1.3.3.1.1.cmml" xref="alg1.l3.m1.3.3.1.1"><ci id="alg1.l3.m1.3.3.1.1.1.cmml" xref="alg1.l3.m1.3.3.1.1.1">←</ci><list id="alg1.l3.m1.3.3.1.1.2.1.cmml" xref="alg1.l3.m1.3.3.1.1.2.2"><ci id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1">𝑆</ci><ci id="alg1.l3.m1.2.2.cmml" xref="alg1.l3.m1.2.2">𝐷</ci></list><list id="alg1.l3.m1.3.3.1.1.3.1.cmml" xref="alg1.l3.m1.3.3.1.1.3.2.1"></list></apply><list id="alg1.l3.m1.4.4.2.2.1.cmml" xref="alg1.l3.m1.4.4.2.2.2.1"></list></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.4c">S,D\leftarrow\{\},\{\}</annotation></semantics></math>

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>     <span id="alg1.l4.2" class="ltx_text ltx_font_bold">for all</span> <math id="alg1.l4.m1.1" class="ltx_Math" alttext="v\in V" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">v</mi><mo id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">∈</mo><mi id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml">V</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><in id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1"></in><ci id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">𝑣</ci><ci id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">v\in V</annotation></semantics></math> <span id="alg1.l4.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>         <span id="alg1.l5.2" class="ltx_text ltx_font_bold">if</span> <math id="alg1.l5.m1.2" class="ltx_Math" alttext="\deg(v)&lt;=n" display="inline"><semantics id="alg1.l5.m1.2a"><mrow id="alg1.l5.m1.2.3" xref="alg1.l5.m1.2.3.cmml"><mrow id="alg1.l5.m1.2.3.2.2" xref="alg1.l5.m1.2.3.2.1.cmml"><mi id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml">deg</mi><mo id="alg1.l5.m1.2.3.2.2a" xref="alg1.l5.m1.2.3.2.1.cmml">⁡</mo><mrow id="alg1.l5.m1.2.3.2.2.1" xref="alg1.l5.m1.2.3.2.1.cmml"><mo stretchy="false" id="alg1.l5.m1.2.3.2.2.1.1" xref="alg1.l5.m1.2.3.2.1.cmml">(</mo><mi id="alg1.l5.m1.2.2" xref="alg1.l5.m1.2.2.cmml">v</mi><mo stretchy="false" id="alg1.l5.m1.2.3.2.2.1.2" xref="alg1.l5.m1.2.3.2.1.cmml">)</mo></mrow></mrow><mo id="alg1.l5.m1.2.3.1" xref="alg1.l5.m1.2.3.1.cmml">&lt;=</mo><mi id="alg1.l5.m1.2.3.3" xref="alg1.l5.m1.2.3.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.2b"><apply id="alg1.l5.m1.2.3.cmml" xref="alg1.l5.m1.2.3"><leq id="alg1.l5.m1.2.3.1.cmml" xref="alg1.l5.m1.2.3.1"></leq><apply id="alg1.l5.m1.2.3.2.1.cmml" xref="alg1.l5.m1.2.3.2.2"><csymbol cd="latexml" id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1">degree</csymbol><ci id="alg1.l5.m1.2.2.cmml" xref="alg1.l5.m1.2.2">𝑣</ci></apply><ci id="alg1.l5.m1.2.3.3.cmml" xref="alg1.l5.m1.2.3.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.2c">\deg(v)&lt;=n</annotation></semantics></math> <span id="alg1.l5.3" class="ltx_text ltx_font_bold">then</span>
<math id="alg1.l5.m2.1" class="ltx_Math" alttext="S=S\cup\{v\}" display="inline"><semantics id="alg1.l5.m2.1a"><mrow id="alg1.l5.m2.1.2" xref="alg1.l5.m2.1.2.cmml"><mi id="alg1.l5.m2.1.2.2" xref="alg1.l5.m2.1.2.2.cmml">S</mi><mo id="alg1.l5.m2.1.2.1" xref="alg1.l5.m2.1.2.1.cmml">=</mo><mrow id="alg1.l5.m2.1.2.3" xref="alg1.l5.m2.1.2.3.cmml"><mi id="alg1.l5.m2.1.2.3.2" xref="alg1.l5.m2.1.2.3.2.cmml">S</mi><mo id="alg1.l5.m2.1.2.3.1" xref="alg1.l5.m2.1.2.3.1.cmml">∪</mo><mrow id="alg1.l5.m2.1.2.3.3.2" xref="alg1.l5.m2.1.2.3.3.1.cmml"><mo stretchy="false" id="alg1.l5.m2.1.2.3.3.2.1" xref="alg1.l5.m2.1.2.3.3.1.cmml">{</mo><mi id="alg1.l5.m2.1.1" xref="alg1.l5.m2.1.1.cmml">v</mi><mo stretchy="false" id="alg1.l5.m2.1.2.3.3.2.2" xref="alg1.l5.m2.1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m2.1b"><apply id="alg1.l5.m2.1.2.cmml" xref="alg1.l5.m2.1.2"><eq id="alg1.l5.m2.1.2.1.cmml" xref="alg1.l5.m2.1.2.1"></eq><ci id="alg1.l5.m2.1.2.2.cmml" xref="alg1.l5.m2.1.2.2">𝑆</ci><apply id="alg1.l5.m2.1.2.3.cmml" xref="alg1.l5.m2.1.2.3"><union id="alg1.l5.m2.1.2.3.1.cmml" xref="alg1.l5.m2.1.2.3.1"></union><ci id="alg1.l5.m2.1.2.3.2.cmml" xref="alg1.l5.m2.1.2.3.2">𝑆</ci><set id="alg1.l5.m2.1.2.3.3.1.cmml" xref="alg1.l5.m2.1.2.3.3.2"><ci id="alg1.l5.m2.1.1.cmml" xref="alg1.l5.m2.1.1">𝑣</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m2.1c">S=S\cup\{v\}</annotation></semantics></math>
              
</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>     <span id="alg1.l6.2" class="ltx_text ltx_font_bold">for all</span> <math id="alg1.l6.m1.1" class="ltx_Math" alttext="v\in S" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">v</mi><mo id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">∈</mo><mi id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml">S</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><in id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></in><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑣</ci><ci id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3">𝑆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">v\in S</annotation></semantics></math> <span id="alg1.l6.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>         <span id="alg1.l7.2" class="ltx_text ltx_font_bold">for all</span> <math id="alg1.l7.m1.1" class="ltx_Math" alttext="e\in E" display="inline"><semantics id="alg1.l7.m1.1a"><mrow id="alg1.l7.m1.1.1" xref="alg1.l7.m1.1.1.cmml"><mi id="alg1.l7.m1.1.1.2" xref="alg1.l7.m1.1.1.2.cmml">e</mi><mo id="alg1.l7.m1.1.1.1" xref="alg1.l7.m1.1.1.1.cmml">∈</mo><mi id="alg1.l7.m1.1.1.3" xref="alg1.l7.m1.1.1.3.cmml">E</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l7.m1.1b"><apply id="alg1.l7.m1.1.1.cmml" xref="alg1.l7.m1.1.1"><in id="alg1.l7.m1.1.1.1.cmml" xref="alg1.l7.m1.1.1.1"></in><ci id="alg1.l7.m1.1.1.2.cmml" xref="alg1.l7.m1.1.1.2">𝑒</ci><ci id="alg1.l7.m1.1.1.3.cmml" xref="alg1.l7.m1.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l7.m1.1c">e\in E</annotation></semantics></math> <span id="alg1.l7.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span>              <span id="alg1.l8.2" class="ltx_text ltx_font_bold">if</span> <math id="alg1.l8.m1.1" class="ltx_Math" alttext="v\in e\land e\notin D" display="inline"><semantics id="alg1.l8.m1.1a"><mrow id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml"><mi id="alg1.l8.m1.1.1.2" xref="alg1.l8.m1.1.1.2.cmml">v</mi><mo id="alg1.l8.m1.1.1.3" xref="alg1.l8.m1.1.1.3.cmml">∈</mo><mrow id="alg1.l8.m1.1.1.4" xref="alg1.l8.m1.1.1.4.cmml"><mi id="alg1.l8.m1.1.1.4.2" xref="alg1.l8.m1.1.1.4.2.cmml">e</mi><mo id="alg1.l8.m1.1.1.4.1" xref="alg1.l8.m1.1.1.4.1.cmml">∧</mo><mi id="alg1.l8.m1.1.1.4.3" xref="alg1.l8.m1.1.1.4.3.cmml">e</mi></mrow><mo id="alg1.l8.m1.1.1.5" xref="alg1.l8.m1.1.1.5.cmml">∉</mo><mi id="alg1.l8.m1.1.1.6" xref="alg1.l8.m1.1.1.6.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><apply id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1"><and id="alg1.l8.m1.1.1a.cmml" xref="alg1.l8.m1.1.1"></and><apply id="alg1.l8.m1.1.1b.cmml" xref="alg1.l8.m1.1.1"><in id="alg1.l8.m1.1.1.3.cmml" xref="alg1.l8.m1.1.1.3"></in><ci id="alg1.l8.m1.1.1.2.cmml" xref="alg1.l8.m1.1.1.2">𝑣</ci><apply id="alg1.l8.m1.1.1.4.cmml" xref="alg1.l8.m1.1.1.4"><and id="alg1.l8.m1.1.1.4.1.cmml" xref="alg1.l8.m1.1.1.4.1"></and><ci id="alg1.l8.m1.1.1.4.2.cmml" xref="alg1.l8.m1.1.1.4.2">𝑒</ci><ci id="alg1.l8.m1.1.1.4.3.cmml" xref="alg1.l8.m1.1.1.4.3">𝑒</ci></apply></apply><apply id="alg1.l8.m1.1.1c.cmml" xref="alg1.l8.m1.1.1"><notin id="alg1.l8.m1.1.1.5.cmml" xref="alg1.l8.m1.1.1.5"></notin><share href="#alg1.l8.m1.1.1.4.cmml" id="alg1.l8.m1.1.1d.cmml" xref="alg1.l8.m1.1.1"></share><ci id="alg1.l8.m1.1.1.6.cmml" xref="alg1.l8.m1.1.1.6">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">v\in e\land e\notin D</annotation></semantics></math> <span id="alg1.l8.3" class="ltx_text ltx_font_bold">then</span>
<math id="alg1.l8.m2.1" class="ltx_Math" alttext="D=D\cup\{e\}" display="inline"><semantics id="alg1.l8.m2.1a"><mrow id="alg1.l8.m2.1.2" xref="alg1.l8.m2.1.2.cmml"><mi id="alg1.l8.m2.1.2.2" xref="alg1.l8.m2.1.2.2.cmml">D</mi><mo id="alg1.l8.m2.1.2.1" xref="alg1.l8.m2.1.2.1.cmml">=</mo><mrow id="alg1.l8.m2.1.2.3" xref="alg1.l8.m2.1.2.3.cmml"><mi id="alg1.l8.m2.1.2.3.2" xref="alg1.l8.m2.1.2.3.2.cmml">D</mi><mo id="alg1.l8.m2.1.2.3.1" xref="alg1.l8.m2.1.2.3.1.cmml">∪</mo><mrow id="alg1.l8.m2.1.2.3.3.2" xref="alg1.l8.m2.1.2.3.3.1.cmml"><mo stretchy="false" id="alg1.l8.m2.1.2.3.3.2.1" xref="alg1.l8.m2.1.2.3.3.1.cmml">{</mo><mi id="alg1.l8.m2.1.1" xref="alg1.l8.m2.1.1.cmml">e</mi><mo stretchy="false" id="alg1.l8.m2.1.2.3.3.2.2" xref="alg1.l8.m2.1.2.3.3.1.cmml">}</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l8.m2.1b"><apply id="alg1.l8.m2.1.2.cmml" xref="alg1.l8.m2.1.2"><eq id="alg1.l8.m2.1.2.1.cmml" xref="alg1.l8.m2.1.2.1"></eq><ci id="alg1.l8.m2.1.2.2.cmml" xref="alg1.l8.m2.1.2.2">𝐷</ci><apply id="alg1.l8.m2.1.2.3.cmml" xref="alg1.l8.m2.1.2.3"><union id="alg1.l8.m2.1.2.3.1.cmml" xref="alg1.l8.m2.1.2.3.1"></union><ci id="alg1.l8.m2.1.2.3.2.cmml" xref="alg1.l8.m2.1.2.3.2">𝐷</ci><set id="alg1.l8.m2.1.2.3.3.1.cmml" xref="alg1.l8.m2.1.2.3.3.2"><ci id="alg1.l8.m2.1.1.cmml" xref="alg1.l8.m2.1.1">𝑒</ci></set></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m2.1c">D=D\cup\{e\}</annotation></semantics></math>
                            
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span>     <span id="alg1.l9.2" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l9.m1.2" class="ltx_Math" alttext="\left(V\setminus S,E\setminus D\right)" display="inline"><semantics id="alg1.l9.m1.2a"><mrow id="alg1.l9.m1.2.2.2" xref="alg1.l9.m1.2.2.3.cmml"><mo id="alg1.l9.m1.2.2.2.3" xref="alg1.l9.m1.2.2.3.cmml">(</mo><mrow id="alg1.l9.m1.1.1.1.1" xref="alg1.l9.m1.1.1.1.1.cmml"><mi id="alg1.l9.m1.1.1.1.1.2" xref="alg1.l9.m1.1.1.1.1.2.cmml">V</mi><mo id="alg1.l9.m1.1.1.1.1.1" xref="alg1.l9.m1.1.1.1.1.1.cmml">∖</mo><mi id="alg1.l9.m1.1.1.1.1.3" xref="alg1.l9.m1.1.1.1.1.3.cmml">S</mi></mrow><mo id="alg1.l9.m1.2.2.2.4" xref="alg1.l9.m1.2.2.3.cmml">,</mo><mrow id="alg1.l9.m1.2.2.2.2" xref="alg1.l9.m1.2.2.2.2.cmml"><mi id="alg1.l9.m1.2.2.2.2.2" xref="alg1.l9.m1.2.2.2.2.2.cmml">E</mi><mo id="alg1.l9.m1.2.2.2.2.1" xref="alg1.l9.m1.2.2.2.2.1.cmml">∖</mo><mi id="alg1.l9.m1.2.2.2.2.3" xref="alg1.l9.m1.2.2.2.2.3.cmml">D</mi></mrow><mo id="alg1.l9.m1.2.2.2.5" xref="alg1.l9.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.2b"><interval closure="open" id="alg1.l9.m1.2.2.3.cmml" xref="alg1.l9.m1.2.2.2"><apply id="alg1.l9.m1.1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1.1"><setdiff id="alg1.l9.m1.1.1.1.1.1.cmml" xref="alg1.l9.m1.1.1.1.1.1"></setdiff><ci id="alg1.l9.m1.1.1.1.1.2.cmml" xref="alg1.l9.m1.1.1.1.1.2">𝑉</ci><ci id="alg1.l9.m1.1.1.1.1.3.cmml" xref="alg1.l9.m1.1.1.1.1.3">𝑆</ci></apply><apply id="alg1.l9.m1.2.2.2.2.cmml" xref="alg1.l9.m1.2.2.2.2"><setdiff id="alg1.l9.m1.2.2.2.2.1.cmml" xref="alg1.l9.m1.2.2.2.2.1"></setdiff><ci id="alg1.l9.m1.2.2.2.2.2.cmml" xref="alg1.l9.m1.2.2.2.2.2">𝐸</ci><ci id="alg1.l9.m1.2.2.2.2.3.cmml" xref="alg1.l9.m1.2.2.2.2.3">𝐷</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.2c">\left(V\setminus S,E\setminus D\right)</annotation></semantics></math>

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span><span id="alg1.l10.2" class="ltx_text ltx_font_bold">procedure</span> <span id="alg1.l10.3" class="ltx_text ltx_font_smallcaps">Subconnected Graph Trimming</span>(<math id="alg1.l10.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="alg1.l10.m1.1a"><mi id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.1b"><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.1c">G</annotation></semantics></math>)

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>     <math id="alg1.l11.m1.4" class="ltx_Math" alttext="\left[S_{1},S_{2},S_{3},...,S_{n}\right]=\text{ConnectedComponent}(G),\text{where each }S_{i}=(V_{i},E_{i})" display="inline"><semantics id="alg1.l11.m1.4a"><mrow id="alg1.l11.m1.4.4.2" xref="alg1.l11.m1.4.4.3.cmml"><mrow id="alg1.l11.m1.3.3.1.1" xref="alg1.l11.m1.3.3.1.1.cmml"><mrow id="alg1.l11.m1.3.3.1.1.4.4" xref="alg1.l11.m1.3.3.1.1.4.5.cmml"><mo id="alg1.l11.m1.3.3.1.1.4.4.5" xref="alg1.l11.m1.3.3.1.1.4.5.cmml">[</mo><msub id="alg1.l11.m1.3.3.1.1.1.1.1" xref="alg1.l11.m1.3.3.1.1.1.1.1.cmml"><mi id="alg1.l11.m1.3.3.1.1.1.1.1.2" xref="alg1.l11.m1.3.3.1.1.1.1.1.2.cmml">S</mi><mn id="alg1.l11.m1.3.3.1.1.1.1.1.3" xref="alg1.l11.m1.3.3.1.1.1.1.1.3.cmml">1</mn></msub><mo id="alg1.l11.m1.3.3.1.1.4.4.6" xref="alg1.l11.m1.3.3.1.1.4.5.cmml">,</mo><msub id="alg1.l11.m1.3.3.1.1.2.2.2" xref="alg1.l11.m1.3.3.1.1.2.2.2.cmml"><mi id="alg1.l11.m1.3.3.1.1.2.2.2.2" xref="alg1.l11.m1.3.3.1.1.2.2.2.2.cmml">S</mi><mn id="alg1.l11.m1.3.3.1.1.2.2.2.3" xref="alg1.l11.m1.3.3.1.1.2.2.2.3.cmml">2</mn></msub><mo id="alg1.l11.m1.3.3.1.1.4.4.7" xref="alg1.l11.m1.3.3.1.1.4.5.cmml">,</mo><msub id="alg1.l11.m1.3.3.1.1.3.3.3" xref="alg1.l11.m1.3.3.1.1.3.3.3.cmml"><mi id="alg1.l11.m1.3.3.1.1.3.3.3.2" xref="alg1.l11.m1.3.3.1.1.3.3.3.2.cmml">S</mi><mn id="alg1.l11.m1.3.3.1.1.3.3.3.3" xref="alg1.l11.m1.3.3.1.1.3.3.3.3.cmml">3</mn></msub><mo id="alg1.l11.m1.3.3.1.1.4.4.8" xref="alg1.l11.m1.3.3.1.1.4.5.cmml">,</mo><mi mathvariant="normal" id="alg1.l11.m1.1.1" xref="alg1.l11.m1.1.1.cmml">…</mi><mo id="alg1.l11.m1.3.3.1.1.4.4.9" xref="alg1.l11.m1.3.3.1.1.4.5.cmml">,</mo><msub id="alg1.l11.m1.3.3.1.1.4.4.4" xref="alg1.l11.m1.3.3.1.1.4.4.4.cmml"><mi id="alg1.l11.m1.3.3.1.1.4.4.4.2" xref="alg1.l11.m1.3.3.1.1.4.4.4.2.cmml">S</mi><mi id="alg1.l11.m1.3.3.1.1.4.4.4.3" xref="alg1.l11.m1.3.3.1.1.4.4.4.3.cmml">n</mi></msub><mo id="alg1.l11.m1.3.3.1.1.4.4.10" xref="alg1.l11.m1.3.3.1.1.4.5.cmml">]</mo></mrow><mo id="alg1.l11.m1.3.3.1.1.5" xref="alg1.l11.m1.3.3.1.1.5.cmml">=</mo><mrow id="alg1.l11.m1.3.3.1.1.6" xref="alg1.l11.m1.3.3.1.1.6.cmml"><mtext id="alg1.l11.m1.3.3.1.1.6.2" xref="alg1.l11.m1.3.3.1.1.6.2a.cmml">ConnectedComponent</mtext><mo lspace="0em" rspace="0em" id="alg1.l11.m1.3.3.1.1.6.1" xref="alg1.l11.m1.3.3.1.1.6.1.cmml">​</mo><mrow id="alg1.l11.m1.3.3.1.1.6.3.2" xref="alg1.l11.m1.3.3.1.1.6.cmml"><mo stretchy="false" id="alg1.l11.m1.3.3.1.1.6.3.2.1" xref="alg1.l11.m1.3.3.1.1.6.cmml">(</mo><mi id="alg1.l11.m1.2.2" xref="alg1.l11.m1.2.2.cmml">G</mi><mo stretchy="false" id="alg1.l11.m1.3.3.1.1.6.3.2.2" xref="alg1.l11.m1.3.3.1.1.6.cmml">)</mo></mrow></mrow></mrow><mo id="alg1.l11.m1.4.4.2.3" xref="alg1.l11.m1.4.4.3a.cmml">,</mo><mrow id="alg1.l11.m1.4.4.2.2" xref="alg1.l11.m1.4.4.2.2.cmml"><mrow id="alg1.l11.m1.4.4.2.2.4" xref="alg1.l11.m1.4.4.2.2.4.cmml"><mtext id="alg1.l11.m1.4.4.2.2.4.2" xref="alg1.l11.m1.4.4.2.2.4.2a.cmml">where each </mtext><mo lspace="0em" rspace="0em" id="alg1.l11.m1.4.4.2.2.4.1" xref="alg1.l11.m1.4.4.2.2.4.1.cmml">​</mo><msub id="alg1.l11.m1.4.4.2.2.4.3" xref="alg1.l11.m1.4.4.2.2.4.3.cmml"><mi id="alg1.l11.m1.4.4.2.2.4.3.2" xref="alg1.l11.m1.4.4.2.2.4.3.2.cmml">S</mi><mi id="alg1.l11.m1.4.4.2.2.4.3.3" xref="alg1.l11.m1.4.4.2.2.4.3.3.cmml">i</mi></msub></mrow><mo id="alg1.l11.m1.4.4.2.2.3" xref="alg1.l11.m1.4.4.2.2.3.cmml">=</mo><mrow id="alg1.l11.m1.4.4.2.2.2.2" xref="alg1.l11.m1.4.4.2.2.2.3.cmml"><mo stretchy="false" id="alg1.l11.m1.4.4.2.2.2.2.3" xref="alg1.l11.m1.4.4.2.2.2.3.cmml">(</mo><msub id="alg1.l11.m1.4.4.2.2.1.1.1" xref="alg1.l11.m1.4.4.2.2.1.1.1.cmml"><mi id="alg1.l11.m1.4.4.2.2.1.1.1.2" xref="alg1.l11.m1.4.4.2.2.1.1.1.2.cmml">V</mi><mi id="alg1.l11.m1.4.4.2.2.1.1.1.3" xref="alg1.l11.m1.4.4.2.2.1.1.1.3.cmml">i</mi></msub><mo id="alg1.l11.m1.4.4.2.2.2.2.4" xref="alg1.l11.m1.4.4.2.2.2.3.cmml">,</mo><msub id="alg1.l11.m1.4.4.2.2.2.2.2" xref="alg1.l11.m1.4.4.2.2.2.2.2.cmml"><mi id="alg1.l11.m1.4.4.2.2.2.2.2.2" xref="alg1.l11.m1.4.4.2.2.2.2.2.2.cmml">E</mi><mi id="alg1.l11.m1.4.4.2.2.2.2.2.3" xref="alg1.l11.m1.4.4.2.2.2.2.2.3.cmml">i</mi></msub><mo stretchy="false" id="alg1.l11.m1.4.4.2.2.2.2.5" xref="alg1.l11.m1.4.4.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l11.m1.4b"><apply id="alg1.l11.m1.4.4.3.cmml" xref="alg1.l11.m1.4.4.2"><csymbol cd="ambiguous" id="alg1.l11.m1.4.4.3a.cmml" xref="alg1.l11.m1.4.4.2.3">formulae-sequence</csymbol><apply id="alg1.l11.m1.3.3.1.1.cmml" xref="alg1.l11.m1.3.3.1.1"><eq id="alg1.l11.m1.3.3.1.1.5.cmml" xref="alg1.l11.m1.3.3.1.1.5"></eq><list id="alg1.l11.m1.3.3.1.1.4.5.cmml" xref="alg1.l11.m1.3.3.1.1.4.4"><apply id="alg1.l11.m1.3.3.1.1.1.1.1.cmml" xref="alg1.l11.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l11.m1.3.3.1.1.1.1.1.1.cmml" xref="alg1.l11.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="alg1.l11.m1.3.3.1.1.1.1.1.2.cmml" xref="alg1.l11.m1.3.3.1.1.1.1.1.2">𝑆</ci><cn type="integer" id="alg1.l11.m1.3.3.1.1.1.1.1.3.cmml" xref="alg1.l11.m1.3.3.1.1.1.1.1.3">1</cn></apply><apply id="alg1.l11.m1.3.3.1.1.2.2.2.cmml" xref="alg1.l11.m1.3.3.1.1.2.2.2"><csymbol cd="ambiguous" id="alg1.l11.m1.3.3.1.1.2.2.2.1.cmml" xref="alg1.l11.m1.3.3.1.1.2.2.2">subscript</csymbol><ci id="alg1.l11.m1.3.3.1.1.2.2.2.2.cmml" xref="alg1.l11.m1.3.3.1.1.2.2.2.2">𝑆</ci><cn type="integer" id="alg1.l11.m1.3.3.1.1.2.2.2.3.cmml" xref="alg1.l11.m1.3.3.1.1.2.2.2.3">2</cn></apply><apply id="alg1.l11.m1.3.3.1.1.3.3.3.cmml" xref="alg1.l11.m1.3.3.1.1.3.3.3"><csymbol cd="ambiguous" id="alg1.l11.m1.3.3.1.1.3.3.3.1.cmml" xref="alg1.l11.m1.3.3.1.1.3.3.3">subscript</csymbol><ci id="alg1.l11.m1.3.3.1.1.3.3.3.2.cmml" xref="alg1.l11.m1.3.3.1.1.3.3.3.2">𝑆</ci><cn type="integer" id="alg1.l11.m1.3.3.1.1.3.3.3.3.cmml" xref="alg1.l11.m1.3.3.1.1.3.3.3.3">3</cn></apply><ci id="alg1.l11.m1.1.1.cmml" xref="alg1.l11.m1.1.1">…</ci><apply id="alg1.l11.m1.3.3.1.1.4.4.4.cmml" xref="alg1.l11.m1.3.3.1.1.4.4.4"><csymbol cd="ambiguous" id="alg1.l11.m1.3.3.1.1.4.4.4.1.cmml" xref="alg1.l11.m1.3.3.1.1.4.4.4">subscript</csymbol><ci id="alg1.l11.m1.3.3.1.1.4.4.4.2.cmml" xref="alg1.l11.m1.3.3.1.1.4.4.4.2">𝑆</ci><ci id="alg1.l11.m1.3.3.1.1.4.4.4.3.cmml" xref="alg1.l11.m1.3.3.1.1.4.4.4.3">𝑛</ci></apply></list><apply id="alg1.l11.m1.3.3.1.1.6.cmml" xref="alg1.l11.m1.3.3.1.1.6"><times id="alg1.l11.m1.3.3.1.1.6.1.cmml" xref="alg1.l11.m1.3.3.1.1.6.1"></times><ci id="alg1.l11.m1.3.3.1.1.6.2a.cmml" xref="alg1.l11.m1.3.3.1.1.6.2"><mtext id="alg1.l11.m1.3.3.1.1.6.2.cmml" xref="alg1.l11.m1.3.3.1.1.6.2">ConnectedComponent</mtext></ci><ci id="alg1.l11.m1.2.2.cmml" xref="alg1.l11.m1.2.2">𝐺</ci></apply></apply><apply id="alg1.l11.m1.4.4.2.2.cmml" xref="alg1.l11.m1.4.4.2.2"><eq id="alg1.l11.m1.4.4.2.2.3.cmml" xref="alg1.l11.m1.4.4.2.2.3"></eq><apply id="alg1.l11.m1.4.4.2.2.4.cmml" xref="alg1.l11.m1.4.4.2.2.4"><times id="alg1.l11.m1.4.4.2.2.4.1.cmml" xref="alg1.l11.m1.4.4.2.2.4.1"></times><ci id="alg1.l11.m1.4.4.2.2.4.2a.cmml" xref="alg1.l11.m1.4.4.2.2.4.2"><mtext id="alg1.l11.m1.4.4.2.2.4.2.cmml" xref="alg1.l11.m1.4.4.2.2.4.2">where each </mtext></ci><apply id="alg1.l11.m1.4.4.2.2.4.3.cmml" xref="alg1.l11.m1.4.4.2.2.4.3"><csymbol cd="ambiguous" id="alg1.l11.m1.4.4.2.2.4.3.1.cmml" xref="alg1.l11.m1.4.4.2.2.4.3">subscript</csymbol><ci id="alg1.l11.m1.4.4.2.2.4.3.2.cmml" xref="alg1.l11.m1.4.4.2.2.4.3.2">𝑆</ci><ci id="alg1.l11.m1.4.4.2.2.4.3.3.cmml" xref="alg1.l11.m1.4.4.2.2.4.3.3">𝑖</ci></apply></apply><interval closure="open" id="alg1.l11.m1.4.4.2.2.2.3.cmml" xref="alg1.l11.m1.4.4.2.2.2.2"><apply id="alg1.l11.m1.4.4.2.2.1.1.1.cmml" xref="alg1.l11.m1.4.4.2.2.1.1.1"><csymbol cd="ambiguous" id="alg1.l11.m1.4.4.2.2.1.1.1.1.cmml" xref="alg1.l11.m1.4.4.2.2.1.1.1">subscript</csymbol><ci id="alg1.l11.m1.4.4.2.2.1.1.1.2.cmml" xref="alg1.l11.m1.4.4.2.2.1.1.1.2">𝑉</ci><ci id="alg1.l11.m1.4.4.2.2.1.1.1.3.cmml" xref="alg1.l11.m1.4.4.2.2.1.1.1.3">𝑖</ci></apply><apply id="alg1.l11.m1.4.4.2.2.2.2.2.cmml" xref="alg1.l11.m1.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l11.m1.4.4.2.2.2.2.2.1.cmml" xref="alg1.l11.m1.4.4.2.2.2.2.2">subscript</csymbol><ci id="alg1.l11.m1.4.4.2.2.2.2.2.2.cmml" xref="alg1.l11.m1.4.4.2.2.2.2.2.2">𝐸</ci><ci id="alg1.l11.m1.4.4.2.2.2.2.2.3.cmml" xref="alg1.l11.m1.4.4.2.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l11.m1.4c">\left[S_{1},S_{2},S_{3},...,S_{n}\right]=\text{ConnectedComponent}(G),\text{where each }S_{i}=(V_{i},E_{i})</annotation></semantics></math>

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span>     <math id="alg1.l12.m1.6" class="ltx_Math" alttext="j=\arg\max\{|V_{1}|,|V_{2}|,|V_{3}|,...,|V_{n}|\}" display="inline"><semantics id="alg1.l12.m1.6a"><mrow id="alg1.l12.m1.6.6" xref="alg1.l12.m1.6.6.cmml"><mi id="alg1.l12.m1.6.6.6" xref="alg1.l12.m1.6.6.6.cmml">j</mi><mo id="alg1.l12.m1.6.6.5" xref="alg1.l12.m1.6.6.5.cmml">=</mo><mrow id="alg1.l12.m1.6.6.4" xref="alg1.l12.m1.6.6.4.cmml"><mi id="alg1.l12.m1.6.6.4.5" xref="alg1.l12.m1.6.6.4.5.cmml">arg</mi><mo lspace="0.167em" id="alg1.l12.m1.6.6.4a" xref="alg1.l12.m1.6.6.4.cmml">⁡</mo><mrow id="alg1.l12.m1.6.6.4.4.4" xref="alg1.l12.m1.6.6.4.4.5.cmml"><mi id="alg1.l12.m1.1.1" xref="alg1.l12.m1.1.1.cmml">max</mi><mo id="alg1.l12.m1.6.6.4.4.4a" xref="alg1.l12.m1.6.6.4.4.5.cmml">⁡</mo><mrow id="alg1.l12.m1.6.6.4.4.4.4" xref="alg1.l12.m1.6.6.4.4.5.cmml"><mo stretchy="false" id="alg1.l12.m1.6.6.4.4.4.4.5" xref="alg1.l12.m1.6.6.4.4.5.cmml">{</mo><mrow id="alg1.l12.m1.3.3.1.1.1.1.1.1" xref="alg1.l12.m1.3.3.1.1.1.1.1.2.cmml"><mo stretchy="false" id="alg1.l12.m1.3.3.1.1.1.1.1.1.2" xref="alg1.l12.m1.3.3.1.1.1.1.1.2.1.cmml">|</mo><msub id="alg1.l12.m1.3.3.1.1.1.1.1.1.1" xref="alg1.l12.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="alg1.l12.m1.3.3.1.1.1.1.1.1.1.2" xref="alg1.l12.m1.3.3.1.1.1.1.1.1.1.2.cmml">V</mi><mn id="alg1.l12.m1.3.3.1.1.1.1.1.1.1.3" xref="alg1.l12.m1.3.3.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="alg1.l12.m1.3.3.1.1.1.1.1.1.3" xref="alg1.l12.m1.3.3.1.1.1.1.1.2.1.cmml">|</mo></mrow><mo id="alg1.l12.m1.6.6.4.4.4.4.6" xref="alg1.l12.m1.6.6.4.4.5.cmml">,</mo><mrow id="alg1.l12.m1.4.4.2.2.2.2.2.1" xref="alg1.l12.m1.4.4.2.2.2.2.2.2.cmml"><mo stretchy="false" id="alg1.l12.m1.4.4.2.2.2.2.2.1.2" xref="alg1.l12.m1.4.4.2.2.2.2.2.2.1.cmml">|</mo><msub id="alg1.l12.m1.4.4.2.2.2.2.2.1.1" xref="alg1.l12.m1.4.4.2.2.2.2.2.1.1.cmml"><mi id="alg1.l12.m1.4.4.2.2.2.2.2.1.1.2" xref="alg1.l12.m1.4.4.2.2.2.2.2.1.1.2.cmml">V</mi><mn id="alg1.l12.m1.4.4.2.2.2.2.2.1.1.3" xref="alg1.l12.m1.4.4.2.2.2.2.2.1.1.3.cmml">2</mn></msub><mo stretchy="false" id="alg1.l12.m1.4.4.2.2.2.2.2.1.3" xref="alg1.l12.m1.4.4.2.2.2.2.2.2.1.cmml">|</mo></mrow><mo id="alg1.l12.m1.6.6.4.4.4.4.7" xref="alg1.l12.m1.6.6.4.4.5.cmml">,</mo><mrow id="alg1.l12.m1.5.5.3.3.3.3.3.1" xref="alg1.l12.m1.5.5.3.3.3.3.3.2.cmml"><mo stretchy="false" id="alg1.l12.m1.5.5.3.3.3.3.3.1.2" xref="alg1.l12.m1.5.5.3.3.3.3.3.2.1.cmml">|</mo><msub id="alg1.l12.m1.5.5.3.3.3.3.3.1.1" xref="alg1.l12.m1.5.5.3.3.3.3.3.1.1.cmml"><mi id="alg1.l12.m1.5.5.3.3.3.3.3.1.1.2" xref="alg1.l12.m1.5.5.3.3.3.3.3.1.1.2.cmml">V</mi><mn id="alg1.l12.m1.5.5.3.3.3.3.3.1.1.3" xref="alg1.l12.m1.5.5.3.3.3.3.3.1.1.3.cmml">3</mn></msub><mo stretchy="false" id="alg1.l12.m1.5.5.3.3.3.3.3.1.3" xref="alg1.l12.m1.5.5.3.3.3.3.3.2.1.cmml">|</mo></mrow><mo id="alg1.l12.m1.6.6.4.4.4.4.8" xref="alg1.l12.m1.6.6.4.4.5.cmml">,</mo><mi mathvariant="normal" id="alg1.l12.m1.2.2" xref="alg1.l12.m1.2.2.cmml">…</mi><mo id="alg1.l12.m1.6.6.4.4.4.4.9" xref="alg1.l12.m1.6.6.4.4.5.cmml">,</mo><mrow id="alg1.l12.m1.6.6.4.4.4.4.4.1" xref="alg1.l12.m1.6.6.4.4.4.4.4.2.cmml"><mo stretchy="false" id="alg1.l12.m1.6.6.4.4.4.4.4.1.2" xref="alg1.l12.m1.6.6.4.4.4.4.4.2.1.cmml">|</mo><msub id="alg1.l12.m1.6.6.4.4.4.4.4.1.1" xref="alg1.l12.m1.6.6.4.4.4.4.4.1.1.cmml"><mi id="alg1.l12.m1.6.6.4.4.4.4.4.1.1.2" xref="alg1.l12.m1.6.6.4.4.4.4.4.1.1.2.cmml">V</mi><mi id="alg1.l12.m1.6.6.4.4.4.4.4.1.1.3" xref="alg1.l12.m1.6.6.4.4.4.4.4.1.1.3.cmml">n</mi></msub><mo stretchy="false" id="alg1.l12.m1.6.6.4.4.4.4.4.1.3" xref="alg1.l12.m1.6.6.4.4.4.4.4.2.1.cmml">|</mo></mrow><mo stretchy="false" id="alg1.l12.m1.6.6.4.4.4.4.10" xref="alg1.l12.m1.6.6.4.4.5.cmml">}</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l12.m1.6b"><apply id="alg1.l12.m1.6.6.cmml" xref="alg1.l12.m1.6.6"><eq id="alg1.l12.m1.6.6.5.cmml" xref="alg1.l12.m1.6.6.5"></eq><ci id="alg1.l12.m1.6.6.6.cmml" xref="alg1.l12.m1.6.6.6">𝑗</ci><apply id="alg1.l12.m1.6.6.4.cmml" xref="alg1.l12.m1.6.6.4"><arg id="alg1.l12.m1.6.6.4.5.cmml" xref="alg1.l12.m1.6.6.4.5"></arg><apply id="alg1.l12.m1.6.6.4.4.5.cmml" xref="alg1.l12.m1.6.6.4.4.4"><max id="alg1.l12.m1.1.1.cmml" xref="alg1.l12.m1.1.1"></max><apply id="alg1.l12.m1.3.3.1.1.1.1.1.2.cmml" xref="alg1.l12.m1.3.3.1.1.1.1.1.1"><abs id="alg1.l12.m1.3.3.1.1.1.1.1.2.1.cmml" xref="alg1.l12.m1.3.3.1.1.1.1.1.1.2"></abs><apply id="alg1.l12.m1.3.3.1.1.1.1.1.1.1.cmml" xref="alg1.l12.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l12.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="alg1.l12.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="alg1.l12.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="alg1.l12.m1.3.3.1.1.1.1.1.1.1.2">𝑉</ci><cn type="integer" id="alg1.l12.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="alg1.l12.m1.3.3.1.1.1.1.1.1.1.3">1</cn></apply></apply><apply id="alg1.l12.m1.4.4.2.2.2.2.2.2.cmml" xref="alg1.l12.m1.4.4.2.2.2.2.2.1"><abs id="alg1.l12.m1.4.4.2.2.2.2.2.2.1.cmml" xref="alg1.l12.m1.4.4.2.2.2.2.2.1.2"></abs><apply id="alg1.l12.m1.4.4.2.2.2.2.2.1.1.cmml" xref="alg1.l12.m1.4.4.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="alg1.l12.m1.4.4.2.2.2.2.2.1.1.1.cmml" xref="alg1.l12.m1.4.4.2.2.2.2.2.1.1">subscript</csymbol><ci id="alg1.l12.m1.4.4.2.2.2.2.2.1.1.2.cmml" xref="alg1.l12.m1.4.4.2.2.2.2.2.1.1.2">𝑉</ci><cn type="integer" id="alg1.l12.m1.4.4.2.2.2.2.2.1.1.3.cmml" xref="alg1.l12.m1.4.4.2.2.2.2.2.1.1.3">2</cn></apply></apply><apply id="alg1.l12.m1.5.5.3.3.3.3.3.2.cmml" xref="alg1.l12.m1.5.5.3.3.3.3.3.1"><abs id="alg1.l12.m1.5.5.3.3.3.3.3.2.1.cmml" xref="alg1.l12.m1.5.5.3.3.3.3.3.1.2"></abs><apply id="alg1.l12.m1.5.5.3.3.3.3.3.1.1.cmml" xref="alg1.l12.m1.5.5.3.3.3.3.3.1.1"><csymbol cd="ambiguous" id="alg1.l12.m1.5.5.3.3.3.3.3.1.1.1.cmml" xref="alg1.l12.m1.5.5.3.3.3.3.3.1.1">subscript</csymbol><ci id="alg1.l12.m1.5.5.3.3.3.3.3.1.1.2.cmml" xref="alg1.l12.m1.5.5.3.3.3.3.3.1.1.2">𝑉</ci><cn type="integer" id="alg1.l12.m1.5.5.3.3.3.3.3.1.1.3.cmml" xref="alg1.l12.m1.5.5.3.3.3.3.3.1.1.3">3</cn></apply></apply><ci id="alg1.l12.m1.2.2.cmml" xref="alg1.l12.m1.2.2">…</ci><apply id="alg1.l12.m1.6.6.4.4.4.4.4.2.cmml" xref="alg1.l12.m1.6.6.4.4.4.4.4.1"><abs id="alg1.l12.m1.6.6.4.4.4.4.4.2.1.cmml" xref="alg1.l12.m1.6.6.4.4.4.4.4.1.2"></abs><apply id="alg1.l12.m1.6.6.4.4.4.4.4.1.1.cmml" xref="alg1.l12.m1.6.6.4.4.4.4.4.1.1"><csymbol cd="ambiguous" id="alg1.l12.m1.6.6.4.4.4.4.4.1.1.1.cmml" xref="alg1.l12.m1.6.6.4.4.4.4.4.1.1">subscript</csymbol><ci id="alg1.l12.m1.6.6.4.4.4.4.4.1.1.2.cmml" xref="alg1.l12.m1.6.6.4.4.4.4.4.1.1.2">𝑉</ci><ci id="alg1.l12.m1.6.6.4.4.4.4.4.1.1.3.cmml" xref="alg1.l12.m1.6.6.4.4.4.4.4.1.1.3">𝑛</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l12.m1.6c">j=\arg\max\{|V_{1}|,|V_{2}|,|V_{3}|,...,|V_{n}|\}</annotation></semantics></math>

</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span>     <span id="alg1.l13.2" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l13.m1.2" class="ltx_Math" alttext="\left(V_{j},E_{j}\right)" display="inline"><semantics id="alg1.l13.m1.2a"><mrow id="alg1.l13.m1.2.2.2" xref="alg1.l13.m1.2.2.3.cmml"><mo id="alg1.l13.m1.2.2.2.3" xref="alg1.l13.m1.2.2.3.cmml">(</mo><msub id="alg1.l13.m1.1.1.1.1" xref="alg1.l13.m1.1.1.1.1.cmml"><mi id="alg1.l13.m1.1.1.1.1.2" xref="alg1.l13.m1.1.1.1.1.2.cmml">V</mi><mi id="alg1.l13.m1.1.1.1.1.3" xref="alg1.l13.m1.1.1.1.1.3.cmml">j</mi></msub><mo id="alg1.l13.m1.2.2.2.4" xref="alg1.l13.m1.2.2.3.cmml">,</mo><msub id="alg1.l13.m1.2.2.2.2" xref="alg1.l13.m1.2.2.2.2.cmml"><mi id="alg1.l13.m1.2.2.2.2.2" xref="alg1.l13.m1.2.2.2.2.2.cmml">E</mi><mi id="alg1.l13.m1.2.2.2.2.3" xref="alg1.l13.m1.2.2.2.2.3.cmml">j</mi></msub><mo id="alg1.l13.m1.2.2.2.5" xref="alg1.l13.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l13.m1.2b"><interval closure="open" id="alg1.l13.m1.2.2.3.cmml" xref="alg1.l13.m1.2.2.2"><apply id="alg1.l13.m1.1.1.1.1.cmml" xref="alg1.l13.m1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l13.m1.1.1.1.1.1.cmml" xref="alg1.l13.m1.1.1.1.1">subscript</csymbol><ci id="alg1.l13.m1.1.1.1.1.2.cmml" xref="alg1.l13.m1.1.1.1.1.2">𝑉</ci><ci id="alg1.l13.m1.1.1.1.1.3.cmml" xref="alg1.l13.m1.1.1.1.1.3">𝑗</ci></apply><apply id="alg1.l13.m1.2.2.2.2.cmml" xref="alg1.l13.m1.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l13.m1.2.2.2.2.1.cmml" xref="alg1.l13.m1.2.2.2.2">subscript</csymbol><ci id="alg1.l13.m1.2.2.2.2.2.cmml" xref="alg1.l13.m1.2.2.2.2.2">𝐸</ci><ci id="alg1.l13.m1.2.2.2.2.3.cmml" xref="alg1.l13.m1.2.2.2.2.3">𝑗</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="alg1.l13.m1.2c">\left(V_{j},E_{j}\right)</annotation></semantics></math>

</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span><span id="alg1.l14.2" class="ltx_text ltx_font_bold">procedure</span> <span id="alg1.l14.3" class="ltx_text ltx_font_smallcaps">Iterative Trimming</span>(<math id="alg1.l14.m1.1" class="ltx_Math" alttext="G" display="inline"><semantics id="alg1.l14.m1.1a"><mi id="alg1.l14.m1.1.1" xref="alg1.l14.m1.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="alg1.l14.m1.1b"><ci id="alg1.l14.m1.1.1.cmml" xref="alg1.l14.m1.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l14.m1.1c">G</annotation></semantics></math>)

</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span>     <span id="alg1.l15.2" class="ltx_text ltx_font_bold">while</span> True <span id="alg1.l15.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span>         <math id="alg1.l16.m1.4" class="ltx_Math" alttext="G^{\prime}=\text{DegreeTrimming}(G,1),\text{where }G^{\prime}=(V^{\prime},E^{\prime})" display="inline"><semantics id="alg1.l16.m1.4a"><mrow id="alg1.l16.m1.4.4.2" xref="alg1.l16.m1.4.4.3.cmml"><mrow id="alg1.l16.m1.3.3.1.1" xref="alg1.l16.m1.3.3.1.1.cmml"><msup id="alg1.l16.m1.3.3.1.1.2" xref="alg1.l16.m1.3.3.1.1.2.cmml"><mi id="alg1.l16.m1.3.3.1.1.2.2" xref="alg1.l16.m1.3.3.1.1.2.2.cmml">G</mi><mo id="alg1.l16.m1.3.3.1.1.2.3" xref="alg1.l16.m1.3.3.1.1.2.3.cmml">′</mo></msup><mo id="alg1.l16.m1.3.3.1.1.1" xref="alg1.l16.m1.3.3.1.1.1.cmml">=</mo><mrow id="alg1.l16.m1.3.3.1.1.3" xref="alg1.l16.m1.3.3.1.1.3.cmml"><mtext id="alg1.l16.m1.3.3.1.1.3.2" xref="alg1.l16.m1.3.3.1.1.3.2a.cmml">DegreeTrimming</mtext><mo lspace="0em" rspace="0em" id="alg1.l16.m1.3.3.1.1.3.1" xref="alg1.l16.m1.3.3.1.1.3.1.cmml">​</mo><mrow id="alg1.l16.m1.3.3.1.1.3.3.2" xref="alg1.l16.m1.3.3.1.1.3.3.1.cmml"><mo stretchy="false" id="alg1.l16.m1.3.3.1.1.3.3.2.1" xref="alg1.l16.m1.3.3.1.1.3.3.1.cmml">(</mo><mi id="alg1.l16.m1.1.1" xref="alg1.l16.m1.1.1.cmml">G</mi><mo id="alg1.l16.m1.3.3.1.1.3.3.2.2" xref="alg1.l16.m1.3.3.1.1.3.3.1.cmml">,</mo><mn id="alg1.l16.m1.2.2" xref="alg1.l16.m1.2.2.cmml">1</mn><mo stretchy="false" id="alg1.l16.m1.3.3.1.1.3.3.2.3" xref="alg1.l16.m1.3.3.1.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="alg1.l16.m1.4.4.2.3" xref="alg1.l16.m1.4.4.3a.cmml">,</mo><mrow id="alg1.l16.m1.4.4.2.2" xref="alg1.l16.m1.4.4.2.2.cmml"><mrow id="alg1.l16.m1.4.4.2.2.4" xref="alg1.l16.m1.4.4.2.2.4.cmml"><mtext id="alg1.l16.m1.4.4.2.2.4.2" xref="alg1.l16.m1.4.4.2.2.4.2a.cmml">where </mtext><mo lspace="0em" rspace="0em" id="alg1.l16.m1.4.4.2.2.4.1" xref="alg1.l16.m1.4.4.2.2.4.1.cmml">​</mo><msup id="alg1.l16.m1.4.4.2.2.4.3" xref="alg1.l16.m1.4.4.2.2.4.3.cmml"><mi id="alg1.l16.m1.4.4.2.2.4.3.2" xref="alg1.l16.m1.4.4.2.2.4.3.2.cmml">G</mi><mo id="alg1.l16.m1.4.4.2.2.4.3.3" xref="alg1.l16.m1.4.4.2.2.4.3.3.cmml">′</mo></msup></mrow><mo id="alg1.l16.m1.4.4.2.2.3" xref="alg1.l16.m1.4.4.2.2.3.cmml">=</mo><mrow id="alg1.l16.m1.4.4.2.2.2.2" xref="alg1.l16.m1.4.4.2.2.2.3.cmml"><mo stretchy="false" id="alg1.l16.m1.4.4.2.2.2.2.3" xref="alg1.l16.m1.4.4.2.2.2.3.cmml">(</mo><msup id="alg1.l16.m1.4.4.2.2.1.1.1" xref="alg1.l16.m1.4.4.2.2.1.1.1.cmml"><mi id="alg1.l16.m1.4.4.2.2.1.1.1.2" xref="alg1.l16.m1.4.4.2.2.1.1.1.2.cmml">V</mi><mo id="alg1.l16.m1.4.4.2.2.1.1.1.3" xref="alg1.l16.m1.4.4.2.2.1.1.1.3.cmml">′</mo></msup><mo id="alg1.l16.m1.4.4.2.2.2.2.4" xref="alg1.l16.m1.4.4.2.2.2.3.cmml">,</mo><msup id="alg1.l16.m1.4.4.2.2.2.2.2" xref="alg1.l16.m1.4.4.2.2.2.2.2.cmml"><mi id="alg1.l16.m1.4.4.2.2.2.2.2.2" xref="alg1.l16.m1.4.4.2.2.2.2.2.2.cmml">E</mi><mo id="alg1.l16.m1.4.4.2.2.2.2.2.3" xref="alg1.l16.m1.4.4.2.2.2.2.2.3.cmml">′</mo></msup><mo stretchy="false" id="alg1.l16.m1.4.4.2.2.2.2.5" xref="alg1.l16.m1.4.4.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l16.m1.4b"><apply id="alg1.l16.m1.4.4.3.cmml" xref="alg1.l16.m1.4.4.2"><csymbol cd="ambiguous" id="alg1.l16.m1.4.4.3a.cmml" xref="alg1.l16.m1.4.4.2.3">formulae-sequence</csymbol><apply id="alg1.l16.m1.3.3.1.1.cmml" xref="alg1.l16.m1.3.3.1.1"><eq id="alg1.l16.m1.3.3.1.1.1.cmml" xref="alg1.l16.m1.3.3.1.1.1"></eq><apply id="alg1.l16.m1.3.3.1.1.2.cmml" xref="alg1.l16.m1.3.3.1.1.2"><csymbol cd="ambiguous" id="alg1.l16.m1.3.3.1.1.2.1.cmml" xref="alg1.l16.m1.3.3.1.1.2">superscript</csymbol><ci id="alg1.l16.m1.3.3.1.1.2.2.cmml" xref="alg1.l16.m1.3.3.1.1.2.2">𝐺</ci><ci id="alg1.l16.m1.3.3.1.1.2.3.cmml" xref="alg1.l16.m1.3.3.1.1.2.3">′</ci></apply><apply id="alg1.l16.m1.3.3.1.1.3.cmml" xref="alg1.l16.m1.3.3.1.1.3"><times id="alg1.l16.m1.3.3.1.1.3.1.cmml" xref="alg1.l16.m1.3.3.1.1.3.1"></times><ci id="alg1.l16.m1.3.3.1.1.3.2a.cmml" xref="alg1.l16.m1.3.3.1.1.3.2"><mtext id="alg1.l16.m1.3.3.1.1.3.2.cmml" xref="alg1.l16.m1.3.3.1.1.3.2">DegreeTrimming</mtext></ci><interval closure="open" id="alg1.l16.m1.3.3.1.1.3.3.1.cmml" xref="alg1.l16.m1.3.3.1.1.3.3.2"><ci id="alg1.l16.m1.1.1.cmml" xref="alg1.l16.m1.1.1">𝐺</ci><cn type="integer" id="alg1.l16.m1.2.2.cmml" xref="alg1.l16.m1.2.2">1</cn></interval></apply></apply><apply id="alg1.l16.m1.4.4.2.2.cmml" xref="alg1.l16.m1.4.4.2.2"><eq id="alg1.l16.m1.4.4.2.2.3.cmml" xref="alg1.l16.m1.4.4.2.2.3"></eq><apply id="alg1.l16.m1.4.4.2.2.4.cmml" xref="alg1.l16.m1.4.4.2.2.4"><times id="alg1.l16.m1.4.4.2.2.4.1.cmml" xref="alg1.l16.m1.4.4.2.2.4.1"></times><ci id="alg1.l16.m1.4.4.2.2.4.2a.cmml" xref="alg1.l16.m1.4.4.2.2.4.2"><mtext id="alg1.l16.m1.4.4.2.2.4.2.cmml" xref="alg1.l16.m1.4.4.2.2.4.2">where </mtext></ci><apply id="alg1.l16.m1.4.4.2.2.4.3.cmml" xref="alg1.l16.m1.4.4.2.2.4.3"><csymbol cd="ambiguous" id="alg1.l16.m1.4.4.2.2.4.3.1.cmml" xref="alg1.l16.m1.4.4.2.2.4.3">superscript</csymbol><ci id="alg1.l16.m1.4.4.2.2.4.3.2.cmml" xref="alg1.l16.m1.4.4.2.2.4.3.2">𝐺</ci><ci id="alg1.l16.m1.4.4.2.2.4.3.3.cmml" xref="alg1.l16.m1.4.4.2.2.4.3.3">′</ci></apply></apply><interval closure="open" id="alg1.l16.m1.4.4.2.2.2.3.cmml" xref="alg1.l16.m1.4.4.2.2.2.2"><apply id="alg1.l16.m1.4.4.2.2.1.1.1.cmml" xref="alg1.l16.m1.4.4.2.2.1.1.1"><csymbol cd="ambiguous" id="alg1.l16.m1.4.4.2.2.1.1.1.1.cmml" xref="alg1.l16.m1.4.4.2.2.1.1.1">superscript</csymbol><ci id="alg1.l16.m1.4.4.2.2.1.1.1.2.cmml" xref="alg1.l16.m1.4.4.2.2.1.1.1.2">𝑉</ci><ci id="alg1.l16.m1.4.4.2.2.1.1.1.3.cmml" xref="alg1.l16.m1.4.4.2.2.1.1.1.3">′</ci></apply><apply id="alg1.l16.m1.4.4.2.2.2.2.2.cmml" xref="alg1.l16.m1.4.4.2.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l16.m1.4.4.2.2.2.2.2.1.cmml" xref="alg1.l16.m1.4.4.2.2.2.2.2">superscript</csymbol><ci id="alg1.l16.m1.4.4.2.2.2.2.2.2.cmml" xref="alg1.l16.m1.4.4.2.2.2.2.2.2">𝐸</ci><ci id="alg1.l16.m1.4.4.2.2.2.2.2.3.cmml" xref="alg1.l16.m1.4.4.2.2.2.2.2.3">′</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l16.m1.4c">G^{\prime}=\text{DegreeTrimming}(G,1),\text{where }G^{\prime}=(V^{\prime},E^{\prime})</annotation></semantics></math>

</div>
<div id="alg1.l17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l17.1.1.1" class="ltx_text" style="font-size:80%;">17:</span></span>         <span id="alg1.l17.2" class="ltx_text ltx_font_bold">if</span> <math id="alg1.l17.m1.1" class="ltx_math_unparsed" alttext="|V|==|V^{\prime}|" display="inline"><semantics id="alg1.l17.m1.1a"><mrow id="alg1.l17.m1.1b"><mo fence="false" rspace="0.167em" stretchy="false" id="alg1.l17.m1.1.2">|</mo><mi id="alg1.l17.m1.1.1">V</mi><mo fence="false" stretchy="false" id="alg1.l17.m1.1.3">|</mo><mo lspace="0.167em" rspace="0em" id="alg1.l17.m1.1.4">=</mo><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.5">=</mo><mo fence="false" rspace="0.167em" stretchy="false" id="alg1.l17.m1.1.6">|</mo><msup id="alg1.l17.m1.1.7"><mi id="alg1.l17.m1.1.7.2">V</mi><mo id="alg1.l17.m1.1.7.3">′</mo></msup><mo fence="false" stretchy="false" id="alg1.l17.m1.1.8">|</mo></mrow><annotation encoding="application/x-tex" id="alg1.l17.m1.1c">|V|==|V^{\prime}|</annotation></semantics></math> <span id="alg1.l17.3" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l18.1.1.1" class="ltx_text" style="font-size:80%;">18:</span></span>              <span id="alg1.l18.2" class="ltx_text ltx_font_bold">break</span>
              
</div>
<div id="alg1.l19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l19.1.1.1" class="ltx_text" style="font-size:80%;">19:</span></span>     <span id="alg1.l19.2" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l19.m1.2" class="ltx_Math" alttext="\left(V^{\prime},E^{\prime}\right)" display="inline"><semantics id="alg1.l19.m1.2a"><mrow id="alg1.l19.m1.2.2.2" xref="alg1.l19.m1.2.2.3.cmml"><mo id="alg1.l19.m1.2.2.2.3" xref="alg1.l19.m1.2.2.3.cmml">(</mo><msup id="alg1.l19.m1.1.1.1.1" xref="alg1.l19.m1.1.1.1.1.cmml"><mi id="alg1.l19.m1.1.1.1.1.2" xref="alg1.l19.m1.1.1.1.1.2.cmml">V</mi><mo id="alg1.l19.m1.1.1.1.1.3" xref="alg1.l19.m1.1.1.1.1.3.cmml">′</mo></msup><mo id="alg1.l19.m1.2.2.2.4" xref="alg1.l19.m1.2.2.3.cmml">,</mo><msup id="alg1.l19.m1.2.2.2.2" xref="alg1.l19.m1.2.2.2.2.cmml"><mi id="alg1.l19.m1.2.2.2.2.2" xref="alg1.l19.m1.2.2.2.2.2.cmml">E</mi><mo id="alg1.l19.m1.2.2.2.2.3" xref="alg1.l19.m1.2.2.2.2.3.cmml">′</mo></msup><mo id="alg1.l19.m1.2.2.2.5" xref="alg1.l19.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="alg1.l19.m1.2b"><interval closure="open" id="alg1.l19.m1.2.2.3.cmml" xref="alg1.l19.m1.2.2.2"><apply id="alg1.l19.m1.1.1.1.1.cmml" xref="alg1.l19.m1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l19.m1.1.1.1.1.1.cmml" xref="alg1.l19.m1.1.1.1.1">superscript</csymbol><ci id="alg1.l19.m1.1.1.1.1.2.cmml" xref="alg1.l19.m1.1.1.1.1.2">𝑉</ci><ci id="alg1.l19.m1.1.1.1.1.3.cmml" xref="alg1.l19.m1.1.1.1.1.3">′</ci></apply><apply id="alg1.l19.m1.2.2.2.2.cmml" xref="alg1.l19.m1.2.2.2.2"><csymbol cd="ambiguous" id="alg1.l19.m1.2.2.2.2.1.cmml" xref="alg1.l19.m1.2.2.2.2">superscript</csymbol><ci id="alg1.l19.m1.2.2.2.2.2.cmml" xref="alg1.l19.m1.2.2.2.2.2">𝐸</ci><ci id="alg1.l19.m1.2.2.2.2.3.cmml" xref="alg1.l19.m1.2.2.2.2.3">′</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="alg1.l19.m1.2c">\left(V^{\prime},E^{\prime}\right)</annotation></semantics></math>

</div>
<div id="alg1.l20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l20.2.1.1" class="ltx_text" style="font-size:80%;">20:</span></span><math id="alg1.l20.m1.2" class="ltx_Math" alttext="G^{\prime}=\text{DegreeTrimming}(G,0)" display="inline"><semantics id="alg1.l20.m1.2a"><mrow id="alg1.l20.m1.2.3" xref="alg1.l20.m1.2.3.cmml"><msup id="alg1.l20.m1.2.3.2" xref="alg1.l20.m1.2.3.2.cmml"><mi id="alg1.l20.m1.2.3.2.2" xref="alg1.l20.m1.2.3.2.2.cmml">G</mi><mo id="alg1.l20.m1.2.3.2.3" xref="alg1.l20.m1.2.3.2.3.cmml">′</mo></msup><mo id="alg1.l20.m1.2.3.1" xref="alg1.l20.m1.2.3.1.cmml">=</mo><mrow id="alg1.l20.m1.2.3.3" xref="alg1.l20.m1.2.3.3.cmml"><mtext id="alg1.l20.m1.2.3.3.2" xref="alg1.l20.m1.2.3.3.2a.cmml">DegreeTrimming</mtext><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.3.3.1" xref="alg1.l20.m1.2.3.3.1.cmml">​</mo><mrow id="alg1.l20.m1.2.3.3.3.2" xref="alg1.l20.m1.2.3.3.3.1.cmml"><mo stretchy="false" id="alg1.l20.m1.2.3.3.3.2.1" xref="alg1.l20.m1.2.3.3.3.1.cmml">(</mo><mi id="alg1.l20.m1.1.1" xref="alg1.l20.m1.1.1.cmml">G</mi><mo id="alg1.l20.m1.2.3.3.3.2.2" xref="alg1.l20.m1.2.3.3.3.1.cmml">,</mo><mn id="alg1.l20.m1.2.2" xref="alg1.l20.m1.2.2.cmml">0</mn><mo stretchy="false" id="alg1.l20.m1.2.3.3.3.2.3" xref="alg1.l20.m1.2.3.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l20.m1.2b"><apply id="alg1.l20.m1.2.3.cmml" xref="alg1.l20.m1.2.3"><eq id="alg1.l20.m1.2.3.1.cmml" xref="alg1.l20.m1.2.3.1"></eq><apply id="alg1.l20.m1.2.3.2.cmml" xref="alg1.l20.m1.2.3.2"><csymbol cd="ambiguous" id="alg1.l20.m1.2.3.2.1.cmml" xref="alg1.l20.m1.2.3.2">superscript</csymbol><ci id="alg1.l20.m1.2.3.2.2.cmml" xref="alg1.l20.m1.2.3.2.2">𝐺</ci><ci id="alg1.l20.m1.2.3.2.3.cmml" xref="alg1.l20.m1.2.3.2.3">′</ci></apply><apply id="alg1.l20.m1.2.3.3.cmml" xref="alg1.l20.m1.2.3.3"><times id="alg1.l20.m1.2.3.3.1.cmml" xref="alg1.l20.m1.2.3.3.1"></times><ci id="alg1.l20.m1.2.3.3.2a.cmml" xref="alg1.l20.m1.2.3.3.2"><mtext id="alg1.l20.m1.2.3.3.2.cmml" xref="alg1.l20.m1.2.3.3.2">DegreeTrimming</mtext></ci><interval closure="open" id="alg1.l20.m1.2.3.3.3.1.cmml" xref="alg1.l20.m1.2.3.3.3.2"><ci id="alg1.l20.m1.1.1.cmml" xref="alg1.l20.m1.1.1">𝐺</ci><cn type="integer" id="alg1.l20.m1.2.2.cmml" xref="alg1.l20.m1.2.2">0</cn></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l20.m1.2c">G^{\prime}=\text{DegreeTrimming}(G,0)</annotation></semantics></math> <span id="alg1.l20.1" class="ltx_text" style="float:right;"><math id="alg1.l20.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l20.1.m1.1a"><mo id="alg1.l20.1.m1.1.1" xref="alg1.l20.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l20.1.m1.1b"><ci id="alg1.l20.1.m1.1.1.cmml" xref="alg1.l20.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l20.1.m1.1c">\triangleright</annotation></semantics></math> Remove 0-deg vertices
</span>
</div>
<div id="alg1.l21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l21.2.1.1" class="ltx_text" style="font-size:80%;">21:</span></span><math id="alg1.l21.m1.1" class="ltx_Math" alttext="G^{\prime}=\text{SubconnectedGraphTrimming}(G^{\prime})" display="inline"><semantics id="alg1.l21.m1.1a"><mrow id="alg1.l21.m1.1.1" xref="alg1.l21.m1.1.1.cmml"><msup id="alg1.l21.m1.1.1.3" xref="alg1.l21.m1.1.1.3.cmml"><mi id="alg1.l21.m1.1.1.3.2" xref="alg1.l21.m1.1.1.3.2.cmml">G</mi><mo id="alg1.l21.m1.1.1.3.3" xref="alg1.l21.m1.1.1.3.3.cmml">′</mo></msup><mo id="alg1.l21.m1.1.1.2" xref="alg1.l21.m1.1.1.2.cmml">=</mo><mrow id="alg1.l21.m1.1.1.1" xref="alg1.l21.m1.1.1.1.cmml"><mtext id="alg1.l21.m1.1.1.1.3" xref="alg1.l21.m1.1.1.1.3a.cmml">SubconnectedGraphTrimming</mtext><mo lspace="0em" rspace="0em" id="alg1.l21.m1.1.1.1.2" xref="alg1.l21.m1.1.1.1.2.cmml">​</mo><mrow id="alg1.l21.m1.1.1.1.1.1" xref="alg1.l21.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l21.m1.1.1.1.1.1.2" xref="alg1.l21.m1.1.1.1.1.1.1.cmml">(</mo><msup id="alg1.l21.m1.1.1.1.1.1.1" xref="alg1.l21.m1.1.1.1.1.1.1.cmml"><mi id="alg1.l21.m1.1.1.1.1.1.1.2" xref="alg1.l21.m1.1.1.1.1.1.1.2.cmml">G</mi><mo id="alg1.l21.m1.1.1.1.1.1.1.3" xref="alg1.l21.m1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="alg1.l21.m1.1.1.1.1.1.3" xref="alg1.l21.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l21.m1.1b"><apply id="alg1.l21.m1.1.1.cmml" xref="alg1.l21.m1.1.1"><eq id="alg1.l21.m1.1.1.2.cmml" xref="alg1.l21.m1.1.1.2"></eq><apply id="alg1.l21.m1.1.1.3.cmml" xref="alg1.l21.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l21.m1.1.1.3.1.cmml" xref="alg1.l21.m1.1.1.3">superscript</csymbol><ci id="alg1.l21.m1.1.1.3.2.cmml" xref="alg1.l21.m1.1.1.3.2">𝐺</ci><ci id="alg1.l21.m1.1.1.3.3.cmml" xref="alg1.l21.m1.1.1.3.3">′</ci></apply><apply id="alg1.l21.m1.1.1.1.cmml" xref="alg1.l21.m1.1.1.1"><times id="alg1.l21.m1.1.1.1.2.cmml" xref="alg1.l21.m1.1.1.1.2"></times><ci id="alg1.l21.m1.1.1.1.3a.cmml" xref="alg1.l21.m1.1.1.1.3"><mtext id="alg1.l21.m1.1.1.1.3.cmml" xref="alg1.l21.m1.1.1.1.3">SubconnectedGraphTrimming</mtext></ci><apply id="alg1.l21.m1.1.1.1.1.1.1.cmml" xref="alg1.l21.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l21.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l21.m1.1.1.1.1.1">superscript</csymbol><ci id="alg1.l21.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l21.m1.1.1.1.1.1.1.2">𝐺</ci><ci id="alg1.l21.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l21.m1.1.1.1.1.1.1.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.m1.1c">G^{\prime}=\text{SubconnectedGraphTrimming}(G^{\prime})</annotation></semantics></math> <span id="alg1.l21.1" class="ltx_text" style="float:right;"><math id="alg1.l21.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l21.1.m1.1a"><mo id="alg1.l21.1.m1.1.1" xref="alg1.l21.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l21.1.m1.1b"><ci id="alg1.l21.1.m1.1.1.cmml" xref="alg1.l21.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l21.1.m1.1c">\triangleright</annotation></semantics></math> Keep largest connected subgraph
</span>
</div>
<div id="alg1.l22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l22.2.1.1" class="ltx_text" style="font-size:80%;">22:</span></span><math id="alg1.l22.m1.1" class="ltx_Math" alttext="G^{\prime}=\text{IterativeTrimming}(G^{\prime})" display="inline"><semantics id="alg1.l22.m1.1a"><mrow id="alg1.l22.m1.1.1" xref="alg1.l22.m1.1.1.cmml"><msup id="alg1.l22.m1.1.1.3" xref="alg1.l22.m1.1.1.3.cmml"><mi id="alg1.l22.m1.1.1.3.2" xref="alg1.l22.m1.1.1.3.2.cmml">G</mi><mo id="alg1.l22.m1.1.1.3.3" xref="alg1.l22.m1.1.1.3.3.cmml">′</mo></msup><mo id="alg1.l22.m1.1.1.2" xref="alg1.l22.m1.1.1.2.cmml">=</mo><mrow id="alg1.l22.m1.1.1.1" xref="alg1.l22.m1.1.1.1.cmml"><mtext id="alg1.l22.m1.1.1.1.3" xref="alg1.l22.m1.1.1.1.3a.cmml">IterativeTrimming</mtext><mo lspace="0em" rspace="0em" id="alg1.l22.m1.1.1.1.2" xref="alg1.l22.m1.1.1.1.2.cmml">​</mo><mrow id="alg1.l22.m1.1.1.1.1.1" xref="alg1.l22.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="alg1.l22.m1.1.1.1.1.1.2" xref="alg1.l22.m1.1.1.1.1.1.1.cmml">(</mo><msup id="alg1.l22.m1.1.1.1.1.1.1" xref="alg1.l22.m1.1.1.1.1.1.1.cmml"><mi id="alg1.l22.m1.1.1.1.1.1.1.2" xref="alg1.l22.m1.1.1.1.1.1.1.2.cmml">G</mi><mo id="alg1.l22.m1.1.1.1.1.1.1.3" xref="alg1.l22.m1.1.1.1.1.1.1.3.cmml">′</mo></msup><mo stretchy="false" id="alg1.l22.m1.1.1.1.1.1.3" xref="alg1.l22.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l22.m1.1b"><apply id="alg1.l22.m1.1.1.cmml" xref="alg1.l22.m1.1.1"><eq id="alg1.l22.m1.1.1.2.cmml" xref="alg1.l22.m1.1.1.2"></eq><apply id="alg1.l22.m1.1.1.3.cmml" xref="alg1.l22.m1.1.1.3"><csymbol cd="ambiguous" id="alg1.l22.m1.1.1.3.1.cmml" xref="alg1.l22.m1.1.1.3">superscript</csymbol><ci id="alg1.l22.m1.1.1.3.2.cmml" xref="alg1.l22.m1.1.1.3.2">𝐺</ci><ci id="alg1.l22.m1.1.1.3.3.cmml" xref="alg1.l22.m1.1.1.3.3">′</ci></apply><apply id="alg1.l22.m1.1.1.1.cmml" xref="alg1.l22.m1.1.1.1"><times id="alg1.l22.m1.1.1.1.2.cmml" xref="alg1.l22.m1.1.1.1.2"></times><ci id="alg1.l22.m1.1.1.1.3a.cmml" xref="alg1.l22.m1.1.1.1.3"><mtext id="alg1.l22.m1.1.1.1.3.cmml" xref="alg1.l22.m1.1.1.1.3">IterativeTrimming</mtext></ci><apply id="alg1.l22.m1.1.1.1.1.1.1.cmml" xref="alg1.l22.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l22.m1.1.1.1.1.1.1.1.cmml" xref="alg1.l22.m1.1.1.1.1.1">superscript</csymbol><ci id="alg1.l22.m1.1.1.1.1.1.1.2.cmml" xref="alg1.l22.m1.1.1.1.1.1.1.2">𝐺</ci><ci id="alg1.l22.m1.1.1.1.1.1.1.3.cmml" xref="alg1.l22.m1.1.1.1.1.1.1.3">′</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l22.m1.1c">G^{\prime}=\text{IterativeTrimming}(G^{\prime})</annotation></semantics></math> <span id="alg1.l22.1" class="ltx_text" style="float:right;"><math id="alg1.l22.1.m1.1" class="ltx_Math" alttext="\triangleright" display="inline"><semantics id="alg1.l22.1.m1.1a"><mo id="alg1.l22.1.m1.1.1" xref="alg1.l22.1.m1.1.1.cmml">▷</mo><annotation-xml encoding="MathML-Content" id="alg1.l22.1.m1.1b"><ci id="alg1.l22.1.m1.1.1.cmml" xref="alg1.l22.1.m1.1.1">▷</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l22.1.m1.1c">\triangleright</annotation></semantics></math> Iteratively remove 1-deg vertices until equilibrium
</span>
</div>
<div id="alg1.l23" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l23.1.1.1" class="ltx_text" style="font-size:80%;">23:</span></span><span id="alg1.l23.2" class="ltx_text ltx_font_bold">return</span> <math id="alg1.l23.m1.1" class="ltx_Math" alttext="G^{\prime}" display="inline"><semantics id="alg1.l23.m1.1a"><msup id="alg1.l23.m1.1.1" xref="alg1.l23.m1.1.1.cmml"><mi id="alg1.l23.m1.1.1.2" xref="alg1.l23.m1.1.1.2.cmml">G</mi><mo id="alg1.l23.m1.1.1.3" xref="alg1.l23.m1.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="alg1.l23.m1.1b"><apply id="alg1.l23.m1.1.1.cmml" xref="alg1.l23.m1.1.1"><csymbol cd="ambiguous" id="alg1.l23.m1.1.1.1.cmml" xref="alg1.l23.m1.1.1">superscript</csymbol><ci id="alg1.l23.m1.1.1.2.cmml" xref="alg1.l23.m1.1.1.2">𝐺</ci><ci id="alg1.l23.m1.1.1.3.cmml" xref="alg1.l23.m1.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l23.m1.1c">G^{\prime}</annotation></semantics></math>

</div>
</div>
</figure>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Quality Control (Qualitative Corpus Reduction).</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Upon manually reviewing the 1,063 titles post-pruning, we found many papers irrelevant to our review’s focus, such as those on training multimodal neural networks and applying multimodal methods in medical imaging. Using regex keyword searches (specified in Appendix <a href="#A2.SS2.SSS2" title="B.2.2. Quality Control (Qualitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2.2</span></a>), we identified 217 titles for potential exclusion. After careful consideration, we removed 204 papers, retaining 13 for further evaluation, thus narrowing our corpus to 859 works. Consistent with Kitchenham’s guidelines <cite class="ltx_cite ltx_citemacro_citep">(Kitchenham, <a href="#bib.bib78" title="" class="ltx_ref">2004</a>)</cite>, we refined our corpus by sequentially reviewing titles, abstracts, and full texts, applying majority voting for exclusions, as detailed in Appendix <a href="#A2.SS2.SSS2" title="B.2.2. Quality Control (Qualitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2.2</span></a>. This process reduced our corpus to 388 from title evaluation, 127 from abstracts, and 75 from full-text assessments. Subsequent feature extraction led to the exclusion of two additional papers deemed outside our review’s scope, culminating in a final corpus of 73 papers.</p>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Feature Extraction</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Once the corpus was finalized, we extracted several features from each of the 73 papers. This included identifying information (e.g., title, authors, publication year), and information related to the paper’s methods (e.g., data collection medium, modalities, and analysis methods). Specifically, we extracted the following features from each paper (as outlined in Section <a href="#S2.SS2" title="2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>): UUID, title, authors, publication year, environment type, data collection mediums, modalities, analysis methods, fusion types, publication, environment settings, domains of study, participant interaction structures, didactic natures, levels of instruction, and analysis approaches. We detail our feature extraction scheme and each feature’s set of values in Appendix <a href="#A2.SS3" title="B.3. Feature Extraction ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.3</span></a>.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Analysis Procedure</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Leveraging our Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1. Framework ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> framework, we conducted a qualitative thematic analysis on the extracted features from our corpus. This yielded descriptive statistics and identified dominant trends for each framework component. We classified multimodal data into five comprehensive modality groups: (1) <span id="S3.SS4.p1.1.1" class="ltx_text ltx_font_italic">natural language</span>, (2) <span id="S3.SS4.p1.1.2" class="ltx_text ltx_font_italic">vision</span>, (3) <span id="S3.SS4.p1.1.3" class="ltx_text ltx_font_italic">sensors</span>, (4) <span id="S3.SS4.p1.1.4" class="ltx_text ltx_font_italic">human-centered</span>, and (5) <span id="S3.SS4.p1.1.5" class="ltx_text ltx_font_italic">logs</span>. For each group and the entire corpus, we explored the state-of-the-art, challenges, research gaps, and outcomes of multimodal learning and training analyses. Furthermore, we distilled multimodal learning and training research into three distinct research types, termed <span id="S3.SS4.p1.1.6" class="ltx_text ltx_font_italic">archetypes</span>. Our thematic findings for each framework component are detailed in Section <a href="#S4" title="4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, the archetypes in Section <a href="#S5" title="5. Archetypes ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, and a comprehensive discussion of the corpus and field in Section <a href="#S6" title="6. Discussion ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Framework Insights</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We present our findings for the individual components in the Figure <a href="#S2.F1" title="Figure 1 ‣ 2.1. Framework ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> framework (i.e., environment, multimodal data, data fusion, analysis, and feedback) in the subsections that follow. For reference, terminology definitions are enumerated in Section <a href="#S2.SS2" title="2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Environments</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We investigate learning and training environments for the three components specified in our framework, i.e., setting, learners/trainees, and data. Setting refers to the environment where the learning and training occur, learners and trainers refer to the environment participants, and sensors refers to the data collection mediums used in the environment.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Setting</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p">In Section <a href="#S2.SS2.SSS6" title="2.2.6. Environment Setting ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.6</span></a>, we categorized environments into four types: virtual, physical, blended, and unspecified. Our corpus revealed that virtual environments were predominant. We attribute this trend to the increasing reliance on online platforms for educational engagement, a phenomenon that the COVID-19 pandemic likely accelerated (evidenced by a spike in our corpus’s use of virtual environments in 2020). We initially hypothesized that recent technological advances may have engendered a rise in virtual multimodal learning and training; however, a temporal analysis of our corpus’ use of environment settings did not support this. 51/73 papers (70%) incorporated at least some virtual component (i.e., used either virtual or blended environments), which suggests most multimodal learning and training research relies, at least in part, on virtual environments to collect and analyze data <cite class="ltx_cite ltx_citemacro_citep">(Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2017b</a>; Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>)</cite>. In addition, we consider the distribution of learning versus training environments, as described in Section <a href="#S2.SS2.SSS1" title="2.2.1. Environment Type ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a>. There were more than three times as many learning environments papers (57/73; 78%) <cite class="ltx_cite ltx_citemacro_citep">(Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021</a>; Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2020a</a>)</cite> relative to training environments papers (16/73; 22%) <cite class="ltx_cite ltx_citemacro_citep">(Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>; Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite>. This imbalance underscores the focus of educational literature on knowledge acquisition. In contrast, the lower frequency of training settings reflects a narrower scope centered on skill enhancement and professional development. Notably, environments emphasizing physical activity were largely absent from our corpus. This includes environments focusing on activities like rehabilitative therapy and athletic training, as well as <span id="S4.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_italic">embodied learning</span> <cite class="ltx_cite ltx_citemacro_citep">(Stolz, <a href="#bib.bib142" title="" class="ltx_ref">2015</a>)</cite> environments that require students to physically engage in the learning activity.</p>
</div>
</section>
<section id="S4.SS1.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Learners/Trainees</h4>

<div id="S4.SS1.SSS2.p1" class="ltx_para">
<p id="S4.SS1.SSS2.p1.1" class="ltx_p">This review examines key elements of the learner’s domain, including the domain of study, participant interaction structure, didactic nature, and level of instruction or training. These elements collectively contribute to a comprehensive understanding of the learner’s experience and the educational context. Our corpus predominantly focuses on STEM+C domains of study (55/73; 75%) <cite class="ltx_cite ltx_citemacro_citep">(Fernandez-Nieto et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>; Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2021a</a>)</cite>, with humanities (11/73; 15%) <cite class="ltx_cite ltx_citemacro_citep">(Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2018</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2017b</a>)</cite> and psychomotor skills (5/73; 7%) <cite class="ltx_cite ltx_citemacro_citep">(Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>; Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>)</cite> being less represented. Four papers did not specify the domain of study, and two addressed domains outside of STEM+C, humanities, and psychomotor skills. This distribution suggests a significant emphasis on STEM+C education, reflecting global trends toward these disciplines’ importance in technology-driven societies and their relevance to the job market and societal advancement.</p>
</div>
<div id="S4.SS1.SSS2.p2" class="ltx_para">
<p id="S4.SS1.SSS2.p2.1" class="ltx_p">Individual-focused learning and training environments are the most prevalent participant interaction structure (45/73; 62%) <cite class="ltx_cite ltx_citemacro_citep">(Standen et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>; Aslan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>, compared to multi-person environments that are present in 31 (42%) papers <cite class="ltx_cite ltx_citemacro_citep">(Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>; Olsen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2020</a>)</cite>. This indicates most studies focus on individual learning and training experiences that allow for personalized and self-paced progress. However, the notable presence of multi-person settings underscores the importance of collaborative and social learning environments in educational research. The didactic nature of environments is predominantly formal and pedagogical (45/73; 62%) <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>; Mangaroska et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2020</a>; Tancredi et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2022</a>)</cite>, followed by training (15/73; 21%) <cite class="ltx_cite ltx_citemacro_citep">(Martinez-Maldonado et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2020</a>; Ochoa and Dominguez, <a href="#bib.bib107" title="" class="ltx_ref">2020</a>; Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite> and informal learning (12/73; 16%) <cite class="ltx_cite ltx_citemacro_citep">(Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>; Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>; Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>)</cite>. This suggests that formal instruction is the predominant mode, with a smaller yet notable focus on training and informal learning, often including more interactive, practical, or workplace-based scenarios. University-level instruction is the most common (36/73; 49%) <cite class="ltx_cite ltx_citemacro_citep">(Noël et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2022</a>; Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>)</cite>, followed closely by K-12 environments (30/73; 41%) <cite class="ltx_cite ltx_citemacro_citep">(Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2022</a>; Nasir et al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2021</a>)</cite>. Professional-level learning is less frequent (5/73; 7%) <cite class="ltx_cite ltx_citemacro_citep">(Noël et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2022</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>. The prominence of university-level participants reflects the research emphasis and academic focus of higher education, while the strong representation of K-12 participants indicates ongoing interest in foundational education practices. The underrepresentation of professional settings suggests a research gap in lifelong learning and continuing education.</p>
</div>
<div id="S4.SS1.SSS2.p3" class="ltx_para">
<p id="S4.SS1.SSS2.p3.1" class="ltx_p">The data on learner characteristics in our corpus highlights a landscape where STEM education is prioritized, individual learning experiences are valued, formal instruction is the standard, and university and K-12 education levels are emphasized. However, the presence of other educational levels and informal learning contexts indicates that there exists a diverse range of learning experiences and instructional approaches. This diversity presents both challenges and opportunities for educators and researchers, emphasizing the need to tailor educational strategies to various learning environments and address the unique requirements of different learner demographics.</p>
</div>
</section>
<section id="S4.SS1.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Data Collection Mediums</h4>

<figure id="S4.F4" class="ltx_figure ltx_align_floatright"><img src="/html/2408.14491/assets/img/mediums2.png" id="S4.F4.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="211" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.3.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>. </span><span id="S4.F4.4.2" class="ltx_text" style="font-size:90%;">Data collection mediums distribution. The x-axis refers to the number of corpus papers.</span></figcaption>
</figure>
<div id="S4.SS1.SSS3.p1" class="ltx_para">
<p id="S4.SS1.SSS3.p1.1" class="ltx_p">Figure <a href="#S4.F4" title="Figure 4 ‣ 4.1.3. Data Collection Mediums ‣ 4.1. Environments ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> presents the distribution of the various data collection mediums used by the papers in our literature corpus. As depicted, the current state-of-the-art in data collection mediums reflects a diverse array of technologies and methodologies, with video leading (61/73; 84%) <cite class="ltx_cite ltx_citemacro_citep">(Worsley, <a href="#bib.bib158" title="" class="ltx_ref">2018</a>; Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite>, followed by audio (37/73; 51%) <cite class="ltx_cite ltx_citemacro_citep">(Vrzakova et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020</a>; Chen, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>. These two mediums indicate a preference for rich multimedia data that can capture the complexities of learning and training, as well as interactions within the environments. Logs (33/73; 45%) <cite class="ltx_cite ltx_citemacro_citep">(Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>; Olsen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2020</a>)</cite> and participant-produced artifacts (30/73; 41%) <cite class="ltx_cite ltx_citemacro_citep">(Ashwin and Guddeti, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>; Larmuseau et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020</a>)</cite> are also popular, suggesting a strong inclination toward capturing learner behaviors and outputs directly from both the environments and the participants themselves.</p>
</div>
<div id="S4.SS1.SSS3.p2" class="ltx_para">
<p id="S4.SS1.SSS3.p2.1" class="ltx_p">Despite these advances, the field faces challenges in integrating data from disparate sources and ensuring data quality and privacy. For instance, sensor data (20/73; 27%) <cite class="ltx_cite ltx_citemacro_citep">(Tisza et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2022</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2018a</a>)</cite> presents challenges in standardization and interpretation. Although less prevalent, eye-tracking and motion capture data raise concerns about intrusiveness and the need for sophisticated analysis techniques. There is also a notable gap in text-based data collection (only one paper <cite class="ltx_cite ltx_citemacro_citep">(Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite> in the corpus), as learning and training environment research currently relies primarily on transcribed speech.</p>
</div>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Multimodal Data</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2. Multimodal Data ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> breaks down the different modalities used in our corpus.</p>
</div>
<figure id="S4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.14491/assets/img/modalities_counts_in2.png" id="S4.F5.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="468" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S4.F5.sf1.3.2" class="ltx_text" style="font-size:90%;">Frequency counts for the number of papers in our corpus containing each modality. </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2408.14491/assets/img/count_papers_number_modalities.png" id="S4.F5.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="471" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.sf2.4.2.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S4.F5.sf2.2.1" class="ltx_text" style="font-size:90%;">The number of modalities used per paper, i.e., how many papers (y-axis) used <math id="S4.F5.sf2.2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.F5.sf2.2.1.m1.1b"><mi id="S4.F5.sf2.2.1.m1.1.1" xref="S4.F5.sf2.2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.F5.sf2.2.1.m1.1c"><ci id="S4.F5.sf2.2.1.m1.1.1.cmml" xref="S4.F5.sf2.2.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.sf2.2.1.m1.1d">n</annotation></semantics></math> modalities (x-axis).</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>. </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">A breakdown of the individual modalities used in our corpus both in terms of frequency count (left) and the number of modalities used per paper (right).</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="S4.F5.4" class="ltx_ERROR ltx_centering ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="S4.F5.5" class="ltx_p ltx_figure_panel ltx_align_center">[Individual modalities used in the corpus]Individual modalities used in the corpus</p>
</div>
</div>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">”Pose” is the most prevalent modality, appearing in some form in roughly 45% of papers (33/73) <cite class="ltx_cite ltx_citemacro_citep">(Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2018</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>; Martinez-Maldonado et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2020</a>)</cite>. Logs, affect, gaze, and prosodic speech modalities are also common. At least one of the top five modalities appears in all but eight papers in our corpus (65/73; 89%). The remaining modalities appear less frequently, with raw text, raw pixel value, and audio spectrogram only appearing in one paper each. A large majority of papers (60/73; 82%) use 2-5 modalities in their multimodal analyses. One paper used only a single modality<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>By our definition of ”multimodal” in Section <a href="#S1.SS3" title="1.3. Scope of This Review ‣ 1. Introduction and Background ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1.3</span></a>, we consider a paper to be multimodal if multiple modalities are used during analysis <span id="footnote1.1" class="ltx_text ltx_font_italic">or</span> multiple data collection mediums are used. One paper <cite class="ltx_cite ltx_citemacro_citep">(Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> collected both video and audio data, from which the authors derived a single modality: researcher-produced artifacts. For this reason, there is one paper in our corpus that uses only one modality in its analysis pipeline, still adhering to our multimodal definition.</span></span></span>, and one paper used 10 modalities. We hypothesize that researchers typically choose between 2-5 as a compromise between overhead and informativeness, but more research is required to evaluate this quantitatively.</p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p">Diving deeper into the multimodal data, we identified five modality groups that best characterize the types of data driving multimodal learning and training methods: natural language, vision, sensors, human-centered, and logs. The following subsections present our findings with respect to each modality group. For each modality group, we identify the individual modalities it comprises and discuss our findings with respect to its prevalence in the corpus, current state-of-the-art, challenges faced, research gaps, and results achieved.</p>
</div>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Natural Language</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">35 out of the 73 (48%) corpus papers collected and analyzed some form of natural language data. The natural language modality group comprises prosodic speech (24/73; 33%), transcribed speech (16/73; 22%), raw text (1/73), audio spectogram (1/73), and affect (when derived from text or audio; 2/73). All but three natural language papers included prosodic or transcribed speech, but only eight papers incorporated both. Because prosodic speech is devoid of semantic meaning, and transcribed speech lacks important prosodic information, combining the two provides a more holistic language representation. However, research combining the two modalities was not well-represented in our corpus and represents a notable research gap.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">Traditional machine learning methods were the most prevalent quantitative approaches in the natural language modality group. In particular, support vector machines <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2022</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2017b</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>)</cite> and logistic regression models <cite class="ltx_cite ltx_citemacro_citep">(Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2017a</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>; Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite> were often used with natural language features. Other approaches like random forest <cite class="ltx_cite ltx_citemacro_citep">(Nasir et al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2021</a>)</cite>, linear regression <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>)</cite>, and naive Bayes <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>)</cite> were also used, typically to predict outcomes such as learning or training gains. There was a noticeable lack of deep learning approaches for natural language processing (NLP) in our corpus. While some papers incorporated recurrent neural networks (e.g., LSTM models <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a href="#bib.bib71" title="" class="ltx_ref">1997</a>)</cite>), these were a relative rarity. Very few used transformer <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a href="#bib.bib151" title="" class="ltx_ref">2017</a>)</cite> models like BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2018</a>)</cite>, which was surprising given their prevalence in contemporary NLP. This indicates that the multimodal methods for learning and training environments using natural language lag behind the current state-of-the-art in NLP <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a href="#bib.bib150" title="" class="ltx_ref">2023</a>)</cite>. However, this is likely in large part due to the small sample sizes and noisy data innate to learning and training environments that are insufficient to train many deep learning models, which we discuss in Section <a href="#S6" title="6. Discussion ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div id="S4.SS2.SSS1.p3" class="ltx_para">
<p id="S4.SS2.SSS1.p3.1" class="ltx_p">Education- and training-specific datasets are often small, imbalanced, and contain domain-specific terminology that language models may not have encountered frequently during training <cite class="ltx_cite ltx_citemacro_citep">(Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2017a</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>; Cochran et al<span class="ltx_text">.</span>, <a href="#bib.bib31" title="" class="ltx_ref">2023c</a>, <a href="#bib.bib30" title="" class="ltx_ref">a</a>)</cite>. These issues complicate the effective training of deep learning models <cite class="ltx_cite ltx_citemacro_citep">(Cohn, <a href="#bib.bib33" title="" class="ltx_ref">2020</a>; Cochran et al<span class="ltx_text">.</span>, <a href="#bib.bib29" title="" class="ltx_ref">2023b</a>; Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2017a</a>; Standen et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>; Aslan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>. Additional challenges include the complexity and time cost of cleaning, processing, and labeling data. Software packages like NLTK <cite class="ltx_cite ltx_citemacro_citep">(Loper and Bird, <a href="#bib.bib89" title="" class="ltx_ref">2002</a>)</cite>, openSMILE <cite class="ltx_cite ltx_citemacro_citep">(Eyben et al<span class="ltx_text">.</span>, <a href="#bib.bib57" title="" class="ltx_ref">2010</a>)</cite>, and TAACO <cite class="ltx_cite ltx_citemacro_citep">(Crossley et al<span class="ltx_text">.</span>, <a href="#bib.bib42" title="" class="ltx_ref">2016</a>, <a href="#bib.bib41" title="" class="ltx_ref">2019</a>)</cite> facilitate the programmatic extraction of audio- and text-based features, yet this can result in large, opaque feature sets <cite class="ltx_cite ltx_citemacro_citep">(Prieto et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2018</a>)</cite>. Conversely, manual preprocessing and feature engineering can be time-intensive, potentially limiting the data researchers are willing to collect and analyze <cite class="ltx_cite ltx_citemacro_citep">(Kubsch et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>)</cite>. This helps explain why qualitative analysis of smaller sample sizes is common in natural language studies.</p>
</div>
<div id="S4.SS2.SSS1.p4" class="ltx_para">
<p id="S4.SS2.SSS1.p4.1" class="ltx_p">Qualitative analyses using natural language primarily involve presenting descriptive statistics, case studies, and researchers’ observations, and conducting various forms of qualitative coding <cite class="ltx_cite ltx_citemacro_citep">(Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2018</a>; Martin et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2019</a>; Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite>. Many natural language studies focus on collaborative learning and training <cite class="ltx_cite ltx_citemacro_citep">(Olsen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2020</a>; López et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2021</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>)</cite>, favoring multi-person environments to leverage the richness of collaborative discourse. However, analyzing transcribed speech poses challenges. Several studies noted that automatic speech recognition (ASR) is a bottleneck in multimodal pipelines using transcribed speech <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>; Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>; Kubsch et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite>. Learning environments often consist of multiple groups participating simultaneously, creating noisy conditions that hinder ASR accuracy, particularly in K-12 settings <cite class="ltx_cite ltx_citemacro_citep">(Kubsch et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite> and among non-English speakers <cite class="ltx_cite ltx_citemacro_citep">(Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS2.SSS1.p5" class="ltx_para">
<p id="S4.SS2.SSS1.p5.1" class="ltx_p">Only one paper in the corpus used raw text as input <cite class="ltx_cite ltx_citemacro_citep">(Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite>, which is surprising given the prevalence of text-based transformer models <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al<span class="ltx_text">.</span>, <a href="#bib.bib48" title="" class="ltx_ref">2018</a>; Radford et al<span class="ltx_text">.</span>, <a href="#bib.bib123" title="" class="ltx_ref">2019</a>; Brown et al<span class="ltx_text">.</span>, <a href="#bib.bib18" title="" class="ltx_ref">2020</a>)</cite>. Considering the capabilities of large language models (LLMs), text-based features could significantly enhance multimodal learning and training pipelines, as raw text quality does not depend on ASR. One potential avenue for leveraging textual features is through conversational agents, which were notably absent in our corpus. While several works addressed multimodal agents or tutors <cite class="ltx_cite ltx_citemacro_citep">(Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>; Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>, these agents typically provided summative performance metrics or canned responses. No studies addressed conversational agents that engage dynamically with learners as peers, mentors, or collaborators.</p>
</div>
<div id="S4.SS2.SSS1.p6" class="ltx_para">
<p id="S4.SS2.SSS1.p6.1" class="ltx_p">Despite these gaps and challenges, natural language features consistently produced positive outcomes. Researchers successfully correlated and predicted various learning outcomes using these features. This was especially evident in studies focusing on collaborative learning and training, where the collaborative environments provided discourse rich in natural language features. Collaboration was examined both as an independent and dependent variable <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>; Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>)</cite>. In collaborative settings, natural language features frequently were the most informative among all modalities <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>)</cite>. Additionally, natural language features were usually the most predictive when combined with features derived from other modalities. This reinforces previous work, where multimodal data harnessed more predictive power than any individual modality <cite class="ltx_cite ltx_citemacro_citep">(Sharma and Giannakos, <a href="#bib.bib132" title="" class="ltx_ref">2020</a>)</cite>. Researchers often reported that including natural language features in the multimodal pipeline led to improved predictive performance <cite class="ltx_cite ltx_citemacro_citep">(Olsen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2020</a>)</cite>. Overall, the results reported in our corpus clearly indicate that natural language features have: 1) high correlations with performance outcomes, and 2) provide enhanced predictive capabilities when combined with features derived from other modalities.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Vision</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">Among the five groups of modalities analyzed, vision-based modalities were the most utilized, appearing in 59 out of 73 papers (81%). The vision modality group includes papers that collected data using cameras or eye-tracking devices and analyzed it for pose recognition, affect detection, gesture recognition, activity recognition, fatigue estimation, participant gaze, or raw image pixel data. Pose, affect, and gaze were the most common, present in 33 (56%), 25 (42%), and 27 (46%) of the 59 papers, respectively. Gesture recognition appeared in 16 papers (27%), activity recognition in 11 papers (19%), and fatigue estimation and raw pixel data in 2 and 1 papers, respectively.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">This distribution is expected. Pose recognition was the most frequent due to the availability of off-the-shelf deep learning models and the use of Microsoft Kinect cameras, which facilitate pose data collection. Gaze tracking was common with specialized hardware like eye-tracking glasses. Affect recognition was also prevalent, again supported by off-the-shelf models. Notably, raw pixel data was the least used, appearing in only one paper. Researchers typically processed raw images using other models before analysis, highlighting the importance of mid-fusion techniques. This pattern reveals a mismatch between core and applied computer vision research, with the latter relying on pre-trained models due to smaller datasets.</p>
</div>
<div id="S4.SS2.SSS2.p3" class="ltx_para">
<p id="S4.SS2.SSS2.p3.1" class="ltx_p">In terms of analysis methods, there was a slight preference for quantitative techniques in the vision subset, with 69% of papers using model-based methods compared to 63% in the full corpus. Despite this, many papers combining qualitative and quantitative analysis also used vision data. Only 24% of the vision papers employed mixed-methods analysis, often combining classification with the qualitative analysis of classes.</p>
</div>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3. </span>Sensors</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">We identified 20 papers (27%) in sensor-based learning and training research, covering various physiological and behavioral data modalities. These papers focused on affective responses (11/73; 15%), body pose analysis (7/73; 10%), electrodermal activity (16/73; 22%), pulse rate (11/73; 15%), activity (5/73; 7%), blood pressure (4/73; 5%), temperature (8/73; 11%), electroencephalography (3/73; 4%), electromyography (2/73; 3%), fatigue (2/73; 3%), and gaze tracking (8/73; 11%).</p>
</div>
<div id="S4.SS2.SSS3.p2" class="ltx_para">
<p id="S4.SS2.SSS3.p2.1" class="ltx_p">Of these 20 papers, 12 were learning-based, and 8 were training-based. This suggests sensors are more frequently used in training-based research, which represents only 22% (16/73) of the full corpus but 40% of papers using sensors. Within MMLA, wearable sensors monitor learners’ emotional and physiological states, predict behavior and performance, provide real-time feedback, and enable multimodal data integration <cite class="ltx_cite ltx_citemacro_citep">(Tisza et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2022</a>; Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>; Henderson et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite>. Sensor use ranges from classroom environments to specialized training scenarios (e.g., CPR instruction <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>), serving as assessment tools and mechanisms for real-time educational interventions. However, integrating and interpreting sensor data presents challenges, particularly for accurate and practical real-time applications <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2022</a>; Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S4.SS2.SSS3.p3" class="ltx_para">
<p id="S4.SS2.SSS3.p3.1" class="ltx_p">The state-of-the-art in sensor-driven multimodal learning and training analytics features advanced predictive modeling, real-time feedback systems, and multimodal data fusion. However, there is a need for more granular data analysis to identify subtle patterns and correlations not apparent through traditional methods. Contextual and behavioral analytics link physiological responses to specific learning activities in real-time. Signal processing methods aggregate sensory information into physical or learning characteristics, such as relative learning gains <cite class="ltx_cite ltx_citemacro_citep">(Tisza et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2022</a>)</cite>, team dynamics <cite class="ltx_cite ltx_citemacro_citep">(Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite>, and shared physiological arousal <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2023</a>)</cite>. The field also requires robust, interactive visualizations that convey complex sensory data intuitively, and Explainable AI (XAI) methods to clarify how sensor data contributes to predictive models, enhancing interpretability <cite class="ltx_cite ltx_citemacro_citep">(Schoonderwoerd et al<span class="ltx_text">.</span>, <a href="#bib.bib128" title="" class="ltx_ref">2021</a>; Rojat et al<span class="ltx_text">.</span>, <a href="#bib.bib126" title="" class="ltx_ref">2021</a>)</cite>.</p>
</div>
<div id="S4.SS2.SSS3.p4" class="ltx_para">
<p id="S4.SS2.SSS3.p4.1" class="ltx_p">There is a noticeable gap in longitudinal studies to assess the sustained impacts of sensor-based technologies. Expanding sensor research to diverse learning contexts and demographics will help us understand its broader applications. Sensor research often occurs in controlled environments, so scaling for widespread use and ensuring generalizability across diverse settings remains challenging. One example is Echeverria et al.’s study <cite class="ltx_cite ltx_citemacro_citep">(Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite> using accelerometer data in nurse training simulations, which could benefit from integrating additional sensory inputs (e.g., gyroscope) to conduct multidimensional analyses. Investigating user experience and acceptance of wearable technologies in education, particularly regarding comfort, usability, perceived effectiveness, and privacy, is also needed.</p>
</div>
</section>
<section id="S4.SS2.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.4. </span>Human-Centered</h4>

<div id="S4.SS2.SSS4.p1" class="ltx_para">
<p id="S4.SS2.SSS4.p1.1" class="ltx_p">Human-centered modalities (qualitative observation, interview, survey, researcher-produced artifacts, and participant-produced artifacts) offer insights into participants’ experiences, perceptions, and behaviors, often identifying nuances that quantitative analyses overlook. Out of 73 papers, 45 (62%) incorporate at least one human-centered modality, indicating a strong focus on human experiences. Participant-produced artifacts are the most common (19/73, 26%), followed by qualitative observation (14/73, 19%), researcher-produced artifacts (14/73, 16%), and both interview notes and survey responses (10/73, 14%). Participant artifacts often include diverse materials, with pre- and post-tests being the most prevalent for calculating learning gains <cite class="ltx_cite ltx_citemacro_citep">(Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2020a</a>; Reilly et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2018</a>; Worsley, <a href="#bib.bib158" title="" class="ltx_ref">2018</a>)</cite>. The considerable use of qualitative observations highlights the importance of insights gained through direct human interpretation of behaviors. Common combinations include qualitative observations and participant artifacts <cite class="ltx_cite ltx_citemacro_citep">(Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>; Worsley, <a href="#bib.bib158" title="" class="ltx_ref">2018</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>)</cite>, participant artifacts and researcher artifacts <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>; Starr et al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2018</a>; Reilly et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2018</a>; Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>, and interview notes and qualitative observations <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021</a>; Noël et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2022</a>; Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>; Aslan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>. One study applied clustering, NLP, and linear modeling to researcher artifacts detailing student behaviors <cite class="ltx_cite ltx_citemacro_citep">(Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S4.SS2.SSS4.p2" class="ltx_para">
<p id="S4.SS2.SSS4.p2.1" class="ltx_p">A predominant strategy involves transforming human-centered modalities into quantifiable data for statistical analysis. Examples include López et al. using survey data <cite class="ltx_cite ltx_citemacro_citep">(López et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2021</a>)</cite>, Ochoa and Dominguez using participant-produced artifacts <cite class="ltx_cite ltx_citemacro_citep">(Ochoa and Dominguez, <a href="#bib.bib107" title="" class="ltx_ref">2020</a>)</cite>, and Bert et al. using both participant-produced artifacts and interview transcriptions <cite class="ltx_cite ltx_citemacro_citep">(Birt et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>. This shows a preference for quantifiable insights from human-centered modalities. Fourteen papers focus on qualitative analysis, emphasizing rich, qualitative insights. Most papers adopt multiple analysis methods, with only 16/45 using one method exclusively, 15/45 integrating two methods, and 13/45 using three. Worsley and Blikstein <cite class="ltx_cite ltx_citemacro_citep">(Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>)</cite> employ four analysis methods to identify correlations between multimodal data, experimental condition, design quality, and learning, using both human-annotated and automatically annotated data.</p>
</div>
<div id="S4.SS2.SSS4.p3" class="ltx_para">
<p id="S4.SS2.SSS4.p3.1" class="ltx_p">Human-centered approaches pose challenges related to subjectivity, scalability, resource intensiveness, and generalizability. The subjectivity of human-centered modalities introduces biases <cite class="ltx_cite ltx_citemacro_citep">(Mehra, <a href="#bib.bib98" title="" class="ltx_ref">2015</a>; Noble and Smith, <a href="#bib.bib104" title="" class="ltx_ref">2015</a>)</cite>. These approaches are resource-intensive, requiring trained researchers for data collection, coding, and analysis. Manual collection and analysis can be time-consuming and often does not scale well, especially in large-scale educational settings. Despite these challenges, human-centered approaches offer transparent and interpretable insights. These insights highlight gaps in integrating qualitative and quantitative methods. Developing methodologies that combine qualitative nuance with quantitative rigor is essential. The lack of standardized coding practices for human-centered modalities hampers replicability and comparability. Establishing standardized coding frameworks is crucial to enhance the reliability and credibility of machine learning analyses. Additionally, automating human-coding processes is a vital research need.</p>
</div>
</section>
<section id="S4.SS2.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.5. </span>Logs</h4>

<div id="S4.SS2.SSS5.p1" class="ltx_para">
<p id="S4.SS2.SSS5.p1.1" class="ltx_p">Thirty papers (40%) incorporated log data (log-analysis papers). Logs, often from computer-based environments, link complementary modalities to learning outcomes and behaviors. Logs are frequently combined with video (25/30, 83%), eye-tracking (12/30, 40%), audio (12/30, 40%), participant-produced artifacts (11/30, 36%), survey responses (6/30, 27%), sensors (8/30, 26%), and motion (3/30, 10%). This highlights the diverse ways environmental logs are contextualized. Human-centered artifacts were less commonly combined with log data. Overall, log-analysis papers focus on computer-based learning environments and individualized instructional or informal activities.</p>
</div>
<div id="S4.SS2.SSS5.p2" class="ltx_para">
<p id="S4.SS2.SSS5.p2.1" class="ltx_p">The state-of-the-art in log-analysis features various approaches. Nearly all classification and regression papers used machine learning algorithms, such as support vector machine, random forest, naive Bayes, and logistic regression <cite class="ltx_cite ltx_citemacro_citep">(Mangaroska et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2020</a>; Fwa, Hua Leong and Lindsay Marshall, <a href="#bib.bib63" title="" class="ltx_ref">2018</a>; Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2020</a>)</cite>, to predict students’ achievement, engagement, or emotional state. Deep learning approaches like CNNs <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>)</cite> and LSTMs <cite class="ltx_cite ltx_citemacro_citep">(Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>; Olsen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2020</a>)</cite> were used in only three papers. Statistical methods were used to correlate learning variables (e.g., perceived student emotion) to outcome variables (e.g., learning gains).</p>
</div>
<div id="S4.SS2.SSS5.p3" class="ltx_para">
<p id="S4.SS2.SSS5.p3.1" class="ltx_p">Analyzing logs presents hurdles, including time-cost, data scarcity, generalizability, and engineering expenses. Temporal aspects introduce difficulties, such as aligning time frames, handling different sampling rates, and managing time-series data. These complexities often result in smaller datasets, limiting scope and scalability. Data scarcity exacerbates the challenge of producing generalizable findings, while high software development and engineering costs hinder integrating modern features like real-time collaboration tools.</p>
</div>
<div id="S4.SS2.SSS5.p4" class="ltx_para">
<p id="S4.SS2.SSS5.p4.1" class="ltx_p">These challenges create gaps in log-analysis research. There is a deficiency in applying methods and findings from one educational setting to another, likely due to diverse educational contexts. Embracing standardized log formats and consistent practices would help overcome this barrier, leading to more unified research approaches and broader applicability of insights. The low adoption rate of industry standards like xAPI <cite class="ltx_cite ltx_citemacro_citep">(Software, <a href="#bib.bib136" title="" class="ltx_ref">2024</a>)</cite>, LTI <cite class="ltx_cite ltx_citemacro_citep">(1EdTech, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite>, and Learning Management Systems (LMS) in educational technology research reflects a broader issue of aligning with best practices. Addressing these gaps and embracing these standards could enhance interoperability, scalability, and more robust analysis of educational data, paving the way for more impactful and transformative educational research and practices.</p>
</div>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Data Fusion</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">The choice between different types of fusion depends on the characteristics of the data, the nature of the environmental task, and the desired level of integration, and we observed multiple approaches to data fusion in our corpus. Each fusion strategy has strengths and limitations, and researchers often select the most suitable approach based on the specific requirements of their study and research goals. One noteworthy observation in this corpus is that several papers do not explicitly explain or justify their fusion choices.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2408.14491/assets/img/fusion_dist_pie_chart_donut_16.png" id="S4.F6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="269" height="187" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.3.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>. </span><span id="S4.F6.4.2" class="ltx_text" style="font-size:90%;">Distribution of Fusion Types</span></figcaption>
</figure>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">Figure <a href="#S4.F6" title="Figure 6 ‣ 4.3. Data Fusion ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the distribution of fusion types across the 73 papers in the corpus. 54 (74%) perform early, mid, late, or hybrid fusion. The distribution of fusion types reveals that mid fusion is the most prevalent (27/73; 37%), showcasing its popularity for integrating modalities by combining derived, observable features. Hybrid fusion follows closely with 19 papers (26%), utilizing a combination of early, mid and/or late fusion strategies. Early fusion is observed only in 3 papers, while late fusion is employed in 8 papers. 20 papers (27%) adopt other types of fusion strategies, no fusion, or do not explicitly mention data fusion. For reference, Figure <a href="#S2.F3" title="Figure 3 ‣ 2.2.5. Data Fusion ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates the differences between fusion types.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1. </span>Early Fusion</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">In early fusion, the joint feature representation incorporates information from all fused modalities, enabling the model to learn relationships and patterns directly from the raw, integrated features. This approach is advantageous when the modalities offer complementary information. In our corpus, early fusion was utilized in less than 5% of the papers and is not always suitable, as it is not always clear what features are the most important until after processing and analyzing them. Further, early fusion is often computationally prohibitive, as the dimensionality of raw data is typically higher than that of its processed output.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2. </span>Mid Fusion</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Mid fusion combines features derived after prior processing but within the observable input space. It is advantageous when individual modalities require unique processing and combining feature-level decisions is more effective than integrating raw features. 27/73 papers (37%) used mid-fusion, suggesting mid fusion is more suitable for addressing the challenges and objectives of multimodal learning and training relative to early fusion.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3. </span>Late Fusion</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">In late fusion, modalities are processed independently until the hypothesis (decision) space, where their outputs are aggregated to make overall inferences. This approach is suitable when modalities are semantically more independent, and their contributions are better understood when combined at a later stage. In our corpus, 8 papers (11%) employed late fusion, with 3 of them also employing other types of fusion <cite class="ltx_cite ltx_citemacro_citep">(Sümer et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2023</a>; Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2021a</a>, <a href="#bib.bib22" title="" class="ltx_ref">b</a>)</cite>. Most papers used late fusion for classification purposes (one used regression <cite class="ltx_cite ltx_citemacro_citep">(Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite>).</p>
</div>
</section>
<section id="S4.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.4. </span>Hybrid Fusion</h4>

<div id="S4.SS3.SSS4.p1" class="ltx_para">
<p id="S4.SS3.SSS4.p1.1" class="ltx_p">Hybrid fusion integrates information at different stages of the analysis pipeline, and its design varies based on the learning or training task, analysis goals, and data characteristics. Hybrid fusion was employed in 19 out of the 54 (35%) papers that performed fusion, highlighting the significance of this approach. Most papers (14/19; 74%) incorporated at least 4 modalities. Classification was the predominant analysis method (15/19; 79%).</p>
</div>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4. </span>Analysis</h3>

<figure id="S4.F7" class="ltx_figure"><img src="/html/2408.14491/assets/img/model_based_model_free_pie_chart.png" id="S4.F7.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="150" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F7.3.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>. </span><span id="S4.F7.4.2" class="ltx_text" style="font-size:90%;">Analysis approaches percentage distribution.</span></figcaption>
</figure>
<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">We defined analysis approaches as either model-based or model-free (see Section <a href="#S2.SS2.SSS11" title="2.2.11. Analysis Approach ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.11</span></a>), depending on each paper’s data, research questions, and analysis methods. Model-based methods rely on assumptions about data and system operations, while model-free methods demand careful attention to data quality and reliability. While these methodologies differ and are often associated with distinct research communities, they are best used together to complement each other’s strengths and weaknesses.</p>
</div>
<div id="S4.SS4.p2" class="ltx_para">
<p id="S4.SS4.p2.1" class="ltx_p">As shown in Figure <a href="#S4.F7" title="Figure 7 ‣ 4.4. Analysis ‣ 4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, 46 corpus papers (63%) used model-based methods, 16 papers (22%) employed model-free methods, and 11 papers (15%) opted for both. This distribution, with 78% (57/73) of papers employing model-based analysis, indicates a strong preference for developing models to inform analysis processes. Conversely, model-free approaches, which make up 37% (27/73) of papers, offer a valuable alternative for investigating learning and training outcomes in a more exploratory manner.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1. </span>Model-Based</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">Model-based methodologies, such as machine learning models, employ mathematical frameworks to generate results from given inputs. Among papers using only model-based approaches, common analysis methods include classification (34/46; 74%), statistical analysis (17/46; 37%), regression (8/46; 17%), and clustering (7/46; 15%). These methods train models using data samples to predict output variables (e.g., learning outcomes). When qualitative and pattern recognition techniques use model outputs to guide their analysis, they are also considered model-based. A notable aspect of model-based approaches is their focus on individual experiences (31/46; 67%) over collaborative ones (17/46; 37%), likely due to the complexities of mathematically representing intricate social interactions in group settings. Modeling an individual’s cognitive, behavioral, and emotional states is challenging; thus, accurately reflecting collaborative dynamics in models is mostly confined to a niche within MMLA and social network analysis.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2. </span>Model-Free</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">Model-free methods adopt a comprehensive, exploratory strategy, focusing on relationships between variables without assuming a specific link between input and output. Predominantly, these involve qualitative (11/16; 69%), statistical (9/16; 56%), and pattern recognition (3/16; 19%) methods. Qualitative methods are used in scenarios like use case and interaction analysis, where observations and learning theories guide the understanding of learning processes. Statistical and pattern recognition methods provide descriptions and correlation metrics between learning activities (e.g., behaviors and strategies) and outcomes. Serving as a counterbalance to the limitations of model-based methods, model-free approaches are widely used in collaborative settings. They are instrumental in dissecting social signals and provide insights into the dynamics of collaboration, including group health and communication.</p>
</div>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5. </span>Feedback</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">This review focuses on MMLA analysis methods, with feedback being a significant yet secondary aspect of the MMLA framework. Feedback in multimodal learning analytics is a bidirectional process essential for completing the analysis cycle, categorized as either <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_italic">direct</span> or <span id="S4.SS5.p1.1.2" class="ltx_text ltx_font_italic">indirect</span>. Direct feedback involves learners or system users and aims to enhance user performance or other metrics. Indirect feedback represents feedback not intended for the end user (e.g., feedback that improves system design).</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Direct feedback can take two forms. One form is the prototypical feedback in the context of a learning or training environment for
improving the user’s performance. Although an exhaustive review of direct feedback literature is outside this paper’s scope, seminal works by Hattie &amp; Timperley <cite class="ltx_cite ltx_citemacro_citep">(Hattie and Timperley, <a href="#bib.bib69" title="" class="ltx_ref">2007</a>)</cite> and Adarkwah <cite class="ltx_cite ltx_citemacro_citep">(Adarkwah, <a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite> provide foundational insights. Users also contribute to MMLA in many forms by offering feedback, integral to user-centered design <cite class="ltx_cite ltx_citemacro_citep">(Abras et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2004</a>)</cite>. Conversely, indirect feedback does not involve the end user but informs system improvement or research findings. It arises from observing user-system interactions or studying learner behavior, leading to enhanced system design or theoretical understanding. Improved research conclusions occur when the study of learners and trainees in these environments leads to new understandings of the subjects and their populations. Such feedback is vital for advancing MMLA research.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Archetypes</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Following the analysis in Section <a href="#S4" title="4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we reexamined our corpus to classify prevailing research objectives in applying multimodal methods to learning and training environments. We identified three primary research objectives, termed <span id="S5.p1.1.1" class="ltx_text ltx_font_italic">archetypes</span>: <span id="S5.p1.1.2" class="ltx_text ltx_font_italic">Designing and Developing Methods</span>, <span id="S5.p1.1.3" class="ltx_text ltx_font_italic">Analyzing Outcomes</span>, and <span id="S5.p1.1.4" class="ltx_text ltx_font_italic">Exploring Behaviors</span>. These archetypes, detailed in subsequent subsections, often overlap within studies; for example, method development research may also yield insights into participant behaviors and outcomes. While these archetypes broadly define the field, they are not exhaustive, and some studies may not align precisely with these categories.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Designing and Developing Methods</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">The Designing and Developing Methods archetype encompasses studies that focus on designing, presenting, and evaluating multimodal research methods that can be applied to learning and training environments. These studies prioritize methodological innovation over the derivation of generalizable findings about a population. Although the developed methods often aim to predict outcomes (Section <a href="#S5.SS2" title="5.2. Analyzing Outcomes ‣ 5. Archetypes ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.2</span></a>) and discern behaviors (Section <a href="#S5.SS3" title="5.3. Exploring Behaviors ‣ 5. Archetypes ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5.3</span></a>), the primary focus remains on the method itself, not the implications of its findings on the study participants. These methods are typically quantitative, utilizing supervised learning techniques such as classification <cite class="ltx_cite ltx_citemacro_citep">(Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2020a</a>; Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>; Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>)</cite> and regression <cite class="ltx_cite ltx_citemacro_citep">(Olsen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2020</a>; Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>; Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite>, and their efficacy is reported through performance metrics like F1-score <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2020</a>; Azcona et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>; Alyuz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>. Data collection often involves video, audio, and log data <cite class="ltx_cite ltx_citemacro_citep">(Standen et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>; López et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2021</a>)</cite>; targeting modalities such as affect, pose, prosodic speech, and logs <cite class="ltx_cite ltx_citemacro_citep">(Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>; Standen et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>; Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>)</cite>; employing data fusion techniques like mid or hybrid fusion <cite class="ltx_cite ltx_citemacro_citep">(Capital Normal University, Beijing, China et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>; Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite> using model-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>; Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>; Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Our corpus reveals a broad spectrum of tasks addressed by Designing and Developing Methods research, ranging from personalized feedback in CPR training <cite class="ltx_cite ltx_citemacro_citep">(Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>)</cite> to engagement detection in educational games <cite class="ltx_cite ltx_citemacro_citep">(Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>)</cite>, and skill classification in sports <cite class="ltx_cite ltx_citemacro_citep">(Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite>. The versatility of multimodal methods is evident in the diverse settings, domains, instructional levels, and didactic approaches, without a dominant trend in any specific area.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">However, a notable gap in the corpus is the limited focus on evaluating the impact of these methods on end users (stakeholders) and the lack of stakeholder involvement in the method development process. While methods for tasks like feedback generation <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2022</a>; Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2018</a>; Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite> and engagement detection <cite class="ltx_cite ltx_citemacro_citep">(Alyuz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>; Capital Normal University, Beijing, China et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>)</cite> are presented, their practical effectiveness in enhancing learning outcomes and engagement is seldom empirically validated. Furthermore, the integration of stakeholder feedback into the development of methods is rare, which can lead to a disconnect between the objectives of researchers and the needs of practitioners <cite class="ltx_cite ltx_citemacro_citep">(Boser and McDaniels, <a href="#bib.bib15" title="" class="ltx_ref">2018</a>)</cite>. This aspect will be further discussed in Section <a href="#S6" title="6. Discussion ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. Although some studies in our corpus do consider stakeholder impact <cite class="ltx_cite ltx_citemacro_citep">(Aslan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>; Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2018</a>)</cite>, such instances are infrequent and not representative of the corpus as a whole.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Analyzing Outcomes</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">The Analyzing Outcomes archetype focuses on specific outcome metrics, such as learning gains, engagement levels, and accuracy rates. The goal is to uncover findings that apply to broader populations, distinguishing it from the Designing and Developing Methods archetype, which focuses on refining analytical techniques. Outcome analysis typically employs supervised learning methods like classification <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>; Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2020</a>; Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>)</cite> and regression <cite class="ltx_cite ltx_citemacro_citep">(Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>; Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>; Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>)</cite>, along with insights from model behaviors, statistical patterns, and unsupervised methods <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>; Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>; Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Outcome analysis has been applied across various learning and training contexts, focusing on constructs like attention and engagement <cite class="ltx_cite ltx_citemacro_citep">(Sümer et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2023</a>; Aslan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>; Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>)</cite>, task performance and accuracy <cite class="ltx_cite ltx_citemacro_citep">(Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>; Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>; Azcona et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>, learning outcomes <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2022</a>; Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>; Tisza et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2022</a>)</cite>, and collaborative outcomes <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2022</a>; Worsley, <a href="#bib.bib158" title="" class="ltx_ref">2018</a>; Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>)</cite>. Despite diverse environments, common outcome variables provide generalizable insights. However, this archetype has limitations. Focusing on outcome variables often overlooks the complexities of learning processes, risking interventions tailored to high-performing learners and neglecting individual differences <cite class="ltx_cite ltx_citemacro_citep">(Fonteles et al<span class="ltx_text">.</span>, <a href="#bib.bib60" title="" class="ltx_ref">2024a</a>)</cite>. Additionally, like the Designing and Developing Methods archetype, these studies often exclude stakeholder perspectives, potentially leading to biased conclusions.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Exploring Behaviors</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The Exploring Behaviors archetype investigates human behavior and experiences in learning and training contexts by employing an exploratory approach to uncover influencing factors. This research examines a variety of human signals that vary temporally, socially, and spatially, and are tailored to specific learning objectives. Unlike other archetypes, it often incorporates qualitative observations <cite class="ltx_cite ltx_citemacro_citep">(Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>; Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2021</a>; Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021</a>; Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2022</a>)</cite>, and employs data exploration techniques like correlation analysis <cite class="ltx_cite ltx_citemacro_citep">(Noel et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2018</a>; López et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2021</a>)</cite> and pattern recognition <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2023</a>; Reilly et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2018</a>; Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>; Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>. Data fusion in this context is typically qualitative <cite class="ltx_cite ltx_citemacro_citep">(Järvelä et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>; Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>; Birt et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>, involving the manual integration of multimodal data sources. This approach enables triangulation of student and trainee behaviors, providing richer context to researchers, statistical analyses, or data visualizations, thereby facilitating deeper insights into the behaviors under study.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Exploring Behaviors research aims to fill knowledge gaps in learning theory and technological applications by investigating human behavior in educational contexts. <cite class="ltx_cite ltx_citemacro_citet">Reilly et al<span class="ltx_text">.</span> (<a href="#bib.bib124" title="" class="ltx_ref">2018</a>)</cite> applied a Markov transition model to assess how students’ physical behaviors during a collaborative programming task correlate with collaboration quality, task performance, and learning gains. <cite class="ltx_cite ltx_citemacro_citet">Noel et al<span class="ltx_text">.</span> (<a href="#bib.bib105" title="" class="ltx_ref">2018</a>)</cite> utilized correlation analysis alongside social network metrics and annotated behaviors to investigate collaborative dynamics in a software engineering course. <cite class="ltx_cite ltx_citemacro_citet">Closser et al<span class="ltx_text">.</span> (<a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite> conducted a qualitative study, using a coding scheme to analyze students’ actions, speech, and gestures in embodied learning activities to understand their conceptualization of measurement. These studies, often grounded in learning theory, employ multimodal learning analytics to dissect the components of effective collaboration, showcasing the nuanced insights that multimodal methods can provide into collaborative learning processes. This research spans various mediums, modalities, and settings, with a discernible focus on collaboration.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Discussion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">Sections <a href="#S4" title="4. Framework Insights ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and <a href="#S5" title="5. Archetypes ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> reveal several trends in multimodal learning and training, including key results, challenges, research gaps, and future research directions. In the following subsections, we discuss each of these and address the limitations of our literature review. Overall, we characterize the current state of the field by presenting several key insights:</p>
</div>
<div id="S6.p2" class="ltx_para">
<ul id="S6.I1" class="ltx_itemize">
<li id="S6.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i1.p1.1" class="ltx_p"><span id="S6.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Environments</span>: Learning environments outnumber training environments 7:2, mostly focusing on STEM environments with a virtual component (virtual or blended).</p>
</div>
</li>
<li id="S6.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i2.p1.1" class="ltx_p"><span id="S6.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Participants</span>: Participants are primarily university or K-12 students, with multi-person environments slightly more common than individual ones (3:2).</p>
</div>
</li>
<li id="S6.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.p1.1" class="ltx_p"><span id="S6.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Data and Modalities</span>:</p>
<ul id="S6.I1.i3.I1" class="ltx_itemize">
<li id="S6.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S6.I1.i3.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S6.I1.i3.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i3.I1.i1.p1.1" class="ltx_p">Video, audio, logs, and participant-produced artifacts are the most common data collection mediums.</p>
</div>
</li>
<li id="S6.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S6.I1.i3.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S6.I1.i3.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i3.I1.i2.p1.1" class="ltx_p">Pose, logs, affect, gaze, and prosodic speech are the most popular modalities.</p>
</div>
</li>
<li id="S6.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S6.I1.i3.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S6.I1.i3.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i3.I1.i3.p1.1" class="ltx_p">Most papers use 2-5 modalities, focusing on vision analysis and human-centered modalities (e.g., artifacts, surveys, and interviews).</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S6.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i4.p1" class="ltx_para">
<p id="S6.I1.i4.p1.1" class="ltx_p"><span id="S6.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">Analysis Methods and Approaches</span>:</p>
<ul id="S6.I1.i4.I1" class="ltx_itemize">
<li id="S6.I1.i4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S6.I1.i4.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S6.I1.i4.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i4.I1.i1.p1.1" class="ltx_p">Classification (for predicting outcomes), statistical analysis (for feature selection and correlation), and qualitative analysis (case studies, coding, and thematic analysis) are the most common analysis methods.</p>
</div>
</li>
<li id="S6.I1.i4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S6.I1.i4.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S6.I1.i4.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i4.I1.i2.p1.1" class="ltx_p">Model-based papers outnumber model-free ones 3:1.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S6.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i5.p1" class="ltx_para">
<p id="S6.I1.i5.p1.1" class="ltx_p"><span id="S6.I1.i5.p1.1.1" class="ltx_text ltx_font_bold">Data Fusion</span>:</p>
<ul id="S6.I1.i5.I1" class="ltx_itemize">
<li id="S6.I1.i5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S6.I1.i5.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S6.I1.i5.I1.i1.p1" class="ltx_para">
<p id="S6.I1.i5.I1.i1.p1.1" class="ltx_p">75% of papers use early, mid, late, or hybrid fusion.</p>
</div>
</li>
<li id="S6.I1.i5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S6.I1.i5.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S6.I1.i5.I1.i2.p1" class="ltx_para">
<p id="S6.I1.i5.I1.i2.p1.1" class="ltx_p">Mid fusion is most prevalent, followed by hybrid fusion.</p>
</div>
</li>
<li id="S6.I1.i5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="S6.I1.i5.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="S6.I1.i5.I1.i3.p1" class="ltx_para">
<p id="S6.I1.i5.I1.i3.p1.1" class="ltx_p">Fused modalities often yield better results than unimodal ones, suggesting researchers should explore data fusion for a holistic understanding of behaviors and outcomes.</p>
</div>
</li>
</ul>
</div>
</li>
<li id="S6.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S6.I1.i6.p1" class="ltx_para">
<p id="S6.I1.i6.p1.1" class="ltx_p"><span id="S6.I1.i6.p1.1.1" class="ltx_text ltx_font_bold">Publication Mediums</span>: The British Journal of Educational Technology (BJET) and International Conference on Learning Analytics &amp; Knowledge (LAK) are the most popular venues for publishing multimodal learning and training research.</p>
</div>
</li>
</ul>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Reported Results</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The results of our corpus’s papers illustrate that multimodal methods are often successful at predicting learning and training outcomes, as well as identifying the most important features for predicting those outcomes <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2017b</a>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>; Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>)</cite>. Vrzakova et al. point out that even when multimodality does not improve a model’s predictive capabilities, patterns in the multimodal data can be informative. Often, multimodal patterns help contextualize and add interpretability to the unimodal primitives by revealing nuances that cannot be identified by one modality alone <cite class="ltx_cite ltx_citemacro_citep">(Vrzakova et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020</a>)</cite>. These same patterns can also highlight performance differences among students and trainees:</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<blockquote id="S6.SS1.p2.1" class="ltx_quote">
<p id="S6.SS1.p2.1.1" class="ltx_p">Our results demonstrate how NLP and ML techniques allow us to use different modalities of the same data, voice and transcript, and different modalities of different data sources, voice data from interviews, answers to a goal orientation questionnaire, and answers to open ended questions about energy, in order to better understand individual differences in students’ performances. <cite class="ltx_cite ltx_citemacro_citep">(Kubsch et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite></p>
</blockquote>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">Human-centered approaches allow researchers to dive deeper and gain a more holistic understanding of learning and training processes. The richness innate to human-centered data (e.g., contextual qualitative observations, tangible artifacts produced by participants and researchers, participant perspectives gleaned from interviews and surveys, etc.) allows researchers to gain unique insights into participants’ experiences and behaviors by identifying subtleties that more opaque (often quantitative) approaches may miss.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">Our corpus’s results also establish that multimodal methods are generally better-performing and more informative relative to unimodal approaches. This is largely due to different modalities conveying markedly different types of information, which helps create more holistic representations of learners that are much richer than is possible with only a single modality. Ma et al. <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2022</a>)</cite> demonstrate this via several key findings:</p>
</div>
<div id="S6.SS1.p5" class="ltx_para">
<blockquote id="S6.SS1.p5.1" class="ltx_quote">
<p id="S6.SS1.p5.1.1" class="ltx_p">The results showed that Linguistic + Audio + Video (F1 Score = 0.65) yielded the best impasse detection performance…
<br class="ltx_break"></p>
<p id="S6.SS1.p5.1.2" class="ltx_p">We found that the semantics and speaker information in the linguistic modality, the pitch variation in the audio modality, and the facial muscle movements in the video modality are the most significant unimodal indicators of impasse.
<br class="ltx_break"></p>
<p id="S6.SS1.p5.1.3" class="ltx_p">…all of our multimodal models outperformed their unimodal models…</p>
</blockquote>
</div>
<div id="S6.SS1.p6" class="ltx_para">
<p id="S6.SS1.p6.1" class="ltx_p">These results underscore the considerable advantages of employing multimodal methods to understand learning and training experiences, behaviors, and outcomes. By integrating diverse modalities, researchers can uncover patterns that combine to create rich, holistic depictions of students’ learning and training. This comprehensive perspective is crucial for capturing the complexities of learner and trainee experiences and behaviors, and suggests that multimodal approaches are not merely additive, but synergistic, offering opportunities for more informative and in depth analyses that are invaluable for advancing educational practice and research.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Challenges, Limitations, and Research Gaps</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_citet">Worsley and Blikstein (<a href="#bib.bib159" title="" class="ltx_ref">2018</a>)</cite>, a primary ”takeaway” is that various strategies for employing multimodal learning analytics offer a ”meaningful glimpse” into complex datasets that traditional approaches may miss. However, multimodal data complexity presents challenges. Liu et al. <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>)</cite> note that ”data from different sources are often difficult to integrate.” Temporal data alignment and sampling rate issues frequently arise, making data collection and labeling time-consuming and requiring ”significant human time and effort” <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">A major challenge is the lack of data. Most studies analyze small groups, making it difficult to use quantitative algorithms, which explains the limited use of deep learning. Kubsch et al. cite data scarcity as a ”major challenge for building robust and reliable multimodal models” <cite class="ltx_cite ltx_citemacro_citep">(Kubsch et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite>. Small datasets hinder the development of scalable approaches, which several researchers noted:</p>
</div>
<div id="S6.SS2.p3" class="ltx_para">
<blockquote id="S6.SS2.p3.1" class="ltx_quote">
<p id="S6.SS2.p3.1.1" class="ltx_p">…the design and sample size of the focus group do not allow us to generalize the results. <cite class="ltx_cite ltx_citemacro_citep">(Noël et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2022</a>)</cite>
<br class="ltx_break"></p>
<p id="S6.SS2.p3.1.2" class="ltx_p">The limited number of pair work EEs does not allow us to make any strong claims in terms of the framework’s reliability. <cite class="ltx_cite ltx_citemacro_citep">(Morell et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>
<br class="ltx_break"></p>
<p id="S6.SS2.p3.1.3" class="ltx_p">…the size of the dataset used is relatively small, and the subject pool is not overly diverse, limiting our ability to explore culture or ethics-related factors in the model reliably. <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>
<br class="ltx_break"></p>
<p id="S6.SS2.p3.1.4" class="ltx_p">…training a model on a reduced dataset introduces a bias to the model, affecting the validity of the model’s predictions when the data inputs come from a different distribution than the training set. <cite class="ltx_cite ltx_citemacro_citep">(Kubsch et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite></p>
</blockquote>
</div>
<div id="S6.SS2.p4" class="ltx_para">
<p id="S6.SS2.p4.1" class="ltx_p">Large, open-source datasets curated for researchers in multimodal learning and training environments are lacking. This represents a major research gap. Despite several papers mentioning data scarcity as a noteworthy challenge, few papers focus on compiling such datasets or developing methods for smaller datasets. Current methods are often one-off and not designed to generalize. Researchers rely on derived, observable features (e.g., affect and pose; particularly in computer vision) as model input rather than raw features (e.g., pixel values). This differs from core computer vision approaches and creates useful space for exploring end-to-end model training using raw inputs in the future.</p>
</div>
<div id="S6.SS2.p5" class="ltx_para">
<p id="S6.SS2.p5.1" class="ltx_p">The field lags behind core AI and ML, where methods often generalize across tasks and domains. For example, GPT-4 was tested on several benchmarks and exams <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib112" title="" class="ltx_ref">2023</a>)</cite>. Resource and access limitations, along with privacy concerns, hinder the application of advanced AI methods in learning and training environments. Similarly, conversational agents are underrepresented, with few papers discussing agents and none employing interactive, dynamic multi-turn agents (although one paper <cite class="ltx_cite ltx_citemacro_citep">(Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>)</cite> did mention exploring this in the future). We anticipate the rise of generative AI will likely have a substantial impact on the field, in terms of multi-turn agents and otherwise. The lack of standardized coding practices and protocols is another gap. Most papers use domain-specific coding schemes, making replication difficult. Developing reliable methods for automating coding and creating standardized log formats would benefit the field.</p>
</div>
<div id="S6.SS2.p6" class="ltx_para">
<p id="S6.SS2.p6.1" class="ltx_p">Another finding is that training literature is sparse compared to learning literature. Physical training environments are underrepresented, and sensor data is rarely used in learning environments. Most papers use quantitative or qualitative analysis, with few employing mixed-methods approaches. Professional development environments and longitudinal analyses are also underrepresented.</p>
</div>
<div id="S6.SS2.p7" class="ltx_para">
<p id="S6.SS2.p7.1" class="ltx_p">Finally, little work focuses on the direct impact of methods on learners or trainees, or considers their input during development. Recently, particularly in education, researchers have adopted a more stakeholder-centric approach to method development <cite class="ltx_cite ltx_citemacro_citep">(Cohn et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2024a</a>, <a href="#bib.bib36" title="" class="ltx_ref">c</a>)</cite> by incorporating <span id="S6.SS2.p7.1.1" class="ltx_text ltx_font_italic">user-centered design</span> <cite class="ltx_cite ltx_citemacro_citep">(Abras et al<span class="ltx_text">.</span>, <a href="#bib.bib3" title="" class="ltx_ref">2004</a>)</cite>, i.e., focusing on users and their needs throughout the design process. Other stakeholder-centric approaches like <span id="S6.SS2.p7.1.2" class="ltx_text ltx_font_italic">participatory design</span> <cite class="ltx_cite ltx_citemacro_citep">(Schuler and Namioka, <a href="#bib.bib129" title="" class="ltx_ref">1993</a>)</cite> and <span id="S6.SS2.p7.1.3" class="ltx_text ltx_font_italic">co-design</span> <cite class="ltx_cite ltx_citemacro_citep">(Penuel et al<span class="ltx_text">.</span>, <a href="#bib.bib114" title="" class="ltx_ref">2007</a>)</cite> are prevalent in learning sciences but not well-represented in our corpus.</p>
</div>
<div id="S6.SS2.p8" class="ltx_para">
<p id="S6.SS2.p8.1" class="ltx_p">While significant strides have been made in the field, numerous challenges and research gaps remain. The complexity of integrating multimodal data, scarcity of large and diverse datasets, and limitations in data alignment continue to hinder the development of robust and scalable models. The underrepresentation of more advanced AI methods, standardized coding practices, and stakeholder-centric approaches further limits the field’s progress. Addressing these challenges will not only advance the state of multimodal learning and training research, but also enhance the utility and impact of educational technologies in diverse learning and training environments.</p>
</div>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Future Research Directions</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">The results demonstrate that multimodal methods can be powerful in learning and training settings. However, persisting challenges and limitations highlight several research directions requiring further exploration. In the following subsections, we discuss directions that would provide the greatest benefit to the field.</p>
</div>
<section id="S6.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.1. </span>LLMs</h4>

<div id="S6.SS3.SSS1.p1" class="ltx_para">
<p id="S6.SS3.SSS1.p1.1" class="ltx_p">The recent boom in generative AI and multimodal LLMs creates tremendous opportunities for multimodal learning and training research. State-of-the-art models like GPT-4x <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a href="#bib.bib112" title="" class="ltx_ref">2023</a>)</cite> and Gemini <cite class="ltx_cite ltx_citemacro_citep">(Team et al<span class="ltx_text">.</span>, <a href="#bib.bib146" title="" class="ltx_ref">2023</a>)</cite> now offer multimodal capabilities and allow for prompt engineering approaches that can bypass the need for traditional model training (i.e., parameter updates) and large datasets <cite class="ltx_cite ltx_citemacro_citep">(Cohn et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024b</a>)</cite>. Smaller, open-source models can also be trained via parameter-efficient methods to ease the computational overhead endemic to large transformer models <cite class="ltx_cite ltx_citemacro_citep">(Dettmers et al<span class="ltx_text">.</span>, <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>. We see both prompt engineering and multimodal conversational agents as two promising research directions.</p>
</div>
<div id="S6.SS3.SSS1.p2" class="ltx_para">
<p id="S6.SS3.SSS1.p2.1" class="ltx_p">Advances in multimodal transformers (especially those combining vision and text) have demonstrated these models’ ability to perform multiple multimodal tasks. Examples include video-moment retrieval with step-captioning <cite class="ltx_cite ltx_citemacro_citep">(Zala et al<span class="ltx_text">.</span>, <a href="#bib.bib164" title="" class="ltx_ref">2023a</a>)</cite> and diagram generation via LLM planning <cite class="ltx_cite ltx_citemacro_citep">(Zala et al<span class="ltx_text">.</span>, <a href="#bib.bib165" title="" class="ltx_ref">2023b</a>)</cite>. Other work has built multimodal pipelines around LLMs by performing log-based discourse segmentation and using students’ environment actions to contextualize students’ discourse in the prompt <cite class="ltx_cite ltx_citemacro_citep">(Cohn et al<span class="ltx_text">.</span>, <a href="#bib.bib37" title="" class="ltx_ref">2024d</a>; Snyder et al<span class="ltx_text">.</span>, <a href="#bib.bib134" title="" class="ltx_ref">2023</a>, <a href="#bib.bib135" title="" class="ltx_ref">2024</a>)</cite>. Given the recent proliferation of multimodal LLMs in core AI research, we expect to see an increase in LLM integration with multimodal learning and training environments.</p>
</div>
</section>
<section id="S6.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.2. </span>Data Scarcity Mitigation</h4>

<div id="S6.SS3.SSS2.p1" class="ltx_para">
<p id="S6.SS3.SSS2.p1.1" class="ltx_p">Data scarcity is a major issue, causing multimodal learning and training methods to lag behind core AI approaches. Compiling large learning corpora could help, but challenges exist. Collecting multimodal data for large studies is more difficult than for unimodal ones, with a negative correlation between the number of modalities analyzed and sample size <cite class="ltx_cite ltx_citemacro_citep">(Sharma and Giannakos, <a href="#bib.bib132" title="" class="ltx_ref">2020</a>)</cite>. Ethical concerns, particularly regarding privacy and surveillance in educational datasets involving children, complicate data collection <cite class="ltx_cite ltx_citemacro_citep">(Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib43" title="" class="ltx_ref">2020</a>)</cite>. One solution is designing generalizable methods requiring limited data, such as zero and few-shot learning approaches, which have become prominent in core AI domains <cite class="ltx_cite ltx_citemacro_citep">(Kadam and Vaidya, <a href="#bib.bib77" title="" class="ltx_ref">2020</a>)</cite>.</p>
</div>
</section>
<section id="S6.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.3. </span>Standardization</h4>

<div id="S6.SS3.SSS3.p1" class="ltx_para">
<p id="S6.SS3.SSS3.p1.1" class="ltx_p">Blanco et al. <cite class="ltx_cite ltx_citemacro_citep">(del Blanco et al<span class="ltx_text">.</span>, <a href="#bib.bib46" title="" class="ltx_ref">2013</a>)</cite> emphasize the need for uniform coding standards for multimodal, temporal, and human-focused data. Current e-learning norms like xAPI <cite class="ltx_cite ltx_citemacro_citep">(Software, <a href="#bib.bib136" title="" class="ltx_ref">2024</a>)</cite> and LTI <cite class="ltx_cite ltx_citemacro_citep">(1EdTech, <a href="#bib.bib2" title="" class="ltx_ref">2019</a>)</cite> are used in platforms like Canvas and Moodle but mainly for unimodal data and are limited by proprietary licenses. Adapting these frameworks for multimodal data is challenging, leading to minimal use in research.</p>
</div>
<div id="S6.SS3.SSS3.p2" class="ltx_para">
<p id="S6.SS3.SSS3.p2.1" class="ltx_p">Multimodal learning and training research merges AI, multimodal data, and educational contexts, requiring novel software. This has led to disparate approaches across research teams <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2022</a>)</cite>. Creating uniform standards is crucial for the reliability of machine learning techniques and improving human-centric data analysis <cite class="ltx_cite ltx_citemacro_citep">(Davalos et al<span class="ltx_text">.</span>, <a href="#bib.bib45" title="" class="ltx_ref">2023</a>)</cite>. Adopting a unified log format for multimodal data could reduce reliance on context-specific methods and improve generalizability. Researchers and engineers should also comply with existing standards and methodologies.</p>
</div>
</section>
<section id="S6.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.4. </span>Active Environments</h4>

<div id="S6.SS3.SSS4.p1" class="ltx_para">
<p id="S6.SS3.SSS4.p1.1" class="ltx_p">Environments where study participants are physically active provide an opportunity for researchers to accommodate motion-based modalities into their multimodal pipelines, e.g., via inertial measurement unit (IMU) sensors. This type of research was largely absent from our corpus, and we envision it being particularly useful for embodied learning and physical training research.</p>
</div>
<div id="S6.SS3.SSS4.p2" class="ltx_para">
<p id="S6.SS3.SSS4.p2.1" class="ltx_p">Embodied learning scenarios, where learners explore concepts through body movement, involve extensive multimodal data, capturing sensory inputs essential for movements, gestures, speech, gaze, interactions, and coordination <cite class="ltx_cite ltx_citemacro_citep">(Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>. Interaction analysis is common but challenging due to human analysts’ cognitive limits and the fast-changing nature of embodied contexts <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a href="#bib.bib166" title="" class="ltx_ref">2024</a>)</cite>. Leveraging multimodal methods to support human analysts in such scenarios is promising. MMLA must address the complexities of 1) multimodal data collection from heterogeneous sensors, 2) data alignment, and 3) analysis to derive meaningful insights into learners’ behaviors, providing educators with a comprehensive understanding of engagement and problem-solving <cite class="ltx_cite ltx_citemacro_citep">(Fonteles et al<span class="ltx_text">.</span>, <a href="#bib.bib59" title="" class="ltx_ref">2024b</a>)</cite>.</p>
</div>
<div id="S6.SS3.SSS4.p3" class="ltx_para">
<p id="S6.SS3.SSS4.p3.1" class="ltx_p">Physical training environments, like rehabilitation therapy, weight lifting, running, and cycling, often use IMU sensors for human activity recognition (HAR). However, this is not typically done using multimodal data. Combining spatial modalities (like pose and gesture) with physiological modalities (such as blood pressure, body temperature, and electrodermal activity) could provide a more holistic interpretation of trainees’ actions. Multimodality can decompose activities into sub-activities too nuanced to identify unimodally and add interpretability that IMU data alone cannot provide. For example, Xia et al. co-trained deep learning models using activities’ images and IMU data, improving HAR generalizability <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a href="#bib.bib161" title="" class="ltx_ref">2024</a>)</cite>. While some physical training works in our corpus leveraged multimodality <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>; Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite>, this was rare, and further research is needed to better inform physical training environments.</p>
</div>
</section>
<section id="S6.SS3.SSS5" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.5. </span>Explainability</h4>

<div id="S6.SS3.SSS5.p1" class="ltx_para">
<p id="S6.SS3.SSS5.p1.1" class="ltx_p">Many AI and ML approaches use black-box algorithms with outputs that lack explainability, hindering teachers’ and trainers’ ability to guide students and fostering distrust in AI systems. Prior work has aimed to create more explainable systems using data visualization tools to make learning processes transparent <cite class="ltx_cite ltx_citemacro_citep">(Hutchins et al<span class="ltx_text">.</span>, <a href="#bib.bib74" title="" class="ltx_ref">2022</a>; Vatral et al<span class="ltx_text">.</span>, <a href="#bib.bib153" title="" class="ltx_ref">2023</a>)</cite>. LLMs have potential for enhancing explainability through <span id="S6.SS3.SSS5.p1.1.1" class="ltx_text ltx_font_italic">Chain-of-Thought</span> prompting, which elicits reasoning chains from the model <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a href="#bib.bib157" title="" class="ltx_ref">2022</a>; Cohn et al<span class="ltx_text">.</span>, <a href="#bib.bib35" title="" class="ltx_ref">2024b</a>)</cite>. Feedback from teachers and students shows they see potential for LLMs to improve learning outcomes, but explainability is crucial for their acceptance <cite class="ltx_cite ltx_citemacro_citep">(Cohn et al<span class="ltx_text">.</span>, <a href="#bib.bib34" title="" class="ltx_ref">2024a</a>)</cite>.</p>
</div>
</section>
<section id="S6.SS3.SSS6" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.6. </span>Longitudinal Analyses</h4>

<div id="S6.SS3.SSS6.p1" class="ltx_para">
<p id="S6.SS3.SSS6.p1.1" class="ltx_p">The vast majority of studies in our corpus focus on using multimodality to either predict overall learning and training outcomes or identify features correlating with those outcomes; however, these approaches do not consider how students and trainees evolve over time. Conducting longitudinal studies and analyses would provide insight into how participants’ behaviors and abilities develop as they progress in their learning or training. Longitudinal investigations have been successfully executed using unimodal and digital trace data <cite class="ltx_cite ltx_citemacro_citep">(Boulton et al<span class="ltx_text">.</span>, <a href="#bib.bib16" title="" class="ltx_ref">2019</a>)</cite>, but less frequently within multimodal studies. The challenges of scalability and standardization of multimodal logs have restricted longitudinal MMLA research <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a href="#bib.bib162" title="" class="ltx_ref">2022</a>)</cite>, affecting both research and software development in multimodal learning and training. There exists a void in the literature concerning longitudinal multimodal learner models encompassing a comprehensive view of learners’ and trainees’ evolution over time, making this an area ripe for further research exploration.</p>
</div>
</section>
<section id="S6.SS3.SSS7" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.3.7. </span>Stakeholder Input and Impact</h4>

<div id="S6.SS3.SSS7.p1" class="ltx_para">
<p id="S6.SS3.SSS7.p1.1" class="ltx_p">Section <a href="#S5" title="5. Archetypes ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> revealed a disconnect between researchers designing multimodal learning and training methods and the stakeholders these methods were intended to benefit. Few efforts incorporated user input in their method development pipelines or evaluated the impact of their methods on stakeholders’ real-world experiences. A larger emphasis on <span id="S6.SS3.SSS7.p1.1.1" class="ltx_text ltx_font_italic">design-based research</span> <cite class="ltx_cite ltx_citemacro_citep">(Armstrong et al<span class="ltx_text">.</span>, <a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>, i.e., iteratively designing and refining methods based on real-world research, would help bridge this gap. Additionally, employing <span id="S6.SS3.SSS7.p1.1.2" class="ltx_text ltx_font_italic">participatory design</span> (i.e., incorporating the input and participation of stakeholders into the design process) and <span id="S6.SS3.SSS7.p1.1.3" class="ltx_text ltx_font_italic">co-design</span> (i.e., giving stakeholders agency in processes leading to design decisions) <cite class="ltx_cite ltx_citemacro_citep">(Sarmiento and Wise, <a href="#bib.bib127" title="" class="ltx_ref">2022</a>)</cite> would help researchers develop multimodal methods better aligned with stakeholder experiences and outcomes.</p>
</div>
</section>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.4. </span>Literature Review Limitations</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.1" class="ltx_p">We acknowledge the limitations of our literature review. While Google Scholar is widely used, it poses reproducibility challenges due to its opaqueness, non-determinism, and user-specific results. Although reconstructing our initial corpus <span id="S6.SS4.p1.1.1" class="ltx_text ltx_font_italic">in its exact form</span> is unlikely, the authors are confident that the variability in Google Scholar searches does not prohibit the <span id="S6.SS4.p1.1.2" class="ltx_text ltx_font_italic">overall</span> reproducibility of the corpus. This is because SerpAPI does not use individual user data when conducting web scrapes, as API calls are made via proxy and random headers.</p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">Initially distilling our literature search corpus using citation graph pruning (see Section <a href="#S3.SS2.SSS1" title="3.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>) is another potential limitation, as relevant papers may have been excluded due to minimal citations. However, since this paper reviews prominent methods in multimodal learning and training, the authors agreed that works not significantly citing other related papers (outgoing citations) or significantly cited by related papers (incoming citations) were outside our review’s scope. For a detailed account of this review’s limitations, see Appendix <a href="#A3" title="Appendix C Literature Review Limitations ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this paper, we conducted a comprehensive literature review of research methods in multimodal learning and training environments. We developed a novel approach, <span id="S7.p1.1.1" class="ltx_text ltx_font_italic">citation graph pruning</span>, to distill our literature corpus. We presented a taxonomy and framework reflecting current advances, identifying and analyzing five modality groups (Natural Language, Vision, Sensors, Human-Centered, and Logs) through descriptive statistics, qualitative thematic analysis, and discussions on state-of-the-art findings, challenges, and research gaps. We derived three archetypes characterizing current research and identified the need for a new type of data fusion, <span id="S7.p1.1.2" class="ltx_text ltx_font_italic">mid fusion</span>, which combines derived, observable features. We concluded with promising research directions and the limitations of our work. As multimodal learning and training analytics expand with generative AI, this review aims to inspire new methods and research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">        




</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">1EdTech (2019)</span>
<span class="ltx_bibblock">
1EdTech. 2019.

</span>
<span class="ltx_bibblock">Learning Tools Interoperability Core Specification 1.3 — IMS Global Learning Consortium — imsglobal.org.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.imsglobal.org/spec/lti/v1p3/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.imsglobal.org/spec/lti/v1p3/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 25-01-2024].

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abras et al<span id="bib.bib3.2.2.1" class="ltx_text">.</span> (2004)</span>
<span class="ltx_bibblock">
Chadia Abras, Diane Maloney-Krichmar, Jenny Preece, et al<span id="bib.bib3.3.1" class="ltx_text">.</span> 2004.

</span>
<span class="ltx_bibblock">User-centered design.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.4.1" class="ltx_emph ltx_font_italic">Bainbridge, W. Encyclopedia of Human-Computer Interaction. Thousand Oaks: Sage Publications</em> 37, 4 (2004), 445–456.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adarkwah (2021)</span>
<span class="ltx_bibblock">
Michael Agyemang Adarkwah. 2021.

</span>
<span class="ltx_bibblock">The power of assessment feedback in teaching and learning: a narrative review and synthesis of the literature.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">SN Social Sciences</em> 1, 3 (March 2021), 75.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1007/s43545-021-00086-w" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/s43545-021-00086-w</a>

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alwahaby et al<span id="bib.bib5.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Haifa Alwahaby, Mutlu Cukurova, Zacharoula Papamitsiou, and Michail Giannakos. 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.3.1" class="ltx_emph ltx_font_italic">The Evidence of Impact and Ethical Considerations of Multimodal Learning Analytics: A Systematic Literature Review</em>.

</span>
<span class="ltx_bibblock">Springer International Publishing, Cham, 289–325.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-031-08076-0_12" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/978-3-031-08076-0˙12</a>

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alyuz et al<span id="bib.bib6.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Nese Alyuz, Eda Okur, Utku Genc, Sinem Aslan, Cagri Tanriover, and Asli Arslan Esme. 2017.

</span>
<span class="ltx_bibblock">An unobtrusive and multimodal approach for behavioral engagement detection of students. In <em id="bib.bib6.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st ACM SIGCHI International Workshop on Multimodal Interaction for Education</em>. ACM, Glasgow UK, 26–32.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3139513.3139521" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3139513.3139521</a>

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andrade (2017)</span>
<span class="ltx_bibblock">
Alejandro Andrade. 2017.

</span>
<span class="ltx_bibblock">Understanding student learning trajectories using multimodal learning analytics within an embodied-interaction learning environment. In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference</em>. ACM, Vancouver British Columbia Canada, 70–79.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3027385.3027429" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3027385.3027429</a>

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Armstrong et al<span id="bib.bib8.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Matthew Armstrong, Cade Dopp, and Jesse Welsh. 2022.

</span>
<span class="ltx_bibblock">Design-based research.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.3.1" class="ltx_emph ltx_font_italic">N/A</em> N/A, N/A (2022), N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashwin and Guddeti (2020)</span>
<span class="ltx_bibblock">
T. S. Ashwin and Ram Mohana Reddy Guddeti. 2020.

</span>
<span class="ltx_bibblock">Impact of inquiry interventions on students in e-learning and classroom environments using affective computing framework.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">User Modeling and User-Adapted Interaction</em> 30, 5 (Nov. 2020), 759–801.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s11257-019-09254-3" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/s11257-019-09254-3</a>

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aslan et al<span id="bib.bib10.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Sinem Aslan, Nese Alyuz, Cagri Tanriover, Sinem E. Mete, Eda Okur, Sidney K. D’Mello, and Asli Arslan Esme. 2019.

</span>
<span class="ltx_bibblock">Investigating the Impact of a Real-time, Multimodal Student Engagement Analytics Technology in Authentic Classrooms. In <em id="bib.bib10.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. ACM, Glasgow Scotland Uk, 1–12.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3290605.3300534" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3290605.3300534</a>

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Azcona et al<span id="bib.bib11.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
David Azcona, I-Han Hsiao, and Alan F. Smeaton. 2018.

</span>
<span class="ltx_bibblock">Personalizing Computer Science Education by Leveraging Multimodal Learning Analytics. In <em id="bib.bib11.3.1" class="ltx_emph ltx_font_italic">2018 IEEE Frontiers in Education Conference (FIE)</em>. IEEE, San Jose, CA, USA, 1–9.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/FIE.2018.8658596" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/FIE.2018.8658596</a>

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birt et al<span id="bib.bib12.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
James Birt, Zane Stromberga, Michael Cowling, and Christian Moro. 2018.

</span>
<span class="ltx_bibblock">Mobile Mixed Reality for Experiential Learning and Simulation in Medical and Health Sciences Education.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.3.1" class="ltx_emph ltx_font_italic">Information</em> 9, 2 (Jan. 2018), 31.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/info9020031" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.3390/info9020031</a>

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blikstein (2013)</span>
<span class="ltx_bibblock">
Paulo Blikstein. 2013.

</span>
<span class="ltx_bibblock">Multimodal learning analytics. In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of the third international conference on learning analytics and knowledge</em>. Association for Computing Machinery, New York, NY, USA, 102–106.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blikstein and Worsley (2016)</span>
<span class="ltx_bibblock">
Paulo Blikstein and Marcelo Worsley. 2016.

</span>
<span class="ltx_bibblock">Multimodal learning analytics and education data mining: Using computational technologies to measure complex learning tasks.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Journal of Learning Analytics</em> 3, 2 (2016), 220–238.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boser and McDaniels (2018)</span>
<span class="ltx_bibblock">
Ulrich Boser and Abel McDaniels. 2018.

</span>
<span class="ltx_bibblock">Addressing the Gap between Education Research and Practice: The Need for State Education Capacity Centers.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Center for American Progress</em> N/A, N/A (2018), N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boulton et al<span id="bib.bib16.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Chris A. Boulton, Emily Hughes, Carmel Kent, Joanne R. Smith, and Hywel T. P. Williams. 2019.

</span>
<span class="ltx_bibblock">Student engagement and wellbeing over time at a higher education institution.

</span>
<span class="ltx_bibblock"><em id="bib.bib16.3.1" class="ltx_emph ltx_font_italic">PLOS ONE</em> 14, 11 (11 2019), 1–20.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1371/journal.pone.0225770" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1371/journal.pone.0225770</a>

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun and Clarke (2006)</span>
<span class="ltx_bibblock">
Virginia Braun and Victoria Clarke. 2006.

</span>
<span class="ltx_bibblock">Using thematic analysis in psychology.

</span>
<span class="ltx_bibblock"><em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Qualitative research in psychology</em> 3, 2 (2006), 77.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span id="bib.bib18.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al<span id="bib.bib18.3.1" class="ltx_text">.</span> 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.4.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 33 (2020), 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Capital Normal University, Beijing, China et al<span id="bib.bib19.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Capital Normal University, Beijing, China, Xiaoyang Ma, Min Xu, Yao Dong, and Zhong Sun. 2021.

</span>
<span class="ltx_bibblock">Automatic Student Engagement in Online Learning Environment Based on Neural Turing Machine.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.3.1" class="ltx_emph ltx_font_italic">International Journal of Information and Education Technology</em> 11, 3 (2021), 107–111.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.18178/ijiet.2021.11.3.1497" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.18178/ijiet.2021.11.3.1497</a>

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al<span id="bib.bib20.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Man Ching Esther Chan, Xavier Ochoa, and David Clarke. 2020.

</span>
<span class="ltx_bibblock">Multimodal Learning Analytics in a Laboratory Classroom.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.3.1" class="ltx_emph ltx_font_italic">Machine Learning Paradigms</em>, Maria Virvou, Efthimios Alepis, George A. Tsihrintzis, and Lakhmi C. Jain (Eds.). Vol. 158. Springer International Publishing, Cham, 131–156.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://link.springer.com/10.1007/978-3-030-13743-4_8" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://link.springer.com/10.1007/978-3-030-13743-4˙8</a>

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chango et al<span id="bib.bib21.2.2.1" class="ltx_text">.</span> (2021a)</span>
<span class="ltx_bibblock">
Wilson Chango, Rebeca Cerezo, and Cristóbal Romero. 2021a.

</span>
<span class="ltx_bibblock">Multi-source and multimodal data fusion for predicting academic performance in blended learning university courses.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.3.1" class="ltx_emph ltx_font_italic">Computers &amp; Electrical Engineering</em> 89 (Jan. 2021), 106908.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.compeleceng.2020.106908" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1016/j.compeleceng.2020.106908</a>

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chango et al<span id="bib.bib22.2.2.1" class="ltx_text">.</span> (2021b)</span>
<span class="ltx_bibblock">
Wilson Chango, Rebeca Cerezo, Miguel Sanchez-Santillan, Roger Azevedo, and Cristóbal Romero. 2021b.

</span>
<span class="ltx_bibblock">Improving prediction of students’ performance in intelligent tutoring systems using attribute selection and ensembles of different multimodal data sources.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.3.1" class="ltx_emph ltx_font_italic">Journal of Computing in Higher Education</em> 33, 3 (Dec. 2021), 614–634.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s12528-021-09298-8" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/s12528-021-09298-8</a>

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chango et al<span id="bib.bib23.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Wilson Chango, Juan A. Lara, Rebeca Cerezo, and Cristóbal Romero. 2022.

</span>
<span class="ltx_bibblock">A review on data fusion in multimodal learning analytics and educational data mining.

</span>
<span class="ltx_bibblock"><em id="bib.bib23.3.1" class="ltx_emph ltx_font_italic">WIREs Data Mining and Knowledge Discovery</em> 12, 4 (2022), e1458.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1002/widm.1458" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1002/widm.1458</a>
arXiv:https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1458

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen (2021)</span>
<span class="ltx_bibblock">
Lujie Karen Chen. 2021.

</span>
<span class="ltx_bibblock">Affect, Support, and Personal Factors: Multimodal Causal Models of One-on-one Coaching.

</span>
<span class="ltx_bibblock"><em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Journal of Educational Data Mining</em> 13, 3 (2021), 36–68.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chinh et al<span id="bib.bib25.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Bonnie Chinh, Himanshu Zade, Abbas Ganji, and Cecilia Aragon. 2019.

</span>
<span class="ltx_bibblock">Ways of Qualitative Coding: A Case Study of Four Strategies for Resolving Disagreements. In <em id="bib.bib25.3.1" class="ltx_emph ltx_font_italic">Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems</em> (Glasgow, Scotland Uk) <em id="bib.bib25.4.2" class="ltx_emph ltx_font_italic">(CHI EA ’19)</em>. Association for Computing Machinery, New York, NY, USA, 1–6.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3290607.3312879" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3290607.3312879</a>

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cholewiak et al<span id="bib.bib26.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Steven A. Cholewiak, Panos Ipeirotis, Victor Silva, and Arun Kannawadi. 2021.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.3.1" class="ltx_emph ltx_font_italic">SCHOLARLY: Simple access to Google Scholar authors and citation using Python</em>.

</span>
<span class="ltx_bibblock">N/A.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.5281/zenodo.5764801" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.5281/zenodo.5764801</a>

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chua et al<span id="bib.bib27.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Yi Han Victoria Chua, Justin Dauwels, and Seng Chee Tan. 2019.

</span>
<span class="ltx_bibblock">Technologies for automated analysis of co-located, real-life, physical learning spaces: Where are we now?. In <em id="bib.bib27.3.1" class="ltx_emph ltx_font_italic">LAK19: 9th International Learning Analytics and Knowledge Conference</em>. ACM, Tempe AZ USA, 10.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://dl-acm-org.proxy.library.vanderbilt.edu/doi/10.1145/3303772.3303811" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://dl-acm-org.proxy.library.vanderbilt.edu/doi/10.1145/3303772.3303811</a>

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Closser et al<span id="bib.bib28.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Avery H. Closser, John A. Erickson, Hannah Smith, Ashvini Varatharaj, and Anthony F. Botelho. 2022.

</span>
<span class="ltx_bibblock">Blending learning analytics and embodied design to model students’ comprehension of measurement using their actions, speech, and gestures.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.3.1" class="ltx_emph ltx_font_italic">International Journal of Child-Computer Interaction</em> 32 (June 2022), 100391.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.ijcci.2021.100391" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1016/j.ijcci.2021.100391</a>

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cochran et al<span id="bib.bib29.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Keith Cochran, Clayton Cohn, Peter Hastings, Noriko Tomuro, and Simon Hughes. 2023b.

</span>
<span class="ltx_bibblock">Using BERT to Identify Causal Structure in Students’ Scientific Explanations.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.3.1" class="ltx_emph ltx_font_italic">International Journal of Artificial Intelligence in Education</em> N/A, N/A (2023), 1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cochran et al<span id="bib.bib30.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Keith Cochran, Clayton Cohn, and Peter M. Hastings. 2023a.

</span>
<span class="ltx_bibblock">Improving NLP Model Performance on Small Educational Data Sets Using Self-Augmentation. In <em id="bib.bib30.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 15th International Conference on Computer Supported Education, CSEDU 2023, Prague, Czech Republic, April 21-23, 2023, Volume 1</em>, Jelena Jovanovic, Irene-Angelica Chounta, James Uhomoibhi, and Bruce M. McLaren (Eds.). SCITEPRESS, N/A, 70–78.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.5220/0011857200003470" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.5220/0011857200003470</a>

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cochran et al<span id="bib.bib31.2.2.1" class="ltx_text">.</span> (2023c)</span>
<span class="ltx_bibblock">
Keith Cochran, Clayton Cohn, Jean Francois Rouet, and Peter Hastings. 2023c.

</span>
<span class="ltx_bibblock">Improving Automated Evaluation of Student Text Responses Using GPT-3.5 for Text Data Augmentation. In <em id="bib.bib31.3.1" class="ltx_emph ltx_font_italic">International Conference on Artificial Intelligence in Education</em>. Springer, N/A, N/A, 217–228.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen (1960)</span>
<span class="ltx_bibblock">
Jacob Cohen. 1960.

</span>
<span class="ltx_bibblock">A coefficient of agreement for nominal scales.

</span>
<span class="ltx_bibblock"><em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">Educational and psychological measurement</em> 20, 1 (1960), 37–46.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohn (2020)</span>
<span class="ltx_bibblock">
Clayton Cohn. 2020.

</span>
<span class="ltx_bibblock"><em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">BERT efficacy on scientific and medical datasets: a systematic literature review</em>.

</span>
<span class="ltx_bibblock">DePaul University, N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohn et al<span id="bib.bib34.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Clayton Cohn, Nicole Hutchins, and Gautam Biswas. 2024a.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting with Stakeholders-in-the-Loop for Evaluating Formative Assessments in STEM+Computing. (August 2024).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Submitting to Education and Information Technologies..

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohn et al<span id="bib.bib35.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Clayton Cohn, Nicole Hutchins, Tuan Le, and Gautam Biswas. 2024b.

</span>
<span class="ltx_bibblock">A Chain-of-Thought Prompting Approach with LLMs for Evaluating Students’ Formative Assessment Responses in Science.

</span>
<span class="ltx_bibblock"><em id="bib.bib35.3.1" class="ltx_emph ltx_font_italic">Proc. Conf. AAAI Artif. Intell.</em> 38, 21 (March 2024), 23182–23190.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohn et al<span id="bib.bib36.2.2.1" class="ltx_text">.</span> (2024c)</span>
<span class="ltx_bibblock">
Clayton Cohn, Caitlin Snyder, Joyce Fonteles, Ashwin T S, Justin Montenegro, and Gautam Biswas. 2024c.

</span>
<span class="ltx_bibblock">A Multimodal Approach to Support Teacher, Researcher, and AI Collaboration in STEM+C Learning Environments. (July 2024).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Submitted to the British Journal of Educational Technology special section Hybrid Intelligence: Human-AI Co-evolution and Learning. Currently under review.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohn et al<span id="bib.bib37.2.2.1" class="ltx_text">.</span> (2024d)</span>
<span class="ltx_bibblock">
Clayton Cohn, Caitlin Snyder, Justin Montenegro, and Gautam Biswas. 2024d.

</span>
<span class="ltx_bibblock">Towards a human-in-the-loop LLM approach to collaborative discourse analysis.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.3.1" class="ltx_emph ltx_font_italic">Artificial Intelligence in Education. Posters and Late Breaking Results, Workshops and Tutorials, Industry and Innovation Tracks, Practitioners, Doctoral Consortium and Blue Sky</em>. Springer Nature Switzerland, Cham, 11–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cook et al<span id="bib.bib38.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Matt Cook, Zack Lischer-Katz, Nathan Hall, Juliet Hardesty, Jennifer Johnson, Robert McDonald, and Tara Carlisle. 2019.

</span>
<span class="ltx_bibblock">Challenges and strategies for educational virtual reality.

</span>
<span class="ltx_bibblock"><em id="bib.bib38.3.1" class="ltx_emph ltx_font_italic">Information Technology and Libraries</em> 38, 4 (2019), 25–48.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cornide-Reyes et al<span id="bib.bib39.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Hector Cornide-Reyes, René Noël, Fabián Riquelme, Matías Gajardo, Cristian Cechinel, Roberto Mac Lean, Carlos Becerra, Rodolfo Villarroel, and Roberto Munoz. 2019.

</span>
<span class="ltx_bibblock">Introducing Low-Cost Sensors into the Classroom Settings: Improving the Assessment in Agile Practices with Multimodal Learning Analytics.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.3.1" class="ltx_emph ltx_font_italic">Sensors</em> 19, 15 (July 2019), 3291.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/s19153291" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.3390/s19153291</a>

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crescenzi-Lanna (2020)</span>
<span class="ltx_bibblock">
Lucrezia Crescenzi-Lanna. 2020.

</span>
<span class="ltx_bibblock">Multimodal Learning Analytics research with young children: A systematic review.

</span>
<span class="ltx_bibblock"><em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (2020), 1485–1504.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1111/bjet.12959" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.12959</a>

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crossley et al<span id="bib.bib41.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Scott A Crossley, Kristopher Kyle, and Mihai Dascalu. 2019.

</span>
<span class="ltx_bibblock">The Tool for the Automatic Analysis of Cohesion 2.0: Integrating semantic similarity and text overlap.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.3.1" class="ltx_emph ltx_font_italic">Behavior research methods</em> 51 (2019), 14–27.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crossley et al<span id="bib.bib42.2.2.1" class="ltx_text">.</span> (2016)</span>
<span class="ltx_bibblock">
Scott A Crossley, Kristopher Kyle, and Danielle S McNamara. 2016.

</span>
<span class="ltx_bibblock">The tool for the automatic analysis of text cohesion (TAACO): Automatic assessment of local, global, and text cohesion.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.3.1" class="ltx_emph ltx_font_italic">Behavior research methods</em> 48 (2016), 1227–1237.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cukurova et al<span id="bib.bib43.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Mutlu Cukurova, Michail Giannakos, and Roberto Martinez-Maldonado. 2020.

</span>
<span class="ltx_bibblock">The promise and challenges of multimodal learning analytics.

</span>
<span class="ltx_bibblock"><em id="bib.bib43.3.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (2020), 1441–1449.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cukurova et al<span id="bib.bib44.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Mutlu Cukurova, Carmel Kent, and Rosemary Luckin. 2019.

</span>
<span class="ltx_bibblock">Artificial intelligence and multimodal data in the service of human decision‐making: A case study in debate tutoring.

</span>
<span class="ltx_bibblock"><em id="bib.bib44.3.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 50, 6 (Nov. 2019), 3032–3046.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.12829" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.12829</a>

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Davalos et al<span id="bib.bib45.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
E. Davalos, U. Timalsina, Y. Zhang, J. Wu, J. Fonteles, and G. Biswas. 2023.

</span>
<span class="ltx_bibblock">ChimeraPy: A Scientific Distributed Streaming Framework for Real-time Multimodal Data Retrieval and Processing. In <em id="bib.bib45.3.1" class="ltx_emph ltx_font_italic">2023 IEEE International Conference on Big Data (BigData)</em>. IEEE Computer Society, Los Alamitos, CA, USA, 201–206.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/BigData59044.2023.10386382" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/BigData59044.2023.10386382</a>

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">del Blanco et al<span id="bib.bib46.2.2.1" class="ltx_text">.</span> (2013)</span>
<span class="ltx_bibblock">
Ángel del Blanco, Ángel Serrano, Manuel Freire, Iván Martínez-Ortiz, and Baltasar Fernández-Manjón. 2013.

</span>
<span class="ltx_bibblock">E-Learning standards and learning analytics. Can data collection be improved by using standard data models?. In <em id="bib.bib46.3.1" class="ltx_emph ltx_font_italic">2013 IEEE Global Engineering Education Conference (EDUCON)</em>. IEEE, Berlin, Germany, 1255–1261.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/EduCon.2013.6530268" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/EduCon.2013.6530268</a>

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers et al<span id="bib.bib47.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock">QLoRA: Efficient Finetuning of Quantized LLMs.

</span>
<span class="ltx_bibblock"><em id="bib.bib47.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> N/A, N/A, Article arXiv:2305.14314 (May 2023), N/A pages.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2305.14314" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.48550/arXiv.2305.14314</a>
arXiv:2305.14314 [cs.LG]

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al<span id="bib.bib48.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1810.04805</em> N/A, N/A (2018), N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Mitri et al<span id="bib.bib49.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Daniele Di Mitri, Maren Scheffel, Hendrik Drachsler, Dirk Börner, Stefaan Ternier, and Marcus Specht. 2017.

</span>
<span class="ltx_bibblock">Learning pulse: a machine learning approach for predicting performance in self-regulated learning using multimodal data. In <em id="bib.bib49.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Seventh International Learning Analytics &amp; Knowledge Conference</em>. ACM, Vancouver British Columbia Canada, 188–197.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3027385.3027447" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3027385.3027447</a>

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Mitri et al<span id="bib.bib50.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Daniele Di Mitri, Jan Schneider, and Hendrik Drachsler. 2022.

</span>
<span class="ltx_bibblock">Keep Me in the Loop: Real-Time Feedback with Multimodal Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib50.3.1" class="ltx_emph ltx_font_italic">International Journal of Artificial Intelligence in Education</em> 32, 4 (Dec. 2022), 1093–1118.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s40593-021-00281-z" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/s40593-021-00281-z</a>

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Mitri et al<span id="bib.bib51.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Daniele Di Mitri, Jan Schneider, Marcus Specht, and Hendrik Drachsler. 2018.

</span>
<span class="ltx_bibblock">From signals to knowledge: A conceptual model for multimodal learning analytics.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.3.1" class="ltx_emph ltx_font_italic">Journal of Computer Assisted Learning</em> 34, 4 (2018), 338–349.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1111/jcal.12288" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/jcal.12288</a>

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Di Mitri et al<span id="bib.bib52.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Daniele Di Mitri, Jan Schneider, Kevin Trebing, Sasa Sopka, Marcus Specht, and Hendrik Drachsler. 2020.

</span>
<span class="ltx_bibblock">Real-Time Multimodal Feedback with the CPR Tutor.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.3.1" class="ltx_emph ltx_font_italic">Artificial Intelligence in Education</em>, Ig Ibert Bittencourt, Mutlu Cukurova, Kasia Muldner, Rose Luckin, and Eva Millán (Eds.). Vol. 12163. Springer International Publishing, Cham, 141–152.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://link.springer.com/10.1007/978-3-030-52237-7_12" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://link.springer.com/10.1007/978-3-030-52237-7˙12</a>

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Drachsler and Schneider (2018)</span>
<span class="ltx_bibblock">
Hendrik Drachsler and Jan Schneider. 2018.

</span>
<span class="ltx_bibblock">JCAL Special Issue on Multimodal Learning Analytics.

</span>
<span class="ltx_bibblock"><em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Journal of Computer Assisted Learning</em> 34, 4 (2018), 335–337.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1111/jcal.12291" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/jcal.12291</a>
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/jcal.12291

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Echeverria et al<span id="bib.bib54.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Vanessa Echeverria, Roberto Martinez-Maldonado, and Simon Buckingham Shum. 2019.

</span>
<span class="ltx_bibblock">Towards Collaboration Translucence: Giving Meaning to Multimodal Group Data. In <em id="bib.bib54.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</em>. ACM, Glasgow Scotland Uk, 1–16.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3290605.3300269" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3290605.3300269</a>

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Emerson et al<span id="bib.bib55.2.2.1" class="ltx_text">.</span> (2020a)</span>
<span class="ltx_bibblock">
Andrew Emerson, Elizabeth B. Cloude, Roger Azevedo, and James Lester. 2020a.

</span>
<span class="ltx_bibblock">Multimodal learning analytics for game‐based learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib55.3.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (Sept. 2020), 1505–1526.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.12992" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.12992</a>

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Emerson et al<span id="bib.bib56.2.2.1" class="ltx_text">.</span> (2020b)</span>
<span class="ltx_bibblock">
Andrew Emerson, Nathan Henderson, Jonathan Rowe, Wookhee Min, Seung Lee, James Minogue, and James Lester. 2020b.

</span>
<span class="ltx_bibblock">Early Prediction of Visitor Engagement in Science Museums with Multimodal Learning Analytics. In <em id="bib.bib56.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 International Conference on Multimodal Interaction</em>. ACM, Virtual Event Netherlands, 107–116.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3382507.3418890" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3382507.3418890</a>

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eyben et al<span id="bib.bib57.2.2.1" class="ltx_text">.</span> (2010)</span>
<span class="ltx_bibblock">
Florian Eyben, Martin Wöllmer, and Björn Schuller. 2010.

</span>
<span class="ltx_bibblock">Opensmile: the munich versatile and fast open-source audio feature extractor. In <em id="bib.bib57.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th ACM international conference on Multimedia</em>. N/A, N/A, 1459–1462.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandez-Nieto et al<span id="bib.bib58.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Gloria Milena Fernandez-Nieto, Vanessa Echeverria, Simon Buckingham Shum, Katerina Mangaroska, Kirsty Kitto, Evelyn Palominos, Carmen Axisa, and Roberto Martinez-Maldonado. 2021.

</span>
<span class="ltx_bibblock">Storytelling With Learner Data: Guiding Student Reflection on Multimodal Team Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib58.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Learning Technologies</em> 14, 5 (Oct. 2021), 695–708.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TLT.2021.3131842" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/TLT.2021.3131842</a>

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fonteles et al<span id="bib.bib59.2.2.1" class="ltx_text">.</span> (2024b)</span>
<span class="ltx_bibblock">
Joyce Fonteles, Eduardo Davalos, T. S. Ashwin, Yike Zhang, Mengxi Zhou, Efrat Ayalon, Alicia Lane, Selena Steinberg, Gabriella Anton, Joshua Danish, Noel Enyedy, and Gautam Biswas. 2024b.

</span>
<span class="ltx_bibblock">A First Step in Using Machine Learning Methods to Enhance Interaction Analysis for Embodied Learning Environments. In <em id="bib.bib59.3.1" class="ltx_emph ltx_font_italic">Artificial Intelligence in Education</em>, Andrew M. Olney, Irene-Angelica Chounta, Zitao Liu, Olga C. Santos, and Ig Ibert Bittencourt (Eds.). Springer Nature Switzerland, Cham, 3–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fonteles et al<span id="bib.bib60.2.2.1" class="ltx_text">.</span> (2024a)</span>
<span class="ltx_bibblock">
Joyce Horn Fonteles, Celestine E Akpanoko, Pamela J. Wisniewski, and Gautam Biswas. 2024a.

</span>
<span class="ltx_bibblock">Promoting Equitable Learning Outcomes for Underserved Students in Open-Ended Learning Environments. In <em id="bib.bib60.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd Annual ACM Interaction Design and Children Conference</em> (Delft, Netherlands) <em id="bib.bib60.4.2" class="ltx_emph ltx_font_italic">(IDC ’24)</em>. Association for Computing Machinery, New York, NY, USA, 307–321.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3628516.3655753" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3628516.3655753</a>

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">for Learning Analytics Research (NA)</span>
<span class="ltx_bibblock">
Society for Learning Analytics Research. N/A.

</span>
<span class="ltx_bibblock">Multimodal learning analytics across spaces (SOLAR crossmmla sig).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.solaresearch.org/community/sigs/crossmmla-sig/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.solaresearch.org/community/sigs/crossmmla-sig/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 07-02-2024].

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">for Learning Analytics Research  (SOLAR)</span>
<span class="ltx_bibblock">
Society for Learning Analytics Research (SOLAR). N/A.

</span>
<span class="ltx_bibblock">What is Learning Analytics?

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.solaresearch.org/about/what-is-learning-analytics/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.solaresearch.org/about/what-is-learning-analytics/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 07-02-2024].

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fwa, Hua Leong and Lindsay Marshall (2018)</span>
<span class="ltx_bibblock">
Fwa, Hua Leong and Lindsay Marshall. 2018.

</span>
<span class="ltx_bibblock">Investigating multimodal affect sensing in an Affective Tutoring System using unobtrusive sensors.

</span>
<span class="ltx_bibblock"><em id="bib.bib63.1.1" class="ltx_emph ltx_font_italic">Psychology of Programming Interest Group</em> 29 (Oct. 2018), 78–85.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span id="bib.bib64.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jing Gao, Peng Li, Zhikui Chen, and Jianing Zhang. 2020.

</span>
<span class="ltx_bibblock">A survey on deep learning for multimodal data fusion.

</span>
<span class="ltx_bibblock"><em id="bib.bib64.3.1" class="ltx_emph ltx_font_italic">Neural Computation</em> 32, 5 (2020), 829–864.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giannakos et al<span id="bib.bib65.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Michail Giannakos, Daniel Spikol, Daniele Di Mitri, Kshitij Sharma, Xavier Ochoa, and Rawad Hammad (Eds.). 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib65.3.1" class="ltx_emph ltx_font_italic">The multimodal learning analytics handbook</em>.

</span>
<span class="ltx_bibblock">Springer International Publishing, Cham.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/978-3-031-08076-0" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/978-3-031-08076-0</a>

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Giannakos et al<span id="bib.bib66.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Michail N. Giannakos, Kshitij Sharma, Ilias O. Pappas, Vassilis Kostakos, and Eduardo Velloso. 2019.

</span>
<span class="ltx_bibblock">Multimodal data as a means to understand the learning experience.

</span>
<span class="ltx_bibblock"><em id="bib.bib66.3.1" class="ltx_emph ltx_font_italic">International Journal of Information Management</em> 48 (Oct. 2019), 108–119.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.ijinfomgt.2019.02.003" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1016/j.ijinfomgt.2019.02.003</a>

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span id="bib.bib67.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Abhishek Gupta, Alagan Anpalagan, Ling Guan, and Ahmed Shaharyar Khwaja. 2021.

</span>
<span class="ltx_bibblock">Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues.

</span>
<span class="ltx_bibblock"><em id="bib.bib67.3.1" class="ltx_emph ltx_font_italic">Array</em> 10 (2021), 100057.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hagberg et al<span id="bib.bib68.2.2.1" class="ltx_text">.</span> (2008)</span>
<span class="ltx_bibblock">
Aric Hagberg, Pieter J. Swart, and Daniel A. Schult. 2008.

</span>
<span class="ltx_bibblock">Exploring network structure, dynamics, and function using NetworkX.

</span>
<span class="ltx_bibblock"><em id="bib.bib68.3.1" class="ltx_emph ltx_font_italic">N/A</em> N/A, N/A (1 2008), N/A.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://www.osti.gov/biblio/960616" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.osti.gov/biblio/960616</a>

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hattie and Timperley (2007)</span>
<span class="ltx_bibblock">
John Hattie and Helen Timperley. 2007.

</span>
<span class="ltx_bibblock">The Power of Feedback.

</span>
<span class="ltx_bibblock"><em id="bib.bib69.1.1" class="ltx_emph ltx_font_italic">Review of Educational Research</em> 77, 1 (2007), 81–112.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.3102/003465430298487" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.3102/003465430298487</a>
arXiv:https://doi.org/10.3102/003465430298487

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Henderson et al<span id="bib.bib70.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Nathan L Henderson, Jonathan P Rowe, Bradford W Mott, and James C Lester. 2019.

</span>
<span class="ltx_bibblock">Sensor-based Data Fusion for Multimodal Affect Detection in Game-based Learning Environments. In <em id="bib.bib70.3.1" class="ltx_emph ltx_font_italic">Proceedings of the EDM and Games Workshop at the 12th International Conference on Educational Data Mining</em>, Vol. 2592. International Educational Data Mining Society, Montreal, CA, 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em id="bib.bib71.1.1" class="ltx_emph ltx_font_italic">Neural computation</em> 9, 8 (1997), 1735–1780.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoppe (2017)</span>
<span class="ltx_bibblock">
H Ulrich Hoppe. 2017.

</span>
<span class="ltx_bibblock"><em id="bib.bib72.1.1" class="ltx_emph ltx_font_italic">Computational methods for the analysis of learning and knowledge building communities</em>.

</span>
<span class="ltx_bibblock">Society for Learning Analytics Research (SoLAR), Beaumont, Alberta, Canada, 23–33.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutchins and Biswas (2023)</span>
<span class="ltx_bibblock">
Nicole Hutchins and Gautam Biswas. 2023.

</span>
<span class="ltx_bibblock">Using Teacher Dashboards to Customize Lesson Plans for a Problem-Based, Middle School STEM Curriculum. In <em id="bib.bib73.1.1" class="ltx_emph ltx_font_italic">LAK23: 13th International Learning Analytics and Knowledge Conference</em> (Arlington, TX, USA) <em id="bib.bib73.2.2" class="ltx_emph ltx_font_italic">(LAK2023)</em>. Association for Computing Machinery, New York, NY, USA, 324–332.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3576050.3576100" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3576050.3576100</a>

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hutchins et al<span id="bib.bib74.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Nicole Marie Hutchins et al<span id="bib.bib74.3.1" class="ltx_text">.</span> 2022.

</span>
<span class="ltx_bibblock"><em id="bib.bib74.4.1" class="ltx_emph ltx_font_italic">Co-Designing Teaching Augmentation Tools to Support the Integration of Problem-Based Learning in K-12 Science</em>.

</span>
<span class="ltx_bibblock">Ph. D. Dissertation. Vanderbilt University.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span id="bib.bib75.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Shiyan Jiang, Blaine E. Smith, and Ji Shen. 2021.

</span>
<span class="ltx_bibblock">Examining how different modes mediate adolescents’ interactions during their collaborative multimodal composing processes.

</span>
<span class="ltx_bibblock"><em id="bib.bib75.3.1" class="ltx_emph ltx_font_italic">Interactive Learning Environments</em> 29, 5 (July 2021), 807–820.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1080/10494820.2019.1612450" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1080/10494820.2019.1612450</a>

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Järvelä et al<span id="bib.bib76.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Sanna Järvelä, Jonna Malmberg, Eetu Haataja, Marta Sobocinski, and Paul A. Kirschner. 2021.

</span>
<span class="ltx_bibblock">What multimodal data can tell us about the students’ regulation of their learning process?

</span>
<span class="ltx_bibblock"><em id="bib.bib76.3.1" class="ltx_emph ltx_font_italic">Learning and Instruction</em> 72 (April 2021), 101203.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.learninstruc.2019.04.004" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1016/j.learninstruc.2019.04.004</a>

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kadam and Vaidya (2020)</span>
<span class="ltx_bibblock">
Suvarna Kadam and Vinay Vaidya. 2020.

</span>
<span class="ltx_bibblock">Review and analysis of zero, one and few shot learning approaches. In <em id="bib.bib77.1.1" class="ltx_emph ltx_font_italic">Intelligent Systems Design and Applications: 18th International Conference on Intelligent Systems Design and Applications (ISDA 2018) held in Vellore, India, December 6-8, 2018, Volume 1</em>. Springer, N/A, N/A, 100–112.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kitchenham (2004)</span>
<span class="ltx_bibblock">
Barbara Kitchenham. 2004.

</span>
<span class="ltx_bibblock">Procedures for performing systematic reviews.

</span>
<span class="ltx_bibblock"><em id="bib.bib78.1.1" class="ltx_emph ltx_font_italic">Keele, UK, Keele University</em> 33, 2004 (2004), 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koedinger et al<span id="bib.bib79.2.2.1" class="ltx_text">.</span> (1997)</span>
<span class="ltx_bibblock">
Kenneth R Koedinger, John R Anderson, William H Hadley, Mary A Mark, et al<span id="bib.bib79.3.1" class="ltx_text">.</span> 1997.

</span>
<span class="ltx_bibblock">Intelligent tutoring goes to school in the big city.

</span>
<span class="ltx_bibblock"><em id="bib.bib79.4.1" class="ltx_emph ltx_font_italic">International Journal of Artificial Intelligence in Education</em> 8, 1 (1997), 30–43.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kubsch et al<span id="bib.bib80.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Marcus Kubsch, Daniela Caballero, and Pablo Uribe. 2022.

</span>
<span class="ltx_bibblock">Once More with Feeling: Emotions in Multimodal Learning Analytics.

</span>
<span class="ltx_bibblock">In <em id="bib.bib80.3.1" class="ltx_emph ltx_font_italic">The Multimodal Learning Analytics Handbook</em>, Michail Giannakos, Daniel Spikol, Daniele Di Mitri, Kshitij Sharma, Xavier Ochoa, and Rawad Hammad (Eds.). Springer International Publishing, Cham, 261–285.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://link.springer.com/10.1007/978-3-031-08076-0_11" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://link.springer.com/10.1007/978-3-031-08076-0˙11</a>

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Larmuseau et al<span id="bib.bib81.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Charlotte Larmuseau, Jan Cornelis, Luigi Lancieri, Piet Desmet, and Fien Depaepe. 2020.

</span>
<span class="ltx_bibblock">Multimodal learning analytics to investigate cognitive load during online problem solving.

</span>
<span class="ltx_bibblock"><em id="bib.bib81.3.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (Sept. 2020), 1548–1562.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.12958" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.12958</a>

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee-Cultura et al<span id="bib.bib82.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Serena Lee-Cultura, Kshitij Sharma, Giulia Cosentino, Sofia Papavlasopoulou, and Michail Giannakos. 2021.

</span>
<span class="ltx_bibblock">Children’s Play and Problem Solving in Motion-Based Educational Games: Synergies between Human Annotations and Multi-Modal Data. In <em id="bib.bib82.3.1" class="ltx_emph ltx_font_italic">Interaction Design and Children</em>. ACM, Athens Greece, 408–420.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3459990.3460702" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3459990.3460702</a>

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee-Cultura et al<span id="bib.bib83.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Serena Lee-Cultura, Kshitij Sharma, and Michail Giannakos. 2022.

</span>
<span class="ltx_bibblock">Children’s play and problem-solving in motion-based learning technologies using a multi-modal mixed methods approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib83.3.1" class="ltx_emph ltx_font_italic">International Journal of Child-Computer Interaction</em> 31 (March 2022), 100355.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.ijcci.2021.100355" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1016/j.ijcci.2021.100355</a>

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee-Cultura et al<span id="bib.bib84.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Serena Lee-Cultura, Kshitij Sharma, Sofia Papavlasopoulou, and Michail Giannakos. 2020.

</span>
<span class="ltx_bibblock">Motion-Based Educational Games: Using Multi-Modal Data to Predict Player’s Performance. In <em id="bib.bib84.3.1" class="ltx_emph ltx_font_italic">2020 IEEE Conference on Games (CoG)</em>. IEEE, Osaka, Japan, 17–24.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/CoG47356.2020.9231892" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/CoG47356.2020.9231892</a>

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leelawong and Biswas (2008)</span>
<span class="ltx_bibblock">
Krittaya Leelawong and Gautam Biswas. 2008.

</span>
<span class="ltx_bibblock">Designing learning by teaching agents: The Betty’s Brain system.

</span>
<span class="ltx_bibblock"><em id="bib.bib85.1.1" class="ltx_emph ltx_font_italic">International Journal of Artificial Intelligence in Education</em> 18, 3 (2008), 181–208.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib86.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Ran Liu, John Stamper, Jodi Davenport, Scott Crossley, Danielle McNamara, Kalonji Nzinga, and Bruce Sherin. 2019.

</span>
<span class="ltx_bibblock">Learning linkages: Integrating data streams of multiple modalities and timescales.

</span>
<span class="ltx_bibblock"><em id="bib.bib86.3.1" class="ltx_emph ltx_font_italic">Journal of Computer Assisted Learning</em> 35, 1 (Feb. 2019), 99–109.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/jcal.12315" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/jcal.12315</a>

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib87.2.2.1" class="ltx_text">.</span> (2018b)</span>
<span class="ltx_bibblock">
Ran Liu, John C Stamper, and Jodi Davenport. 2018b.

</span>
<span class="ltx_bibblock">A Novel Method for the In-Depth Multimodal Analysis of Student Learning Trajectories in Intelligent Tutoring Systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib87.3.1" class="ltx_emph ltx_font_italic">Journal of Learning Analytics</em> 5, 1 (April 2018), 41–54.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.18608/jla.2018.51.4" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.18608/jla.2018.51.4</a>

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span id="bib.bib88.2.2.1" class="ltx_text">.</span> (2018a)</span>
<span class="ltx_bibblock">
Su Liu, Ye Chen, Hui Huang, Liang Xiao, and Xiaojun Hei. 2018a.

</span>
<span class="ltx_bibblock">Towards Smart Educational Recommendations with Reinforcement Learning in Classroom. In <em id="bib.bib88.3.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Teaching, Assessment, and Learning for Engineering (TALE)</em>. IEEE, Wollongong, NSW, 1079–1084.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TALE.2018.8615217" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/TALE.2018.8615217</a>

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loper and Bird (2002)</span>
<span class="ltx_bibblock">
Edward Loper and Steven Bird. 2002.

</span>
<span class="ltx_bibblock">NLTK: The Natural Language Toolkit.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/ARXIV.CS/0205028" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.48550/ARXIV.CS/0205028</a>

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">López et al<span id="bib.bib90.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
María Ximena López, Francesco Strada, Andrea Bottino, and Carlo Fabricatore. 2021.

</span>
<span class="ltx_bibblock">Using Multimodal Learning Analytics to Explore Collaboration in a Sustainability Co-located Tabletop Game. In <em id="bib.bib90.3.1" class="ltx_emph ltx_font_italic">15th European Conference on Game-Based Learning</em>. Academic Conferences LTD, Brighton, UK, 482–489.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span id="bib.bib91.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Yingbo Ma, Mehmet Celepkolu, and Kristy Elizabeth Boyer. 2022.

</span>
<span class="ltx_bibblock">Detecting Impasse During Collaborative Problem Solving with Multimodal Learning Analytics. In <em id="bib.bib91.3.1" class="ltx_emph ltx_font_italic">LAK22: 12th International Learning Analytics and Knowledge Conference</em>. ACM, Online USA, 45–55.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3506860.3506865" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3506860.3506865</a>

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangaroska et al<span id="bib.bib92.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Katerina Mangaroska, Kshitij Sharma, Dragan Gašević, and Michalis Giannakos. 2020.

</span>
<span class="ltx_bibblock">Multimodal Learning Analytics to Inform Learning Design: Lessons Learned from Computing Education.

</span>
<span class="ltx_bibblock"><em id="bib.bib92.3.1" class="ltx_emph ltx_font_italic">Journal of Learning Analytics</em> 7, 3 (Dec. 2020), 79–97.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.18608/jla.2020.73.7" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.18608/jla.2020.73.7</a>

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martin et al<span id="bib.bib93.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Kit Martin, Emily Q. Wang, Connor Bain, and Marcelo Worsley. 2019.

</span>
<span class="ltx_bibblock">Computationally Augmented Ethnography: Emotion Tracking and Learning in Museum Games.

</span>
<span class="ltx_bibblock">In <em id="bib.bib93.3.1" class="ltx_emph ltx_font_italic">Advances in Quantitative Ethnography</em>, Brendan Eagan, Morten Misfeldt, and Amanda Siebert-Evenstone (Eds.). Vol. 1112. Springer International Publishing, Cham, 141–153.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://link.springer.com/10.1007/978-3-030-33232-7_12" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://link.springer.com/10.1007/978-3-030-33232-7˙12</a>

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez-Maldonado et al<span id="bib.bib94.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Roberto Martinez-Maldonado, Vanessa Echeverria, Gloria Fernandez Nieto, and Simon Buckingham Shum. 2020.

</span>
<span class="ltx_bibblock">From Data to Insights: A Layered Storytelling Approach for Multimodal Learning Analytics. In <em id="bib.bib94.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems</em>. ACM, Honolulu HI USA, 1–15.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3313831.3376148" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3313831.3376148</a>

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maseleno et al<span id="bib.bib95.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Andino Maseleno, Noraisikin Sabani, Miftachul Huda, Roslee Bin Ahmad, Kamarul Azmi Jasmi, and Bushrah Basiron. 2018.

</span>
<span class="ltx_bibblock">Demystifying learning analytics in personalised learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib95.3.1" class="ltx_emph ltx_font_italic">International Journal of Engineering and Technology (UAE)</em> 7 (2018), 1124–1129.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mat Sanusi et al<span id="bib.bib96.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Khaleel Asyraaf Mat Sanusi, Daniele Di Mitri, Bibeg Limbu, and Roland Klemke. 2021.

</span>
<span class="ltx_bibblock">Table Tennis Tutor: Forehand Strokes Classification Based on Multimodal Data and Neural Networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib96.3.1" class="ltx_emph ltx_font_italic">Sensors</em> 21, 9 (April 2021), 3121.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/s21093121" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.3390/s21093121</a>

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MDPI (2021)</span>
<span class="ltx_bibblock">
MDPI. 2021.

</span>
<span class="ltx_bibblock">New Trends on Multimodal Learning Analytics: Using Sensors to Understand and Improve Learning.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.mdpi.com/journal/sensors/special_issues/multimodal_learning_analytics_sensor" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.mdpi.com/journal/sensors/special˙issues/multimodal˙learning˙analytics˙sensor</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 08-02-2024].

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehra (2015)</span>
<span class="ltx_bibblock">
Beloo Mehra. 2015.

</span>
<span class="ltx_bibblock">Bias in Qualitative Research: Voices from an Online Classroom.

</span>
<span class="ltx_bibblock"><em id="bib.bib98.1.1" class="ltx_emph ltx_font_italic">The Qualitative Report</em> N/A, N/A (Jan 2015), N/A pages.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.46743/2160-3715/2002.1986" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.46743/2160-3715/2002.1986</a>

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mitri (2019)</span>
<span class="ltx_bibblock">
Daniele Di Mitri. 2019.

</span>
<span class="ltx_bibblock">Detecting Medical Simulation Errors with Machine learning and Multimodal Data. In <em id="bib.bib99.1.1" class="ltx_emph ltx_font_italic">17th Conference on Artificial Intelligence in Medicine</em>. Springer International Publishing, Poznan, Poland, 1–6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Morell et al<span id="bib.bib100.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Teresa Morell, Vicent Beltrán-Palanques, and Natalia Norte. 2022.

</span>
<span class="ltx_bibblock">A multimodal analysis of pair work engagement episodes: Implications for EMI lecturer training.

</span>
<span class="ltx_bibblock"><em id="bib.bib100.3.1" class="ltx_emph ltx_font_italic">Journal of English for Academic Purposes</em> 58 (July 2022), 101124.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.jeap.2022.101124" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1016/j.jeap.2022.101124</a>

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mu et al<span id="bib.bib101.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Su Mu, Meng Cui, and Xiaodi Huang. 2020.

</span>
<span class="ltx_bibblock">Multimodal Data Fusion in Learning Analytics: A Systematic Review.

</span>
<span class="ltx_bibblock"><em id="bib.bib101.3.1" class="ltx_emph ltx_font_italic">Sensors</em> 20, 23 (2020), 6856.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/s20236856" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.3390/s20236856</a>

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasir et al<span id="bib.bib102.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Jauwairia Nasir, Aditi Kothiyal, Barbara Bruno, and Pierre Dillenbourg. 2021.

</span>
<span class="ltx_bibblock">Many are the ways to learn identifying multi-modal behavioral profiles of collaborative learning in constructivist activities.

</span>
<span class="ltx_bibblock"><em id="bib.bib102.3.1" class="ltx_emph ltx_font_italic">International Journal of Computer-Supported Collaborative Learning</em> 16, 4 (Dec. 2021), 485–523.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s11412-021-09358-2" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/s11412-021-09358-2</a>

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al<span id="bib.bib103.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Andy Nguyen, Sanna Järvelä, Carolyn Rosé, Hanna Järvenoja, and Jonna Malmberg. 2023.

</span>
<span class="ltx_bibblock">Examining socially shared regulation and shared physiological arousal events with multimodal learning analytics.

</span>
<span class="ltx_bibblock"><em id="bib.bib103.3.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 54, 1 (Jan. 2023), 293–312.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.13280" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.13280</a>

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noble and Smith (2015)</span>
<span class="ltx_bibblock">
Helen Noble and Joanna Smith. 2015.

</span>
<span class="ltx_bibblock">Issues of validity and reliability in qualitative research.

</span>
<span class="ltx_bibblock"><em id="bib.bib104.1.1" class="ltx_emph ltx_font_italic">Evidence Based Nursing</em> 18, 2 (Feb. 2015), 34–35.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1136/eb-2015-102054" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1136/eb-2015-102054</a>

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noel et al<span id="bib.bib105.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Rene Noel, Fabian Riquelme, Roberto Mac Lean, Erick Merino, Cristian Cechinel, Thiago S. Barcelos, Rodolfo Villarroel, and Roberto Munoz. 2018.

</span>
<span class="ltx_bibblock">Exploring Collaborative Writing of User Stories With Multimodal Learning Analytics: A Case Study on a Software Engineering Course.

</span>
<span class="ltx_bibblock"><em id="bib.bib105.3.1" class="ltx_emph ltx_font_italic">IEEE Access</em> 6 (2018), 67783–67798.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/ACCESS.2018.2876801" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/ACCESS.2018.2876801</a>

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Noël et al<span id="bib.bib106.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
René Noël, Diego Miranda, Cristian Cechinel, Fabián Riquelme, Tiago Thompsen Primo, and Roberto Munoz. 2022.

</span>
<span class="ltx_bibblock">Visualizing Collaboration in Teamwork: A Multimodal Learning Analytics Platform for Non-Verbal Communication.

</span>
<span class="ltx_bibblock"><em id="bib.bib106.3.1" class="ltx_emph ltx_font_italic">Applied Sciences</em> 12, 15 (July 2022), 7499.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.3390/app12157499" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.3390/app12157499</a>

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ochoa and Dominguez (2020)</span>
<span class="ltx_bibblock">
Xavier Ochoa and Federico Dominguez. 2020.

</span>
<span class="ltx_bibblock">Controlled evaluation of a multimodal system to improve oral presentation skills in a real learning setting.

</span>
<span class="ltx_bibblock"><em id="bib.bib107.1.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (Sept. 2020), 1615–1630.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.12987" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.12987</a>

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ochoa et al<span id="bib.bib108.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Xavier Ochoa, Federico Domínguez, Bruno Guamán, Ricardo Maya, Gabriel Falcones, and Jaime Castells. 2018.

</span>
<span class="ltx_bibblock">The RAP system: automatic feedback of oral presentation skills using multimodal analysis and low-cost sensors. In <em id="bib.bib108.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th International Conference on Learning Analytics and Knowledge</em>. ACM, Sydney New South Wales Australia, 360–364.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3170358.3170406" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3170358.3170406</a>

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ochoa et al<span id="bib.bib109.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Xavier Ochoa, AWDG Charles Lang, and George Siemens. 2017.

</span>
<span class="ltx_bibblock">Multimodal learning analytics.

</span>
<span class="ltx_bibblock"><em id="bib.bib109.3.1" class="ltx_emph ltx_font_italic">The handbook of learning analytics</em> 1 (2017), 129–141.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">of Learning Analytics (2015)</span>
<span class="ltx_bibblock">
Journal of Learning Analytics. 2015.

</span>
<span class="ltx_bibblock">Special section on multimodal learning analytics.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://learning-analytics.info/index.php/JLA/announcement/view/102" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://learning-analytics.info/index.php/JLA/announcement/view/102</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 08-02-2024].

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olsen et al<span id="bib.bib111.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Jennifer K. Olsen, Kshitij Sharma, Nikol Rummel, and Vincent Aleven. 2020.

</span>
<span class="ltx_bibblock">Temporal analysis of multimodal data to predict collaborative learning outcomes.

</span>
<span class="ltx_bibblock"><em id="bib.bib111.3.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (Sept. 2020), 1527–1547.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.12982" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.12982</a>

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI. 2023.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock"><em id="bib.bib112.1.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> N/A, N/A, Article arXiv:2303.08774 (March 2023), N/A pages.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2303.08774" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.48550/arXiv.2303.08774</a>
arXiv:2303.08774 [cs.CL]

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papamitsiou et al<span id="bib.bib113.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Zacharoula Papamitsiou, Ilias O. Pappas, Kshitij Sharma, and Michail N. Giannakos. 2020.

</span>
<span class="ltx_bibblock">Utilizing Multimodal Data Through fsQCA to Explain Engagement in Adaptive Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib113.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Learning Technologies</em> 13, 4 (Oct. 2020), 689–703.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TLT.2020.3020499" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/TLT.2020.3020499</a>

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Penuel et al<span id="bib.bib114.2.2.1" class="ltx_text">.</span> (2007)</span>
<span class="ltx_bibblock">
William R. Penuel, Jeremy Roschelle, and Nicole Shechtman. 2007.

</span>
<span class="ltx_bibblock">Designing Formative Assessment Software With Teachers: An Analysis of the Co-Design Process.

</span>
<span class="ltx_bibblock"><em id="bib.bib114.3.1" class="ltx_emph ltx_font_italic">Research and Practice in Technology Enhanced Learning</em> 02, 01 (2007), 51–74.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1142/S1793206807000300" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1142/S1793206807000300</a>
arXiv:https://doi.org/10.1142/S1793206807000300

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petukhova et al<span id="bib.bib115.2.2.1" class="ltx_text">.</span> (2017a)</span>
<span class="ltx_bibblock">
Volha Petukhova, Tobias Mayer, Andrei Malchanau, and Harry Bunt. 2017a.

</span>
<span class="ltx_bibblock">Virtual debate coach design: assessing multimodal argumentation performance. In <em id="bib.bib115.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 19th ACM International Conference on Multimodal Interaction</em>. ACM, Glasgow UK, 41–50.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3136755.3136775" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3136755.3136775</a>

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petukhova et al<span id="bib.bib116.2.2.1" class="ltx_text">.</span> (2017b)</span>
<span class="ltx_bibblock">
Volha Petukhova, Manoj Raju, and Harry Bunt. 2017b.

</span>
<span class="ltx_bibblock">Multimodal Markers of Persuasive Speech: Designing a Virtual Debate Coach. In <em id="bib.bib116.3.1" class="ltx_emph ltx_font_italic">Interspeech 2017</em>. ISCA, Stockholm, Sweden, 142–146.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.21437/Interspeech.2017-98" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.21437/Interspeech.2017-98</a>

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham and Wang (2017)</span>
<span class="ltx_bibblock">
Phuong Pham and Jingtao Wang. 2017.

</span>
<span class="ltx_bibblock">AttentiveLearner2: A Multimodal Approach for Improving MOOC Learning on Mobile Devices.

</span>
<span class="ltx_bibblock">In <em id="bib.bib117.1.1" class="ltx_emph ltx_font_italic">Artificial Intelligence in Education</em>, Elisabeth André, Ryan Baker, Xiangen Hu, Ma. Mercedes T. Rodrigo, and Benedict Du Boulay (Eds.). Vol. 10331. Springer International Publishing, Cham, 561–564.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://link.springer.com/10.1007/978-3-319-61425-0_64" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://link.springer.com/10.1007/978-3-319-61425-0˙64</a>

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pham and Wang (2018)</span>
<span class="ltx_bibblock">
Phuong Pham and Jingtao Wang. 2018.

</span>
<span class="ltx_bibblock">Predicting Learners’ Emotions in Mobile MOOC Learning via a Multimodal Intelligent Tutor.

</span>
<span class="ltx_bibblock">In <em id="bib.bib118.1.1" class="ltx_emph ltx_font_italic">Intelligent Tutoring Systems</em>, Roger Nkambou, Roger Azevedo, and Julita Vassileva (Eds.). Vol. 10858. Springer International Publishing, Cham, 150–159.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://link.springer.com/10.1007/978-3-319-91464-0_15" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://link.springer.com/10.1007/978-3-319-91464-0˙15</a>

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Philippe et al<span id="bib.bib119.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Stéphanie Philippe, Alexis D. Souchet, Petros Lameras, Panagiotis Petridis, Julien Caporal, Gildas Coldeboeuf, and Hadrien Duzan. 2020.

</span>
<span class="ltx_bibblock">Multimodal teaching, learning and training in virtual reality: a review and case study.

</span>
<span class="ltx_bibblock"><em id="bib.bib119.3.1" class="ltx_emph ltx_font_italic">Virtual Reality &amp; Intelligent Hardware</em> 2, 5 (Oct. 2020), 421–442.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1016/j.vrih.2020.07.008" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1016/j.vrih.2020.07.008</a>

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Prieto et al<span id="bib.bib120.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
L.P. Prieto, K. Sharma, Ł. Kidzinski, M.J. Rodríguez-Triana, and P. Dillenbourg. 2018.

</span>
<span class="ltx_bibblock">Multimodal teaching analytics: Automated extraction of orchestration graphs from wearable sensor data.

</span>
<span class="ltx_bibblock"><em id="bib.bib120.3.1" class="ltx_emph ltx_font_italic">Journal of Computer Assisted Learning</em> 34, 2 (April 2018), 193–203.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/jcal.12232" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/jcal.12232</a>

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Psaltis et al<span id="bib.bib121.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Athanasios Psaltis, Konstantinos C. Apostolakis, Kosmas Dimitropoulos, and Petros Daras. 2018.

</span>
<span class="ltx_bibblock">Multimodal Student Engagement Recognition in Prosocial Games.

</span>
<span class="ltx_bibblock"><em id="bib.bib121.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Games</em> 10, 3 (Sept. 2018), 292–303.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TCIAIG.2017.2743341" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/TCIAIG.2017.2743341</a>

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qushem (2020)</span>
<span class="ltx_bibblock">
Umar Bin Qushem. 2020.

</span>
<span class="ltx_bibblock"><em id="bib.bib122.1.1" class="ltx_emph ltx_font_italic">Trends of Multimodal Learning Analytics: A Systematic Literature Review</em>.

</span>
<span class="ltx_bibblock">Ph. D. Dissertation. UNIVERSITY OF EASTERN FINLAND.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://erepo.uef.fi/bitstream/handle/123456789/23508/urn_nbn_fi_uef-20201250.pdf?sequence=1" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://erepo.uef.fi/bitstream/handle/123456789/23508/urn˙nbn˙fi˙uef-20201250.pdf?sequence=1</a>

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span id="bib.bib123.2.2.1" class="ltx_text">.</span> (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al<span id="bib.bib123.3.1" class="ltx_text">.</span> 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib123.4.1" class="ltx_emph ltx_font_italic">OpenAI blog</em> 1, 8 (2019), 9.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reilly et al<span id="bib.bib124.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Joseph M Reilly, Milan Ravenell, and Bertrand Schneider. 2018.

</span>
<span class="ltx_bibblock">Exploring Collaboration Using Motion Sensors and Multi- Modal Learning Analytics. In <em id="bib.bib124.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th International Conference on Educational Data Mining</em>. International Educational Data Mining Society, Buffalo, NY, USA, 333–339.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodríguez-Triana et al<span id="bib.bib125.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
María Jesüs Rodríguez-Triana, Luis P Prieto, Alejandra Martínez-Monés, Juan I Asensio-Pérez, and Yannis Dimitriadis. 2018.

</span>
<span class="ltx_bibblock">The teacher in the loop: Customizing multimodal learning analytics for blended learning. In <em id="bib.bib125.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th international conference on learning analytics and knowledge</em>. Association for Computing Machinery, New York, NY, USA, 417–426.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rojat et al<span id="bib.bib126.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Thomas Rojat, Raphaël Puget, David Filliat, Javier Del Ser, Rodolphe Gelin, and Natalia Díaz-Rodríguez. 2021.

</span>
<span class="ltx_bibblock">Explainable artificial intelligence (xai) on timeseries data: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bib126.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.00950</em> N/A, N/A (2021), N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarmiento and Wise (2022)</span>
<span class="ltx_bibblock">
Juan Pablo Sarmiento and Alyssa Friend Wise. 2022.

</span>
<span class="ltx_bibblock">Participatory and Co-Design of Learning Analytics: An Initial Review of the Literature. In <em id="bib.bib127.1.1" class="ltx_emph ltx_font_italic">LAK22: 12th International Learning Analytics and Knowledge Conference</em> (Online, USA) <em id="bib.bib127.2.2" class="ltx_emph ltx_font_italic">(LAK22)</em>. Association for Computing Machinery, New York, NY, USA, 535–541.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3506860.3506910" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3506860.3506910</a>

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schoonderwoerd et al<span id="bib.bib128.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Tjeerd AJ Schoonderwoerd, Wiard Jorritsma, Mark A Neerincx, and Karel Van Den Bosch. 2021.

</span>
<span class="ltx_bibblock">Human-centered XAI: Developing design patterns for explanations of clinical decision support systems.

</span>
<span class="ltx_bibblock"><em id="bib.bib128.3.1" class="ltx_emph ltx_font_italic">International Journal of Human-Computer Studies</em> 154 (2021), 102684.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuler and Namioka (1993)</span>
<span class="ltx_bibblock">
Douglas Schuler and Aki Namioka. 1993.

</span>
<span class="ltx_bibblock"><em id="bib.bib129.1.1" class="ltx_emph ltx_font_italic">Participatory design: Principles and practices</em>.

</span>
<span class="ltx_bibblock">CRC Press, N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">SerpApi (NA)</span>
<span class="ltx_bibblock">
SerpApi. N/A.

</span>
<span class="ltx_bibblock">Google Scholar API.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://serpapi.com/google-scholar-api" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://serpapi.com/google-scholar-api</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 08-02-2024].

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shankar et al<span id="bib.bib131.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Shashi Kant Shankar, Luis P. Prieto, María Jesús Rodríguez-Triana, and Adolfo Ruiz-Calleja. 2018.

</span>
<span class="ltx_bibblock">A Review of Multimodal Learning Analytics Architectures. In <em id="bib.bib131.3.1" class="ltx_emph ltx_font_italic">2018 IEEE 18th International Conference on Advanced Learning Technologies (ICALT)</em>. IEEE, Piscataway, NJ, USA, 212–214.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1109/ICALT.2018.00057" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/ICALT.2018.00057</a>

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma and Giannakos (2020)</span>
<span class="ltx_bibblock">
Kshitij Sharma and Michail Giannakos. 2020.

</span>
<span class="ltx_bibblock">Multimodal data capabilities for learning: What can multimodal data tell us about learning?

</span>
<span class="ltx_bibblock"><em id="bib.bib132.1.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (2020), 1450–1484.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.12993" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.12993</a>

</span>
<span class="ltx_bibblock">_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/bjet.12993.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma et al<span id="bib.bib133.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Kshitij Sharma, Zacharoula Papamitsiou, Jennifer K. Olsen, and Michail Giannakos. 2020.

</span>
<span class="ltx_bibblock">Predicting learners’ effortful behaviour in adaptive assessment using multimodal data. In <em id="bib.bib133.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge</em>. ACM, Frankfurt Germany, 480–489.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3375462.3375498" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3375462.3375498</a>

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snyder et al<span id="bib.bib134.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Caitlin Snyder, Nicole Hutchins, Clayton Cohn, Joyce Fonteles, and Gautam Biswas. 2023.

</span>
<span class="ltx_bibblock">Using Collaborative Interactivity Metrics to analyze students’ Problem-Solving Behaviors during STEM+C Computational Modeling Tasks. (2023).

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">Submitted to Learning and Individual Differences. Currently under review.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snyder et al<span id="bib.bib135.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Caitlin Snyder, Nicole M Hutchins, Clayton Cohn, Joyce Horn Fonteles, and Gautam Biswas. 2024.

</span>
<span class="ltx_bibblock">Analyzing Students Collaborative Problem-Solving Behaviors in Synergistic STEM+C Learning. In <em id="bib.bib135.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 14th Learning Analytics and Knowledge Conference</em> (Kyoto, Japan) <em id="bib.bib135.4.2" class="ltx_emph ltx_font_italic">(LAK ’24)</em>. Association for Computing Machinery, New York, NY, USA, 540–550.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3636555.3636912" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3636555.3636912</a>

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Software (2024)</span>
<span class="ltx_bibblock">
Rustici Software. 2024.

</span>
<span class="ltx_bibblock">xAPI.com — xapi.com.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://xapi.com/?utm_source=google&amp;utm_medium=natural_search" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://xapi.com/?utm˙source=google&amp;utm˙medium=natural˙search</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 25-01-2024].

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spikol et al<span id="bib.bib137.2.2.1" class="ltx_text">.</span> (2017a)</span>
<span class="ltx_bibblock">
Daniel Spikol, Emanuele Ruffaldi, and Mutlu Cukurova. 2017a.

</span>
<span class="ltx_bibblock">Using Multimodal Learning Analytics to Identify Aspects of Collaboration in Project-Based Learning. In <em id="bib.bib137.3.1" class="ltx_emph ltx_font_italic">Making a Difference: Prioritizing Equity and Access in CSCL</em>, Vol. 1. International Society of the Learning Sciences, Philadelphia, PA USA, 263–270.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spikol et al<span id="bib.bib138.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Daniel Spikol, Emanuele Ruffaldi, Giacomo Dabisias, and Mutlu Cukurova. 2018.

</span>
<span class="ltx_bibblock">Supervised machine learning in multimodal learning analytics for estimating success in project-based learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib138.3.1" class="ltx_emph ltx_font_italic">Journal of Computer Assisted Learning</em> 34, 4 (Aug. 2018), 366–377.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/jcal.12263" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/jcal.12263</a>

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spikol et al<span id="bib.bib139.2.2.1" class="ltx_text">.</span> (2017b)</span>
<span class="ltx_bibblock">
Daniel Spikol, Emanuele Ruffaldi, Lorenzo Landolfi, and Mutlu Cukurova. 2017b.

</span>
<span class="ltx_bibblock">Estimation of Success in Collaborative Learning Based on Multimodal Learning Analytics Features. In <em id="bib.bib139.3.1" class="ltx_emph ltx_font_italic">2017 IEEE 17th International Conference on Advanced Learning Technologies (ICALT)</em>. IEEE, Timisoara, Romania, 269–273.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/ICALT.2017.122" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/ICALT.2017.122</a>

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Standen et al<span id="bib.bib140.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Penelope J. Standen, David J. Brown, Mohammad Taheri, Maria J. Galvez Trigo, Helen Boulton, Andrew Burton, Madeline J. Hallewell, James G. Lathe, Nicholas Shopland, Maria A. Blanco Gonzalez, Gosia M. Kwiatkowska, Elena Milli, Stefano Cobello, Annaleda Mazzucato, Marco Traversi, and Enrique Hortal. 2020.

</span>
<span class="ltx_bibblock">An evaluation of an adaptive learning system based on multimodal affect recognition for learners with intellectual disabilities.

</span>
<span class="ltx_bibblock"><em id="bib.bib140.3.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (Sept. 2020), 1748–1765.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.13010" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.13010</a>

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Starr et al<span id="bib.bib141.2.2.1" class="ltx_text">.</span> (2018)</span>
<span class="ltx_bibblock">
Emma L Starr, Joseph M Reilly, and Bertrand Schneider. 2018.

</span>
<span class="ltx_bibblock">Toward Using Multi-Modal Learning Analytics to Support and Measure Collaboration in Co-Located Dyads. In <em id="bib.bib141.3.1" class="ltx_emph ltx_font_italic">ICLS 2018</em>. International Society of the Learning Sciences, London, UK, 448–455.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stolz (2015)</span>
<span class="ltx_bibblock">
Steven A. Stolz. 2015.

</span>
<span class="ltx_bibblock">Embodied Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib142.1.1" class="ltx_emph ltx_font_italic">Educational Philosophy and Theory</em> 47, 5 (2015), 474–487.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1080/00131857.2013.879694" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1080/00131857.2013.879694</a>

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sümer et al<span id="bib.bib143.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Ömer Sümer, Patricia Goldberg, Sidney D’Mello, Peter Gerjets, Ulrich Trautwein, and Enkelejda Kasneci. 2023.

</span>
<span class="ltx_bibblock">Multimodal Engagement Analysis From Facial Videos in the Classroom.

</span>
<span class="ltx_bibblock"><em id="bib.bib143.3.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Affective Computing</em> 14, 2 (April 2023), 1012–1027.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1109/TAFFC.2021.3127692" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1109/TAFFC.2021.3127692</a>

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tanaka et al<span id="bib.bib144.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Hiroki Tanaka, Hideki Negoro, Hidemi Iwasaka, and Satoshi Nakamura. 2017.

</span>
<span class="ltx_bibblock">Embodied conversational agents for multimodal automated social skills training in people with autism spectrum disorders.

</span>
<span class="ltx_bibblock"><em id="bib.bib144.3.1" class="ltx_emph ltx_font_italic">PLOS ONE</em> 12, 8 (Aug. 2017), e0182151.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1371/journal.pone.0182151" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1371/journal.pone.0182151</a>

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tancredi et al<span id="bib.bib145.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Sofia Tancredi, Rotem Abdu, Ramesh Balasubramaniam, and Dor Abrahamson. 2022.

</span>
<span class="ltx_bibblock">Intermodality in Multimodal Learning Analytics for Cognitive Theory Development: A Case from Embodied Design for Mathematics Learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib145.3.1" class="ltx_emph ltx_font_italic">The Multimodal Learning Analytics Handbook</em>, Michail Giannakos, Daniel Spikol, Daniele Di Mitri, Kshitij Sharma, Xavier Ochoa, and Rawad Hammad (Eds.). Springer International Publishing, Cham, 133–158.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://link.springer.com/10.1007/978-3-031-08076-0_6" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://link.springer.com/10.1007/978-3-031-08076-0˙6</a>

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team et al<span id="bib.bib146.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al<span id="bib.bib146.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Gemini: a family of highly capable multimodal models.

</span>
<span class="ltx_bibblock"><em id="bib.bib146.4.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2312.11805</em> N/A, N/A (2023), N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomas (2006)</span>
<span class="ltx_bibblock">
David R. Thomas. 2006.

</span>
<span class="ltx_bibblock">A General Inductive Approach for Analyzing Qualitative Evaluation Data.

</span>
<span class="ltx_bibblock"><em id="bib.bib147.1.1" class="ltx_emph ltx_font_italic">American Journal of Evaluation</em> 27, 2 (2006), 237–246.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1177/1098214005283748" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1177/1098214005283748</a>
arXiv:https://doi.org/10.1177/1098214005283748

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thomas Thiebaud” (2020)</span>
<span class="ltx_bibblock">
Thomas Thiebaud”. 2020.

</span>
<span class="ltx_bibblock">Spacy FastLang.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://spacy.io/universe/project/spacy_fastlang" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://spacy.io/universe/project/spacy˙fastlang</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 08-02-2024].

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tisza et al<span id="bib.bib149.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Gabriella Tisza, Kshitij Sharma, Sofia Papavlasopoulou, Panos Markopoulos, and Michail Giannakos. 2022.

</span>
<span class="ltx_bibblock">Understanding Fun in Learning to Code: A Multi-Modal Data approach. In <em id="bib.bib149.3.1" class="ltx_emph ltx_font_italic">Interaction Design and Children</em>. ACM, Braga Portugal, 274–287.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3501712.3529716" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3501712.3529716</a>

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span id="bib.bib150.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al<span id="bib.bib150.3.1" class="ltx_text">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models, 2023.

</span>
<span class="ltx_bibblock"><em id="bib.bib150.4.1" class="ltx_emph ltx_font_italic">URL https://arxiv. org/abs/2307.09288</em> N/A, N/A (2023), N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span id="bib.bib151.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em id="bib.bib151.3.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em> 30 (2017), N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vatral et al<span id="bib.bib152.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Caleb Vatral, Gautam Biswas, Clayton Cohn, Eduardo Davalos, and Naveeduddin Mohammed. 2022.

</span>
<span class="ltx_bibblock">Using the DiCoT framework for integrated multimodal analysis in mixed-reality training environments.

</span>
<span class="ltx_bibblock"><em id="bib.bib152.3.1" class="ltx_emph ltx_font_italic">Frontiers in artificial intelligence</em> 5 (2022), 941825.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vatral et al<span id="bib.bib153.2.2.1" class="ltx_text">.</span> (2023)</span>
<span class="ltx_bibblock">
Caleb Vatral, Naveeduddin Mohammed, Gautam Biswas, Nicholas Roberts, and Benjamin Goldberg. 2023.

</span>
<span class="ltx_bibblock">A Comparative Analysis Interface to Streamline After-Action Review in Experiential Learning Environments. In <em id="bib.bib153.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 11th Annual Generalized Intelligent Framework for Tutoring (GIFT) Users Symposium (GIFTSym11)</em>. US Army Combat Capabilities Development Command–Soldier Center, N/A, N/A, 101.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Venthur (2010)</span>
<span class="ltx_bibblock">
Bastian Venthur. 2010.

</span>
<span class="ltx_bibblock">GitHub - venthur/gscholar: Query Google Scholar with Python.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/venthur/gscholar" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/venthur/gscholar</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">[Accessed 08-02-2024].

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vrzakova et al<span id="bib.bib155.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Hana Vrzakova, Mary Jean Amon, Angela Stewart, Nicholas D. Duran, and Sidney K. D’Mello. 2020.

</span>
<span class="ltx_bibblock">Focused or stuck together: multimodal patterns reveal triads’ performance in collaborative problem solving. In <em id="bib.bib155.3.1" class="ltx_emph ltx_font_italic">Proceedings of the Tenth International Conference on Learning Analytics &amp; Knowledge</em>. ACM, Frankfurt Germany, 295–304.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3375462.3375467" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3375462.3375467</a>

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vujovic et al<span id="bib.bib156.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Milica Vujovic, Davinia Hernández‐Leo, Simone Tassani, and Daniel Spikol. 2020.

</span>
<span class="ltx_bibblock">Round or rectangular tables for collaborative problem solving? A multimodal learning analytics study.

</span>
<span class="ltx_bibblock"><em id="bib.bib156.3.1" class="ltx_emph ltx_font_italic">British Journal of Educational Technology</em> 51, 5 (Sept. 2020), 1597–1614.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1111/bjet.12988" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1111/bjet.12988</a>

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span id="bib.bib157.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock"><em id="bib.bib157.3.1" class="ltx_emph ltx_font_italic">arXiv e-prints</em> N/A, N/A, Article arXiv:2201.11903 (Jan. 2022), N/A pages.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.48550/arXiv.2201.11903" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.48550/arXiv.2201.11903</a>
arXiv:2201.11903 [cs.CL]

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Worsley (2018)</span>
<span class="ltx_bibblock">
Marcelo Worsley. 2018.

</span>
<span class="ltx_bibblock">(Dis)engagement matters: identifying efficacious learning practices with multimodal learning analytics. In <em id="bib.bib158.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 8th International Conference on Learning Analytics and Knowledge</em>. ACM, Sydney New South Wales Australia, 365–369.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3170358.3170420" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3170358.3170420</a>

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Worsley and Blikstein (2018)</span>
<span class="ltx_bibblock">
Marcelo Worsley and Paulo Blikstein. 2018.

</span>
<span class="ltx_bibblock">A Multimodal Analysis of Making.

</span>
<span class="ltx_bibblock"><em id="bib.bib159.1.1" class="ltx_emph ltx_font_italic">International Journal of Artificial Intelligence in Education</em> 28, 3 (Sept. 2018), 385–419.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1007/s40593-017-0160-1" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1007/s40593-017-0160-1</a>

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Worsley et al<span id="bib.bib160.2.2.1" class="ltx_text">.</span> (2021)</span>
<span class="ltx_bibblock">
Marcelo Worsley, Kevin Mendoza Tudares, Timothy Mwiti, Mitchell Zhen, and Marc Jiang. 2021.

</span>
<span class="ltx_bibblock">Multicraft: A Multimodal Interface for Supporting and Studying Learning in Minecraft.

</span>
<span class="ltx_bibblock">In <em id="bib.bib160.3.1" class="ltx_emph ltx_font_italic">HCI in Games: Serious and Immersive Games</em>, Xiaowen Fang (Ed.). Vol. 12790. Springer International Publishing, Cham, 113–131.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://link.springer.com/10.1007/978-3-030-77414-1_10" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://link.springer.com/10.1007/978-3-030-77414-1˙10</a>

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span id="bib.bib161.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Kang Xia, Wenzhong Li, Shiwei Gan, and Sanglu Lu. 2024.

</span>
<span class="ltx_bibblock">TS2ACT: Few-Shot Human Activity Sensing with Cross-Modal Co-Learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib161.3.1" class="ltx_emph ltx_font_italic">Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em> 7, 4 (2024), 1–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span id="bib.bib162.2.2.1" class="ltx_text">.</span> (2022)</span>
<span class="ltx_bibblock">
Lixiang Yan, Linxuan Zhao, Dragan Gasevic, and Roberto Martinez-Maldonado. 2022.

</span>
<span class="ltx_bibblock">Scalability, Sustainability, and Ethicality of Multimodal Learning Analytics. In <em id="bib.bib162.3.1" class="ltx_emph ltx_font_italic">LAK22: 12th International Learning Analytics and Knowledge Conference</em> (Online, USA) <em id="bib.bib162.4.2" class="ltx_emph ltx_font_italic">(LAK22)</em>. Association for Computing Machinery, New York, NY, USA, 13–23.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="https://doi.org/10.1145/3506860.3506862" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1145/3506860.3506862</a>

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span id="bib.bib163.2.2.1" class="ltx_text">.</span> (2020)</span>
<span class="ltx_bibblock">
Xi Yang, Yeo-Jin Kim, Michelle Taub, Roger Azevedo, and Min Chi. 2020.

</span>
<span class="ltx_bibblock">PRIME: Block-Wise Missingness Handling for Multi-modalities in Intelligent Tutoring Systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib163.3.1" class="ltx_emph ltx_font_italic">MultiMedia Modeling</em>, Yong Man Ro, Wen-Huang Cheng, Junmo Kim, Wei-Ta Chu, Peng Cui, Jung-Woo Choi, Min-Chun Hu, and Wesley De Neve (Eds.). Vol. 11962. Springer International Publishing, Cham, 63–75.

</span>
<span class="ltx_bibblock">

<a target="_blank" href="http://link.springer.com/10.1007/978-3-030-37734-2_6" title="" class="ltx_ref ltx_url" style="color:#0000FF;">http://link.springer.com/10.1007/978-3-030-37734-2˙6</a>

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zala et al<span id="bib.bib164.2.2.1" class="ltx_text">.</span> (2023a)</span>
<span class="ltx_bibblock">
Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal. 2023a.

</span>
<span class="ltx_bibblock">Hierarchical Video-Moment Retrieval and Step-Captioning. In <em id="bib.bib164.3.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. N/A, N/A, 23056–23065.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zala et al<span id="bib.bib165.2.2.1" class="ltx_text">.</span> (2023b)</span>
<span class="ltx_bibblock">
Abhay Zala, Han Lin, Jaemin Cho, and Mohit Bansal. 2023b.

</span>
<span class="ltx_bibblock">DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning.

</span>
<span class="ltx_bibblock"><em id="bib.bib165.3.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2310.12128</em> N/A, N/A (2023), N/A.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span id="bib.bib166.2.2.1" class="ltx_text">.</span> (2024)</span>
<span class="ltx_bibblock">
Mengxi Zhou, Joyce Fonteles, Joshua Danish, Eduardo Davalos, Selena Steinberg, Gautam Biswas, and Noel Enyedy. 2024.

</span>
<span class="ltx_bibblock">Exploring artificial intelligence supported interaction analysis. In <em id="bib.bib166.3.1" class="ltx_emph ltx_font_italic">Proceedings of the 18th International Conference of the Learning Sciences - ICLS 2024</em>. International Society of the Learning Sciences, NY, USA, 2327–2328.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zilvinskis et al<span id="bib.bib167.2.2.1" class="ltx_text">.</span> (2017)</span>
<span class="ltx_bibblock">
John Zilvinskis, James Willis III, and Victor M. H. Borden. 2017.

</span>
<span class="ltx_bibblock">An Overview of Learning Analytics.

</span>
<span class="ltx_bibblock"><em id="bib.bib167.3.1" class="ltx_emph ltx_font_italic">New Directions for Higher Education</em> 2017, 179 (2017), 9–17.

</span>
<span class="ltx_bibblock">
<a target="_blank" href="https://doi.org/10.1002/he.20239" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://doi.org/10.1002/he.20239</a>
arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/he.20239

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Corpus Table</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">Table <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:tab:corpus_table</span> enumerates the 73 papers in this literature review’s corpus.</p>
</div>
<figure id="A1.T5" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A1.T5.2.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>. </span><span id="A1.T5.3.2" class="ltx_text" style="font-size:90%;">Each of the 73 works in our corpus.</span></figcaption>
<table id="A1.T5.4" class="ltx_tabular">
<tr id="A1.T5.4.1" class="ltx_tr">
<td id="A1.T5.4.1.1" class="ltx_td ltx_align_left ltx_border_tt">UUID</td>
<td id="A1.T5.4.1.2" class="ltx_td ltx_align_left ltx_border_tt">First Author</td>
<td id="A1.T5.4.1.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt">
<span id="A1.T5.4.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.1.3.1.1" class="ltx_p" style="width:281.9pt;">Title</span>
</span>
</td>
<td id="A1.T5.4.1.4" class="ltx_td ltx_align_right ltx_border_tt">Year</td>
<td id="A1.T5.4.1.5" class="ltx_td ltx_align_left ltx_border_tt">Publication</td>
</tr>
<tr id="A1.T5.4.2" class="ltx_tr">
<td id="A1.T5.4.2.1" class="ltx_td ltx_align_left ltx_border_t">2456887548 <cite class="ltx_cite ltx_citemacro_citep">(Alyuz et al<span class="ltx_text">.</span>, <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="A1.T5.4.2.2" class="ltx_td ltx_align_left ltx_border_t">Alyuz</td>
<td id="A1.T5.4.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_t">
<span id="A1.T5.4.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.2.3.1.1" class="ltx_p" style="width:281.9pt;">An Unobtrusive And Multimodal Approach For Behavioral Engagement Detection Of Students</span>
</span>
</td>
<td id="A1.T5.4.2.4" class="ltx_td ltx_align_right ltx_border_t">2017</td>
<td id="A1.T5.4.2.5" class="ltx_td ltx_align_left ltx_border_t">MIE</td>
</tr>
<tr id="A1.T5.4.3" class="ltx_tr">
<td id="A1.T5.4.3.1" class="ltx_td ltx_align_left">818492192 <cite class="ltx_cite ltx_citemacro_citep">(Andrade, <a href="#bib.bib7" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="A1.T5.4.3.2" class="ltx_td ltx_align_left">Andrade</td>
<td id="A1.T5.4.3.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.3.3.1.1" class="ltx_p" style="width:281.9pt;">Understanding Student Learning Trajectories Using Multimodal Learning Analytics Within An Embodied-Interaction Learning Environment</span>
</span>
</td>
<td id="A1.T5.4.3.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.3.5" class="ltx_td ltx_align_left">LAK</td>
</tr>
<tr id="A1.T5.4.4" class="ltx_tr">
<td id="A1.T5.4.4.1" class="ltx_td ltx_align_left">3637456466 <cite class="ltx_cite ltx_citemacro_citep">(Ashwin and Guddeti, <a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.4.2" class="ltx_td ltx_align_left">Ashwin</td>
<td id="A1.T5.4.4.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.4.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.4.3.1.1" class="ltx_p" style="width:281.9pt;">Impact Of Inquiry Interventions On Students In E-Learning And Classroom Environments Using Affective Computing Framework</span>
</span>
</td>
<td id="A1.T5.4.4.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.4.5" class="ltx_td ltx_align_left">UMUAI</td>
</tr>
<tr id="A1.T5.4.5" class="ltx_tr">
<td id="A1.T5.4.5.1" class="ltx_td ltx_align_left">3448122334 <cite class="ltx_cite ltx_citemacro_citep">(Aslan et al<span class="ltx_text">.</span>, <a href="#bib.bib10" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.5.2" class="ltx_td ltx_align_left">Aslan</td>
<td id="A1.T5.4.5.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.5.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.5.3.1.1" class="ltx_p" style="width:281.9pt;">Investigating The Impact Of A Real-Time, Multimodal Student Engagement Analytics Technology In Authentic Classrooms</span>
</span>
</td>
<td id="A1.T5.4.5.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.5.5" class="ltx_td ltx_align_left">CHI</td>
</tr>
<tr id="A1.T5.4.6" class="ltx_tr">
<td id="A1.T5.4.6.1" class="ltx_td ltx_align_left">1886134458 <cite class="ltx_cite ltx_citemacro_citep">(Azcona et al<span class="ltx_text">.</span>, <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.6.2" class="ltx_td ltx_align_left">Azcona</td>
<td id="A1.T5.4.6.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.6.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.6.3.1.1" class="ltx_p" style="width:281.9pt;">Personalizing Computer Science Education By Leveraging Multimodal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.6.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.6.5" class="ltx_td ltx_align_left">FIE</td>
</tr>
<tr id="A1.T5.4.7" class="ltx_tr">
<td id="A1.T5.4.7.1" class="ltx_td ltx_align_left">3146393211 <cite class="ltx_cite ltx_citemacro_citep">(Birt et al<span class="ltx_text">.</span>, <a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.7.2" class="ltx_td ltx_align_left">Birt</td>
<td id="A1.T5.4.7.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.7.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.7.3.1.1" class="ltx_p" style="width:281.9pt;">Mobile Mixed Reality For Experiential Learning And Simulation In Medical And Health Sciences Education</span>
</span>
</td>
<td id="A1.T5.4.7.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.7.5" class="ltx_td ltx_align_left">Information</td>
</tr>
<tr id="A1.T5.4.8" class="ltx_tr">
<td id="A1.T5.4.8.1" class="ltx_td ltx_align_left">1326191931 <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a href="#bib.bib20" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.8.2" class="ltx_td ltx_align_left">Chan</td>
<td id="A1.T5.4.8.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.8.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.8.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Learning Analytics In A Laboratory Classroom</span>
</span>
</td>
<td id="A1.T5.4.8.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.8.5" class="ltx_td ltx_align_left">MLPALA</td>
</tr>
<tr id="A1.T5.4.9" class="ltx_tr">
<td id="A1.T5.4.9.1" class="ltx_td ltx_align_left">2936220551 <cite class="ltx_cite ltx_citemacro_citep">(Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib21" title="" class="ltx_ref">2021a</a>)</cite>
</td>
<td id="A1.T5.4.9.2" class="ltx_td ltx_align_left">Chango</td>
<td id="A1.T5.4.9.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.9.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.9.3.1.1" class="ltx_p" style="width:281.9pt;">Multi-Source And Multimodal Data Fusion For Predicting Academic Performance In Blended Learning University Courses</span>
</span>
</td>
<td id="A1.T5.4.9.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.9.5" class="ltx_td ltx_align_left">CEE</td>
</tr>
<tr id="A1.T5.4.10" class="ltx_tr">
<td id="A1.T5.4.10.1" class="ltx_td ltx_align_left">4277812050 <cite class="ltx_cite ltx_citemacro_citep">(Chango et al<span class="ltx_text">.</span>, <a href="#bib.bib22" title="" class="ltx_ref">2021b</a>)</cite>
</td>
<td id="A1.T5.4.10.2" class="ltx_td ltx_align_left">Chango</td>
<td id="A1.T5.4.10.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.10.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.10.3.1.1" class="ltx_p" style="width:281.9pt;">Improving Prediction Of Students’ Performance In Intelligent Tutoring Systems Using Attribute Selection And Ensembles Of Different Multimodal Data Sources</span>
</span>
</td>
<td id="A1.T5.4.10.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.10.5" class="ltx_td ltx_align_left">JCHE</td>
</tr>
<tr id="A1.T5.4.11" class="ltx_tr">
<td id="A1.T5.4.11.1" class="ltx_td ltx_align_left">1426267857 <cite class="ltx_cite ltx_citemacro_citep">(Chen, <a href="#bib.bib24" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.11.2" class="ltx_td ltx_align_left">Chen</td>
<td id="A1.T5.4.11.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.11.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.11.3.1.1" class="ltx_p" style="width:281.9pt;">Affect, Support, And Personal Factors: Multimodal Causal Models Of One-On-One Coaching</span>
</span>
</td>
<td id="A1.T5.4.11.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.11.5" class="ltx_td ltx_align_left">JEDM</td>
</tr>
<tr id="A1.T5.4.12" class="ltx_tr">
<td id="A1.T5.4.12.1" class="ltx_td ltx_align_left">3809293172 <cite class="ltx_cite ltx_citemacro_citep">(Closser et al<span class="ltx_text">.</span>, <a href="#bib.bib28" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.12.2" class="ltx_td ltx_align_left">Closser</td>
<td id="A1.T5.4.12.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.12.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.12.3.1.1" class="ltx_p" style="width:281.9pt;">Blending Learning Analytics And Embodied Design To Model Students’ Comprehension Of Measurement Using Their Actions, Speech, And Gestures</span>
</span>
</td>
<td id="A1.T5.4.12.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.12.5" class="ltx_td ltx_align_left">IJCCI</td>
</tr>
<tr id="A1.T5.4.13" class="ltx_tr">
<td id="A1.T5.4.13.1" class="ltx_td ltx_align_left">4019205162 <cite class="ltx_cite ltx_citemacro_citep">(Cornide-Reyes et al<span class="ltx_text">.</span>, <a href="#bib.bib39" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.13.2" class="ltx_td ltx_align_left">Cornide-Reyes</td>
<td id="A1.T5.4.13.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.13.3.1.1" class="ltx_p" style="width:281.9pt;">Introducing Low-Cost Sensors Into The Classroom Settings: Improving The Assessment In Agile Practices With Multimodal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.13.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.13.5" class="ltx_td ltx_align_left">Sensors</td>
</tr>
<tr id="A1.T5.4.14" class="ltx_tr">
<td id="A1.T5.4.14.1" class="ltx_td ltx_align_left">1576545447 <cite class="ltx_cite ltx_citemacro_citep">(Cukurova et al<span class="ltx_text">.</span>, <a href="#bib.bib44" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.14.2" class="ltx_td ltx_align_left">Cukurova</td>
<td id="A1.T5.4.14.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.14.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.14.3.1.1" class="ltx_p" style="width:281.9pt;">Artificial Intelligence And Multimodal Data In The Service Of Human Decision-Making: A Case Study In Debate Tutoring</span>
</span>
</td>
<td id="A1.T5.4.14.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.14.5" class="ltx_td ltx_align_left">BJET</td>
</tr>
<tr id="A1.T5.4.15" class="ltx_tr">
<td id="A1.T5.4.15.1" class="ltx_td ltx_align_left">1609706685 <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib49" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="A1.T5.4.15.2" class="ltx_td ltx_align_left">Di Mitri</td>
<td id="A1.T5.4.15.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.15.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.15.3.1.1" class="ltx_p" style="width:281.9pt;">Learning Pulse: A Machine Learning Approach For Predicting Performance In Self-Regulated Learning Using Multimodal Data</span>
</span>
</td>
<td id="A1.T5.4.15.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.15.5" class="ltx_td ltx_align_left">LAK</td>
</tr>
<tr id="A1.T5.4.16" class="ltx_tr">
<td id="A1.T5.4.16.1" class="ltx_td ltx_align_left">2070224207 <cite class="ltx_cite ltx_citemacro_citep">(Mitri, <a href="#bib.bib99" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.16.2" class="ltx_td ltx_align_left">Di Mitri</td>
<td id="A1.T5.4.16.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.16.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.16.3.1.1" class="ltx_p" style="width:281.9pt;">Detecting Medical Simulation Errors With Machine Learning And Multimodal Data</span>
</span>
</td>
<td id="A1.T5.4.16.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.16.5" class="ltx_td ltx_align_left">CAIM</td>
</tr>
<tr id="A1.T5.4.17" class="ltx_tr">
<td id="A1.T5.4.17.1" class="ltx_td ltx_align_left">3009548670 <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib52" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.17.2" class="ltx_td ltx_align_left">Di Mitri</td>
<td id="A1.T5.4.17.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.17.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.17.3.1.1" class="ltx_p" style="width:281.9pt;">Real-Time Multimodal Feedback With The Cpr Tutor</span>
</span>
</td>
<td id="A1.T5.4.17.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.17.5" class="ltx_td ltx_align_left">AIED</td>
</tr>
<tr id="A1.T5.4.18" class="ltx_tr">
<td id="A1.T5.4.18.1" class="ltx_td ltx_align_left">1763513559 <cite class="ltx_cite ltx_citemacro_citep">(Di Mitri et al<span class="ltx_text">.</span>, <a href="#bib.bib50" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.18.2" class="ltx_td ltx_align_left">Di Mitri</td>
<td id="A1.T5.4.18.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.18.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.18.3.1.1" class="ltx_p" style="width:281.9pt;">Keep Me In The Loop: Real-Time Feedback With Multimodal Data</span>
</span>
</td>
<td id="A1.T5.4.18.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.18.5" class="ltx_td ltx_align_left">IJAIED</td>
</tr>
<tr id="A1.T5.4.19" class="ltx_tr">
<td id="A1.T5.4.19.1" class="ltx_td ltx_align_left">1296637108 <cite class="ltx_cite ltx_citemacro_citep">(Echeverria et al<span class="ltx_text">.</span>, <a href="#bib.bib54" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.19.2" class="ltx_td ltx_align_left">Echeverria</td>
<td id="A1.T5.4.19.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.19.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.19.3.1.1" class="ltx_p" style="width:281.9pt;">Towards Collaboration Translucence: Giving Meaning To Multimodal Group Data</span>
</span>
</td>
<td id="A1.T5.4.19.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.19.5" class="ltx_td ltx_align_left">CHI</td>
</tr>
<tr id="A1.T5.4.20" class="ltx_tr">
<td id="A1.T5.4.20.1" class="ltx_td ltx_align_left">1581261659 <cite class="ltx_cite ltx_citemacro_citep">(Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib56" title="" class="ltx_ref">2020b</a>)</cite>
</td>
<td id="A1.T5.4.20.2" class="ltx_td ltx_align_left">Emerson</td>
<td id="A1.T5.4.20.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.20.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.20.3.1.1" class="ltx_p" style="width:281.9pt;">Early Prediction Of Visitor Engagement In Science Museums With Multimodal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.20.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.20.5" class="ltx_td ltx_align_left">ICMI</td>
</tr>
<tr id="A1.T5.4.21" class="ltx_tr">
<td id="A1.T5.4.21.1" class="ltx_td ltx_align_left">1598166515 <cite class="ltx_cite ltx_citemacro_citep">(Emerson et al<span class="ltx_text">.</span>, <a href="#bib.bib55" title="" class="ltx_ref">2020a</a>)</cite>
</td>
<td id="A1.T5.4.21.2" class="ltx_td ltx_align_left">Emerson</td>
<td id="A1.T5.4.21.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.21.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.21.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Learning Analytics For Game-Based Learning</span>
</span>
</td>
<td id="A1.T5.4.21.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.21.5" class="ltx_td ltx_align_left">BJET</td>
</tr>
<tr id="A1.T5.4.22" class="ltx_tr">
<td id="A1.T5.4.22.1" class="ltx_td ltx_align_left">4035649049 <cite class="ltx_cite ltx_citemacro_citep">(Fernandez-Nieto et al<span class="ltx_text">.</span>, <a href="#bib.bib58" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.22.2" class="ltx_td ltx_align_left">Fernández-Nieto</td>
<td id="A1.T5.4.22.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.22.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.22.3.1.1" class="ltx_p" style="width:281.9pt;">Storytelling With Learner Data: Guiding Student Reflection On Multimodal Team Data</span>
</span>
</td>
<td id="A1.T5.4.22.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.22.5" class="ltx_td ltx_align_left">TLT</td>
</tr>
<tr id="A1.T5.4.23" class="ltx_tr">
<td id="A1.T5.4.23.1" class="ltx_td ltx_align_left">483140962 <cite class="ltx_cite ltx_citemacro_citep">(Fwa, Hua Leong and Lindsay Marshall, <a href="#bib.bib63" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.23.2" class="ltx_td ltx_align_left">Fwa</td>
<td id="A1.T5.4.23.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.23.3.1.1" class="ltx_p" style="width:281.9pt;">Investigating Multimodal Affect Sensing In An Affective Tutoring System Using Unobtrusive Sensors</span>
</span>
</td>
<td id="A1.T5.4.23.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.23.5" class="ltx_td ltx_align_left">PPIG</td>
</tr>
<tr id="A1.T5.4.24" class="ltx_tr">
<td id="A1.T5.4.24.1" class="ltx_td ltx_align_left">4278392816 <cite class="ltx_cite ltx_citemacro_citep">(Giannakos et al<span class="ltx_text">.</span>, <a href="#bib.bib66" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.24.2" class="ltx_td ltx_align_left">Giannakos</td>
<td id="A1.T5.4.24.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.24.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.24.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Data As A Means To Understand The Learning Experience</span>
</span>
</td>
<td id="A1.T5.4.24.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.24.5" class="ltx_td ltx_align_left">IJIM</td>
</tr>
<tr id="A1.T5.4.25" class="ltx_tr">
<td id="A1.T5.4.25.1" class="ltx_td ltx_align_left">853680639 <cite class="ltx_cite ltx_citemacro_citep">(Henderson et al<span class="ltx_text">.</span>, <a href="#bib.bib70" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.25.2" class="ltx_td ltx_align_left">Henderson</td>
<td id="A1.T5.4.25.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.25.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.25.3.1.1" class="ltx_p" style="width:281.9pt;">Sensor-Based Data Fusion For Multimodal Affect Detection In Game-Based Learning Environments</span>
</span>
</td>
<td id="A1.T5.4.25.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.25.5" class="ltx_td ltx_align_left">EDM</td>
</tr>
<tr id="A1.T5.4.26" class="ltx_tr">
<td id="A1.T5.4.26.1" class="ltx_td ltx_align_left">86191824 <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a href="#bib.bib75" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.26.2" class="ltx_td ltx_align_left">Jiang</td>
<td id="A1.T5.4.26.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.26.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.26.3.1.1" class="ltx_p" style="width:281.9pt;">Examining How Different Modes Mediate Adolescents’ Interactions During Their Collaborative Multimodal Composing Processes</span>
</span>
</td>
<td id="A1.T5.4.26.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.26.5" class="ltx_td ltx_align_left">ILE</td>
</tr>
<tr id="A1.T5.4.27" class="ltx_tr">
<td id="A1.T5.4.27.1" class="ltx_td ltx_align_left">3398902089 <cite class="ltx_cite ltx_citemacro_citep">(Järvelä et al<span class="ltx_text">.</span>, <a href="#bib.bib76" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.27.2" class="ltx_td ltx_align_left">Järvelä</td>
<td id="A1.T5.4.27.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.27.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.27.3.1.1" class="ltx_p" style="width:281.9pt;">What Multimodal Data Can Tell Us About The Students’ Regulation Of Their Learning Process?</span>
</span>
</td>
<td id="A1.T5.4.27.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.27.5" class="ltx_td ltx_align_left">LAI</td>
</tr>
<tr id="A1.T5.4.28" class="ltx_tr">
<td id="A1.T5.4.28.1" class="ltx_td ltx_align_left">32184286 <cite class="ltx_cite ltx_citemacro_citep">(Kubsch et al<span class="ltx_text">.</span>, <a href="#bib.bib80" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.28.2" class="ltx_td ltx_align_left">Kubsch</td>
<td id="A1.T5.4.28.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.28.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.28.3.1.1" class="ltx_p" style="width:281.9pt;">Once More With Feeling: Emotions In Multimodal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.28.4" class="ltx_td ltx_align_right">2022</td>
<td id="A1.T5.4.28.5" class="ltx_td ltx_align_left">MMLA Handbook</td>
</tr>
<tr id="A1.T5.4.29" class="ltx_tr">
<td id="A1.T5.4.29.1" class="ltx_td ltx_align_left">205660768 <cite class="ltx_cite ltx_citemacro_citep">(Larmuseau et al<span class="ltx_text">.</span>, <a href="#bib.bib81" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.29.2" class="ltx_td ltx_align_left">Larmuseau</td>
<td id="A1.T5.4.29.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.29.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.29.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Learning Analytics To Investigate Cognitive Load During Online Problem Solving</span>
</span>
</td>
<td id="A1.T5.4.29.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.29.5" class="ltx_td ltx_align_left">BJET</td>
</tr>
<tr id="A1.T5.4.30" class="ltx_tr">
<td id="A1.T5.4.30.1" class="ltx_td ltx_align_left">1877483551 <cite class="ltx_cite ltx_citemacro_citep">(Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib84" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.30.2" class="ltx_td ltx_align_left">Lee-Cultura</td>
<td id="A1.T5.4.30.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.30.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.30.3.1.1" class="ltx_p" style="width:281.9pt;">Motion-Based Educational Games: Using Multi-Modal Data To Predict Player’S Performance</span>
</span>
</td>
<td id="A1.T5.4.30.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.30.5" class="ltx_td ltx_align_left">COG</td>
</tr>
<tr id="A1.T5.4.31" class="ltx_tr">
<td id="A1.T5.4.31.1" class="ltx_td ltx_align_left">3660066725 <cite class="ltx_cite ltx_citemacro_citep">(Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib82" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.31.2" class="ltx_td ltx_align_left">Lee-Cultura</td>
<td id="A1.T5.4.31.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.31.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.31.3.1.1" class="ltx_p" style="width:281.9pt;">Children’S Play And Problem Solving In Motion-Based Educational Games: Synergies Between Human Annotations And Multi-Modal Data</span>
</span>
</td>
<td id="A1.T5.4.31.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.31.5" class="ltx_td ltx_align_left">IDC</td>
</tr>
<tr id="A1.T5.4.32" class="ltx_tr">
<td id="A1.T5.4.32.1" class="ltx_td ltx_align_left">3856280479 <cite class="ltx_cite ltx_citemacro_citep">(Lee-Cultura et al<span class="ltx_text">.</span>, <a href="#bib.bib83" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.32.2" class="ltx_td ltx_align_left">Lee-Cultura</td>
<td id="A1.T5.4.32.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.32.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.32.3.1.1" class="ltx_p" style="width:281.9pt;">Children’S Play And Problem-Solving In Motion-Based Learning Technologies Using A Multi-Modal Mixed Methods Approach</span>
</span>
</td>
<td id="A1.T5.4.32.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.32.5" class="ltx_td ltx_align_left">IJCCI</td>
</tr>
<tr id="A1.T5.4.33" class="ltx_tr">
<td id="A1.T5.4.33.1" class="ltx_td ltx_align_left">804659204 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib88" title="" class="ltx_ref">2018a</a>)</cite>
</td>
<td id="A1.T5.4.33.2" class="ltx_td ltx_align_left">Liu</td>
<td id="A1.T5.4.33.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.33.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.33.3.1.1" class="ltx_p" style="width:281.9pt;">Towards Smart Educational Recommendations With Reinforcement Learning In Classroom</span>
</span>
</td>
<td id="A1.T5.4.33.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.33.5" class="ltx_td ltx_align_left">TALE</td>
</tr>
<tr id="A1.T5.4.34" class="ltx_tr">
<td id="A1.T5.4.34.1" class="ltx_td ltx_align_left">3783339081 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib87" title="" class="ltx_ref">2018b</a>)</cite>
</td>
<td id="A1.T5.4.34.2" class="ltx_td ltx_align_left">Liu</td>
<td id="A1.T5.4.34.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.34.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.34.3.1.1" class="ltx_p" style="width:281.9pt;">A Novel Method For The In-Depth Multimodal Analysis Of Student Learning Trajectories In Intelligent Tutoring Systems</span>
</span>
</td>
<td id="A1.T5.4.34.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.34.5" class="ltx_td ltx_align_left">JLA</td>
</tr>
<tr id="A1.T5.4.35" class="ltx_tr">
<td id="A1.T5.4.35.1" class="ltx_td ltx_align_left">3796180663 <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a href="#bib.bib86" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.35.2" class="ltx_td ltx_align_left">Liu</td>
<td id="A1.T5.4.35.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.35.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.35.3.1.1" class="ltx_p" style="width:281.9pt;">Learning Linkages: Integrating Data Streams Of Multiple Modalities And Timescales</span>
</span>
</td>
<td id="A1.T5.4.35.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.35.5" class="ltx_td ltx_align_left">JCAL</td>
</tr>
<tr id="A1.T5.4.36" class="ltx_tr">
<td id="A1.T5.4.36.1" class="ltx_td ltx_align_left">518268671 <cite class="ltx_cite ltx_citemacro_citep">(López et al<span class="ltx_text">.</span>, <a href="#bib.bib90" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.36.2" class="ltx_td ltx_align_left">López</td>
<td id="A1.T5.4.36.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.36.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.36.3.1.1" class="ltx_p" style="width:281.9pt;">Using Multimodal Learning Analytics To Explore Collaboration In A Sustainability Co-Located Tabletop Game</span>
</span>
</td>
<td id="A1.T5.4.36.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.36.5" class="ltx_td ltx_align_left">ECGBL</td>
</tr>
<tr id="A1.T5.4.37" class="ltx_tr">
<td id="A1.T5.4.37.1" class="ltx_td ltx_align_left">566043228 <cite class="ltx_cite ltx_citemacro_citep">(Capital Normal University, Beijing, China et al<span class="ltx_text">.</span>, <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.37.2" class="ltx_td ltx_align_left">Ma</td>
<td id="A1.T5.4.37.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.37.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.37.3.1.1" class="ltx_p" style="width:281.9pt;">Automatic Student Engagement In Online Learning Environment Based On Neural Turing Machine</span>
</span>
</td>
<td id="A1.T5.4.37.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.37.5" class="ltx_td ltx_align_left">IJIET</td>
</tr>
<tr id="A1.T5.4.38" class="ltx_tr">
<td id="A1.T5.4.38.1" class="ltx_td ltx_align_left">3754172825 <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a href="#bib.bib91" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.38.2" class="ltx_td ltx_align_left">Ma</td>
<td id="A1.T5.4.38.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.38.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.38.3.1.1" class="ltx_p" style="width:281.9pt;">Detecting Impasse During Collaborative Problem Solving With Multimodal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.38.4" class="ltx_td ltx_align_right">2022</td>
<td id="A1.T5.4.38.5" class="ltx_td ltx_align_left">LAK</td>
</tr>
<tr id="A1.T5.4.39" class="ltx_tr">
<td id="A1.T5.4.39.1" class="ltx_td ltx_align_left">147203129 <cite class="ltx_cite ltx_citemacro_citep">(Mangaroska et al<span class="ltx_text">.</span>, <a href="#bib.bib92" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.39.2" class="ltx_td ltx_align_left">Mangaroska</td>
<td id="A1.T5.4.39.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.39.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.39.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Learning Analytics To Inform Learning Design: Lessons Learned From Computing Education</span>
</span>
</td>
<td id="A1.T5.4.39.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.39.5" class="ltx_td ltx_align_left">JLA</td>
</tr>
<tr id="A1.T5.4.40" class="ltx_tr">
<td id="A1.T5.4.40.1" class="ltx_td ltx_align_left">1847468084 <cite class="ltx_cite ltx_citemacro_citep">(Martin et al<span class="ltx_text">.</span>, <a href="#bib.bib93" title="" class="ltx_ref">2019</a>)</cite>
</td>
<td id="A1.T5.4.40.2" class="ltx_td ltx_align_left">Martin</td>
<td id="A1.T5.4.40.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.40.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.40.3.1.1" class="ltx_p" style="width:281.9pt;">Computationally Augmented Ethnography: Emotion Tracking And Learning In Museum Games</span>
</span>
</td>
<td id="A1.T5.4.40.4" class="ltx_td ltx_align_right">2019</td>
<td id="A1.T5.4.40.5" class="ltx_td ltx_align_left">ICQE</td>
</tr>
<tr id="A1.T5.4.41" class="ltx_tr">
<td id="A1.T5.4.41.1" class="ltx_td ltx_align_left">2879332689 <cite class="ltx_cite ltx_citemacro_citep">(Martinez-Maldonado et al<span class="ltx_text">.</span>, <a href="#bib.bib94" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.41.2" class="ltx_td ltx_align_left">Martinez-Maldonado</td>
<td id="A1.T5.4.41.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.41.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.41.3.1.1" class="ltx_p" style="width:281.9pt;">From Data To Insights: A Layered Storytelling Approach For Multimodal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.41.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.41.5" class="ltx_td ltx_align_left">CHI</td>
</tr>
<tr id="A1.T5.4.42" class="ltx_tr">
<td id="A1.T5.4.42.1" class="ltx_td ltx_align_left">2155422499 <cite class="ltx_cite ltx_citemacro_citep">(Morell et al<span class="ltx_text">.</span>, <a href="#bib.bib100" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.42.2" class="ltx_td ltx_align_left">Morell</td>
<td id="A1.T5.4.42.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.42.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.42.3.1.1" class="ltx_p" style="width:281.9pt;">A Multimodal Analysis Of Pair Work Engagement Episodes: Implications For Emi Lecturer Training</span>
</span>
</td>
<td id="A1.T5.4.42.4" class="ltx_td ltx_align_right">2022</td>
<td id="A1.T5.4.42.5" class="ltx_td ltx_align_left">JEAP</td>
</tr>
<tr id="A1.T5.4.43" class="ltx_tr">
<td id="A1.T5.4.43.1" class="ltx_td ltx_align_left">2273914836 <cite class="ltx_cite ltx_citemacro_citep">(Nasir et al<span class="ltx_text">.</span>, <a href="#bib.bib102" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.43.2" class="ltx_td ltx_align_left">Nasir</td>
<td id="A1.T5.4.43.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.43.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.43.3.1.1" class="ltx_p" style="width:281.9pt;">Many Are The Ways To Learn Identifying Multi-Modal Behavioral Profiles Of Collaborative Learning In Constructivist Activities</span>
</span>
</td>
<td id="A1.T5.4.43.4" class="ltx_td ltx_align_right">2022</td>
<td id="A1.T5.4.43.5" class="ltx_td ltx_align_left">IJCSCL</td>
</tr>
<tr id="A1.T5.4.44" class="ltx_tr">
<td id="A1.T5.4.44.1" class="ltx_td ltx_align_left">1469065963 <cite class="ltx_cite ltx_citemacro_citep">(Nguyen et al<span class="ltx_text">.</span>, <a href="#bib.bib103" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="A1.T5.4.44.2" class="ltx_td ltx_align_left">Nguyen</td>
<td id="A1.T5.4.44.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.44.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.44.3.1.1" class="ltx_p" style="width:281.9pt;">Examining Socially Shared Regulation And Shared Physiological Arousal Events With Multimodal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.44.4" class="ltx_td ltx_align_right">2022</td>
<td id="A1.T5.4.44.5" class="ltx_td ltx_align_left">BJET</td>
</tr>
<tr id="A1.T5.4.45" class="ltx_tr">
<td id="A1.T5.4.45.1" class="ltx_td ltx_align_left">2345021698 <cite class="ltx_cite ltx_citemacro_citep">(Noel et al<span class="ltx_text">.</span>, <a href="#bib.bib105" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.45.2" class="ltx_td ltx_align_left">Noël</td>
<td id="A1.T5.4.45.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.45.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.45.3.1.1" class="ltx_p" style="width:281.9pt;">Exploring Collaborative Writing Of User Stories With Multimodal Learning Analytics: A Case Study On A Software Engineering Course</span>
</span>
</td>
<td id="A1.T5.4.45.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.45.5" class="ltx_td ltx_align_left">Access</td>
</tr>
<tr id="A1.T5.4.46" class="ltx_tr">
<td id="A1.T5.4.46.1" class="ltx_td ltx_align_left">2609260641 <cite class="ltx_cite ltx_citemacro_citep">(Noël et al<span class="ltx_text">.</span>, <a href="#bib.bib106" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.46.2" class="ltx_td ltx_align_left">Noël</td>
<td id="A1.T5.4.46.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.46.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.46.3.1.1" class="ltx_p" style="width:281.9pt;">Visualizing Collaboration In Teamwork: A Multimodal Learning Analytics Platform For Non-Verbal Communication</span>
</span>
</td>
<td id="A1.T5.4.46.4" class="ltx_td ltx_align_right">2022</td>
<td id="A1.T5.4.46.5" class="ltx_td ltx_align_left">DAMLE</td>
</tr>
<tr id="A1.T5.4.47" class="ltx_tr">
<td id="A1.T5.4.47.1" class="ltx_td ltx_align_left">2497456347 <cite class="ltx_cite ltx_citemacro_citep">(Ochoa et al<span class="ltx_text">.</span>, <a href="#bib.bib108" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.47.2" class="ltx_td ltx_align_left">Ochoa</td>
<td id="A1.T5.4.47.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.47.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.47.3.1.1" class="ltx_p" style="width:281.9pt;">The Rap System: Automatic Feedback Of Oral Presentation Skills Using Multimodal Analysis And Low-Cost Sensors</span>
</span>
</td>
<td id="A1.T5.4.47.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.47.5" class="ltx_td ltx_align_left">LAK</td>
</tr>
<tr id="A1.T5.4.48" class="ltx_tr">
<td id="A1.T5.4.48.1" class="ltx_td ltx_align_left">2634033325 <cite class="ltx_cite ltx_citemacro_citep">(Ochoa and Dominguez, <a href="#bib.bib107" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.48.2" class="ltx_td ltx_align_left">Ochoa</td>
<td id="A1.T5.4.48.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.48.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.48.3.1.1" class="ltx_p" style="width:281.9pt;">Controlled Evaluation Of A Multimodal System To Improve Oral Presentation Skills In A Real Learning Setting</span>
</span>
</td>
<td id="A1.T5.4.48.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.48.5" class="ltx_td ltx_align_left">BJET</td>
</tr>
<tr id="A1.T5.4.49" class="ltx_tr">
<td id="A1.T5.4.49.1" class="ltx_td ltx_align_left">3051560548 <cite class="ltx_cite ltx_citemacro_citep">(Olsen et al<span class="ltx_text">.</span>, <a href="#bib.bib111" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.49.2" class="ltx_td ltx_align_left">Olsen</td>
<td id="A1.T5.4.49.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.49.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.49.3.1.1" class="ltx_p" style="width:281.9pt;">Temporal Analysis Of Multimodal Data To Predict Collaborative Learning Outcomes</span>
</span>
</td>
<td id="A1.T5.4.49.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.49.5" class="ltx_td ltx_align_left">BJET</td>
</tr>
<tr id="A1.T5.4.50" class="ltx_tr">
<td id="A1.T5.4.50.1" class="ltx_td ltx_align_left">123412197 <cite class="ltx_cite ltx_citemacro_citep">(Papamitsiou et al<span class="ltx_text">.</span>, <a href="#bib.bib113" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.50.2" class="ltx_td ltx_align_left">Papamitsiou</td>
<td id="A1.T5.4.50.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.50.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.50.3.1.1" class="ltx_p" style="width:281.9pt;">Utilizing Multimodal Data Through Fsqca To Explain Engagement In Adaptive Learning</span>
</span>
</td>
<td id="A1.T5.4.50.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.50.5" class="ltx_td ltx_align_left">TLT</td>
</tr>
<tr id="A1.T5.4.51" class="ltx_tr">
<td id="A1.T5.4.51.1" class="ltx_td ltx_align_left">85990093 <cite class="ltx_cite ltx_citemacro_citep">(Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib116" title="" class="ltx_ref">2017b</a>)</cite>
</td>
<td id="A1.T5.4.51.2" class="ltx_td ltx_align_left">Petukhova</td>
<td id="A1.T5.4.51.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.51.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.51.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Markers Of Persuasive Speech : Designing A Virtual Debate Coach</span>
</span>
</td>
<td id="A1.T5.4.51.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.51.5" class="ltx_td ltx_align_left">INTERSPEECH</td>
</tr>
<tr id="A1.T5.4.52" class="ltx_tr">
<td id="A1.T5.4.52.1" class="ltx_td ltx_align_left">957160695 <cite class="ltx_cite ltx_citemacro_citep">(Petukhova et al<span class="ltx_text">.</span>, <a href="#bib.bib115" title="" class="ltx_ref">2017a</a>)</cite>
</td>
<td id="A1.T5.4.52.2" class="ltx_td ltx_align_left">Petukhova</td>
<td id="A1.T5.4.52.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.52.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.52.3.1.1" class="ltx_p" style="width:281.9pt;">Virtual Debate Coach Design: Assessing Multimodal Argumentation Performance</span>
</span>
</td>
<td id="A1.T5.4.52.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.52.5" class="ltx_td ltx_align_left">ICMI</td>
</tr>
<tr id="A1.T5.4.53" class="ltx_tr">
<td id="A1.T5.4.53.1" class="ltx_td ltx_align_left">1374035721 <cite class="ltx_cite ltx_citemacro_citep">(Pham and Wang, <a href="#bib.bib117" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="A1.T5.4.53.2" class="ltx_td ltx_align_left">Pham</td>
<td id="A1.T5.4.53.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.53.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.53.3.1.1" class="ltx_p" style="width:281.9pt;">Attentivelearner2: A Multimodal Approach For Improving Mooc Learning On Mobile Devices</span>
</span>
</td>
<td id="A1.T5.4.53.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.53.5" class="ltx_td ltx_align_left">AIED</td>
</tr>
<tr id="A1.T5.4.54" class="ltx_tr">
<td id="A1.T5.4.54.1" class="ltx_td ltx_align_left">2836996318 <cite class="ltx_cite ltx_citemacro_citep">(Pham and Wang, <a href="#bib.bib118" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.54.2" class="ltx_td ltx_align_left">Pham</td>
<td id="A1.T5.4.54.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.54.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.54.3.1.1" class="ltx_p" style="width:281.9pt;">Predicting Learners’ Emotions In Mobile Mooc Learning Via A Multimodal Intelligent Tutor</span>
</span>
</td>
<td id="A1.T5.4.54.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.54.5" class="ltx_td ltx_align_left">ITS</td>
</tr>
<tr id="A1.T5.4.55" class="ltx_tr">
<td id="A1.T5.4.55.1" class="ltx_td ltx_align_left">3135645357 <cite class="ltx_cite ltx_citemacro_citep">(Prieto et al<span class="ltx_text">.</span>, <a href="#bib.bib120" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.55.2" class="ltx_td ltx_align_left">Prieto</td>
<td id="A1.T5.4.55.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.55.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.55.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Teaching Analytics: Automated Extraction Of Orchestration Graphs From Wearable Sensor Data</span>
</span>
</td>
<td id="A1.T5.4.55.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.55.5" class="ltx_td ltx_align_left">JCAL</td>
</tr>
<tr id="A1.T5.4.56" class="ltx_tr">
<td id="A1.T5.4.56.1" class="ltx_td ltx_align_left">3408664396 <cite class="ltx_cite ltx_citemacro_citep">(Psaltis et al<span class="ltx_text">.</span>, <a href="#bib.bib121" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.56.2" class="ltx_td ltx_align_left">Psaltis</td>
<td id="A1.T5.4.56.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.56.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.56.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Student Engagement Recognition In Prosocial Games</span>
</span>
</td>
<td id="A1.T5.4.56.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.56.5" class="ltx_td ltx_align_left">T-CIAIG</td>
</tr>
<tr id="A1.T5.4.57" class="ltx_tr">
<td id="A1.T5.4.57.1" class="ltx_td ltx_align_left">3308658121 <cite class="ltx_cite ltx_citemacro_citep">(Reilly et al<span class="ltx_text">.</span>, <a href="#bib.bib124" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.57.2" class="ltx_td ltx_align_left">Reilly</td>
<td id="A1.T5.4.57.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.57.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.57.3.1.1" class="ltx_p" style="width:281.9pt;">Exploring Collaboration Using Motion Sensors And Multi-Modal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.57.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.57.5" class="ltx_td ltx_align_left">EDM</td>
</tr>
<tr id="A1.T5.4.58" class="ltx_tr">
<td id="A1.T5.4.58.1" class="ltx_td ltx_align_left">3625722965 <cite class="ltx_cite ltx_citemacro_citep">(Mat Sanusi et al<span class="ltx_text">.</span>, <a href="#bib.bib96" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.58.2" class="ltx_td ltx_align_left">Sanusi</td>
<td id="A1.T5.4.58.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.58.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.58.3.1.1" class="ltx_p" style="width:281.9pt;">Table Tennis Tutor: Forehand Strokes Classification Based On Multimodal Data And Neural Networks</span>
</span>
</td>
<td id="A1.T5.4.58.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.58.5" class="ltx_td ltx_align_left">Sensors</td>
</tr>
<tr id="A1.T5.4.59" class="ltx_tr">
<td id="A1.T5.4.59.1" class="ltx_td ltx_align_left">2000036002 <cite class="ltx_cite ltx_citemacro_citep">(Sharma et al<span class="ltx_text">.</span>, <a href="#bib.bib133" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.59.2" class="ltx_td ltx_align_left">Sharma</td>
<td id="A1.T5.4.59.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.59.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.59.3.1.1" class="ltx_p" style="width:281.9pt;">Predicting Learners’ Effortful Behaviour In Adaptive Assessment Using Multimodal Data</span>
</span>
</td>
<td id="A1.T5.4.59.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.59.5" class="ltx_td ltx_align_left">LAK</td>
</tr>
<tr id="A1.T5.4.60" class="ltx_tr">
<td id="A1.T5.4.60.1" class="ltx_td ltx_align_left">1118315889 <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib137" title="" class="ltx_ref">2017a</a>)</cite>
</td>
<td id="A1.T5.4.60.2" class="ltx_td ltx_align_left">Spikol</td>
<td id="A1.T5.4.60.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.60.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.60.3.1.1" class="ltx_p" style="width:281.9pt;">Using Multimodal Learning Analytics To Identify Aspects Of Collaboration In Project-Based Learning</span>
</span>
</td>
<td id="A1.T5.4.60.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.60.5" class="ltx_td ltx_align_left">CSCL</td>
</tr>
<tr id="A1.T5.4.61" class="ltx_tr">
<td id="A1.T5.4.61.1" class="ltx_td ltx_align_left">3339002981 <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib139" title="" class="ltx_ref">2017b</a>)</cite>
</td>
<td id="A1.T5.4.61.2" class="ltx_td ltx_align_left">Spikol</td>
<td id="A1.T5.4.61.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.61.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.61.3.1.1" class="ltx_p" style="width:281.9pt;">Estimation Of Success In Collaborative Learning Based On Multimodal Learning Analytics Features</span>
</span>
</td>
<td id="A1.T5.4.61.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.61.5" class="ltx_td ltx_align_left">ICALT</td>
</tr>
<tr id="A1.T5.4.62" class="ltx_tr">
<td id="A1.T5.4.62.1" class="ltx_td ltx_align_left">1637690235 <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.62.2" class="ltx_td ltx_align_left">Spikol</td>
<td id="A1.T5.4.62.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.62.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.62.3.1.1" class="ltx_p" style="width:281.9pt;">Supervised Machine Learning In Multimodal Learning Analytics For Estimating Success In Project-Based Learning</span>
</span>
</td>
<td id="A1.T5.4.62.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.62.5" class="ltx_td ltx_align_left">JCAL</td>
</tr>
<tr id="A1.T5.4.63" class="ltx_tr">
<td id="A1.T5.4.63.1" class="ltx_td ltx_align_left">3796643912 <cite class="ltx_cite ltx_citemacro_citep">(Standen et al<span class="ltx_text">.</span>, <a href="#bib.bib140" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.63.2" class="ltx_td ltx_align_left">Standen</td>
<td id="A1.T5.4.63.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.63.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.63.3.1.1" class="ltx_p" style="width:281.9pt;">An Evaluation Of An Adaptive Learning System Based On Multimodal Affect Recognition For Learners With Intellectual Disabilities</span>
</span>
</td>
<td id="A1.T5.4.63.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.63.5" class="ltx_td ltx_align_left">BJET</td>
</tr>
<tr id="A1.T5.4.64" class="ltx_tr">
<td id="A1.T5.4.64.1" class="ltx_td ltx_align_left">2181637610 <cite class="ltx_cite ltx_citemacro_citep">(Starr et al<span class="ltx_text">.</span>, <a href="#bib.bib141" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.64.2" class="ltx_td ltx_align_left">Starr</td>
<td id="A1.T5.4.64.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.64.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.64.3.1.1" class="ltx_p" style="width:281.9pt;">Toward Using Multi-Modal Learning Analytics To Support And Measure Collaboration In Co-Located Dyads</span>
</span>
</td>
<td id="A1.T5.4.64.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.64.5" class="ltx_td ltx_align_left">ICLS</td>
</tr>
<tr id="A1.T5.4.65" class="ltx_tr">
<td id="A1.T5.4.65.1" class="ltx_td ltx_align_left">1315379489 <cite class="ltx_cite ltx_citemacro_citep">(Sümer et al<span class="ltx_text">.</span>, <a href="#bib.bib143" title="" class="ltx_ref">2023</a>)</cite>
</td>
<td id="A1.T5.4.65.2" class="ltx_td ltx_align_left">Sümer</td>
<td id="A1.T5.4.65.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.65.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.65.3.1.1" class="ltx_p" style="width:281.9pt;">Multimodal Engagement Analysis From Facial Videos In The Classroom</span>
</span>
</td>
<td id="A1.T5.4.65.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.65.5" class="ltx_td ltx_align_left">TAC</td>
</tr>
<tr id="A1.T5.4.66" class="ltx_tr">
<td id="A1.T5.4.66.1" class="ltx_td ltx_align_left">3093310941 <cite class="ltx_cite ltx_citemacro_citep">(Tanaka et al<span class="ltx_text">.</span>, <a href="#bib.bib144" title="" class="ltx_ref">2017</a>)</cite>
</td>
<td id="A1.T5.4.66.2" class="ltx_td ltx_align_left">Tanaka</td>
<td id="A1.T5.4.66.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.66.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.66.3.1.1" class="ltx_p" style="width:281.9pt;">Embodied Conversational Agents For Multimodal Automated Social Skills Training In People With Autism Spectrum Disorders</span>
</span>
</td>
<td id="A1.T5.4.66.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.66.5" class="ltx_td ltx_align_left">PLOS</td>
</tr>
<tr id="A1.T5.4.67" class="ltx_tr">
<td id="A1.T5.4.67.1" class="ltx_td ltx_align_left">1345598079 <cite class="ltx_cite ltx_citemacro_citep">(Tancredi et al<span class="ltx_text">.</span>, <a href="#bib.bib145" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.67.2" class="ltx_td ltx_align_left">Tancredi</td>
<td id="A1.T5.4.67.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.67.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.67.3.1.1" class="ltx_p" style="width:281.9pt;">Intermodality In Multimodal Learning Analytics For Cognitive Theory Development: A Case From Embodied Design For Mathematics Learning</span>
</span>
</td>
<td id="A1.T5.4.67.4" class="ltx_td ltx_align_right">2022</td>
<td id="A1.T5.4.67.5" class="ltx_td ltx_align_left">MMLA Handbook</td>
</tr>
<tr id="A1.T5.4.68" class="ltx_tr">
<td id="A1.T5.4.68.1" class="ltx_td ltx_align_left">433919853 <cite class="ltx_cite ltx_citemacro_citep">(Tisza et al<span class="ltx_text">.</span>, <a href="#bib.bib149" title="" class="ltx_ref">2022</a>)</cite>
</td>
<td id="A1.T5.4.68.2" class="ltx_td ltx_align_left">Tisza</td>
<td id="A1.T5.4.68.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.68.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.68.3.1.1" class="ltx_p" style="width:281.9pt;">Understanding Fun In Learning To Code: A Multi-Modal Data Approach</span>
</span>
</td>
<td id="A1.T5.4.68.4" class="ltx_td ltx_align_right">2022</td>
<td id="A1.T5.4.68.5" class="ltx_td ltx_align_left">IDC</td>
</tr>
<tr id="A1.T5.4.69" class="ltx_tr">
<td id="A1.T5.4.69.1" class="ltx_td ltx_align_left">1770989706 <cite class="ltx_cite ltx_citemacro_citep">(Vrzakova et al<span class="ltx_text">.</span>, <a href="#bib.bib155" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.69.2" class="ltx_td ltx_align_left">Vrzakova</td>
<td id="A1.T5.4.69.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.69.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.69.3.1.1" class="ltx_p" style="width:281.9pt;">Focused Or Stuck Together: Multimodal Patterns Reveal Triads’ Performance In Collaborative Problem Solving</span>
</span>
</td>
<td id="A1.T5.4.69.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.69.5" class="ltx_td ltx_align_left">LAK</td>
</tr>
<tr id="A1.T5.4.70" class="ltx_tr">
<td id="A1.T5.4.70.1" class="ltx_td ltx_align_left">2055153191 <cite class="ltx_cite ltx_citemacro_citep">(Vujovic et al<span class="ltx_text">.</span>, <a href="#bib.bib156" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.70.2" class="ltx_td ltx_align_left">Vujovic</td>
<td id="A1.T5.4.70.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.70.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.70.3.1.1" class="ltx_p" style="width:281.9pt;">Round Or Rectangular Tables For Collaborative Problem Solving? A Multimodal Learning Analytics Study</span>
</span>
</td>
<td id="A1.T5.4.70.4" class="ltx_td ltx_align_right">2020</td>
<td id="A1.T5.4.70.5" class="ltx_td ltx_align_left">BJET</td>
</tr>
<tr id="A1.T5.4.71" class="ltx_tr">
<td id="A1.T5.4.71.1" class="ltx_td ltx_align_left">3095923626 <cite class="ltx_cite ltx_citemacro_citep">(Worsley and Blikstein, <a href="#bib.bib159" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.71.2" class="ltx_td ltx_align_left">Worsley</td>
<td id="A1.T5.4.71.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.71.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.71.3.1.1" class="ltx_p" style="width:281.9pt;">A Multimodal Analysis Of Making</span>
</span>
</td>
<td id="A1.T5.4.71.4" class="ltx_td ltx_align_right">2017</td>
<td id="A1.T5.4.71.5" class="ltx_td ltx_align_left">IJAIED</td>
</tr>
<tr id="A1.T5.4.72" class="ltx_tr">
<td id="A1.T5.4.72.1" class="ltx_td ltx_align_left">3309250332 <cite class="ltx_cite ltx_citemacro_citep">(Worsley, <a href="#bib.bib158" title="" class="ltx_ref">2018</a>)</cite>
</td>
<td id="A1.T5.4.72.2" class="ltx_td ltx_align_left">Worsley</td>
<td id="A1.T5.4.72.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.72.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.72.3.1.1" class="ltx_p" style="width:281.9pt;">(Dis)Engagement Matters: Identifying Efficacious Learning Practices With Multimodal Learning Analytics</span>
</span>
</td>
<td id="A1.T5.4.72.4" class="ltx_td ltx_align_right">2018</td>
<td id="A1.T5.4.72.5" class="ltx_td ltx_align_left">LAK</td>
</tr>
<tr id="A1.T5.4.73" class="ltx_tr">
<td id="A1.T5.4.73.1" class="ltx_td ltx_align_left">666050348 <cite class="ltx_cite ltx_citemacro_citep">(Worsley et al<span class="ltx_text">.</span>, <a href="#bib.bib160" title="" class="ltx_ref">2021</a>)</cite>
</td>
<td id="A1.T5.4.73.2" class="ltx_td ltx_align_left">Worsley</td>
<td id="A1.T5.4.73.3" class="ltx_td ltx_align_justify ltx_align_top">
<span id="A1.T5.4.73.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.73.3.1.1" class="ltx_p" style="width:281.9pt;">Multicraft: A Multimodal Interface For Supporting And Studying Learning In Minecraft</span>
</span>
</td>
<td id="A1.T5.4.73.4" class="ltx_td ltx_align_right">2021</td>
<td id="A1.T5.4.73.5" class="ltx_td ltx_align_left">HCII</td>
</tr>
<tr id="A1.T5.4.74" class="ltx_tr">
<td id="A1.T5.4.74.1" class="ltx_td ltx_align_left ltx_border_bb">1019093033 <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a href="#bib.bib163" title="" class="ltx_ref">2020</a>)</cite>
</td>
<td id="A1.T5.4.74.2" class="ltx_td ltx_align_left ltx_border_bb">Yang</td>
<td id="A1.T5.4.74.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb">
<span id="A1.T5.4.74.3.1" class="ltx_inline-block ltx_align_top">
<span id="A1.T5.4.74.3.1.1" class="ltx_p" style="width:281.9pt;">Prime: Block-Wise Missingness Handling For Multi-Modalities In Intelligent Tutoring Systems</span>
</span>
</td>
<td id="A1.T5.4.74.4" class="ltx_td ltx_align_right ltx_border_bb">2019</td>
<td id="A1.T5.4.74.5" class="ltx_td ltx_align_left ltx_border_bb">MMM</td>
</tr>
</table>
</figure>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Corpus Distillation Procedure</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">This appendix contains a detailed account of the steps we took to gather relevant works for our literature review and how we distilled the initial search results to the 73 papers in our final corpus.</p>
</div>
<section id="A2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1. </span>Literature Search</h3>

<div id="A2.SS1.p1" class="ltx_para">
<p id="A2.SS1.p1.1" class="ltx_p">Our literature search consisted of 42 search strings defined, discussed, and agreed upon by the authors as being representative of the body of works this literature review would be conducted on. Instead of performing our queries manually, we opted to perform our queries programmatically via an API-based Google Scholar web scraping tool. There are several available tools for scraping Google Scholar, such as scholarly <cite class="ltx_cite ltx_citemacro_citep">(Cholewiak et al<span class="ltx_text">.</span>, <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite> and gscholar <cite class="ltx_cite ltx_citemacro_citep">(Venthur, <a href="#bib.bib154" title="" class="ltx_ref">2010</a>)</cite>. Ultimately, we employed SerpAPI <cite class="ltx_cite ltx_citemacro_citep">(SerpApi, <a href="#bib.bib130" title="" class="ltx_ref">NA</a>)</cite>, a third-party Google Scholar web scraping API, for its most essential feature: organic web results. Other API tools’ results are not organic, i.e., a query made via the API and one manually queried in a browser-based environment will produce two different sets of results.</p>
</div>
<div id="A2.SS1.p2" class="ltx_para">
<p id="A2.SS1.p2.1" class="ltx_p">Queries were posed via API request to Google Scholar for papers published between 1/1/2017 and 10/22/2022 (the date of our literature search). 2017 was collectively agreed upon as being the best cutoff date for inclusion in our search due to the rapid technological advancements in the field over the past 5 years. Several papers prior to 2017 are discussed in Section <a href="#S1" title="1. Introduction and Background ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, as they are seminal works; however, they are not considered for inclusion in our corpus.</p>
</div>
<div id="A2.SS1.p3" class="ltx_para">
<p id="A2.SS1.p3.1" class="ltx_p">For the literature search, this review’s authors decided on 14 distinct search phrases, and each phrase was searched 3 times with a different spelling of the word <span id="A2.SS1.p3.1.1" class="ltx_text ltx_font_italic">multimodal</span> — multimodal, multi-modal, and multi modal — prepended to it. The 14 search phrases are enumerated in Table <a href="#A2.T6" title="Table 6 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The term ”xai” was included in the search due to the authors’ interest in exploring explainable AI methods applied to learning and training environments. Unfortunately, the field is still nascent, and no usable query results were returned with this search string.</span></span></span></p>
</div>
<figure id="A2.T6" class="ltx_table">
<table id="A2.T6.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T6.2.1.1" class="ltx_tr">
<th id="A2.T6.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">education technology</th>
<th id="A2.T6.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt">explainable artificial intelligence</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T6.2.2.1" class="ltx_tr">
<td id="A2.T6.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">learning analytics</td>
<td id="A2.T6.2.2.1.2" class="ltx_td ltx_align_left ltx_border_t">learning environments</td>
</tr>
<tr id="A2.T6.2.3.2" class="ltx_tr">
<td id="A2.T6.2.3.2.1" class="ltx_td ltx_align_left ltx_border_t">learning environments literature review</td>
<td id="A2.T6.2.3.2.2" class="ltx_td ltx_align_left ltx_border_t">learning environments survey</td>
</tr>
<tr id="A2.T6.2.4.3" class="ltx_tr">
<td id="A2.T6.2.4.3.1" class="ltx_td ltx_align_left ltx_border_t">literature review</td>
<td id="A2.T6.2.4.3.2" class="ltx_td ltx_align_left ltx_border_t">simulation environments</td>
</tr>
<tr id="A2.T6.2.5.4" class="ltx_tr">
<td id="A2.T6.2.5.4.1" class="ltx_td ltx_align_left ltx_border_t">survey</td>
<td id="A2.T6.2.5.4.2" class="ltx_td ltx_align_left ltx_border_t">training environments</td>
</tr>
<tr id="A2.T6.2.6.5" class="ltx_tr">
<td id="A2.T6.2.6.5.1" class="ltx_td ltx_align_left ltx_border_t">training environments literature review</td>
<td id="A2.T6.2.6.5.2" class="ltx_td ltx_align_left ltx_border_t">training environments survey</td>
</tr>
<tr id="A2.T6.2.7.6" class="ltx_tr">
<td id="A2.T6.2.7.6.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">tutoring systems</td>
<td id="A2.T6.2.7.6.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">xai</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T6.3.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>. </span><span id="A2.T6.4.2" class="ltx_text" style="font-size:90%;">Search strings used for the initial literature search.</span></figcaption>
</figure>
<div id="A2.SS1.p4" class="ltx_para">
<p id="A2.SS1.p4.1" class="ltx_p">For each of the 42 search strings, the top 5 pages (100 publications) deemed most relevant by Google Scholar were collected. The top-5 cutoff was financially imposed because of our subsequent citation graph construction (see Appendix <a href="#A2.SS2.SSS1" title="B.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2.1</span></a>). To build the citation graph, each individual paper’s citation information is queried, but each query is capped at 20 citations per API call by SerpAPI. This means that a paper with 100 citations requires 5 additional API calls to gather all of its citation information. The number of API calls needed to construct the citation graph would be intractable if the initial search was left unbounded; therefore, the top-5 cutoff was put in place.</p>
</div>
<div id="A2.SS1.p5" class="ltx_para">
<p id="A2.SS1.p5.1" class="ltx_p">Our initial search yielded a total of 4,200 papers (14 unique search terms * 3 spellings of multimodal * 100 publications per search string). Our corpus reduction procedure is enumerated in Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> and discussed in the following subappendices. Throughout this appendix, each step of our corpus reduction procedure is identified via its Step ID in Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<figure id="A2.T7" class="ltx_table">
<table id="A2.T7.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T7.2.1.1" class="ltx_tr">
<th id="A2.T7.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;">Step ID</th>
<th id="A2.T7.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;">Procedure</th>
<th id="A2.T7.2.1.1.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;">Removed</th>
<th id="A2.T7.2.1.1.4" class="ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_column" style="padding-top:1.5pt;padding-bottom:1.5pt;">Remaining</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T7.2.2.1" class="ltx_tr">
<td id="A2.T7.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0</td>
<td id="A2.T7.2.2.1.2" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">Literature search</td>
<td id="A2.T7.2.2.1.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">0</td>
<td id="A2.T7.2.2.1.4" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_t" style="padding-top:1.5pt;padding-bottom:1.5pt;">4200</td>
</tr>
<tr id="A2.T7.2.3.2" class="ltx_tr">
<td id="A2.T7.2.3.2.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">1</td>
<td id="A2.T7.2.3.2.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Remove duplicates</td>
<td id="A2.T7.2.3.2.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">2079</td>
<td id="A2.T7.2.3.2.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">2121</td>
</tr>
<tr id="A2.T7.2.4.3" class="ltx_tr">
<td id="A2.T7.2.4.3.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">2</td>
<td id="A2.T7.2.4.3.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Remove non-English</td>
<td id="A2.T7.2.4.3.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">1</td>
<td id="A2.T7.2.4.3.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">2120</td>
</tr>
<tr id="A2.T7.2.5.4" class="ltx_tr">
<td id="A2.T7.2.5.4.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">3</td>
<td id="A2.T7.2.5.4.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Remove degree-0 nodes</td>
<td id="A2.T7.2.5.4.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">488</td>
<td id="A2.T7.2.5.4.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">1632</td>
</tr>
<tr id="A2.T7.2.6.5" class="ltx_tr">
<td id="A2.T7.2.6.5.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">4</td>
<td id="A2.T7.2.6.5.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Remove disconnected components</td>
<td id="A2.T7.2.6.5.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">101</td>
<td id="A2.T7.2.6.5.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">1531</td>
</tr>
<tr id="A2.T7.2.7.6" class="ltx_tr">
<td id="A2.T7.2.7.6.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">5</td>
<td id="A2.T7.2.7.6.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Iteratively remove degree-1 nodes</td>
<td id="A2.T7.2.7.6.3" class="ltx_td ltx_nopad_l" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="A2.T7.2.7.6.4" class="ltx_td ltx_nopad_l" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="A2.T7.2.8.7" class="ltx_tr">
<td id="A2.T7.2.8.7.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   5.1</td>
<td id="A2.T7.2.8.7.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Iteration 1</td>
<td id="A2.T7.2.8.7.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">373</td>
<td id="A2.T7.2.8.7.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">1158</td>
</tr>
<tr id="A2.T7.2.9.8" class="ltx_tr">
<td id="A2.T7.2.9.8.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   5.2</td>
<td id="A2.T7.2.9.8.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Iteration 2</td>
<td id="A2.T7.2.9.8.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">74</td>
<td id="A2.T7.2.9.8.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">1084</td>
</tr>
<tr id="A2.T7.2.10.9" class="ltx_tr">
<td id="A2.T7.2.10.9.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   5.3</td>
<td id="A2.T7.2.10.9.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Iteration 3</td>
<td id="A2.T7.2.10.9.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">19</td>
<td id="A2.T7.2.10.9.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">1065</td>
</tr>
<tr id="A2.T7.2.11.10" class="ltx_tr">
<td id="A2.T7.2.11.10.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   5.4</td>
<td id="A2.T7.2.11.10.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Iteration 4</td>
<td id="A2.T7.2.11.10.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">2</td>
<td id="A2.T7.2.11.10.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">1063</td>
</tr>
<tr id="A2.T7.2.12.11" class="ltx_tr">
<td id="A2.T7.2.12.11.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">6</td>
<td id="A2.T7.2.12.11.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Remove titles with keywords</td>
<td id="A2.T7.2.12.11.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">204</td>
<td id="A2.T7.2.12.11.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">859</td>
</tr>
<tr id="A2.T7.2.13.12" class="ltx_tr">
<td id="A2.T7.2.13.12.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">7</td>
<td id="A2.T7.2.13.12.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Title reads</td>
<td id="A2.T7.2.13.12.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">471</td>
<td id="A2.T7.2.13.12.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">388</td>
</tr>
<tr id="A2.T7.2.14.13" class="ltx_tr">
<td id="A2.T7.2.14.13.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">8</td>
<td id="A2.T7.2.14.13.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Abstract reads</td>
<td id="A2.T7.2.14.13.3" class="ltx_td ltx_nopad_l" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="A2.T7.2.14.13.4" class="ltx_td ltx_nopad_l" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="A2.T7.2.15.14" class="ltx_tr">
<td id="A2.T7.2.15.14.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   8.1</td>
<td id="A2.T7.2.15.14.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Remove inaccessible abstracts</td>
<td id="A2.T7.2.15.14.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">10</td>
<td id="A2.T7.2.15.14.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">378</td>
</tr>
<tr id="A2.T7.2.16.15" class="ltx_tr">
<td id="A2.T7.2.16.15.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   8.2</td>
<td id="A2.T7.2.16.15.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">First abstract round</td>
<td id="A2.T7.2.16.15.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">211</td>
<td id="A2.T7.2.16.15.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">167</td>
</tr>
<tr id="A2.T7.2.17.16" class="ltx_tr">
<td id="A2.T7.2.17.16.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   8.3</td>
<td id="A2.T7.2.17.16.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Second abstract round</td>
<td id="A2.T7.2.17.16.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">40</td>
<td id="A2.T7.2.17.16.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">127</td>
</tr>
<tr id="A2.T7.2.18.17" class="ltx_tr">
<td id="A2.T7.2.18.17.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">9</td>
<td id="A2.T7.2.18.17.2" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Full paper reads</td>
<td id="A2.T7.2.18.17.3" class="ltx_td ltx_nopad_l" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td id="A2.T7.2.18.17.4" class="ltx_td ltx_nopad_l" style="padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
<tr id="A2.T7.2.19.18" class="ltx_tr">
<td id="A2.T7.2.19.18.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   9.1</td>
<td id="A2.T7.2.19.18.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">First full paper round</td>
<td id="A2.T7.2.19.18.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">52</td>
<td id="A2.T7.2.19.18.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">75</td>
</tr>
<tr id="A2.T7.2.20.19" class="ltx_tr">
<td id="A2.T7.2.20.19.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   9.2</td>
<td id="A2.T7.2.20.19.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Feature discretization and extraction</td>
<td id="A2.T7.2.20.19.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">2</td>
<td id="A2.T7.2.20.19.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">73</td>
</tr>
<tr id="A2.T7.2.21.20" class="ltx_tr">
<td id="A2.T7.2.21.20.1" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">   9.3</td>
<td id="A2.T7.2.21.20.2" class="ltx_td ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">Second full paper round</td>
<td id="A2.T7.2.21.20.3" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">0</td>
<td id="A2.T7.2.21.20.4" class="ltx_td ltx_nopad_l ltx_align_left" style="padding-top:1.5pt;padding-bottom:1.5pt;">73</td>
</tr>
<tr id="A2.T7.2.22.21" class="ltx_tr">
<td id="A2.T7.2.22.21.1" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">   9.4</td>
<td id="A2.T7.2.22.21.2" class="ltx_td ltx_align_left ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">Second feature extraction round</td>
<td id="A2.T7.2.22.21.3" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">0</td>
<td id="A2.T7.2.22.21.4" class="ltx_td ltx_nopad_l ltx_align_left ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">73</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T7.3.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>. </span><span id="A2.T7.4.2" class="ltx_text" style="font-size:90%;">Our corpus reduction procedure. Step ID 0 is the literature search. Steps 1 and 2 used programmatic filtering via Python packages. Steps 3-5 were performed quantitatively via CGP (see Section <a href="#S3" title="3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). Step 6 uses human-in-the-loop regex filtering. Steps 7-9 were performed qualitatively via our quality control procedure. Each Step ID lists the number of papers removed and remaining.</span></figcaption>
</figure>
<div id="A2.SS1.p6" class="ltx_para">
<p id="A2.SS1.p6.1" class="ltx_p">Our initial corpus contained 2,079 duplicates, which were removed by hashing paper titles (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 1). If a paper had multiple versions (or other duplicates), we used the official source (e.g., journal or conference) of publication. We removed 1 non-English paper (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 2) due to pragmatism (English is the only language shared between all of this review’s authors). Non-English papers were identified using spaCy FastLang <cite class="ltx_cite ltx_citemacro_citep">(Thomas Thiebaud”, <a href="#bib.bib148" title="" class="ltx_ref">2020</a>)</cite>, where any paper whose title was identified as having less than a 100% chance of being English was selected for manual review and potential exclusion. In total, our initial search yielded 2,120 unique English papers published within our search window.</p>
</div>
</section>
<section id="A2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2. </span>Study Selection</h3>

<div id="A2.SS2.p1" class="ltx_para">
<p id="A2.SS2.p1.1" class="ltx_p">To reduce our corpus to a reviewable body of works, we employed both quantitative ans qualitative methods. After the initial search, we distilled the corpus quantitatively via CGP, which we discuss in Appendix <a href="#A2.SS2.SSS1" title="B.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2.1</span></a>. Subsequent distillation was performed via qualitative means and is discussed in Appendix <a href="#A2.SS2.SSS2" title="B.2.2. Quality Control (Qualitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">B.2.2</span></a>.</p>
</div>
<section id="A2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.2.1. </span>Citation Graph Pruning (Quantitative Corpus Reduction).</h4>

<div id="A2.SS2.SSS1.p1" class="ltx_para">
<p id="A2.SS2.SSS1.p1.3" class="ltx_p">For visualization, analysis, and distillation purposes, we used NetworkX <cite class="ltx_cite ltx_citemacro_citep">(Hagberg et al<span class="ltx_text">.</span>, <a href="#bib.bib68" title="" class="ltx_ref">2008</a>)</cite> to create and display a <span id="A2.SS2.SSS1.p1.3.1" class="ltx_text ltx_font_italic">citation graph</span> of the initial 2,120 works considered for inclusion in this review. The citation graph is a directed acyclic graph (DAG), where each node is a paper uniquely identifiable by its UUID (universally unique identifier) on Google Scholar, and each directed edge from A to B indicates paper A cites paper B. For the purposes of this paper, we consider the degree of each node (paper) <math id="A2.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="A2.SS2.SSS1.p1.1.m1.1a"><mi id="A2.SS2.SSS1.p1.1.m1.1.1" xref="A2.SS2.SSS1.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.SSS1.p1.1.m1.1b"><ci id="A2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="A2.SS2.SSS1.p1.1.m1.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.SSS1.p1.1.m1.1c">p</annotation></semantics></math> to be the sum of both incoming and outgoing edges, i.e., papers citing <math id="A2.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="A2.SS2.SSS1.p1.2.m2.1a"><mi id="A2.SS2.SSS1.p1.2.m2.1.1" xref="A2.SS2.SSS1.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.SSS1.p1.2.m2.1b"><ci id="A2.SS2.SSS1.p1.2.m2.1.1.cmml" xref="A2.SS2.SSS1.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.SSS1.p1.2.m2.1c">p</annotation></semantics></math> and papers cited by <math id="A2.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="A2.SS2.SSS1.p1.3.m3.1a"><mi id="A2.SS2.SSS1.p1.3.m3.1.1" xref="A2.SS2.SSS1.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.SSS1.p1.3.m3.1b"><ci id="A2.SS2.SSS1.p1.3.m3.1.1.cmml" xref="A2.SS2.SSS1.p1.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.SSS1.p1.3.m3.1c">p</annotation></semantics></math>, respectively. We again used SerpAPI for collecting the list of works that cited each paper. The citation search did not need to be conducted in both directions, as any paper citing another paper in our corpus would already have been identified by the ”cited by” list of the paper being cited. Citations by papers not included in our initial search (i.e., not in the DAG) were ignored. Initially, our DAG contained a 3-node cycle. This was due to papers by the same author citing each other during preprint. Once the cycle was identified, the cycle’s edges were removed from the edge set. No nodes were removed as a result of correcting the cycle.</p>
</div>
<div id="A2.SS2.SSS1.p2" class="ltx_para">
<p id="A2.SS2.SSS1.p2.1" class="ltx_p">Once the DAG was constructed, we removed all 0-degree nodes (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 3; i.e., nodes with no edges coming in or going out). We felt it reasonable that if a paper did not cite (or was not cited by) any other papers in the field (as determined by our literature search), then the paper was either not relevant to the field or did not yield methods or findings referenced by subsequent works. Importantly, our approach strikes a balance between incoming and outgoing citations, as earlier works are unable to <span id="A2.SS2.SSS1.p2.1.1" class="ltx_text ltx_font_italic">cite</span> many works in the corpus, and later works are unable to <span id="A2.SS2.SSS1.p2.1.2" class="ltx_text ltx_font_italic">be cited by</span> many works in the corpus. For example, works from early 2017 may not have any outgoing edges simply due to being some of the earliest works in the corpus, which would have prevented them from citing papers that had not yet been published. However, these same papers had a greater opportunity to be cited by subsequent papers, which is why we felt it important to consider both incoming and outgoing edges. We expected earlier papers to have more incoming edges and later papers to have more outgoing edges, which was supported by our final corpus’s relatively uniform distribution over publication years. Altogether, pruning 0-degree nodes from the DAG reduced our corpus by 488, dropping our corpus count to 1,632 works.</p>
</div>
<div id="A2.SS2.SSS1.p3" class="ltx_para">
<p id="A2.SS2.SSS1.p3.1" class="ltx_p">After removing 0-degree nodes, we examined the DAG’s connectivity (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 4) to identify disconnected components deemed irrelevant to the field, which was necessary to account for overlapping terminology across domains. For example, a cursory look at our initial search results included several ”multimodal training” papers related to deep learning (DL), where artificial neural networks (ANNs) are trained using data across multiple modalities but are not applied to multimodal learning or training environments. Our hypothesis, based on our search strings, was that the works relevant to this review would comprise the largest component of the DAG, leaving other smaller, disconnected components to be discarded as irrelevant because they lacked any edge to or from the DAG’s primary component.</p>
</div>
<div id="A2.SS2.SSS1.p4" class="ltx_para">
<p id="A2.SS2.SSS1.p4.1" class="ltx_p">Evaluating the DAG’s connectivity, we found one large component consisting of 1,531 nodes (papers) and 44 smaller, disconnected components of various sizes totaling 101 papers. The sizes of the disconnected components, their frequencies of occurrence in the DAG, and the total number of papers for each component size are listed in Table <a href="#A2.T8" title="Table 8 ‣ B.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. All 101 papers were removed from the corpus by pruning the DAG’s disconnected components, which left 1,531 papers represented by a single, connected graph.</p>
</div>
<figure id="A2.T8" class="ltx_table">
<table id="A2.T8.2" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T8.2.1.1" class="ltx_tr">
<th id="A2.T8.2.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column">Size</th>
<th id="A2.T8.2.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">#</th>
<th id="A2.T8.2.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">Papers</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T8.2.2.1" class="ltx_tr">
<td id="A2.T8.2.2.1.1" class="ltx_td ltx_align_left ltx_border_t">2</td>
<td id="A2.T8.2.2.1.2" class="ltx_td ltx_align_left ltx_border_t">35</td>
<td id="A2.T8.2.2.1.3" class="ltx_td ltx_align_left ltx_border_t">70</td>
</tr>
<tr id="A2.T8.2.3.2" class="ltx_tr">
<td id="A2.T8.2.3.2.1" class="ltx_td ltx_align_left ltx_border_t">3</td>
<td id="A2.T8.2.3.2.2" class="ltx_td ltx_align_left ltx_border_t">6</td>
<td id="A2.T8.2.3.2.3" class="ltx_td ltx_align_left ltx_border_t">18</td>
</tr>
<tr id="A2.T8.2.4.3" class="ltx_tr">
<td id="A2.T8.2.4.3.1" class="ltx_td ltx_align_left ltx_border_t">4</td>
<td id="A2.T8.2.4.3.2" class="ltx_td ltx_align_left ltx_border_t">2</td>
<td id="A2.T8.2.4.3.3" class="ltx_td ltx_align_left ltx_border_t">8</td>
</tr>
<tr id="A2.T8.2.5.4" class="ltx_tr">
<td id="A2.T8.2.5.4.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">5</td>
<td id="A2.T8.2.5.4.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">1</td>
<td id="A2.T8.2.5.4.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">5</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="A2.T8.3.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>. </span><span id="A2.T8.4.2" class="ltx_text" style="font-size:90%;">Disconnected DAG components by number of nodes in the component (size), frequency of occurrence (#), and total number of papers (papers). For instance, the first row indicates that there were 35 disconnected components of size 2 in the graph, totaling to 70 papers.</span></figcaption>
</figure>
<div id="A2.SS2.SSS1.p5" class="ltx_para">
<p id="A2.SS2.SSS1.p5.1" class="ltx_p">Once we had our single component graph, we removed 1-degree nodes to further prune it. This created new 1-degree nodes, which were also removed. This process of removing 1-degree nodes was repeated four times (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 5) until the graph was stable (i.e., removing 1-degree nodes did not create any new 1-degree nodes). By iteratively removing 1-degree nodes, we felt we could effectively identify and remove works outside the scope of our literature review without losing works directly related to multimodal learning and training environments. This is because the field of multimodal learning and training environments spans several sub-fields across computer science, education, psychology, etc., and the authors agreed it was unlikely papers with so few edges would be relevant to our review. We removed 373 nodes in the first iteration (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 5.1), 74 nodes in the second iteration (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 5.2), 19 nodes in the third iteration (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 5.3), and 2 nodes in the fourth and final iteration (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 5.4). Altogether, we removed 468 papers over four iterations, reducing our corpus from 1,531 papers to 1,063. The CGP pseudocode is presented in Section <a href="#S3.SS2.SSS1" title="3.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a> (Algorithm <a href="#alg1" title="Algorithm 1 ‣ 3.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). At this point we concluded our quantitative pruning procedure and began qualitatively reducing the corpus.</p>
</div>
</section>
<section id="A2.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">B.2.2. </span>Quality Control (Qualitative Corpus Reduction).</h4>

<div id="A2.SS2.SSS2.p1" class="ltx_para">
<p id="A2.SS2.SSS2.p1.1" class="ltx_p">Manually examining the remaining 1,063 titles informed us that a large part of our corpus was still outside the scope of our review. First, we noticed there were still many papers related to training multimodal neural networks. We also noticed many works applying multimodal methods to the medical field, usually in terms of medical imaging. To remove papers pertaining to multimodal neural network training and multimodal medical applications, we programmatically identified 217 titles via regex keyword search (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 6) that contained at least one of the six following words: neural, deep, machine, medical, medicine, and healthcare. We then evaluated the selected titles by hand. Of the 217, 13 were kept in the corpus due to their potential relevance to our review. Papers employing deep learning methods in MMLA or applying multimodal methods to medical learning or training environments were within our scope, for example. Specific examples include removing one paper titled, ”deep learning for object detection and scene perception in self-driving cars: survey, challenges, and open issues” <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a href="#bib.bib67" title="" class="ltx_ref">2021</a>)</cite>; and keeping one titled, ”supervised machine learning in multimodal learning analytics for estimating success in project‐based learning” <cite class="ltx_cite ltx_citemacro_citep">(Spikol et al<span class="ltx_text">.</span>, <a href="#bib.bib138" title="" class="ltx_ref">2018</a>)</cite>. The remaining 204 papers were removed from the corpus, reducing it to 859 potentially relevant works.</p>
</div>
<div id="A2.SS2.SSS2.p2" class="ltx_para">
<p id="A2.SS2.SSS2.p2.1" class="ltx_p">Next, we selected papers for exclusion based on consensus. Pursuant to Kitchenham <cite class="ltx_cite ltx_citemacro_citep">(Kitchenham, <a href="#bib.bib78" title="" class="ltx_ref">2004</a>)</cite>, we initially excluded works based on reading papers’ titles, then abstracts, and eventually full manuscripts. The first five authors of this review acted as reviewers (henceforth referred to as ”the Reviewers”) for the quality control procedure. For the title reads (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 7), four of the Reviewers read all 859 titles. For each title, each Reviewer independently determined whether the title was likely to fall inside the scope of the review. The results were tallied, and papers were then selected for inclusion/exclusion based on majority voting, i.e., papers with at least three votes ”for” were automatically included, and papers with at least three votes ”against” were automatically excluded. For the papers with a 2-2 tie, a fifth reviewer was used as a tie breaker. The
Reviewers selected 347 papers for inclusion and 372 papers for exclusion. 140 papers were tied, and a fifth reviewer selected 41 of those for inclusion. In total, 388 papers were selected for inclusion after the title reads — 347 by majority vote, and 41 by tie-breaker.</p>
</div>
<div id="A2.SS2.SSS2.p3" class="ltx_para">
<p id="A2.SS2.SSS2.p3.1" class="ltx_p">Before conducting the abstract reads (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 8), several works were excluded due to their inaccessibility (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 8.1). While gathering the abstracts, we noticed not all papers were publicly available. Several were defined by invalid URLs or behind paywalls. Whenever a paper’s abstract (or introduction, in the case of a book or book chapter) was unavailable via its SerpAPI URL, a Google search was conducted in order to obtain the abstract manually through websites such as ResearchGate and other academic repositories. When this failed, we relied on the Vanderbilt University Library’s proxy to access papers behind paywalls. If we were unable to freely access a paper’s abstract online through Google search or via Vanderbilt’s proxy, the paper was excluded from the corpus. Altogether, 10 papers were removed due to inaccessibility, leaving 378 papers for abstract reads.</p>
</div>
<div id="A2.SS2.SSS2.p4" class="ltx_para">
<p id="A2.SS2.SSS2.p4.1" class="ltx_p">The ”abstracts” quality control procedure consisted of two rounds. Similar to the procedure for the title reads, each of the remaining 378 abstracts was first assigned to two Reviewers, and a majority voting scheme was employed (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 8.2). Papers were then selected for inclusion or exclusion based on a predefined set of exclusion criteria. The exclusion criteria for the abstracts is listed in Table <a href="#A2.T9" title="Table 9 ‣ B.2.2. Quality Control (Qualitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. Exclusion criteria are cumulative, so each criterion applies to subsequent steps in our corpus reduction procedure. An exclusion criterion for the abstracts will similarly apply to full paper reads later on, for example.</p>
</div>
<figure id="A2.T9" class="ltx_table">
<table id="A2.T9.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T9.2.1.1" class="ltx_tr">
<td id="A2.T9.2.1.1.1" class="ltx_td ltx_align_left ltx_border_t">1. Paper does not deal with learning or training environments</td>
</tr>
<tr id="A2.T9.2.2.2" class="ltx_tr">
<td id="A2.T9.2.2.2.1" class="ltx_td ltx_align_left">2. Paper’s environment is VR-only</td>
</tr>
<tr id="A2.T9.2.3.3" class="ltx_tr">
<td id="A2.T9.2.3.3.1" class="ltx_td ltx_align_left">3. Paper does not analyze multimodal data</td>
</tr>
<tr id="A2.T9.2.4.4" class="ltx_tr">
<td id="A2.T9.2.4.4.1" class="ltx_td ltx_align_left">4. Paper does not apply multimodal analysis methods</td>
</tr>
<tr id="A2.T9.2.5.5" class="ltx_tr">
<td id="A2.T9.2.5.5.1" class="ltx_td ltx_align_left ltx_border_bb">5. Paper is not original applied research</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T9.3.1.1" class="ltx_text" style="font-size:90%;">Table 9</span>. </span><span id="A2.T9.4.2" class="ltx_text" style="font-size:90%;">Exclusion criteria for the abstract reads. Each of the 378 abstracts was assigned to two different Reviewers. Each reviewer was instructed to exclude works based on this set of criteria.</span></figcaption>
</figure>
<div id="A2.SS2.SSS2.p5" class="ltx_para">
<p id="A2.SS2.SSS2.p5.1" class="ltx_p">Because this literature review focuses on multimodal methods applied to learning and training environments, any paper not dealing with a learning or training environment was not considered for this review. As mentioned in Section <a href="#S1" title="1. Introduction and Background ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, virtual reality (VR) environments were also not considered for inclusion in our corpus due to issues with scaling this technology in classroom settings. If a paper does not analyze multimodal data, it is similarly out-of-scope for this review. Papers must also include systematic methods for analyzing the multimodal data, and those methods must be original, applied research. Papers that are literature reviews, pedagogical tools, theoretical foundations, doctoral consortiums, etc., may be used for reference in our Introduction and Background, but they are not considered for inclusion in the actual review corpus unless they additionally provide original, applied research via multimodal methods and analysis.</p>
</div>
<div id="A2.SS2.SSS2.p6" class="ltx_para">
<p id="A2.SS2.SSS2.p6.1" class="ltx_p">Of the 378 abstracts, Reviewers agreed to keep 96 papers (i.e., both Reviewers selected the work for inclusion) and discard 211 (i.e., both Reviewers selected the work for exclusion). 71 were selected for further review (i.e., one reviewer selected the work for inclusion and one reviewer selected the work for exclusion). To address the 71 abstracts that did not receive unanimous agreement among Reviewers, a second round of abstract reads was performed (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 8.3). This round consisted of each of the 71 abstracts without unanimous agreement receiving three additional reads: one read from each of the three Reviewers who did not read the abstract in the initial abstract round. Each of the 71 papers was subsequently included or excluded based on majority voting (i.e., papers were kept if and only if at least two out of the three second abstract round Reviewers elected to keep the abstract in the corpus). Of the 71 second abstract round papers, 31 were selected for inclusion, and 40 were removed from the corpus. With 96 papers selected for inclusion from the first round of abstract reads, and 31 papers selected from the second round, 127 papers in total were kept in the corpus for the next round of quality control: full paper reads.</p>
</div>
<div id="A2.SS2.SSS2.p7" class="ltx_para">
<p id="A2.SS2.SSS2.p7.1" class="ltx_p">The ”full paper” quality control procedure also involved two rounds of review. To conduct full paper reads (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 9), the 127 papers kept from the abstract round were split into 5 approximately equal partitions and randomly assigned to the 5 Reviewers. Conducting full paper reads took several weeks, during which two additional exclusion criteria were defined. They are enumerated in Table <a href="#A2.T10" title="Table 10 ‣ B.2.2. Quality Control (Qualitative Corpus Reduction). ‣ B.2. Study Selection ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>.</p>
</div>
<figure id="A2.T10" class="ltx_table">
<table id="A2.T10.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T10.2.1.1" class="ltx_tr">
<td id="A2.T10.2.1.1.1" class="ltx_td ltx_align_left ltx_border_t">1. Paper’s results are not informative with respect to learning or training</td>
</tr>
<tr id="A2.T10.2.2.2" class="ltx_tr">
<td id="A2.T10.2.2.2.1" class="ltx_td ltx_align_left ltx_border_bb">2. Paper’s analysis methods are not able to be determined from the manuscript</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T10.3.1.1" class="ltx_text" style="font-size:90%;">Table 10</span>. </span><span id="A2.T10.4.2" class="ltx_text" style="font-size:90%;">Exclusion criteria for full paper reads. Each reviewer was instructed to recommend works for exclusion based on this set of criteria and the previously established exclusion criteria.</span></figcaption>
</figure>
<div id="A2.SS2.SSS2.p8" class="ltx_para">
<p id="A2.SS2.SSS2.p8.1" class="ltx_p">Certain papers deal with learning or training environments but are outside the scope of this review because they are not informative with respect to learning or training. Consider a paper presenting a novel neural network architecture that uses a classroom dataset as a performance benchmark. While the classroom constitutes a learning environment, the paper itself is not conducting research to inform learning or training, but rather is using a dataset collected from a learning environment to evaluate a core AI approach. We elected not to include these types of works in our review, as we aim to focus on multimodal methods that are explicitly used to inform learning or training. Additionally, a few papers we encountered did not have analysis methods that were well-defined enough for feature extraction (i.e., we were unsure of their exact methods for analyzing the data). This often included short workshop papers whose method details were unable to be determined without referencing external works.<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>This does not include all workshop papers; only those whose analysis methods could not be determined from the manuscript.</span></span></span> Because these types of papers would be very difficult to reproduce on their own, we elected to exclude them from our review.</p>
</div>
<div id="A2.SS2.SSS2.p9" class="ltx_para">
<p id="A2.SS2.SSS2.p9.1" class="ltx_p">During the first round of full paper reads (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 9.1), Reviewers marked each paper as ”immediate exclude,” ”immediate accept,” ”borderline exclude,” or ”borderline accept.” Papers marked as ”immediate exclude” were discussed by all 5 Reviewers and excluded only if all agreed. These were papers with easily identifiable reasons for exclusion based on our criteria (for instance, a proposed theoretical framework with no analysis or a doctoral consortium presenting ideas for future research). No papers were ever excluded from our corpus during full paper reads without unanimous agreement from all five Reviewers. Papers marked as ”immediate accept” were kept in the corpus for the second full paper read round. Papers marked as ”borderline exclude” or ”borderline accept” were assigned to a separate reader for further review and were subsequently discussed. Similar to papers marked for immediate exclusion, borderline papers were excluded prior to the second full paper read round only if all Reviewers agreed. Altogether, 52 papers were excluded during the first round of full paper reads, which left 75 works remaining in the corpus.</p>
</div>
</section>
</section>
<section id="A2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3. </span>Feature Extraction</h3>

<div id="A2.SS3.p1" class="ltx_para">
<p id="A2.SS3.p1.1" class="ltx_p">During the first full paper read round, several features were extracted from each paper (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 9.2). Features included identifying information (e.g., title, first author, publication year), and information related to the paper’s methods (e.g., data collection mediums, modalities, and analysis methods). The extracted features and their descriptions are found in Table <a href="#A2.T11" title="Table 11 ‣ B.3. Feature Extraction ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>.<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>For the ”Year” category, we used the date the manuscript was first publicly available (if listed, otherwise we used the publication date) in order to most accurately represent when the methods were performed. In some instances, the first date of online availability preceded the official publication date by over a year. Additionally, only data that was ultimately used in the paper’s analysis was considered for the ”Data Collection Mediums” category (i.e., if data was collected but never analyzed, we did not include it).</span></span></span></p>
</div>
<figure id="A2.T11" class="ltx_table">
<table id="A2.T11.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T11.2.1.1" class="ltx_tr">
<th id="A2.T11.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.1.1.1.1.1" class="ltx_p" style="width:108.4pt;">Feature</span>
</span>
</th>
<th id="A2.T11.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.1.1.2.1.1" class="ltx_p" style="width:281.9pt;">Description</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T11.2.2.1" class="ltx_tr">
<td id="A2.T11.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.2.1.1.1.1" class="ltx_p" style="width:108.4pt;">UUID</span>
</span>
</td>
<td id="A2.T11.2.2.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.2.1.2.1.1" class="ltx_p" style="width:281.9pt;">Universally unique identifier on Google Scholar</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.3.2" class="ltx_tr">
<td id="A2.T11.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.3.2.1.1.1" class="ltx_p" style="width:108.4pt;">Title</span>
</span>
</td>
<td id="A2.T11.2.3.2.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.3.2.2.1.1" class="ltx_p" style="width:281.9pt;">Publication title</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.4.3" class="ltx_tr">
<td id="A2.T11.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.4.3.1.1.1" class="ltx_p" style="width:108.4pt;">First Author</span>
</span>
</td>
<td id="A2.T11.2.4.3.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.4.3.2.1.1" class="ltx_p" style="width:281.9pt;">Publication’s first author</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.5.4" class="ltx_tr">
<td id="A2.T11.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.5.4.1.1.1" class="ltx_p" style="width:108.4pt;">Year</span>
</span>
</td>
<td id="A2.T11.2.5.4.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.5.4.2.1.1" class="ltx_p" style="width:281.9pt;">Year publication was first publicly available</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.6.5" class="ltx_tr">
<td id="A2.T11.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.6.5.1.1.1" class="ltx_p" style="width:108.4pt;">Environment Type</span>
</span>
</td>
<td id="A2.T11.2.6.5.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.6.5.2.1.1" class="ltx_p" style="width:281.9pt;">Type of environment analyzed in the publication</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.7.6" class="ltx_tr">
<td id="A2.T11.2.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.7.6.1.1.1" class="ltx_p" style="width:108.4pt;">Data Collection Mediums</span>
</span>
</td>
<td id="A2.T11.2.7.6.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.7.6.2.1.1" class="ltx_p" style="width:281.9pt;">Types of data collected from the environment</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.8.7" class="ltx_tr">
<td id="A2.T11.2.8.7.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.8.7.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.8.7.1.1.1" class="ltx_p" style="width:108.4pt;">Modalities</span>
</span>
</td>
<td id="A2.T11.2.8.7.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.8.7.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.8.7.2.1.1" class="ltx_p" style="width:281.9pt;">List of the different modalities used during analysis</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.9.8" class="ltx_tr">
<td id="A2.T11.2.9.8.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.9.8.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.9.8.1.1.1" class="ltx_p" style="width:108.4pt;">Analysis Methods</span>
</span>
</td>
<td id="A2.T11.2.9.8.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.9.8.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.9.8.2.1.1" class="ltx_p" style="width:281.9pt;">List of the analysis methods used in the publication</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.10.9" class="ltx_tr">
<td id="A2.T11.2.10.9.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.10.9.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.10.9.1.1.1" class="ltx_p" style="width:108.4pt;">Fusion Type</span>
</span>
</td>
<td id="A2.T11.2.10.9.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.10.9.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.10.9.2.1.1" class="ltx_p" style="width:281.9pt;">List of data fusion types used in the publication</span>
</span>
</td>
</tr>
<tr id="A2.T11.2.11.10" class="ltx_tr">
<td id="A2.T11.2.11.10.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.11.10.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.11.10.1.1.1" class="ltx_p" style="width:108.4pt;">Publication Source</span>
</span>
</td>
<td id="A2.T11.2.11.10.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T11.2.11.10.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T11.2.11.10.2.1.1" class="ltx_p" style="width:281.9pt;">Publication journal, conference, workshop, etc.</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T11.3.1.1" class="ltx_text" style="font-size:90%;">Table 11</span>. </span><span id="A2.T11.4.2" class="ltx_text" style="font-size:90%;">Initial features extracted from each paper.</span></figcaption>
</figure>
<div id="A2.SS3.p2" class="ltx_para">
<p id="A2.SS3.p2.1" class="ltx_p">After the first read, the Reviewers discussed their extracted features. To ensure alignment and understanding between the Reviewers with respect to the features, feature categories were discretized via inductive coding <cite class="ltx_cite ltx_citemacro_citep">(Thomas, <a href="#bib.bib147" title="" class="ltx_ref">2006</a>)</cite>, where four Reviewers each extracted initial feature sets from 25% of the corpus’s papers. For example, the initially extracted <span id="A2.SS3.p2.1.1" class="ltx_text ltx_font_italic">data collection mediums</span> feature included instances of video camera, web camera, and Kinect camera, all of which were mapped to the ”VIDEO” data collection medium. Once the Reviewers agreed on the discrete sets of features, papers were reread by their original Reviewers, and their features were extracted into the discrete sets. The initial feature-space is described below in Table <a href="#A2.T12" title="Table 12 ‣ B.3. Feature Extraction ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a>. We call these features <span id="A2.SS3.p2.1.2" class="ltx_text ltx_font_italic">circumscribing features</span> to delineate them relative to the identifying features (e.g., UUID, paper title, author, etc.) that were extracted for identification purposes but not used in our analysis. In total, two sets of circumscribing features were extracted from the corpus to gather the information needed to conduct our analysis (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step IDs 9.2 and 9.4).</p>
</div>
<figure id="A2.T12" class="ltx_table">
<table id="A2.T12.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T12.2.1.1" class="ltx_tr">
<th id="A2.T12.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.1.1.1.1.1" class="ltx_p" style="width:69.4pt;">Feature</span>
</span>
</th>
<th id="A2.T12.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.1.1.2.1.1" class="ltx_p" style="width:325.2pt;">Feature Set</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T12.2.2.1" class="ltx_tr">
<td id="A2.T12.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.2.1.1.1.1" class="ltx_p" style="width:69.4pt;">Environment Type</span>
</span>
</td>
<td id="A2.T12.2.2.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.2.1.2.1.1" class="ltx_p" style="width:325.2pt;">learning, training</span>
</span>
</td>
</tr>
<tr id="A2.T12.2.3.2" class="ltx_tr">
<td id="A2.T12.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.3.2.1.1.1" class="ltx_p" style="width:69.4pt;">Data Collection Mediums</span>
</span>
</td>
<td id="A2.T12.2.3.2.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.3.2.2.1.1" class="ltx_p" style="width:325.2pt;">video, audio, screen recording, eye tracking, logs, physiological sensor, interview, survey, participant produced artifacts, researcher produced artifacts, motion, text</span>
</span>
</td>
</tr>
<tr id="A2.T12.2.4.3" class="ltx_tr">
<td id="A2.T12.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.4.3.1.1.1" class="ltx_p" style="width:69.4pt;">Modalities</span>
</span>
</td>
<td id="A2.T12.2.4.3.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.4.3.2.1.1" class="ltx_p" style="width:325.2pt;">affect, pose, gesture, activity, prosodic speech, transcribed speech, qualitative observation, logs, gaze, interview notes, survey, pulse, EDA, body temperature, blood pressure, EEG, fatigue, EMG, participant artifacts, researcher artifacts, audio spectrogram, text, pixel</span>
</span>
</td>
</tr>
<tr id="A2.T12.2.5.4" class="ltx_tr">
<td id="A2.T12.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.5.4.1.1.1" class="ltx_p" style="width:69.4pt;">Analysis Methods</span>
</span>
</td>
<td id="A2.T12.2.5.4.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.5.4.2.1.1" class="ltx_p" style="width:325.2pt;">Classification, regression, clustering, qualitative, statistical methods, network analysis, pattern extraction</span>
</span>
</td>
</tr>
<tr id="A2.T12.2.6.5" class="ltx_tr">
<td id="A2.T12.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.6.5.1.1.1" class="ltx_p" style="width:69.4pt;">Fusion Type</span>
</span>
</td>
<td id="A2.T12.2.6.5.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T12.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T12.2.6.5.2.1.1" class="ltx_p" style="width:325.2pt;">early, mid, late, hybrid, other</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T12.5.1.1" class="ltx_text" style="font-size:90%;">Table 12</span>. </span><span id="A2.T12.6.2" class="ltx_text" style="font-size:90%;">The first set of circumscribing features and their corresponding feature sets. For <span id="A2.T12.6.2.1" class="ltx_text ltx_font_italic">Environment Type</span>, items in the feature set are mutually exclusive (i.e., an environment can either be a learning <span id="A2.T12.6.2.2" class="ltx_text ltx_font_italic">or</span> training environment for the purposes of this paper, but it cannot be both). All other circumscribing features can consist of multiple items in the feature set (e.g., each paper in our corpus will contain multiple data collection mediums or modalities). Features are discussed individually in Section <a href="#S2.SS2" title="2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</span></figcaption>
</figure>
<div id="A2.SS3.p3" class="ltx_para">
<p id="A2.SS3.p3.3" class="ltx_p">During feature discretization and extraction (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 9.2), additional papers were newly identified for possible exclusion pursuant to our aforementioned criteria. After discussing each paper selected for possible exclusion, 2 papers were removed from the corpus due to all five Reviewers agreeing that each paper violated at least one exclusion criterion. After the two removals, 73 papers remained in the corpus, all of whose features were extracted into discrete sets pursuant to Table <a href="#A2.T11" title="Table 11 ‣ B.3. Feature Extraction ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> by the first full paper read round reviewer. At this point, a second and final quality control round was performed for full paper reads (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 9.3), where each of the 73 papers remaining in the corpus was assigned to a reviewer who had not yet read that particular paper. For this round, Reviewers were instructed to perform two tasks: identify any papers remaining in the corpus that violated any of the exclusion criteria (to discuss later for possible exclusion), and perform a round of feature extraction (to determine inter-rater reliability, or IRR, with respect to the initial feature extraction via Cohen’s <math id="A2.SS3.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS3.p3.1.m1.1a"><mi id="A2.SS3.p3.1.m1.1.1" xref="A2.SS3.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p3.1.m1.1b"><ci id="A2.SS3.p3.1.m1.1.1.cmml" xref="A2.SS3.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p3.1.m1.1c">k</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Cohen, <a href="#bib.bib32" title="" class="ltx_ref">1960</a>)</cite>). For this round, no additional papers were identified for exclusion, resulting in a final corpus of 73 works. Each paper’s discrete feature sets were ultimately determined via consensus coding <cite class="ltx_cite ltx_citemacro_citep">(Chinh et al<span class="ltx_text">.</span>, <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite> by the two Reviewers who read that particular paper (i.e., for each paper, both Reviewers needed to agree on the presence or absence of each item in each feature’s feature set). For reference, Cohen’s <math id="A2.SS3.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.SS3.p3.2.m2.1a"><mi id="A2.SS3.p3.2.m2.1.1" xref="A2.SS3.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.SS3.p3.2.m2.1b"><ci id="A2.SS3.p3.2.m2.1.1.cmml" xref="A2.SS3.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p3.2.m2.1c">k</annotation></semantics></math> before consensus for the first round of feature extraction was <math id="A2.SS3.p3.3.m3.1" class="ltx_Math" alttext="k=0.873" display="inline"><semantics id="A2.SS3.p3.3.m3.1a"><mrow id="A2.SS3.p3.3.m3.1.1" xref="A2.SS3.p3.3.m3.1.1.cmml"><mi id="A2.SS3.p3.3.m3.1.1.2" xref="A2.SS3.p3.3.m3.1.1.2.cmml">k</mi><mo id="A2.SS3.p3.3.m3.1.1.1" xref="A2.SS3.p3.3.m3.1.1.1.cmml">=</mo><mn id="A2.SS3.p3.3.m3.1.1.3" xref="A2.SS3.p3.3.m3.1.1.3.cmml">0.873</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.p3.3.m3.1b"><apply id="A2.SS3.p3.3.m3.1.1.cmml" xref="A2.SS3.p3.3.m3.1.1"><eq id="A2.SS3.p3.3.m3.1.1.1.cmml" xref="A2.SS3.p3.3.m3.1.1.1"></eq><ci id="A2.SS3.p3.3.m3.1.1.2.cmml" xref="A2.SS3.p3.3.m3.1.1.2">𝑘</ci><cn type="float" id="A2.SS3.p3.3.m3.1.1.3.cmml" xref="A2.SS3.p3.3.m3.1.1.3">0.873</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p3.3.m3.1c">k=0.873</annotation></semantics></math>.</p>
</div>
<div id="A2.SS3.p4" class="ltx_para">
<p id="A2.SS3.p4.1" class="ltx_p">Once our corpus was finalized, we performed one additional round of feature extraction (Table <a href="#A2.T7" title="Table 7 ‣ B.1. Literature Search ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, Step ID 9.4) to allow for greater insight into the corpus via a more in depth analysis. The features we extracted are: Environment Setting, Domain of Study, Participant Interaction Structure, Didactic Nature, Level of Instruction or Training, Analysis Approach, and Analysis Results (the findings reported from each paper). All of these features are explained in Section <a href="#S2.SS1" title="2.1. Framework ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.1</span></a> and presented again here in Table <a href="#A2.T13" title="Table 13 ‣ B.3. Feature Extraction ‣ Appendix B Corpus Distillation Procedure ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> for readability alongside their discrete values. The one exception is Analysis Results, which was not discretized due to the wide degree of variability across each paper’s findings. Instead, we noted each paper’s findings, and used them in our thematic analysis <cite class="ltx_cite ltx_citemacro_citep">(Braun and Clarke, <a href="#bib.bib17" title="" class="ltx_ref">2006</a>)</cite>, which we describe in Section <a href="#S3.SS4" title="3.4. Analysis Procedure ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<figure id="A2.T13" class="ltx_table">
<table id="A2.T13.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T13.2.1.1" class="ltx_tr">
<th id="A2.T13.2.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.1.1.1.1.1" class="ltx_p" style="width:117.1pt;">Feature</span>
</span>
</th>
<th id="A2.T13.2.1.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.1.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.1.1.2.1.1" class="ltx_p" style="width:208.1pt;">Feature Set</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T13.2.2.1" class="ltx_tr">
<td id="A2.T13.2.2.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.2.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.2.1.1.1.1" class="ltx_p" style="width:117.1pt;">Environment Setting</span>
</span>
</td>
<td id="A2.T13.2.2.1.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_tt" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.2.1.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.2.1.2.1.1" class="ltx_p" style="width:208.1pt;">physical, virtual, blended, unspecified</span>
</span>
</td>
</tr>
<tr id="A2.T13.2.3.2" class="ltx_tr">
<td id="A2.T13.2.3.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.3.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.3.2.1.1.1" class="ltx_p" style="width:117.1pt;">Domain of Study</span>
</span>
</td>
<td id="A2.T13.2.3.2.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.3.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.3.2.2.1.1" class="ltx_p" style="width:208.1pt;">STEM, humanities, psychomotor skills, other, unspecified</span>
</span>
</td>
</tr>
<tr id="A2.T13.2.4.3" class="ltx_tr">
<td id="A2.T13.2.4.3.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.4.3.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.4.3.1.1.1" class="ltx_p" style="width:117.1pt;">Participant Interaction Structure</span>
</span>
</td>
<td id="A2.T13.2.4.3.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.4.3.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.4.3.2.1.1" class="ltx_p" style="width:208.1pt;">individual, multi-person</span>
</span>
</td>
</tr>
<tr id="A2.T13.2.5.4" class="ltx_tr">
<td id="A2.T13.2.5.4.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.5.4.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.5.4.1.1.1" class="ltx_p" style="width:117.1pt;">Didactic Nature</span>
</span>
</td>
<td id="A2.T13.2.5.4.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.5.4.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.5.4.2.1.1" class="ltx_p" style="width:208.1pt;">instructional, training, informal, unspecified</span>
</span>
</td>
</tr>
<tr id="A2.T13.2.6.5" class="ltx_tr">
<td id="A2.T13.2.6.5.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.6.5.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.6.5.1.1.1" class="ltx_p" style="width:117.1pt;">Level of Instruction or Training</span>
</span>
</td>
<td id="A2.T13.2.6.5.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.6.5.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.6.5.2.1.1" class="ltx_p" style="width:208.1pt;">K-12, university, professional development, unspecified</span>
</span>
</td>
</tr>
<tr id="A2.T13.2.7.6" class="ltx_tr">
<td id="A2.T13.2.7.6.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_r" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.7.6.1.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.7.6.1.1.1" class="ltx_p" style="width:117.1pt;">Analysis Approach</span>
</span>
</td>
<td id="A2.T13.2.7.6.2" class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb" style="padding-top:1.5pt;padding-bottom:1.5pt;">
<span id="A2.T13.2.7.6.2.1" class="ltx_inline-block ltx_align_top">
<span id="A2.T13.2.7.6.2.1.1" class="ltx_p" style="width:208.1pt;">model-free, model-based</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="A2.T13.3.1.1" class="ltx_text" style="font-size:90%;">Table 13</span>. </span><span id="A2.T13.4.2" class="ltx_text" style="font-size:90%;">The second set of circumscribing features, all of which are multi-label, and their corresponding feature sets. Features are discussed individually in Section <a href="#S2.SS2" title="2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</span></figcaption>
</figure>
<div id="A2.SS3.p5" class="ltx_para">
<p id="A2.SS3.p5.1" class="ltx_p">Similar to our initial round of feature extraction, we began with inductive coding, where four Reviewers first extracted the new circumscribing features for the same papers he or she performed inductive coding on during the previous round of feature extraction. We then discussed each paper’s extracted features and formulated discrete sets for the new circumscribing features (with the exception of Analysis Results). Next, we conducted two rounds of full paper reads to extract the second set of circumscribing features. During the first round, Reviewers revisited the same papers they read during inductive coding and extracted the new circumscribing features pursuant to the agreed-upon feature sets devised during inductive coding. During the second round, Reviewers reread (and extracted the additional features from) the same set of papers they were the 2<sup id="A2.SS3.p5.1.1" class="ltx_sup">nd</sup> reviewer for during the initial round of feature extraction. At this point, for each paper, the two Reviewers who extracted that paper’s additional features performed consensus coding to define that paper’s final set of features. For reference, Cohen’s <math id="A2.SS3.p5.1.m1.1" class="ltx_Math" alttext="k=0.71" display="inline"><semantics id="A2.SS3.p5.1.m1.1a"><mrow id="A2.SS3.p5.1.m1.1.1" xref="A2.SS3.p5.1.m1.1.1.cmml"><mi id="A2.SS3.p5.1.m1.1.1.2" xref="A2.SS3.p5.1.m1.1.1.2.cmml">k</mi><mo id="A2.SS3.p5.1.m1.1.1.1" xref="A2.SS3.p5.1.m1.1.1.1.cmml">=</mo><mn id="A2.SS3.p5.1.m1.1.1.3" xref="A2.SS3.p5.1.m1.1.1.3.cmml">0.71</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.p5.1.m1.1b"><apply id="A2.SS3.p5.1.m1.1.1.cmml" xref="A2.SS3.p5.1.m1.1.1"><eq id="A2.SS3.p5.1.m1.1.1.1.cmml" xref="A2.SS3.p5.1.m1.1.1.1"></eq><ci id="A2.SS3.p5.1.m1.1.1.2.cmml" xref="A2.SS3.p5.1.m1.1.1.2">𝑘</ci><cn type="float" id="A2.SS3.p5.1.m1.1.1.3.cmml" xref="A2.SS3.p5.1.m1.1.1.3">0.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p5.1.m1.1c">k=0.71</annotation></semantics></math> for the second round of feature extraction prior to consensus coding.</p>
</div>
<div id="A2.SS3.p6" class="ltx_para">
<p id="A2.SS3.p6.1" class="ltx_p">Each item in each of the circumscribing feature sets is described in Sections <a href="#S2.SS2.SSS1" title="2.2.1. Environment Type ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.1</span></a> (Environment Type), <a href="#S2.SS2.SSS2" title="2.2.2. Data Collection Mediums ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.2</span></a> (Data Collection Mediums), <a href="#S2.SS2.SSS3" title="2.2.3. Modalities ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.3</span></a> (Modalities), <a href="#S2.SS2.SSS4" title="2.2.4. Analysis Methods ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.4</span></a> (Analysis Methods), <a href="#S2.SS2.SSS5" title="2.2.5. Data Fusion ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.5</span></a> (Data Fusion), <a href="#S2.SS2.SSS6" title="2.2.6. Environment Setting ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.6</span></a> (Environment Setting), <a href="#S2.SS2.SSS7" title="2.2.7. Domain of Study ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.7</span></a> (Domain of Study), <a href="#S2.SS2.SSS8" title="2.2.8. Participant Interaction Structure ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.8</span></a> (Participant Interaction Structure), <a href="#S2.SS2.SSS9" title="2.2.9. Didactic Nature ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.9</span></a> (Didactic Nature), <a href="#S2.SS2.SSS10" title="2.2.10. Level of Instruction or Training ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.10</span></a> (Level of Instruction or Training), and <a href="#S2.SS2.SSS11" title="2.2.11. Analysis Approach ‣ 2.2. Taxonomy ‣ 2. Framework and Taxonomy ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.2.11</span></a> (Analysis Approach).</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Literature Review Limitations</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p">The limitations of this work involve the use of Google Scholar to conduct the literature search, the use of a citation graph for programmatic corpus reduction, and a lack of screening for peer reviewed papers. All are discussed below.</p>
</div>
<section id="A3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1. </span>Google Scholar.</h3>

<div id="A3.SS1.p1" class="ltx_para">
<p id="A3.SS1.p1.1" class="ltx_p">While Google Scholar is widely used by researchers across both academia and industry, it poses a challenge for reproducibility. Like Google Search, Google Scholar is a proprietary search algorithm that is assumed to vary its results based on context. Factors such as the individual user conducting the search, the user’s geolocation, the date the search is conducted, and the user’s search history may all affect how Google Scholar collates search results. Google may also perform A/B testing in live environments to determine which version of its algorithm users deem more effective. The algorithm is also (presumably) continually evolving, and users are unable to know exactly which version of the algorithm was used to conduct a particular search. As such, there is little expectation that our initial corpus will be able to be reconstructed <span id="A3.SS1.p1.1.1" class="ltx_text ltx_font_italic">in its exact form</span> without at least some degree of variability.</p>
</div>
<div id="A3.SS1.p2" class="ltx_para">
<p id="A3.SS1.p2.1" class="ltx_p">However, the authors are confident the degree of variability from different Google Scholar searches does not prohibit the <span id="A3.SS1.p2.1.1" class="ltx_text ltx_font_italic">overall</span> reproducibility of the initial corpus. While SerpAPI’s web scraping method is proprietary, its creators address several of our concerns in their documentation <cite class="ltx_cite ltx_citemacro_citep">(SerpApi, <a href="#bib.bib130" title="" class="ltx_ref">NA</a>)</cite>. The API’s search does not use information from any individual user’s Google account when conducting the web scrape, as no Google account is attached to the SerpAPI account, API key, or API calls themselves. Instead, calls are made via proxy and random headers, as illustrated in Figure <a href="#A3.F8" title="Figure 8 ‣ C.1. Google Scholar. ‣ Appendix C Literature Review Limitations ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. When trying to reproduce the API’s results via manual search, SerpAPI recommends using the URL in the API’s JSON results in ”incognito mode”.</p>
</div>
<figure id="A3.F8" class="ltx_figure ltx_align_floatleft">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2408.14491/assets/img/SERP_API_diagram.drawio.png" id="A3.F8.1.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="299" height="95" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A3.F8.3.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>. </span><span id="A3.F8.4.2" class="ltx_text" style="font-size:90%;">Searching Google Scholar via SerpAPI.</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span id="A3.F8.5" class="ltx_ERROR ltx_figure_panel undefined">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p id="A3.F8.6" class="ltx_p ltx_figure_panel">[Searching Google Scholar via SerpAPI]Searching Google Scholar via SerpAPI.</p>
</div>
</div>
</figure>
<div id="A3.SS1.p3" class="ltx_para">
<p id="A3.SS1.p3.1" class="ltx_p">Additionally, we reached out to SerpAPI directly and asked, ”Does SerpAPI attach personal or identifying information when making request?”, to which SerpAPI responded, ”No, we don’t add any personal information.” SerpAPI also stated, ”…others can reproduce your results by using Google Scholar website, if they use the same search criteria…”, but we believe this to be an overstatement given Google’s lack of transparency. While we cannot guarantee perfect reproducibility due to the aforementioned issues, we can state with a reasonable degree of confidence that our own individual search biases did not influence the initial search results (outside of the choosing of the search terms) due to how SerpAPI handles API calls to Google Scholar. For reference, this review’s literature search was conducted by an author of this paper in Nashville, TN, USA.</p>
</div>
</section>
<section id="A3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2. </span>Citation Graph Pruning.</h3>

<div id="A3.SS2.p1" class="ltx_para">
<p id="A3.SS2.p1.1" class="ltx_p">As discussed in Section <a href="#S3.SS2.SSS1" title="3.2.1. Citation Graph Pruning (Quantitative Corpus Reduction). ‣ 3.2. Study Selection ‣ 3. Methods ‣ Multimodal Methods for Analyzing Learning and Training Environments: A Systematic Literature Review" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>, we initially distilled our corpus quantitatively via citation graph pruning. In doing so, it is possible we excluded relevant works from our corpus based on them only having cited or been cited by a minimal number of other works in our corpus. However, this paper is a literature review of the prominent methods researchers are applying to multimodal learning and training environments. As such, the authors agreed that if a work did not utilize a large degree of previous research (i.e., cite several other works in the corpus) or serve as a base from which a large degree of other research has built upon (i.e., be cited by several other works in the corpus), then that work was, by definition, outside the scope of our review. Considering our corpus was still largely comprised (over 50%) of works later deemed to be outside the scope of this review after CGP, the authors are confident that few papers (if any) directly pertaining to multimodal learning and training environments were discarded as a result of CGP.</p>
</div>
</section>
<section id="A3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3. </span>Peer Review.</h3>

<div id="A3.SS3.p1" class="ltx_para">
<p id="A3.SS3.p1.1" class="ltx_p">Due to the prevalence of papers being published to open, non-peer-reviewed platforms like arXiv in recent years (particularly in computer science), we did not screen for non-peer-reviewed works during study selection (i.e., we did not adopt a paper’s not being peer-reviewed as an exclusion criterion). To the best of our knowledge, all papers in our corpus underwent formal peer-review, with one possible exception. There is one paper in the corpus that was submitted to a workshop that none of this review’s authors are familiar with. We are, therefore, unsure of whether or not the paper underwent formal peer review. However, the workshop includes submission, notification, and camera ready dates, so we are confident that the workshop was at least refereed.</p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2408.14487" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2408.14491" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2408.14491">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2408.14491" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2408.14492" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Sep  5 16:11:14 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
