<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.09795] An Advanced Data Fabric Architecture Leveraging Homomorphic Encryption and Federated Learning</title><meta property="og:description" content="Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems. In a Federated learning architec…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="An Advanced Data Fabric Architecture Leveraging Homomorphic Encryption and Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="An Advanced Data Fabric Architecture Leveraging Homomorphic Encryption and Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.09795">

<!--Generated on Tue Mar  5 15:51:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_fleqn">
<h1 class="ltx_title ltx_title_document">An Advanced Data Fabric Architecture Leveraging Homomorphic Encryption and Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sakib Anwar Rieyan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sakib.anwar.rieyan@g.bracu.ac.bd">sakib.anwar.rieyan@g.bracu.ac.bd</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md. Raisul Kabir News
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:md.raisul.kabir.news@g.bracu.ac.bd">md.raisul.kabir.news@g.bracu.ac.bd</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">A.B.M. Muntasir Rahman
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:a.b.m.muntasir.rahman@g.bracu.ac.bd">a.b.m.muntasir.rahman@g.bracu.ac.bd</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sadia Afrin Khan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sadia.afrin.khan@g.bracu.ac.bd">sadia.afrin.khan@g.bracu.ac.bd</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sultan Tasneem Jawad Zaarif
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:sultan.tasneem.jawad.zaarif@g.bracu.ac.bd">sultan.tasneem.jawad.zaarif@g.bracu.ac.bd</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Md. Golam Rabiul Alam
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:rabiul.alam@bracu.ac.bd">rabiul.alam@bracu.ac.bd</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mohammad Mehedi Hassan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:mmhassan@ksu.edu.sa">mmhassan@ksu.edu.sa</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Michele Ianni
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:michele.ianni@unical.it">michele.ianni@unical.it</a>
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giancarlo Fortino
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:giancarlo.fortino@unical.it">giancarlo.fortino@unical.it</a>
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p"><span id="id1.id1.1" class="ltx_text ltx_font_bold">Data fabric is an automated and AI-driven data fusion approach to accomplish data management unification without moving data to a centralized location for solving complex data problems. In a Federated learning architecture, the global model is trained based on the learned parameters of several local models that eliminate the necessity of moving data to a centralized repository for machine learning. This paper introduces a secure approach for medical image analysis using federated learning and partially homomorphic encryption within a distributed data fabric architecture. With this method, multiple parties can collaborate in training a machine-learning model without exchanging raw data but using the learned or fused features. The approach complies with laws and regulations such as HIPAA and GDPR, ensuring the privacy and security of the data. The study demonstrates the method’s effectiveness through a case study on pituitary tumor classification, achieving a significant level of accuracy. However, the primary focus of the study is on the development and evaluation of federated learning and partially homomorphic encryption as tools for secure medical image analysis. The results highlight the potential of these techniques to be applied to other privacy-sensitive domains and contribute to the growing body of research on secure and privacy-preserving machine learning.</span></p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Data Fabric , Federated Learning , Partially Homomorphic Encryption, Data Fusion , Data Lake

</div>
<span id="id1" class="ltx_note ltx_note_frontmatter ltx_role_journal"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>Information Fusion</span></span></span>
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\affiliation</span>
<p id="p1.2" class="ltx_p">[inst1]organization=Department of Computer Science and Engineering, School of Data and Sciences, BRAC University,addressline=66 Mohakhali,
city=Dhaka,
postcode=1212,
country=Bangladesh</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\affiliation</span>
<p id="p2.2" class="ltx_p">[2]organization=Department of Information Systems, College of Computer and Information Sciences ,
addressline=King Saud University,
city=Riyadh,
postcode=11543,
country=Saudi Arabia

</p>
</div>
<div id="p3" class="ltx_para">
<span id="p3.1" class="ltx_ERROR undefined">\affiliation</span>
<p id="p3.2" class="ltx_p">[3]organization=Department of Informatics, Modeling, Electronics, and Systems,,
addressline=University of Calabria,
city=Rende, CS,
postcode=87036,
country=Italy
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Inroduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Artificial intelligence (AI) has emerged as an indispensable component of our everyday lives, particularly within the realm of healthcare. To harness the full potential of AI in healthcare, it becomes imperative to obtain substantial quantities of meticulously curated data. Nevertheless, the preservation and examination of healthcare data encounter notable impediments due to the privacy and sensitivity associated with it. The sheer magnitude of healthcare data is frequently substantial, primarily attributable to the abundance of image-based data. Moreover, the preservation of the confidentiality of healthcare data is paramount, considering its personal and delicate nature.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To address these challenges, we have built an advanced data fabric architecture that brings together healthcare centers in a region and stores patient data and diagnoses in a secure and privacy-preserving manner. Data fabric is a data fusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> and integration approach to accomplish data management unification through analytics and AI. The proposed approach utilizes federated learning and partially homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> to allow for collaborative machine learning on encrypted data, while still maintaining compliance with laws and regulations such as the Health Insurance Portability and Accountability Act (HIPAA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and the General Data Protection Regulation (GDPR) Act 2018 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this study, we have used pituitary tumor classification as a case study, employing various deep-learning models such as VGG16, VGG19, ResNet50, and ResNet152. Our results show promising potential for the use of federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> and partially homomorphic encryption in secure medical image analysis. Specifically, we achieved good performance with VGG16 and VGG19 models, while ResNet50 and ResNet152 achieved lower accuracy and precision for both classes. However, our custom CNN architecture outperformed all of these pre-trained models in almost every metric that we used. Our findings contribute to the growing body of research on secure and privacy-preserving <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> machine learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> and demonstrate the potential for these techniques to be applied in other privacy-sensitive domains.</p>
</div>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Motivation</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">In the field of medical image analysis, ensuring the security of sensitive patient data is of utmost importance. However, with the increasing use of machine learning and deep learning techniques for medical image analysis, there is a pressing need for an effective and secure architecture to handle such data. Previous studies have shown that the use of conventional security measures, such as encryption and access control, is not enough to ensure the privacy of patient data in the context of machine learning and deep learning operations. In addition, the use of traditional centralized architectures for processing medical image data can be slow and resource-intensive, which can further compromise the security of the data.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Therefore, there is a clear need for a new, advanced data fabric architecture that is specifically designed to handle the unique challenges of securing medical image data while also supporting efficient machine learning and deep learning operations. This research aims to address this gap in the current state of the art by proposing and evaluating a novel architecture that is capable of effectively securing medical image data while also enabling fast and accurate machine learning and deep learning operations.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Research Problems</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">The integration of data into healthcare has the potential to improve the prediction of diseases and epidemics, enhance treatment outcomes, and prevent premature deaths. However, the confidentiality of healthcare data and the complexity of managing large and diverse datasets pose significant challenges to the integration of data into healthcare. Ensuring data security and privacy is of utmost importance, as security breaches in healthcare are on the rise. According to a study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, there were 3,033 data breaches reported between 2010 and 2019, resulting in the exposure of 255.18 million records of data.
Furthermore, the substantial volume of healthcare data presents challenges in terms of efficient processing, storage, and communication. Conventional methods may prove inadequate when confronted with the magnitude of the data at hand.
In one proposed solution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, a big data healthcare cloud would host clinical, financial, social, physical, and psychological data from patients in a centralized location. However, proper governance of the data cloud is necessary to effectively work with and analyze complex data.</p>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">In this study, we aim to address these challenges by proposing an advanced data fabric architecture that brings together healthcare centers in a region and stores patient data and diagnoses in a secure and privacy-preserving manner using federated learning and partially homomorphic encryption. We demonstrate the effectiveness of our approach using pituitary tumor classification as a case study. However, the primary focus of our work is on the development and evaluation of federated learning and partially homomorphic encryption as tools for secure medical image analysis in the healthcare sector.</p>
</div>
<div id="S1.SS2.p3" class="ltx_para">
<p id="S1.SS2.p3.1" class="ltx_p">The primary objective of this study is to address the research question:
</p>
</div>
<div id="S1.SS2.p4" class="ltx_para">
<p id="S1.SS2.p4.1" class="ltx_p"><span id="S1.SS2.p4.1.1" class="ltx_text ltx_font_italic">How effective and practical is the implementation of advanced data fabric architecture using federated learning and partially homomorphic encryption for secure medical image analysis in the healthcare sector?</span></p>
</div>
</section>
<section id="S1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>Contributions</h3>

<div id="S1.SS3.p1" class="ltx_para">
<p id="S1.SS3.p1.1" class="ltx_p">Through our work, we show a fully-fledged data fabric architecture based on healthcare data can be built whilst complying with privacy regulations and maintaining good accuracy scores. Our main contribution is threefold:</p>
</div>
<div id="S1.SS3.p2" class="ltx_para">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We propose an advanced data fabric architecture for storing and collaborating and fusing healthcare data in an encrypted form by using Partial Homomorphic Encryption (PHE) and sharing it with other parties without revealing its content. In this architecture, medical images of various patients/clients are encrypted on the client side and these encrypted images are then used as inputs for deep learning models, enabling the models to learn and classify tumors. Subsequently, the system collects the classified tumor data for further analysis and processing. Further processing of data is done in its encrypted state. Here, The raw data was encrypted and generated to local weights using the local FL Model before getting global attention. Therefore even if the data was backtracked the end result will produce nothing but an encrypted image. Thus, this architecture provides a secure and efficient mechanism for processing encrypted data, while preserving data privacy and confidentiality regulations such as HIPAA and GDPR.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">The proposed federated learning framework enables multiple clients to collaboratively train machine learning models on their respective data and then store the local updates, unlike the existing general federated learning frameworks which function on real-time local and global updates. Our framework offers the flexibility to modify, scale, merge or select the local model updates before using them into the global model. In this way, the framework we proposed facilitates the systematic exchange of model updates between the local and global models.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">Moreover, we have tailored a convolutional neural network (CNN) architecture, inspired by VGG16 and VGG19, with a smaller input size, resulting in a reduced parameter size compared to the aforementioned models. This customization enables enhanced efficiency by reducing computational complexity, particularly when leveraging Partially Homomorphic Encryption (PHE) techniques.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">We further evaluate the proposed approach by implementing a prototype of the homomorphic encryption-based data fabric and the federated learning framework. The assessment indicates that the suggested method offers an effective and reliable data fusion for sharing and analyzing data securely. The experimental results demonstrate that the proposed approach achieves satisfactory accuracy in the collaborative training of machine learning models, even when the data is encrypted.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature Review</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Data fabric architecture is a relatively new concept that has already been utilized by notable organizations, including IBM, for data fusion, management, and unification purposes. Despite its potential benefits, there is limited research available on the implementation of this architecture in the healthcare system. Due to the sensitive nature of healthcare, developing a secure data fabric architecture can present challenges. This chapter examines various data architectures, processes, and encryption methods that can be employed to ensure the security of healthcare data.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">There are a few works that are related to the architecture we are working on. They are described below:</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, the authors describe a proof-of-concept implementation that uses the Hyperledger Fabric framework. They claim that this concept is capable of storing patient records effectively with keeping all the privacy protocols intact. Lastly, they compare the read-write times according to their claim.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, Roehrs et al. divide personal health record (PHR) information into data blocks that may appear to be centrally stored but are actually distributed among participating devices. The authors claim that their proposed openPHR protocol is practical, flexible, and scalable for adoption by multiple organizations. Although the authors provide a detailed architecture, questions have been raised about the feasibility of their approach, especially regarding security and privacy concerns. It is important to note that PHRs are controlled by patients, while electronic health records (EHRs) are managed by healthcare institutions. Nevertheless, EHRs and PHRs are electronically stored and distributed and can be assessed based on metrics such as performance, scalability, privacy protection, and compliance with the GDPR.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, the MeDShare platform shares several similarities with PREHEALTH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. However, the authors do not explicitly specify the underlying blockchain framework. Additionally, their emphasis is more on examining the fundamental components of blockchain technology, including data blocks and smart contracts, rather than presenting a practical solution.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">Ming and Zhang <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> present an effective privacy-preserving access control (PPAC) strategy for cloud-based EHR systems. Their approach utilizes the cuckoo filter and an innovative attribute-based signcryption (ABSC) mechanism to achieve both anonymity and computational efficiency. The authors offer comprehensive assurances of privacy and conduct thorough performance evaluations for comparison. However, it is uncertain whether their approach complies with the GDPR regulations.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, Fu et al. mainly focus on sharing data among different participants safely. Using Hyperledger Fabric, they propose a more secure decentralized distributed data storage over traditional centralized data for SKA data due to high management costs and low credible traceability. The SKA Data Management Alliance significantly reduces costs and improves the overall security of its data by adopting this distributed storage system.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Our architecture utilizes federated learning to train deep learning models. In our research, we have come across some related studies that are somewhat aligned with our work. Here are brief descriptions of these studies:</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the authors propose a secured model of federated learning using homomorphic encryption and attempt to classify Covid-19 using X-Ray images. They secure the federated process and claim that sensitive information can fall into the hands of attackers if the DL process is not secured. They do not encrypt the dataset but rather encrypt the weight matrix used in federated learning. They claim to achieve an accuracy of 84.00% with a precision of 86.89%.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p">It is obvious that with the vast growth of artificial intelligence and big data, contradictions between user data policy and data policy also grow proportionally. That is why in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, the authors come up with a vertical federated learning system for Bayesian machine learning using homomorphic encryption. Their model can be compared with 90% of models trained by a single union server. Additionally, their system can be used in education, finance, medicine, risk controls, and other fields.</p>
</div>
<div id="S2.p11" class="ltx_para">
<p id="S2.p11.1" class="ltx_p">Here are some additional studies that are relevant to our research and have been reviewed and summarized below:
</p>
</div>
<div id="S2.p12" class="ltx_para">
<p id="S2.p12.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, the authors provide insights into the difficulties of medical data analysis and security and propose a solution based on a decentralized architecture. They utilize the Exonum framework. In their proposed architecture, they separate the whole system into two parts - ’Closed Information’ and ’Open Information.’ In the closed part, encrypted data is stored in a blockchain, while in the open part, non-encrypted service-related data is stored.</p>
</div>
<div id="S2.p13" class="ltx_para">
<p id="S2.p13.1" class="ltx_p">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> propose a meaningful usage of optimizing Electronic Health Records (EHRs) using big data analytics. Here, they propose an insight into how to improve electronic health records using three methods: Data Collection, Data Storage, and Data Utilization. Firstly, in the data collection method, records are divided into structured and unstructured data. Structured data includes demographics, health status, lab results, billing, etc., while surgical videos or diagnosis notes fall under unstructured data. After collecting, they propose a transformation engine where data is moved, cleaned, merged, and validated, and is stored in DBMS, Cloud, or NoSQL. Finally, transformed data is processed using mapping and reduction, and stream computing and in-database analytics are used for generating reporting systems, which help achieve a meaningful usage of EHRs using big data analytics. However, the authors also map out the limitations of this research, and emphasize that it lays the foundation for interesting opportunities in the future.</p>
</div>
<div id="S2.p14" class="ltx_para">
<p id="S2.p14.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, the authors introduce a novel big data platform that can redesign modern medical data and bring an effective and quick solution to the healthcare system. They propose a system that is lightning-fast, supports stream processing, and integrates with both NoSQL and RDBMS. This process aims to exploit open-source technologies as much as possible and build the system on top of them. The core of the system is Spark core, and to utilize it, the system uses the Spark framework with real-time graphical image processing. For handling structured data, the authors propose SparkSQL Structured Data and MLib Machine Learning for classifying the data.</p>
</div>
<div id="S2.p15" class="ltx_para">
<p id="S2.p15.1" class="ltx_p">In <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, the authors describe an architecture where an improvised big data model is involved to create a cloud computing environment for healthcare. In this process, huge amounts of data from medical sources and processes are collected in cloud storage, and real-time analysis is done using cloud computing for better accuracy. The Healthcare Data Management Framework is built on Hadoop Clusters and certain key components. Semantic Practitioner, Big data container and processing layer, Query formulator, Batch scheduling, and Data reader are examples of such tools. This architecture is also open to implementing various cryptographic techniques on the Cloud.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background Studies</h3>

</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data Fabric</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">According to Gartner <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, Data Fabric is a unified and integrated platform that enables data discovery, fusion and integration, management, and access across multiple environments. It provides a consistent and scalable approach to managing data assets that are distributed across various locations, such as on-premises, cloud, and edge computing. Data Fabric helps organizations to simplify and optimize their data management processes, reduce data silos, and enable real-time access to data. It also supports the creation of a self-service data marketplace, allowing users to discover, share, and consume data in a secure and governed manner. Data Fabric is increasingly becoming a critical component of modern data architectures, as organizations seek to manage the growing volume, velocity, and variety of data generated by digital business initiatives.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In our research, we are utilizing homomorphic encryption to classify pituitary tumors from MRI images in our dataset. We have used a Data Fabric architecture to store the weights of different machine-learning models as encrypted data. The ML models are run on client PCs, and the resulting encrypted data is saved in our Data Lake. Using homomorphic encryption, a server PC can perform computations on the encrypted data, allowing for the creation of a homogeneous global model. The server can then provide users with the requested results without compromising the privacy of the MRI images. This approach benefits from the Data Fabric’s ability to provide a unified and integrated platform that enables data discovery, integration, management, and access across multiple environments. By utilizing homomorphic encryption and a Data Fabric architecture, we can classify pituitary tumors from MRI images in a privacy-preserving manner, contributing to the development of more secure and privacy-preserving medical imaging technologies.</p>
</div>
<section id="S2.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Vanilla Architecture of Data Fabric</h4>

<div id="S2.SS2.SSS1.p1" class="ltx_para">
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Accessing Data:</span></p>
<ol id="S2.I1.i1.I1" class="ltx_enumerate">
<li id="S2.I1.i1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S2.I1.i1.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Data Collecting and Encryption:</span> Data is a volatile resource, and Medical Data is considered highly sensitive as it can contain personally identifiable information such as names, addresses, dates of birth, and medical records which can be exploited if they fell into wrong hands. To comply with this shortcoming, in this architecture, data is neither collected nor stored in a central server which may possess the risk of data leakage.</p>
</div>
<div id="S2.I1.i1.I1.i1.p2" class="ltx_para">
<p id="S2.I1.i1.I1.i1.p2.1" class="ltx_p">Here, firstly, to ensure privacy and reduce data volatility, medical data from various users are first selected and then encrypted with Partially Homomorphic Encryption (PHE). Subsequently, the encrypted data is selected to train the model locally for collecting updated model weights. The data of each user is generated and stored locally, without being transferred to the central server. Instead, the generated model updates are stored and merged for the global model formation.</p>
</div>
</li>
<li id="S2.I1.i1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S2.I1.i1.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i1.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Master Data Management:</span> Following the generation of local model updates and subsequent merging of data, feature selection is employed as a means of optimizing and enhancing efficiency. By selecting the most relevant features or weights, the dimensionality of the data can be reduced, facilitating ease of analysis. Moreover, feature selection mitigates the risk of overfitting, improves model accuracy, and reduces computational costs, thereby achieving heightened efficiency through the utilization of a reduced training dataset.</p>
</div>
<div id="S2.I1.i1.I1.i2.p2" class="ltx_para">
<p id="S2.I1.i1.I1.i2.p2.1" class="ltx_p">FedMax, FedAvg, and FedMin are optimization algorithms used in Federated Learning for feature selection. In all three algorithms, updated model weights are sent to the server/stored for future usage. However, in the case of FedMax, the server/user selects the model with the highest accuracy while it chooses the lowest loss model for FedMin and an average of all models for FedAvg.</p>
</div>
<div id="S2.I1.i1.I1.i2.p3" class="ltx_para">
<p id="S2.I1.i1.I1.i2.p3.1" class="ltx_p">In our architecture, we selected FedMax as our feature selection algorithm to select important and relevant model weights as it showed more accuracy and efficiency compared to FedAvg and FedMin. The selected data is collected and kept together as “Master Data”.
</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Managing Life Cycle:</span></p>
<ol id="S2.I1.i2.I1" class="ltx_enumerate">
<li id="S2.I1.i2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(a)</span> 
<div id="S2.I1.i2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p1.1" class="ltx_p"><span id="S2.I1.i2.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Governance:</span> Data governance is an essential component of data fabric architecture. Data fabric architecture is an approach to data management that enables organizations to manage and process data from multiple sources, locations, and formats. It provides a unified view of data across the organization and supports various data processing requirements, such as data integration, analytics, and artificial intelligence.</p>
</div>
<div id="S2.I1.i2.I1.i1.p2" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p2.1" class="ltx_p">Data governance in data fabric architecture refers to the policies, processes, and standards that organizations implement to manage their data assets effectively. Data governance helps organizations ensure that their data is accurate, consistent, and compliant with regulatory requirements. It also helps organizations manage data privacy, security, and access.</p>
</div>
<div id="S2.I1.i2.I1.i1.p3" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p3.1" class="ltx_p">The following are some key considerations for data governance in data fabric architecture:</p>
</div>
<div id="S2.I1.i2.I1.i1.p4" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p4.1" class="ltx_p"><span id="S2.I1.i2.I1.i1.p4.1.1" class="ltx_text ltx_font_bold">Data quality:</span> Data governance policies should include measures to ensure data quality, such as data profiling, data cleansing, and data validation.</p>
</div>
<div id="S2.I1.i2.I1.i1.p5" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p5.1" class="ltx_p"><span id="S2.I1.i2.I1.i1.p5.1.1" class="ltx_text ltx_font_bold">Metadata management:</span> Data governance policies should include metadata management to ensure that data is properly tagged, categorized, and classified. Metadata helps organizations understand the meaning and context of their data and facilitates data discovery and reuse.</p>
</div>
<div id="S2.I1.i2.I1.i1.p6" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p6.1" class="ltx_p"><span id="S2.I1.i2.I1.i1.p6.1.1" class="ltx_text ltx_font_bold">Data privacy and security:</span> Data governance policies should include measures to ensure data privacy and security, such as access controls, data encryption, and data masking.</p>
</div>
<div id="S2.I1.i2.I1.i1.p7" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p7.1" class="ltx_p"><span id="S2.I1.i2.I1.i1.p7.1.1" class="ltx_text ltx_font_bold">Data lineage:</span> Data governance policies should include data lineage to track the origin, transformation, and movement of data across the organization. Data lineage helps organizations understand how data is used and facilitates compliance with regulatory requirements.</p>
</div>
<div id="S2.I1.i2.I1.i1.p8" class="ltx_para">
<p id="S2.I1.i2.I1.i1.p8.1" class="ltx_p"><span id="S2.I1.i2.I1.i1.p8.1.1" class="ltx_text ltx_font_bold">Data ownership and stewardship:</span> Data governance policies should define data ownership and stewardship to ensure that data is managed and maintained by the appropriate individuals and teams.</p>
</div>
</li>
<li id="S2.I1.i2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(b)</span> 
<div id="S2.I1.i2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.I1.i2.p1.1" class="ltx_p"><span id="S2.I1.i2.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Compliance: </span> Data compliance refers to adhering to relevant laws, regulations, and industry standards related to the handling, processing, and storage of data. In the context of data fabric, data compliance refers to ensuring that data is managed in accordance with these requirements across the entire data fabric. To ensure data compliance in a data fabric, it is necessary to establish policies and procedures that cover the entire data lifecycle, from data ingestion to archival and deletion. Personal data must be collected, processed, and stored in compliance with privacy regulations such as GDPR, CCPA, HIPAA, etc.</p>
</div>
<div id="S2.I1.i2.I1.i2.p2" class="ltx_para">
<p id="S2.I1.i2.I1.i2.p2.1" class="ltx_p">In this architecture, the feature selected weights were trained on various models such as VGG16, VGG19, ResNet 50, ResNet 152, etc. and updates are stored in a data lake structurally based on models they were trained on complying with HIPAA regulations.
</p>
</div>
</li>
</ol>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p"><span id="S2.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Exposing Data:</span> Data exposure refers to making data available for consumption and analysis by users or applications within an organization. Exposing data in a data fabric involves providing access to the data for authorized users or applications. There are several ways to expose data in a data fabric, including:</p>
</div>
<div id="S2.I1.i3.p2" class="ltx_para">
<p id="S2.I1.i3.p2.1" class="ltx_p"><span id="S2.I1.i3.p2.1.1" class="ltx_text ltx_font_bold">APIs:</span> Application Programming Interfaces (APIs) enable applications to access and retrieve data from the data fabric.</p>
</div>
<div id="S2.I1.i3.p3" class="ltx_para">
<p id="S2.I1.i3.p3.1" class="ltx_p"><span id="S2.I1.i3.p3.1.1" class="ltx_text ltx_font_bold">Data Catalogs:</span> A data catalog provides a searchable inventory of data assets in the data fabric. Users can discover and access data assets through the data catalog.</p>
</div>
<div id="S2.I1.i3.p4" class="ltx_para">
<p id="S2.I1.i3.p4.1" class="ltx_p"><span id="S2.I1.i3.p4.1.1" class="ltx_text ltx_font_bold">Self-Service Analytics:</span> A self-service analytics platform enables users to create their own queries and reports using the data available in the data fabric.</p>
</div>
<div id="S2.I1.i3.p5" class="ltx_para">
<p id="S2.I1.i3.p5.1" class="ltx_p"><span id="S2.I1.i3.p5.1.1" class="ltx_text ltx_font_bold">Data Virtualization:</span> Data virtualization enables users to access and combine data from multiple sources as if it were in a single location.</p>
</div>
</li>
</ol>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2402.09795/assets/images/Vanilla_Architecture_Data_Fabric.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="515" height="172" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Vanilla Architecture of Data Fabric</figcaption>
</figure>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Federated Learning</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Federated Learning is a distributed machine learning technique that enables multiple clients to collaboratively learn a shared model without exchanging their raw data. This technique has gained popularity in recent years due to its ability to preserve data privacy and security while improving model performance. In Federated Learning, each client trains a local model using its own data and then sends the local model weights to a central server. The central server then aggregates the local model weights to update a global model that is shared among all clients. This process continues iteratively until the global model achieves the desired level of accuracy. According to the report <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, Federated Learning has been successfully applied to various domains, such as speech recognition, natural language processing, and healthcare, where data privacy is a major concern.</p>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Federated Learning Process</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span>Initialize global model

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span>Split data into shards and distribute among clients

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><span id="alg1.l3.2" class="ltx_text ltx_font_bold">for</span>  i in range(num_communication_rounds)  <span id="alg1.l3.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span>     Initialize local model

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span>     Send local model to each client

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span>     <span id="alg1.l6.2" class="ltx_text ltx_font_bold">for</span>  client in clients_data  <span id="alg1.l6.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>         client.model.train(client.data)

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span>     <span id="alg1.l8.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l8.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span>     Clients send updates to the global model

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span>     Scale updates by the weight scaling factor

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>     Aggregate updates using FedMax

</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span>     
<span id="alg1.l12.2" class="ltx_ERROR undefined">{fleqn}</span>[]


<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_left_padleft"></td>
<td class="ltx_eqn_cell ltx_align_left"><math id="S2.Ex1.m1.66" class="ltx_Math" alttext="\begin{multlined}global\_model.update\_weights(max(updates)\\
\times weight\_scaling\_factors)\end{multlined}global\_model.update\_weights(max(updates)\\
\times weight\_scaling\_factors)" display="block"><semantics id="S2.Ex1.m1.66a"><mtable displaystyle="true" rowspacing="0pt" id="S2.Ex1.m1.64.64"><mtr id="S2.Ex1.m1.64.64a"><mtd class="ltx_align_left" columnalign="left" id="S2.Ex1.m1.64.64b"><mrow id="S2.Ex1.m1.40.40.40.40.40"><mi id="S2.Ex1.m1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.cmml">g</mi><mi id="S2.Ex1.m1.2.2.2.2.2.2" xref="S2.Ex1.m1.2.2.2.2.2.2.cmml">l</mi><mi id="S2.Ex1.m1.3.3.3.3.3.3" xref="S2.Ex1.m1.3.3.3.3.3.3.cmml">o</mi><mi id="S2.Ex1.m1.4.4.4.4.4.4" xref="S2.Ex1.m1.4.4.4.4.4.4.cmml">b</mi><mi id="S2.Ex1.m1.5.5.5.5.5.5" xref="S2.Ex1.m1.5.5.5.5.5.5.cmml">a</mi><mi id="S2.Ex1.m1.6.6.6.6.6.6" xref="S2.Ex1.m1.6.6.6.6.6.6.cmml">l</mi><mi mathvariant="normal" id="S2.Ex1.m1.7.7.7.7.7.7" xref="S2.Ex1.m1.7.7.7.7.7.7.cmml">_</mi><mi id="S2.Ex1.m1.8.8.8.8.8.8" xref="S2.Ex1.m1.8.8.8.8.8.8.cmml">m</mi><mi id="S2.Ex1.m1.9.9.9.9.9.9" xref="S2.Ex1.m1.9.9.9.9.9.9.cmml">o</mi><mi id="S2.Ex1.m1.10.10.10.10.10.10" xref="S2.Ex1.m1.10.10.10.10.10.10.cmml">d</mi><mi id="S2.Ex1.m1.11.11.11.11.11.11" xref="S2.Ex1.m1.11.11.11.11.11.11.cmml">e</mi><mi id="S2.Ex1.m1.12.12.12.12.12.12" xref="S2.Ex1.m1.12.12.12.12.12.12.cmml">l</mi><mo lspace="0em" rspace="0.167em" id="S2.Ex1.m1.13.13.13.13.13.13" xref="S2.Ex1.m1.66.66.2.3.cmml">.</mo><mi id="S2.Ex1.m1.14.14.14.14.14.14" xref="S2.Ex1.m1.14.14.14.14.14.14.cmml">u</mi><mi id="S2.Ex1.m1.15.15.15.15.15.15" xref="S2.Ex1.m1.15.15.15.15.15.15.cmml">p</mi><mi id="S2.Ex1.m1.16.16.16.16.16.16" xref="S2.Ex1.m1.16.16.16.16.16.16.cmml">d</mi><mi id="S2.Ex1.m1.17.17.17.17.17.17" xref="S2.Ex1.m1.17.17.17.17.17.17.cmml">a</mi><mi id="S2.Ex1.m1.18.18.18.18.18.18" xref="S2.Ex1.m1.18.18.18.18.18.18.cmml">t</mi><mi id="S2.Ex1.m1.19.19.19.19.19.19" xref="S2.Ex1.m1.19.19.19.19.19.19.cmml">e</mi><mi mathvariant="normal" id="S2.Ex1.m1.20.20.20.20.20.20" xref="S2.Ex1.m1.20.20.20.20.20.20.cmml">_</mi><mi id="S2.Ex1.m1.21.21.21.21.21.21" xref="S2.Ex1.m1.21.21.21.21.21.21.cmml">w</mi><mi id="S2.Ex1.m1.22.22.22.22.22.22" xref="S2.Ex1.m1.22.22.22.22.22.22.cmml">e</mi><mi id="S2.Ex1.m1.23.23.23.23.23.23" xref="S2.Ex1.m1.23.23.23.23.23.23.cmml">i</mi><mi id="S2.Ex1.m1.24.24.24.24.24.24" xref="S2.Ex1.m1.24.24.24.24.24.24.cmml">g</mi><mi id="S2.Ex1.m1.25.25.25.25.25.25" xref="S2.Ex1.m1.25.25.25.25.25.25.cmml">h</mi><mi id="S2.Ex1.m1.26.26.26.26.26.26" xref="S2.Ex1.m1.26.26.26.26.26.26.cmml">t</mi><mi id="S2.Ex1.m1.27.27.27.27.27.27" xref="S2.Ex1.m1.27.27.27.27.27.27.cmml">s</mi><mrow id="S2.Ex1.m1.40.40.40.40.40.41"><mo stretchy="false" id="S2.Ex1.m1.28.28.28.28.28.28" xref="S2.Ex1.m1.66.66.2.3.cmml">(</mo><mi id="S2.Ex1.m1.29.29.29.29.29.29" xref="S2.Ex1.m1.29.29.29.29.29.29.cmml">m</mi><mi id="S2.Ex1.m1.30.30.30.30.30.30" xref="S2.Ex1.m1.30.30.30.30.30.30.cmml">a</mi><mi id="S2.Ex1.m1.31.31.31.31.31.31" xref="S2.Ex1.m1.31.31.31.31.31.31.cmml">x</mi><mrow id="S2.Ex1.m1.40.40.40.40.40.41.1"><mo stretchy="false" id="S2.Ex1.m1.32.32.32.32.32.32" xref="S2.Ex1.m1.66.66.2.3.cmml">(</mo><mi id="S2.Ex1.m1.33.33.33.33.33.33" xref="S2.Ex1.m1.33.33.33.33.33.33.cmml">u</mi><mi id="S2.Ex1.m1.34.34.34.34.34.34" xref="S2.Ex1.m1.34.34.34.34.34.34.cmml">p</mi><mi id="S2.Ex1.m1.35.35.35.35.35.35" xref="S2.Ex1.m1.35.35.35.35.35.35.cmml">d</mi><mi id="S2.Ex1.m1.36.36.36.36.36.36" xref="S2.Ex1.m1.36.36.36.36.36.36.cmml">a</mi><mi id="S2.Ex1.m1.37.37.37.37.37.37" xref="S2.Ex1.m1.37.37.37.37.37.37.cmml">t</mi><mi id="S2.Ex1.m1.38.38.38.38.38.38" xref="S2.Ex1.m1.38.38.38.38.38.38.cmml">e</mi><mi id="S2.Ex1.m1.39.39.39.39.39.39" xref="S2.Ex1.m1.39.39.39.39.39.39.cmml">s</mi><mo stretchy="false" id="S2.Ex1.m1.40.40.40.40.40.40" xref="S2.Ex1.m1.66.66.2.3.cmml">)</mo></mrow></mrow></mrow></mtd></mtr><mtr id="S2.Ex1.m1.64.64c"><mtd class="ltx_align_right" columnalign="right" id="S2.Ex1.m1.64.64d"><mrow id="S2.Ex1.m1.64.64.64.24.24"><mo rspace="0.222em" id="S2.Ex1.m1.41.41.41.1.1.1" xref="S2.Ex1.m1.41.41.41.1.1.1.cmml">×</mo><mi id="S2.Ex1.m1.42.42.42.2.2.2" xref="S2.Ex1.m1.42.42.42.2.2.2.cmml">w</mi><mi id="S2.Ex1.m1.43.43.43.3.3.3" xref="S2.Ex1.m1.43.43.43.3.3.3.cmml">e</mi><mi id="S2.Ex1.m1.44.44.44.4.4.4" xref="S2.Ex1.m1.44.44.44.4.4.4.cmml">i</mi><mi id="S2.Ex1.m1.45.45.45.5.5.5" xref="S2.Ex1.m1.45.45.45.5.5.5.cmml">g</mi><mi id="S2.Ex1.m1.46.46.46.6.6.6" xref="S2.Ex1.m1.46.46.46.6.6.6.cmml">h</mi><mi id="S2.Ex1.m1.47.47.47.7.7.7" xref="S2.Ex1.m1.47.47.47.7.7.7.cmml">t</mi><mi mathvariant="normal" id="S2.Ex1.m1.48.48.48.8.8.8" xref="S2.Ex1.m1.48.48.48.8.8.8.cmml">_</mi><mi id="S2.Ex1.m1.49.49.49.9.9.9" xref="S2.Ex1.m1.49.49.49.9.9.9.cmml">s</mi><mi id="S2.Ex1.m1.50.50.50.10.10.10" xref="S2.Ex1.m1.50.50.50.10.10.10.cmml">c</mi><mi id="S2.Ex1.m1.51.51.51.11.11.11" xref="S2.Ex1.m1.51.51.51.11.11.11.cmml">a</mi><mi id="S2.Ex1.m1.52.52.52.12.12.12" xref="S2.Ex1.m1.52.52.52.12.12.12.cmml">l</mi><mi id="S2.Ex1.m1.53.53.53.13.13.13" xref="S2.Ex1.m1.53.53.53.13.13.13.cmml">i</mi><mi id="S2.Ex1.m1.54.54.54.14.14.14" xref="S2.Ex1.m1.54.54.54.14.14.14.cmml">n</mi><mi id="S2.Ex1.m1.55.55.55.15.15.15" xref="S2.Ex1.m1.55.55.55.15.15.15.cmml">g</mi><mi mathvariant="normal" id="S2.Ex1.m1.56.56.56.16.16.16" xref="S2.Ex1.m1.56.56.56.16.16.16.cmml">_</mi><mi id="S2.Ex1.m1.57.57.57.17.17.17" xref="S2.Ex1.m1.57.57.57.17.17.17.cmml">f</mi><mi id="S2.Ex1.m1.58.58.58.18.18.18" xref="S2.Ex1.m1.58.58.58.18.18.18.cmml">a</mi><mi id="S2.Ex1.m1.59.59.59.19.19.19" xref="S2.Ex1.m1.59.59.59.19.19.19.cmml">c</mi><mi id="S2.Ex1.m1.60.60.60.20.20.20" xref="S2.Ex1.m1.60.60.60.20.20.20.cmml">t</mi><mi id="S2.Ex1.m1.61.61.61.21.21.21" xref="S2.Ex1.m1.61.61.61.21.21.21.cmml">o</mi><mi id="S2.Ex1.m1.62.62.62.22.22.22" xref="S2.Ex1.m1.62.62.62.22.22.22.cmml">r</mi><mi id="S2.Ex1.m1.63.63.63.23.23.23" xref="S2.Ex1.m1.63.63.63.23.23.23.cmml">s</mi><mo stretchy="false" id="S2.Ex1.m1.64.64.64.24.24.24" xref="S2.Ex1.m1.66.66.2.3.cmml">)</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.66b"><apply id="S2.Ex1.m1.66.66.2.3.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"><csymbol cd="ambiguous" id="S2.Ex1.m1.66.66.2.3a.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13">formulae-sequence</csymbol><apply id="S2.Ex1.m1.65.65.1.1.1.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"><times id="S2.Ex1.m1.65.65.1.1.1.1.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"></times><ci id="S2.Ex1.m1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1">𝑔</ci><ci id="S2.Ex1.m1.2.2.2.2.2.2.cmml" xref="S2.Ex1.m1.2.2.2.2.2.2">𝑙</ci><ci id="S2.Ex1.m1.3.3.3.3.3.3.cmml" xref="S2.Ex1.m1.3.3.3.3.3.3">𝑜</ci><ci id="S2.Ex1.m1.4.4.4.4.4.4.cmml" xref="S2.Ex1.m1.4.4.4.4.4.4">𝑏</ci><ci id="S2.Ex1.m1.5.5.5.5.5.5.cmml" xref="S2.Ex1.m1.5.5.5.5.5.5">𝑎</ci><ci id="S2.Ex1.m1.6.6.6.6.6.6.cmml" xref="S2.Ex1.m1.6.6.6.6.6.6">𝑙</ci><ci id="S2.Ex1.m1.7.7.7.7.7.7.cmml" xref="S2.Ex1.m1.7.7.7.7.7.7">_</ci><ci id="S2.Ex1.m1.8.8.8.8.8.8.cmml" xref="S2.Ex1.m1.8.8.8.8.8.8">𝑚</ci><ci id="S2.Ex1.m1.9.9.9.9.9.9.cmml" xref="S2.Ex1.m1.9.9.9.9.9.9">𝑜</ci><ci id="S2.Ex1.m1.10.10.10.10.10.10.cmml" xref="S2.Ex1.m1.10.10.10.10.10.10">𝑑</ci><ci id="S2.Ex1.m1.11.11.11.11.11.11.cmml" xref="S2.Ex1.m1.11.11.11.11.11.11">𝑒</ci><ci id="S2.Ex1.m1.12.12.12.12.12.12.cmml" xref="S2.Ex1.m1.12.12.12.12.12.12">𝑙</ci></apply><apply id="S2.Ex1.m1.66.66.2.2.2.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"><times id="S2.Ex1.m1.66.66.2.2.2.2.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"></times><ci id="S2.Ex1.m1.14.14.14.14.14.14.cmml" xref="S2.Ex1.m1.14.14.14.14.14.14">𝑢</ci><ci id="S2.Ex1.m1.15.15.15.15.15.15.cmml" xref="S2.Ex1.m1.15.15.15.15.15.15">𝑝</ci><ci id="S2.Ex1.m1.16.16.16.16.16.16.cmml" xref="S2.Ex1.m1.16.16.16.16.16.16">𝑑</ci><ci id="S2.Ex1.m1.17.17.17.17.17.17.cmml" xref="S2.Ex1.m1.17.17.17.17.17.17">𝑎</ci><ci id="S2.Ex1.m1.18.18.18.18.18.18.cmml" xref="S2.Ex1.m1.18.18.18.18.18.18">𝑡</ci><ci id="S2.Ex1.m1.19.19.19.19.19.19.cmml" xref="S2.Ex1.m1.19.19.19.19.19.19">𝑒</ci><ci id="S2.Ex1.m1.20.20.20.20.20.20.cmml" xref="S2.Ex1.m1.20.20.20.20.20.20">_</ci><ci id="S2.Ex1.m1.21.21.21.21.21.21.cmml" xref="S2.Ex1.m1.21.21.21.21.21.21">𝑤</ci><ci id="S2.Ex1.m1.22.22.22.22.22.22.cmml" xref="S2.Ex1.m1.22.22.22.22.22.22">𝑒</ci><ci id="S2.Ex1.m1.23.23.23.23.23.23.cmml" xref="S2.Ex1.m1.23.23.23.23.23.23">𝑖</ci><ci id="S2.Ex1.m1.24.24.24.24.24.24.cmml" xref="S2.Ex1.m1.24.24.24.24.24.24">𝑔</ci><ci id="S2.Ex1.m1.25.25.25.25.25.25.cmml" xref="S2.Ex1.m1.25.25.25.25.25.25">ℎ</ci><ci id="S2.Ex1.m1.26.26.26.26.26.26.cmml" xref="S2.Ex1.m1.26.26.26.26.26.26">𝑡</ci><ci id="S2.Ex1.m1.27.27.27.27.27.27.cmml" xref="S2.Ex1.m1.27.27.27.27.27.27">𝑠</ci><apply id="S2.Ex1.m1.66.66.2.2.2.1.1.1.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"><times id="S2.Ex1.m1.66.66.2.2.2.1.1.1.2.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"></times><apply id="S2.Ex1.m1.66.66.2.2.2.1.1.1.1.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"><times id="S2.Ex1.m1.41.41.41.1.1.1.cmml" xref="S2.Ex1.m1.41.41.41.1.1.1"></times><apply id="S2.Ex1.m1.66.66.2.2.2.1.1.1.1.1.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"><times id="S2.Ex1.m1.66.66.2.2.2.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"></times><ci id="S2.Ex1.m1.29.29.29.29.29.29.cmml" xref="S2.Ex1.m1.29.29.29.29.29.29">𝑚</ci><ci id="S2.Ex1.m1.30.30.30.30.30.30.cmml" xref="S2.Ex1.m1.30.30.30.30.30.30">𝑎</ci><ci id="S2.Ex1.m1.31.31.31.31.31.31.cmml" xref="S2.Ex1.m1.31.31.31.31.31.31">𝑥</ci><apply id="S2.Ex1.m1.66.66.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"><times id="S2.Ex1.m1.66.66.2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.13.13.13.13.13.13"></times><ci id="S2.Ex1.m1.33.33.33.33.33.33.cmml" xref="S2.Ex1.m1.33.33.33.33.33.33">𝑢</ci><ci id="S2.Ex1.m1.34.34.34.34.34.34.cmml" xref="S2.Ex1.m1.34.34.34.34.34.34">𝑝</ci><ci id="S2.Ex1.m1.35.35.35.35.35.35.cmml" xref="S2.Ex1.m1.35.35.35.35.35.35">𝑑</ci><ci id="S2.Ex1.m1.36.36.36.36.36.36.cmml" xref="S2.Ex1.m1.36.36.36.36.36.36">𝑎</ci><ci id="S2.Ex1.m1.37.37.37.37.37.37.cmml" xref="S2.Ex1.m1.37.37.37.37.37.37">𝑡</ci><ci id="S2.Ex1.m1.38.38.38.38.38.38.cmml" xref="S2.Ex1.m1.38.38.38.38.38.38">𝑒</ci><ci id="S2.Ex1.m1.39.39.39.39.39.39.cmml" xref="S2.Ex1.m1.39.39.39.39.39.39">𝑠</ci></apply></apply><ci id="S2.Ex1.m1.42.42.42.2.2.2.cmml" xref="S2.Ex1.m1.42.42.42.2.2.2">𝑤</ci></apply><ci id="S2.Ex1.m1.43.43.43.3.3.3.cmml" xref="S2.Ex1.m1.43.43.43.3.3.3">𝑒</ci><ci id="S2.Ex1.m1.44.44.44.4.4.4.cmml" xref="S2.Ex1.m1.44.44.44.4.4.4">𝑖</ci><ci id="S2.Ex1.m1.45.45.45.5.5.5.cmml" xref="S2.Ex1.m1.45.45.45.5.5.5">𝑔</ci><ci id="S2.Ex1.m1.46.46.46.6.6.6.cmml" xref="S2.Ex1.m1.46.46.46.6.6.6">ℎ</ci><ci id="S2.Ex1.m1.47.47.47.7.7.7.cmml" xref="S2.Ex1.m1.47.47.47.7.7.7">𝑡</ci><ci id="S2.Ex1.m1.48.48.48.8.8.8.cmml" xref="S2.Ex1.m1.48.48.48.8.8.8">_</ci><ci id="S2.Ex1.m1.49.49.49.9.9.9.cmml" xref="S2.Ex1.m1.49.49.49.9.9.9">𝑠</ci><ci id="S2.Ex1.m1.50.50.50.10.10.10.cmml" xref="S2.Ex1.m1.50.50.50.10.10.10">𝑐</ci><ci id="S2.Ex1.m1.51.51.51.11.11.11.cmml" xref="S2.Ex1.m1.51.51.51.11.11.11">𝑎</ci><ci id="S2.Ex1.m1.52.52.52.12.12.12.cmml" xref="S2.Ex1.m1.52.52.52.12.12.12">𝑙</ci><ci id="S2.Ex1.m1.53.53.53.13.13.13.cmml" xref="S2.Ex1.m1.53.53.53.13.13.13">𝑖</ci><ci id="S2.Ex1.m1.54.54.54.14.14.14.cmml" xref="S2.Ex1.m1.54.54.54.14.14.14">𝑛</ci><ci id="S2.Ex1.m1.55.55.55.15.15.15.cmml" xref="S2.Ex1.m1.55.55.55.15.15.15">𝑔</ci><ci id="S2.Ex1.m1.56.56.56.16.16.16.cmml" xref="S2.Ex1.m1.56.56.56.16.16.16">_</ci><ci id="S2.Ex1.m1.57.57.57.17.17.17.cmml" xref="S2.Ex1.m1.57.57.57.17.17.17">𝑓</ci><ci id="S2.Ex1.m1.58.58.58.18.18.18.cmml" xref="S2.Ex1.m1.58.58.58.18.18.18">𝑎</ci><ci id="S2.Ex1.m1.59.59.59.19.19.19.cmml" xref="S2.Ex1.m1.59.59.59.19.19.19">𝑐</ci><ci id="S2.Ex1.m1.60.60.60.20.20.20.cmml" xref="S2.Ex1.m1.60.60.60.20.20.20">𝑡</ci><ci id="S2.Ex1.m1.61.61.61.21.21.21.cmml" xref="S2.Ex1.m1.61.61.61.21.21.21">𝑜</ci><ci id="S2.Ex1.m1.62.62.62.22.22.22.cmml" xref="S2.Ex1.m1.62.62.62.22.22.22">𝑟</ci><ci id="S2.Ex1.m1.63.63.63.23.23.23.cmml" xref="S2.Ex1.m1.63.63.63.23.23.23">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.66c">\begin{multlined}global\_model.update\_weights(max(updates)\\
\times weight\_scaling\_factors)\end{multlined}global\_model.update\_weights(max(updates)\\
\times weight\_scaling\_factors)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_left_padright"></td>
</tr></tbody>
</table>


</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span><span id="alg1.l13.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l13.3" class="ltx_text ltx_font_bold">for</span>
</div>
</div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2402.09795/assets/images/Federated_Learning_Model.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="397" height="309" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Federated Learning Model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite></figcaption>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Homomorphic Encryption</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">A cryptographic method called homomorphic encryption enables mathematical operations to be carried out on ciphertext without exposing the underlying plaintext. In our research, we used partially homomorphic encryption to encrypt sensitive medical images, specifically brain MRI scans.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Partially homomorphic encryption (PHE) is a type of homomorphic encryption that only supports a limited set of mathematical operations, such as addition or multiplication. By encrypting the medical images using this technique, we were able to process and analyze the data without exposing the sensitive information contained within it.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">One major benefit of using partially homomorphic encryption in this context is that it ensures the confidentiality of medical data. As medical information is often highly sensitive and personal, it is important to protect it from unauthorized access. By encrypting the data, we were able to securely process and analyze it without compromising its confidentiality.
</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">In addition, partially homomorphic encryption allows for more efficient processing of the encrypted data. Because the mathematical operations can be performed directly on the ciphertext, there is no need to decrypt the data first, which can be a time-consuming process. This was particularly useful when working with large datasets or when processing data in real time. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite></p>
</div>
<div id="S2.SS4.p5" class="ltx_para">
<p id="S2.SS4.p5.1" class="ltx_p">Overall, our use of partially homomorphic encryption proved to be a successful and effective method for protecting the confidentiality of sensitive medical images while still enabling their analysis.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison among different types of Homomorphic Encryption</figcaption>
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:161.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.3pt,12.0pt) scale(0.870332608579871,0.870332608579871) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<table id="S2.T1.1.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Types of Homomorphic</span></td>
</tr>
<tr id="S2.T1.1.1.1.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.1.1.1.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Encryption</span></td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<table id="S2.T1.1.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.2.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Partially Homomorphic</span></td>
</tr>
<tr id="S2.T1.1.1.1.2.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Encryption (PHE)</span></td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<table id="S2.T1.1.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.3.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Somewhat Homomorphic</span></td>
</tr>
<tr id="S2.T1.1.1.1.3.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Encryption (SHE)</span></td>
</tr>
</table>
</td>
<td id="S2.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<table id="S2.T1.1.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S2.T1.1.1.1.4.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Fully Homomorphic</span></td>
</tr>
<tr id="S2.T1.1.1.1.4.1.2" class="ltx_tr">
<td id="S2.T1.1.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Encryption (FHE)</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S2.T1.1.1.2" class="ltx_tr">
<td id="S2.T1.1.1.2.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="4"></td>
</tr>
<tr id="S2.T1.1.1.3" class="ltx_tr">
<td id="S2.T1.1.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.3.1.1" class="ltx_text ltx_font_bold">Supported Operations</span></td>
<td id="S2.T1.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Addition or Multiplication</td>
<td id="S2.T1.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Addition &amp; Multiplication</td>
<td id="S2.T1.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Arbitrary Computations</td>
</tr>
<tr id="S2.T1.1.1.4" class="ltx_tr">
<td id="S2.T1.1.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.4.1.1" class="ltx_text ltx_font_bold">Security</span></td>
<td id="S2.T1.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">High</td>
<td id="S2.T1.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Moderate to High</td>
<td id="S2.T1.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">High</td>
</tr>
<tr id="S2.T1.1.1.5" class="ltx_tr">
<td id="S2.T1.1.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.5.1.1" class="ltx_text ltx_font_bold">Computational Efficiency</span></td>
<td id="S2.T1.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">High</td>
<td id="S2.T1.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Moderate</td>
<td id="S2.T1.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Low</td>
</tr>
<tr id="S2.T1.1.1.6" class="ltx_tr">
<td id="S2.T1.1.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.6.1.1" class="ltx_text ltx_font_bold">Computational Intensity</span></td>
<td id="S2.T1.1.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Low</td>
<td id="S2.T1.1.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Moderate</td>
<td id="S2.T1.1.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">High</td>
</tr>
<tr id="S2.T1.1.1.7" class="ltx_tr">
<td id="S2.T1.1.1.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.7.1.1" class="ltx_text ltx_font_bold">Encryption</span></td>
<td id="S2.T1.1.1.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Simple</td>
<td id="S2.T1.1.1.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">More Complex than PHE</td>
<td id="S2.T1.1.1.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Very Complex</td>
</tr>
<tr id="S2.T1.1.1.8" class="ltx_tr">
<td id="S2.T1.1.1.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.8.1.1" class="ltx_text ltx_font_bold">Encryption Overhead</span></td>
<td id="S2.T1.1.1.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Low</td>
<td id="S2.T1.1.1.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Moderate</td>
<td id="S2.T1.1.1.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">High</td>
</tr>
<tr id="S2.T1.1.1.9" class="ltx_tr">
<td id="S2.T1.1.1.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span id="S2.T1.1.1.9.1.1" class="ltx_text ltx_font_bold">Implementation Ability</span></td>
<td id="S2.T1.1.1.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Easy</td>
<td id="S2.T1.1.1.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Moderate</td>
<td id="S2.T1.1.1.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Difficult</td>
</tr>
</table>
</span></div>
</figure>
<figure id="S2.F3" class="ltx_figure"><img src="/html/2402.09795/assets/images/Homomorphic_Encryption.png" id="S2.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="382" height="279" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>An Overview of the Homomorphic Encryption</figcaption>
</figure>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Description</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">In our experimental evaluation, we used
the dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> named “Brain MRI Images for Brain Tumor Detection”, available on Kaggle<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset</a></span></span></span>. The dataset includes a large number of 2D MRI images for the classification of brain tumors. Brain tumors are classified into three types: Benign, Malignant, and Pituitary. However, for our selected dataset, images are classified into two categories; Pituitary Tumor and No Tumor.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">From the dataset, 1852 images have been used by us as shown in the figure below. On the images, we performed encryption and machine learning techniques for ensuring privacy and achieving desired results.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2402.09795/assets/images/dataset_image.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="401" height="281" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>A few samples of the used Brain Tumor dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite></figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">For preparing our own data fabric to perform MLOps, an image was selected from the dataset and converted into a Numpy array with its pixel values. After that, homomorphic encryption was performed on the array to make the data fabric encrypted. A brief overview is shown in the following figure.</p>
</div>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2402.09795/assets/images/dataset.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="252" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Overview of the encrypted dataset</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Data Preprocessing</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">To prepare our data we needed to preprocess the whole dataset. Firstly, we have encrypted our dataset using the Partially Homomorphic Encryption Algorithm. After that, we resized the images to <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="128\times 128" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">128</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">128\times 128</annotation></semantics></math> dimensions. The shape of our dataset became (1852, 128, 128). Then we reshaped our dataset by multiplying the dimensions of individual pixels. Then the shape ultimately (1852, 15376). For machine learning classifiers we have scaled the image dataset to value 0 to 1.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Proposed Model</h3>

<section id="S3.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Advanced Architecture of Data Fabric</h4>

<div id="S3.SS3.SSS1.p1" class="ltx_para">
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Accessing Data:</span> Here, initially, to ensure privacy and reduce data volatility, medical data from various users are first selected and then encrypted with Partially Homomorphic Encryption (PHE). Partially Homomorphic Encryption (PHE) enabled us to perform DLOps on the data securely. Subsequently, the encrypted data is selected to train the model locally for collecting updated model weights. For training, various federated learning models like VGG16, VGG19, ResNet50, ResNet152, and our Custom CNN were used which generated model updates for each model. The data of each user is generated and stored locally, without being transferred to the central server. Instead, the local model updates were stored and merged for the global model formation.</p>
</div>
<div id="S3.I1.i1.p2" class="ltx_para">
<p id="S3.I1.i1.p2.1" class="ltx_p">After combining the local model weights, we selected FedMax as our feature selection algorithm to select important and relevant model weights as it showed more accuracy and efficiency compared to FedAvg and FedMin. The selected data is collected and kept together as “Master Data”, with which we will continue our next works.</p>
</div>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2402.09795/assets/images/Data_Accessing.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Accessing Data</figcaption>
</figure>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><span id="S3.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Managing Life Cycle:</span> Selected model weights were differentiated based on the models they were trained on. In this case, for example, updates of the VGG16 model were kept structurally under the “VGG16” name. This enables effective data governance, as the risks of data inconsistency and complicated integration across the whole architecture get lowered. Additionally, organizing data in a structured way ensures proper usage of data and helps strike a balance between data collaboration and privacy mandates.</p>
</div>
<div id="S3.I1.i2.p2" class="ltx_para">
<p id="S3.I1.i2.p2.1" class="ltx_p">Furthermore, as the privacy of the data was already ensured in the first step, the collected model weights already comply with existing privacy regulations such as HIPAA, GDPR, etc.</p>
</div>
<figure id="S3.F7" class="ltx_figure"><img src="/html/2402.09795/assets/images/Data_Life_cycle_Management.png" id="S3.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="304" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Data Lifecycle Management</figcaption>
</figure>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><span id="S3.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Exposing Data:</span> Since exposing data is a comprehensive approach where appropriate data access controls, data fusion, and cataloging must be implemented, we stored all the collected local model updates in a “Data Lake”. This enabled us to store our raw model weights structurally in their native format. Additionally, since there were huge amounts of model weights, using a Data Lake helped us to store and process it easily.</p>
</div>
<div id="S3.I1.i3.p2" class="ltx_para">
<p id="S3.I1.i3.p2.1" class="ltx_p">Most importantly, we can also perform MLOps on the Data Lake directly. On the client side, clients can train their data on the global federated model and can compare the global weights with the local weights of that model stored in the data lake and generate the desired output.</p>
</div>
<figure id="S3.F8" class="ltx_figure"><img src="/html/2402.09795/assets/images/Data_Exposing.png" id="S3.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="447" height="301" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Exposing Data</figcaption>
</figure>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S3.F9" class="ltx_figure"><img src="/html/2402.09795/assets/images/Advanced_Architecture_Data_Fabric_NEW.png" id="S3.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="610" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The Proposed Data Fabric Architecture leveraging Federated Learning and Homomorphic Encryption</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Implemented Federated Learning Framework</h4>

<div id="S3.SS3.SSS2.p1" class="ltx_para">
<p id="S3.SS3.SSS2.p1.1" class="ltx_p">As we delve into the world of medical imaging and machine learning, we have utilized a cutting-edge approach to store the local weights of our machine learning models using a Data Fabric architecture. This architecture has allowed us to securely store and manage distributed data across multiple environments, providing a consistent and scalable approach to managing data assets.</p>
</div>
<figure id="S3.F10" class="ltx_figure"><img src="/html/2402.09795/assets/images/Local_Federated_Learning_Model.png" id="S3.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="371" height="329" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Local Federated Learning Model</figcaption>
</figure>
<div id="S3.SS3.SSS2.p2" class="ltx_para">
<p id="S3.SS3.SSS2.p2.1" class="ltx_p">Our goal was to develop a pituitary tumor classification model that leverages the decentralized data in a privacy-preserving manner, improving the accuracy of the model while ensuring the privacy of patient information. We have used the FedMax algorithm, an improvement on the popular FedAvg algorithm, to compile a global model and local models for the classification of pituitary tumors from MRI images in our dataset. The FedMax algorithm considers the model performance of each client and assigns a weighting factor that allows clients with the best performance to contribute more to the global model. We have stored the resulting model weights in a data lake, allowing us to generate a global model and test it using test cases when the user prompts to show results.
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S3.F11" class="ltx_figure"><img src="/html/2402.09795/assets/images/Global_Federated_Learning_Model.png" id="S3.F11.g1" class="ltx_graphics ltx_centering ltx_img_square" width="382" height="343" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Global Federated Learning Model</figcaption>
</figure>
</section>
<section id="S3.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Comparative Analysis with Related Works</h4>

<div id="S3.SS3.SSS3.p1" class="ltx_para">
<p id="S3.SS3.SSS3.p1.1" class="ltx_p">Our work is different from existing approaches as it enables effective and secure handling of highly sensitive health data through our proposed Data Fabric architecture. Another advantage of this work is that its privacy-preserving features (Homormophic encryption) are compliant with GDPR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite>, as it supports the right to be forgotten, as the actual health data is neither collected nor stored. Rather, after training, the encrypted data which is collected is stored as model updates. Similarly, it is possible to delete a client’s data from the data lake upon request. In addition to this, compared to other works, our model also complies with HIPAA guidelines, as it ensures confidentiality, integrity, and availability of personal health information, safeguards data from threats, and protects impermissible access as mentioned here<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.</p>
</div>
<div id="S3.SS3.SSS3.p2" class="ltx_para">
<p id="S3.SS3.SSS3.p2.1" class="ltx_p">The following table displays a comparison between our data fabric architecture and other existing works that have demonstrated proof-of-concept implementations, which can be implemented and practical for real-world scenarios. In this table, the technology, privacy-preserving features, GDPR and HIPAA compliance, and also performance assessment results of each existing proposal have been provided.
</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparative Analysis</figcaption>
<div id="S3.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:121.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-59.5pt,16.7pt) scale(0.784575044122083,0.784575044122083) ;">
<table id="S3.T2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Architecture</td>
<td id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Technology</td>
<td id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Access Level</td>
<td id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">HIPAA</td>
<td id="S3.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">GDPR</td>
<td id="S3.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<table id="S3.T2.1.1.1.6.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.1.6.1.1" class="ltx_tr">
<td id="S3.T2.1.1.1.6.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Privacy</td>
</tr>
<tr id="S3.T2.1.1.1.6.1.2" class="ltx_tr">
<td id="S3.T2.1.1.1.6.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Preserving</td>
</tr>
</table>
</td>
<td id="S3.T2.1.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Performance</td>
</tr>
<tr id="S3.T2.1.1.2" class="ltx_tr">
<td id="S3.T2.1.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">PREHEALTH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</td>
<td id="S3.T2.1.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Hyperledger Fabric</td>
<td id="S3.T2.1.1.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Private</td>
<td id="S3.T2.1.1.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Not Mentioned</td>
<td id="S3.T2.1.1.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
<td id="S3.T2.1.1.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
<td id="S3.T2.1.1.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
</tr>
<tr id="S3.T2.1.1.3" class="ltx_tr">
<td id="S3.T2.1.1.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">OmniPHR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</td>
<td id="S3.T2.1.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Peer - to - Peer</td>
<td id="S3.T2.1.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Private</td>
<td id="S3.T2.1.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No</td>
<td id="S3.T2.1.1.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No</td>
<td id="S3.T2.1.1.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No</td>
<td id="S3.T2.1.1.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
</tr>
<tr id="S3.T2.1.1.4" class="ltx_tr">
<td id="S3.T2.1.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">MeDShare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>
</td>
<td id="S3.T2.1.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Agnostic</td>
<td id="S3.T2.1.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Open</td>
<td id="S3.T2.1.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No</td>
<td id="S3.T2.1.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No</td>
<td id="S3.T2.1.1.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No</td>
<td id="S3.T2.1.1.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
</tr>
<tr id="S3.T2.1.1.5" class="ltx_tr">
<td id="S3.T2.1.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<table id="S3.T2.1.1.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S3.T2.1.1.5.1.1.1" class="ltx_tr">
<td id="S3.T2.1.1.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Access Control</td>
</tr>
<tr id="S3.T2.1.1.5.1.1.2" class="ltx_tr">
<td id="S3.T2.1.1.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-top:2.5pt;padding-bottom:2.5pt;">Based EHR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S3.T2.1.1.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">AC Scheme</td>
<td id="S3.T2.1.1.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Private</td>
<td id="S3.T2.1.1.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No</td>
<td id="S3.T2.1.1.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No</td>
<td id="S3.T2.1.1.5.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
<td id="S3.T2.1.1.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
</tr>
<tr id="S3.T2.1.1.6" class="ltx_tr">
<td id="S3.T2.1.1.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Our Proposed Work</td>
<td id="S3.T2.1.1.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Data Fabric</td>
<td id="S3.T2.1.1.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Private</td>
<td id="S3.T2.1.1.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
<td id="S3.T2.1.1.6.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
<td id="S3.T2.1.1.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
<td id="S3.T2.1.1.6.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Yes</td>
</tr>
</table>
</span></div>
</figure>
</section>
<section id="S3.SS3.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.4 </span>Model Specification</h4>

<div id="S3.SS3.SSS4.p1" class="ltx_para">
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p"><span id="S3.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">VGG16:</span> A 16-layered convolutional neural network model trained on the ImageNet dataset, it is considered to be one of the best models to date. It is widely regarded for its simple architecture and excellent image classification performance, retaining 92.7% test accuracy in the ImageNet dataset, which consists of almost 14 million training images across a thousand object classes.</p>
</div>
<div id="S3.I2.i1.p2" class="ltx_para">
<p id="S3.I2.i1.p2.1" class="ltx_p">As its name suggests, it is composed of 16 layers which include 13 convolutional layers and 3 fully connected layers. It comprises 138 million parameters and uses small convolutional filters and deep architectures to gain a large receptive field and strong discrimination ability, which helps in image classification, object detection, and semantic segmentation. To control overfitting and reduce spatial dimensions, its architecture has five max pooling layers.</p>
</div>
<div id="S3.I2.i1.p3" class="ltx_para">
<p id="S3.I2.i1.p3.2" class="ltx_p">The most unique thing about VGG16 is that it is focused on convolution layers of <math id="S3.I2.i1.p3.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.I2.i1.p3.1.m1.1a"><mrow id="S3.I2.i1.p3.1.m1.1.1" xref="S3.I2.i1.p3.1.m1.1.1.cmml"><mn id="S3.I2.i1.p3.1.m1.1.1.2" xref="S3.I2.i1.p3.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I2.i1.p3.1.m1.1.1.1" xref="S3.I2.i1.p3.1.m1.1.1.1.cmml">×</mo><mn id="S3.I2.i1.p3.1.m1.1.1.3" xref="S3.I2.i1.p3.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p3.1.m1.1b"><apply id="S3.I2.i1.p3.1.m1.1.1.cmml" xref="S3.I2.i1.p3.1.m1.1.1"><times id="S3.I2.i1.p3.1.m1.1.1.1.cmml" xref="S3.I2.i1.p3.1.m1.1.1.1"></times><cn type="integer" id="S3.I2.i1.p3.1.m1.1.1.2.cmml" xref="S3.I2.i1.p3.1.m1.1.1.2">3</cn><cn type="integer" id="S3.I2.i1.p3.1.m1.1.1.3.cmml" xref="S3.I2.i1.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p3.1.m1.1c">3\times 3</annotation></semantics></math> filter with stride 1 and always used the same padding and max pol layer of <math id="S3.I2.i1.p3.2.m2.1" class="ltx_Math" alttext="2\times 2" display="inline"><semantics id="S3.I2.i1.p3.2.m2.1a"><mrow id="S3.I2.i1.p3.2.m2.1.1" xref="S3.I2.i1.p3.2.m2.1.1.cmml"><mn id="S3.I2.i1.p3.2.m2.1.1.2" xref="S3.I2.i1.p3.2.m2.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I2.i1.p3.2.m2.1.1.1" xref="S3.I2.i1.p3.2.m2.1.1.1.cmml">×</mo><mn id="S3.I2.i1.p3.2.m2.1.1.3" xref="S3.I2.i1.p3.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i1.p3.2.m2.1b"><apply id="S3.I2.i1.p3.2.m2.1.1.cmml" xref="S3.I2.i1.p3.2.m2.1.1"><times id="S3.I2.i1.p3.2.m2.1.1.1.cmml" xref="S3.I2.i1.p3.2.m2.1.1.1"></times><cn type="integer" id="S3.I2.i1.p3.2.m2.1.1.2.cmml" xref="S3.I2.i1.p3.2.m2.1.1.2">2</cn><cn type="integer" id="S3.I2.i1.p3.2.m2.1.1.3.cmml" xref="S3.I2.i1.p3.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i1.p3.2.m2.1c">2\times 2</annotation></semantics></math> filter with stride 2, and the layers are constantly arranged over the whole architecture. Due to its high-level feature representation, it provides good performance on object detection, fine-grained image classification, etc. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S3.F12" class="ltx_figure"><img src="/html/2402.09795/assets/images/VGG_16_NEW.png" id="S3.F12.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="168" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>VGG16 Model Architecture</figcaption>
</figure>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p"><span id="S3.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">VGG19:</span> A variant of the VGG neural network, it is a 19-layer version of the VGG network and similar to the VGG16 architecture. Compared to VGG16, it has 5 convolutional layers and 1 fully connected layer. It has around 143 million parameters and is trained on a dataset with 1.2 million images and 1000 classes.</p>
</div>
<div id="S3.I2.i2.p2" class="ltx_para">
<p id="S3.I2.i2.p2.1" class="ltx_p">The function accepts an image of shape (128, 128, 1) as input, and the image is passed through concatenate layer which concatenates the image 3 times resulting in a tensor with shape (128, 128, 3), this is passed to a VGG19 model which is a pre-trained convolutional neural network model that is trained on the ImageNet dataset. The VGG19 model serves as a feature extractor and it extracts features from the image by stacking convolutional layers and pooling layers. The output of the final max pooling layer is then passed through a flattened layer which reshapes the output tensor into a 2D array. This flattened layer’s output is passed through a dense layer with 1 unit and a sigmoid activation function, it produces the final output of the network which represents the predicted probability of the input image belonging to the target class. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S3.F13" class="ltx_figure"><img src="/html/2402.09795/assets/images/vgg_19_new.png" id="S3.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="233" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>VGG19 Model Architecture</figcaption>
</figure>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p"><span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">ResNet50:</span> ResNet50 is a deep convolutional neural network architecture that is trained on more than a million images from the ImageNet dataset. It is known for its use of residual blocks, which address the problem of vanishing gradients in deep networks. These residual blocks have shortcut connections that bypass one or more layers, allowing gradients to flow more easily and making it possible to train very deep networks without the problem of vanishing gradients. The architecture also uses batch normalization layers, which normalize the activation of the layers, making it possible to train the network more quickly and effectively. Additionally, the number of filters increases as the network gets deeper, allowing it to automatically learn more complex features. ResNet50 is widely used in many computer vision tasks and is commonly used as a feature extractor for other tasks.</p>
</div>
<div id="S3.I2.i3.p2" class="ltx_para">
<p id="S3.I2.i3.p2.1" class="ltx_p">The ResNet50 model is used as a feature extractor in our experiment, and the output of the final convolutional layer is passed through a flattened layer which reshapes the output tensor into a 2D array. The flattened feature map is then passed through a Dropout layer with a drop rate of 0.5, which is used for regularization to prevent overfitting. The dropout layer is optional and could be removed by commenting out the line. Finally, the output of the dropout layer is passed through a dense layer with 1 unit and a sigmoid activation function, which is used to produce the final output of the network, which represents the predicted probability of the input image belonging to the target class.<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S3.F14" class="ltx_figure"><img src="/html/2402.09795/assets/images/ResNet_50_NEW.png" id="S3.F14.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>ResNet50 Model Architecture</figcaption>
</figure>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p"><span id="S3.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">ResNet152:</span> We also used a ResNet152 architecture model. The architecture is composed of a stack of convolutional and pooling layers. The model accepts an image of shape (128, 128, 3) as input and the output of the final average pooling layer is passed through a Flatten layer which reshapes the output tensor into a 2D array. Then, there is a Dense layer with classes number of units, and softmax activation function. This dense layer produces the final output of the network which represents the predicted probability of the input image belonging to different classes. This architecture does not have a dropout layer and the model is trainable, this means that the model can be fine-tuned with new data for a different task. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite></p>
</div>
<figure id="S3.F15" class="ltx_figure"><img src="/html/2402.09795/assets/images/ResNet_152_NEW.png" id="S3.F15.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>ResNet152 Model Architecture</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</li>
<li id="S3.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S3.I2.i5.p1" class="ltx_para">
<p id="S3.I2.i5.p1.4" class="ltx_p"><span id="S3.I2.i5.p1.4.1" class="ltx_text ltx_font_bold">Custom CNN:</span> We have customized a convolutional neural network (CNN) in a similar structure to existing VGG16 and VGG19 models. It has a total of 16 convolutional layers and 3 fully connected layers. The architecture starts with an image of size (128, 128, 1) as input and predicts a binary output. The first Conv2D layer consists of 64 filters of size <math id="S3.I2.i5.p1.1.m1.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.I2.i5.p1.1.m1.1a"><mrow id="S3.I2.i5.p1.1.m1.1.1" xref="S3.I2.i5.p1.1.m1.1.1.cmml"><mn id="S3.I2.i5.p1.1.m1.1.1.2" xref="S3.I2.i5.p1.1.m1.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I2.i5.p1.1.m1.1.1.1" xref="S3.I2.i5.p1.1.m1.1.1.1.cmml">×</mo><mn id="S3.I2.i5.p1.1.m1.1.1.3" xref="S3.I2.i5.p1.1.m1.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i5.p1.1.m1.1b"><apply id="S3.I2.i5.p1.1.m1.1.1.cmml" xref="S3.I2.i5.p1.1.m1.1.1"><times id="S3.I2.i5.p1.1.m1.1.1.1.cmml" xref="S3.I2.i5.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.I2.i5.p1.1.m1.1.1.2.cmml" xref="S3.I2.i5.p1.1.m1.1.1.2">3</cn><cn type="integer" id="S3.I2.i5.p1.1.m1.1.1.3.cmml" xref="S3.I2.i5.p1.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i5.p1.1.m1.1c">3\times 3</annotation></semantics></math>, followed by another Conv2D layer of 64 filters of size <math id="S3.I2.i5.p1.2.m2.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.I2.i5.p1.2.m2.1a"><mrow id="S3.I2.i5.p1.2.m2.1.1" xref="S3.I2.i5.p1.2.m2.1.1.cmml"><mn id="S3.I2.i5.p1.2.m2.1.1.2" xref="S3.I2.i5.p1.2.m2.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I2.i5.p1.2.m2.1.1.1" xref="S3.I2.i5.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.I2.i5.p1.2.m2.1.1.3" xref="S3.I2.i5.p1.2.m2.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i5.p1.2.m2.1b"><apply id="S3.I2.i5.p1.2.m2.1.1.cmml" xref="S3.I2.i5.p1.2.m2.1.1"><times id="S3.I2.i5.p1.2.m2.1.1.1.cmml" xref="S3.I2.i5.p1.2.m2.1.1.1"></times><cn type="integer" id="S3.I2.i5.p1.2.m2.1.1.2.cmml" xref="S3.I2.i5.p1.2.m2.1.1.2">3</cn><cn type="integer" id="S3.I2.i5.p1.2.m2.1.1.3.cmml" xref="S3.I2.i5.p1.2.m2.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i5.p1.2.m2.1c">3\times 3</annotation></semantics></math>, and a max pooling layer with a pool size <math id="S3.I2.i5.p1.3.m3.1" class="ltx_Math" alttext="2\times 2" display="inline"><semantics id="S3.I2.i5.p1.3.m3.1a"><mrow id="S3.I2.i5.p1.3.m3.1.1" xref="S3.I2.i5.p1.3.m3.1.1.cmml"><mn id="S3.I2.i5.p1.3.m3.1.1.2" xref="S3.I2.i5.p1.3.m3.1.1.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I2.i5.p1.3.m3.1.1.1" xref="S3.I2.i5.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.I2.i5.p1.3.m3.1.1.3" xref="S3.I2.i5.p1.3.m3.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i5.p1.3.m3.1b"><apply id="S3.I2.i5.p1.3.m3.1.1.cmml" xref="S3.I2.i5.p1.3.m3.1.1"><times id="S3.I2.i5.p1.3.m3.1.1.1.cmml" xref="S3.I2.i5.p1.3.m3.1.1.1"></times><cn type="integer" id="S3.I2.i5.p1.3.m3.1.1.2.cmml" xref="S3.I2.i5.p1.3.m3.1.1.2">2</cn><cn type="integer" id="S3.I2.i5.p1.3.m3.1.1.3.cmml" xref="S3.I2.i5.p1.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i5.p1.3.m3.1c">2\times 2</annotation></semantics></math>. The next two layers are similar, with 128 filters of size <math id="S3.I2.i5.p1.4.m4.1" class="ltx_Math" alttext="3\times 3" display="inline"><semantics id="S3.I2.i5.p1.4.m4.1a"><mrow id="S3.I2.i5.p1.4.m4.1.1" xref="S3.I2.i5.p1.4.m4.1.1.cmml"><mn id="S3.I2.i5.p1.4.m4.1.1.2" xref="S3.I2.i5.p1.4.m4.1.1.2.cmml">3</mn><mo lspace="0.222em" rspace="0.222em" id="S3.I2.i5.p1.4.m4.1.1.1" xref="S3.I2.i5.p1.4.m4.1.1.1.cmml">×</mo><mn id="S3.I2.i5.p1.4.m4.1.1.3" xref="S3.I2.i5.p1.4.m4.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.I2.i5.p1.4.m4.1b"><apply id="S3.I2.i5.p1.4.m4.1.1.cmml" xref="S3.I2.i5.p1.4.m4.1.1"><times id="S3.I2.i5.p1.4.m4.1.1.1.cmml" xref="S3.I2.i5.p1.4.m4.1.1.1"></times><cn type="integer" id="S3.I2.i5.p1.4.m4.1.1.2.cmml" xref="S3.I2.i5.p1.4.m4.1.1.2">3</cn><cn type="integer" id="S3.I2.i5.p1.4.m4.1.1.3.cmml" xref="S3.I2.i5.p1.4.m4.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I2.i5.p1.4.m4.1c">3\times 3</annotation></semantics></math> and another max pooling layer. This is followed by several more pairs of DepthwiseConv2D with a different number of filters and MaxPooling2D layers that are stacked on top of each other and learn increasingly complex features from the images. After two fully connected layers with 4096 units and a dropout of 0.5 applied on each layer, the output is generated with activation “sigmoid” and Dense Layer 1 unit. Compared to VGG16, our customized CNN has a deeper architecture with more convolutional layers and is similar to that of VGG19. It also has a higher number of filters in layers than VGG16 and VGG19. Additionally, Custom CNN uses a dropout layer after each fully connected layer, something that is not present in VGG16 and VGG19.</p>
</div>
<figure id="S3.F16" class="ltx_figure"><img src="/html/2402.09795/assets/images/CNN.png" id="S3.F16.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Custom CNN Model Architecture</figcaption>
</figure>
</li>
</ol>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Result Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We used homomorphic encrypted MRI data for our experiment. In our architecture, this part works as data governance. We tested simple machine-learning algorithms on both encrypted and unencrypted data. We saw on unencrypted data we were able to get the highest 97.1% accuracy whereas, on encrypted data, we were able to get the highest 70.12% accuracy. That shows The approach that we followed to encrypt the data is somewhat usable.</p>
</div>
<figure id="S4.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2402.09795/assets/images/Unencryped_Output.png" id="S4.F17.g1" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="172" height="129" alt="Refer to caption"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img src="/html/2402.09795/assets/images/Encrypted_Output.png" id="S4.F17.g2" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" width="169" height="129" alt="Refer to caption"></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Performance of tested models on unencrypted and encrypted dataset</figcaption>
</figure>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">For our experiment, we used four different pre-trained deep learning models - VGG16, VGG19, ResNet50, and ResNet152 - as well as a custom CNN model that we developed in-house, to classify pituitary tumor and no tumor from homomorphic encrypted MRI data during federated learning. We evaluated the performance of these models using accuracy and F1-score, as well as precision for each class. These results are from global models of federated learning which we used in our architecture.
</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">Accuracy is the proportion of correctly classified cases out of all cases. In this study, all five models achieved accuracy greater than 50%, indicating that they performed better than random guessing. The Custom CNN model achieved the highest accuracy of 83.31%, followed by VGG19 with an accuracy of 78.58%, VGG16 with an accuracy of 77.25%, ResNet152 with an accuracy of 65.09%, and ResNet50 with an accuracy of 61.51%.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">F1-score is a harmonic mean of precision and recall and is often used as a measure of overall model performance. Looking at the F1-score results, we can see that the Custom CNN model achieved the highest score of 83.13%, followed by VGG19 with an F1-score of 78.31%, VGG16 with an F1-score of 77.11%, ResNet152 with an F1-score of 64.46%, and ResNet50 with an F1-score of 61.45%.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Accuracy scores across different models</figcaption>
<div id="S4.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:102.6pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.5pt,12.1pt) scale(0.808195682091398,0.808195682091398) ;">
<table id="S4.T3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;" rowspan="2"><span id="S4.T3.1.1.1.1.1" class="ltx_text">Model</span></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Accuracy Scores</td>
<td id="S4.T3.1.1.1.4" class="ltx_td ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Precision</td>
<td id="S4.T3.1.1.1.6" class="ltx_td ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.1.7" class="ltx_td ltx_align_center ltx_border_tt" style="padding-top:2.5pt;padding-bottom:2.5pt;" colspan="2">Recall</td>
</tr>
<tr id="S4.T3.1.1.2" class="ltx_tr">
<td id="S4.T3.1.1.2.1" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy</td>
<td id="S4.T3.1.1.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">F1 - Score</td>
<td id="S4.T3.1.1.2.4" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Pituitary Tumor</td>
<td id="S4.T3.1.1.2.6" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No Tumor</td>
<td id="S4.T3.1.1.2.7" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">Pituitary Tumor</td>
<td id="S4.T3.1.1.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">No Tumor</td>
</tr>
<tr id="S4.T3.1.1.3" class="ltx_tr">
<td id="S4.T3.1.1.3.1" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">VGG16</td>
<td id="S4.T3.1.1.3.2" class="ltx_td ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.3.3" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">77.25%</td>
<td id="S4.T3.1.1.3.4" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">77.11%</td>
<td id="S4.T3.1.1.3.5" class="ltx_td ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.3.6" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">75.28%</td>
<td id="S4.T3.1.1.3.7" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">79.22%</td>
<td id="S4.T3.1.1.3.8" class="ltx_td ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.3.9" class="ltx_td ltx_align_right ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.72%</td>
<td id="S4.T3.1.1.3.10" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_t" style="padding-top:2.5pt;padding-bottom:2.5pt;">73.49%</td>
</tr>
<tr id="S4.T3.1.1.4" class="ltx_tr">
<td id="S4.T3.1.1.4.1" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">VGG19</td>
<td id="S4.T3.1.1.4.2" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.4.3" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">78.58%</td>
<td id="S4.T3.1.1.4.4" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">78.31%</td>
<td id="S4.T3.1.1.4.5" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.4.6" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">75.82%</td>
<td id="S4.T3.1.1.4.7" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">81.33%</td>
<td id="S4.T3.1.1.4.8" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.4.9" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">83.13%</td>
<td id="S4.T3.1.1.4.10" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">73.49%</td>
</tr>
<tr id="S4.T3.1.1.5" class="ltx_tr">
<td id="S4.T3.1.1.5.1" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">ResNet50</td>
<td id="S4.T3.1.1.5.2" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.5.3" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">61.51%</td>
<td id="S4.T3.1.1.5.4" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">61.45%</td>
<td id="S4.T3.1.1.5.5" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.5.6" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">62.38%</td>
<td id="S4.T3.1.1.5.7" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">60.67%</td>
<td id="S4.T3.1.1.5.8" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.5.9" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">57.83%</td>
<td id="S4.T3.1.1.5.10" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">65.06%</td>
</tr>
<tr id="S4.T3.1.1.6" class="ltx_tr">
<td id="S4.T3.1.1.6.1" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">ResNet152</td>
<td id="S4.T3.1.1.6.2" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.6.3" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">65.09%</td>
<td id="S4.T3.1.1.6.4" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">64.46%</td>
<td id="S4.T3.1.1.6.5" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.6.6" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">68.18%</td>
<td id="S4.T3.1.1.6.7" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">62.00%</td>
<td id="S4.T3.1.1.6.8" class="ltx_td" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.6.9" class="ltx_td ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">54.22%</td>
<td id="S4.T3.1.1.6.10" class="ltx_td ltx_nopad_r ltx_align_right" style="padding-top:2.5pt;padding-bottom:2.5pt;">74.70%</td>
</tr>
<tr id="S4.T3.1.1.7" class="ltx_tr">
<td id="S4.T3.1.1.7.1" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">Custom CNN</td>
<td id="S4.T3.1.1.7.2" class="ltx_td ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.7.3" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">83.31%</td>
<td id="S4.T3.1.1.7.4" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">83.13%</td>
<td id="S4.T3.1.1.7.5" class="ltx_td ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.7.6" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">85.71%</td>
<td id="S4.T3.1.1.7.7" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">80.90%</td>
<td id="S4.T3.1.1.7.8" class="ltx_td ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
<td id="S4.T3.1.1.7.9" class="ltx_td ltx_align_right ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">79.52%</td>
<td id="S4.T3.1.1.7.10" class="ltx_td ltx_nopad_r ltx_align_right ltx_border_bb" style="padding-top:2.5pt;padding-bottom:2.5pt;">86.75%</td>
</tr>
</table>
</span></div>
</figure>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">The VGG16 model had a precision of 75.28% for pituitary tumors and 79.22% for no tumors in the binary classification task and 80.72% and 73.49% recall percentages accordingly. This indicates that the model had a moderate ability to correctly identify true positive cases, with slightly higher precision for no tumor cases than for pituitary tumor cases.
</p>
</div>
<figure id="S4.F18" class="ltx_figure"><img src="/html/2402.09795/assets/images/VGG_16_Confusion.png" id="S4.F18.g1" class="ltx_graphics ltx_centering ltx_img_square" width="273" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Confusion Matrix of VGG16</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S4.F19" class="ltx_figure"><img src="/html/2402.09795/assets/images/loss_vgg16.png" id="S4.F19.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Training History of VGG16</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">The VGG19 model had the second-highest precision for both classes in the binary classification task. The precision for the pituitary tumor was 75.82% and for no tumor was 81.33%. The recall percentage is 83.13% and 73.49% for the classes. This suggests that the model had a moderate ability to correctly identify true positive cases, with slightly higher precision for no tumor cases than for pituitary tumor cases.
</p>
</div>
<figure id="S4.F20" class="ltx_figure"><img src="/html/2402.09795/assets/images/VGG_19_Confusion.png" id="S4.F20.g1" class="ltx_graphics ltx_centering ltx_img_square" width="273" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Confusion Matrix of VGG19</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S4.F21" class="ltx_figure"><img src="/html/2402.09795/assets/images/loss_vgg19.png" id="S4.F21.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="294" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>Training History of VGG19</figcaption>
</figure>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">The ResNet50 model had the lowest precision for both classes in the binary classification task. The precision for the pituitary tumor was 62.38% and for no tumor was 60.67% and the recall percentage was 57.83% and 65.06% for the classes. This indicates that the model had a lower ability to correctly identify true positive cases, with a higher number of false positives compared to the other models.
</p>
</div>
<figure id="S4.F22" class="ltx_figure"><img src="/html/2402.09795/assets/images/ResNet50_Confusion.png" id="S4.F22.g1" class="ltx_graphics ltx_centering ltx_img_square" width="274" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>Confusion Matrix of ResNet50</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S4.F23" class="ltx_figure"><img src="/html/2402.09795/assets/images/loss_resnet50.png" id="S4.F23.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="296" height="222" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 23: </span>Training History of ResNet50</figcaption>
</figure>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">The ResNet152 model had a precision of 68.18% for pituitary tumors and 62.00% for no tumors in the binary classification task while having 54.22% and 74.70% recall. This suggests that the model had a lower ability to correctly identify true positive cases, with a higher number of false positives, particularly for no tumor cases.
</p>
</div>
<figure id="S4.F24" class="ltx_figure"><img src="/html/2402.09795/assets/images/ResNet152_Confusion.png" id="S4.F24.g1" class="ltx_graphics ltx_centering ltx_img_square" width="272" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 24: </span>Confusion Matrix of ResNet152</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S4.F25" class="ltx_figure"><img src="/html/2402.09795/assets/images/loss_resnet152.png" id="S4.F25.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="295" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 25: </span>Training History of ResNet152</figcaption>
</figure>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">The Custom CNN model achieved the highest precision for both classes in the binary classification task. The precision for the pituitary tumor was 85.71% and for no tumor was 80.90%. The recall percentage is 79.52% and 86.75% accordingly. This indicates that the model had a high ability to correctly identify true positive cases, with a relatively low number of false positives.
</p>
</div>
<figure id="S4.F26" class="ltx_figure"><img src="/html/2402.09795/assets/images/CNN_Confusion.png" id="S4.F26.g1" class="ltx_graphics ltx_centering ltx_img_square" width="273" height="271" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 26: </span>Confusion Matrix of Custom CNN</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S4.F27" class="ltx_figure"><img src="/html/2402.09795/assets/images/loss_cnn.png" id="S4.F27.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="294" height="223" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 27: </span>Training History of Custom CNN</figcaption>
</figure>
<div id="S4.p10" class="ltx_para">
<p id="S4.p10.1" class="ltx_p">The ROC (Receiver Operating Characteristic) curve is a powerful tool for evaluating the performance of binary classification models. It provides a graphical representation of the trade-off between true positive rate (TPR) and false positive rate (FPR) at various threshold settings, allowing you to choose an appropriate threshold based on your specific needs. A good classifier should have a ROC curve that is as close as possible to the upper left corner of the graph, indicating high TPR and low FPR. By analyzing the ROC curve, you can gain insights into the strengths and weaknesses of your model and make informed decisions about how to improve its performance. The ROC curve is widely used in various fields, including medicine, finance, and machine learning, and is an essential tool for anyone working with binary classification models. The ROC curves of our used DL models are shown below:</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="S4.F28" class="ltx_figure"><img src="/html/2402.09795/assets/images/receiver_operating_characteristic.png" id="S4.F28.g1" class="ltx_graphics ltx_centering ltx_img_square" width="313" height="298" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 28: </span>Receiver Operating Characteristic of Different Models</figcaption>
</figure>
<figure id="S4.F29" class="ltx_figure"><img src="/html/2402.09795/assets/images/CNN_ROC.png" id="S4.F29.g1" class="ltx_graphics ltx_centering ltx_img_square" width="309" height="295" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 29: </span>Receiver Operating Characteristic of Custom CNN</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="S4.p11" class="ltx_para">
<p id="S4.p11.1" class="ltx_p">Overall, the results suggest that the Custom CNN model performed the best in classifying pituitary tumors and no tumors from homomorphic encrypted MRI data, achieving the highest accuracy, F1-score, and precision for both classes. This is particularly notable as the Custom CNN model was developed in-house, suggesting the potential for further research and development in this area. However, it is important to note that the use of partially homomorphic encryption may have influenced the results, and further research is required to confirm this hypothesis.
</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This research demonstrates an advanced data fabric architecture that enables data fusion, integration, and model parameters sharing framework to apply machine learning models without moving the data to a centralized repository. The Partial Homomorphic EncryptionWe and Federated Learning ensured data integrity, privacy, and decentralized learning. We explored the use of pre-trained deep learning models to classify pituitary tumors from homomorphic encrypted MRI data. Our analysis showed that the VGG16 and VGG19 models outperformed the ResNet50 and ResNet152 models in terms of accuracy, precision, recall, and F1-score for both classes. We achieved overall satisfactory accuracy from VGG16 and VGG19. Out of all of these pre-trained models, our custom CNN model performed better with 83.31% accuracy. The reason for the model to perform better is we have made the model similar to VGG16 and VGG19 while making it much less complex. Our model has a total of around 15 million parameters compared to 138 million of VGG16. Despite several significant achievements, the proposed method can be improved by using Fully Homomorphic Encryption. However, it will greatly increase the size of the images, which resulted in significant storage requirements. We have used homogeneous deep learning models for local and global model training in the federated learning approach. The heterogeneous learning models can be used for the robust data fabric architecture,</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgement</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">The authors are also grateful to King Saud University, Riyadh, Saudi Arabia for funding this work through Researchers Supporting Project Number (RSP2023R18). In addition, we acknowledge the support of the PNRR project FAIR - Future AI Research (PE00000013), Spoke 9 - Green-aware AI, under the NRRP MUR program funded by the NextGenerationEU.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> Public law 104 - 191 - health insurance portability and accountability act of 1996, en, https://www.govinfo.gov/app/details/PLAW-104publ191.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Council of european union, 2018 reform of eu data protection rules, european commission.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> A. Hussain, M. Zarour, M. Alenezi, et al., “Healthcare data breaches: Insights and implications,” Healthcare, vol. 8, p. 133, May 2020. doi: 10.3390/healthcare8020133.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> H. Kupwade Patil and R. Seshadri, “Big data security and privacy issues in healthcare,” in 2014 IEEE International Congress on Big Data, Anchorage, AK: IEEE, Jun. 2014.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> Definition of data fabric-gartner information technology glossary.[Online].Available:https://www.gartner.com/en/information-technology/glossary/data-fabric.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> K. Bonawitz, H. Eichner, W. Grieskamp, et al., “Towards federated learning at scale: System design,” Proceedings of machine learning and systems, vol. 1, pp. 374–388, 2019.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"> L. Morris, “Analysis of partially and fully homomorphic encryption,” Rochester Institute of Technology, pp. 1–5, 2013.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> N. G. Kuftinova, O. I. Maksimychev, A. V. Ostroukh, A. V. Volosova, and E. N. Matukhina, “Data fabric as an effective method of data management in traffic and road systems,” in 2022 Systems of Signals Generating and Processing in the Field of on Board Communications, Moscow, Russian Federation: IEEE, Mar. 2022.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> C. Stamatellis, P. Papadopoulos, N. Pitropakis, S. Katsikas, and W. J. Buchanan, “A privacy-preserving healthcare framework using hyperledger fabric,” Sensors, vol. 20, no. 22, 2020, ISSN: 1424-8220. DOI: 10.3390/s20226587. [Online]. Available: https://www.mdpi.com/1424-8220/20/22/6587.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> A. Roehrs, C. A. da Costa, and R. da Rosa Righi, “Omniphr: A distributed architecture model to integrate personal health records,” Journal of Biomedical Informatics, vol. 71, pp. 70–81, 2017, ISSN: 1532-0464. DOI: https://doi.org/10.1016/j.jbi.2017.05.012. [Online].Available: https://www.sciencedirect.com/science/article/pii/S1532046417301089.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> Q. Xia, E. B. Sifah, K. O. Asamoah, J. Gao, X. Du, and M. Guizani, “Medshare: Trust-less medical data sharing among cloud service providers via blockchain,” IEEE Access, vol. 5, pp. 14 757–14 767, 2017. DOI: 10.1109/ACCESS.2017.2730843.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> Y. Ming and T. Zhang, “Efficient privacy-preserving access control scheme in electronic health records system,” Sensors, vol. 18, no. 10, p. 3520, 2018.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"> J. Fu, J. Xu, S. Zhang, and C. Zhang, “Research and design of square kilometer array astronomical data management model based on fabric,” in 2020 International Conferences on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData) and IEEE Congress on Cybermatics (Cybermatics), Rhodes, Greece: IEEE, Nov. 2020.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"> P. Kalyani, M. Masooda, and P. Namrata, “Preserving privacy of data in distributed systems using homomorphic encryption,” in Advances in Intelligent Systems and Computing, ser. Advances in intelligent systems and computing, Singapore: Springer Singapore, 2021, pp. 306–313.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"> K. Munjal and R. Bhatia, “A systematic review of homomorphic encryption and its contributions in healthcare industry,” en, Complex intell. syst., pp. 1–28, May 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"> F. Wibawa, F. O. Catak, M. Kuzlu, S. Sarp, and U. Cali, “Homomorphic encryption and federated learning based privacy preserving cnn training: Covid-19 detection use-case,” in Proceedings of the 2022 European Interdisciplinary Cybersecurity Conference, 2022, pp. 85–90.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"> A. Madi, O. Stan, A. Mayoue, A. Grivet-Sebert, C. Gouy-Pailler, and R. Sirdey, “A secure federated learning framework using homomorphic encryption and verifiable computing,” in 2021 Reconciling Data Analytics, Automation, Privacy, and Security: A Big Data Challenge (RDAAPS), IEEE, 2021, pp. 1–8.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"> W. Ou, J. Zeng, Z. Guo, W. Yan, D. Liu, and S. Fuentes, “A homomorphic-encryption-based vertical federated learning scheme for risk management,” Computer Science and Information Systems, vol. 17, no. 3, pp. 819–834, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"> I. Kotsiuba, A. Velvkzhanin, Y. Yanovich, I. S. Bandurova, Y. Dyachenko, and V. Zhygulin, “Decentralized e-health architecture for boosting healthcare analytics,” in 2018 Second World Conference on Smart Trends in Systems, Security and Sustainability (WorldS4), IEEE, 2018, pp. 113–118.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"> C. Zhang, R. Ma, S. Sun, Y. Li, Y. Wang, and Z. Yan, “Optimizing the electronic health records through big data analytics: A knowledge-based view,” IEEE Access, vol. 7, pp. 136 223–136 231, 2019.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"> N. Janbi, R. Mehmood, I. Katib, A. Albeshri, J. M. Corchado, and T. Yigitcanlar, “Imtidad: A reference architecture and a case study on developing distributed AI services for skin disease diagnosis over cloud, fog and edge,” en, Sensors (Basel), vol. 22, no. 5, p. 1854, Feb. 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"> G. A. Montes and B. Goertzel, “Distributed, decentralized, and democratized artificial intelligence,” en, Technol. Forecast. Soc. Change, vol. 141, pp. 354–358, Dec. 2018.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"> F. Rahman, M. Slepian, and A. Mitra, “A novel bigdata processing framwork for healthcare applications: Big-data-healthcare-in-a-box,” in 2016 IEEE International Conference on Big Data (Big Data), Washington DC,USA: IEEE, Dec. 2016.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"> M. J. Kaur and V. P. Mishra, “Analysis of big data cloud computing environment on healthcare organizations by implementing hadoop clusters,” in 2018 Fifth HCT Information Technology Trends (ITT), Dubai, United Arab Emirates: IEEE, Nov. 2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"> V. K. Nigam and S. Bhatia, “Impact of cloud computing on health care,” Int Res J Eng Technol, vol. 3, no. 5, pp. 2804–2810, 2016.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"> F. Gao, S. Thiebes, and A. Sunyaev, “Rethinking the meaning of cloud computing for health care: A taxonomic perspective and future research directions,” en, vol. 20, no. 7, e10041, Jul. 2018.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"> H. A. Al Hamid, S. M. M. Rahman, M. S. Hossain, A. Almogren, and A. Alamri, “A security model for preserving the privacy of medical big data in a healthcare cloud using a fog computing facility with pairing-based cryptography,” IEEE Access, vol. 5, pp. 22 313–22 328,2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"> T. D. Dang, D. Hoang, and D. N. Nguyen, “Trustbased scheduling framework for big data processing with MapReduce,” IEEE trans. serv. comput., vol. 15, no. 1, pp. 279–293, Jan. 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"> M. Nickparvar, Brain tumor mri dataset, 2021. [Online]. Available: https://www.kaggle.com/datasets/masoudnickparvar/brain-tumor-mri-dataset.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"> P. Voigt and A. Von dem Bussche, “The eu general data protection regulation (gdpr),” A Practical Guide, 1st Ed., Cham: Springer International Publishing, vol. 10, no. 3152676, pp. 10–5555, 2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"> K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"> K. Simonyan, &amp; A. Zisserman, ”Very deep convolutional networks for large-scale image recognition” arXiv preprint arXiv:1409.1556, 2014

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"> B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y. Arcas, “Communication Efficient Learning of Deep Networks from Decentralized Data,” in Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, A. Singh and J. Zhu, Eds., ser. Proceedings of Machine Learning Research, vol. 54, PMLR, 20–22 Apr 2017, pp. 1273–1282.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"> Pian Qi, Diletta Chiaro, Francesco Piccialli,
”FL-FD: Federated learning-based fall detection with multimodal data fusion,”
Information Fusion, 2023, 101890,
ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2023.101890.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"> Muna Al-Hawawreh, M. Shamim Hossain,
”A privacy-aware framework for detecting cyber attacks on internet of medical things systems using data fusion and quantum deep learning,”
Information Fusion, 2023, 101889,

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"> Wang, K., Chen, C.M., Liang, Z., Hassan, M.M., Sarne, G.M., Fotia, L. and Fortino, G., ”A trusted consensus fusion scheme for decentralized collaborated learning in massive IoT domain. Information Fusion”, 72, pp.100-109.2021

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"> Chahinez Ounoughi, Sadok Ben Yahia,
”Data fusion for ITS: A systematic literature review,” Information Fusion,
Volume 89, 2023, Pages 267-291,
ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2022.08.016.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"> Akira Imakura, Tetsuya Sakurai, Yukihiko Okada, Tomoya Fujii, Teppei Sakamoto, Hiroyuki Abe,
”Non-readily identifiable data collaboration analysis for multiple datasets including personal information,” Information Fusion,
Volume 98, 2023, 101826, ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2023.101826.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Mohammad Mehedi Hassan, Md. Golam Rabiul Alam, Md. Zia Uddin, Shamsul Huda, Ahmad Almogren, Giancarlo Fortino,
”Human emotion recognition using deep belief network architecture,” Information Fusion, Volume 51,
2019, Pages 10-18, ISSN 1566-2535,
https://doi.org/10.1016/j.inffus.2018.10.009.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.09794" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.09795" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.09795">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.09795" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.09796" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 15:51:34 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
