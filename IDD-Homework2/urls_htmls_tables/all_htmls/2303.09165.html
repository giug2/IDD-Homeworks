<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2303.09165] A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation</title><meta property="og:description" content="Deep learning in computer vision has achieved great success with the price of large-scale labeled training data. However, exhaustive data annotation is impracticable for each task of all domains of interest, due to hig…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2303.09165">

<!--Generated on Thu Feb 29 19:35:57 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A New Benchmark: On the Utility of Synthetic Data with Blender for 
<br class="ltx_break">Bare Supervised Learning and Downstream Domain Adaptation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hui Tang<sup id="id5.5.id1" class="ltx_sup"><span id="id5.5.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup> and Kui Jia<sup id="id6.6.id2" class="ltx_sup"><span id="id6.6.id2.1" class="ltx_text ltx_font_italic">1,</span></sup>
<br class="ltx_break"><sup id="id7.7.id3" class="ltx_sup">1</sup> South China University of Technology  <sup id="id8.8.id4" class="ltx_sup">2</sup> DexForce Co. Ltd. 
<br class="ltx_break"><span id="id9.9.id5" class="ltx_text ltx_font_typewriter" style="font-size:90%;">eehuitang@mail.scut.edu.cn</span>, <span id="id10.10.id6" class="ltx_text ltx_font_typewriter" style="font-size:90%;">kuijia@scut.edu.cn</span>
</span><span class="ltx_author_notes">Corresponding author.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id11.id1" class="ltx_p">Deep learning in computer vision has achieved great success with the price of large-scale labeled training data. However, exhaustive data annotation is impracticable for each task of all domains of interest, due to high labor costs and unguaranteed labeling accuracy. Besides, the uncontrollable data collection process produces non-IID training and test data, where undesired duplication may exist. All these nuisances may hinder the verification of typical theories and exposure to new findings. To circumvent them, an alternative is to generate synthetic data via 3D rendering with domain randomization. We in this work push forward along this line by doing profound and extensive research on bare supervised learning and downstream domain adaptation.
Specifically, under the well-controlled, IID data setting enabled by 3D rendering, we systematically verify the typical, important learning insights, e.g., shortcut learning, and discover the new laws of various data regimes and network architectures in generalization.
We further investigate the effect of image formation factors on generalization, e.g., object scale, material texture, illumination, camera viewpoint, and background in a 3D scene.
Moreover, we use the simulation-to-reality adaptation as a downstream task for comparing the transferability between synthetic and real data when used for pre-training, which demonstrates that synthetic data pre-training is also promising to improve real test results.
Lastly, to promote future research, we develop a new large-scale synthetic-to-real benchmark for image classification, termed S2RDA, which provides more significant challenges for transfer from simulation to reality.
</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.2" class="ltx_p">Recently, we have witnessed considerable advances in various computer vision applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib67" title="" class="ltx_ref">67</a>, <a href="#bib.bib56" title="" class="ltx_ref">56</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. However, such a success is vulnerable and expensive in that it has been limited to supervised learning methods with abundant labeled data.
Some publicly available datasets exist certainly, which include a great mass of real-world images and acquire their labels via crowdsourcing generally. For example, ImageNet-1K <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is of <math id="S1.p1.1.m1.1" class="ltx_Math" alttext="1.28" display="inline"><semantics id="S1.p1.1.m1.1a"><mn id="S1.p1.1.m1.1.1" xref="S1.p1.1.m1.1.1.cmml">1.28</mn><annotation-xml encoding="MathML-Content" id="S1.p1.1.m1.1b"><cn type="float" id="S1.p1.1.m1.1.1.cmml" xref="S1.p1.1.m1.1.1">1.28</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.1.m1.1c">1.28</annotation></semantics></math>M images; MetaShift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> has <math id="S1.p1.2.m2.1" class="ltx_Math" alttext="2.56" display="inline"><semantics id="S1.p1.2.m2.1a"><mn id="S1.p1.2.m2.1.1" xref="S1.p1.2.m2.1.1.cmml">2.56</mn><annotation-xml encoding="MathML-Content" id="S1.p1.2.m2.1b"><cn type="float" id="S1.p1.2.m2.1.1.cmml" xref="S1.p1.2.m2.1.1">2.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p1.2.m2.1c">2.56</annotation></semantics></math>M natural images. Nevertheless, data collection and annotation for all tasks of domains of interest are impractical since many of them require exhaustive manual efforts and valuable domain expertise, e.g., self-driving and medical diagnosis. What’s worse, the label given by humans has no guarantee to be correct, resulting in unpredictable label noise. Besides, the poor-controlled data collection process produces a lot of nuisances, e.g., training and test data aren’t independent identically distributed (IID) and even have duplicate images. All of these shortcomings could prevent the validation of typical insights and exposure to new findings.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.4" class="ltx_p">To remedy them, one can resort to synthetic data generation via 3D rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, where an arbitrary number of images can be produced with diverse values of imaging factors randomly chosen in a reasonable range, i.e., domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>; such a dataset creation pipeline is thus very lucrative, where data with labels come for free. For image classification, Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> propose the first large-scale synthetic-to-real benchmark for visual domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>, VisDA-2017; it includes <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="152" display="inline"><semantics id="S1.p2.1.m1.1a"><mn id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">152</mn><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><cn type="integer" id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">152</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">152</annotation></semantics></math>K synthetic images and <math id="S1.p2.2.m2.1" class="ltx_Math" alttext="55" display="inline"><semantics id="S1.p2.2.m2.1a"><mn id="S1.p2.2.m2.1.1" xref="S1.p2.2.m2.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S1.p2.2.m2.1b"><cn type="integer" id="S1.p2.2.m2.1.1.cmml" xref="S1.p2.2.m2.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.2.m2.1c">55</annotation></semantics></math>K natural ones. Ros et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> produce <math id="S1.p2.3.m3.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S1.p2.3.m3.1a"><mn id="S1.p2.3.m3.1.1" xref="S1.p2.3.m3.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S1.p2.3.m3.1b"><cn type="integer" id="S1.p2.3.m3.1.1.cmml" xref="S1.p2.3.m3.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.3.m3.1c">9</annotation></semantics></math>K synthetic cityscape images for cross-domain semantic segmentation. Hinterstoisser et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> densely render a set of <math id="S1.p2.4.m4.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S1.p2.4.m4.1a"><mn id="S1.p2.4.m4.1.1" xref="S1.p2.4.m4.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S1.p2.4.m4.1b"><cn type="integer" id="S1.p2.4.m4.1.1.cmml" xref="S1.p2.4.m4.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.4.m4.1c">64</annotation></semantics></math> retail objects for retail detection. All these datasets are customized for specific tasks in cross-domain transfer. In this work, we push forward along this line extensively and profoundly.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The deep models tend to find simple, unintended solutions and learn shortcut features less related to the semantics of particular object classes, due to systematic biases, as revealed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. For example, a model basing its prediction on context would misclassify an airplane floating on water as a boat. The seminal work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> emphasizes that shortcut opportunities are present in most data and rarely disappear by simply adding more data. Modifying the training data to block specific shortcuts may be a promising solution, e.g., making image variation factors consistently distributed across all categories. To empirically verify the insight, we propose to compare the traditional fixed-dataset periodic training strategy with a new strategy of training with unduplicated examples generated by 3D rendering, under the well-controlled, IID data setting. We run experiments on three representative network architectures of ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, ViT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and MLP-Mixer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, which consistently show obvious advantages of the data-unrepeatable training (cf. Sec. <a href="#S4.SS1" title="4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>). This also naturally validates the typical arguments of probably approximately correct (PAC) generalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>
and variance-bias trade-off <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Thanks to the ideal IID data condition enabled by the well-controlled 3D rendering, we can also discover more reliable laws of various data regimes and network architectures in generalization. Some interesting observations are as follows.
</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Do not learn shortcuts!</span> The test results on synthetic data without background are good enough to show that the synthetically trained models do not learn shortcut solutions relying on context clues <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">A zero-sum game.</span> For the data-unrepeatable training, IID and OOD (Out-of-Distribution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>) generalizations are some type of zero-sum game w.r.t. the strength of data augmentation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Data augmentations do not help ViT much!</span> In IID tests, ViT performs surprisingly poorly whatever the data augmentation is and even the triple number of training epochs does not improve much.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p"><span id="S1.I1.i4.p1.1.1" class="ltx_text ltx_font_bold">There is always a bottleneck from synthetic data to OOD/real data</span>. Here, increasing data size and model capacity brings no more benefits, and domain adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> to bridge the distribution gap is indispensable except for evolving the image generation pipeline to synthesize more realistic images.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Furthermore, we comprehensively assess image variation factors, e.g., object scale, material texture, illumination, camera viewpoint, and background in a 3D scene. We then find that to generalize well, deep neural networks must <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">learn to ignore non-semantic variability</em>, which may appear in the test. To this end, sufficient images with different values of one imaging factor should be generated to learn a robust, unbiased model, proving the necessity of sample diversity for generalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>. We also observe that <em id="S1.p4.1.2" class="ltx_emph ltx_font_italic">different factors and even their different values have uneven importance to IID generalization</em>, implying that the under-explored weighted rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is worth studying.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Bare supervised learning on synthetic data results in poor performance in OOD/real tests, and pre-training and then domain adaptation can improve. Domain adaptation (DA) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> is a hot research area, which aims to make predictions for unlabeled instances in the target domain by transferring knowledge from the labeled source domain. To our knowledge, there is little research on pre-training for DA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (with real data). We thus use the popular simulation-to-real classification adaptation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> as a downstream task, study the transferability of synthetic data pre-trained models by comparing with those pre-trained on real data like ImageNet and MetaShift. We report results for several representative DA methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> on the commonly used backbone, and our experiments yield some surprising findings.</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p"><span id="S1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">The importance of pre-training for DA.</span> DA fails without pre-training (cf. Sec. <a href="#S4.SS3.SSS1" title="4.3.1 Results and Discussions ‣ 4.3 Exploring Pre-training for Domain Adaptation ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3.1</span></a>). </p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p"><span id="S1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Effects of different pre-training schemes.</span> Different DA methods exhibit different relative advantages under different pre-training data. The reliability of existing DA method evaluation criteria is unguaranteed. </p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p"><span id="S1.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Synthetic data pre-training vs. real data pre-training.</span> Synthetic data pre-training is better than pre-training on real data in our study. </p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p"><span id="S1.I2.i4.p1.1.1" class="ltx_text ltx_font_bold">Implications for pre-training data setting.</span> Big Synthesis Small Real is worth researching. Pre-train with target classes first under limited computing resources. </p>
</div>
</li>
<li id="S1.I2.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i5.p1" class="ltx_para">
<p id="S1.I2.i5.p1.1" class="ltx_p"><span id="S1.I2.i5.p1.1.1" class="ltx_text ltx_font_bold">The improved generalization of DA models.</span> Real data pre-training with extra non-target classes, fine-grained target subclasses, or our synthesized data added for target classes helps DA.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Last but not least, we introduce a new, large-scale synthetic-to-real benchmark for classification adaptation (S2RDA), which has two challenging tasks S2RDA-49 and S2RDA-MS-39. S2RDA contains more categories, more realistically synthesized source domain data coming for free, and more complicated target domain data collected from diverse real-world sources, setting a more practical and challenging benchmark for future DA research. </p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.5" class="ltx_p"><span id="S2.p1.5.1" class="ltx_text ltx_font_bold">Real Datasets.</span> A lot of large-scale real datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> have harnessed and organized the explosive image data from the Internet or real world for deep learning of meaningful visual representations. For example, ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> is a large-scale image database consisting of <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="1.28" display="inline"><semantics id="S2.p1.1.m1.1a"><mn id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">1.28</mn><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><cn type="float" id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">1.28</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">1.28</annotation></semantics></math>M images from <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S2.p1.2.m2.1a"><mn id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><cn type="integer" id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">1</annotation></semantics></math>K common object categories, and serves as the primary dataset for pre-training deep models for vision tasks. Barbu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> collect a large real-world test set for more realistic object recognition, ObjectNet, which has <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S2.p1.3.m3.1a"><mn id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><cn type="integer" id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">50</annotation></semantics></math>K images and is bias-controlled. MetaShift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> of <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="2.56" display="inline"><semantics id="S2.p1.4.m4.1a"><mn id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">2.56</mn><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><cn type="float" id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">2.56</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">2.56</annotation></semantics></math>M natural images (<math id="S2.p1.5.m5.1" class="ltx_Math" alttext="\sim 400" display="inline"><semantics id="S2.p1.5.m5.1a"><mrow id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml"><mi id="S2.p1.5.m5.1.1.2" xref="S2.p1.5.m5.1.1.2.cmml"></mi><mo id="S2.p1.5.m5.1.1.1" xref="S2.p1.5.m5.1.1.1.cmml">∼</mo><mn id="S2.p1.5.m5.1.1.3" xref="S2.p1.5.m5.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><apply id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1"><csymbol cd="latexml" id="S2.p1.5.m5.1.1.1.cmml" xref="S2.p1.5.m5.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="S2.p1.5.m5.1.1.2.cmml" xref="S2.p1.5.m5.1.1.2">absent</csymbol><cn type="integer" id="S2.p1.5.m5.1.1.3.cmml" xref="S2.p1.5.m5.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\sim 400</annotation></semantics></math> classes) is formed by context guided clustering of the images from GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S2.p2" class="ltx_para ltx_noindent">
<p id="S2.p2.5" class="ltx_p"><span id="S2.p2.5.1" class="ltx_text ltx_font_bold">Synthetic Datasets.</span> Thanks to 3D rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> and domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>, synthetic data with increased sample diversity can be generated for free now, facilitating various vision tasks.
VisDA-2017 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> is a large-scale benchmark dataset for cross-domain object classification, focusing on the simulation-to-reality shift, with <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="152" display="inline"><semantics id="S2.p2.1.m1.1a"><mn id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">152</mn><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><cn type="integer" id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">152</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">152</annotation></semantics></math>K synthetic images and <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="55" display="inline"><semantics id="S2.p2.2.m2.1a"><mn id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">55</mn><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><cn type="integer" id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">55</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">55</annotation></semantics></math>K natural ones across <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S2.p2.3.m3.1a"><mn id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><cn type="integer" id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">12</annotation></semantics></math> categories. For cross-domain semantic segmentation, Ros et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>]</cite> produce <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="9" display="inline"><semantics id="S2.p2.4.m4.1a"><mn id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><cn type="integer" id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">9</annotation></semantics></math>K synthetic images by rendering a virtual city using the Unity engine;
many other works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>, <a href="#bib.bib84" title="" class="ltx_ref">84</a>, <a href="#bib.bib85" title="" class="ltx_ref">85</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> focus on computer graphics simulations to synthesize outdoor or indoor scene images. For retail detection, Hinterstoisser et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> densely render <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S2.p2.5.m5.1a"><mn id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><cn type="integer" id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">64</annotation></semantics></math> retail objects and a large dataset of 3D background models. For car detection, domain randomization is also utilized and developed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Domain Adaptation.</span> Domain adaptation is a developing field with a huge diversity of approaches. A popular strategy is to explicitly model and minimize the distribution shift between the source and target domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, such that the domain-invariant features can be learned.
Differently, works of another emerging strategy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> take steps towards implicit domain adaptation, without explicit feature alignment.
In this work, we consider these representative DA methods for the empirical study, and broader introductions to the rich literature are provided in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data Synthesis via Domain Randomization</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We adopt the widely used 3D rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> in a simulator for data synthesis and generate synthetic RGB images for model training. To increase sample diversity for better generalization, we apply domain randomization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite> during rendering, whose efficacy has been demonstrated in various applications <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite>.
Specifically, we start by sampling a 3D object model from one specific class of interest from ShapeNet repository <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and place it in a blank scene; next, we set the lighting condition with a point source of randomized parameters and place the camera at random positions on the surface of a sphere of random radius, which has lens looking at the object and the intrinsic resolution of 256<math id="S3.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.p1.1.m1.1a"><mo id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><times id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\times</annotation></semantics></math>256; next, we apply random materials and textures to the object; then, we use an RGB renderer to take pictures from different camera viewpoints in the configured scene; finally, the rendered images are composed over a background image chosen at random from open resources. The synthesized images with the automatically generated ground truth class labels are used as low-cost training data.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2303.09165/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="215" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S3.F1.3.2" class="ltx_text" style="font-size:90%;">Sample images from the training (left) and validation (middle) domains of VisDA-2017 and our synthesized data (right).</span></figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.7" class="ltx_p">The changing ranges of image variation factors in the 3D scene are as follows.
<span id="S3.p2.7.1" class="ltx_text ltx_font_bold">(1)</span> The scale of the object in an image depends on its distance to the camera, namely the radius of a sphere on whose surface the camera is located. The radius is ranged in <math id="S3.p2.1.m1.2" class="ltx_Math" alttext="[0.8,2.4]" display="inline"><semantics id="S3.p2.1.m1.2a"><mrow id="S3.p2.1.m1.2.3.2" xref="S3.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.p2.1.m1.2.3.2.1" xref="S3.p2.1.m1.2.3.1.cmml">[</mo><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">0.8</mn><mo id="S3.p2.1.m1.2.3.2.2" xref="S3.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.p2.1.m1.2.2" xref="S3.p2.1.m1.2.2.cmml">2.4</mn><mo stretchy="false" id="S3.p2.1.m1.2.3.2.3" xref="S3.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.2b"><interval closure="closed" id="S3.p2.1.m1.2.3.1.cmml" xref="S3.p2.1.m1.2.3.2"><cn type="float" id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">0.8</cn><cn type="float" id="S3.p2.1.m1.2.2.cmml" xref="S3.p2.1.m1.2.2">2.4</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.2c">[0.8,2.4]</annotation></semantics></math>.
<span id="S3.p2.7.2" class="ltx_text ltx_font_bold">(2)</span> The material texture of an object is from CCTextures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, which contains actual images.
<span id="S3.p2.7.3" class="ltx_text ltx_font_bold">(3)</span> The point light is on a shell centered at <math id="S3.p2.2.m2.3" class="ltx_Math" alttext="[1,2,3]" display="inline"><semantics id="S3.p2.2.m2.3a"><mrow id="S3.p2.2.m2.3.4.2" xref="S3.p2.2.m2.3.4.1.cmml"><mo stretchy="false" id="S3.p2.2.m2.3.4.2.1" xref="S3.p2.2.m2.3.4.1.cmml">[</mo><mn id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml">1</mn><mo id="S3.p2.2.m2.3.4.2.2" xref="S3.p2.2.m2.3.4.1.cmml">,</mo><mn id="S3.p2.2.m2.2.2" xref="S3.p2.2.m2.2.2.cmml">2</mn><mo id="S3.p2.2.m2.3.4.2.3" xref="S3.p2.2.m2.3.4.1.cmml">,</mo><mn id="S3.p2.2.m2.3.3" xref="S3.p2.2.m2.3.3.cmml">3</mn><mo stretchy="false" id="S3.p2.2.m2.3.4.2.4" xref="S3.p2.2.m2.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.3b"><list id="S3.p2.2.m2.3.4.1.cmml" xref="S3.p2.2.m2.3.4.2"><cn type="integer" id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1">1</cn><cn type="integer" id="S3.p2.2.m2.2.2.cmml" xref="S3.p2.2.m2.2.2">2</cn><cn type="integer" id="S3.p2.2.m2.3.3.cmml" xref="S3.p2.2.m2.3.3">3</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.3c">[1,2,3]</annotation></semantics></math>, whose radius and elevation are ranged in <math id="S3.p2.3.m3.2" class="ltx_Math" alttext="[1,7]" display="inline"><semantics id="S3.p2.3.m3.2a"><mrow id="S3.p2.3.m3.2.3.2" xref="S3.p2.3.m3.2.3.1.cmml"><mo stretchy="false" id="S3.p2.3.m3.2.3.2.1" xref="S3.p2.3.m3.2.3.1.cmml">[</mo><mn id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">1</mn><mo id="S3.p2.3.m3.2.3.2.2" xref="S3.p2.3.m3.2.3.1.cmml">,</mo><mn id="S3.p2.3.m3.2.2" xref="S3.p2.3.m3.2.2.cmml">7</mn><mo stretchy="false" id="S3.p2.3.m3.2.3.2.3" xref="S3.p2.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.2b"><interval closure="closed" id="S3.p2.3.m3.2.3.1.cmml" xref="S3.p2.3.m3.2.3.2"><cn type="integer" id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">1</cn><cn type="integer" id="S3.p2.3.m3.2.2.cmml" xref="S3.p2.3.m3.2.2">7</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.2c">[1,7]</annotation></semantics></math> and <math id="S3.p2.4.m4.2" class="ltx_Math" alttext="[15,70]" display="inline"><semantics id="S3.p2.4.m4.2a"><mrow id="S3.p2.4.m4.2.3.2" xref="S3.p2.4.m4.2.3.1.cmml"><mo stretchy="false" id="S3.p2.4.m4.2.3.2.1" xref="S3.p2.4.m4.2.3.1.cmml">[</mo><mn id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml">15</mn><mo id="S3.p2.4.m4.2.3.2.2" xref="S3.p2.4.m4.2.3.1.cmml">,</mo><mn id="S3.p2.4.m4.2.2" xref="S3.p2.4.m4.2.2.cmml">70</mn><mo stretchy="false" id="S3.p2.4.m4.2.3.2.3" xref="S3.p2.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.2b"><interval closure="closed" id="S3.p2.4.m4.2.3.1.cmml" xref="S3.p2.4.m4.2.3.2"><cn type="integer" id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1">15</cn><cn type="integer" id="S3.p2.4.m4.2.2.cmml" xref="S3.p2.4.m4.2.2">70</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.2c">[15,70]</annotation></semantics></math> respectively.
<span id="S3.p2.7.4" class="ltx_text ltx_font_bold">(4)</span> The camera lies on the surface of a sphere centered at <math id="S3.p2.5.m5.3" class="ltx_Math" alttext="[0,0,0]" display="inline"><semantics id="S3.p2.5.m5.3a"><mrow id="S3.p2.5.m5.3.4.2" xref="S3.p2.5.m5.3.4.1.cmml"><mo stretchy="false" id="S3.p2.5.m5.3.4.2.1" xref="S3.p2.5.m5.3.4.1.cmml">[</mo><mn id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml">0</mn><mo id="S3.p2.5.m5.3.4.2.2" xref="S3.p2.5.m5.3.4.1.cmml">,</mo><mn id="S3.p2.5.m5.2.2" xref="S3.p2.5.m5.2.2.cmml">0</mn><mo id="S3.p2.5.m5.3.4.2.3" xref="S3.p2.5.m5.3.4.1.cmml">,</mo><mn id="S3.p2.5.m5.3.3" xref="S3.p2.5.m5.3.3.cmml">0</mn><mo stretchy="false" id="S3.p2.5.m5.3.4.2.4" xref="S3.p2.5.m5.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.3b"><list id="S3.p2.5.m5.3.4.1.cmml" xref="S3.p2.5.m5.3.4.2"><cn type="integer" id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1">0</cn><cn type="integer" id="S3.p2.5.m5.2.2.cmml" xref="S3.p2.5.m5.2.2">0</cn><cn type="integer" id="S3.p2.5.m5.3.3.cmml" xref="S3.p2.5.m5.3.3">0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.3c">[0,0,0]</annotation></semantics></math> (also the object center) and its azimuth and elevation are from <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="0^{\circ}" display="inline"><semantics id="S3.p2.6.m6.1a"><msup id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml"><mn id="S3.p2.6.m6.1.1.2" xref="S3.p2.6.m6.1.1.2.cmml">0</mn><mo id="S3.p2.6.m6.1.1.3" xref="S3.p2.6.m6.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><apply id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p2.6.m6.1.1.1.cmml" xref="S3.p2.6.m6.1.1">superscript</csymbol><cn type="integer" id="S3.p2.6.m6.1.1.2.cmml" xref="S3.p2.6.m6.1.1.2">0</cn><compose id="S3.p2.6.m6.1.1.3.cmml" xref="S3.p2.6.m6.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">0^{\circ}</annotation></semantics></math> to <math id="S3.p2.7.m7.1" class="ltx_Math" alttext="360^{\circ}" display="inline"><semantics id="S3.p2.7.m7.1a"><msup id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mn id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">360</mn><mo id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">∘</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1">superscript</csymbol><cn type="integer" id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">360</cn><compose id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">360^{\circ}</annotation></semantics></math>.
<span id="S3.p2.7.5" class="ltx_text ltx_font_bold">(5)</span> The background images are from Haven <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, which includes environment HDRs.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Remarks.</span> It is noteworthy that VisDA-2017 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> generates synthetic images by rendering 3D models just under varied camera angles and lighting conditions. Differently, we vary the values of much more image variation factors, leading to more realistic and diverse samples, as illustrated in Fig. <a href="#S3.F1" title="Figure 1 ‣ 3 Data Synthesis via Domain Randomization ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment and Evaluation</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We empirically demonstrate the utility of our synthetic data for supervised learning and downstream transferring by exploring the answers to the following questions:</p>
</div>
<div id="S4.p2" class="ltx_para ltx_noindent">
<p id="S4.p2.1" class="ltx_p"><math id="S4.p2.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p2.1.m1.1a"><mo id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><ci id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">\bullet</annotation></semantics></math>   Can we utilize synthetic data to verify typical theories and expose new findings? What will we find when investigating the learning characteristics and properties of our synthesized new dataset comprehensively?</p>
</div>
<div id="S4.p3" class="ltx_para ltx_noindent">
<p id="S4.p3.1" class="ltx_p"><math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p3.1.m1.1a"><mo id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><ci id="S4.p3.1.m1.1.1.cmml" xref="S4.p3.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\bullet</annotation></semantics></math>   Can a model trained on non-repetitive samples converge? If it could, how will the new training strategy perform when compared to fixed-dataset periodic training? Can the comparison provide any significant intuitions for shortcut learning and other insights?</p>
</div>
<div id="S4.p4" class="ltx_para ltx_noindent">
<p id="S4.p4.1" class="ltx_p"><math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p4.1.m1.1a"><mo id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><ci id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\bullet</annotation></semantics></math>   How will the image variation factors in domain randomization affect the model generalization? What new insights can the study provide for 3D rendering?</p>
</div>
<div id="S4.p5" class="ltx_para ltx_noindent">
<p id="S4.p5.1" class="ltx_p"><math id="S4.p5.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p5.1.m1.1a"><mo id="S4.p5.1.m1.1.1" xref="S4.p5.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p5.1.m1.1b"><ci id="S4.p5.1.m1.1.1.cmml" xref="S4.p5.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p5.1.m1.1c">\bullet</annotation></semantics></math>   Can synthetic data pre-training be on par with real data pre-training when applied to downstream synthetic-to-real classification adaptation? How about large-scale synthetic pre-training with a small amount of real data?</p>
</div>
<div id="S4.p6" class="ltx_para ltx_noindent">
<p id="S4.p6.1" class="ltx_p"><math id="S4.p6.1.m1.1" class="ltx_Math" alttext="\bullet" display="inline"><semantics id="S4.p6.1.m1.1a"><mo id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><ci id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\bullet</annotation></semantics></math>   Is our S2RDA benchmark more challenging and realistic? How does it differ from VisDA-2017?</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Empirical Study on Supervised Learning</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.5" class="ltx_p"><span id="S4.SS1.p1.5.1" class="ltx_text ltx_font_bold">Data Settings.</span> We use the <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mn id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><cn type="integer" id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">10</annotation></semantics></math> object classes common in ShapeNet and VisDA-2017 for the empirical study. We term the synthetic and real domains of the <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mn id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><cn type="integer" id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">10</annotation></semantics></math> classes in VisDA-2017 as SubVisDA-10. For the traditional fixed-dataset periodic training, we generate <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mn id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><cn type="integer" id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">12</annotation></semantics></math>K synthetic images in each class and train the model on the dataset with fixed size epoch by epoch. For our used sample-unrepeatable training, we have mutually exclusive batches of synthesized samples per iteration. At inference, we evaluate the learned model on three types of test data: IID data of <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mn id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><cn type="integer" id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">6</annotation></semantics></math>K samples per class which follow the same distribution as the synthesized training data, IID data of <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mn id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><cn type="integer" id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">60</annotation></semantics></math>K images without background to examine the dependency of network predictions on contexts, and OOD data, i.e., real images from SubVisDA-10. For training, we consider three data augmentation strategies with different strengths: no augmentation which has only the center crop operation, weak augmentation based on pixel positions such as random crop and flip <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and strong augmentation which transforms both position and value of pixels in an image, e.g., random resized crop and color jitter<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In the test phase, we use no augmentation.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.2" class="ltx_p"><span id="S4.SS1.p2.2.1" class="ltx_text ltx_font_bold">Implemental Details.</span> We adopt the standard cross-entropy loss function for pattern learning. We do experiments using ResNet-50 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, ViT-B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>, and Mixer-B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> as the backbone. We train the model from scratch for <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="200" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mn id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><cn type="integer" id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">200</annotation></semantics></math>K iterations and use the SGD optimizer with batch size <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mn id="S4.SS1.p2.2.m2.1.1" xref="S4.SS1.p2.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><cn type="integer" id="S4.SS1.p2.2.m2.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">64</annotation></semantics></math> and the cosine learning rate schedule to update network parameters.
We report the overall accuracy (Acc. %) or mean class precision (Mean %) at the same fixed random seed across all experiments. Other settings are detailed in the appendix.</p>
</div>
<section id="S4.SS1.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Results</h4>

<div id="S4.SS1.SSS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p1.1" class="ltx_p"><span id="S4.SS1.SSS1.p1.1.1" class="ltx_text ltx_font_bold">Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples.</span>
Under the ideal IID data condition enabled by 3D rendering, we empirically verify significant insights on shortcut learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, PAC generalization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite>, and variance-bias trade-off <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, by making comparisons between fixed-dataset periodic training and training on non-repetitive samples. Results are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Figs. <a href="#S4.F2" title="Figure 2 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and A1 (learning process in the appendix). We highlight several observations below.
<span id="S4.SS1.SSS1.p1.1.2" class="ltx_text ltx_font_bold">(1)</span> With more training data of increased sample diversity, the data-unrepeatable training exhibits <em id="S4.SS1.SSS1.p1.1.3" class="ltx_emph ltx_font_italic">higher generalization accuracy and better convergence performance</em> than the fixed-dataset training. <span id="S4.SS1.SSS1.p1.1.4" class="ltx_text ltx_font_bold">(2)</span> To intuitively understand what the models have learned, we visualize the saliency/attention maps in Figs. A2-A5. We observe that all models attend to image regions from global (context) to local (object) as the learning process proceeds; the data-unrepeatable training achieves <em id="S4.SS1.SSS1.p1.1.5" class="ltx_emph ltx_font_italic">qualitative improvements</em> over the fixed-dataset training.
<span id="S4.SS1.SSS1.p1.1.6" class="ltx_text ltx_font_bold">(3)</span> Our synthesized data used for training yield <em id="S4.SS1.SSS1.p1.1.7" class="ltx_emph ltx_font_italic">higher OOD test accuracy</em> than SubVisDA-10 as they share more similarities to the real data, as shown in Fig. <a href="#S3.F1" title="Figure 1 ‣ 3 Data Synthesis via Domain Randomization ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
<span id="S4.SS1.SSS1.p1.1.8" class="ltx_text ltx_font_bold">(4)</span> The fixed-dataset training displays <em id="S4.SS1.SSS1.p1.1.9" class="ltx_emph ltx_font_italic">overfitting phenomenons</em> whilst the data-unrepeatable training <em id="S4.SS1.SSS1.p1.1.10" class="ltx_emph ltx_font_italic">does not</em> (cf. (a-d) in Fig. A1), since the former samples training instances from an empirical distribution with high bias and low variance, and thus cannot perfectly generalize to the unseen test instances sampled from the true distribution. <span id="S4.SS1.SSS1.p1.1.11" class="ltx_text ltx_font_bold">(5)</span> With strong data augmentation, the data-unrepeatable training has the test results on IID w/o BG data not only at their best but also better than those on IID data, implying that the trained models <em id="S4.SS1.SSS1.p1.1.12" class="ltx_emph ltx_font_italic">do not learn shortcut solutions that rely on context clues in the background</em>.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.3.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T1.4.2" class="ltx_text" style="font-size:90%;">Training on a fixed dataset vs. non-repetitive samples. FD: Fixed Dataset, True (T) or False (F). DA: Data Augmentation, None (N), Weak (W), or Strong (S). BG: BackGround.
</span></figcaption>
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:346.9pt;height:628.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(9.4pt,-17.1pt) scale(1.05756749811646,1.05756749811646) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.2.1.1.1" class="ltx_text">Data</span></th>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.2.1.2.1" class="ltx_text">FD</span></td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt" rowspan="2"><span id="S4.T1.1.1.2.1.3.1" class="ltx_text">DA</span></td>
<td id="S4.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt">IID</td>
<td id="S4.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt">IID w/o BG</td>
<td id="S4.T1.1.1.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">OOD</td>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<td id="S4.T1.1.1.3.2.1" class="ltx_td ltx_align_center">Acc./Mean</td>
<td id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_center">Acc./Mean</td>
<td id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_center">Acc.</td>
<td id="S4.T1.1.1.3.2.4" class="ltx_td ltx_align_center">Mean</td>
</tr>
<tr id="S4.T1.1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7"><span id="S4.T1.1.1.4.3.1.1" class="ltx_text ltx_font_bold">Backbone: ResNet-50 (23.53M)</span></th>
</tr>
<tr id="S4.T1.1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.5.4.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.5.4.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.5.4.4" class="ltx_td ltx_align_center">11.25</td>
<td id="S4.T1.1.1.5.4.5" class="ltx_td ltx_align_center">11.72</td>
<td id="S4.T1.1.1.5.4.6" class="ltx_td ltx_align_center">22.02</td>
<td id="S4.T1.1.1.5.4.7" class="ltx_td ltx_align_center">14.71</td>
</tr>
<tr id="S4.T1.1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.6.5.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.6.5.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.6.5.4" class="ltx_td ltx_align_center">87.63</td>
<td id="S4.T1.1.1.6.5.5" class="ltx_td ltx_align_center">78.55</td>
<td id="S4.T1.1.1.6.5.6" class="ltx_td ltx_align_center">23.35</td>
<td id="S4.T1.1.1.6.5.7" class="ltx_td ltx_align_center">23.36</td>
</tr>
<tr id="S4.T1.1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.7.6.2" class="ltx_td ltx_align_center">F</td>
<td id="S4.T1.1.1.7.6.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.7.6.4.1" class="ltx_text ltx_font_bold">98.19</span></td>
<td id="S4.T1.1.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.7.6.5.1" class="ltx_text ltx_font_bold">96.39</span></td>
<td id="S4.T1.1.1.7.6.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.7.6.6.1" class="ltx_text ltx_font_bold">25.04</span></td>
<td id="S4.T1.1.1.7.6.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.7.6.7.1" class="ltx_text ltx_font_bold">26.05</span></td>
</tr>
<tr id="S4.T1.1.1.8.7" class="ltx_tr">
<th id="S4.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_t">T</td>
<td id="S4.T1.1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_t">W</td>
<td id="S4.T1.1.1.8.7.4" class="ltx_td ltx_align_center ltx_border_t">12.31</td>
<td id="S4.T1.1.1.8.7.5" class="ltx_td ltx_align_center ltx_border_t">13.53</td>
<td id="S4.T1.1.1.8.7.6" class="ltx_td ltx_align_center">25.95</td>
<td id="S4.T1.1.1.8.7.7" class="ltx_td ltx_align_center">16.83</td>
</tr>
<tr id="S4.T1.1.1.9.8" class="ltx_tr">
<th id="S4.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.9.8.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.9.8.3" class="ltx_td ltx_align_center">W</td>
<td id="S4.T1.1.1.9.8.4" class="ltx_td ltx_align_center">95.54</td>
<td id="S4.T1.1.1.9.8.5" class="ltx_td ltx_align_center">91.37</td>
<td id="S4.T1.1.1.9.8.6" class="ltx_td ltx_align_center">23.97</td>
<td id="S4.T1.1.1.9.8.7" class="ltx_td ltx_align_center">22.89</td>
</tr>
<tr id="S4.T1.1.1.10.9" class="ltx_tr">
<th id="S4.T1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.10.9.2" class="ltx_td ltx_align_center">F</td>
<td id="S4.T1.1.1.10.9.3" class="ltx_td ltx_align_center">W</td>
<td id="S4.T1.1.1.10.9.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.10.9.4.1" class="ltx_text ltx_font_bold">98.10</span></td>
<td id="S4.T1.1.1.10.9.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.10.9.5.1" class="ltx_text ltx_font_bold">96.35</span></td>
<td id="S4.T1.1.1.10.9.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.10.9.6.1" class="ltx_text ltx_font_bold">27.47</span></td>
<td id="S4.T1.1.1.10.9.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.10.9.7.1" class="ltx_text ltx_font_bold">27.49</span></td>
</tr>
<tr id="S4.T1.1.1.11.10" class="ltx_tr">
<th id="S4.T1.1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.11.10.2" class="ltx_td ltx_align_center ltx_border_t">T</td>
<td id="S4.T1.1.1.11.10.3" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S4.T1.1.1.11.10.4" class="ltx_td ltx_align_center ltx_border_t">17.39</td>
<td id="S4.T1.1.1.11.10.5" class="ltx_td ltx_align_center ltx_border_t">20.32</td>
<td id="S4.T1.1.1.11.10.6" class="ltx_td ltx_align_center">33.07</td>
<td id="S4.T1.1.1.11.10.7" class="ltx_td ltx_align_center">27.48</td>
</tr>
<tr id="S4.T1.1.1.12.11" class="ltx_tr">
<th id="S4.T1.1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.12.11.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.12.11.3" class="ltx_td ltx_align_center">S</td>
<td id="S4.T1.1.1.12.11.4" class="ltx_td ltx_align_center">94.86</td>
<td id="S4.T1.1.1.12.11.5" class="ltx_td ltx_align_center">95.33</td>
<td id="S4.T1.1.1.12.11.6" class="ltx_td ltx_align_center">42.24</td>
<td id="S4.T1.1.1.12.11.7" class="ltx_td ltx_align_center">41.73</td>
</tr>
<tr id="S4.T1.1.1.13.12" class="ltx_tr">
<th id="S4.T1.1.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.13.12.2" class="ltx_td ltx_align_center">F</td>
<td id="S4.T1.1.1.13.12.3" class="ltx_td ltx_align_center">S</td>
<td id="S4.T1.1.1.13.12.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.13.12.4.1" class="ltx_text ltx_font_bold">96.26</span></td>
<td id="S4.T1.1.1.13.12.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.13.12.5.1" class="ltx_text ltx_font_bold">96.50</span></td>
<td id="S4.T1.1.1.13.12.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.13.12.6.1" class="ltx_text ltx_font_bold">42.82</span></td>
<td id="S4.T1.1.1.13.12.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.13.12.7.1" class="ltx_text ltx_font_bold">42.25</span></td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7">
<span id="S4.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Backbone: ViT-B (85.78M)</span>  †: Training for <math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="600" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mn id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">600</mn><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><cn type="integer" id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">600</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">600</annotation></semantics></math>K iterations</th>
</tr>
<tr id="S4.T1.1.1.14.13" class="ltx_tr">
<th id="S4.T1.1.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.14.13.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.14.13.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.14.13.4" class="ltx_td ltx_align_center">12.68</td>
<td id="S4.T1.1.1.14.13.5" class="ltx_td ltx_align_center">11.30</td>
<td id="S4.T1.1.1.14.13.6" class="ltx_td ltx_align_center">24.28</td>
<td id="S4.T1.1.1.14.13.7" class="ltx_td ltx_align_center">17.81</td>
</tr>
<tr id="S4.T1.1.1.15.14" class="ltx_tr">
<th id="S4.T1.1.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.15.14.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.15.14.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.15.14.4" class="ltx_td ltx_align_center">68.51</td>
<td id="S4.T1.1.1.15.14.5" class="ltx_td ltx_align_center">61.50</td>
<td id="S4.T1.1.1.15.14.6" class="ltx_td ltx_align_center">26.65</td>
<td id="S4.T1.1.1.15.14.7" class="ltx_td ltx_align_center">24.13</td>
</tr>
<tr id="S4.T1.1.1.16.15" class="ltx_tr">
<th id="S4.T1.1.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours†</th>
<td id="S4.T1.1.1.16.15.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.16.15.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.16.15.4" class="ltx_td ltx_align_center">70.58</td>
<td id="S4.T1.1.1.16.15.5" class="ltx_td ltx_align_center">62.15</td>
<td id="S4.T1.1.1.16.15.6" class="ltx_td ltx_align_center">26.57</td>
<td id="S4.T1.1.1.16.15.7" class="ltx_td ltx_align_center">24.23</td>
</tr>
<tr id="S4.T1.1.1.17.16" class="ltx_tr">
<th id="S4.T1.1.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.17.16.2" class="ltx_td ltx_align_center">F</td>
<td id="S4.T1.1.1.17.16.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.17.16.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.17.16.4.1" class="ltx_text ltx_font_bold">76.34</span></td>
<td id="S4.T1.1.1.17.16.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.17.16.5.1" class="ltx_text ltx_font_bold">71.46</span></td>
<td id="S4.T1.1.1.17.16.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.17.16.6.1" class="ltx_text ltx_font_bold">30.10</span></td>
<td id="S4.T1.1.1.17.16.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.17.16.7.1" class="ltx_text ltx_font_bold">26.93</span></td>
</tr>
<tr id="S4.T1.1.1.18.17" class="ltx_tr">
<th id="S4.T1.1.1.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.18.17.2" class="ltx_td ltx_align_center ltx_border_t">T</td>
<td id="S4.T1.1.1.18.17.3" class="ltx_td ltx_align_center ltx_border_t">W</td>
<td id="S4.T1.1.1.18.17.4" class="ltx_td ltx_align_center ltx_border_t">11.77</td>
<td id="S4.T1.1.1.18.17.5" class="ltx_td ltx_align_center ltx_border_t">11.20</td>
<td id="S4.T1.1.1.18.17.6" class="ltx_td ltx_align_center">26.53</td>
<td id="S4.T1.1.1.18.17.7" class="ltx_td ltx_align_center">19.22</td>
</tr>
<tr id="S4.T1.1.1.19.18" class="ltx_tr">
<th id="S4.T1.1.1.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.19.18.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.19.18.3" class="ltx_td ltx_align_center">W</td>
<td id="S4.T1.1.1.19.18.4" class="ltx_td ltx_align_center">72.79</td>
<td id="S4.T1.1.1.19.18.5" class="ltx_td ltx_align_center">67.46</td>
<td id="S4.T1.1.1.19.18.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.19.18.6.1" class="ltx_text ltx_font_bold">30.04</span></td>
<td id="S4.T1.1.1.19.18.7" class="ltx_td ltx_align_center">26.45</td>
</tr>
<tr id="S4.T1.1.1.20.19" class="ltx_tr">
<th id="S4.T1.1.1.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.20.19.2" class="ltx_td ltx_align_center">F</td>
<td id="S4.T1.1.1.20.19.3" class="ltx_td ltx_align_center">W</td>
<td id="S4.T1.1.1.20.19.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.20.19.4.1" class="ltx_text ltx_font_bold">73.93</span></td>
<td id="S4.T1.1.1.20.19.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.20.19.5.1" class="ltx_text ltx_font_bold">68.59</span></td>
<td id="S4.T1.1.1.20.19.6" class="ltx_td ltx_align_center">29.92</td>
<td id="S4.T1.1.1.20.19.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.20.19.7.1" class="ltx_text ltx_font_bold">26.80</span></td>
</tr>
<tr id="S4.T1.1.1.21.20" class="ltx_tr">
<th id="S4.T1.1.1.21.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.21.20.2" class="ltx_td ltx_align_center ltx_border_t">T</td>
<td id="S4.T1.1.1.21.20.3" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S4.T1.1.1.21.20.4" class="ltx_td ltx_align_center ltx_border_t">14.45</td>
<td id="S4.T1.1.1.21.20.5" class="ltx_td ltx_align_center ltx_border_t">12.89</td>
<td id="S4.T1.1.1.21.20.6" class="ltx_td ltx_align_center">31.52</td>
<td id="S4.T1.1.1.21.20.7" class="ltx_td ltx_align_center">23.74</td>
</tr>
<tr id="S4.T1.1.1.22.21" class="ltx_tr">
<th id="S4.T1.1.1.22.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.22.21.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.22.21.3" class="ltx_td ltx_align_center">S</td>
<td id="S4.T1.1.1.22.21.4" class="ltx_td ltx_align_center">62.85</td>
<td id="S4.T1.1.1.22.21.5" class="ltx_td ltx_align_center">63.96</td>
<td id="S4.T1.1.1.22.21.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.22.21.6.1" class="ltx_text ltx_font_bold">31.79</span></td>
<td id="S4.T1.1.1.22.21.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.22.21.7.1" class="ltx_text ltx_font_bold">26.56</span></td>
</tr>
<tr id="S4.T1.1.1.23.22" class="ltx_tr">
<th id="S4.T1.1.1.23.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.23.22.2" class="ltx_td ltx_align_center">F</td>
<td id="S4.T1.1.1.23.22.3" class="ltx_td ltx_align_center">S</td>
<td id="S4.T1.1.1.23.22.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.23.22.4.1" class="ltx_text ltx_font_bold">64.26</span></td>
<td id="S4.T1.1.1.23.22.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.23.22.5.1" class="ltx_text ltx_font_bold">64.30</span></td>
<td id="S4.T1.1.1.23.22.6" class="ltx_td ltx_align_center">30.89</td>
<td id="S4.T1.1.1.23.22.7" class="ltx_td ltx_align_center">26.28</td>
</tr>
<tr id="S4.T1.1.1.24.23" class="ltx_tr">
<th id="S4.T1.1.1.24.23.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="7"><span id="S4.T1.1.1.24.23.1.1" class="ltx_text ltx_font_bold">Backbone: Mixer-B (59.12M)</span></th>
</tr>
<tr id="S4.T1.1.1.25.24" class="ltx_tr">
<th id="S4.T1.1.1.25.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.25.24.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.25.24.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.25.24.4" class="ltx_td ltx_align_center">12.85</td>
<td id="S4.T1.1.1.25.24.5" class="ltx_td ltx_align_center">15.17</td>
<td id="S4.T1.1.1.25.24.6" class="ltx_td ltx_align_center">21.56</td>
<td id="S4.T1.1.1.25.24.7" class="ltx_td ltx_align_center">17.02</td>
</tr>
<tr id="S4.T1.1.1.26.25" class="ltx_tr">
<th id="S4.T1.1.1.26.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.26.25.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.26.25.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.26.25.4" class="ltx_td ltx_align_center">66.05</td>
<td id="S4.T1.1.1.26.25.5" class="ltx_td ltx_align_center">57.66</td>
<td id="S4.T1.1.1.26.25.6" class="ltx_td ltx_align_center">21.85</td>
<td id="S4.T1.1.1.26.25.7" class="ltx_td ltx_align_center">21.22</td>
</tr>
<tr id="S4.T1.1.1.27.26" class="ltx_tr">
<th id="S4.T1.1.1.27.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.27.26.2" class="ltx_td ltx_align_center">F</td>
<td id="S4.T1.1.1.27.26.3" class="ltx_td ltx_align_center">N</td>
<td id="S4.T1.1.1.27.26.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.27.26.4.1" class="ltx_text ltx_font_bold">90.22</span></td>
<td id="S4.T1.1.1.27.26.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.27.26.5.1" class="ltx_text ltx_font_bold">85.86</span></td>
<td id="S4.T1.1.1.27.26.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.27.26.6.1" class="ltx_text ltx_font_bold">28.54</span></td>
<td id="S4.T1.1.1.27.26.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.27.26.7.1" class="ltx_text ltx_font_bold">27.98</span></td>
</tr>
<tr id="S4.T1.1.1.28.27" class="ltx_tr">
<th id="S4.T1.1.1.28.27.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.28.27.2" class="ltx_td ltx_align_center ltx_border_t">T</td>
<td id="S4.T1.1.1.28.27.3" class="ltx_td ltx_align_center ltx_border_t">W</td>
<td id="S4.T1.1.1.28.27.4" class="ltx_td ltx_align_center ltx_border_t">13.99</td>
<td id="S4.T1.1.1.28.27.5" class="ltx_td ltx_align_center ltx_border_t">23.12</td>
<td id="S4.T1.1.1.28.27.6" class="ltx_td ltx_align_center">27.67</td>
<td id="S4.T1.1.1.28.27.7" class="ltx_td ltx_align_center">19.86</td>
</tr>
<tr id="S4.T1.1.1.29.28" class="ltx_tr">
<th id="S4.T1.1.1.29.28.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.29.28.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.29.28.3" class="ltx_td ltx_align_center">W</td>
<td id="S4.T1.1.1.29.28.4" class="ltx_td ltx_align_center">78.43</td>
<td id="S4.T1.1.1.29.28.5" class="ltx_td ltx_align_center">71.48</td>
<td id="S4.T1.1.1.29.28.6" class="ltx_td ltx_align_center">27.15</td>
<td id="S4.T1.1.1.29.28.7" class="ltx_td ltx_align_center">26.01</td>
</tr>
<tr id="S4.T1.1.1.30.29" class="ltx_tr">
<th id="S4.T1.1.1.30.29.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.30.29.2" class="ltx_td ltx_align_center">F</td>
<td id="S4.T1.1.1.30.29.3" class="ltx_td ltx_align_center">W</td>
<td id="S4.T1.1.1.30.29.4" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.30.29.4.1" class="ltx_text ltx_font_bold">90.32</span></td>
<td id="S4.T1.1.1.30.29.5" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.30.29.5.1" class="ltx_text ltx_font_bold">86.13</span></td>
<td id="S4.T1.1.1.30.29.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.30.29.6.1" class="ltx_text ltx_font_bold">29.11</span></td>
<td id="S4.T1.1.1.30.29.7" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.30.29.7.1" class="ltx_text ltx_font_bold">29.49</span></td>
</tr>
<tr id="S4.T1.1.1.31.30" class="ltx_tr">
<th id="S4.T1.1.1.31.30.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SubVisDA-10</th>
<td id="S4.T1.1.1.31.30.2" class="ltx_td ltx_align_center ltx_border_t">T</td>
<td id="S4.T1.1.1.31.30.3" class="ltx_td ltx_align_center ltx_border_t">S</td>
<td id="S4.T1.1.1.31.30.4" class="ltx_td ltx_align_center ltx_border_t">14.88</td>
<td id="S4.T1.1.1.31.30.5" class="ltx_td ltx_align_center ltx_border_t">24.85</td>
<td id="S4.T1.1.1.31.30.6" class="ltx_td ltx_align_center">33.19</td>
<td id="S4.T1.1.1.31.30.7" class="ltx_td ltx_align_center">26.12</td>
</tr>
<tr id="S4.T1.1.1.32.31" class="ltx_tr">
<th id="S4.T1.1.1.32.31.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<td id="S4.T1.1.1.32.31.2" class="ltx_td ltx_align_center">T</td>
<td id="S4.T1.1.1.32.31.3" class="ltx_td ltx_align_center">S</td>
<td id="S4.T1.1.1.32.31.4" class="ltx_td ltx_align_center">81.72</td>
<td id="S4.T1.1.1.32.31.5" class="ltx_td ltx_align_center">83.06</td>
<td id="S4.T1.1.1.32.31.6" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.32.31.6.1" class="ltx_text ltx_font_bold">36.57</span></td>
<td id="S4.T1.1.1.32.31.7" class="ltx_td ltx_align_center">33.43</td>
</tr>
<tr id="S4.T1.1.1.33.32" class="ltx_tr">
<th id="S4.T1.1.1.33.32.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">Ours</th>
<td id="S4.T1.1.1.33.32.2" class="ltx_td ltx_align_center ltx_border_bb">F</td>
<td id="S4.T1.1.1.33.32.3" class="ltx_td ltx_align_center ltx_border_bb">S</td>
<td id="S4.T1.1.1.33.32.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.33.32.4.1" class="ltx_text ltx_font_bold">84.16</span></td>
<td id="S4.T1.1.1.33.32.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.33.32.5.1" class="ltx_text ltx_font_bold">85.25</span></td>
<td id="S4.T1.1.1.33.32.6" class="ltx_td ltx_align_center ltx_border_bb">36.50</td>
<td id="S4.T1.1.1.33.32.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.33.32.7.1" class="ltx_text ltx_font_bold">33.75</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F2.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:119.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-n_res50_syn_id.svg.png" id="S4.F2.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="156" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F2.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:119.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-w_res50_syn_id.svg.png" id="S4.F2.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="156" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F2.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:119.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-s_res50_syn_id.svg.png" id="S4.F2.sf3.1.g1" class="ltx_graphics ltx_img_landscape" width="156" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F2.sf4.1" class="ltx_block ltx_minipage ltx_align_top" style="width:139.1pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/test_acc1_real_3models_sda.svg.png" id="S4.F2.sf4.1.g1" class="ltx_graphics ltx_img_landscape" width="166" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.9.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S4.F2.10.2" class="ltx_text" style="font-size:90%;">Learning process. <span id="S4.F2.10.2.1" class="ltx_text ltx_font_bold">(a-c):</span> Training ResNet-50 on a fixed dataset (<span id="S4.F2.10.2.2" class="ltx_text" style="color:#0000FF;">blue</span>) or non-repetitive samples (<span id="S4.F2.10.2.3" class="ltx_text" style="color:#FF0000;">red</span>) for no, weak, and strong data augmentations. <span id="S4.F2.10.2.4" class="ltx_text ltx_font_bold">(d):</span> Training ResNet-50 (<span id="S4.F2.10.2.5" class="ltx_text" style="color:#FF0000;">red</span>), ViT-B (<span id="S4.F2.10.2.6" class="ltx_text" style="color:#00FF00;">green</span>), and Mixer-B (<span id="S4.F2.10.2.7" class="ltx_text" style="color:#0000FF;">blue</span>) on non-repetitive samples with strong data augmentation.</span></figcaption>
</figure>
<div id="S4.SS1.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p2.1" class="ltx_p"><span id="S4.SS1.SSS1.p2.1.1" class="ltx_text ltx_font_bold">Evaluating Various Network Architectures.</span> In addition to Table <a href="#S4.T1" title="Table 1 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we also show the learning process of various network architectures in Figs. <a href="#S4.F2.sf4" title="Figure 2(d) ‣ Figure 2 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(d)</span></a> and A6. We take the following interesting observations.
<span id="S4.SS1.SSS1.p2.1.2" class="ltx_text ltx_font_bold">(1)</span> On the fixed-dataset training and IID tests, ViT-B <em id="S4.SS1.SSS1.p2.1.3" class="ltx_emph ltx_font_italic">performs surprisingly poorly</em> whatever the data augmentation is, when compared with ResNet-50; even the triple number of training epochs does not work as well as expected (e.g., in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>).
<span id="S4.SS1.SSS1.p2.1.4" class="ltx_text ltx_font_bold">(2)</span> When training on non-repetitive images without strong data augmentation, ViT-B and Mixer-B perform better than ResNet-50 in OOD tests whereas they perform much worse with strong data augmentation. Maybe they are more suitable for handling data with a certain (or smaller) range of diversity. Namely, <em id="S4.SS1.SSS1.p2.1.5" class="ltx_emph ltx_font_italic">different network architectures have different advantages for different data augmentations</em>, suggesting that neural architecture search (NAS) should also consider the search for data augmentation.
<span id="S4.SS1.SSS1.p2.1.6" class="ltx_text ltx_font_bold">(3)</span> With strong data augmentation, ResNet-50 fits best and shows the best convergence, though it has a more volatile learning process for the OOD test (cf. Figs. <a href="#S4.F2.sf4" title="Figure 2(d) ‣ Figure 2 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(d)</span></a> and A6).
<span id="S4.SS1.SSS1.p2.1.7" class="ltx_text ltx_font_bold">(4)</span> ResNet-50 produces more accurate saliency map visualizations, where the attended regions are semantically related (cf. Figs. A2-A5).</p>
</div>
<figure id="S4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.fig1" class="ltx_figure ltx_figure_panel ltx_align_center">
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F3.fig2" class="ltx_figure ltx_figure_panel ltx_align_center">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F3.4.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S4.F3.5.2" class="ltx_text" style="font-size:90%;">Generalization accuracy w.r.t. model capacity (<span id="S4.F3.5.2.1" class="ltx_text ltx_font_bold">a/left</span>) or training data quantity (<span id="S4.F3.5.2.2" class="ltx_text ltx_font_bold">b/right</span>). </span></figcaption>
</figure>
<div id="S4.SS1.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p3.4" class="ltx_p"><span id="S4.SS1.SSS1.p3.4.1" class="ltx_text ltx_font_bold">Impact of Model Capacity.</span> When Mixer is used as the backbone for data-unrepeatable training with strong data augmentation, we do experiments by varying the number of layers in <math id="S4.SS1.SSS1.p3.1.m1.3" class="ltx_Math" alttext="[8,12,24]" display="inline"><semantics id="S4.SS1.SSS1.p3.1.m1.3a"><mrow id="S4.SS1.SSS1.p3.1.m1.3.4.2" xref="S4.SS1.SSS1.p3.1.m1.3.4.1.cmml"><mo stretchy="false" id="S4.SS1.SSS1.p3.1.m1.3.4.2.1" xref="S4.SS1.SSS1.p3.1.m1.3.4.1.cmml">[</mo><mn id="S4.SS1.SSS1.p3.1.m1.1.1" xref="S4.SS1.SSS1.p3.1.m1.1.1.cmml">8</mn><mo id="S4.SS1.SSS1.p3.1.m1.3.4.2.2" xref="S4.SS1.SSS1.p3.1.m1.3.4.1.cmml">,</mo><mn id="S4.SS1.SSS1.p3.1.m1.2.2" xref="S4.SS1.SSS1.p3.1.m1.2.2.cmml">12</mn><mo id="S4.SS1.SSS1.p3.1.m1.3.4.2.3" xref="S4.SS1.SSS1.p3.1.m1.3.4.1.cmml">,</mo><mn id="S4.SS1.SSS1.p3.1.m1.3.3" xref="S4.SS1.SSS1.p3.1.m1.3.3.cmml">24</mn><mo stretchy="false" id="S4.SS1.SSS1.p3.1.m1.3.4.2.4" xref="S4.SS1.SSS1.p3.1.m1.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.1.m1.3b"><list id="S4.SS1.SSS1.p3.1.m1.3.4.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.3.4.2"><cn type="integer" id="S4.SS1.SSS1.p3.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p3.1.m1.1.1">8</cn><cn type="integer" id="S4.SS1.SSS1.p3.1.m1.2.2.cmml" xref="S4.SS1.SSS1.p3.1.m1.2.2">12</cn><cn type="integer" id="S4.SS1.SSS1.p3.1.m1.3.3.cmml" xref="S4.SS1.SSS1.p3.1.m1.3.3">24</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.1.m1.3c">[8,12,24]</annotation></semantics></math>, i.e., Mixer-S (<math id="S4.SS1.SSS1.p3.2.m2.1" class="ltx_Math" alttext="18.02" display="inline"><semantics id="S4.SS1.SSS1.p3.2.m2.1a"><mn id="S4.SS1.SSS1.p3.2.m2.1.1" xref="S4.SS1.SSS1.p3.2.m2.1.1.cmml">18.02</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.2.m2.1b"><cn type="float" id="S4.SS1.SSS1.p3.2.m2.1.1.cmml" xref="S4.SS1.SSS1.p3.2.m2.1.1">18.02</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.2.m2.1c">18.02</annotation></semantics></math>M), Mixer-B (<math id="S4.SS1.SSS1.p3.3.m3.1" class="ltx_Math" alttext="59.12" display="inline"><semantics id="S4.SS1.SSS1.p3.3.m3.1a"><mn id="S4.SS1.SSS1.p3.3.m3.1.1" xref="S4.SS1.SSS1.p3.3.m3.1.1.cmml">59.12</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.3.m3.1b"><cn type="float" id="S4.SS1.SSS1.p3.3.m3.1.1.cmml" xref="S4.SS1.SSS1.p3.3.m3.1.1">59.12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.3.m3.1c">59.12</annotation></semantics></math>M), and Mixer-L (<math id="S4.SS1.SSS1.p3.4.m4.1" class="ltx_Math" alttext="207.18" display="inline"><semantics id="S4.SS1.SSS1.p3.4.m4.1a"><mn id="S4.SS1.SSS1.p3.4.m4.1.1" xref="S4.SS1.SSS1.p3.4.m4.1.1.cmml">207.18</mn><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p3.4.m4.1b"><cn type="float" id="S4.SS1.SSS1.p3.4.m4.1.1.cmml" xref="S4.SS1.SSS1.p3.4.m4.1.1">207.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p3.4.m4.1c">207.18</annotation></semantics></math>M). The results are shown in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS1.SSS1.p3.4.2" class="ltx_text" style="color:#FF0000;">a</span>. Although better results are achieved by higher capacity models, the performance gain is <em id="S4.SS1.SSS1.p3.4.3" class="ltx_emph ltx_font_italic">less and less significant</em>. It also suggests that given a specific task, one model always has a bottleneck, which may be broken by adjusting the training data and learning algorithm.</p>
</div>
<div id="S4.SS1.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p4.1" class="ltx_p"><span id="S4.SS1.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Impact of Training Data Quantity.</span> We do experiments by doubling the number of training samples and the last 12.8M is the upper bound. We use the fixed-dataset periodic training with ResNet-50 and no data augmentation, and show the results in Fig. <a href="#S4.F3" title="Figure 3 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS1.SSS1.p4.1.2" class="ltx_text" style="color:#FF0000;">b</span>.
<span id="S4.SS1.SSS1.p4.1.3" class="ltx_text ltx_font_bold">(1)</span> In IID tests, as the number of training samples is continuously doubled, the performance gets better and is near perfect finally. <span id="S4.SS1.SSS1.p4.1.4" class="ltx_text ltx_font_bold">(2)</span> In real OOD tests, the performance gain is slighter and along the last 4 numbers, the performance gain almost disappears under our data regime. It demonstrates that simply generating more synthesized images may <em id="S4.SS1.SSS1.p4.1.5" class="ltx_emph ltx_font_italic">get stuck</em> at last and one can resort to domain adaptation approaches to reduce the distribution shift or more realistic simulation for image synthesis.</p>
</div>
<div id="S4.SS1.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS1.SSS1.p5.1" class="ltx_p"><span id="S4.SS1.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Impact of Data Augmentations.</span>
Data augmentation plays an important role in deep learning and we separately analyze its impact. There are a few noteworthy findings in Table <a href="#S4.T1" title="Table 1 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
<span id="S4.SS1.SSS1.p5.1.2" class="ltx_text ltx_font_bold">(1)</span> For training ResNet-50 on a fixed dataset, weak augmentation can enable the learnability from our synthesized data to IID data (e.g., <math id="S4.SS1.SSS1.p5.1.m1.1" class="ltx_Math" alttext="&gt;95\%" display="inline"><semantics id="S4.SS1.SSS1.p5.1.m1.1a"><mrow id="S4.SS1.SSS1.p5.1.m1.1.1" xref="S4.SS1.SSS1.p5.1.m1.1.1.cmml"><mi id="S4.SS1.SSS1.p5.1.m1.1.1.2" xref="S4.SS1.SSS1.p5.1.m1.1.1.2.cmml"></mi><mo id="S4.SS1.SSS1.p5.1.m1.1.1.1" xref="S4.SS1.SSS1.p5.1.m1.1.1.1.cmml">&gt;</mo><mrow id="S4.SS1.SSS1.p5.1.m1.1.1.3" xref="S4.SS1.SSS1.p5.1.m1.1.1.3.cmml"><mn id="S4.SS1.SSS1.p5.1.m1.1.1.3.2" xref="S4.SS1.SSS1.p5.1.m1.1.1.3.2.cmml">95</mn><mo id="S4.SS1.SSS1.p5.1.m1.1.1.3.1" xref="S4.SS1.SSS1.p5.1.m1.1.1.3.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p5.1.m1.1b"><apply id="S4.SS1.SSS1.p5.1.m1.1.1.cmml" xref="S4.SS1.SSS1.p5.1.m1.1.1"><gt id="S4.SS1.SSS1.p5.1.m1.1.1.1.cmml" xref="S4.SS1.SSS1.p5.1.m1.1.1.1"></gt><csymbol cd="latexml" id="S4.SS1.SSS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.SSS1.p5.1.m1.1.1.2">absent</csymbol><apply id="S4.SS1.SSS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.SSS1.p5.1.m1.1.1.3"><csymbol cd="latexml" id="S4.SS1.SSS1.p5.1.m1.1.1.3.1.cmml" xref="S4.SS1.SSS1.p5.1.m1.1.1.3.1">percent</csymbol><cn type="integer" id="S4.SS1.SSS1.p5.1.m1.1.1.3.2.cmml" xref="S4.SS1.SSS1.p5.1.m1.1.1.3.2">95</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p5.1.m1.1c">&gt;95\%</annotation></semantics></math>); from synthetic to IID w/o BG, strong augmentation is necessary; from synthetic to OOD, strong augmentation has the highest learnability. These observations enlighten us: <em id="S4.SS1.SSS1.p5.1.3" class="ltx_emph ltx_font_italic">given a limited set of training data, is there necessarily some kind of data augmentation that makes it learnable from training to test?</em>
<span id="S4.SS1.SSS1.p5.1.4" class="ltx_text ltx_font_bold">(2)</span> For the data-unrepeatable training, the results in IID tests <em id="S4.SS1.SSS1.p5.1.5" class="ltx_emph ltx_font_italic">get worse</em> while those in OOD tests <em id="S4.SS1.SSS1.p5.1.6" class="ltx_emph ltx_font_italic">get better</em> when strengthening the data augmentation, since the distribution of strongly augmented training data <em id="S4.SS1.SSS1.p5.1.7" class="ltx_emph ltx_font_italic">differs</em> from that of IID test data but is <em id="S4.SS1.SSS1.p5.1.8" class="ltx_emph ltx_font_italic">more similar</em> to that of OOD test data.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.3.2" class="ltx_text" style="font-size:90%;">Fix vs. randomize image variation factors (ResNet-50).</span></figcaption>
<div id="S4.T2.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:452.1pt;height:345.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(49.2pt,-37.6pt) scale(1.27843612656295,1.27843612656295) ;">
<table id="S4.T2.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.4.1.1.1" class="ltx_tr">
<th id="S4.T2.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="3"><span id="S4.T2.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Object Scale</span></th>
<th id="S4.T2.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" colspan="3"><span id="S4.T2.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Material Texture</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.4.1.2.1" class="ltx_tr">
<th id="S4.T2.4.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Value</th>
<td id="S4.T2.4.1.2.1.2" class="ltx_td ltx_align_center">IID</td>
<td id="S4.T2.4.1.2.1.3" class="ltx_td ltx_align_center">IID w/o BG</td>
<th id="S4.T2.4.1.2.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Value</th>
<td id="S4.T2.4.1.2.1.5" class="ltx_td ltx_align_center">IID</td>
<td id="S4.T2.4.1.2.1.6" class="ltx_td ltx_align_center">IID w/o BG</td>
</tr>
<tr id="S4.T2.4.1.3.2" class="ltx_tr">
<th id="S4.T2.4.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S4.T2.4.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">68.77</td>
<td id="S4.T2.4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">58.00</td>
<th id="S4.T2.4.1.3.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Metal</th>
<td id="S4.T2.4.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t">79.58</td>
<td id="S4.T2.4.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t">68.78</td>
</tr>
<tr id="S4.T2.4.1.4.3" class="ltx_tr">
<th id="S4.T2.4.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">1.5</th>
<td id="S4.T2.4.1.4.3.2" class="ltx_td ltx_align_center">80.80</td>
<td id="S4.T2.4.1.4.3.3" class="ltx_td ltx_align_center">72.22</td>
<th id="S4.T2.4.1.4.3.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Plastic</th>
<td id="S4.T2.4.1.4.3.5" class="ltx_td ltx_align_center">50.29</td>
<td id="S4.T2.4.1.4.3.6" class="ltx_td ltx_align_center">46.82</td>
</tr>
<tr id="S4.T2.4.1.5.4" class="ltx_tr">
<th id="S4.T2.4.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">2</th>
<td id="S4.T2.4.1.5.4.2" class="ltx_td ltx_align_center">77.61</td>
<td id="S4.T2.4.1.5.4.3" class="ltx_td ltx_align_center">70.10</td>
<th id="S4.T2.4.1.5.4.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Fingerprints</th>
<td id="S4.T2.4.1.5.4.5" class="ltx_td ltx_align_center">50.35</td>
<td id="S4.T2.4.1.5.4.6" class="ltx_td ltx_align_center">62.27</td>
</tr>
<tr id="S4.T2.4.1.6.5" class="ltx_tr">
<th id="S4.T2.4.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Mix</th>
<td id="S4.T2.4.1.6.5.2" class="ltx_td ltx_align_center">87.12</td>
<td id="S4.T2.4.1.6.5.3" class="ltx_td ltx_align_center">77.55</td>
<th id="S4.T2.4.1.6.5.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Moss</th>
<td id="S4.T2.4.1.6.5.5" class="ltx_td ltx_align_center">68.62</td>
<td id="S4.T2.4.1.6.5.6" class="ltx_td ltx_align_center">63.93</td>
</tr>
<tr id="S4.T2.4.1.7.6" class="ltx_tr">
<th id="S4.T2.4.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="S4.T2.4.1.7.6.1.1" class="ltx_text ltx_font_bold">Illumination</span></th>
<th id="S4.T2.4.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="S4.T2.4.1.7.6.2.1" class="ltx_text ltx_font_bold">Camera Viewpoint</span></th>
</tr>
<tr id="S4.T2.4.1.8.7" class="ltx_tr">
<th id="S4.T2.4.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Value</th>
<td id="S4.T2.4.1.8.7.2" class="ltx_td ltx_align_center">IID</td>
<td id="S4.T2.4.1.8.7.3" class="ltx_td ltx_align_center">IID w/o BG</td>
<th id="S4.T2.4.1.8.7.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Value</th>
<td id="S4.T2.4.1.8.7.5" class="ltx_td ltx_align_center">IID</td>
<td id="S4.T2.4.1.8.7.6" class="ltx_td ltx_align_center">IID w/o BG</td>
</tr>
<tr id="S4.T2.4.1.9.8" class="ltx_tr">
<th id="S4.T2.4.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Location 1</th>
<td id="S4.T2.4.1.9.8.2" class="ltx_td ltx_align_center ltx_border_t">86.48</td>
<td id="S4.T2.4.1.9.8.3" class="ltx_td ltx_align_center ltx_border_t">76.02</td>
<th id="S4.T2.4.1.9.8.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">Location 1</th>
<td id="S4.T2.4.1.9.8.5" class="ltx_td ltx_align_center ltx_border_t">24.60</td>
<td id="S4.T2.4.1.9.8.6" class="ltx_td ltx_align_center ltx_border_t">26.56</td>
</tr>
<tr id="S4.T2.4.1.10.9" class="ltx_tr">
<th id="S4.T2.4.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Location 2</th>
<td id="S4.T2.4.1.10.9.2" class="ltx_td ltx_align_center">86.60</td>
<td id="S4.T2.4.1.10.9.3" class="ltx_td ltx_align_center">76.75</td>
<th id="S4.T2.4.1.10.9.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Location 2</th>
<td id="S4.T2.4.1.10.9.5" class="ltx_td ltx_align_center">27.21</td>
<td id="S4.T2.4.1.10.9.6" class="ltx_td ltx_align_center">28.88</td>
</tr>
<tr id="S4.T2.4.1.11.10" class="ltx_tr">
<th id="S4.T2.4.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Radius</th>
<td id="S4.T2.4.1.11.10.2" class="ltx_td ltx_align_center">86.91</td>
<td id="S4.T2.4.1.11.10.3" class="ltx_td ltx_align_center">78.83</td>
<th id="S4.T2.4.1.11.10.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Location 3</th>
<td id="S4.T2.4.1.11.10.5" class="ltx_td ltx_align_center">32.82</td>
<td id="S4.T2.4.1.11.10.6" class="ltx_td ltx_align_center">32.76</td>
</tr>
<tr id="S4.T2.4.1.12.11" class="ltx_tr">
<th id="S4.T2.4.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Elevation</th>
<td id="S4.T2.4.1.12.11.2" class="ltx_td ltx_align_center">87.12</td>
<td id="S4.T2.4.1.12.11.3" class="ltx_td ltx_align_center">77.39</td>
<th id="S4.T2.4.1.12.11.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Location 4</th>
<td id="S4.T2.4.1.12.11.5" class="ltx_td ltx_align_center">33.79</td>
<td id="S4.T2.4.1.12.11.6" class="ltx_td ltx_align_center">33.07</td>
</tr>
<tr id="S4.T2.4.1.13.12" class="ltx_tr">
<th id="S4.T2.4.1.13.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="S4.T2.4.1.13.12.1.1" class="ltx_text ltx_font_bold">Background</span></th>
<th id="S4.T2.4.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="S4.T2.4.1.13.12.2.1" class="ltx_text ltx_font_bold">Full Randomization</span></th>
</tr>
<tr id="S4.T2.4.1.14.13" class="ltx_tr">
<th id="S4.T2.4.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Value</th>
<td id="S4.T2.4.1.14.13.2" class="ltx_td ltx_align_center">IID</td>
<td id="S4.T2.4.1.14.13.3" class="ltx_td ltx_align_center">IID w/o BG</td>
<th id="S4.T2.4.1.14.13.4" class="ltx_td ltx_align_left ltx_th ltx_th_row">Value</th>
<td id="S4.T2.4.1.14.13.5" class="ltx_td ltx_align_center">IID</td>
<td id="S4.T2.4.1.14.13.6" class="ltx_td ltx_align_center">IID w/o BG</td>
</tr>
<tr id="S4.T2.4.1.15.14" class="ltx_tr">
<th id="S4.T2.4.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">No Background</th>
<td id="S4.T2.4.1.15.14.2" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">17.68</td>
<td id="S4.T2.4.1.15.14.3" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.1.15.14.3.1" class="ltx_text ltx_font_bold">94.75</span></td>
<th id="S4.T2.4.1.15.14.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t">Random</th>
<td id="S4.T2.4.1.15.14.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="S4.T2.4.1.15.14.5.1" class="ltx_text ltx_font_bold">87.63</span></td>
<td id="S4.T2.4.1.15.14.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t">78.55</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.5.2.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S4.T3.2.1" class="ltx_text" style="font-size:90%;">Domain adaptation performance on SubVisDA-10 with varied pre-training schemes (ResNet-50). <math id="S4.T3.2.1.m1.1" class="ltx_Math" alttext="\bigstar" display="inline"><semantics id="S4.T3.2.1.m1.1b"><mi mathvariant="normal" id="S4.T3.2.1.m1.1.1" xref="S4.T3.2.1.m1.1.1.cmml">★</mi><annotation-xml encoding="MathML-Content" id="S4.T3.2.1.m1.1c"><ci id="S4.T3.2.1.m1.1.1.cmml" xref="S4.T3.2.1.m1.1.1">★</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.1.m1.1d">\bigstar</annotation></semantics></math>: Official checkpoint. Green or red: Best Acc. or Mean in each row (among compared DA methods). Ours w. SelfSup: Supervised pre-training + contrastive learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</span></figcaption>
<div id="S4.T3.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:329.5pt;height:189pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-149.9pt,85.7pt) scale(0.523715962589997,0.523715962589997) ;">
<table id="S4.T3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.2.1" class="ltx_tr">
<th id="S4.T3.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T3.3.1.2.1.1.1" class="ltx_text">Pre-training Data</span></th>
<th id="S4.T3.3.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T3.3.1.2.1.2.1" class="ltx_text"># Iters</span></th>
<th id="S4.T3.3.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T3.3.1.2.1.3.1" class="ltx_text"># Epochs</span></th>
<td id="S4.T3.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">No Adaptation</td>
<td id="S4.T3.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">DANN</td>
<td id="S4.T3.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">MCD</td>
<td id="S4.T3.3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">RCA</td>
<td id="S4.T3.3.1.2.1.8" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">SRDC</td>
<td id="S4.T3.3.1.2.1.9" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">DisClusterDA</td>
</tr>
<tr id="S4.T3.3.1.3.2" class="ltx_tr">
<td id="S4.T3.3.1.3.2.1" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.3.2.1.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T3.3.1.3.2.2" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T3.3.1.3.2.3" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.3.2.3.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T3.3.1.3.2.4" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T3.3.1.3.2.5" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.3.2.5.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T3.3.1.3.2.6" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T3.3.1.3.2.7" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.3.2.7.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T3.3.1.3.2.8" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T3.3.1.3.2.9" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.3.2.9.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T3.3.1.3.2.10" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T3.3.1.3.2.11" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.3.2.11.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T3.3.1.3.2.12" class="ltx_td ltx_align_center">Mean</td>
</tr>
<tr id="S4.T3.3.1.4.3" class="ltx_tr">
<th id="S4.T3.3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">No Pre-training</th>
<th id="S4.T3.3.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">-</th>
<th id="S4.T3.3.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">-</th>
<td id="S4.T3.3.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.4.3.4.1" class="ltx_text" style="background-color:#00FF00;">23.89</span></td>
<td id="S4.T3.3.1.4.3.5" class="ltx_td ltx_align_center ltx_border_t">14.21</td>
<td id="S4.T3.3.1.4.3.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.4.3.6.1" class="ltx_text" style="background-color:#C7C7C7;">22.30</span></td>
<td id="S4.T3.3.1.4.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.4.3.7.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">17.72</span></td>
<td id="S4.T3.3.1.4.3.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.4.3.8.1" class="ltx_text" style="background-color:#C7C7C7;">17.99</span></td>
<td id="S4.T3.3.1.4.3.9" class="ltx_td ltx_align_center ltx_border_t">16.20</td>
<td id="S4.T3.3.1.4.3.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.4.3.10.1" class="ltx_text" style="background-color:#C7C7C7;">19.15</span></td>
<td id="S4.T3.3.1.4.3.11" class="ltx_td ltx_align_center ltx_border_t">15.19</td>
<td id="S4.T3.3.1.4.3.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.4.3.12.1" class="ltx_text" style="background-color:#C7C7C7;">19.58</span></td>
<td id="S4.T3.3.1.4.3.13" class="ltx_td ltx_align_center ltx_border_t">15.92</td>
<td id="S4.T3.3.1.4.3.14" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.4.3.14.1" class="ltx_text" style="background-color:#C7C7C7;">20.87</span></td>
<td id="S4.T3.3.1.4.3.15" class="ltx_td ltx_align_center ltx_border_t">17.31</td>
</tr>
<tr id="S4.T3.3.1.5.4" class="ltx_tr">
<th id="S4.T3.3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours</th>
<th id="S4.T3.3.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">200K</th>
<th id="S4.T3.3.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">107</th>
<td id="S4.T3.3.1.5.4.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.5.4.4.1" class="ltx_text" style="background-color:#C7C7C7;">47.73</span></td>
<td id="S4.T3.3.1.5.4.5" class="ltx_td ltx_align_center">42.96</td>
<td id="S4.T3.3.1.5.4.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.5.4.6.1" class="ltx_text" style="background-color:#C7C7C7;">47.91</span></td>
<td id="S4.T3.3.1.5.4.7" class="ltx_td ltx_align_center">48.94</td>
<td id="S4.T3.3.1.5.4.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.5.4.8.1" class="ltx_text" style="background-color:#00FF00;">55.23</span></td>
<td id="S4.T3.3.1.5.4.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.5.4.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">56.86</span></td>
<td id="S4.T3.3.1.5.4.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.5.4.10.1" class="ltx_text" style="background-color:#C7C7C7;">54.27</span></td>
<td id="S4.T3.3.1.5.4.11" class="ltx_td ltx_align_center">52.72</td>
<td id="S4.T3.3.1.5.4.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.5.4.12.1" class="ltx_text" style="background-color:#C7C7C7;">44.70</span></td>
<td id="S4.T3.3.1.5.4.13" class="ltx_td ltx_align_center">47.45</td>
<td id="S4.T3.3.1.5.4.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.5.4.14.1" class="ltx_text" style="background-color:#C7C7C7;">54.09</span></td>
<td id="S4.T3.3.1.5.4.15" class="ltx_td ltx_align_center">54.91</td>
</tr>
<tr id="S4.T3.3.1.6.5" class="ltx_tr">
<th id="S4.T3.3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours w. SelfSup</th>
<th id="S4.T3.3.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">200K</th>
<th id="S4.T3.3.1.6.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">107</th>
<td id="S4.T3.3.1.6.5.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.6.5.4.1" class="ltx_text" style="background-color:#C7C7C7;">47.80</span></td>
<td id="S4.T3.3.1.6.5.5" class="ltx_td ltx_align_center">42.81</td>
<td id="S4.T3.3.1.6.5.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.6.5.6.1" class="ltx_text" style="background-color:#C7C7C7;">47.25</span></td>
<td id="S4.T3.3.1.6.5.7" class="ltx_td ltx_align_center">48.32</td>
<td id="S4.T3.3.1.6.5.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.6.5.8.1" class="ltx_text" style="background-color:#00FF00;">56.71</span></td>
<td id="S4.T3.3.1.6.5.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.6.5.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">58.33</span></td>
<td id="S4.T3.3.1.6.5.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.6.5.10.1" class="ltx_text" style="background-color:#C7C7C7;">53.44</span></td>
<td id="S4.T3.3.1.6.5.11" class="ltx_td ltx_align_center">53.31</td>
<td id="S4.T3.3.1.6.5.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.6.5.12.1" class="ltx_text" style="background-color:#C7C7C7;">40.21</span></td>
<td id="S4.T3.3.1.6.5.13" class="ltx_td ltx_align_center">40.50</td>
<td id="S4.T3.3.1.6.5.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.6.5.14.1" class="ltx_text" style="background-color:#C7C7C7;">54.37</span></td>
<td id="S4.T3.3.1.6.5.15" class="ltx_td ltx_align_center">54.15</td>
</tr>
<tr id="S4.T3.3.1.7.6" class="ltx_tr">
<th id="S4.T3.3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SynSL</th>
<th id="S4.T3.3.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">200K</th>
<th id="S4.T3.3.1.7.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">1</th>
<td id="S4.T3.3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.7.6.4.1" class="ltx_text" style="background-color:#C7C7C7;">47.50</span></td>
<td id="S4.T3.3.1.7.6.5" class="ltx_td ltx_align_center ltx_border_t">44.12</td>
<td id="S4.T3.3.1.7.6.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.7.6.6.1" class="ltx_text" style="background-color:#C7C7C7;">47.41</span></td>
<td id="S4.T3.3.1.7.6.7" class="ltx_td ltx_align_center ltx_border_t">49.48</td>
<td id="S4.T3.3.1.7.6.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.7.6.8.1" class="ltx_text" style="background-color:#00FF00;">55.06</span></td>
<td id="S4.T3.3.1.7.6.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.7.6.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">56.92</span></td>
<td id="S4.T3.3.1.7.6.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.7.6.10.1" class="ltx_text" style="background-color:#C7C7C7;">53.61</span></td>
<td id="S4.T3.3.1.7.6.11" class="ltx_td ltx_align_center ltx_border_t">53.50</td>
<td id="S4.T3.3.1.7.6.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.7.6.12.1" class="ltx_text" style="background-color:#C7C7C7;">36.30</span></td>
<td id="S4.T3.3.1.7.6.13" class="ltx_td ltx_align_center ltx_border_t">37.57</td>
<td id="S4.T3.3.1.7.6.14" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.7.6.14.1" class="ltx_text" style="background-color:#C7C7C7;">53.10</span></td>
<td id="S4.T3.3.1.7.6.15" class="ltx_td ltx_align_center ltx_border_t">54.88</td>
</tr>
<tr id="S4.T3.3.1.8.7" class="ltx_tr">
<th id="S4.T3.3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SynSL</th>
<th id="S4.T3.3.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1.2M</th>
<th id="S4.T3.3.1.8.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">6</th>
<td id="S4.T3.3.1.8.7.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.8.7.4.1" class="ltx_text" style="background-color:#C7C7C7;">51.22</span></td>
<td id="S4.T3.3.1.8.7.5" class="ltx_td ltx_align_center">48.57</td>
<td id="S4.T3.3.1.8.7.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.8.7.6.1" class="ltx_text" style="background-color:#C7C7C7;">55.90</span></td>
<td id="S4.T3.3.1.8.7.7" class="ltx_td ltx_align_center">56.50</td>
<td id="S4.T3.3.1.8.7.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.8.7.8.1" class="ltx_text" style="background-color:#00FF00;">64.52</span></td>
<td id="S4.T3.3.1.8.7.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.8.7.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">67.70</span></td>
<td id="S4.T3.3.1.8.7.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.8.7.10.1" class="ltx_text" style="background-color:#C7C7C7;">58.19</span></td>
<td id="S4.T3.3.1.8.7.11" class="ltx_td ltx_align_center">59.67</td>
<td id="S4.T3.3.1.8.7.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.8.7.12.1" class="ltx_text" style="background-color:#C7C7C7;">51.87</span></td>
<td id="S4.T3.3.1.8.7.13" class="ltx_td ltx_align_center">52.32</td>
<td id="S4.T3.3.1.8.7.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.8.7.14.1" class="ltx_text" style="background-color:#C7C7C7;">61.32</span></td>
<td id="S4.T3.3.1.8.7.15" class="ltx_td ltx_align_center">63.70</td>
</tr>
<tr id="S4.T3.3.1.9.8" class="ltx_tr">
<th id="S4.T3.3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SynSL</th>
<th id="S4.T3.3.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">2.4M</th>
<th id="S4.T3.3.1.9.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">12</th>
<td id="S4.T3.3.1.9.8.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.9.8.4.1" class="ltx_text" style="background-color:#C7C7C7;">53.47</span></td>
<td id="S4.T3.3.1.9.8.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.9.8.5.1" class="ltx_text ltx_font_bold">53.84</span></td>
<td id="S4.T3.3.1.9.8.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.9.8.6.1" class="ltx_text" style="background-color:#C7C7C7;">59.59</span></td>
<td id="S4.T3.3.1.9.8.7" class="ltx_td ltx_align_center">59.11</td>
<td id="S4.T3.3.1.9.8.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.9.8.8.1" class="ltx_text" style="background-color:#00FF00;">65.55</span></td>
<td id="S4.T3.3.1.9.8.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.9.8.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">68.83</span></td>
<td id="S4.T3.3.1.9.8.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.9.8.10.1" class="ltx_text" style="background-color:#C7C7C7;">60.47</span></td>
<td id="S4.T3.3.1.9.8.11" class="ltx_td ltx_align_center">61.19</td>
<td id="S4.T3.3.1.9.8.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.9.8.12.1" class="ltx_text" style="background-color:#C7C7C7;">55.17</span></td>
<td id="S4.T3.3.1.9.8.13" class="ltx_td ltx_align_center">58.10</td>
<td id="S4.T3.3.1.9.8.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.9.8.14.1" class="ltx_text" style="background-color:#C7C7C7;">63.62</span></td>
<td id="S4.T3.3.1.9.8.15" class="ltx_td ltx_align_center">64.89</td>
</tr>
<tr id="S4.T3.3.1.10.9" class="ltx_tr">
<th id="S4.T3.3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">SynSL</th>
<th id="S4.T3.3.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">4.8M</th>
<th id="S4.T3.3.1.10.9.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">24</th>
<td id="S4.T3.3.1.10.9.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.10.9.4.1" class="ltx_text" style="background-color:#C7C7C7;">55.02</span></td>
<td id="S4.T3.3.1.10.9.5" class="ltx_td ltx_align_center">53.72</td>
<td id="S4.T3.3.1.10.9.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.10.9.6.1" class="ltx_text" style="background-color:#C7C7C7;">60.55</span></td>
<td id="S4.T3.3.1.10.9.7" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.10.9.7.1" class="ltx_text ltx_font_bold">60.78</span></td>
<td id="S4.T3.3.1.10.9.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.10.9.8.1" class="ltx_text ltx_font_bold" style="background-color:#00FF00;">65.69</span></td>
<td id="S4.T3.3.1.10.9.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.10.9.9.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="border-color: #FF0000;">69.53</span></td>
<td id="S4.T3.3.1.10.9.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.10.9.10.1" class="ltx_text" style="background-color:#C7C7C7;">60.33</span></td>
<td id="S4.T3.3.1.10.9.11" class="ltx_td ltx_align_center">60.05</td>
<td id="S4.T3.3.1.10.9.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.10.9.12.1" class="ltx_text" style="background-color:#C7C7C7;">55.80</span></td>
<td id="S4.T3.3.1.10.9.13" class="ltx_td ltx_align_center">58.36</td>
<td id="S4.T3.3.1.10.9.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.10.9.14.1" class="ltx_text" style="background-color:#C7C7C7;">64.01</span></td>
<td id="S4.T3.3.1.10.9.15" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.10.9.15.1" class="ltx_text ltx_font_bold">65.36</span></td>
</tr>
<tr id="S4.T3.3.1.11.10" class="ltx_tr">
<th id="S4.T3.3.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">SubImageNet</th>
<th id="S4.T3.3.1.11.10.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">200K</th>
<th id="S4.T3.3.1.11.10.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">499</th>
<td id="S4.T3.3.1.11.10.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.11.10.4.1" class="ltx_text" style="background-color:#C7C7C7;">42.74</span></td>
<td id="S4.T3.3.1.11.10.5" class="ltx_td ltx_align_center ltx_border_t">37.16</td>
<td id="S4.T3.3.1.11.10.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.11.10.6.1" class="ltx_text" style="background-color:#C7C7C7;">49.64</span></td>
<td id="S4.T3.3.1.11.10.7" class="ltx_td ltx_align_center ltx_border_t">45.43</td>
<td id="S4.T3.3.1.11.10.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.11.10.8.1" class="ltx_text" style="background-color:#C7C7C7;">54.88</span></td>
<td id="S4.T3.3.1.11.10.9" class="ltx_td ltx_align_center ltx_border_t">52.12</td>
<td id="S4.T3.3.1.11.10.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.11.10.10.1" class="ltx_text" style="background-color:#00FF00;">58.24</span></td>
<td id="S4.T3.3.1.11.10.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.11.10.11.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">55.45</span></td>
<td id="S4.T3.3.1.11.10.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.11.10.12.1" class="ltx_text" style="background-color:#C7C7C7;">56.78</span></td>
<td id="S4.T3.3.1.11.10.13" class="ltx_td ltx_align_center ltx_border_t">51.24</td>
<td id="S4.T3.3.1.11.10.14" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.11.10.14.1" class="ltx_text" style="background-color:#C7C7C7;">56.21</span></td>
<td id="S4.T3.3.1.11.10.15" class="ltx_td ltx_align_center ltx_border_t">51.09</td>
</tr>
<tr id="S4.T3.3.1.12.11" class="ltx_tr">
<th id="S4.T3.3.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">Ours+SubImageNet</th>
<th id="S4.T3.3.1.12.11.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">200K</th>
<th id="S4.T3.3.1.12.11.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">88</th>
<td id="S4.T3.3.1.12.11.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.12.11.4.1" class="ltx_text" style="background-color:#C7C7C7;">49.61</span></td>
<td id="S4.T3.3.1.12.11.5" class="ltx_td ltx_align_center">47.88</td>
<td id="S4.T3.3.1.12.11.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.12.11.6.1" class="ltx_text" style="background-color:#C7C7C7;">55.35</span></td>
<td id="S4.T3.3.1.12.11.7" class="ltx_td ltx_align_center">56.22</td>
<td id="S4.T3.3.1.12.11.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.12.11.8.1" class="ltx_text" style="background-color:#C7C7C7;">60.90</span></td>
<td id="S4.T3.3.1.12.11.9" class="ltx_td ltx_align_center">62.16</td>
<td id="S4.T3.3.1.12.11.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.12.11.10.1" class="ltx_text" style="background-color:#C7C7C7;">61.11</span></td>
<td id="S4.T3.3.1.12.11.11" class="ltx_td ltx_align_center">60.31</td>
<td id="S4.T3.3.1.12.11.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.12.11.12.1" class="ltx_text" style="background-color:#C7C7C7;">60.07</span></td>
<td id="S4.T3.3.1.12.11.13" class="ltx_td ltx_align_center">61.74</td>
<td id="S4.T3.3.1.12.11.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.12.11.14.1" class="ltx_text" style="background-color:#00FF00;">62.22</span></td>
<td id="S4.T3.3.1.12.11.15" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.12.11.15.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">62.47</span></td>
</tr>
<tr id="S4.T3.3.1.13.12" class="ltx_tr">
<th id="S4.T3.3.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ImageNet-990</th>
<th id="S4.T3.3.1.13.12.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">200K</th>
<th id="S4.T3.3.1.13.12.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">10</th>
<td id="S4.T3.3.1.13.12.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.13.12.4.1" class="ltx_text" style="background-color:#C7C7C7;">31.91</span></td>
<td id="S4.T3.3.1.13.12.5" class="ltx_td ltx_align_center">26.31</td>
<td id="S4.T3.3.1.13.12.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.13.12.6.1" class="ltx_text" style="background-color:#C7C7C7;">34.68</span></td>
<td id="S4.T3.3.1.13.12.7" class="ltx_td ltx_align_center">32.29</td>
<td id="S4.T3.3.1.13.12.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.13.12.8.1" class="ltx_text" style="background-color:#C7C7C7;">39.48</span></td>
<td id="S4.T3.3.1.13.12.9" class="ltx_td ltx_align_center">37.84</td>
<td id="S4.T3.3.1.13.12.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.13.12.10.1" class="ltx_text" style="background-color:#00FF00;">45.10</span></td>
<td id="S4.T3.3.1.13.12.11" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.13.12.11.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">43.10</span></td>
<td id="S4.T3.3.1.13.12.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.13.12.12.1" class="ltx_text" style="background-color:#C7C7C7;">43.69</span></td>
<td id="S4.T3.3.1.13.12.13" class="ltx_td ltx_align_center">40.95</td>
<td id="S4.T3.3.1.13.12.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.13.12.14.1" class="ltx_text" style="background-color:#C7C7C7;">41.56</span></td>
<td id="S4.T3.3.1.13.12.15" class="ltx_td ltx_align_center">39.40</td>
</tr>
<tr id="S4.T3.3.1.14.13" class="ltx_tr">
<th id="S4.T3.3.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ImageNet-990+Ours</th>
<th id="S4.T3.3.1.14.13.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">200K</th>
<th id="S4.T3.3.1.14.13.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">9</th>
<td id="S4.T3.3.1.14.13.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.14.13.4.1" class="ltx_text" style="background-color:#C7C7C7;">36.53</span></td>
<td id="S4.T3.3.1.14.13.5" class="ltx_td ltx_align_center">30.58</td>
<td id="S4.T3.3.1.14.13.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.14.13.6.1" class="ltx_text" style="background-color:#C7C7C7;">38.15</span></td>
<td id="S4.T3.3.1.14.13.7" class="ltx_td ltx_align_center">35.22</td>
<td id="S4.T3.3.1.14.13.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.14.13.8.1" class="ltx_text" style="background-color:#C7C7C7;">42.38</span></td>
<td id="S4.T3.3.1.14.13.9" class="ltx_td ltx_align_center">41.84</td>
<td id="S4.T3.3.1.14.13.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.14.13.10.1" class="ltx_text" style="background-color:#00FF00;">46.19</span></td>
<td id="S4.T3.3.1.14.13.11" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.14.13.11.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">43.45</span></td>
<td id="S4.T3.3.1.14.13.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.14.13.12.1" class="ltx_text" style="background-color:#C7C7C7;">45.87</span></td>
<td id="S4.T3.3.1.14.13.13" class="ltx_td ltx_align_center">42.95</td>
<td id="S4.T3.3.1.14.13.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.14.13.14.1" class="ltx_text" style="background-color:#C7C7C7;">42.07</span></td>
<td id="S4.T3.3.1.14.13.15" class="ltx_td ltx_align_center">39.40</td>
</tr>
<tr id="S4.T3.3.1.15.14" class="ltx_tr">
<th id="S4.T3.3.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">ImageNet</th>
<th id="S4.T3.3.1.15.14.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">200K</th>
<th id="S4.T3.3.1.15.14.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">10</th>
<td id="S4.T3.3.1.15.14.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.15.14.4.1" class="ltx_text" style="background-color:#C7C7C7;">40.37</span></td>
<td id="S4.T3.3.1.15.14.5" class="ltx_td ltx_align_center ltx_border_t">33.25</td>
<td id="S4.T3.3.1.15.14.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.15.14.6.1" class="ltx_text" style="background-color:#C7C7C7;">42.57</span></td>
<td id="S4.T3.3.1.15.14.7" class="ltx_td ltx_align_center ltx_border_t">40.22</td>
<td id="S4.T3.3.1.15.14.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.15.14.8.1" class="ltx_text" style="background-color:#C7C7C7;">49.04</span></td>
<td id="S4.T3.3.1.15.14.9" class="ltx_td ltx_align_center ltx_border_t">47.86</td>
<td id="S4.T3.3.1.15.14.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.15.14.10.1" class="ltx_text" style="background-color:#00FF00;">52.36</span></td>
<td id="S4.T3.3.1.15.14.11" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.15.14.11.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">47.90</span></td>
<td id="S4.T3.3.1.15.14.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.15.14.12.1" class="ltx_text" style="background-color:#C7C7C7;">51.62</span></td>
<td id="S4.T3.3.1.15.14.13" class="ltx_td ltx_align_center ltx_border_t">47.88</td>
<td id="S4.T3.3.1.15.14.14" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.15.14.14.1" class="ltx_text" style="background-color:#C7C7C7;">49.29</span></td>
<td id="S4.T3.3.1.15.14.15" class="ltx_td ltx_align_center ltx_border_t">46.17</td>
</tr>
<tr id="S4.T3.3.1.16.15" class="ltx_tr">
<th id="S4.T3.3.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ImageNet</th>
<th id="S4.T3.3.1.16.15.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1.2M</th>
<th id="S4.T3.3.1.16.15.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">60</th>
<td id="S4.T3.3.1.16.15.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.16.15.4.1" class="ltx_text" style="background-color:#C7C7C7;">54.69</span></td>
<td id="S4.T3.3.1.16.15.5" class="ltx_td ltx_align_center">51.27</td>
<td id="S4.T3.3.1.16.15.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.16.15.6.1" class="ltx_text" style="background-color:#C7C7C7;">58.50</span></td>
<td id="S4.T3.3.1.16.15.7" class="ltx_td ltx_align_center">56.02</td>
<td id="S4.T3.3.1.16.15.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.16.15.8.1" class="ltx_text" style="background-color:#00FF00;">65.28</span></td>
<td id="S4.T3.3.1.16.15.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.16.15.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">65.88</span></td>
<td id="S4.T3.3.1.16.15.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.16.15.10.1" class="ltx_text" style="background-color:#C7C7C7;">62.69</span></td>
<td id="S4.T3.3.1.16.15.11" class="ltx_td ltx_align_center">60.28</td>
<td id="S4.T3.3.1.16.15.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.16.15.12.1" class="ltx_text" style="background-color:#C7C7C7;">60.33</span></td>
<td id="S4.T3.3.1.16.15.13" class="ltx_td ltx_align_center">55.00</td>
<td id="S4.T3.3.1.16.15.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.16.15.14.1" class="ltx_text" style="background-color:#C7C7C7;">62.28</span></td>
<td id="S4.T3.3.1.16.15.15" class="ltx_td ltx_align_center">61.00</td>
</tr>
<tr id="S4.T3.3.1.17.16" class="ltx_tr">
<th id="S4.T3.3.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ImageNet</th>
<th id="S4.T3.3.1.17.16.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">2.4M</th>
<th id="S4.T3.3.1.17.16.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">120</th>
<td id="S4.T3.3.1.17.16.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.17.16.4.1" class="ltx_text" style="background-color:#C7C7C7;">53.84</span></td>
<td id="S4.T3.3.1.17.16.5" class="ltx_td ltx_align_center">47.55</td>
<td id="S4.T3.3.1.17.16.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.17.16.6.1" class="ltx_text" style="background-color:#C7C7C7;">58.45</span></td>
<td id="S4.T3.3.1.17.16.7" class="ltx_td ltx_align_center">55.38</td>
<td id="S4.T3.3.1.17.16.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.17.16.8.1" class="ltx_text" style="background-color:#00FF00;">65.27</span></td>
<td id="S4.T3.3.1.17.16.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.17.16.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">65.38</span></td>
<td id="S4.T3.3.1.17.16.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.17.16.10.1" class="ltx_text" style="background-color:#C7C7C7;">61.65</span></td>
<td id="S4.T3.3.1.17.16.11" class="ltx_td ltx_align_center">60.82</td>
<td id="S4.T3.3.1.17.16.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.17.16.12.1" class="ltx_text" style="background-color:#C7C7C7;">61.65</span></td>
<td id="S4.T3.3.1.17.16.13" class="ltx_td ltx_align_center">56.30</td>
<td id="S4.T3.3.1.17.16.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.17.16.14.1" class="ltx_text" style="background-color:#C7C7C7;">62.02</span></td>
<td id="S4.T3.3.1.17.16.15" class="ltx_td ltx_align_center">60.46</td>
</tr>
<tr id="S4.T3.3.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">ImageNet<sup id="S4.T3.3.1.1.1.1" class="ltx_sup">★</sup>
</th>
<th id="S4.T3.3.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">600K</th>
<th id="S4.T3.3.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">120</th>
<td id="S4.T3.3.1.1.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.1.4.1" class="ltx_text ltx_font_bold" style="background-color:#C7C7C7;">57.10</span></td>
<td id="S4.T3.3.1.1.5" class="ltx_td ltx_align_center">51.83</td>
<td id="S4.T3.3.1.1.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.1.6.1" class="ltx_text ltx_font_bold" style="background-color:#C7C7C7;">61.92</span></td>
<td id="S4.T3.3.1.1.7" class="ltx_td ltx_align_center">58.75</td>
<td id="S4.T3.3.1.1.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.1.8.1" class="ltx_text" style="background-color:#C7C7C7;">64.59</span></td>
<td id="S4.T3.3.1.1.9" class="ltx_td ltx_align_center">64.87</td>
<td id="S4.T3.3.1.1.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.1.10.1" class="ltx_text ltx_font_bold" style="background-color:#C7C7C7;">67.72</span></td>
<td id="S4.T3.3.1.1.11" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.1.11.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="border-color: #FF0000;">66.17</span></td>
<td id="S4.T3.3.1.1.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.1.12.1" class="ltx_text ltx_font_bold" style="background-color:#00FF00;">69.00</span></td>
<td id="S4.T3.3.1.1.13" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.1.13.1" class="ltx_text ltx_font_bold">64.92</span></td>
<td id="S4.T3.3.1.1.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.1.14.1" class="ltx_text ltx_font_bold" style="background-color:#C7C7C7;">68.35</span></td>
<td id="S4.T3.3.1.1.15" class="ltx_td ltx_align_center">64.65</td>
</tr>
<tr id="S4.T3.3.1.18.17" class="ltx_tr">
<th id="S4.T3.3.1.18.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t">MetaShift</th>
<th id="S4.T3.3.1.18.17.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">200K</th>
<th id="S4.T3.3.1.18.17.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">5</th>
<td id="S4.T3.3.1.18.17.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.18.17.4.1" class="ltx_text" style="background-color:#C7C7C7;">38.18</span></td>
<td id="S4.T3.3.1.18.17.5" class="ltx_td ltx_align_center ltx_border_t">30.31</td>
<td id="S4.T3.3.1.18.17.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.18.17.6.1" class="ltx_text" style="background-color:#C7C7C7;">38.29</span></td>
<td id="S4.T3.3.1.18.17.7" class="ltx_td ltx_align_center ltx_border_t">34.04</td>
<td id="S4.T3.3.1.18.17.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.18.17.8.1" class="ltx_text" style="background-color:#C7C7C7;">45.39</span></td>
<td id="S4.T3.3.1.18.17.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.18.17.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">43.63</span></td>
<td id="S4.T3.3.1.18.17.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.18.17.10.1" class="ltx_text" style="background-color:#00FF00;">45.93</span></td>
<td id="S4.T3.3.1.18.17.11" class="ltx_td ltx_align_center ltx_border_t">42.67</td>
<td id="S4.T3.3.1.18.17.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.18.17.12.1" class="ltx_text" style="background-color:#C7C7C7;">42.83</span></td>
<td id="S4.T3.3.1.18.17.13" class="ltx_td ltx_align_center ltx_border_t">38.02</td>
<td id="S4.T3.3.1.18.17.14" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.18.17.14.1" class="ltx_text" style="background-color:#C7C7C7;">40.17</span></td>
<td id="S4.T3.3.1.18.17.15" class="ltx_td ltx_align_center ltx_border_t">35.09</td>
</tr>
<tr id="S4.T3.3.1.19.18" class="ltx_tr">
<th id="S4.T3.3.1.19.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">MetaShift</th>
<th id="S4.T3.3.1.19.18.2" class="ltx_td ltx_align_center ltx_th ltx_th_row">1.2M</th>
<th id="S4.T3.3.1.19.18.3" class="ltx_td ltx_align_center ltx_th ltx_th_row">30</th>
<td id="S4.T3.3.1.19.18.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.19.18.4.1" class="ltx_text" style="background-color:#C7C7C7;">48.00</span></td>
<td id="S4.T3.3.1.19.18.5" class="ltx_td ltx_align_center">39.99</td>
<td id="S4.T3.3.1.19.18.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.19.18.6.1" class="ltx_text" style="background-color:#C7C7C7;">53.00</span></td>
<td id="S4.T3.3.1.19.18.7" class="ltx_td ltx_align_center">48.17</td>
<td id="S4.T3.3.1.19.18.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.19.18.8.1" class="ltx_text" style="background-color:#00FF00;">64.04</span></td>
<td id="S4.T3.3.1.19.18.9" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.19.18.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">61.30</span></td>
<td id="S4.T3.3.1.19.18.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.19.18.10.1" class="ltx_text" style="background-color:#C7C7C7;">53.97</span></td>
<td id="S4.T3.3.1.19.18.11" class="ltx_td ltx_align_center">51.09</td>
<td id="S4.T3.3.1.19.18.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.19.18.12.1" class="ltx_text" style="background-color:#C7C7C7;">48.69</span></td>
<td id="S4.T3.3.1.19.18.13" class="ltx_td ltx_align_center">44.49</td>
<td id="S4.T3.3.1.19.18.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.19.18.14.1" class="ltx_text" style="background-color:#C7C7C7;">60.28</span></td>
<td id="S4.T3.3.1.19.18.15" class="ltx_td ltx_align_center">57.15</td>
</tr>
<tr id="S4.T3.3.1.20.19" class="ltx_tr">
<th id="S4.T3.3.1.20.19.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">MetaShift</th>
<th id="S4.T3.3.1.20.19.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">2.4M</th>
<th id="S4.T3.3.1.20.19.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">60</th>
<td id="S4.T3.3.1.20.19.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.20.19.4.1" class="ltx_text" style="background-color:#C7C7C7;">47.24</span></td>
<td id="S4.T3.3.1.20.19.5" class="ltx_td ltx_align_center ltx_border_bb">39.21</td>
<td id="S4.T3.3.1.20.19.6" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.20.19.6.1" class="ltx_text" style="background-color:#C7C7C7;">58.41</span></td>
<td id="S4.T3.3.1.20.19.7" class="ltx_td ltx_align_center ltx_border_bb">53.85</td>
<td id="S4.T3.3.1.20.19.8" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.20.19.8.1" class="ltx_text" style="background-color:#C7C7C7;">61.10</span></td>
<td id="S4.T3.3.1.20.19.9" class="ltx_td ltx_align_center ltx_border_bb">58.52</td>
<td id="S4.T3.3.1.20.19.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.20.19.10.1" class="ltx_text" style="background-color:#C7C7C7;">58.64</span></td>
<td id="S4.T3.3.1.20.19.11" class="ltx_td ltx_align_center ltx_border_bb">55.35</td>
<td id="S4.T3.3.1.20.19.12" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.20.19.12.1" class="ltx_text" style="background-color:#C7C7C7;">51.71</span></td>
<td id="S4.T3.3.1.20.19.13" class="ltx_td ltx_align_center ltx_border_bb">47.29</td>
<td id="S4.T3.3.1.20.19.14" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T3.3.1.20.19.14.1" class="ltx_text" style="background-color:#00FF00;">62.71</span></td>
<td id="S4.T3.3.1.20.19.15" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T3.3.1.20.19.15.1" class="ltx_text ltx_framed ltx_framed_underline" style="border-color: #FF0000;">60.18</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Assessing Image Variation Factors</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">To know <em id="S4.SS2.p1.1.1" class="ltx_emph ltx_font_italic">how individual image variation factors in domain randomization affect the model generalization</em>, we do an ablation study by fixing one of them at a time. For each variant, we accordingly synthesize <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mn id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><cn type="integer" id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">120</annotation></semantics></math>K training images and train a ResNet-50 from scratch with no data augmentation. Other implementation details are the same as those in Sec. <a href="#S4.SS1" title="4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a>. We compare the one-fixed variants with the baseline of full randomization and report results in Table <a href="#S4.T2" title="Table 2 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS2.p2.6" class="ltx_p"><span id="S4.SS2.p2.6.1" class="ltx_text ltx_font_bold">Object Scale.</span> The scale of target object in an image is changed by modifying the distance between the object and camera in the virtual 3D scene, whose value is set as <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mn id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><cn type="integer" id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">1</annotation></semantics></math>, <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mn id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><cn type="float" id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">1.5</annotation></semantics></math>, <math id="S4.SS2.p2.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S4.SS2.p2.3.m3.1a"><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.1b"><cn type="integer" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.1c">2</annotation></semantics></math>, or a mix of the three. We can observe that when the object scale is fixed to one value, the recognition accuracy in IID and IID w/o BG tests drops significantly, e.g., by <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="18.86\%" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml"><mn id="S4.SS2.p2.4.m4.1.1.2" xref="S4.SS2.p2.4.m4.1.1.2.cmml">18.86</mn><mo id="S4.SS2.p2.4.m4.1.1.1" xref="S4.SS2.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS2.p2.4.m4.1.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.4.m4.1.1.2.cmml" xref="S4.SS2.p2.4.m4.1.1.2">18.86</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">18.86\%</annotation></semantics></math> and <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="20.55\%" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><mrow id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mn id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">20.55</mn><mo id="S4.SS2.p2.5.m5.1.1.1" xref="S4.SS2.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><csymbol cd="latexml" id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">20.55</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">20.55\%</annotation></semantics></math> respectively; setting the object-to-camera distance as <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="1.5" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mn id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">1.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><cn type="float" id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">1.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">1.5</annotation></semantics></math> achieves the best performance among the three scales; mixing the three scales restores most of the baseline performance. The observations show that <em id="S4.SS2.p2.6.2" class="ltx_emph ltx_font_italic">decreasing the sample diversity would damage the generalization performance; different scales have different importance.</em></p>
</div>
<div id="S4.SS2.p3" class="ltx_para ltx_noindent">
<p id="S4.SS2.p3.2" class="ltx_p"><span id="S4.SS2.p3.2.1" class="ltx_text ltx_font_bold">Material Texture.</span> We fix the material texture of target object in an image as metal, plastic, fingerprints, or moss. We observe that compared to full randomization, fixing the material texture degenerates the performance largely in IID and IID w/o BG tests, e.g., by <math id="S4.SS2.p3.1.m1.1" class="ltx_Math" alttext="37.34\%" display="inline"><semantics id="S4.SS2.p3.1.m1.1a"><mrow id="S4.SS2.p3.1.m1.1.1" xref="S4.SS2.p3.1.m1.1.1.cmml"><mn id="S4.SS2.p3.1.m1.1.1.2" xref="S4.SS2.p3.1.m1.1.1.2.cmml">37.34</mn><mo id="S4.SS2.p3.1.m1.1.1.1" xref="S4.SS2.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.1.m1.1b"><apply id="S4.SS2.p3.1.m1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p3.1.m1.1.1.1.cmml" xref="S4.SS2.p3.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.1.m1.1.1.2.cmml" xref="S4.SS2.p3.1.m1.1.1.2">37.34</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.1.m1.1c">37.34\%</annotation></semantics></math> and <math id="S4.SS2.p3.2.m2.1" class="ltx_Math" alttext="31.73\%" display="inline"><semantics id="S4.SS2.p3.2.m2.1a"><mrow id="S4.SS2.p3.2.m2.1.1" xref="S4.SS2.p3.2.m2.1.1.cmml"><mn id="S4.SS2.p3.2.m2.1.1.2" xref="S4.SS2.p3.2.m2.1.1.2.cmml">31.73</mn><mo id="S4.SS2.p3.2.m2.1.1.1" xref="S4.SS2.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p3.2.m2.1b"><apply id="S4.SS2.p3.2.m2.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p3.2.m2.1.1.1.cmml" xref="S4.SS2.p3.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p3.2.m2.1.1.2.cmml" xref="S4.SS2.p3.2.m2.1.1.2">31.73</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p3.2.m2.1c">31.73\%</annotation></semantics></math> respectively.</p>
</div>
<div id="S4.SS2.p4" class="ltx_para ltx_noindent">
<p id="S4.SS2.p4.10" class="ltx_p"><span id="S4.SS2.p4.10.1" class="ltx_text ltx_font_bold">Illumination.</span> We change the illumination by fixing the light location to [<math id="S4.SS2.p4.1.m1.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS2.p4.1.m1.1a"><mn id="S4.SS2.p4.1.m1.1.1" xref="S4.SS2.p4.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.1.m1.1b"><cn type="integer" id="S4.SS2.p4.1.m1.1.1.cmml" xref="S4.SS2.p4.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.1.m1.1c">5</annotation></semantics></math>, <math id="S4.SS2.p4.2.m2.1" class="ltx_Math" alttext="-5" display="inline"><semantics id="S4.SS2.p4.2.m2.1a"><mrow id="S4.SS2.p4.2.m2.1.1" xref="S4.SS2.p4.2.m2.1.1.cmml"><mo id="S4.SS2.p4.2.m2.1.1a" xref="S4.SS2.p4.2.m2.1.1.cmml">−</mo><mn id="S4.SS2.p4.2.m2.1.1.2" xref="S4.SS2.p4.2.m2.1.1.2.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.2.m2.1b"><apply id="S4.SS2.p4.2.m2.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"><minus id="S4.SS2.p4.2.m2.1.1.1.cmml" xref="S4.SS2.p4.2.m2.1.1"></minus><cn type="integer" id="S4.SS2.p4.2.m2.1.1.2.cmml" xref="S4.SS2.p4.2.m2.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.2.m2.1c">-5</annotation></semantics></math>, <math id="S4.SS2.p4.3.m3.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS2.p4.3.m3.1a"><mn id="S4.SS2.p4.3.m3.1.1" xref="S4.SS2.p4.3.m3.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.3.m3.1b"><cn type="integer" id="S4.SS2.p4.3.m3.1.1.cmml" xref="S4.SS2.p4.3.m3.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.3.m3.1c">5</annotation></semantics></math>] or [<math id="S4.SS2.p4.4.m4.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS2.p4.4.m4.1a"><mn id="S4.SS2.p4.4.m4.1.1" xref="S4.SS2.p4.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.4.m4.1b"><cn type="integer" id="S4.SS2.p4.4.m4.1.1.cmml" xref="S4.SS2.p4.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.4.m4.1c">4</annotation></semantics></math>, <math id="S4.SS2.p4.5.m5.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS2.p4.5.m5.1a"><mn id="S4.SS2.p4.5.m5.1.1" xref="S4.SS2.p4.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.5.m5.1b"><cn type="integer" id="S4.SS2.p4.5.m5.1.1.cmml" xref="S4.SS2.p4.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.5.m5.1c">5</annotation></semantics></math>, <math id="S4.SS2.p4.6.m6.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.SS2.p4.6.m6.1a"><mn id="S4.SS2.p4.6.m6.1.1" xref="S4.SS2.p4.6.m6.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.6.m6.1b"><cn type="integer" id="S4.SS2.p4.6.m6.1.1.cmml" xref="S4.SS2.p4.6.m6.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.6.m6.1c">6</annotation></semantics></math>] or narrowing the range of radius or elevation to [<math id="S4.SS2.p4.7.m7.1" class="ltx_Math" alttext="3" display="inline"><semantics id="S4.SS2.p4.7.m7.1a"><mn id="S4.SS2.p4.7.m7.1.1" xref="S4.SS2.p4.7.m7.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.7.m7.1b"><cn type="integer" id="S4.SS2.p4.7.m7.1.1.cmml" xref="S4.SS2.p4.7.m7.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.7.m7.1c">3</annotation></semantics></math>, <math id="S4.SS2.p4.8.m8.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS2.p4.8.m8.1a"><mn id="S4.SS2.p4.8.m8.1.1" xref="S4.SS2.p4.8.m8.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.8.m8.1b"><cn type="integer" id="S4.SS2.p4.8.m8.1.1.cmml" xref="S4.SS2.p4.8.m8.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.8.m8.1c">4</annotation></semantics></math>] and [<math id="S4.SS2.p4.9.m9.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS2.p4.9.m9.1a"><mn id="S4.SS2.p4.9.m9.1.1" xref="S4.SS2.p4.9.m9.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.9.m9.1b"><cn type="integer" id="S4.SS2.p4.9.m9.1.1.cmml" xref="S4.SS2.p4.9.m9.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.9.m9.1c">20</annotation></semantics></math>, <math id="S4.SS2.p4.10.m10.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.SS2.p4.10.m10.1a"><mn id="S4.SS2.p4.10.m10.1.1" xref="S4.SS2.p4.10.m10.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.p4.10.m10.1b"><cn type="integer" id="S4.SS2.p4.10.m10.1.1.cmml" xref="S4.SS2.p4.10.m10.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p4.10.m10.1c">30</annotation></semantics></math>]. We find that the location of light source has a greater influence than its radius and elevation. Compared with other factors, illumination has much less impact on class discrimination.</p>
</div>
<div id="S4.SS2.p5" class="ltx_para ltx_noindent">
<p id="S4.SS2.p5.2" class="ltx_p"><span id="S4.SS2.p5.2.1" class="ltx_text ltx_font_bold">Camera Viewpoint.</span> Recall that the camera always looks at the target object. We change the viewpoint of camera by placing it in different 3D locations: [0, 1, 1], [0, -1, 1], [1, 0, 1], or [-1, 0, 1]. We can see that fixing the camera viewpoint makes the results in IID and IID w/o BG tests deteriorate greatly, e.g., by <math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="63.03\%" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mrow id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml"><mn id="S4.SS2.p5.1.m1.1.1.2" xref="S4.SS2.p5.1.m1.1.1.2.cmml">63.03</mn><mo id="S4.SS2.p5.1.m1.1.1.1" xref="S4.SS2.p5.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><apply id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p5.1.m1.1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p5.1.m1.1.1.2.cmml" xref="S4.SS2.p5.1.m1.1.1.2">63.03</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">63.03\%</annotation></semantics></math> and <math id="S4.SS2.p5.2.m2.1" class="ltx_Math" alttext="51.99\%" display="inline"><semantics id="S4.SS2.p5.2.m2.1a"><mrow id="S4.SS2.p5.2.m2.1.1" xref="S4.SS2.p5.2.m2.1.1.cmml"><mn id="S4.SS2.p5.2.m2.1.1.2" xref="S4.SS2.p5.2.m2.1.1.2.cmml">51.99</mn><mo id="S4.SS2.p5.2.m2.1.1.1" xref="S4.SS2.p5.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.2.m2.1b"><apply id="S4.SS2.p5.2.m2.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p5.2.m2.1.1.1.cmml" xref="S4.SS2.p5.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p5.2.m2.1.1.2.cmml" xref="S4.SS2.p5.2.m2.1.1.2">51.99</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.2.m2.1c">51.99\%</annotation></semantics></math> respectively.</p>
</div>
<div id="S4.SS2.p6" class="ltx_para ltx_noindent">
<p id="S4.SS2.p6.2" class="ltx_p"><span id="S4.SS2.p6.2.1" class="ltx_text ltx_font_bold">Background.</span> When the background is lacking in an image, the accuracy in the IID test suffers an abrupt decrease of <math id="S4.SS2.p6.1.m1.1" class="ltx_Math" alttext="69.95\%" display="inline"><semantics id="S4.SS2.p6.1.m1.1a"><mrow id="S4.SS2.p6.1.m1.1.1" xref="S4.SS2.p6.1.m1.1.1.cmml"><mn id="S4.SS2.p6.1.m1.1.1.2" xref="S4.SS2.p6.1.m1.1.1.2.cmml">69.95</mn><mo id="S4.SS2.p6.1.m1.1.1.1" xref="S4.SS2.p6.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.1.m1.1b"><apply id="S4.SS2.p6.1.m1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1"><csymbol cd="latexml" id="S4.SS2.p6.1.m1.1.1.1.cmml" xref="S4.SS2.p6.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p6.1.m1.1.1.2.cmml" xref="S4.SS2.p6.1.m1.1.1.2">69.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.1.m1.1c">69.95\%</annotation></semantics></math> while that in the IID w/o BG test improves by <math id="S4.SS2.p6.2.m2.1" class="ltx_Math" alttext="16.2\%" display="inline"><semantics id="S4.SS2.p6.2.m2.1a"><mrow id="S4.SS2.p6.2.m2.1.1" xref="S4.SS2.p6.2.m2.1.1.cmml"><mn id="S4.SS2.p6.2.m2.1.1.2" xref="S4.SS2.p6.2.m2.1.1.2.cmml">16.2</mn><mo id="S4.SS2.p6.2.m2.1.1.1" xref="S4.SS2.p6.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p6.2.m2.1b"><apply id="S4.SS2.p6.2.m2.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1"><csymbol cd="latexml" id="S4.SS2.p6.2.m2.1.1.1.cmml" xref="S4.SS2.p6.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS2.p6.2.m2.1.1.2.cmml" xref="S4.SS2.p6.2.m2.1.1.2">16.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p6.2.m2.1c">16.2\%</annotation></semantics></math> due to reduced distribution shift. Among 5 factors, the background is <em id="S4.SS2.p6.2.2" class="ltx_emph ltx_font_italic">the most important</em> for IID generalization.</p>
</div>
<div id="S4.SS2.p7" class="ltx_para ltx_noindent">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text ltx_font_bold">Remarks.</span> Different rendering variation factors and even their different values have uneven importance to model generalization. It suggests that the under-explored direction of weighted rendering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is worth studying, and our results in Table <a href="#S4.T2" title="Table 2 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provide preliminary guidance/prior knowledge for learning the distributions of variation factors.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Exploring Pre-training for Domain Adaptation</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p id="S4.SS3.p1.9" class="ltx_p"><span id="S4.SS3.p1.9.1" class="ltx_text ltx_font_bold">Data Settings.</span> The data used for pre-training can be selected in several datasets and their variants: <span id="S4.SS3.p1.9.2" class="ltx_text ltx_font_bold">(1)</span> our synthesized <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mn id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><cn type="integer" id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">120</annotation></semantics></math>K images of the <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mn id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><cn type="integer" id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">10</annotation></semantics></math> object classes shared by SubVisDA-10 (Ours), <span id="S4.SS3.p1.9.3" class="ltx_text ltx_font_bold">(2)</span> our synthesized <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="12.8" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><mn id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml">12.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><cn type="float" id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">12.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">12.8</annotation></semantics></math>M images of the <math id="S4.SS3.p1.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS3.p1.4.m4.1a"><mn id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><cn type="integer" id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">10</annotation></semantics></math> classes (for supervised learning, termed SynSL), <span id="S4.SS3.p1.9.4" class="ltx_text ltx_font_bold">(3)</span> the subset collecting examples of the <math id="S4.SS3.p1.5.m5.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS3.p1.5.m5.1a"><mn id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><cn type="integer" id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">10</annotation></semantics></math> classes from ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> (25,686 images, termed SubImageNet), <span id="S4.SS3.p1.9.5" class="ltx_text ltx_font_bold">(4)</span> our synthesized <math id="S4.SS3.p1.6.m6.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.SS3.p1.6.m6.1a"><mn id="S4.SS3.p1.6.m6.1.1" xref="S4.SS3.p1.6.m6.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.6.m6.1b"><cn type="integer" id="S4.SS3.p1.6.m6.1.1.cmml" xref="S4.SS3.p1.6.m6.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.6.m6.1c">120</annotation></semantics></math>K images combined with SubImageNet,
<span id="S4.SS3.p1.9.6" class="ltx_text ltx_font_bold">(5)</span> ImageNet-990, where the fine-grained subclasses for each of the <math id="S4.SS3.p1.7.m7.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS3.p1.7.m7.1a"><mn id="S4.SS3.p1.7.m7.1.1" xref="S4.SS3.p1.7.m7.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.7.m7.1b"><cn type="integer" id="S4.SS3.p1.7.m7.1.1.cmml" xref="S4.SS3.p1.7.m7.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.7.m7.1c">10</annotation></semantics></math> classes are merged into one,
<span id="S4.SS3.p1.9.7" class="ltx_text ltx_font_bold">(6)</span> ImageNet-990 combined with our 120K synthetic images,
<span id="S4.SS3.p1.9.8" class="ltx_text ltx_font_bold">(7)</span> the full set of ImageNet (1K classes), and <span id="S4.SS3.p1.9.9" class="ltx_text ltx_font_bold">(8)</span> MetaShift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> (2.56M). For fine-tuning, we use domain adaptation (DA) on SubVisDA-10 as the downstream task, which comprises <math id="S4.SS3.p1.8.m8.2" class="ltx_Math" alttext="130,725" display="inline"><semantics id="S4.SS3.p1.8.m8.2a"><mrow id="S4.SS3.p1.8.m8.2.3.2" xref="S4.SS3.p1.8.m8.2.3.1.cmml"><mn id="S4.SS3.p1.8.m8.1.1" xref="S4.SS3.p1.8.m8.1.1.cmml">130</mn><mo id="S4.SS3.p1.8.m8.2.3.2.1" xref="S4.SS3.p1.8.m8.2.3.1.cmml">,</mo><mn id="S4.SS3.p1.8.m8.2.2" xref="S4.SS3.p1.8.m8.2.2.cmml">725</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.8.m8.2b"><list id="S4.SS3.p1.8.m8.2.3.1.cmml" xref="S4.SS3.p1.8.m8.2.3.2"><cn type="integer" id="S4.SS3.p1.8.m8.1.1.cmml" xref="S4.SS3.p1.8.m8.1.1">130</cn><cn type="integer" id="S4.SS3.p1.8.m8.2.2.cmml" xref="S4.SS3.p1.8.m8.2.2">725</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.8.m8.2c">130,725</annotation></semantics></math> labeled instances in the source domain and <math id="S4.SS3.p1.9.m9.2" class="ltx_Math" alttext="46,697" display="inline"><semantics id="S4.SS3.p1.9.m9.2a"><mrow id="S4.SS3.p1.9.m9.2.3.2" xref="S4.SS3.p1.9.m9.2.3.1.cmml"><mn id="S4.SS3.p1.9.m9.1.1" xref="S4.SS3.p1.9.m9.1.1.cmml">46</mn><mo id="S4.SS3.p1.9.m9.2.3.2.1" xref="S4.SS3.p1.9.m9.2.3.1.cmml">,</mo><mn id="S4.SS3.p1.9.m9.2.2" xref="S4.SS3.p1.9.m9.2.2.cmml">697</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.9.m9.2b"><list id="S4.SS3.p1.9.m9.2.3.1.cmml" xref="S4.SS3.p1.9.m9.2.3.2"><cn type="integer" id="S4.SS3.p1.9.m9.1.1.cmml" xref="S4.SS3.p1.9.m9.1.1">46</cn><cn type="integer" id="S4.SS3.p1.9.m9.2.2.cmml" xref="S4.SS3.p1.9.m9.2.2">697</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.9.m9.2c">46,697</annotation></semantics></math> unlabeled ones in the target domain. We follow the standard DA training protocol <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>. We report classification results of overall accuracy (Acc. %) and mean class precision (Mean %) on the target domain under a fixed random seed.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.p2.2" class="ltx_p"><span id="S4.SS3.p2.2.1" class="ltx_text ltx_font_bold">Implemental Details.</span> For domain adaptation, we use DANN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, MCD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>, RCA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, SRDC <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite>, and DisClusterDA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> as baselines. We closely follow the specific algorithms in the respective papers of these baselines. We use a pre-trained ResNet-50 as the base model. We train the model for <math id="S4.SS3.p2.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS3.p2.1.m1.1a"><mn id="S4.SS3.p2.1.m1.1.1" xref="S4.SS3.p2.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.1b"><cn type="integer" id="S4.SS3.p2.1.m1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.1c">20</annotation></semantics></math> epochs with batch size <math id="S4.SS3.p2.2.m2.1" class="ltx_Math" alttext="64" display="inline"><semantics id="S4.SS3.p2.2.m2.1a"><mn id="S4.SS3.p2.2.m2.1.1" xref="S4.SS3.p2.2.m2.1.1.cmml">64</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.1b"><cn type="integer" id="S4.SS3.p2.2.m2.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1">64</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.1c">64</annotation></semantics></math> via SGD. Refer to Sec. <a href="#S4.SS1" title="4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.1</span></a> and the appendix for other details.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Results and Discussions</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">To <em id="S4.SS3.SSS1.p1.1.1" class="ltx_emph ltx_font_italic">study the effects of pre-training on synthetic-to-real adaptation</em>, we examine several DA methods when varying the pre-training scheme in terms of pre-training data and duration. The results are reported in Table <a href="#S4.T3" title="Table 3 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and Figs. <a href="#S4.F4" title="Figure 4 ‣ 4.3.1 Results and Discussions ‣ 4.3 Exploring Pre-training for Domain Adaptation ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and A7. We emphasize several remarkable findings below.</p>
</div>
<div id="S4.SS3.SSS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p"><span id="S4.SS3.SSS1.p2.1.1" class="ltx_text ltx_font_bold">The importance of pre-training for DA.</span> DA fails without pre-training. With no pre-training, the very baseline No Adaptation that trains the model only on the labeled source data, outperforms all compared DA methods in overall accuracy, despite the worst mean class precision. It verifies that pre-training is indispensable for DA and involving the target data in training may alleviate class imbalance.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p"><span id="S4.SS3.SSS1.p3.1.1" class="ltx_text ltx_font_bold">Effects of different pre-training schemes.</span> Different DA methods exhibit different relative advantages under different pre-training data. When pre-training on our synthesized data, MCD achieves the best results; when pre-training on Ours+SubImageNet, DisClusterDA outperforms the others; when pre-training on ImageNet<sup id="S4.SS3.SSS1.p3.1.2" class="ltx_sup">★</sup>, SRDC yields the best performance.
What’s worse, the reliability of existing DA method evaluation criteria is unguaranteed. With different pre-training schemes, the best performance is achieved by different DA methods. When pre-training on ImageNet for 10, 60, or 120 epochs, the best results are achieved by RCA, MCD, and SRDC respectively; when pre-training on MetaShift for 5, 30, or 60 epochs, the best results are achieved by RCA, MCD, and DisClusterDA respectively.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p4.1" class="ltx_p"><span id="S4.SS3.SSS1.p4.1.1" class="ltx_text ltx_font_bold">Synthetic data pre-training vs. real data pre-training.</span> Synthetic data pre-training is better than pre-training on real data in our study. With the same 200K pre-training iterations, our synthetic data often bring more benefits than real data from ImageNet or MetaShift, though the top-ranked performance is achieved by extending the pre-training time on real data.
Under the same experimental configuration, SynSL pre-training for <math id="S4.SS3.SSS1.p4.1.m1.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S4.SS3.SSS1.p4.1.m1.1a"><mn id="S4.SS3.SSS1.p4.1.m1.1.1" xref="S4.SS3.SSS1.p4.1.m1.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p4.1.m1.1b"><cn type="integer" id="S4.SS3.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p4.1.m1.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p4.1.m1.1c">24</annotation></semantics></math> epochs is comparable to or better than pre-training on ImageNet for 120 epochs and maybe it’s because SynSL is 10 times ImageNet’s size. The observation indicates that with our 12.8M synthetic data pre-training, the DA methods can yield the new state of the art.</p>
</div>
<div id="S4.SS3.SSS1.p5" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p5.1" class="ltx_p"><span id="S4.SS3.SSS1.p5.1.1" class="ltx_text ltx_font_bold">Implications for pre-training data setting.</span> Big Synthesis Small Real is worth deeply researching. Ours+SubImageNet augmenting our synthetic data with a small amount of real data, achieves remarkable performance gain over Ours, suggesting a promising paradigm of supervised pre-training — Big Synthesis Small Real.
On the other hand, pre-train with target classes first under limited computing resources. With 200K pre-training iterations, SubImageNet performs much better than ImageNet (10 Epochs), suggesting that one should consider pre-training with target classes first in cases of low computation budget, e.g., real-time deployment on low-power devices like mobile phones, laptops, and smartwatches. Here, we have two questions to be considered: do we have unlimited computing resources for pre-training? Is the domain-specific pre-training more suitable for some industrial applications?</p>
</div>
<figure id="S4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F4.4.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S4.F4.5.2" class="ltx_text" style="font-size:90%;">Learning process (Mean) of MCD (<span id="S4.F4.5.2.1" class="ltx_text ltx_font_bold">left</span>) and DisClusterDA (<span id="S4.F4.5.2.2" class="ltx_text ltx_font_bold">right</span>) when varying the pre-training scheme.</span></figcaption>
</figure>
<div id="S4.SS3.SSS1.p6" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p6.1" class="ltx_p"><span id="S4.SS3.SSS1.p6.1.1" class="ltx_text ltx_font_bold">The improved generalization of DA models.</span> Real data pre-training with extra non-target classes, fine-grained target subclasses, or our synthesized data added for target classes helps DA.
ImageNet (120 Epochs) involving both target and non-target classes in pre-training is better than SubImageNet involving only target classes, indicating that learning rich category relationships is helpful for downstream transferring.
with 200K pre-training iterations, ImageNet-990 performs much worse than ImageNet, implying that pre-training in a fine-grained visual categorization manner may bring surprising benefits.
Ours+SubImageNet adding our synthesized data for target classes in SubImageNet, produces significant improvements and is close to ImageNet (120 Epochs); ImageNet-990+Ours improves over ImageNet-990, suggesting that synthetic data may help improve the performance further.</p>
</div>
<div id="S4.SS3.SSS1.p7" class="ltx_para ltx_noindent">
<p id="S4.SS3.SSS1.p7.3" class="ltx_p"><span id="S4.SS3.SSS1.p7.3.1" class="ltx_text ltx_font_bold">Convergence analysis.</span> In Figs. <a href="#S4.F4" title="Figure 4 ‣ 4.3.1 Results and Discussions ‣ 4.3 Exploring Pre-training for Domain Adaptation ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and A7, the convergence from different pre-training schemes for the same DA method differs in speed, stability, and accuracy. In <a href="#S4.F4" title="In 4.3.1 Results and Discussions ‣ 4.3 Exploring Pre-training for Domain Adaptation ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Fig.</span> <span class="ltx_text ltx_ref_tag">4</span></a>, SynSL with <math id="S4.SS3.SSS1.p7.1.m1.1" class="ltx_Math" alttext="24" display="inline"><semantics id="S4.SS3.SSS1.p7.1.m1.1a"><mn id="S4.SS3.SSS1.p7.1.m1.1.1" xref="S4.SS3.SSS1.p7.1.m1.1.1.cmml">24</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p7.1.m1.1b"><cn type="integer" id="S4.SS3.SSS1.p7.1.m1.1.1.cmml" xref="S4.SS3.SSS1.p7.1.m1.1.1">24</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p7.1.m1.1c">24</annotation></semantics></math> epochs outperforms ImageNet with <math id="S4.SS3.SSS1.p7.2.m2.1" class="ltx_Math" alttext="120" display="inline"><semantics id="S4.SS3.SSS1.p7.2.m2.1a"><mn id="S4.SS3.SSS1.p7.2.m2.1.1" xref="S4.SS3.SSS1.p7.2.m2.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="S4.SS3.SSS1.p7.2.m2.1b"><cn type="integer" id="S4.SS3.SSS1.p7.2.m2.1.1.cmml" xref="S4.SS3.SSS1.p7.2.m2.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.SSS1.p7.2.m2.1c">120</annotation></semantics></math> epochs significantly; notably, SynSL is on par with or better than ImageNet<sup id="S4.SS3.SSS1.p7.3.2" class="ltx_sup">★</sup>, supporting our aforementioned findings.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2303.09165/assets/x2.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="528" height="213" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S4.F5.3.2" class="ltx_text" style="font-size:90%;">Sample images from the synthetic (left) domain and the real domains of S2RDA-49 (middle) and S2RDA-MS-39 (right).</span></figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F6.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/x3.png" id="S4.F6.sf1.1.g1" class="ltx_graphics ltx_img_square" width="115" height="115" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F6.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/x4.png" id="S4.F6.sf2.1.g1" class="ltx_graphics ltx_img_square" width="115" height="115" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F6.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/x5.png" id="S4.F6.sf3.1.g1" class="ltx_graphics ltx_img_square" width="115" height="115" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf3.3.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="S4.F6.sf4.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/x6.png" id="S4.F6.sf4.1.g1" class="ltx_graphics ltx_img_square" width="115" height="115" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.sf4.3.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S4.F6.3.2" class="ltx_text" style="font-size:90%;">The t-SNE visualization of target domain features extracted by different models on S2RDA-49 (a-b) and S2RDA-MS-39 (c-d).</span></figcaption>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>A New Synthetic-to-Real Benchmark</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p id="S4.SS4.p1.5" class="ltx_p"><span id="S4.SS4.p1.5.1" class="ltx_text ltx_font_bold">Dataset Construction.</span> Our proposed Synthetic-to-Real benchmark for more practical visual DA (termed S2RDA) includes two challenging transfer tasks of S2RDA-49 and S2RDA-MS-39 (cf. Fig. <a href="#S4.F5" title="Figure 5 ‣ 4.3.1 Results and Discussions ‣ 4.3 Exploring Pre-training for Domain Adaptation ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>). In each task, source/synthetic domain samples are synthesized by rendering 3D models from ShapeNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> (cf. Sec. <a href="#S3" title="3 Data Synthesis via Domain Randomization ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). The used 3D models are in the same label space as the target/real domain and each class has <math id="S4.SS4.p1.1.m1.1" class="ltx_Math" alttext="12" display="inline"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn type="integer" id="S4.SS4.p1.1.m1.1.1.cmml" xref="S4.SS4.p1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">12</annotation></semantics></math>K rendered RGB images.
The real domain of S2RDA-49 comprises <math id="S4.SS4.p1.2.m2.2" class="ltx_Math" alttext="60,535" display="inline"><semantics id="S4.SS4.p1.2.m2.2a"><mrow id="S4.SS4.p1.2.m2.2.3.2" xref="S4.SS4.p1.2.m2.2.3.1.cmml"><mn id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml">60</mn><mo id="S4.SS4.p1.2.m2.2.3.2.1" xref="S4.SS4.p1.2.m2.2.3.1.cmml">,</mo><mn id="S4.SS4.p1.2.m2.2.2" xref="S4.SS4.p1.2.m2.2.2.cmml">535</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.2b"><list id="S4.SS4.p1.2.m2.2.3.1.cmml" xref="S4.SS4.p1.2.m2.2.3.2"><cn type="integer" id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1">60</cn><cn type="integer" id="S4.SS4.p1.2.m2.2.2.cmml" xref="S4.SS4.p1.2.m2.2.2">535</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.2c">60,535</annotation></semantics></math> images of <math id="S4.SS4.p1.3.m3.1" class="ltx_Math" alttext="49" display="inline"><semantics id="S4.SS4.p1.3.m3.1a"><mn id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml">49</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><cn type="integer" id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1">49</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">49</annotation></semantics></math> classes, collected from ImageNet validation set, ObjectNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, VisDA-2017 validation set, and the web <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
For S2RDA-MS-39, the real domain collects <math id="S4.SS4.p1.4.m4.2" class="ltx_Math" alttext="41,735" display="inline"><semantics id="S4.SS4.p1.4.m4.2a"><mrow id="S4.SS4.p1.4.m4.2.3.2" xref="S4.SS4.p1.4.m4.2.3.1.cmml"><mn id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml">41</mn><mo id="S4.SS4.p1.4.m4.2.3.2.1" xref="S4.SS4.p1.4.m4.2.3.1.cmml">,</mo><mn id="S4.SS4.p1.4.m4.2.2" xref="S4.SS4.p1.4.m4.2.2.cmml">735</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.2b"><list id="S4.SS4.p1.4.m4.2.3.1.cmml" xref="S4.SS4.p1.4.m4.2.3.2"><cn type="integer" id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1">41</cn><cn type="integer" id="S4.SS4.p1.4.m4.2.2.cmml" xref="S4.SS4.p1.4.m4.2.2">735</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.2c">41,735</annotation></semantics></math> natural images exclusive for <math id="S4.SS4.p1.5.m5.1" class="ltx_Math" alttext="39" display="inline"><semantics id="S4.SS4.p1.5.m5.1a"><mn id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml">39</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><cn type="integer" id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1">39</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">39</annotation></semantics></math> classes from MetaShift <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite>, which contain complex and distinct contexts, e.g., object presence (co-occurrence of different objects), general contexts (indoor or outdoor), and object attributes (color or shape), leading to a much harder task.
In Fig. A8, we show the long-tailed distribution of image number per class in each real domain.
Compared to VisDA-2017 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>, our S2RDA contains more categories, more realistically synthesized source domain data coming for free, and more complicated target domain data collected from diverse real-world sources, setting a more practical, challenging benchmark for future DA research.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S4.T4.3.2" class="ltx_text" style="font-size:90%;">Domain adaptation performance on S2RDA (ResNet-50).</span></figcaption>
<div id="S4.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:62.7pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-35.8pt,5.1pt) scale(0.858368756470942,0.858368756470942) ;">
<table id="S4.T4.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.4.1.1.1" class="ltx_tr">
<th id="S4.T4.4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="S4.T4.4.1.1.1.1.1" class="ltx_text">Transfer Task</span></th>
<th id="S4.T4.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">No Adaptation</th>
<th id="S4.T4.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">DANN</th>
<th id="S4.T4.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">MCD</th>
<th id="S4.T4.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">RCA</th>
<th id="S4.T4.4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">SRDC</th>
<th id="S4.T4.4.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">DisClusterDA</th>
</tr>
<tr id="S4.T4.4.1.2.2" class="ltx_tr">
<td id="S4.T4.4.1.2.2.1" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.2.2.1.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T4.4.1.2.2.2" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T4.4.1.2.2.3" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.2.2.3.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T4.4.1.2.2.4" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T4.4.1.2.2.5" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.2.2.5.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T4.4.1.2.2.6" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T4.4.1.2.2.7" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.2.2.7.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T4.4.1.2.2.8" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T4.4.1.2.2.9" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.2.2.9.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T4.4.1.2.2.10" class="ltx_td ltx_align_center">Mean</td>
<td id="S4.T4.4.1.2.2.11" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.2.2.11.1" class="ltx_text" style="background-color:#C7C7C7;">Acc.</span></td>
<td id="S4.T4.4.1.2.2.12" class="ltx_td ltx_align_center">Mean</td>
</tr>
<tr id="S4.T4.4.1.3.3" class="ltx_tr">
<th id="S4.T4.4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t">S2RDA-49</th>
<th id="S4.T4.4.1.3.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.3.3.2.1" class="ltx_text" style="background-color:#C7C7C7;">51.89</span></th>
<th id="S4.T4.4.1.3.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">42.19</th>
<th id="S4.T4.4.1.3.3.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.3.3.4.1" class="ltx_text" style="background-color:#C7C7C7;">47.06</span></th>
<th id="S4.T4.4.1.3.3.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">47.64</th>
<th id="S4.T4.4.1.3.3.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.3.3.6.1" class="ltx_text" style="background-color:#C7C7C7;">42.51</span></th>
<th id="S4.T4.4.1.3.3.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">47.77</th>
<th id="S4.T4.4.1.3.3.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.3.3.8.1" class="ltx_text" style="background-color:#C7C7C7;">47.07</span></th>
<th id="S4.T4.4.1.3.3.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">48.46</th>
<th id="S4.T4.4.1.3.3.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.3.3.10.1" class="ltx_text ltx_font_bold" style="background-color:#00FF00;">61.52</span></th>
<th id="S4.T4.4.1.3.3.11" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S4.T4.4.1.3.3.11.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="border-color: #FF0000;">52.98</span></th>
<th id="S4.T4.4.1.3.3.12" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.3.3.12.1" class="ltx_text" style="background-color:#C7C7C7;">53.03</span></th>
<th id="S4.T4.4.1.3.3.13" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">52.34</th>
</tr>
<tr id="S4.T4.4.1.4.4" class="ltx_tr">
<th id="S4.T4.4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb">S2RDA-MS-39</th>
<td id="S4.T4.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.4.4.2.1" class="ltx_text" style="background-color:#C7C7C7;">22.03</span></td>
<td id="S4.T4.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_bb">20.54</td>
<td id="S4.T4.4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.4.4.4.1" class="ltx_text" style="background-color:#C7C7C7;">22.82</span></td>
<td id="S4.T4.4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_bb">22.20</td>
<td id="S4.T4.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.4.4.6.1" class="ltx_text" style="background-color:#C7C7C7;">22.07</span></td>
<td id="S4.T4.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_bb">22.16</td>
<td id="S4.T4.4.1.4.4.8" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.4.4.8.1" class="ltx_text" style="background-color:#C7C7C7;">23.34</span></td>
<td id="S4.T4.4.1.4.4.9" class="ltx_td ltx_align_center ltx_border_bb">22.53</td>
<td id="S4.T4.4.1.4.4.10" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.4.4.10.1" class="ltx_text" style="background-color:#C7C7C7;">25.83</span></td>
<td id="S4.T4.4.1.4.4.11" class="ltx_td ltx_align_center ltx_border_bb">24.55</td>
<td id="S4.T4.4.1.4.4.12" class="ltx_td ltx_align_center ltx_border_bb" style="background-color:#C7C7C7;"><span id="S4.T4.4.1.4.4.12.1" class="ltx_text ltx_font_bold" style="background-color:#00FF00;">27.14</span></td>
<td id="S4.T4.4.1.4.4.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T4.4.1.4.4.13.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="border-color: #FF0000;">25.33</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS4.p2.5" class="ltx_p"><span id="S4.SS4.p2.5.1" class="ltx_text ltx_font_bold">Benchmarking DA Methods.</span> We use ResNet-50 as the backbone, initialized by the official ImageNet pre-trained checkpoint <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Refer to Sec. <a href="#S4.SS3" title="4.3 Exploring Pre-training for Domain Adaptation ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4.3</span></a> and the appendix for other implementation details.
We report the results on S2RDA in Table <a href="#S4.T4" title="Table 4 ‣ 4.4 A New Synthetic-to-Real Benchmark ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> and show t-SNE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite> visualizations in Fig. <a href="#S4.F6" title="Figure 6 ‣ 4.3.1 Results and Discussions ‣ 4.3 Exploring Pre-training for Domain Adaptation ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>. In the overall accuracy (Acc.) on S2RDA-49, the adversarial training based methods DANN, MCD, and RCA perform worse than No Adaptation, demonstrating that explicit domain adaptation could deteriorate the intrinsic discriminative structures of target domain data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite>, especially in cases of more categories; in contrast, SRDC produces significant quantitative and qualitative improvements, <math id="S4.SS4.p2.1.m1.1" class="ltx_Math" alttext="14.45\%" display="inline"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mn id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">14.45</mn><mo id="S4.SS4.p2.1.m1.1.1.1" xref="S4.SS4.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><csymbol cd="latexml" id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.1.m1.1.1.2.cmml" xref="S4.SS4.p2.1.m1.1.1.2">14.45</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">14.45\%</annotation></semantics></math> and <math id="S4.SS4.p2.2.m2.1" class="ltx_Math" alttext="8.49\%" display="inline"><semantics id="S4.SS4.p2.2.m2.1a"><mrow id="S4.SS4.p2.2.m2.1.1" xref="S4.SS4.p2.2.m2.1.1.cmml"><mn id="S4.SS4.p2.2.m2.1.1.2" xref="S4.SS4.p2.2.m2.1.1.2.cmml">8.49</mn><mo id="S4.SS4.p2.2.m2.1.1.1" xref="S4.SS4.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.2.m2.1b"><apply id="S4.SS4.p2.2.m2.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1"><csymbol cd="latexml" id="S4.SS4.p2.2.m2.1.1.1.cmml" xref="S4.SS4.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.2.m2.1.1.2.cmml" xref="S4.SS4.p2.2.m2.1.1.2">8.49</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.2.m2.1c">8.49\%</annotation></semantics></math> higher than RCA and DisClusterDA respectively, but <math id="S4.SS4.p2.3.m3.1" class="ltx_Math" alttext="7.48\%" display="inline"><semantics id="S4.SS4.p2.3.m3.1a"><mrow id="S4.SS4.p2.3.m3.1.1" xref="S4.SS4.p2.3.m3.1.1.cmml"><mn id="S4.SS4.p2.3.m3.1.1.2" xref="S4.SS4.p2.3.m3.1.1.2.cmml">7.48</mn><mo id="S4.SS4.p2.3.m3.1.1.1" xref="S4.SS4.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.3.m3.1b"><apply id="S4.SS4.p2.3.m3.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1"><csymbol cd="latexml" id="S4.SS4.p2.3.m3.1.1.1.cmml" xref="S4.SS4.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.3.m3.1.1.2.cmml" xref="S4.SS4.p2.3.m3.1.1.2">7.48</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.3.m3.1c">7.48\%</annotation></semantics></math> lower than Acc. on SubVisDA-10 (cf. Table <a href="#S4.T3" title="Table 3 ‣ 4.1.1 Results ‣ 4.1 Empirical Study on Supervised Learning ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), indicating the difficulty of the S2RDA-49 task. On S2RDA-MS-39, the classification accuracy is much worse than that on S2RDA-49 (decrease of more than <math id="S4.SS4.p2.4.m4.1" class="ltx_Math" alttext="20\%" display="inline"><semantics id="S4.SS4.p2.4.m4.1a"><mrow id="S4.SS4.p2.4.m4.1.1" xref="S4.SS4.p2.4.m4.1.1.cmml"><mn id="S4.SS4.p2.4.m4.1.1.2" xref="S4.SS4.p2.4.m4.1.1.2.cmml">20</mn><mo id="S4.SS4.p2.4.m4.1.1.1" xref="S4.SS4.p2.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.4.m4.1b"><apply id="S4.SS4.p2.4.m4.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1"><csymbol cd="latexml" id="S4.SS4.p2.4.m4.1.1.1.cmml" xref="S4.SS4.p2.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS4.p2.4.m4.1.1.2.cmml" xref="S4.SS4.p2.4.m4.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.4.m4.1c">20\%</annotation></semantics></math>), and the compared methods show much less performance difference; the highest accuracy is only <math id="S4.SS4.p2.5.m5.1" class="ltx_Math" alttext="27.14\%" display="inline"><semantics id="S4.SS4.p2.5.m5.1a"><mrow id="S4.SS4.p2.5.m5.1.1" xref="S4.SS4.p2.5.m5.1.1.cmml"><mn id="S4.SS4.p2.5.m5.1.1.2" xref="S4.SS4.p2.5.m5.1.1.2.cmml">27.14</mn><mo id="S4.SS4.p2.5.m5.1.1.1" xref="S4.SS4.p2.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.5.m5.1b"><apply id="S4.SS4.p2.5.m5.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1"><csymbol cd="latexml" id="S4.SS4.p2.5.m5.1.1.1.cmml" xref="S4.SS4.p2.5.m5.1.1.1">percent</csymbol><cn type="float" id="S4.SS4.p2.5.m5.1.1.2.cmml" xref="S4.SS4.p2.5.m5.1.1.2">27.14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.5.m5.1c">27.14\%</annotation></semantics></math> achieved by DisClusterDA, showing that S2RDA-MS-39 is a very challenging task. To summarize, <em id="S4.SS4.p2.5.2" class="ltx_emph ltx_font_italic">domain adaptation is far from being solved</em> and we expect that our results contribute to the DA community as new benchmarks, though more careful studies in different algorithmic frameworks are certainly necessary to be conducted.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions and Future Perspectives</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This paper primarily aims to publish new datasets including our synthetic dataset SynSL (12.8M) and S2RDA, and benchmarks the datasets via supervised learning and downstream transferring.
In the context of image classification, our work is the first comprehensive study on synthetic data learning, which is completely missing.
We propose exploiting synthetic datasets to explore questions on model generalization and benchmark pre-training strategies for DA. We build randomized synthetic datasets using a 3D rendering engine and use this established system to manipulate the generation of images by altering several imaging factors. We find that synthetic data pre-training has the potential to be better than pre-training on real data, our new benchmark S2RDA is much more practical for synthetic-to-real DA, to name a few. We expect that these results contribute to the transfer learning community as new benchmarks, though the research on more synthetic datasets, more models, and more DA methods is certainly to be done.
</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><em id="S5.p2.1.1" class="ltx_emph ltx_font_italic">Synthetic data as a new benchmark.</em> Synthetic data are well suited for use as toy examples to verify existing deep learning theoretical results or explore new theories.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><em id="S5.p3.1.1" class="ltx_emph ltx_font_italic">Evaluation metrics robust to pre-training.</em> The comparison among various DA methods yields different or even opposite results when using different pre-training schemes (cf. <a href="#S4.SS3" title="4.3 Exploring Pre-training for Domain Adaptation ‣ 4 Experiment and Evaluation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">Sec.</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>). DA researchers should propose and follow evaluation metrics enabling effective and fair comparison.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><em id="S5.p4.1.1" class="ltx_emph ltx_font_italic">More realistic simulation synthesis.</em> We will consider more imaging parameters, e.g., randomizing the type and hue of the light, including 77 physical objects with actual textures from YCB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, and using the flying distractor <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S5.p5" class="ltx_para ltx_noindent">
<p id="S5.p5.1" class="ltx_p"><em id="S5.p5.1.1" class="ltx_emph ltx_font_italic">To explore deep learning based data generation.</em> Our proposed paradigm of empirical study can generalize to any data generation pipeline. Our findings may be data source specific and the generalizability to other pipelines like GANs, NeRFs, and AutoSimulate <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is to be explored.</p>
</div>
<div id="S5.p6" class="ltx_para ltx_noindent">
<p id="S5.p6.1" class="ltx_p"><em id="S5.p6.1.1" class="ltx_emph ltx_font_italic">Applicability to other vision tasks.</em> Our new paradigm of empirical study for image classification can also be applied to other vision tasks of semantic analysis, e.g., Kubric <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> and HyperSim <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> for segmentation and object detection.</p>
</div>
<div id="S5.p7" class="ltx_para ltx_noindent">
<p id="S5.p7.1" class="ltx_p"><span id="S5.p7.1.1" class="ltx_text ltx_font_bold">Acknowledgments.</span> This work is supported in part by Program for Guangdong Introducing Innovative and Enterpreneurial Teams (No.: 2017ZT07X183) and Guangdong R&amp;D key project of China (No.: 2019B010155001).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Samira Abnar and Willem Zuidema.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Quantifying attention flow in transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages 4190–4197, Online, July 2020.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
NIKHIL AKKI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Furniture detector.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">CC0: Public Domain</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang Fu, Weizhu Chen,
Nanning Zheng, and Jian-Guang Lou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Input-tuning: Adapting unfamiliar inputs to frozen pretrained models,
2022.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Metareg: Towards domain generalization using meta-regularization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib4.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib4.5.3" class="ltx_text" style="font-size:90%;">, volume 31. Curran Associates,
Inc., 2018.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan
Gutfreund, Josh Tenenbaum, and Boris Katz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Objectnet: A large-scale bias-controlled dataset for pushing the
limits of object recognition models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, volume 32, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Sara Beery, Grant Van Horn, and Pietro Perona.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Recognition in terra incognita.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the European Conference on Computer Vision
(ECCV)</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, September 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Harkirat Singh Behl, Atilim Güneş Baydin, Ran Gal, Philip H. S. Torr,
and Vibhav Vineet.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Autosimulate: (quickly) learning synthetic data generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
editors, </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Eur. Conf. Comput. Vis.</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 255–271, Cham, 2020.
Springer International Publishing.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Zhaowei Cai, Avinash Ravichandran, Subhransu Maji, Charless Fowlkes, Zhuowen
Tu, and Stefano Soatto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">Exponential moving average normalization for self-supervised and
semi-supervised learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages
194–203, June 2021.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srinivasa, Pieter Abbeel, ,
and Aaron M. Dollar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Benchmarking in manipulation research: The ycb object and model set
and benchmarking protocols.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Robotics and Automation Magazine</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 36–52, 2015.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and
Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Unsupervised learning of visual features by contrasting cluster
assignments.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, Red Hook, NY, USA, 2020.
Curran Associates Inc.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang,
Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao,
Li Yi, and Fisher Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">ShapeNet: An Information-Rich 3D Model Repository.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">Technical Report arXiv:1512.03012 [cs.GR], Stanford University —
Princeton University — Toyota Technological Institute at Chicago, 2015.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">A simple framework for contrastive learning of visual
representations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. Mach. Learn.</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Wuyang Chen, Zhiding Yu, Shalini De Mello, Sifei Liu, Jose M. Alvarez,
Zhangyang Wang, and Anima Anandkumar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Contrastive syn-to-real generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. on Learn. Rep.</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
S. Cicek and S. Soatto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Unsupervised domain adaptation via regularized conditional alignment.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Int. Conf. Comput. Vis.</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, pages 1416–1425, 2019.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Adam Coates, Andrew Ng, and Honglak Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">An analysis of single-layer networks in unsupervised feature
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the Fourteenth International Conference on
Artificial Intelligence and Statistics</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">, pages 215–223, 2011.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, and Qi Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Towards discriminability and diversity: Batch nuclear-norm
maximization under label insufficient situations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">, pages
3941–3950, 2020.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Imagenet: A large-scale hierarchical image database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, pages
248–255, 2009.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan,
Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Blenderproc.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.01911</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Pedro Domingos.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">A few useful things to know about machine learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Commun. ACM</span><span id="bib.bib19.4.2" class="ltx_text" style="font-size:90%;">, 55:78–87, 2012.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at
scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib20.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. on Learn. Rep.</span><span id="bib.bib20.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Xinjie Fan, Qifei Wang, Junjie Ke, Feng Yang, Boqing Gong, and Mingyuan Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Adversarially adaptive normalization for single domain
generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">, pages 8208–8217, June 2021.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Yutong Feng, Jianwen Jiang, Mingqian Tang, Rong Jin, and Yue Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Rethinking supervised pre-training for better downstream
transferring.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. on Learn. Rep.</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Domain-adversarial training of neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journ. of Mach. Learn. Res.</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">, 17:2096–2030, 2016.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
R. Geirhos, JH. Jacobsen, C. Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A. Wichmann.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">Shortcut learning in deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nat. Mach. Intell.</span><span id="bib.bib24.4.2" class="ltx_text" style="font-size:90%;">, 2:665–673, 2020.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A.
Wichmann, and Wieland Brendel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">Imagenet-trained CNNs are biased towards texture; increasing shape
bias improves accuracy and robustness.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib25.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Learning Representations</span><span id="bib.bib25.5.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Kamran Ghasedi, Xiaoqian Wang, Cheng Deng, and Heng Huang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Balanced self-paced learning for generative adversarial clustering
network.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">, pages
4386–4395, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Rich feature hierarchies for accurate object detection and semantic
segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, pages
580–587, 2014.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel
Duckworth, David J. Fleet, Dan Gnanapragasam, Florian Golemo, Charles
Herrmann, Thomas Kipf, Abhijit Kundu, Dmitry Lagun, Issam Laradji,
Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek Nowrouzezahrai, Cengiz
Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi S. M.
Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora,
Ziyu Wang, Tianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea
Tagliasacchi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Kubric: A scalable dataset generator.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib28.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib28.5.3" class="ltx_text" style="font-size:90%;">, pages 3749–3761, June 2022.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre
Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan
Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and
Michal Valko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">Bootstrap your own latent - a new approach to self-supervised
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,
editors, </span><span id="bib.bib29.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib29.5.3" class="ltx_text" style="font-size:90%;">, volume 33, pages 21271–21284.
Curran Associates, Inc., 2020.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Momentum contrast for unsupervised visual representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib30.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2020 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib30.5.3" class="ltx_text" style="font-size:90%;">, pages 9726–9735, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
K. He, X. Zhang, S. Ren, and J. Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib31.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib31.5.3" class="ltx_text" style="font-size:90%;">, pages
770–778, 2016.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Stefan Hinterstoisser, Olivier Pauly, Hauke Heibel, Marek Martina, and Martin
Bokeloh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">An annotation saved is an annotation earned: Using fully synthetic
training for object detection.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib32.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop of IEEE Conf. Comput. Vis.</span><span id="bib.bib32.5.3" class="ltx_text" style="font-size:90%;">, Oct 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
http://cc0textures.com.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Cctextures dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Creative Commons CC0 1.0 Universal License</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
https://3dmodelhaven.com/.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Haven dataset.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib34.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Creative Commons CC0 1.0 Universal License</span><span id="bib.bib34.5.3" class="ltx_text" style="font-size:90%;">.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Drew A. Hudson and Christopher D. Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Gqa: A new dataset for real-world visual reasoning and compositional
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib35.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib35.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Mona Jalal, Josef Spjut, Ben Boudaoud, and Margrit Betke.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Sidod: A synthetic image dataset for 3d object pose recognition with
distractors.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib36.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 IEEE/CVF Conference on Computer Vision and Pattern
Recognition Workshops (CVPRW)</span><span id="bib.bib36.5.3" class="ltx_text" style="font-size:90%;">, pages 475–477, 2019.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Junguang Jiang, Yang Shu, Jianmin Wang, and Mingsheng Long.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Transferability in deep learning: A survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ArXiv</span><span id="bib.bib37.4.2" class="ltx_text" style="font-size:90%;">, abs/2201.05867, 2022.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Longlong Jing and Yingli Tian.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Self-supervised visual feature learning with deep neural networks: A
survey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span id="bib.bib38.4.2" class="ltx_text" style="font-size:90%;">, 43(11):4037–4058,
2021.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed Qureshi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">A survey of the recent architectures of deep convolutional neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Artif. Intell. Rev.</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 53:5455–5516, 2020.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Donghyun Kim, Kuniaki Saito, Tae-Hyun Oh, Bryan A. Plummer, Stan Sclaroff, and
Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">Cds: Cross-domain self-supervised pre-training.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Int. Conf. Comput. Vis.</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, pages 9123–9132,
October 2021.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Donghyun Kim, Kaihong Wang, Stan Sclaroff, and Kate Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">A broad study of pre-training for domain generalization and
adaptation, 2022.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
Sylvain Gelly, and Neil Houlsby.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Big transfer (bit): General visual representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib42.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Eur. Conf. Comput. Vis.</span><span id="bib.bib42.5.3" class="ltx_text" style="font-size:90%;">, pages 491–507, 2020.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Simon Kornblith, Jonathon Shlens, and Quoc V. Le.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Do better imagenet models transfer better?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib43.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib43.5.3" class="ltx_text" style="font-size:90%;">, June 2019.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander Toshev,
Tom Duerig, James Philbin, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">The unreasonable effectiveness of noisy data for fine-grained
recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Eur. Conf. Comput. Vis.</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 301–320, 2016.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
R. Krishna, Y. Zhu, and O. et al Groth.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. J. Comput. Vis.</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, page 32–73, 2017.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
A. Krizhevsky.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Learning multiple layers of features from tiny images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib46.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Technical report</span><span id="bib.bib46.5.3" class="ltx_text" style="font-size:90%;">, 2009.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Ananya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy
Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text" style="font-size:90%;">Fine-tuning can distort pretrained features and underperform
out-of-distribution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib47.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. on Learn. Rep.</span><span id="bib.bib47.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Deeper, broader and artier domain generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib48.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Computer Vision</span><span id="bib.bib48.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Dongyue Li and Hongyang Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Improved regularization and robustness for fine-tuning in neural
networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text" style="font-size:90%;">In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan, editors, </span><span id="bib.bib49.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib49.5.3" class="ltx_text" style="font-size:90%;">, volume 34, pages
27249–27262. Curran Associates, Inc., 2021.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Lei Li, Ke Gao, Juan Cao, Ziyao Huang, Yepeng Weng, Xiaoyue Mi, Zhengze Yu,
Xiaoya Li, and Boyang Xia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Progressive domain expansion network for single domain
generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib50.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib50.5.3" class="ltx_text" style="font-size:90%;">, pages 224–233, June 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Suichan Li, Dongdong Chen, Yinpeng Chen, Lu Yuan, Lei Zhang, Qi Chu, Bin Liu,
and Nenghai Yu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">Improve unsupervised pretraining for few-label transfer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib51.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Int. Conf. Comput. Vis.</span><span id="bib.bib51.5.3" class="ltx_text" style="font-size:90%;">, pages 10201–10210,
October 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Weixin Liang and James Zou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Metashift: A dataset of datasets for evaluating contextual
distribution shifts and training conflicts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib52.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. on Learn. Rep.</span><span id="bib.bib52.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Microsoft coco: Common objects in context.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib53.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Eur. Conf. Comput. Vis.</span><span id="bib.bib53.5.3" class="ltx_text" style="font-size:90%;">, pages 740–755, 2014.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Hong Liu, Mingsheng Long, Jianmin Wang, and Michael Jordan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">Transferable adversarial training: A general approach to adapting
deep classifiers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib54.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. Mach. Learn.</span><span id="bib.bib54.5.3" class="ltx_text" style="font-size:90%;">, volume 97, pages 4013–4022,
2019.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Ziquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Antoni B. Chan, and Rong Jin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">Improved fine-tuning by leveraging pre-training data: Theory and
practice.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib55.4.2" class="ltx_text" style="font-size:90%;">, abs/2111.12292, 2021.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text" style="font-size:90%;">Fully convolutional networks for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib56.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib56.5.3" class="ltx_text" style="font-size:90%;">, pages
3431–3440, 2015.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y
Ng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">Reading digits in natural images with unsupervised feature learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib57.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop of Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib57.5.3" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
S. J. Pan and Q. Yang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">A survey on transfer learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Trans. Knowl. Data Eng.</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 22:1345–1359, 2010.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Y. Pan, T. Yao, Y. Li, Y. Wang, C. Ngo, and T. Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">Transferrable prototypical networks for unsupervised domain
adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, pages
2234–2242, 2019.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">Automatic differentiation in pytorch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib60.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop of Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib60.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
X. Peng, B. Usman, N. Kaushik, D. Wang, J. Hoffman, and K. Saenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Visda: A synthetic-to-real benchmark for visual domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib61.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop of IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib61.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Aayush Prakash, Shaad Boochoon, Mark Brophy, David Acuna, Eric Cameracci,
Gavriel State, Omer Shapira, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Structured domain randomization: Bridging the reality gap by
context-aware synthetic data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib62.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Robotics and Automation
(ICRA)</span><span id="bib.bib62.5.3" class="ltx_text" style="font-size:90%;">, pages 7249–7255, 2019.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Fengchun Qiao, Long Zhao, and Xi Peng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Learning to learn single domain generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib63.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib63.5.3" class="ltx_text" style="font-size:90%;">, June 2020.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Imagenet-21k pretraining for the masses.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Thirty-fifth Conference on Neural Information Processing
Systems Datasets and Benchmarks Track (Round 1)</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel
Bautista, Nathan Paczan, Russ Webb, and Joshua M. Susskind.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">Hypersim: A photorealistic synthetic dataset for holistic indoor
scene understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib65.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Int. Conf. Comput. Vis.</span><span id="bib.bib65.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text" style="font-size:90%;">
G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M. Lopez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.2.1" class="ltx_text" style="font-size:90%;">The synthia dataset: A large collection of synthetic images for
semantic segmentation of urban scenes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib66.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib66.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib66.5.3" class="ltx_text" style="font-size:90%;">, pages
3234–3243, 2016.
</span>
</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text" style="font-size:90%;">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.2.1" class="ltx_text" style="font-size:90%;">Imagenet large scale visual recognition challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib67.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Int. J. Comput. Vis.</span><span id="bib.bib67.4.2" class="ltx_text" style="font-size:90%;">, 115:211–252, 2015.
</span>
</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text" style="font-size:90%;">
Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao, Sang Michael Xie, Kendrick
Shen, Ananya Kumar, Weihua Hu, Michihiro Yasunaga, Henrik Marklund, Sara
Beery, Etienne David, Ian Stavness, Wei Guo, Jure Leskovec, Kate Saenko,
Tatsunori Hashimoto, Sergey Levine, Chelsea Finn, and Percy Liang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.2.1" class="ltx_text" style="font-size:90%;">Extending the WILDS benchmark for unsupervised adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib68.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib68.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. on Learn. Rep.</span><span id="bib.bib68.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text" style="font-size:90%;">
K. Saito, K. Watanabe, Y. Ushiku, and T. Harada.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.2.1" class="ltx_text" style="font-size:90%;">Maximum classifier discrepancy for unsupervised domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib69.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib69.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib69.5.3" class="ltx_text" style="font-size:90%;">, pages
3723–3732, 2018.
</span>
</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span id="bib.bib70.1.1" class="ltx_text" style="font-size:90%;">
Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam,
Devi Parikh, and Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.2.1" class="ltx_text" style="font-size:90%;">Grad-cam: Visual explanations from deep networks via gradient-based
localization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib70.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib70.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Int. Conf. Comput. Vis.</span><span id="bib.bib70.5.3" class="ltx_text" style="font-size:90%;">, pages 618–626, 2017.
</span>
</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span id="bib.bib71.1.1" class="ltx_text" style="font-size:90%;">
Shai Shalev-Shwartz and Shai Ben-David.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Understanding Machine Learning: From Theory to Algorithms</span><span id="bib.bib71.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib71.4.1" class="ltx_text" style="font-size:90%;">Cambridge University Press, 2014.
</span>
</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text" style="font-size:90%;">
Connor Shorten and Taghi M. Khoshgoftaar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.2.1" class="ltx_text" style="font-size:90%;">A survey on image data augmentation for deep learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib72.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">J. Big Data</span><span id="bib.bib72.4.2" class="ltx_text" style="font-size:90%;">, 6, 2019.
</span>
</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text" style="font-size:90%;">
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.2.1" class="ltx_text" style="font-size:90%;">Deep inside convolutional networks: Visualising image classification
models and saliency maps.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib73.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib73.4.2" class="ltx_text" style="font-size:90%;">, abs/1312.6034, 2014.
</span>
</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text" style="font-size:90%;">
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.2.1" class="ltx_text" style="font-size:90%;">Revisiting unreasonable effectiveness of data in deep learning era.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib74.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib74.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Int. Conf. Comput. Vis.</span><span id="bib.bib74.5.3" class="ltx_text" style="font-size:90%;">, Oct 2017.
</span>
</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span id="bib.bib75.1.1" class="ltx_text" style="font-size:90%;">
Zeren Sun, Yazhou Yao, Xiu-Shen Wei, Yongshun Zhang, Fumin Shen, Jianxin Wu,
Jian Zhang, and Heng Tao Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.2.1" class="ltx_text" style="font-size:90%;">Webly supervised fine-grained recognition: Benchmark datasets and an
approach.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib75.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib75.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Int. Conf. Comput. Vis.</span><span id="bib.bib75.5.3" class="ltx_text" style="font-size:90%;">, pages 10602–10611,
October 2021.
</span>
</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span id="bib.bib76.1.1" class="ltx_text" style="font-size:90%;">
Hui Tang, Ke Chen, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.2.1" class="ltx_text" style="font-size:90%;">Unsupervised domain adaptation via structurally regularized deep
clustering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib76.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib76.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib76.5.3" class="ltx_text" style="font-size:90%;">, pages
8725–8735, 2020.
</span>
</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text" style="font-size:90%;">
Hui Tang and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.2.1" class="ltx_text" style="font-size:90%;">Vicinal and categorical domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib77.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition</span><span id="bib.bib77.4.2" class="ltx_text" style="font-size:90%;">, 115, 2021.
</span>
</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text" style="font-size:90%;">
Hui Tang, Yaowei Wang, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.2.1" class="ltx_text" style="font-size:90%;">Unsupervised domain adaptation via distilled discriminative
clustering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib78.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Pattern Recognition</span><span id="bib.bib78.4.2" class="ltx_text" style="font-size:90%;">, 127:108638, 2022.
</span>
</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text" style="font-size:90%;">
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.2.1" class="ltx_text" style="font-size:90%;">Domain randomization for transferring deep neural networks from
simulation to the real world.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib79.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib79.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE/RSJ International Conference on Intelligent Robots
and Systems (IROS)</span><span id="bib.bib79.5.3" class="ltx_text" style="font-size:90%;">, pages 23–30, 2017.
</span>
</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span id="bib.bib80.1.1" class="ltx_text" style="font-size:90%;">
Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.2.1" class="ltx_text" style="font-size:90%;">Mlp-mixer: An all-mlp architecture for vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib80.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib80.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib80.5.3" class="ltx_text" style="font-size:90%;">, volume 34, pages 24261–24272,
2021.
</span>
</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text" style="font-size:90%;">
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem
Anil, Thang To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.2.1" class="ltx_text" style="font-size:90%;">Training deep networks with synthetic data: Bridging the reality gap
by domain randomization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib81.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib81.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop of IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib81.5.3" class="ltx_text" style="font-size:90%;">, June
2018.
</span>
</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text" style="font-size:90%;">
Laurens van der Maaten and Geoffrey Hinton.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.2.1" class="ltx_text" style="font-size:90%;">Visualizing data using t-sne.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib82.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journ. of Mach. Learn. Res.</span><span id="bib.bib82.4.2" class="ltx_text" style="font-size:90%;">, 9:2579–2605, 2008.
</span>
</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text" style="font-size:90%;">
VSR Veeravasarapu, Constantin Rothkopf, and Ramesh Visvanathan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.2.1" class="ltx_text" style="font-size:90%;">Adversarially tuned scene generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib83.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib83.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</span><span id="bib.bib83.5.3" class="ltx_text" style="font-size:90%;">, July 2017.
</span>
</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text" style="font-size:90%;">
VSR Veeravasarapu, Constantin Rothkopf, and Ramesh Visvanathan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.2.1" class="ltx_text" style="font-size:90%;">Model-driven simulations for computer vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib84.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib84.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Winter Conference on Applications of Computer
Vision (WACV)</span><span id="bib.bib84.5.3" class="ltx_text" style="font-size:90%;">, pages 1063–1071, 2017.
</span>
</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text" style="font-size:90%;">
V. S. R. Veeravasarapu, Rudra Narayan Hota, Constantin A. Rothkopf, and
Visvanathan Ramesh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.2.1" class="ltx_text" style="font-size:90%;">Simulations for validation of vision systems.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib85.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib85.4.2" class="ltx_text" style="font-size:90%;">, abs/1512.01030, 2015.
</span>
</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text" style="font-size:90%;">
Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C Duchi, Vittorio Murino,
and Silvio Savarese.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.2.1" class="ltx_text" style="font-size:90%;">Generalizing to unseen domains via adversarial data augmentation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib86.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib86.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib86.5.3" class="ltx_text" style="font-size:90%;">, volume 31, 2018.
</span>
</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text" style="font-size:90%;">
Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.2.1" class="ltx_text" style="font-size:90%;">Generalizing to unseen domains: A survey on domain generalization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.3.1" class="ltx_text" style="font-size:90%;">In Zhi-Hua Zhou, editor, </span><span id="bib.bib87.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Jo. Conf. of Artif.
Intell.</span><span id="bib.bib87.5.3" class="ltx_text" style="font-size:90%;">, pages 4627–4635. International Joint Conferences on Artificial
Intelligence Organization, 8 2021.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib87.6.1" class="ltx_text" style="font-size:90%;">Survey Track.
</span>
</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text" style="font-size:90%;">
Yizhou Wang, Shixiang Tang, Feng Zhu, Lei Bai, Rui Zhao, Donglian Qi, and Wanli
Ouyang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.2.1" class="ltx_text" style="font-size:90%;">Revisiting the transferability of supervised pretraining: an MLP
perspective.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib88.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib88.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib88.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text" style="font-size:90%;">
Garrett Wilson and Diane J. Cook.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.2.1" class="ltx_text" style="font-size:90%;">A survey of unsupervised deep domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib89.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Trans. Intell. Syst. Technol.</span><span id="bib.bib89.4.2" class="ltx_text" style="font-size:90%;">, 11(5), Jul 2020.
</span>
</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock"><span id="bib.bib90.1.1" class="ltx_text" style="font-size:90%;">
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.2.1" class="ltx_text" style="font-size:90%;">How transferable are features in deep neural networks?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib90.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib90.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib90.5.3" class="ltx_text" style="font-size:90%;">, pages 3320–3328, 2014.
</span>
</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock"><span id="bib.bib91.1.1" class="ltx_text" style="font-size:90%;">
Yaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.2.1" class="ltx_text" style="font-size:90%;">Learning diverse and discriminative representations via the principle
of maximal coding rate reduction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib91.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib91.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Neur. Info. Proc. Sys.</span><span id="bib.bib91.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text" style="font-size:90%;">
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.2.1" class="ltx_text" style="font-size:90%;">mixup: Beyond empirical risk minimization.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib92.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib92.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. Int. Conf. on Learn. Rep.</span><span id="bib.bib92.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock"><span id="bib.bib93.1.1" class="ltx_text" style="font-size:90%;">
Yabin Zhang, Hui Tang, Kui Jia, and Mingkui Tan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.2.1" class="ltx_text" style="font-size:90%;">Domain-symmetric networks for adversarial domain adaptation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib93.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib93.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proc. IEEE Conf. Comput. Vis. Pattern Recognit.</span><span id="bib.bib93.5.3" class="ltx_text" style="font-size:90%;">, pages
5026–5035, 2019.
</span>
</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text" style="font-size:90%;">
Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui
Xiong, and Qing He.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.2.1" class="ltx_text" style="font-size:90%;">A comprehensive survey on transfer learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib94.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE</span><span id="bib.bib94.4.2" class="ltx_text" style="font-size:90%;">, 109(1):43–76, 2021.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div id="p1" class="ltx_para ltx_noindent">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text" style="font-size:173%;">Appendix for 
<br class="ltx_break">A New Benchmark: On the Utility of Synthetic Data with Blender for 
<br class="ltx_break">Bare Supervised Learning and Downstream Domain Adaptation
<br class="ltx_break"></span></p>
</div>
<div id="p2" class="ltx_para">
<p id="p2.1" class="ltx_p"><span id="p2.1.1" class="ltx_text" style="font-size:173%;">The catalog of this appendix is in the following.</span></p>
<ul id="A0.I1" class="ltx_itemize">
<li id="A0.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i1.p1" class="ltx_para">
<p id="A0.I1.i1.p1.1" class="ltx_p"><span id="A0.I1.i1.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A1" title="Appendix A Our Main Contributions ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A</span></a><span id="A0.I1.i1.p1.1.2" class="ltx_text" style="font-size:173%;"> summarizes our main contributions.</span></p>
</div>
</li>
<li id="A0.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i2.p1" class="ltx_para">
<p id="A0.I1.i2.p1.1" class="ltx_p"><span id="A0.I1.i2.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A2" title="Appendix B Summary of Paper Novelty ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">B</span></a><span id="A0.I1.i2.p1.1.2" class="ltx_text" style="font-size:173%;"> provides a well-organized summary for the paper novelty.</span></p>
</div>
</li>
<li id="A0.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i3.p1" class="ltx_para">
<p id="A0.I1.i3.p1.1" class="ltx_p"><span id="A0.I1.i3.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A3" title="Appendix C More Clarifications on Our Empirical Study ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">C</span></a><span id="A0.I1.i3.p1.1.2" class="ltx_text" style="font-size:173%;"> makes more clarifications on our empirical study.</span></p>
</div>
</li>
<li id="A0.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i4.p1" class="ltx_para">
<p id="A0.I1.i4.p1.1" class="ltx_p"><span id="A0.I1.i4.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A4.SS1" title="D.1 Examining Learning Process ‣ Appendix D Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">D.1</span></a><span id="A0.I1.i4.p1.1.2" class="ltx_text" style="font-size:173%;"> examines the learning process, Sec. </span><a href="#A4.SS2" title="D.2 Visualizing Saliency Map ‣ Appendix D Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">D.2</span></a><span id="A0.I1.i4.p1.1.3" class="ltx_text" style="font-size:173%;"> visualizes the saliency map, and Sec. </span><a href="#A4.SS3" title="D.3 More Impact of Data Augmentations ‣ Appendix D Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">D.3</span></a><span id="A0.I1.i4.p1.1.4" class="ltx_text" style="font-size:173%;"> depicts more impact of data augmentations for the comprehensive comparison between fixed-dataset periodic training and training on non-repetitive samples.</span></p>
</div>
</li>
<li id="A0.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i5.p1" class="ltx_para">
<p id="A0.I1.i5.p1.1" class="ltx_p"><span id="A0.I1.i5.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A5" title="Appendix E Evaluating Various Network Architectures ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">E</span></a><span id="A0.I1.i5.p1.1.2" class="ltx_text" style="font-size:173%;"> evaluates various network architectures by plotting their learning curves.</span></p>
</div>
</li>
<li id="A0.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i6.p1" class="ltx_para">
<p id="A0.I1.i6.p1.1" class="ltx_p"><span id="A0.I1.i6.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A6" title="Appendix F Comparing Pre-training for Domain Adaptation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">F</span></a><span id="A0.I1.i6.p1.1.2" class="ltx_text" style="font-size:173%;"> shows the learning process of different pre-training data using domain adaptation as the downstream task.</span></p>
</div>
</li>
<li id="A0.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i7.p1" class="ltx_para">
<p id="A0.I1.i7.p1.1" class="ltx_p"><span id="A0.I1.i7.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A7" title="Appendix G More Details on Our Proposed S2RDA Benchmark ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">G</span></a><span id="A0.I1.i7.p1.1.2" class="ltx_text" style="font-size:173%;"> presents more details on our proposed S2RDA benchmark, e.g., comparing synthetic data with real data from different angles.</span></p>
</div>
</li>
<li id="A0.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i8.p1" class="ltx_para">
<p id="A0.I1.i8.p1.1" class="ltx_p"><span id="A0.I1.i8.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A8" title="Appendix H Other Implementation Details ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">H</span></a><span id="A0.I1.i8.p1.1.2" class="ltx_text" style="font-size:173%;"> provides other implementation details for supervised learning/pre-training and downstream domain adaptation.</span></p>
</div>
</li>
<li id="A0.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A0.I1.i9.p1" class="ltx_para">
<p id="A0.I1.i9.p1.1" class="ltx_p"><span id="A0.I1.i9.p1.1.1" class="ltx_text" style="font-size:173%;">Sec. </span><a href="#A9" title="Appendix I Other Related Works ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">I</span></a><span id="A0.I1.i9.p1.1.2" class="ltx_text" style="font-size:173%;"> reviews other related works on real datasets, data manipulation </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A0.I1.i9.p1.1.3.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a><span id="A0.I1.i9.p1.1.4.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A0.I1.i9.p1.1.5" class="ltx_text" style="font-size:173%;">, deep models </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A0.I1.i9.p1.1.6.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib80" title="" class="ltx_ref">80</a><span id="A0.I1.i9.p1.1.7.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A0.I1.i9.p1.1.8" class="ltx_text" style="font-size:173%;">, transfer learning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A0.I1.i9.p1.1.9.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="A0.I1.i9.p1.1.10.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A0.I1.i9.p1.1.11" class="ltx_text" style="font-size:173%;">, domain adaptation </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A0.I1.i9.p1.1.12.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib89" title="" class="ltx_ref">89</a><span id="A0.I1.i9.p1.1.13.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A0.I1.i9.p1.1.14" class="ltx_text" style="font-size:173%;">, and OOD generalization </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A0.I1.i9.p1.1.15.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="A0.I1.i9.p1.1.16.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A0.I1.i9.p1.1.17" class="ltx_text" style="font-size:173%;">.</span></p>
</div>
</li>
</ul>
</div>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Our Main Contributions</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p"><span id="A1.p1.1.1" class="ltx_text" style="font-size:173%;">Our main contributions are summarized as follows.</span></p>
<ul id="A1.I1" class="ltx_itemize">
<li id="A1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i1.p1" class="ltx_para">
<p id="A1.I1.i1.p1.1" class="ltx_p"><span id="A1.I1.i1.p1.1.1" class="ltx_text" style="font-size:173%;">On the well-controlled IID experimental condition enabled by 3D rendering, we empirically verify the typical insights on shortcut learning, PAC generalization, and variance-bias trade-off, and explore the effects of changing data regimes and network structures on model generalization. The key design wherein is to compare the traditional fixed-dataset periodic training with a new strategy of training on non-repetitive samples.</span></p>
</div>
</li>
<li id="A1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i2.p1" class="ltx_para">
<p id="A1.I1.i2.p1.1" class="ltx_p"><span id="A1.I1.i2.p1.1.1" class="ltx_text" style="font-size:173%;">We explore how variation factors of an image affect the model generalization, e.g., object scale, material texture, illumination, camera viewpoint, and background, and in return provide new perceptions for data generation.</span></p>
</div>
</li>
<li id="A1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i3.p1" class="ltx_para">
<p id="A1.I1.i3.p1.1" class="ltx_p"><span id="A1.I1.i3.p1.1.1" class="ltx_text" style="font-size:173%;">Using the popular simulation-to-real classification adaptation as a downstream task, we investigate how synthetic data pre-training performs by comparing with pre-training on real data. We have some surprising and important discoveries including synthetic data pre-training is also prospective and a promising paradigm of pre-training on big synthetic data together with small real data is proposed for realistic supervised pre-training.</span></p>
</div>
</li>
<li id="A1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A1.I1.i4.p1" class="ltx_para">
<p id="A1.I1.i4.p1.1" class="ltx_p"><span id="A1.I1.i4.p1.1.1" class="ltx_text" style="font-size:173%;">We propose a more large-scale synthetic-to-real benchmark for classification adaptation (termed S2RDA), on which we also provide a baseline performance analysis for representative DA approaches.</span></p>
</div>
</li>
</ul>
</div>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Summary of Paper Novelty</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p"><span id="A2.p1.1.1" class="ltx_text" style="font-size:173%;">Now it is becoming more and more important to work on methods that use simulated data but perform well in practical domains whose data or annotation are difficult to acquire, e.g., medical imaging. However, previous research works </span><em id="A2.p1.1.2" class="ltx_emph ltx_font_italic" style="font-size:173%;">have not</em><span id="A2.p1.1.3" class="ltx_text" style="font-size:173%;"> studied various factors on a synthesized dataset for image classification and domain adaptation comprehensively and systematically. To fill the gap, we present </span><em id="A2.p1.1.4" class="ltx_emph ltx_font_italic" style="font-size:173%;">the first work</em><span id="A2.p1.1.5" class="ltx_text" style="font-size:173%;">, ranging from bare supervised learning to downstream domain adaptation. It provides many new, valuable learning insights for OOD/real data generalization, though the verification of some existing, known theories in our well-controlled IID experimental condition has also been done for comprehensive coverage. It is essential for synthetic data learning analysis, which is </span><em id="A2.p1.1.6" class="ltx_emph ltx_font_italic" style="font-size:173%;">completely missing</em><span id="A2.p1.1.7" class="ltx_text" style="font-size:173%;"> in the context of image classification. We clarify the paper novelty below.</span></p>
</div>
<div id="A2.p2" class="ltx_para">
<ul id="A2.I1" class="ltx_itemize">
<li id="A2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i1.p1.1.1" class="ltx_text" style="font-size:173%;">The motivation that we utilize synthetic data to verify typical theories and expose new findings is novel. Real data are noisy and uncontrolled, which may hinder the verification of typical theories and exposure to new findings. In the context of image classification, existing works verify classical theories and reveal new findings on real data. However, the process of acquiring real data cannot be controlled, the annotation accuracy cannot be guaranteed, and there may be duplicate images in the training set and test set, which leads to the fact that the training set and test set are no longer independent and identically distributed (IID). To remedy them, we resort to synthetic data generated by 3D rendering with domain randomization.</span></p>
</div>
</li>
<li id="A2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i2.p1.1.1" class="ltx_text" style="font-size:173%;">The comparison between fixed-dataset periodic training and training on non-repetitive samples and the study of shortcut learning on our synthesized dataset are novel. We admit that some of our findings are classical theories, e.g., PAC generalization and variance-bias trade-off, which should be verified when one introduces a new dataset. We introduce a new dataset of synthetic data and thus do such a study for comprehensive coverage, which first compares fixed-dataset periodic training with training on non-repetitive samples generated by 3D rendering. Particularly, we also verify a recent, significant perspective of shortcut learning and design new experiments to demonstrate that randomizing the variation factors of training images can block shortcut solutions that rely on context clues in the background.</span></p>
</div>
</li>
<li id="A2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.p1.1.1" class="ltx_text" style="font-size:173%;">Investigating the learning characteristics and properties of our synthesized new dataset comprehensively is novel, and our experiments yield many interesting and valuable observations. Synthetic data are cheap, label-rich, and well-controlled, but there hasn’t been a comprehensive study of bare supervised learning on synthetic data in the context of image classification. To our knowledge, we are the first to investigate the learning characteristics and properties of our synthesized new dataset comprehensively, in terms of refreshed architecture, model capacity, training data quantity, data augmentation, and rendering variations. The empirical study on bare supervised learning yields many new findings, e.g.,</span></p>
<ul id="A2.I1.i3.I1" class="ltx_itemize">
<li id="A2.I1.i3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i3.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i3.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i3.I1.i1.p1.1.1" class="ltx_text" style="font-size:173%;">IID and OOD generalizations are some type of zero-sum game,</span></p>
</div>
</li>
<li id="A2.I1.i3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i3.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i3.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i3.I1.i2.p1.1.1" class="ltx_text" style="font-size:173%;">ViT performs surprisingly poorly,</span></p>
</div>
</li>
<li id="A2.I1.i3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i3.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i3.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i3.I1.i3.p1.1.1" class="ltx_text" style="font-size:173%;">there is always a bottleneck from synthetic data to OOD/real data,</span></p>
</div>
</li>
<li id="A2.I1.i3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i3.I1.i4.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i3.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i3.I1.i4.p1.1.1" class="ltx_text" style="font-size:173%;">neural architecture search (NAS) should also consider the search for data augmentation, and</span></p>
</div>
</li>
<li id="A2.I1.i3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i3.I1.i5.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i3.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i3.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i3.I1.i5.p1.1.1" class="ltx_text" style="font-size:173%;">different factors and even their different values have uneven importance to IID generalization.</span></p>
</div>
</li>
</ul>
</div>
</li>
<li id="A2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i4.p1.1.1" class="ltx_text" style="font-size:173%;">Synthetic data pre-training, its comparison to real data pre-training, and its application to downstream synthetic-to-real classification adaptation are novel, and our experiments yield many interesting and valuable observations. To our knowledge, there is little research on pre-training for domain adaptation. Kin et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.I1.i4.p1.1.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="A2.I1.i4.p1.1.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A2.I1.i4.p1.1.4" class="ltx_text" style="font-size:173%;"> preliminarily study the effects of real data pre-training on domain transfer tasks. Differently, we focus on the learning utility of synthetic data and take the first step towards clearing the cloud of mystery surrounding how different pre-training schemes including synthetic data pre-training affect the practical, large-scale synthetic-to-real classification adaptation. Besides, we first study and compare pre-training on the latest MetaShift dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.I1.i4.p1.1.5.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib52" title="" class="ltx_ref">52</a><span id="A2.I1.i4.p1.1.6.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A2.I1.i4.p1.1.7" class="ltx_text" style="font-size:173%;">. The empirical study on downstream domain adaptation yields many new findings, e.g.,</span></p>
<ul id="A2.I1.i4.I1" class="ltx_itemize">
<li id="A2.I1.i4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i4.I1.i1.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i4.I1.i1.p1" class="ltx_para">
<p id="A2.I1.i4.I1.i1.p1.1" class="ltx_p"><span id="A2.I1.i4.I1.i1.p1.1.1" class="ltx_text" style="font-size:173%;">DA fails without pre-training,</span></p>
</div>
</li>
<li id="A2.I1.i4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i4.I1.i2.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i4.I1.i2.p1" class="ltx_para">
<p id="A2.I1.i4.I1.i2.p1.1" class="ltx_p"><span id="A2.I1.i4.I1.i2.p1.1.1" class="ltx_text" style="font-size:173%;">different DA methods exhibit different relative advantages under different pre-training data,</span></p>
</div>
</li>
<li id="A2.I1.i4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i4.I1.i3.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i4.I1.i3.p1" class="ltx_para">
<p id="A2.I1.i4.I1.i3.p1.1" class="ltx_p"><span id="A2.I1.i4.I1.i3.p1.1.1" class="ltx_text" style="font-size:173%;">the reliability of existing DA method evaluation criteria is unguaranteed,</span></p>
</div>
</li>
<li id="A2.I1.i4.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i4.I1.i4.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i4.I1.i4.p1" class="ltx_para">
<p id="A2.I1.i4.I1.i4.p1.1" class="ltx_p"><span id="A2.I1.i4.I1.i4.p1.1.1" class="ltx_text" style="font-size:173%;">synthetic data pre-training is better than pre-training on real data (e.g., ImageNet) in our study, and</span></p>
</div>
</li>
<li id="A2.I1.i4.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span id="A2.I1.i4.I1.i5.1.1.1" class="ltx_text ltx_font_bold">–</span></span> 
<div id="A2.I1.i4.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i4.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i4.I1.i5.p1.1.1" class="ltx_text" style="font-size:173%;">Big Synthesis Small Real is worth deeply researching.</span></p>
</div>
</li>
</ul>
</div>
</li>
<li id="A2.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="A2.I1.i5.p1" class="ltx_para">
<p id="A2.I1.i5.p1.1" class="ltx_p"><span id="A2.I1.i5.p1.1.1" class="ltx_text" style="font-size:173%;">Our introduced S2RDA benchmark is novel and will advance the field of domain adaptation research on transfer from synthetic to real.</span></p>
</div>
</li>
</ul>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p"><span id="A2.p3.1.1" class="ltx_text" style="font-size:173%;">Our findings may challenge some of the current conclusions, but they also shed some light on the important fields in computer vision and take a step towards uncovering the mystery of deep learning.</span></p>
</div>
<figure id="A2.T1" class="ltx_table">
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_table"><span id="A2.T1.4.1.1" class="ltx_text" style="font-size:52%;">Table A1</span>: </span><span id="A2.T1.5.2" class="ltx_text" style="font-size:52%;">Domain adaptation performance on SubVisDA-10 with varied pre-training schemes (ResNet-50). Green or red: Best Acc. or Mean in each row (among compared DA methods).</span></figcaption>
<div id="A2.T1.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:39.2pt;vertical-align:-0.4pt;"><span class="ltx_transformed_inner" style="transform:translate(-272.2pt,25.6pt) scale(0.43075929756393,0.43075929756393) ;">
<table id="A2.T1.6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T1.6.1.1.1" class="ltx_tr">
<th id="A2.T1.6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="A2.T1.6.1.1.1.1.1" class="ltx_text" style="font-size:173%;">Pre-training Data</span></th>
<th id="A2.T1.6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="A2.T1.6.1.1.1.2.1" class="ltx_text" style="font-size:173%;"># Iters</span></th>
<th id="A2.T1.6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" rowspan="2"><span id="A2.T1.6.1.1.1.3.1" class="ltx_text" style="font-size:173%;"># Epochs</span></th>
<th id="A2.T1.6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A2.T1.6.1.1.1.4.1" class="ltx_text" style="font-size:173%;">No Adaptation</span></th>
<th id="A2.T1.6.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A2.T1.6.1.1.1.5.1" class="ltx_text" style="font-size:173%;">DANN</span></th>
<th id="A2.T1.6.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A2.T1.6.1.1.1.6.1" class="ltx_text" style="font-size:173%;">MCD</span></th>
<th id="A2.T1.6.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A2.T1.6.1.1.1.7.1" class="ltx_text" style="font-size:173%;">RCA</span></th>
<th id="A2.T1.6.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A2.T1.6.1.1.1.8.1" class="ltx_text" style="font-size:173%;">SRDC</span></th>
<th id="A2.T1.6.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2"><span id="A2.T1.6.1.1.1.9.1" class="ltx_text" style="font-size:173%;">DisClusterDA</span></th>
</tr>
<tr id="A2.T1.6.1.2.2" class="ltx_tr">
<th id="A2.T1.6.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.2.2.1.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">Acc.</span></th>
<th id="A2.T1.6.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A2.T1.6.1.2.2.2.1" class="ltx_text" style="font-size:173%;">Mean</span></th>
<th id="A2.T1.6.1.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.2.2.3.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">Acc.</span></th>
<th id="A2.T1.6.1.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A2.T1.6.1.2.2.4.1" class="ltx_text" style="font-size:173%;">Mean</span></th>
<th id="A2.T1.6.1.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.2.2.5.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">Acc.</span></th>
<th id="A2.T1.6.1.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A2.T1.6.1.2.2.6.1" class="ltx_text" style="font-size:173%;">Mean</span></th>
<th id="A2.T1.6.1.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.2.2.7.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">Acc.</span></th>
<th id="A2.T1.6.1.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A2.T1.6.1.2.2.8.1" class="ltx_text" style="font-size:173%;">Mean</span></th>
<th id="A2.T1.6.1.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.2.2.9.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">Acc.</span></th>
<th id="A2.T1.6.1.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A2.T1.6.1.2.2.10.1" class="ltx_text" style="font-size:173%;">Mean</span></th>
<th id="A2.T1.6.1.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.2.2.11.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">Acc.</span></th>
<th id="A2.T1.6.1.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="A2.T1.6.1.2.2.12.1" class="ltx_text" style="font-size:173%;">Mean</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T1.6.1.3.1" class="ltx_tr">
<th id="A2.T1.6.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="A2.T1.6.1.3.1.1.1" class="ltx_text" style="font-size:173%;">ImageNet-990</span></th>
<th id="A2.T1.6.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="A2.T1.6.1.3.1.2.1" class="ltx_text" style="font-size:173%;">200K</span></th>
<th id="A2.T1.6.1.3.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="A2.T1.6.1.3.1.3.1" class="ltx_text" style="font-size:173%;">10</span></th>
<td id="A2.T1.6.1.3.1.4" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.3.1.4.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">50.11</span></td>
<td id="A2.T1.6.1.3.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.6.1.3.1.5.1" class="ltx_text" style="font-size:173%;">45.45</span></td>
<td id="A2.T1.6.1.3.1.6" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.3.1.6.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">55.68</span></td>
<td id="A2.T1.6.1.3.1.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.6.1.3.1.7.1" class="ltx_text" style="font-size:173%;">54.67</span></td>
<td id="A2.T1.6.1.3.1.8" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.3.1.8.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">58.84</span></td>
<td id="A2.T1.6.1.3.1.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.6.1.3.1.9.1" class="ltx_text" style="font-size:173%;">58.44</span></td>
<td id="A2.T1.6.1.3.1.10" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.3.1.10.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">58.79</span></td>
<td id="A2.T1.6.1.3.1.11" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.6.1.3.1.11.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:173%;border-color: #FF0000;">60.18</span></td>
<td id="A2.T1.6.1.3.1.12" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.3.1.12.1" class="ltx_text" style="font-size:173%;background-color:#00FF00;">60.25</span></td>
<td id="A2.T1.6.1.3.1.13" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.6.1.3.1.13.1" class="ltx_text" style="font-size:173%;">57.68</span></td>
<td id="A2.T1.6.1.3.1.14" class="ltx_td ltx_align_center ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.3.1.14.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">57.62</span></td>
<td id="A2.T1.6.1.3.1.15" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.6.1.3.1.15.1" class="ltx_text" style="font-size:173%;">57.42</span></td>
</tr>
<tr id="A2.T1.6.1.4.2" class="ltx_tr">
<th id="A2.T1.6.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T1.6.1.4.2.1.1" class="ltx_text" style="font-size:173%;">ImageNet-990+Ours</span></th>
<th id="A2.T1.6.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A2.T1.6.1.4.2.2.1" class="ltx_text" style="font-size:173%;">200K</span></th>
<th id="A2.T1.6.1.4.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="A2.T1.6.1.4.2.3.1" class="ltx_text" style="font-size:173%;">9</span></th>
<td id="A2.T1.6.1.4.2.4" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.4.2.4.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">52.87</span></td>
<td id="A2.T1.6.1.4.2.5" class="ltx_td ltx_align_center"><span id="A2.T1.6.1.4.2.5.1" class="ltx_text ltx_font_bold" style="font-size:173%;">48.85</span></td>
<td id="A2.T1.6.1.4.2.6" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.4.2.6.1" class="ltx_text ltx_font_bold" style="font-size:173%;background-color:#C7C7C7;">58.42</span></td>
<td id="A2.T1.6.1.4.2.7" class="ltx_td ltx_align_center"><span id="A2.T1.6.1.4.2.7.1" class="ltx_text ltx_font_bold" style="font-size:173%;">58.02</span></td>
<td id="A2.T1.6.1.4.2.8" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.4.2.8.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">60.52</span></td>
<td id="A2.T1.6.1.4.2.9" class="ltx_td ltx_align_center"><span id="A2.T1.6.1.4.2.9.1" class="ltx_text ltx_font_bold" style="font-size:173%;">62.27</span></td>
<td id="A2.T1.6.1.4.2.10" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.4.2.10.1" class="ltx_text ltx_font_bold" style="font-size:173%;background-color:#C7C7C7;">62.28</span></td>
<td id="A2.T1.6.1.4.2.11" class="ltx_td ltx_align_center"><span id="A2.T1.6.1.4.2.11.1" class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" style="font-size:173%;border-color: #FF0000;">63.35</span></td>
<td id="A2.T1.6.1.4.2.12" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.4.2.12.1" class="ltx_text ltx_font_bold" style="font-size:173%;background-color:#00FF00;">63.60</span></td>
<td id="A2.T1.6.1.4.2.13" class="ltx_td ltx_align_center"><span id="A2.T1.6.1.4.2.13.1" class="ltx_text ltx_font_bold" style="font-size:173%;">60.89</span></td>
<td id="A2.T1.6.1.4.2.14" class="ltx_td ltx_align_center" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.4.2.14.1" class="ltx_text ltx_font_bold" style="font-size:173%;background-color:#C7C7C7;">61.90</span></td>
<td id="A2.T1.6.1.4.2.15" class="ltx_td ltx_align_center"><span id="A2.T1.6.1.4.2.15.1" class="ltx_text ltx_font_bold" style="font-size:173%;">63.10</span></td>
</tr>
<tr id="A2.T1.6.1.5.3" class="ltx_tr">
<th id="A2.T1.6.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.1.1" class="ltx_text" style="font-size:173%;">ImageNet</span></th>
<th id="A2.T1.6.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.2.1" class="ltx_text" style="font-size:173%;">200K</span></th>
<th id="A2.T1.6.1.5.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.3.1" class="ltx_text" style="font-size:173%;">10</span></th>
<td id="A2.T1.6.1.5.3.4" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.5.3.4.1" class="ltx_text ltx_font_bold" style="font-size:173%;background-color:#C7C7C7;">53.24</span></td>
<td id="A2.T1.6.1.5.3.5" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.5.1" class="ltx_text" style="font-size:173%;">45.38</span></td>
<td id="A2.T1.6.1.5.3.6" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.5.3.6.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">57.77</span></td>
<td id="A2.T1.6.1.5.3.7" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.7.1" class="ltx_text" style="font-size:173%;">55.59</span></td>
<td id="A2.T1.6.1.5.3.8" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.5.3.8.1" class="ltx_text ltx_font_bold" style="font-size:173%;background-color:#C7C7C7;">61.90</span></td>
<td id="A2.T1.6.1.5.3.9" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="font-size:173%;border-color: #FF0000;">61.75</span></td>
<td id="A2.T1.6.1.5.3.10" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.5.3.10.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">61.59</span></td>
<td id="A2.T1.6.1.5.3.11" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.11.1" class="ltx_text" style="font-size:173%;">60.72</span></td>
<td id="A2.T1.6.1.5.3.12" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.5.3.12.1" class="ltx_text" style="font-size:173%;background-color:#00FF00;">62.56</span></td>
<td id="A2.T1.6.1.5.3.13" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.13.1" class="ltx_text" style="font-size:173%;">59.24</span></td>
<td id="A2.T1.6.1.5.3.14" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" style="background-color:#C7C7C7;"><span id="A2.T1.6.1.5.3.14.1" class="ltx_text" style="font-size:173%;background-color:#C7C7C7;">61.18</span></td>
<td id="A2.T1.6.1.5.3.15" class="ltx_td ltx_align_center ltx_border_bb ltx_border_t"><span id="A2.T1.6.1.5.3.15.1" class="ltx_text" style="font-size:173%;">59.01</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>More Clarifications on Our Empirical Study</h2>

<section id="A3.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Necessity of domain randomization study in Table 2.</h5>

<div id="A3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="A3.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:173%;">We expect that assessing variation factors should be essential for synthesizing data for image classification, which is </span><em id="A3.SS0.SSS0.Px1.p1.1.2" class="ltx_emph ltx_font_italic" style="font-size:173%;">missing</em><span id="A3.SS0.SSS0.Px1.p1.1.3" class="ltx_text" style="font-size:173%;"> in previous work </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A3.SS0.SSS0.Px1.p1.1.4.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib61" title="" class="ltx_ref">61</a><span id="A3.SS0.SSS0.Px1.p1.1.5.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A3.SS0.SSS0.Px1.p1.1.6" class="ltx_text" style="font-size:173%;">. We admit that such a study has been done for detection and scene understanding in prior methods </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A3.SS0.SSS0.Px1.p1.1.7.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a><span id="A3.SS0.SSS0.Px1.p1.1.8.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A3.SS0.SSS0.Px1.p1.1.9" class="ltx_text" style="font-size:173%;">, but the generalizability of those results to image classification is </span><em id="A3.SS0.SSS0.Px1.p1.1.10" class="ltx_emph ltx_font_italic" style="font-size:173%;">lack of guarantee</em><span id="A3.SS0.SSS0.Px1.p1.1.11" class="ltx_text" style="font-size:173%;">. Thus, we follow them and do the study for image classification, where we consider a </span><em id="A3.SS0.SSS0.Px1.p1.1.12" class="ltx_emph ltx_font_italic" style="font-size:173%;">different</em><span id="A3.SS0.SSS0.Px1.p1.1.13" class="ltx_text" style="font-size:173%;"> set of factors (e.g., light, texture, and flying distractors in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A3.SS0.SSS0.Px1.p1.1.14.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib81" title="" class="ltx_ref">81</a><span id="A3.SS0.SSS0.Px1.p1.1.15.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A3.SS0.SSS0.Px1.p1.1.16" class="ltx_text" style="font-size:173%;">). It may be expected that a fixed value would underperform randomized values, but how much each factor and each value degrade is </span><em id="A3.SS0.SSS0.Px1.p1.1.17" class="ltx_emph ltx_font_italic" style="font-size:173%;">unknown</em><span id="A3.SS0.SSS0.Px1.p1.1.18" class="ltx_text" style="font-size:173%;">. One cannot know how important they are without such a </span><em id="A3.SS0.SSS0.Px1.p1.1.19" class="ltx_emph ltx_font_italic" style="font-size:173%;">quantitative</em><span id="A3.SS0.SSS0.Px1.p1.1.20" class="ltx_text" style="font-size:173%;"> study. Insightfully, our new results in Table 2 also suggest that the under-explored direction of weighted rendering is worth studying and provide preliminary guidance/prior knowledge for learning factor distribution.</span></p>
</div>
</section>
<section id="A3.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Additonal experiments on ImageNet-990+Ours improving over ImageNet-990.</h5>

<div id="A3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px2.p1.5" class="ltx_p"><span id="A3.SS0.SSS0.Px2.p1.5.1" class="ltx_text" style="font-size:173%;">To further justify that ImageNet-990+Ours improves over ImageNet-990, we do additional experiments by using a different cosine decay schedule for the learning rate: </span><math id="A3.SS0.SSS0.Px2.p1.1.m1.3" class="ltx_Math" alttext="\eta_{p}=\eta_{1}+0.5(\eta_{0}-\eta_{1})(1+\cos(\pi p))" display="inline"><semantics id="A3.SS0.SSS0.Px2.p1.1.m1.3a"><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.3.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.cmml"><msub id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.cmml"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.2.cmml">η</mi><mi mathsize="172%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.3.cmml">p</mi></msub><mo mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.3.cmml">=</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.cmml"><msub id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.cmml"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.2.cmml">η</mi><mn mathsize="172%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.3.cmml">1</mn></msub><mo mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.3.cmml">+</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.cmml"><mn mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.4" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.4.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.3.cmml">​</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.cmml"><mo maxsize="173%" minsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.cmml">(</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.cmml"><msub id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.cmml"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.2.cmml">η</mi><mn mathsize="172%" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.3.cmml">0</mn></msub><mo mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.1.cmml">−</mo><msub id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.cmml"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.2.cmml">η</mi><mn mathsize="172%" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo maxsize="173%" minsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.3a" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.3.cmml">​</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.cmml"><mo maxsize="173%" minsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.cmml">(</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.cmml"><mn mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.3.cmml">1</mn><mo mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.2.cmml">+</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.2.cmml"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">cos</mi><mo id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1a" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.2.cmml">⁡</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.2.cmml"><mo maxsize="173%" minsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.2.cmml">(</mo><mrow id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.cmml"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.2" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.2.cmml">π</mi><mo lspace="0em" rspace="0em" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.1" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.1.cmml">​</mo><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.3.cmml">p</mi></mrow><mo maxsize="173%" minsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo maxsize="173%" minsize="173%" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.3" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.1.m1.3b"><apply id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3"><eq id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.3"></eq><apply id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.2">𝜂</ci><ci id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.4.3">𝑝</ci></apply><apply id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2"><plus id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.3"></plus><apply id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.2">𝜂</ci><cn type="integer" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.4.3">1</cn></apply><apply id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2"><times id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.3"></times><cn type="float" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.4.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.4">0.5</cn><apply id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1"><minus id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.1"></minus><apply id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.2">𝜂</ci><cn type="integer" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.2.3">0</cn></apply><apply id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.2">𝜂</ci><cn type="integer" id="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1"><plus id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.2"></plus><cn type="integer" id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.3">1</cn><apply id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1"><cos id="A3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.1.1"></cos><apply id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1"><times id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.1"></times><ci id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.2">𝜋</ci><ci id="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.3.cmml" xref="A3.SS0.SSS0.Px2.p1.1.m1.3.3.2.2.2.1.1.1.1.1.1.3">𝑝</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.1.m1.3c">\eta_{p}=\eta_{1}+0.5(\eta_{0}-\eta_{1})(1+\cos(\pi p))</annotation></semantics></math><span id="A3.SS0.SSS0.Px2.p1.5.2" class="ltx_text" style="font-size:173%;">, where </span><math id="A3.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="p" display="inline"><semantics id="A3.SS0.SSS0.Px2.p1.2.m2.1a"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.2.m2.1.1" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.2.m2.1b"><ci id="A3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.2.m2.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.2.m2.1c">p</annotation></semantics></math><span id="A3.SS0.SSS0.Px2.p1.5.3" class="ltx_text" style="font-size:173%;"> is the process of training iterations normalized to be in </span><math id="A3.SS0.SSS0.Px2.p1.3.m3.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="A3.SS0.SSS0.Px2.p1.3.m3.2a"><mrow id="A3.SS0.SSS0.Px2.p1.3.m3.2.3.2" xref="A3.SS0.SSS0.Px2.p1.3.m3.2.3.1.cmml"><mo maxsize="173%" minsize="173%" id="A3.SS0.SSS0.Px2.p1.3.m3.2.3.2.1" xref="A3.SS0.SSS0.Px2.p1.3.m3.2.3.1.cmml">[</mo><mn mathsize="173%" id="A3.SS0.SSS0.Px2.p1.3.m3.1.1" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">0</mn><mo mathsize="173%" id="A3.SS0.SSS0.Px2.p1.3.m3.2.3.2.2" xref="A3.SS0.SSS0.Px2.p1.3.m3.2.3.1.cmml">,</mo><mn mathsize="173%" id="A3.SS0.SSS0.Px2.p1.3.m3.2.2" xref="A3.SS0.SSS0.Px2.p1.3.m3.2.2.cmml">1</mn><mo maxsize="173%" minsize="173%" id="A3.SS0.SSS0.Px2.p1.3.m3.2.3.2.3" xref="A3.SS0.SSS0.Px2.p1.3.m3.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.3.m3.2b"><interval closure="closed" id="A3.SS0.SSS0.Px2.p1.3.m3.2.3.1.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.2.3.2"><cn type="integer" id="A3.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.1.1">0</cn><cn type="integer" id="A3.SS0.SSS0.Px2.p1.3.m3.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.3.m3.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.3.m3.2c">[0,1]</annotation></semantics></math><span id="A3.SS0.SSS0.Px2.p1.5.4" class="ltx_text" style="font-size:173%;">, the initial learning rate </span><math id="A3.SS0.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="\eta_{0}=0.1" display="inline"><semantics id="A3.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="A3.SS0.SSS0.Px2.p1.4.m4.1.1" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><msub id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2.cmml">η</mi><mn mathsize="172%" id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3.cmml">0</mn></msub><mo mathsize="173%" id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">=</mo><mn mathsize="173%" id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1"><eq id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.1"></eq><apply id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.1.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.2">𝜂</ci><cn type="integer" id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.2.3">0</cn></apply><cn type="float" id="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="A3.SS0.SSS0.Px2.p1.4.m4.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.4.m4.1c">\eta_{0}=0.1</annotation></semantics></math><span id="A3.SS0.SSS0.Px2.p1.5.5" class="ltx_text" style="font-size:173%;">, and the final learning rate </span><math id="A3.SS0.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\eta_{1}=0.001" display="inline"><semantics id="A3.SS0.SSS0.Px2.p1.5.m5.1a"><mrow id="A3.SS0.SSS0.Px2.p1.5.m5.1.1" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><msub id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml"><mi mathsize="173%" id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.2" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.2.cmml">η</mi><mn mathsize="172%" id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.3" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.3.cmml">1</mn></msub><mo mathsize="173%" id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.1" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml">=</mo><mn mathsize="173%" id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.3" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1"><eq id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.1"></eq><apply id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.1.cmml" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2">subscript</csymbol><ci id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.2.cmml" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.2">𝜂</ci><cn type="integer" id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.3.cmml" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.2.3">1</cn></apply><cn type="float" id="A3.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="A3.SS0.SSS0.Px2.p1.5.m5.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px2.p1.5.m5.1c">\eta_{1}=0.001</annotation></semantics></math><span id="A3.SS0.SSS0.Px2.p1.5.6" class="ltx_text" style="font-size:173%;">. The results for several DA methods are reported in Table </span><a href="#A2.T1" title="Table A1 ‣ Appendix B Summary of Paper Novelty ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A1</span></a><span id="A3.SS0.SSS0.Px2.p1.5.7" class="ltx_text" style="font-size:173%;">. As we can see, with fine-grained subclasses merged into one, ImageNet-990 underperforms ImageNet by a large margin, suggesting that it may be helpful to use fine-grained visual categorization for pre-training; in contrast, by adding our 120K synthetic images, ImageNet-990+Ours is comparable to or better than ImageNet, confirming the utility of our synthetic data.</span></p>
</div>
</section>
<section id="A3.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Pre-training with an increased number of classes helps DA.</h5>

<div id="A3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px3.p1.1" class="ltx_p"><span id="A3.SS0.SSS0.Px3.p1.1.1" class="ltx_text" style="font-size:173%;">In Table 3, we have compared SubImageNet involving only target classes in training with ImageNet involving both target and non-target classes; with abundant pre-training epochs, the latter is evidently better than the former, indicating that learning rich category relationships is helpful for downstream DA. </span><em id="A3.SS0.SSS0.Px3.p1.1.2" class="ltx_emph ltx_font_italic" style="font-size:173%;">A similar phenomenon is observed in synthetic data pre-training.</em><span id="A3.SS0.SSS0.Px3.p1.1.3" class="ltx_text" style="font-size:173%;"> For example, we have done experiments of pre-training on the synthetic domain of our proposed S2RDA-49 task; compared with pre-training on Ours (120K images, 10 classes), MCD improves by 5.97% in Acc. and 6.06% in Mean, and DisClusterDA improves by 4.69% in Acc. and 5.36% in Mean.</span></p>
</div>
</section>
<section id="A3.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Necessity and applicability of the proposed S2RDA.</h5>

<div id="A3.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A3.SS0.SSS0.Px4.p1.4" class="ltx_p"><span id="A3.SS0.SSS0.Px4.p1.4.1" class="ltx_text" style="font-size:173%;">Note that SRDC outperforms the baseline No Adaptation by </span><math id="A3.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A3.SS0.SSS0.Px4.p1.1.m1.1a"><mo mathsize="173%" id="A3.SS0.SSS0.Px4.p1.1.m1.1.1" xref="A3.SS0.SSS0.Px4.p1.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px4.p1.1.m1.1b"><csymbol cd="latexml" id="A3.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="A3.SS0.SSS0.Px4.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px4.p1.1.m1.1c">\sim</annotation></semantics></math><span id="A3.SS0.SSS0.Px4.p1.4.2" class="ltx_text" style="font-size:173%;">10% on S2RDA-49 and DisClusterDA outperforms that by </span><math id="A3.SS0.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A3.SS0.SSS0.Px4.p1.2.m2.1a"><mo mathsize="173%" id="A3.SS0.SSS0.Px4.p1.2.m2.1.1" xref="A3.SS0.SSS0.Px4.p1.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px4.p1.2.m2.1b"><csymbol cd="latexml" id="A3.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="A3.SS0.SSS0.Px4.p1.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px4.p1.2.m2.1c">\sim</annotation></semantics></math><span id="A3.SS0.SSS0.Px4.p1.4.3" class="ltx_text" style="font-size:173%;">5%, verifying the efficacy of these DA methods. The observations also demonstrate that S2RDA </span><em id="A3.SS0.SSS0.Px4.p1.4.4" class="ltx_emph ltx_font_italic" style="font-size:173%;">can</em><span id="A3.SS0.SSS0.Px4.p1.4.5" class="ltx_text" style="font-size:173%;"> benchmark different DA methods. Compared to SubVisDA-10 (cf. Table 3), SRDC degrades by </span><math id="A3.SS0.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="\sim" display="inline"><semantics id="A3.SS0.SSS0.Px4.p1.3.m3.1a"><mo mathsize="173%" id="A3.SS0.SSS0.Px4.p1.3.m3.1.1" xref="A3.SS0.SSS0.Px4.p1.3.m3.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px4.p1.3.m3.1b"><csymbol cd="latexml" id="A3.SS0.SSS0.Px4.p1.3.m3.1.1.cmml" xref="A3.SS0.SSS0.Px4.p1.3.m3.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px4.p1.3.m3.1c">\sim</annotation></semantics></math><span id="A3.SS0.SSS0.Px4.p1.4.6" class="ltx_text" style="font-size:173%;">7% on S2RDA-49, which is reasonable as our real domain contains more practical images from real-world sources, though </span><em id="A3.SS0.SSS0.Px4.p1.4.7" class="ltx_emph ltx_font_italic" style="font-size:173%;">our synthetic data contain much more diversity, e.g., background</em><span id="A3.SS0.SSS0.Px4.p1.4.8" class="ltx_text" style="font-size:173%;"> (cf. Fig. 1).
Differently, S2RDA-MS-39, which decreases by </span><math id="A3.SS0.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="&gt;" display="inline"><semantics id="A3.SS0.SSS0.Px4.p1.4.m4.1a"><mo mathsize="173%" id="A3.SS0.SSS0.Px4.p1.4.m4.1.1" xref="A3.SS0.SSS0.Px4.p1.4.m4.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A3.SS0.SSS0.Px4.p1.4.m4.1b"><gt id="A3.SS0.SSS0.Px4.p1.4.m4.1.1.cmml" xref="A3.SS0.SSS0.Px4.p1.4.m4.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A3.SS0.SSS0.Px4.p1.4.m4.1c">&gt;</annotation></semantics></math><span id="A3.SS0.SSS0.Px4.p1.4.9" class="ltx_text" style="font-size:173%;">20% over S2RDA-49, evaluates different DA approaches on the </span><em id="A3.SS0.SSS0.Px4.p1.4.10" class="ltx_emph ltx_font_italic" style="font-size:173%;">worst/extreme cases</em><span id="A3.SS0.SSS0.Px4.p1.4.11" class="ltx_text" style="font-size:173%;"> (cf. Fig. 6), making a more comprehensive comparison and acting as a touchstone to examine and advance DA algorithms.
Reducing the domain gap between simple and difficult backgrounds is by nature one of the key issues in simulation-to-real transfer, as also shown in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A3.SS0.SSS0.Px4.p1.4.12.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib61" title="" class="ltx_ref">61</a><span id="A3.SS0.SSS0.Px4.p1.4.13.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A3.SS0.SSS0.Px4.p1.4.14" class="ltx_text" style="font-size:173%;">; therefore, reducing such a domain gap is one of the criteria for judging excellent DA methods.
To sum up, S2RDA is a </span><em id="A3.SS0.SSS0.Px4.p1.4.15" class="ltx_emph ltx_font_italic" style="font-size:173%;">better</em><span id="A3.SS0.SSS0.Px4.p1.4.16" class="ltx_text" style="font-size:173%;"> benchmark than VisDA-2017, as it has more realistic synthetic data and more practical real data with more object categories, and enables a larger room of improvement for promoting the progress of DA algorithms and models.</span></p>
</div>
</section>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples</h2>

<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:173%;">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Examining Learning Process</h3>

<div id="A4.SS1.p1" class="ltx_para">
<p id="A4.SS1.p1.1" class="ltx_p"><span id="A4.SS1.p1.1.1" class="ltx_text" style="font-size:173%;">In Fig. </span><a href="#A4.F1" title="Figure A1 ‣ D.1 Examining Learning Process ‣ Appendix D Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A1</span></a><span id="A4.SS1.p1.1.2" class="ltx_text" style="font-size:173%;">, we examine the learning process of fixed-dataset periodic training and training on non-repetitive samples based on ResNet-50 with no, weak, and strong data augmentations. To this end, we plot the evolving curves of the following eight quantities with the training: training loss measured on the synthetic training set, test loss (IID) measured on the synthetic IID test set, training accuracy measured on the synthetic training set, test accuracy (IID) measured on the synthetic IID test set, test loss (IID w/o BG) measured on the synthetic IID without background test set, test loss (OOD) measured on the SubVisDA-10 real/OOD test set, test accuracy (IID w/o BG) measured on the synthetic IID without background test set, and test accuracy (OOD) measured on the SubVisDA-10 real/OOD test set. The accuracy is measured using the ground truth labels, just for visualization.</span></p>
</div>
<figure id="A4.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/train_1.train_loss_da-n_res50.svg.png" id="A4.F1.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="127" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf1.4.1.1" class="ltx_text" style="font-size:52%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-n_res50_syn_id.svg.png" id="A4.F1.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="126" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf2.4.1.1" class="ltx_text" style="font-size:52%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/train_2.train_acc1_da-n_res50.svg.png" id="A4.F1.sf3.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf3.4.1.1" class="ltx_text" style="font-size:52%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf4.1" class="ltx_block ltx_minipage ltx_align_top" style="width:129.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-n_res50_syn_id.svg.png" id="A4.F1.sf4.1.g1" class="ltx_graphics ltx_img_landscape" width="132" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf4.4.1.1" class="ltx_text" style="font-size:52%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf5.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-n_res50_syn_ood.svg.png" id="A4.F1.sf5.1.g1" class="ltx_graphics ltx_img_landscape" width="134" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf5.4.1.1" class="ltx_text" style="font-size:52%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf6" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf6.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-n_res50_real.svg.png" id="A4.F1.sf6.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf6.4.1.1" class="ltx_text" style="font-size:52%;">(f)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf7" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf7.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-n_res50_syn_ood.svg.png" id="A4.F1.sf7.1.g1" class="ltx_graphics ltx_img_landscape" width="132" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf7.4.1.1" class="ltx_text" style="font-size:52%;">(g)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf8" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf8.1" class="ltx_block ltx_minipage ltx_align_top" style="width:129.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-n_res50_real.svg.png" id="A4.F1.sf8.1.g1" class="ltx_graphics ltx_img_landscape" width="135" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf8.4.1.1" class="ltx_text" style="font-size:52%;">(h)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf9" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf9.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/train_1.train_loss_da-w_res50.svg.png" id="A4.F1.sf9.1.g1" class="ltx_graphics ltx_img_landscape" width="126" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf9.4.1.1" class="ltx_text" style="font-size:52%;">(i)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf10" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf10.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-w_res50_syn_id.svg.png" id="A4.F1.sf10.1.g1" class="ltx_graphics ltx_img_landscape" width="127" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf10.4.1.1" class="ltx_text" style="font-size:52%;">(j)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf11" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf11.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/train_2.train_acc1_da-w_res50.svg.png" id="A4.F1.sf11.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf11.4.1.1" class="ltx_text" style="font-size:52%;">(k)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf12" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf12.1" class="ltx_block ltx_minipage ltx_align_top" style="width:129.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-w_res50_syn_id.svg.png" id="A4.F1.sf12.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf12.4.1.1" class="ltx_text" style="font-size:52%;">(l)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf13" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf13.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-w_res50_syn_ood.svg.png" id="A4.F1.sf13.1.g1" class="ltx_graphics ltx_img_landscape" width="134" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf13.4.1.1" class="ltx_text" style="font-size:52%;">(m)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf14" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf14.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-w_res50_real.svg.png" id="A4.F1.sf14.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf14.4.1.1" class="ltx_text" style="font-size:52%;">(n)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf15" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf15.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-w_res50_syn_ood.svg.png" id="A4.F1.sf15.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf15.4.1.1" class="ltx_text" style="font-size:52%;">(o)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf16" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf16.1" class="ltx_block ltx_minipage ltx_align_top" style="width:129.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-w_res50_real.svg.png" id="A4.F1.sf16.1.g1" class="ltx_graphics ltx_img_landscape" width="133" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf16.4.1.1" class="ltx_text" style="font-size:52%;">(p)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf17" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf17.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/train_1.train_loss_da-s_res50.svg.png" id="A4.F1.sf17.1.g1" class="ltx_graphics ltx_img_landscape" width="128" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf17.4.1.1" class="ltx_text" style="font-size:52%;">(q)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf18" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf18.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-s_res50_syn_id.svg.png" id="A4.F1.sf18.1.g1" class="ltx_graphics ltx_img_landscape" width="127" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf18.4.1.1" class="ltx_text" style="font-size:52%;">(r)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf19" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf19.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/train_2.train_acc1_da-s_res50.svg.png" id="A4.F1.sf19.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf19.4.1.1" class="ltx_text" style="font-size:52%;">(s)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf20" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf20.1" class="ltx_block ltx_minipage ltx_align_top" style="width:129.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-s_res50_syn_id.svg.png" id="A4.F1.sf20.1.g1" class="ltx_graphics ltx_img_landscape" width="132" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf20.4.1.1" class="ltx_text" style="font-size:52%;">(t)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf21" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf21.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-s_res50_syn_ood.svg.png" id="A4.F1.sf21.1.g1" class="ltx_graphics ltx_img_landscape" width="132" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf21.4.1.1" class="ltx_text" style="font-size:52%;">(u)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf22" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf22.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_1.test_loss_da-s_res50_real.svg.png" id="A4.F1.sf22.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf22.4.1.1" class="ltx_text" style="font-size:52%;">(v)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf23" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf23.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-s_res50_syn_ood.svg.png" id="A4.F1.sf23.1.g1" class="ltx_graphics ltx_img_landscape" width="131" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf23.4.1.1" class="ltx_text" style="font-size:52%;">(w)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F1.sf24" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F1.sf24.1" class="ltx_block ltx_minipage ltx_align_top" style="width:129.2pt;">
<img src="/html/2303.09165/assets/images/learning_curves/test_2.test_acc1_da-s_res50_real.svg.png" id="A4.F1.sf24.1.g1" class="ltx_graphics ltx_img_landscape" width="134" height="85" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.sf24.4.1.1" class="ltx_text" style="font-size:52%;">(x)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F1.8.1.1" class="ltx_text" style="font-size:52%;">Figure A1</span>: </span><span id="A4.F1.9.2" class="ltx_text" style="font-size:52%;">Learning process of training ResNet-50 on a fixed dataset (<span id="A4.F1.9.2.1" class="ltx_text" style="color:#0000FF;">blue</span>) or non-repetitive samples (<span id="A4.F1.9.2.2" class="ltx_text" style="color:#FF0000;">red</span>). Note that (a-h), (i-p), and (q-x) are for no, weak, and strong data augmentations respectively.</span></figcaption>
</figure>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:173%;">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>Visualizing Saliency Map</h3>

<div id="A4.SS2.p1" class="ltx_para">
<p id="A4.SS2.p1.5" class="ltx_p"><span id="A4.SS2.p1.5.1" class="ltx_text" style="font-size:173%;">We visualize the saliency maps, obtained from the ResNet-50 (Fig. </span><a href="#A4.F2" title="Figure A2 ‣ D.2 Visualizing Saliency Map ‣ Appendix D Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A2</span></a><span id="A4.SS2.p1.5.2" class="ltx_text" style="font-size:173%;">), ViT-B (Fig. </span><a href="#A4.F3" title="Figure A3 ‣ D.2 Visualizing Saliency Map ‣ Appendix D Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A3</span></a><span id="A4.SS2.p1.5.3" class="ltx_text" style="font-size:173%;">), and Mixer-B (Fig. </span><a href="#A4.F5" title="Figure A5 ‣ D.2 Visualizing Saliency Map ‣ Appendix D Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A5</span></a><span id="A4.SS2.p1.5.4" class="ltx_text" style="font-size:173%;">) trained on a fixed dataset or non-repetitive samples with no data augmentation. We consider two types of saliency visualization methods: input gradients which backpropagates the output score at the ground-truth category to the input image, and Grad-CAM which weights the feature maps with the gradients w.r.t. the features. For ViT-B, in Fig. </span><a href="#A4.F4" title="Figure A4 ‣ D.2 Visualizing Saliency Map ‣ Appendix D Fixed-Dataset Periodic Training vs. Training on Non-Repetitive Samples ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A4</span></a><span id="A4.SS2.p1.5.5" class="ltx_text" style="font-size:173%;">, we also visualize the attention maps of the classification token to all image patches at the last multi-head self-attention layer. The last five columns correspond to results at the </span><math id="A4.SS2.p1.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.SS2.p1.1.m1.1a"><mn mathsize="173%" id="A4.SS2.p1.1.m1.1.1" xref="A4.SS2.p1.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p1.1.m1.1b"><cn type="integer" id="A4.SS2.p1.1.m1.1.1.cmml" xref="A4.SS2.p1.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p1.1.m1.1c">20</annotation></semantics></math><span id="A4.SS2.p1.5.6" class="ltx_text" style="font-size:173%;">-th, </span><math id="A4.SS2.p1.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.SS2.p1.2.m2.1a"><mn mathsize="173%" id="A4.SS2.p1.2.m2.1.1" xref="A4.SS2.p1.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p1.2.m2.1b"><cn type="integer" id="A4.SS2.p1.2.m2.1.1.cmml" xref="A4.SS2.p1.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p1.2.m2.1c">200</annotation></semantics></math><span id="A4.SS2.p1.5.7" class="ltx_text" style="font-size:173%;">-th, </span><math id="A4.SS2.p1.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="A4.SS2.p1.3.m3.1a"><mn mathsize="173%" id="A4.SS2.p1.3.m3.1.1" xref="A4.SS2.p1.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p1.3.m3.1b"><cn type="integer" id="A4.SS2.p1.3.m3.1.1.cmml" xref="A4.SS2.p1.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p1.3.m3.1c">2</annotation></semantics></math><span id="A4.SS2.p1.5.8" class="ltx_text" style="font-size:173%;">K-th, </span><math id="A4.SS2.p1.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.SS2.p1.4.m4.1a"><mn mathsize="173%" id="A4.SS2.p1.4.m4.1.1" xref="A4.SS2.p1.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p1.4.m4.1b"><cn type="integer" id="A4.SS2.p1.4.m4.1.1.cmml" xref="A4.SS2.p1.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p1.4.m4.1c">20</annotation></semantics></math><span id="A4.SS2.p1.5.9" class="ltx_text" style="font-size:173%;">K-th, and </span><math id="A4.SS2.p1.5.m5.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.SS2.p1.5.m5.1a"><mn mathsize="173%" id="A4.SS2.p1.5.m5.1.1" xref="A4.SS2.p1.5.m5.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.SS2.p1.5.m5.1b"><cn type="integer" id="A4.SS2.p1.5.m5.1.1.cmml" xref="A4.SS2.p1.5.m5.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.SS2.p1.5.m5.1c">200</annotation></semantics></math><span id="A4.SS2.p1.5.10" class="ltx_text" style="font-size:173%;">K-th training iterations respectively. The example image in each row is randomly selected from IID test data.</span></p>
</div>
<figure id="A4.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F2.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x7.png" id="A4.F2.sf1.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F2.sf1.4.1.1" class="ltx_text" style="font-size:52%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F2.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x8.png" id="A4.F2.sf2.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F2.sf2.4.1.1" class="ltx_text" style="font-size:52%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F2.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x9.png" id="A4.F2.sf3.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F2.sf3.4.1.1" class="ltx_text" style="font-size:52%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F2.sf4.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x10.png" id="A4.F2.sf4.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F2.sf4.4.1.1" class="ltx_text" style="font-size:52%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F2.25.6.1" class="ltx_text" style="font-size:52%;">Figure A2</span>: </span><span id="A4.F2.10.5" class="ltx_text" style="font-size:52%;">Saliency maps of randomly selected IID test samples, obtained from the ResNet-50 trained on a fixed dataset or non-repetitive samples with no data augmentation, at the <math id="A4.F2.6.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.F2.6.1.m1.1b"><mn id="A4.F2.6.1.m1.1.1" xref="A4.F2.6.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.F2.6.1.m1.1c"><cn type="integer" id="A4.F2.6.1.m1.1.1.cmml" xref="A4.F2.6.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F2.6.1.m1.1d">20</annotation></semantics></math>-th, <math id="A4.F2.7.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.F2.7.2.m2.1b"><mn id="A4.F2.7.2.m2.1.1" xref="A4.F2.7.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.F2.7.2.m2.1c"><cn type="integer" id="A4.F2.7.2.m2.1.1.cmml" xref="A4.F2.7.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F2.7.2.m2.1d">200</annotation></semantics></math>-th, <math id="A4.F2.8.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="A4.F2.8.3.m3.1b"><mn id="A4.F2.8.3.m3.1.1" xref="A4.F2.8.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A4.F2.8.3.m3.1c"><cn type="integer" id="A4.F2.8.3.m3.1.1.cmml" xref="A4.F2.8.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F2.8.3.m3.1d">2</annotation></semantics></math>K-th, <math id="A4.F2.9.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.F2.9.4.m4.1b"><mn id="A4.F2.9.4.m4.1.1" xref="A4.F2.9.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.F2.9.4.m4.1c"><cn type="integer" id="A4.F2.9.4.m4.1.1.cmml" xref="A4.F2.9.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F2.9.4.m4.1d">20</annotation></semantics></math>K-th, and <math id="A4.F2.10.5.m5.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.F2.10.5.m5.1b"><mn id="A4.F2.10.5.m5.1.1" xref="A4.F2.10.5.m5.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.F2.10.5.m5.1c"><cn type="integer" id="A4.F2.10.5.m5.1.1.cmml" xref="A4.F2.10.5.m5.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F2.10.5.m5.1d">200</annotation></semantics></math>K-th training iterations. Note that rows 1 and 2 show input gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> and gradient weighted class activation maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> on input images respectively; the number on top of each picture means the ground-truth (first column) or predicted labels (other columns).</span></figcaption>
</figure>
<figure id="A4.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F3.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x11.png" id="A4.F3.sf1.1.g1" class="ltx_graphics ltx_img_portrait" width="176" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F3.sf1.4.1.1" class="ltx_text" style="font-size:52%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F3.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x12.png" id="A4.F3.sf2.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F3.sf2.4.1.1" class="ltx_text" style="font-size:52%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F3.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F3.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x13.png" id="A4.F3.sf3.1.g1" class="ltx_graphics ltx_img_portrait" width="176" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F3.sf3.4.1.1" class="ltx_text" style="font-size:52%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F3.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F3.sf4.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x14.png" id="A4.F3.sf4.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F3.sf4.4.1.1" class="ltx_text" style="font-size:52%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F3.25.6.1" class="ltx_text" style="font-size:52%;">Figure A3</span>: </span><span id="A4.F3.10.5" class="ltx_text" style="font-size:52%;">Saliency maps of randomly selected IID test samples, obtained from the ViT-B trained on a fixed dataset or non-repetitive samples with no data augmentation, at the <math id="A4.F3.6.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.F3.6.1.m1.1b"><mn id="A4.F3.6.1.m1.1.1" xref="A4.F3.6.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.F3.6.1.m1.1c"><cn type="integer" id="A4.F3.6.1.m1.1.1.cmml" xref="A4.F3.6.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F3.6.1.m1.1d">20</annotation></semantics></math>-th, <math id="A4.F3.7.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.F3.7.2.m2.1b"><mn id="A4.F3.7.2.m2.1.1" xref="A4.F3.7.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.F3.7.2.m2.1c"><cn type="integer" id="A4.F3.7.2.m2.1.1.cmml" xref="A4.F3.7.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F3.7.2.m2.1d">200</annotation></semantics></math>-th, <math id="A4.F3.8.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="A4.F3.8.3.m3.1b"><mn id="A4.F3.8.3.m3.1.1" xref="A4.F3.8.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A4.F3.8.3.m3.1c"><cn type="integer" id="A4.F3.8.3.m3.1.1.cmml" xref="A4.F3.8.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F3.8.3.m3.1d">2</annotation></semantics></math>K-th, <math id="A4.F3.9.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.F3.9.4.m4.1b"><mn id="A4.F3.9.4.m4.1.1" xref="A4.F3.9.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.F3.9.4.m4.1c"><cn type="integer" id="A4.F3.9.4.m4.1.1.cmml" xref="A4.F3.9.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F3.9.4.m4.1d">20</annotation></semantics></math>K-th, and <math id="A4.F3.10.5.m5.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.F3.10.5.m5.1b"><mn id="A4.F3.10.5.m5.1.1" xref="A4.F3.10.5.m5.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.F3.10.5.m5.1c"><cn type="integer" id="A4.F3.10.5.m5.1.1.cmml" xref="A4.F3.10.5.m5.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F3.10.5.m5.1d">200</annotation></semantics></math>K-th training iterations. Note that rows 1 and 2 show input gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> and gradient weighted class activation maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> on input images respectively; the number on top of each picture means the ground-truth (first column) or predicted labels (other columns).</span></figcaption>
</figure>
<figure id="A4.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F4.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x15.png" id="A4.F4.sf1.1.g1" class="ltx_graphics ltx_img_portrait" width="176" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F4.sf1.4.1.1" class="ltx_text" style="font-size:52%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F4.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x16.png" id="A4.F4.sf2.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F4.sf2.4.1.1" class="ltx_text" style="font-size:52%;">(b)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F4.22.6.1" class="ltx_text" style="font-size:52%;">Figure A4</span>: </span><span id="A4.F4.10.5" class="ltx_text" style="font-size:52%;">Attention maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> of randomly selected IID test samples, obtained from the ViT-B trained on a fixed dataset or non-repetitive samples with no data augmentation, at the <math id="A4.F4.6.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.F4.6.1.m1.1b"><mn id="A4.F4.6.1.m1.1.1" xref="A4.F4.6.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.F4.6.1.m1.1c"><cn type="integer" id="A4.F4.6.1.m1.1.1.cmml" xref="A4.F4.6.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F4.6.1.m1.1d">20</annotation></semantics></math>-th, <math id="A4.F4.7.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.F4.7.2.m2.1b"><mn id="A4.F4.7.2.m2.1.1" xref="A4.F4.7.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.F4.7.2.m2.1c"><cn type="integer" id="A4.F4.7.2.m2.1.1.cmml" xref="A4.F4.7.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F4.7.2.m2.1d">200</annotation></semantics></math>-th, <math id="A4.F4.8.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="A4.F4.8.3.m3.1b"><mn id="A4.F4.8.3.m3.1.1" xref="A4.F4.8.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A4.F4.8.3.m3.1c"><cn type="integer" id="A4.F4.8.3.m3.1.1.cmml" xref="A4.F4.8.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F4.8.3.m3.1d">2</annotation></semantics></math>K-th, <math id="A4.F4.9.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.F4.9.4.m4.1b"><mn id="A4.F4.9.4.m4.1.1" xref="A4.F4.9.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.F4.9.4.m4.1c"><cn type="integer" id="A4.F4.9.4.m4.1.1.cmml" xref="A4.F4.9.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F4.9.4.m4.1d">20</annotation></semantics></math>K-th, and <math id="A4.F4.10.5.m5.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.F4.10.5.m5.1b"><mn id="A4.F4.10.5.m5.1.1" xref="A4.F4.10.5.m5.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.F4.10.5.m5.1c"><cn type="integer" id="A4.F4.10.5.m5.1.1.cmml" xref="A4.F4.10.5.m5.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F4.10.5.m5.1d">200</annotation></semantics></math>K-th training iterations.</span></figcaption>
</figure>
<figure id="A4.F5" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F5.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F5.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x17.png" id="A4.F5.sf1.1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F5.sf1.4.1.1" class="ltx_text" style="font-size:52%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F5.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F5.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x18.png" id="A4.F5.sf2.1.g1" class="ltx_graphics ltx_img_portrait" width="176" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F5.sf2.4.1.1" class="ltx_text" style="font-size:52%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F5.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F5.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x19.png" id="A4.F5.sf3.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F5.sf3.4.1.1" class="ltx_text" style="font-size:52%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A4.F5.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A4.F5.sf4.1" class="ltx_block ltx_minipage ltx_align_top" style="width:248.4pt;">
<img src="/html/2303.09165/assets/x20.png" id="A4.F5.sf4.1.g1" class="ltx_graphics ltx_img_portrait" width="177" height="292" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F5.sf4.4.1.1" class="ltx_text" style="font-size:52%;">(d)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A4.F5.25.6.1" class="ltx_text" style="font-size:52%;">Figure A5</span>: </span><span id="A4.F5.10.5" class="ltx_text" style="font-size:52%;">Saliency maps of randomly selected IID test samples, obtained from the Mixer-B trained on a fixed dataset or non-repetitive samples with no data augmentation, at the <math id="A4.F5.6.1.m1.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.F5.6.1.m1.1b"><mn id="A4.F5.6.1.m1.1.1" xref="A4.F5.6.1.m1.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.F5.6.1.m1.1c"><cn type="integer" id="A4.F5.6.1.m1.1.1.cmml" xref="A4.F5.6.1.m1.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F5.6.1.m1.1d">20</annotation></semantics></math>-th, <math id="A4.F5.7.2.m2.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.F5.7.2.m2.1b"><mn id="A4.F5.7.2.m2.1.1" xref="A4.F5.7.2.m2.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.F5.7.2.m2.1c"><cn type="integer" id="A4.F5.7.2.m2.1.1.cmml" xref="A4.F5.7.2.m2.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F5.7.2.m2.1d">200</annotation></semantics></math>-th, <math id="A4.F5.8.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="A4.F5.8.3.m3.1b"><mn id="A4.F5.8.3.m3.1.1" xref="A4.F5.8.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A4.F5.8.3.m3.1c"><cn type="integer" id="A4.F5.8.3.m3.1.1.cmml" xref="A4.F5.8.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F5.8.3.m3.1d">2</annotation></semantics></math>K-th, <math id="A4.F5.9.4.m4.1" class="ltx_Math" alttext="20" display="inline"><semantics id="A4.F5.9.4.m4.1b"><mn id="A4.F5.9.4.m4.1.1" xref="A4.F5.9.4.m4.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="A4.F5.9.4.m4.1c"><cn type="integer" id="A4.F5.9.4.m4.1.1.cmml" xref="A4.F5.9.4.m4.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F5.9.4.m4.1d">20</annotation></semantics></math>K-th, and <math id="A4.F5.10.5.m5.1" class="ltx_Math" alttext="200" display="inline"><semantics id="A4.F5.10.5.m5.1b"><mn id="A4.F5.10.5.m5.1.1" xref="A4.F5.10.5.m5.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A4.F5.10.5.m5.1c"><cn type="integer" id="A4.F5.10.5.m5.1.1.cmml" xref="A4.F5.10.5.m5.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.F5.10.5.m5.1d">200</annotation></semantics></math>K-th training iterations. Note that rows 1 and 2 show input gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite> and gradient weighted class activation maps <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite> on input images respectively; the number on top of each picture means the ground-truth (first column) or predicted labels (other columns).</span></figcaption>
</figure>
</section>
<section id="A4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:173%;">
<span class="ltx_tag ltx_tag_subsection">D.3 </span>More Impact of Data Augmentations</h3>

<div id="A4.SS3.p1" class="ltx_para">
<p id="A4.SS3.p1.1" class="ltx_p"><span id="A4.SS3.p1.1.1" class="ltx_text" style="font-size:173%;">From Table 1, in OOD tests, training on non-repetitive images with no augmentation is superior to the fixed-dataset periodic training with weak augmentation, but far inferior to that with strong augmentation. It to some extent implies that the image transformations produced by 3D rendering itself do contain the hand-crafted weak augmentation changing pixel position in an image, but not the strong one changing both position and value of pixels.</span></p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Evaluating Various Network Architectures</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p"><span id="A5.p1.1.1" class="ltx_text" style="font-size:173%;">In Fig. </span><a href="#A5.F6" title="Figure A6 ‣ Appendix E Evaluating Various Network Architectures ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A6</span></a><span id="A5.p1.1.2" class="ltx_text" style="font-size:173%;">, we evaluate various network architectures by plotting their learning curves, in terms of training loss, test loss (IID), training accuracy, test accuracy (IID), test loss (IID w/o BG), test loss (OOD), test accuracy (IID w/o BG), and test accuracy (OOD). Various network architectures — ResNet-50 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A5.p1.1.3.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="A5.p1.1.4.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A5.p1.1.5" class="ltx_text" style="font-size:173%;">, ViT-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A5.p1.1.6.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="A5.p1.1.7.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A5.p1.1.8" class="ltx_text" style="font-size:173%;">, and Mixer-B </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A5.p1.1.9.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib80" title="" class="ltx_ref">80</a><span id="A5.p1.1.10.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A5.p1.1.11" class="ltx_text" style="font-size:173%;"> are trained on non-repetitive samples with strong data augmentation.</span></p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p"><span id="A5.p2.1.1" class="ltx_text" style="font-size:173%;">We note that ViT performs poorly despite data augmentation and more training epochs. It may be because ViT cannot well fit datasets with high dimension and large variance, and does not learn features less dependent on the background. Two possible solutions to improve are decreasing the input patch size for fine-grained feature interaction and smoothing the dataset (e.g. mixup </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A5.p2.1.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib92" title="" class="ltx_ref">92</a><span id="A5.p2.1.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A5.p2.1.4" class="ltx_text" style="font-size:173%;">) for data distribution completeness.</span></p>
</div>
<figure id="A5.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A5.F6.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/train_loss_3models_sda.svg.png" id="A5.F6.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="155" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.sf1.4.1.1" class="ltx_text" style="font-size:52%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A5.F6.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/test_loss_syn_id_3models_sda.svg.png" id="A5.F6.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="156" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.sf2.4.1.1" class="ltx_text" style="font-size:52%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A5.F6.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/train_acc1_3models_sda.svg.png" id="A5.F6.sf3.1.g1" class="ltx_graphics ltx_img_landscape" width="162" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.sf3.4.1.1" class="ltx_text" style="font-size:52%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F6.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A5.F6.sf4.1" class="ltx_block ltx_minipage ltx_align_top" style="width:129.2pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/test_acc1_syn_id_3models_sda.svg.png" id="A5.F6.sf4.1.g1" class="ltx_graphics ltx_img_landscape" width="163" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.sf4.4.1.1" class="ltx_text" style="font-size:52%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F6.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A5.F6.sf5.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/test_loss_syn_ood_3models_sda.svg.png" id="A5.F6.sf5.1.g1" class="ltx_graphics ltx_img_landscape" width="151" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.sf5.4.1.1" class="ltx_text" style="font-size:52%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F6.sf6" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A5.F6.sf6.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/test_loss_real_3models_sda.svg.png" id="A5.F6.sf6.1.g1" class="ltx_graphics ltx_img_landscape" width="162" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.sf6.4.1.1" class="ltx_text" style="font-size:52%;">(f)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F6.sf7" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A5.F6.sf7.1" class="ltx_block ltx_minipage ltx_align_top" style="width:124.2pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/test_acc1_syn_ood_3models_sda.svg.png" id="A5.F6.sf7.1.g1" class="ltx_graphics ltx_img_landscape" width="163" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.sf7.4.1.1" class="ltx_text" style="font-size:52%;">(g)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A5.F6.sf8" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A5.F6.sf8.1" class="ltx_block ltx_minipage ltx_align_top" style="width:129.2pt;">
<img src="/html/2303.09165/assets/images/comparison_of_3models/test_acc1_real_3models_sda.svg.png" id="A5.F6.sf8.1.g1" class="ltx_graphics ltx_img_landscape" width="166" height="105" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.sf8.4.1.1" class="ltx_text" style="font-size:52%;">(h)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A5.F6.10.1.1" class="ltx_text" style="font-size:52%;">Figure A6</span>: </span><span id="A5.F6.11.2" class="ltx_text" style="font-size:52%;">Learning process of training various network architectures on non-repetitive samples with strong data augmentation. Note that <span id="A5.F6.11.2.1" class="ltx_text" style="color:#FF0000;">red</span>, <span id="A5.F6.11.2.2" class="ltx_text" style="color:#00FF00;">green</span>, and <span id="A5.F6.11.2.3" class="ltx_text" style="color:#0000FF;">blue</span> indicate ResNet-50, ViT-B, and Mixer-B respectively.</span></figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Comparing Pre-training for Domain Adaptation</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.7" class="ltx_p"><span id="A6.p1.7.1" class="ltx_text" style="font-size:173%;">In Fig. </span><a href="#A6.F7" title="Figure A7 ‣ Appendix F Comparing Pre-training for Domain Adaptation ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A7</span></a><span id="A6.p1.7.2" class="ltx_text" style="font-size:173%;">, we compare different pre-training data using domain adaptation (DA) on SubVisDA-10 as the downstream task and show the learning process for several representative DA approaches. The considered pre-training schemes include </span><span id="A6.p1.7.3" class="ltx_text ltx_font_bold" style="font-size:173%;">(1)</span><span id="A6.p1.7.4" class="ltx_text" style="font-size:173%;"> No Pre-training where the model parameters are randomly initialized, </span><span id="A6.p1.7.5" class="ltx_text ltx_font_bold" style="font-size:173%;">(2)</span><span id="A6.p1.7.6" class="ltx_text" style="font-size:173%;"> Ours denotes our synthesized </span><math id="A6.p1.1.m1.1" class="ltx_Math" alttext="120" display="inline"><semantics id="A6.p1.1.m1.1a"><mn mathsize="173%" id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><cn type="integer" id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">120</annotation></semantics></math><span id="A6.p1.7.7" class="ltx_text" style="font-size:173%;">K images of the </span><math id="A6.p1.2.m2.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A6.p1.2.m2.1a"><mn mathsize="173%" id="A6.p1.2.m2.1.1" xref="A6.p1.2.m2.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A6.p1.2.m2.1b"><cn type="integer" id="A6.p1.2.m2.1.1.cmml" xref="A6.p1.2.m2.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.2.m2.1c">10</annotation></semantics></math><span id="A6.p1.7.8" class="ltx_text" style="font-size:173%;"> object classes shared by SubVisDA-10, </span><span id="A6.p1.7.9" class="ltx_text ltx_font_bold" style="font-size:173%;">(3)</span><span id="A6.p1.7.10" class="ltx_text" style="font-size:173%;"> SubImageNet is the subset collecting examples of the </span><math id="A6.p1.3.m3.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A6.p1.3.m3.1a"><mn mathsize="173%" id="A6.p1.3.m3.1.1" xref="A6.p1.3.m3.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A6.p1.3.m3.1b"><cn type="integer" id="A6.p1.3.m3.1.1.cmml" xref="A6.p1.3.m3.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.3.m3.1c">10</annotation></semantics></math><span id="A6.p1.7.11" class="ltx_text" style="font-size:173%;"> classes from ImageNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A6.p1.7.12.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="A6.p1.7.13.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A6.p1.7.14" class="ltx_text" style="font-size:173%;">, </span><span id="A6.p1.7.15" class="ltx_text ltx_font_bold" style="font-size:173%;">(4)</span><span id="A6.p1.7.16" class="ltx_text" style="font-size:173%;"> ImageNet (</span><math id="A6.p1.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A6.p1.4.m4.1a"><mn mathsize="173%" id="A6.p1.4.m4.1.1" xref="A6.p1.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A6.p1.4.m4.1b"><cn type="integer" id="A6.p1.4.m4.1.1.cmml" xref="A6.p1.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.4.m4.1c">10</annotation></semantics></math><span id="A6.p1.7.17" class="ltx_text" style="font-size:173%;"> Epoch) has 1K classes and </span><math id="A6.p1.5.m5.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A6.p1.5.m5.1a"><mn mathsize="173%" id="A6.p1.5.m5.1.1" xref="A6.p1.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A6.p1.5.m5.1b"><cn type="integer" id="A6.p1.5.m5.1.1.cmml" xref="A6.p1.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.5.m5.1c">10</annotation></semantics></math><span id="A6.p1.7.18" class="ltx_text" style="font-size:173%;"> pre-training epochs, and </span><span id="A6.p1.7.19" class="ltx_text ltx_font_bold" style="font-size:173%;">(5)</span><span id="A6.p1.7.20" class="ltx_text" style="font-size:173%;"> ImageNet</span><sup id="A6.p1.7.21" class="ltx_sup"><span id="A6.p1.7.21.1" class="ltx_text" style="font-size:173%;">★</span></sup><span id="A6.p1.7.22" class="ltx_text" style="font-size:173%;"> uses the official ResNet-50 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A6.p1.7.23.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="A6.p1.7.24.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A6.p1.7.25" class="ltx_text" style="font-size:173%;"> checkpoint pre-trained on ImageNet for </span><math id="A6.p1.7.m7.1" class="ltx_Math" alttext="120" display="inline"><semantics id="A6.p1.7.m7.1a"><mn mathsize="173%" id="A6.p1.7.m7.1.1" xref="A6.p1.7.m7.1.1.cmml">120</mn><annotation-xml encoding="MathML-Content" id="A6.p1.7.m7.1b"><cn type="integer" id="A6.p1.7.m7.1.1.cmml" xref="A6.p1.7.m7.1.1">120</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.7.m7.1c">120</annotation></semantics></math><span id="A6.p1.7.26" class="ltx_text" style="font-size:173%;"> epochs. The compared DA methods include No Adaptation that trains the model only on the labeled source data, DANN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A6.p1.7.27.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="A6.p1.7.28.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A6.p1.7.29" class="ltx_text" style="font-size:173%;">, MCD, </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A6.p1.7.30.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib69" title="" class="ltx_ref">69</a><span id="A6.p1.7.31.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A6.p1.7.32" class="ltx_text" style="font-size:173%;">, RCA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A6.p1.7.33.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="A6.p1.7.34.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A6.p1.7.35" class="ltx_text" style="font-size:173%;">, SRDC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A6.p1.7.36.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib76" title="" class="ltx_ref">76</a><span id="A6.p1.7.37.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A6.p1.7.38" class="ltx_text" style="font-size:173%;">, and DisClusterDA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A6.p1.7.39.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib78" title="" class="ltx_ref">78</a><span id="A6.p1.7.40.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A6.p1.7.41" class="ltx_text" style="font-size:173%;">.</span></p>
</div>
<figure id="A6.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A6.F7.sf1.1" class="ltx_block ltx_minipage ltx_align_top" style="width:164.0pt;">
<img src="/html/2303.09165/assets/images/da_learning_curves/subvisda_no_adaptation.png" id="A6.F7.sf1.1.g1" class="ltx_graphics ltx_img_landscape" width="187" height="150" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A6.F7.sf1.4.1.1" class="ltx_text" style="font-size:52%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A6.F7.sf2.1" class="ltx_block ltx_minipage ltx_align_top" style="width:164.0pt;">
<img src="/html/2303.09165/assets/images/da_learning_curves/subvisda_dann.png" id="A6.F7.sf2.1.g1" class="ltx_graphics ltx_img_landscape" width="188" height="150" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A6.F7.sf2.4.1.1" class="ltx_text" style="font-size:52%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A6.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A6.F7.sf3.1" class="ltx_block ltx_minipage ltx_align_top" style="width:164.0pt;">
<img src="/html/2303.09165/assets/images/da_learning_curves/subvisda_mcd.png" id="A6.F7.sf3.1.g1" class="ltx_graphics ltx_img_landscape" width="188" height="150" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A6.F7.sf3.4.1.1" class="ltx_text" style="font-size:52%;">(c)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F7.sf4" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A6.F7.sf4.1" class="ltx_block ltx_minipage ltx_align_top" style="width:164.0pt;">
<img src="/html/2303.09165/assets/images/da_learning_curves/subvisda_rca.png" id="A6.F7.sf4.1.g1" class="ltx_graphics ltx_img_landscape" width="188" height="150" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A6.F7.sf4.4.1.1" class="ltx_text" style="font-size:52%;">(d)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A6.F7.sf5" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A6.F7.sf5.1" class="ltx_block ltx_minipage ltx_align_top" style="width:164.0pt;">
<img src="/html/2303.09165/assets/images/da_learning_curves/subvisda_srdc.png" id="A6.F7.sf5.1.g1" class="ltx_graphics ltx_img_landscape" width="188" height="150" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A6.F7.sf5.4.1.1" class="ltx_text" style="font-size:52%;">(e)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A6.F7.sf6" class="ltx_figure ltx_figure_panel ltx_align_center">
<div id="A6.F7.sf6.1" class="ltx_block ltx_minipage ltx_align_top" style="width:164.0pt;">
<img src="/html/2303.09165/assets/images/da_learning_curves/subvisda_disclusterda.png" id="A6.F7.sf6.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="187" height="150" alt="Refer to caption">
</div>
<figcaption class="ltx_caption" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A6.F7.sf6.4.1.1" class="ltx_text" style="font-size:52%;">(f)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A6.F7.4.1.1" class="ltx_text" style="font-size:52%;">Figure A7</span>: </span><span id="A6.F7.5.2" class="ltx_text" style="font-size:52%;">Learning process (Acc.) of domain adaptation when varying the pre-training scheme.</span></figcaption>
</figure>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>More Details on Our Proposed S2RDA Benchmark</h2>

<section id="A7.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Dataset Details.</h5>

<div id="A7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A7.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="A7.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:173%;">Our proposed Synthetic-to-Real (S2RDA) benchmark for more practical visual domain adaptation (DA) includes two challenging transfer tasks of S2RDA-49 and S2RDA-MS-39. In Fig. </span><a href="#A7.F8" title="Figure A8 ‣ Comparing Synthetic Data with Real Data. ‣ Appendix G More Details on Our Proposed S2RDA Benchmark ‣ A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation" class="ltx_ref" style="font-size:173%;"><span class="ltx_text ltx_ref_tag">A8</span></a><span id="A7.SS0.SSS0.Px1.p1.1.2" class="ltx_text" style="font-size:173%;">, we show the distribution of the number of images per class in each real domain, which is exhibited to be a long-tailed distribution where a small number of classes dominate. How we collect the real data from diverse real-world sources is recorded in respective files included in the code. Our S2RDA dataset is publicly available at </span><a target="_blank" href="https://pan.baidu.com/s/1fHHaqrEHbUZLXEg9XKpgSg?pwd=w9wa" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:173%;">https://pan.baidu.com/s/1fHHaqrEHbUZLXEg9XKpgSg?pwd=w9wa</a><span id="A7.SS0.SSS0.Px1.p1.1.3" class="ltx_text" style="font-size:173%;">.</span></p>
</div>
</section>
<section id="A7.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Comparing Synthetic Data with Real Data.</h5>

<div id="A7.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A7.SS0.SSS0.Px2.p1.1" class="ltx_p"><span id="A7.SS0.SSS0.Px2.p1.1.1" class="ltx_text" style="font-size:173%;">We provide quantitative and qualitative comparisons for VisDA-2017, our synthesized dataset, and ImageNet as follows. The mean and standard deviation of VisDA-2017, our synthesized dataset, and ImageNet are [0.878, 0.876, 0.874] and [0.207, 0.210, 0.216], [0.487, 0.450, 0.462] and [0.237, 0.251, 0.270], and [0.485, 0.456, 0.406] and [0.229, 0.224, 0.225] respectively. As we can see, the statistics of our synthesized dataset are closer to the real dataset ImageNet than VisDA-2017. It is consistent with the observation in Table 1 that our synthesized dataset used for training yields higher OOD/real test accuracy than SubVisDA-10. Except for quantitative comparisons, we have also provided the qualitative visualization for the three datasets and the proposed S2RDA benchmark in Fig. 1 and Fig. 6 respectively, which demonstrates that our synthesized dataset is visually more similar to ImageNet.</span></p>
</div>
<figure id="A7.F8" class="ltx_figure"><img src="/html/2303.09165/assets/images/dataset_statistics.png" id="A7.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="685" height="351" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:173%;"><span class="ltx_tag ltx_tag_figure"><span id="A7.F8.4.1.1" class="ltx_text" style="font-size:52%;">Figure A8</span>: </span><span id="A7.F8.5.2" class="ltx_text" style="font-size:52%;">Distribution of the number of images per class in each real domain of our proposed S2RDA.</span></figcaption>
</figure>
</section>
</section>
<section id="A8" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Other Implementation Details</h2>

<section id="A8.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Supervised Learning/Pre-training.</h5>

<div id="A8.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A8.SS0.SSS0.Px1.p1.8" class="ltx_p"><span id="A8.SS0.SSS0.Px1.p1.8.1" class="ltx_text" style="font-size:173%;">For each backbone in Sec. 4.1, all its layers up to the second last one are used as the feature extractor and the neuron number of its last FC layer is set as </span><math id="A8.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A8.SS0.SSS0.Px1.p1.1.m1.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A8.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px1.p1.1.m1.1b"><cn type="integer" id="A8.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px1.p1.1.m1.1c">10</annotation></semantics></math><span id="A8.SS0.SSS0.Px1.p1.8.2" class="ltx_text" style="font-size:173%;"> to have the classifier. We use the cosine learning rate schedule: the learning rate is adjusted by </span><math id="A8.SS0.SSS0.Px1.p1.2.m2.2" class="ltx_Math" alttext="\eta_{p}=0.5\eta_{0}(1+\cos(\pi p))" display="inline"><semantics id="A8.SS0.SSS0.Px1.p1.2.m2.2a"><mrow id="A8.SS0.SSS0.Px1.p1.2.m2.2.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.cmml"><msub id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.2.cmml">η</mi><mi mathsize="172%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.3" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.3.cmml">p</mi></msub><mo mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.2.cmml">=</mo><mrow id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.cmml"><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.3" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.3.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.2.cmml">​</mo><msub id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.2.cmml">η</mi><mn mathsize="172%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.3" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.3.cmml">0</mn></msub><mo lspace="0em" rspace="0em" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.2a" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.2.cmml">​</mo><mrow id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.cmml"><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.cmml">(</mo><mrow id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.cmml"><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.3" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.3.cmml">1</mn><mo mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.2.cmml">+</mo><mrow id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.2.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A8.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">cos</mi><mo id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1a" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.2.cmml">⁡</mo><mrow id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.2.cmml"><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.2.cmml">(</mo><mrow id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.2" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.2.cmml">π</mi><mo lspace="0em" rspace="0em" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.1" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.1.cmml">​</mo><mi mathsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.3" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.3.cmml">p</mi></mrow><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.3" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.3" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px1.p1.2.m2.2b"><apply id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2"><eq id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.2.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.2"></eq><apply id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3"><csymbol cd="ambiguous" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.1.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3">subscript</csymbol><ci id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.2.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.2">𝜂</ci><ci id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.3.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.3.3">𝑝</ci></apply><apply id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1"><times id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.2.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.2"></times><cn type="float" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.3.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.3">0.5</cn><apply id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4"><csymbol cd="ambiguous" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.1.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4">subscript</csymbol><ci id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.2.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.2">𝜂</ci><cn type="integer" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.3.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.4.3">0</cn></apply><apply id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1"><plus id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.2.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.2"></plus><cn type="integer" id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.3.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.3">1</cn><apply id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.2.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1"><cos id="A8.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.1.1"></cos><apply id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1"><times id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.1"></times><ci id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.2">𝜋</ci><ci id="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="A8.SS0.SSS0.Px1.p1.2.m2.2.2.1.1.1.1.1.1.1.1.3">𝑝</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px1.p1.2.m2.2c">\eta_{p}=0.5\eta_{0}(1+\cos(\pi p))</annotation></semantics></math><span id="A8.SS0.SSS0.Px1.p1.8.3" class="ltx_text" style="font-size:173%;">, where </span><math id="A8.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="A8.SS0.SSS0.Px1.p1.3.m3.1a"><mi mathsize="173%" id="A8.SS0.SSS0.Px1.p1.3.m3.1.1" xref="A8.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="A8.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px1.p1.3.m3.1c">p</annotation></semantics></math><span id="A8.SS0.SSS0.Px1.p1.8.4" class="ltx_text" style="font-size:173%;"> is the process of training iterations normalized to be in </span><math id="A8.SS0.SSS0.Px1.p1.4.m4.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="A8.SS0.SSS0.Px1.p1.4.m4.2a"><mrow id="A8.SS0.SSS0.Px1.p1.4.m4.2.3.2" xref="A8.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml"><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px1.p1.4.m4.2.3.2.1" xref="A8.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml">[</mo><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.4.m4.1.1" xref="A8.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">0</mn><mo mathsize="173%" id="A8.SS0.SSS0.Px1.p1.4.m4.2.3.2.2" xref="A8.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml">,</mo><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.4.m4.2.2" xref="A8.SS0.SSS0.Px1.p1.4.m4.2.2.cmml">1</mn><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px1.p1.4.m4.2.3.2.3" xref="A8.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px1.p1.4.m4.2b"><interval closure="closed" id="A8.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="A8.SS0.SSS0.Px1.p1.4.m4.2.3.2"><cn type="integer" id="A8.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.4.m4.1.1">0</cn><cn type="integer" id="A8.SS0.SSS0.Px1.p1.4.m4.2.2.cmml" xref="A8.SS0.SSS0.Px1.p1.4.m4.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px1.p1.4.m4.2c">[0,1]</annotation></semantics></math><span id="A8.SS0.SSS0.Px1.p1.8.5" class="ltx_text" style="font-size:173%;"> and the initial learning rate </span><math id="A8.SS0.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="\eta_{0}=0.01" display="inline"><semantics id="A8.SS0.SSS0.Px1.p1.5.m5.1a"><mrow id="A8.SS0.SSS0.Px1.p1.5.m5.1.1" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.cmml"><msub id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.2" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.2.cmml">η</mi><mn mathsize="172%" id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.3" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.3.cmml">0</mn></msub><mo mathsize="173%" id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.1" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.1.cmml">=</mo><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.3" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px1.p1.5.m5.1b"><apply id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1"><eq id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.1"></eq><apply id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.1.cmml" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2">subscript</csymbol><ci id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.2.cmml" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.2">𝜂</ci><cn type="integer" id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.3.cmml" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.2.3">0</cn></apply><cn type="float" id="A8.SS0.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="A8.SS0.SSS0.Px1.p1.5.m5.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px1.p1.5.m5.1c">\eta_{0}=0.01</annotation></semantics></math><span id="A8.SS0.SSS0.Px1.p1.8.6" class="ltx_text" style="font-size:173%;">. The momentum, weight decay, and random seed are set as </span><math id="A8.SS0.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="A8.SS0.SSS0.Px1.p1.6.m6.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.6.m6.1.1" xref="A8.SS0.SSS0.Px1.p1.6.m6.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px1.p1.6.m6.1b"><cn type="float" id="A8.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.6.m6.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px1.p1.6.m6.1c">0.9</annotation></semantics></math><span id="A8.SS0.SSS0.Px1.p1.8.7" class="ltx_text" style="font-size:173%;">, </span><math id="A8.SS0.SSS0.Px1.p1.7.m7.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="A8.SS0.SSS0.Px1.p1.7.m7.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.7.m7.1.1" xref="A8.SS0.SSS0.Px1.p1.7.m7.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px1.p1.7.m7.1b"><cn type="float" id="A8.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.7.m7.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px1.p1.7.m7.1c">0.0001</annotation></semantics></math><span id="A8.SS0.SSS0.Px1.p1.8.8" class="ltx_text" style="font-size:173%;">, and </span><math id="A8.SS0.SSS0.Px1.p1.8.m8.1" class="ltx_Math" alttext="1" display="inline"><semantics id="A8.SS0.SSS0.Px1.p1.8.m8.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px1.p1.8.m8.1.1" xref="A8.SS0.SSS0.Px1.p1.8.m8.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px1.p1.8.m8.1b"><cn type="integer" id="A8.SS0.SSS0.Px1.p1.8.m8.1.1.cmml" xref="A8.SS0.SSS0.Px1.p1.8.m8.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px1.p1.8.m8.1c">1</annotation></semantics></math><span id="A8.SS0.SSS0.Px1.p1.8.9" class="ltx_text" style="font-size:173%;"> respectively. In light of fairness, the final normalization operation uses the ImageNet statistics consistently for all experiments.</span></p>
</div>
</section>
<section id="A8.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Downstream Domain Adaptation.</h5>

<div id="A8.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A8.SS0.SSS0.Px2.p1.13" class="ltx_p"><span id="A8.SS0.SSS0.Px2.p1.13.1" class="ltx_text" style="font-size:173%;">In domain adaptation training, we use all labeled source samples and all unlabeled target samples as the training data. In each base model, the last FC layer is replaced with a new task-specific FC layer as the classifier. We fine-tune the pre-trained layers and train the new layer from scratch, where the learning rate of the latter is </span><math id="A8.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.1.m1.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.1.m1.1.1" xref="A8.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.1.m1.1b"><cn type="integer" id="A8.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.1.m1.1c">10</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.2" class="ltx_text" style="font-size:173%;"> times that of the former. The learning rate is adjusted by </span><math id="A8.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="\eta_{p}=\eta_{0}(1+\alpha p)^{-\beta}" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="A8.SS0.SSS0.Px2.p1.2.m2.1.1" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><msub id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.2" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml">η</mi><mi mathsize="172%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.3" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml">p</mi></msub><mo mathsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">=</mo><mrow id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml"><msub id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.2" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.2.cmml">η</mi><mn mathsize="172%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.3" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.3.cmml">0</mn></msub><mo lspace="0em" rspace="0em" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.2" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.2.cmml">​</mo><msup id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.cmml"><mrow id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.cmml"><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.2" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.cmml"><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.2" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.2.cmml">1</mn><mo mathsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.1" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.1.cmml">+</mo><mrow id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.2" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.1" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.1.cmml">​</mo><mi mathsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.3" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.3.cmml">p</mi></mrow></mrow><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.3" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml"><mo mathsize="172%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3a" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml">−</mo><mi mathsize="172%" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3.2" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3.2.cmml">β</mi></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1"><eq id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.2"></eq><apply id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3">subscript</csymbol><ci id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.2.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.2">𝜂</ci><ci id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.3.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.3.3">𝑝</ci></apply><apply id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1"><times id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.2.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.2"></times><apply id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3"><csymbol cd="ambiguous" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3">subscript</csymbol><ci id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.2.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.2">𝜂</ci><cn type="integer" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.3.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.3.3">0</cn></apply><apply id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.2.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1">superscript</csymbol><apply id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1"><plus id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.1"></plus><cn type="integer" id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.2.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.2">1</cn><apply id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3"><times id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.1"></times><ci id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.2.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.2">𝛼</ci><ci id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.3.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.1.1.1.3.3">𝑝</ci></apply></apply><apply id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3"><minus id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3.1.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3"></minus><ci id="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3.2.cmml" xref="A8.SS0.SSS0.Px2.p1.2.m2.1.1.1.1.3.2">𝛽</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.2.m2.1c">\eta_{p}=\eta_{0}(1+\alpha p)^{-\beta}</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.3" class="ltx_text" style="font-size:173%;">, where </span><math id="A8.SS0.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="p" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.3.m3.1a"><mi mathsize="173%" id="A8.SS0.SSS0.Px2.p1.3.m3.1.1" xref="A8.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.3.m3.1b"><ci id="A8.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.3.m3.1.1">𝑝</ci></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.3.m3.1c">p</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.4" class="ltx_text" style="font-size:173%;"> denotes the training process of training epochs normalized to be in </span><math id="A8.SS0.SSS0.Px2.p1.4.m4.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.4.m4.2a"><mrow id="A8.SS0.SSS0.Px2.p1.4.m4.2.3.2" xref="A8.SS0.SSS0.Px2.p1.4.m4.2.3.1.cmml"><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px2.p1.4.m4.2.3.2.1" xref="A8.SS0.SSS0.Px2.p1.4.m4.2.3.1.cmml">[</mo><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.4.m4.1.1" xref="A8.SS0.SSS0.Px2.p1.4.m4.1.1.cmml">0</mn><mo mathsize="173%" id="A8.SS0.SSS0.Px2.p1.4.m4.2.3.2.2" xref="A8.SS0.SSS0.Px2.p1.4.m4.2.3.1.cmml">,</mo><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.4.m4.2.2" xref="A8.SS0.SSS0.Px2.p1.4.m4.2.2.cmml">1</mn><mo maxsize="173%" minsize="173%" id="A8.SS0.SSS0.Px2.p1.4.m4.2.3.2.3" xref="A8.SS0.SSS0.Px2.p1.4.m4.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.4.m4.2b"><interval closure="closed" id="A8.SS0.SSS0.Px2.p1.4.m4.2.3.1.cmml" xref="A8.SS0.SSS0.Px2.p1.4.m4.2.3.2"><cn type="integer" id="A8.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.4.m4.1.1">0</cn><cn type="integer" id="A8.SS0.SSS0.Px2.p1.4.m4.2.2.cmml" xref="A8.SS0.SSS0.Px2.p1.4.m4.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.4.m4.2c">[0,1]</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.5" class="ltx_text" style="font-size:173%;">, the initial learning rate </span><math id="A8.SS0.SSS0.Px2.p1.5.m5.1" class="ltx_Math" alttext="\eta_{0}" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.5.m5.1a"><msub id="A8.SS0.SSS0.Px2.p1.5.m5.1.1" xref="A8.SS0.SSS0.Px2.p1.5.m5.1.1.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px2.p1.5.m5.1.1.2" xref="A8.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml">η</mi><mn mathsize="172%" id="A8.SS0.SSS0.Px2.p1.5.m5.1.1.3" xref="A8.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="A8.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A8.SS0.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.5.m5.1.1">subscript</csymbol><ci id="A8.SS0.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="A8.SS0.SSS0.Px2.p1.5.m5.1.1.2">𝜂</ci><cn type="integer" id="A8.SS0.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="A8.SS0.SSS0.Px2.p1.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.5.m5.1c">\eta_{0}</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.6" class="ltx_text" style="font-size:173%;"> is </span><math id="A8.SS0.SSS0.Px2.p1.6.m6.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.6.m6.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.6.m6.1.1" xref="A8.SS0.SSS0.Px2.p1.6.m6.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.6.m6.1b"><cn type="float" id="A8.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.6.m6.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.6.m6.1c">0.001</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.7" class="ltx_text" style="font-size:173%;"> for MCD and </span><math id="A8.SS0.SSS0.Px2.p1.7.m7.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.7.m7.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.7.m7.1.1" xref="A8.SS0.SSS0.Px2.p1.7.m7.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.7.m7.1b"><cn type="float" id="A8.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.7.m7.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.7.m7.1c">0.0001</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.8" class="ltx_text" style="font-size:173%;"> for other methods, </span><math id="A8.SS0.SSS0.Px2.p1.8.m8.1" class="ltx_Math" alttext="\alpha=10" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.8.m8.1a"><mrow id="A8.SS0.SSS0.Px2.p1.8.m8.1.1" xref="A8.SS0.SSS0.Px2.p1.8.m8.1.1.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px2.p1.8.m8.1.1.2" xref="A8.SS0.SSS0.Px2.p1.8.m8.1.1.2.cmml">α</mi><mo mathsize="173%" id="A8.SS0.SSS0.Px2.p1.8.m8.1.1.1" xref="A8.SS0.SSS0.Px2.p1.8.m8.1.1.1.cmml">=</mo><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.8.m8.1.1.3" xref="A8.SS0.SSS0.Px2.p1.8.m8.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.8.m8.1b"><apply id="A8.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.8.m8.1.1"><eq id="A8.SS0.SSS0.Px2.p1.8.m8.1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.8.m8.1.1.1"></eq><ci id="A8.SS0.SSS0.Px2.p1.8.m8.1.1.2.cmml" xref="A8.SS0.SSS0.Px2.p1.8.m8.1.1.2">𝛼</ci><cn type="integer" id="A8.SS0.SSS0.Px2.p1.8.m8.1.1.3.cmml" xref="A8.SS0.SSS0.Px2.p1.8.m8.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.8.m8.1c">\alpha=10</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.9" class="ltx_text" style="font-size:173%;">, and </span><math id="A8.SS0.SSS0.Px2.p1.9.m9.1" class="ltx_Math" alttext="\beta=0.75" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.9.m9.1a"><mrow id="A8.SS0.SSS0.Px2.p1.9.m9.1.1" xref="A8.SS0.SSS0.Px2.p1.9.m9.1.1.cmml"><mi mathsize="173%" id="A8.SS0.SSS0.Px2.p1.9.m9.1.1.2" xref="A8.SS0.SSS0.Px2.p1.9.m9.1.1.2.cmml">β</mi><mo mathsize="173%" id="A8.SS0.SSS0.Px2.p1.9.m9.1.1.1" xref="A8.SS0.SSS0.Px2.p1.9.m9.1.1.1.cmml">=</mo><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.9.m9.1.1.3" xref="A8.SS0.SSS0.Px2.p1.9.m9.1.1.3.cmml">0.75</mn></mrow><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.9.m9.1b"><apply id="A8.SS0.SSS0.Px2.p1.9.m9.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.9.m9.1.1"><eq id="A8.SS0.SSS0.Px2.p1.9.m9.1.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.9.m9.1.1.1"></eq><ci id="A8.SS0.SSS0.Px2.p1.9.m9.1.1.2.cmml" xref="A8.SS0.SSS0.Px2.p1.9.m9.1.1.2">𝛽</ci><cn type="float" id="A8.SS0.SSS0.Px2.p1.9.m9.1.1.3.cmml" xref="A8.SS0.SSS0.Px2.p1.9.m9.1.1.3">0.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.9.m9.1c">\beta=0.75</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.10" class="ltx_text" style="font-size:173%;">. The momentum, weight decay, and random seed are set as </span><math id="A8.SS0.SSS0.Px2.p1.10.m10.1" class="ltx_Math" alttext="0.9" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.10.m10.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.10.m10.1.1" xref="A8.SS0.SSS0.Px2.p1.10.m10.1.1.cmml">0.9</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.10.m10.1b"><cn type="float" id="A8.SS0.SSS0.Px2.p1.10.m10.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.10.m10.1.1">0.9</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.10.m10.1c">0.9</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.11" class="ltx_text" style="font-size:173%;">, </span><math id="A8.SS0.SSS0.Px2.p1.11.m11.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.11.m11.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.11.m11.1.1" xref="A8.SS0.SSS0.Px2.p1.11.m11.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.11.m11.1b"><cn type="float" id="A8.SS0.SSS0.Px2.p1.11.m11.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.11.m11.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.11.m11.1c">0.0001</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.12" class="ltx_text" style="font-size:173%;">, and </span><math id="A8.SS0.SSS0.Px2.p1.12.m12.1" class="ltx_Math" alttext="0" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.12.m12.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.12.m12.1.1" xref="A8.SS0.SSS0.Px2.p1.12.m12.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.12.m12.1b"><cn type="integer" id="A8.SS0.SSS0.Px2.p1.12.m12.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.12.m12.1.1">0</cn></annotation-xml></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.13" class="ltx_text" style="font-size:173%;"> respectively. By convention, strong and weak data augmentations are applied in pre-training and domain adaptation respectively. For domain adaptation on our proposed S2RDA benchmark, we use ResNet-50 as the backbone, which is initialized by the official ImageNet pre-trained checkpoint </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A8.SS0.SSS0.Px2.p1.13.14.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="A8.SS0.SSS0.Px2.p1.13.15.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A8.SS0.SSS0.Px2.p1.13.16" class="ltx_text" style="font-size:173%;">. The initial learning rate is set as </span><math id="A8.SS0.SSS0.Px2.p1.13.m13.1" class="ltx_Math" alttext="0.0001" display="inline"><semantics id="A8.SS0.SSS0.Px2.p1.13.m13.1a"><mn mathsize="173%" id="A8.SS0.SSS0.Px2.p1.13.m13.1.1" xref="A8.SS0.SSS0.Px2.p1.13.m13.1.1.cmml">0.0001</mn><annotation-xml encoding="MathML-Content" id="A8.SS0.SSS0.Px2.p1.13.m13.1b"><cn type="float" id="A8.SS0.SSS0.Px2.p1.13.m13.1.1.cmml" xref="A8.SS0.SSS0.Px2.p1.13.m13.1.1">0.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.SS0.SSS0.Px2.p1.13.m13.1c">0.0001</annotation></semantics></math><span id="A8.SS0.SSS0.Px2.p1.13.17" class="ltx_text" style="font-size:173%;"> across all experiments. Other implementation details are the same as those described above.</span></p>
</div>
<div id="A8.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="A8.SS0.SSS0.Px2.p2.1" class="ltx_p"><span id="A8.SS0.SSS0.Px2.p2.1.1" class="ltx_text" style="font-size:173%;">More details are as follows.</span></p>
<ol id="A8.I1" class="ltx_enumerate">
<li id="A8.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="A8.I1.i1.p1" class="ltx_para">
<p id="A8.I1.i1.p1.1" class="ltx_p"><span id="A8.I1.i1.p1.1.1" class="ltx_text" style="font-size:173%;">For 3D rendering with domain randomization, the monocular camera default in BlenderProc </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A8.I1.i1.p1.1.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib18" title="" class="ltx_ref">18</a><span id="A8.I1.i1.p1.1.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A8.I1.i1.p1.1.4" class="ltx_text" style="font-size:173%;"> is used in the 3D renderer.</span></p>
</div>
</li>
<li id="A8.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="A8.I1.i2.p1" class="ltx_para">
<p id="A8.I1.i2.p1.1" class="ltx_p"><span id="A8.I1.i2.p1.1.1" class="ltx_text" style="font-size:173%;">SubVisDA-10 includes the following </span><math id="A8.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="A8.I1.i2.p1.1.m1.1a"><mn mathsize="173%" id="A8.I1.i2.p1.1.m1.1.1" xref="A8.I1.i2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="A8.I1.i2.p1.1.m1.1b"><cn type="integer" id="A8.I1.i2.p1.1.m1.1.1.cmml" xref="A8.I1.i2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.I1.i2.p1.1.m1.1c">10</annotation></semantics></math><span id="A8.I1.i2.p1.1.2" class="ltx_text" style="font-size:173%;"> classes: airplane, bicycle, bus, car, knife, motorbike, plant, skateboard, train, and truck.</span></p>
</div>
</li>
<li id="A8.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="A8.I1.i3.p1" class="ltx_para">
<p id="A8.I1.i3.p1.4" class="ltx_p"><span id="A8.I1.i3.p1.4.1" class="ltx_text" style="font-size:173%;">MetaShift </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A8.I1.i3.p1.4.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib52" title="" class="ltx_ref">52</a><span id="A8.I1.i3.p1.4.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A8.I1.i3.p1.4.4" class="ltx_text" style="font-size:173%;"> we use is a filtered version of </span><math id="A8.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="2559865" display="inline"><semantics id="A8.I1.i3.p1.1.m1.1a"><mn mathsize="173%" id="A8.I1.i3.p1.1.m1.1.1" xref="A8.I1.i3.p1.1.m1.1.1.cmml">2559865</mn><annotation-xml encoding="MathML-Content" id="A8.I1.i3.p1.1.m1.1b"><cn type="integer" id="A8.I1.i3.p1.1.m1.1.1.cmml" xref="A8.I1.i3.p1.1.m1.1.1">2559865</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.I1.i3.p1.1.m1.1c">2559865</annotation></semantics></math><span id="A8.I1.i3.p1.4.5" class="ltx_text" style="font-size:173%;"> images from </span><math id="A8.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="376" display="inline"><semantics id="A8.I1.i3.p1.2.m2.1a"><mn mathsize="173%" id="A8.I1.i3.p1.2.m2.1.1" xref="A8.I1.i3.p1.2.m2.1.1.cmml">376</mn><annotation-xml encoding="MathML-Content" id="A8.I1.i3.p1.2.m2.1b"><cn type="integer" id="A8.I1.i3.p1.2.m2.1.1.cmml" xref="A8.I1.i3.p1.2.m2.1.1">376</cn></annotation-xml><annotation encoding="application/x-tex" id="A8.I1.i3.p1.2.m2.1c">376</annotation></semantics></math><span id="A8.I1.i3.p1.4.6" class="ltx_text" style="font-size:173%;"> classes by running the officially provided code of dataset construction. It is formed by setting a threshold for subset size (</span><math id="A8.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="&gt;=25" display="inline"><semantics id="A8.I1.i3.p1.3.m3.1a"><mrow id="A8.I1.i3.p1.3.m3.1.1" xref="A8.I1.i3.p1.3.m3.1.1.cmml"><mi id="A8.I1.i3.p1.3.m3.1.1.2" xref="A8.I1.i3.p1.3.m3.1.1.2.cmml"></mi><mo lspace="0.278em" mathsize="173%" rspace="0.278em" id="A8.I1.i3.p1.3.m3.1.1.1" xref="A8.I1.i3.p1.3.m3.1.1.1.cmml">&gt;=</mo><mn mathsize="173%" id="A8.I1.i3.p1.3.m3.1.1.3" xref="A8.I1.i3.p1.3.m3.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="A8.I1.i3.p1.3.m3.1b"><apply id="A8.I1.i3.p1.3.m3.1.1.cmml" xref="A8.I1.i3.p1.3.m3.1.1"><geq id="A8.I1.i3.p1.3.m3.1.1.1.cmml" xref="A8.I1.i3.p1.3.m3.1.1.1"></geq><csymbol cd="latexml" id="A8.I1.i3.p1.3.m3.1.1.2.cmml" xref="A8.I1.i3.p1.3.m3.1.1.2">absent</csymbol><cn type="integer" id="A8.I1.i3.p1.3.m3.1.1.3.cmml" xref="A8.I1.i3.p1.3.m3.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.I1.i3.p1.3.m3.1c">&gt;=25</annotation></semantics></math><span id="A8.I1.i3.p1.4.7" class="ltx_text" style="font-size:173%;">) and subset number in one class (</span><math id="A8.I1.i3.p1.4.m4.1" class="ltx_Math" alttext="&gt;5" display="inline"><semantics id="A8.I1.i3.p1.4.m4.1a"><mrow id="A8.I1.i3.p1.4.m4.1.1" xref="A8.I1.i3.p1.4.m4.1.1.cmml"><mi id="A8.I1.i3.p1.4.m4.1.1.2" xref="A8.I1.i3.p1.4.m4.1.1.2.cmml"></mi><mo mathsize="173%" id="A8.I1.i3.p1.4.m4.1.1.1" xref="A8.I1.i3.p1.4.m4.1.1.1.cmml">&gt;</mo><mn mathsize="173%" id="A8.I1.i3.p1.4.m4.1.1.3" xref="A8.I1.i3.p1.4.m4.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A8.I1.i3.p1.4.m4.1b"><apply id="A8.I1.i3.p1.4.m4.1.1.cmml" xref="A8.I1.i3.p1.4.m4.1.1"><gt id="A8.I1.i3.p1.4.m4.1.1.1.cmml" xref="A8.I1.i3.p1.4.m4.1.1.1"></gt><csymbol cd="latexml" id="A8.I1.i3.p1.4.m4.1.1.2.cmml" xref="A8.I1.i3.p1.4.m4.1.1.2">absent</csymbol><cn type="integer" id="A8.I1.i3.p1.4.m4.1.1.3.cmml" xref="A8.I1.i3.p1.4.m4.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.I1.i3.p1.4.m4.1c">&gt;5</annotation></semantics></math><span id="A8.I1.i3.p1.4.8" class="ltx_text" style="font-size:173%;">).</span></p>
</div>
</li>
<li id="A8.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="A8.I1.i4.p1" class="ltx_para">
<p id="A8.I1.i4.p1.1" class="ltx_p"><span id="A8.I1.i4.p1.1.1" class="ltx_text" style="font-size:173%;">Mean class precision is the average over recognition precisions of all classes. It is an indicator of class imbalance that different categories have different prediction accuracy. When it deviates from the overall accuracy in a test, class imbalance happens.</span></p>
</div>
</li>
<li id="A8.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="A8.I1.i5.p1" class="ltx_para">
<p id="A8.I1.i5.p1.1" class="ltx_p"><span id="A8.I1.i5.p1.1.1" class="ltx_text" style="font-size:173%;">In Table 3, the number highlighted by the green color indicates the best Acc. in each row (among all compared DA methods), the number underlined by the red color indicates the best Mean in each row (among all compared DA methods), and the bold number in each column indicates the best result among all considered pre-training schemes. In Table 4, the bold number highlighted by the green color indicates the best Acc. in each row (among all compared DA methods), and the bold number underlined by the red color indicates the best Mean in each row (among all compared DA methods).</span></p>
</div>
</li>
<li id="A8.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="A8.I1.i6.p1" class="ltx_para">
<p id="A8.I1.i6.p1.1" class="ltx_p"><span id="A8.I1.i6.p1.1.1" class="ltx_text" style="font-size:173%;">PyTorch </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A8.I1.i6.p1.1.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib60" title="" class="ltx_ref">60</a><span id="A8.I1.i6.p1.1.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A8.I1.i6.p1.1.4" class="ltx_text" style="font-size:173%;"> is used for implementation. Grid Search is used for hyperparameter tuning. We use an 8-GPU NVIDIA GeForce GTX 1080 and an 8-GPU NVIDIA Tesla M40 to run experiments. For the used assets, we have cited the corresponding references in the main paper, and we mention their licenses here: CCTextures under CC0 License, Haven under CC0 License, ShapeNet under a custom license, VisDA-2017 under MIT License, ImageNet under a custom license, and MetaShift under MIT License.</span></p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="A9" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:173%;">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Other Related Works</h2>

<div id="A9.p1" class="ltx_para ltx_noindent">
<p id="A9.p1.9" class="ltx_p"><span id="A9.p1.9.1" class="ltx_text ltx_font_bold" style="font-size:173%;">Real Datasets.</span><span id="A9.p1.9.2" class="ltx_text" style="font-size:173%;"> A lot of large-scale real datasets </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.3.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a>, <a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a><span id="A9.p1.9.4.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.5" class="ltx_text" style="font-size:173%;"> have harnessed and organized the explosive image data from Internet or the real world for deep learning of meaningful visual representations. For example, ImageNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.6.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib17" title="" class="ltx_ref">17</a><span id="A9.p1.9.7.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.8" class="ltx_text" style="font-size:173%;"> is a large-scale database of images built upon the backbone of the WordNet structure; ImageNet-1K, consisting of </span><math id="A9.p1.1.m1.1" class="ltx_Math" alttext="1.28" display="inline"><semantics id="A9.p1.1.m1.1a"><mn mathsize="173%" id="A9.p1.1.m1.1.1" xref="A9.p1.1.m1.1.1.cmml">1.28</mn><annotation-xml encoding="MathML-Content" id="A9.p1.1.m1.1b"><cn type="float" id="A9.p1.1.m1.1.1.cmml" xref="A9.p1.1.m1.1.1">1.28</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.1.m1.1c">1.28</annotation></semantics></math><span id="A9.p1.9.9" class="ltx_text" style="font-size:173%;">M images from </span><math id="A9.p1.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="A9.p1.2.m2.1a"><mn mathsize="173%" id="A9.p1.2.m2.1.1" xref="A9.p1.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A9.p1.2.m2.1b"><cn type="integer" id="A9.p1.2.m2.1.1.cmml" xref="A9.p1.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.2.m2.1c">1</annotation></semantics></math><span id="A9.p1.9.10" class="ltx_text" style="font-size:173%;">K common object categories, which serves as the primary dataset for pre-training deep models for computer vision tasks. Barbu et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.11.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib5" title="" class="ltx_ref">5</a><span id="A9.p1.9.12.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.13" class="ltx_text" style="font-size:173%;"> collect a large real-world test set for more realistic object recognition, ObjectNet, which has </span><math id="A9.p1.3.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="A9.p1.3.m3.1a"><mn mathsize="173%" id="A9.p1.3.m3.1.1" xref="A9.p1.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="A9.p1.3.m3.1b"><cn type="integer" id="A9.p1.3.m3.1.1.cmml" xref="A9.p1.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.3.m3.1c">50</annotation></semantics></math><span id="A9.p1.9.14" class="ltx_text" style="font-size:173%;">K images and is bias-controlled. Ridnik et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.15.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib64" title="" class="ltx_ref">64</a><span id="A9.p1.9.16.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.17" class="ltx_text" style="font-size:173%;"> dedicatedly preprocess the full set of ImageNet — ImageNet-21K with the WordNet hierarchical structure utilized, such that high-quality efficient pre-training on the resulted ImageNet-21K-P (of </span><math id="A9.p1.4.m4.1" class="ltx_Math" alttext="12" display="inline"><semantics id="A9.p1.4.m4.1a"><mn mathsize="173%" id="A9.p1.4.m4.1.1" xref="A9.p1.4.m4.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="A9.p1.4.m4.1b"><cn type="integer" id="A9.p1.4.m4.1.1.cmml" xref="A9.p1.4.m4.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.4.m4.1c">12</annotation></semantics></math><span id="A9.p1.9.18" class="ltx_text" style="font-size:173%;">M images) can be made for practical use. In </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.19.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib74" title="" class="ltx_ref">74</a><span id="A9.p1.9.20.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.21" class="ltx_text" style="font-size:173%;">, JFT-300M of more than </span><math id="A9.p1.5.m5.1" class="ltx_Math" alttext="375" display="inline"><semantics id="A9.p1.5.m5.1a"><mn mathsize="173%" id="A9.p1.5.m5.1.1" xref="A9.p1.5.m5.1.1.cmml">375</mn><annotation-xml encoding="MathML-Content" id="A9.p1.5.m5.1b"><cn type="integer" id="A9.p1.5.m5.1.1.cmml" xref="A9.p1.5.m5.1.1">375</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.5.m5.1c">375</annotation></semantics></math><span id="A9.p1.9.22" class="ltx_text" style="font-size:173%;">M noisy labels for </span><math id="A9.p1.6.m6.1" class="ltx_Math" alttext="300" display="inline"><semantics id="A9.p1.6.m6.1a"><mn mathsize="173%" id="A9.p1.6.m6.1.1" xref="A9.p1.6.m6.1.1.cmml">300</mn><annotation-xml encoding="MathML-Content" id="A9.p1.6.m6.1b"><cn type="integer" id="A9.p1.6.m6.1.1.cmml" xref="A9.p1.6.m6.1.1">300</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.6.m6.1c">300</annotation></semantics></math><span id="A9.p1.9.23" class="ltx_text" style="font-size:173%;">M images is exploited to study the effects of pre-training on current vision tasks. Geirhos et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.24.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="A9.p1.9.25.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.26" class="ltx_text" style="font-size:173%;"> construct StylizedImageNet by replacing the object texture in an image with a random painting style via style transfer, to learn a shape-based representation. MS COCO </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.27.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="A9.p1.9.28.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.29" class="ltx_text" style="font-size:173%;"> has </span><math id="A9.p1.7.m7.1" class="ltx_Math" alttext="328" display="inline"><semantics id="A9.p1.7.m7.1a"><mn mathsize="173%" id="A9.p1.7.m7.1.1" xref="A9.p1.7.m7.1.1.cmml">328</mn><annotation-xml encoding="MathML-Content" id="A9.p1.7.m7.1b"><cn type="integer" id="A9.p1.7.m7.1.1.cmml" xref="A9.p1.7.m7.1.1">328</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.7.m7.1c">328</annotation></semantics></math><span id="A9.p1.9.30" class="ltx_text" style="font-size:173%;">K images, where considerably more object instances exist as compared to ImageNet, enabling deep models to learn precise 2D localization.
MetaShift </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.31.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib52" title="" class="ltx_ref">52</a><span id="A9.p1.9.32.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.33" class="ltx_text" style="font-size:173%;"> of </span><math id="A9.p1.8.m8.1" class="ltx_Math" alttext="2.56" display="inline"><semantics id="A9.p1.8.m8.1a"><mn mathsize="173%" id="A9.p1.8.m8.1.1" xref="A9.p1.8.m8.1.1.cmml">2.56</mn><annotation-xml encoding="MathML-Content" id="A9.p1.8.m8.1b"><cn type="float" id="A9.p1.8.m8.1.1.cmml" xref="A9.p1.8.m8.1.1">2.56</cn></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.8.m8.1c">2.56</annotation></semantics></math><span id="A9.p1.9.34" class="ltx_text" style="font-size:173%;">M natural images (</span><math id="A9.p1.9.m9.1" class="ltx_Math" alttext="\sim 400" display="inline"><semantics id="A9.p1.9.m9.1a"><mrow id="A9.p1.9.m9.1.1" xref="A9.p1.9.m9.1.1.cmml"><mi id="A9.p1.9.m9.1.1.2" xref="A9.p1.9.m9.1.1.2.cmml"></mi><mo mathsize="173%" id="A9.p1.9.m9.1.1.1" xref="A9.p1.9.m9.1.1.1.cmml">∼</mo><mn mathsize="173%" id="A9.p1.9.m9.1.1.3" xref="A9.p1.9.m9.1.1.3.cmml">400</mn></mrow><annotation-xml encoding="MathML-Content" id="A9.p1.9.m9.1b"><apply id="A9.p1.9.m9.1.1.cmml" xref="A9.p1.9.m9.1.1"><csymbol cd="latexml" id="A9.p1.9.m9.1.1.1.cmml" xref="A9.p1.9.m9.1.1.1">similar-to</csymbol><csymbol cd="latexml" id="A9.p1.9.m9.1.1.2.cmml" xref="A9.p1.9.m9.1.1.2">absent</csymbol><cn type="integer" id="A9.p1.9.m9.1.1.3.cmml" xref="A9.p1.9.m9.1.1.3">400</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A9.p1.9.m9.1c">\sim 400</annotation></semantics></math><span id="A9.p1.9.35" class="ltx_text" style="font-size:173%;"> classes) is formed by context guided clustering of the images from GQA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.36.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib35" title="" class="ltx_ref">35</a><span id="A9.p1.9.37.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.38" class="ltx_text" style="font-size:173%;">, a cleaned version of Visual Genome </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.39.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib45" title="" class="ltx_ref">45</a><span id="A9.p1.9.40.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.41" class="ltx_text" style="font-size:173%;"> connecting language and vision. CCT-20 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.42.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a><span id="A9.p1.9.43.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.44" class="ltx_text" style="font-size:173%;"> and PACS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.45.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="A9.p1.9.46.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.47" class="ltx_text" style="font-size:173%;"> are designed to measure recognition generalization to novel visual domains. Some works </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.48.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib44" title="" class="ltx_ref">44</a>, <a href="#bib.bib75" title="" class="ltx_ref">75</a><span id="A9.p1.9.49.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.50" class="ltx_text" style="font-size:173%;"> leverage free, numerous web data to benchmark or assist fine-grained recognition. Some small datasets are used as benchmarks for semi-supervised learning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.51.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib57" title="" class="ltx_ref">57</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a><span id="A9.p1.9.52.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.53" class="ltx_text" style="font-size:173%;"> or domain generalization </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.p1.9.54.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib48" title="" class="ltx_ref">48</a><span id="A9.p1.9.55.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.p1.9.56" class="ltx_text" style="font-size:173%;">.</span></p>
</div>
<section id="A9.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Data Manipulation.</h5>

<div id="A9.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A9.SS0.SSS0.Px1.p1.1" class="ltx_p"><span id="A9.SS0.SSS0.Px1.p1.1.1" class="ltx_text" style="font-size:173%;">Deep models are hungry for more training data in that the generalization ability often relies on the quantity and diversity of training samples. To improve model generalization with a limited set of training data available, the cheapest and simplest way is data manipulation, which increases the sample diversity from two different perspectives of data augmentation and data generation. The former applies a series of random image transformations </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px1.p1.1.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib72" title="" class="ltx_ref">72</a><span id="A9.SS0.SSS0.Px1.p1.1.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px1.p1.1.4" class="ltx_text" style="font-size:173%;"> or appends adversarial examples at each iteration </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px1.p1.1.5.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a><span id="A9.SS0.SSS0.Px1.p1.1.6.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px1.p1.1.7" class="ltx_text" style="font-size:173%;">; the latter uses generative models to generate diverse and rich data such as Variational Auto-Encoder (VAE) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px1.p1.1.8.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib63" title="" class="ltx_ref">63</a><span id="A9.SS0.SSS0.Px1.p1.1.9.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px1.p1.1.10" class="ltx_text" style="font-size:173%;"> and Generative Adversarial Network (GAN) </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px1.p1.1.11.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="A9.SS0.SSS0.Px1.p1.1.12.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px1.p1.1.13" class="ltx_text" style="font-size:173%;"> or renders 3D object models into RGB images via domain randomization </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px1.p1.1.14.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib79" title="" class="ltx_ref">79</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib81" title="" class="ltx_ref">81</a><span id="A9.SS0.SSS0.Px1.p1.1.15.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px1.p1.1.16" class="ltx_text" style="font-size:173%;">.</span></p>
</div>
</section>
<section id="A9.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Deep Models.</h5>

<div id="A9.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A9.SS0.SSS0.Px2.p1.1" class="ltx_p"><span id="A9.SS0.SSS0.Px2.p1.1.1" class="ltx_text" style="font-size:173%;">Deep model has strong representational capacity in that they can learn powerful, hierarchical representations when trained on large amounts of data; they can be highly scalable from various aspects of architectural innovation, such as spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Deep Convolutional Neural Networks (CNNs) have been popularized for decades in a wide range of computer vision tasks. A comprehensive survey for CNNs can be found in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px2.p1.1.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib39" title="" class="ltx_ref">39</a><span id="A9.SS0.SSS0.Px2.p1.1.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px2.p1.1.4" class="ltx_text" style="font-size:173%;">. The most commonly used CNN-based network architecture is ResNet </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px2.p1.1.5.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib31" title="" class="ltx_ref">31</a><span id="A9.SS0.SSS0.Px2.p1.1.6.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px2.p1.1.7" class="ltx_text" style="font-size:173%;">, which reformulates the layers as learning residual functions concerning the layer inputs, instead of learning unreferenced functions. Recently, some new types of network architectures have emerged, such as ViT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px2.p1.1.8.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib20" title="" class="ltx_ref">20</a><span id="A9.SS0.SSS0.Px2.p1.1.9.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px2.p1.1.10" class="ltx_text" style="font-size:173%;"> and MLP-Mixer </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px2.p1.1.11.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib80" title="" class="ltx_ref">80</a><span id="A9.SS0.SSS0.Px2.p1.1.12.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px2.p1.1.13" class="ltx_text" style="font-size:173%;">.
ViT stacks a certain number of multi-head self-attention layers and is applied directly to sequences of fixed-size image patches.
MLP-Mixer is based exclusively on multi-layer perceptrons (MLPs) and contains two types of MLP layers: one for mixing channels in individual image patches and one for mixing features across patches of different spatial locations. In this work, we experiment on the three representative types of networks.</span></p>
</div>
</section>
<section id="A9.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">Transfer Learning.</h5>

<div id="A9.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A9.SS0.SSS0.Px3.p1.1" class="ltx_p"><span id="A9.SS0.SSS0.Px3.p1.1.1" class="ltx_text" style="font-size:173%;">There has been a huge literature in the field of transfer learning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib58" title="" class="ltx_ref">58</a>, <a href="#bib.bib94" title="" class="ltx_ref">94</a>, <a href="#bib.bib37" title="" class="ltx_ref">37</a><span id="A9.SS0.SSS0.Px3.p1.1.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.4" class="ltx_text" style="font-size:173%;">, where the paradigm of pre-training and then fine-tuning has made outstanding achievements in many deep learning applications. Extensive studies have been done for supervised pre-training </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.5.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib90" title="" class="ltx_ref">90</a>, <a href="#bib.bib43" title="" class="ltx_ref">43</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib88" title="" class="ltx_ref">88</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="A9.SS0.SSS0.Px3.p1.1.6.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.7" class="ltx_text" style="font-size:173%;">. For example, Yosinski et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.8.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib90" title="" class="ltx_ref">90</a><span id="A9.SS0.SSS0.Px3.p1.1.9.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.10" class="ltx_text" style="font-size:173%;"> examine the transferability of features at different layers along the network; the relationship between ImageNet accuracy and transferability is evaluated in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.11.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib43" title="" class="ltx_ref">43</a><span id="A9.SS0.SSS0.Px3.p1.1.12.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.13" class="ltx_text" style="font-size:173%;">; BiT </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.14.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib42" title="" class="ltx_ref">42</a><span id="A9.SS0.SSS0.Px3.p1.1.15.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.16" class="ltx_text" style="font-size:173%;"> provides a recipe of the minimal number of existing tricks for pre-training and downstream transferring; in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.17.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib88" title="" class="ltx_ref">88</a><span id="A9.SS0.SSS0.Px3.p1.1.18.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.19" class="ltx_text" style="font-size:173%;">, an MLP projector is added before the classifier to improve the transferability; LOOK </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.20.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib22" title="" class="ltx_ref">22</a><span id="A9.SS0.SSS0.Px3.p1.1.21.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.22" class="ltx_text" style="font-size:173%;"> solves the problem of overfitting upstream tasks by only allowing nearest neighbors to share the class label, in order to preserve the intra-class semantic difference; particularly, Kin et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.23.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="A9.SS0.SSS0.Px3.p1.1.24.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.25" class="ltx_text" style="font-size:173%;"> preliminarily study the effects of pre-training on domain transfer tasks, from the aspects of network architectures, size, pre-training loss, and datasets. Another popular branch of self-supervised learning is increasingly important for transfer learning. Previous works have proposed various pretext tasks, such as image inpainting and jigsaw puzzle </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.26.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a><span id="A9.SS0.SSS0.Px3.p1.1.27.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.28" class="ltx_text" style="font-size:173%;">. Recent works concentrate on self-supervised/unsupervised pre-training </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.29.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib51" title="" class="ltx_ref">51</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="A9.SS0.SSS0.Px3.p1.1.30.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.31" class="ltx_text" style="font-size:173%;"> and have shown powerful transferability on multiple downstream tasks, comparable to supervised pre-training. They often rely on contrastive learning to learn visual representations of rich intra-class diversity </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.32.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib91" title="" class="ltx_ref">91</a><span id="A9.SS0.SSS0.Px3.p1.1.33.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.34" class="ltx_text" style="font-size:173%;">, e.g., contrasting feature embeddings </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.35.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib29" title="" class="ltx_ref">29</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="A9.SS0.SSS0.Px3.p1.1.36.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.37" class="ltx_text" style="font-size:173%;"> or cluster assignments </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.38.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib10" title="" class="ltx_ref">10</a><span id="A9.SS0.SSS0.Px3.p1.1.39.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.40" class="ltx_text" style="font-size:173%;"> of anchor, positive, and negative instances. Note that CDS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.41.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="A9.SS0.SSS0.Px3.p1.1.42.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.43" class="ltx_text" style="font-size:173%;"> proposes a second self-supervised pre-training stage using the unlabeled downstream data from multiple domains, which applies instance discrimination not only in individual domains but also across domains. Also, many researchers are devoted to improving fine-tuning by leveraging the pre-trained ImageNet knowledge </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.44.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="A9.SS0.SSS0.Px3.p1.1.45.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.46" class="ltx_text" style="font-size:173%;">, using pre-training data for fine-tuning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.47.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib55" title="" class="ltx_ref">55</a><span id="A9.SS0.SSS0.Px3.p1.1.48.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.49" class="ltx_text" style="font-size:173%;">, improving regularization and robustness </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.50.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib49" title="" class="ltx_ref">49</a><span id="A9.SS0.SSS0.Px3.p1.1.51.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.52" class="ltx_text" style="font-size:173%;">, adapting unfamiliar inputs </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.53.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib3" title="" class="ltx_ref">3</a><span id="A9.SS0.SSS0.Px3.p1.1.54.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.55" class="ltx_text" style="font-size:173%;">, applying the easy two-step strategy of linear probing and then full fine-tuning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.56.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib47" title="" class="ltx_ref">47</a><span id="A9.SS0.SSS0.Px3.p1.1.57.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.58" class="ltx_text" style="font-size:173%;">, to name a few.
Different from </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p1.1.59.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib41" title="" class="ltx_ref">41</a><span id="A9.SS0.SSS0.Px3.p1.1.60.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p1.1.61" class="ltx_text" style="font-size:173%;">, we focus on the utility of synthetic data and take the first step towards clearing the cloud of mystery surrounding how different pre-training schemes including synthetic data pre-training affect the practical, large-scale synthetic-to-real adaptation.</span></p>
</div>
<div id="A9.SS0.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<p id="A9.SS0.SSS0.Px3.p2.1" class="ltx_p"><span id="A9.SS0.SSS0.Px3.p2.1.1" class="ltx_text ltx_font_bold" style="font-size:173%;">Domain Adaptation.</span><span id="A9.SS0.SSS0.Px3.p2.1.2" class="ltx_text" style="font-size:173%;"> Domain adaptation is a developing field with a huge diversity of approaches. A popular strategy is to explicitly model and minimize the distribution shift between the source and target domains </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p2.1.3.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib93" title="" class="ltx_ref">93</a>, <a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a><span id="A9.SS0.SSS0.Px3.p2.1.4.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p2.1.5" class="ltx_text" style="font-size:173%;">, such that the domain-invariant features can be learned and thus the task classifier trained on the labeled source data can well generalize to the unlabeled target domain.
DANN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p2.1.6.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib23" title="" class="ltx_ref">23</a><span id="A9.SS0.SSS0.Px3.p2.1.7.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p2.1.8" class="ltx_text" style="font-size:173%;"> aligns the source and target domains as a whole by domain-adversarial training, i.e., reversing the signal from a domain discriminator, but does not utilize the discriminative information from the target domain.
MCD </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p2.1.9.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib69" title="" class="ltx_ref">69</a><span id="A9.SS0.SSS0.Px3.p2.1.10.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p2.1.11" class="ltx_text" style="font-size:173%;"> minimizes the maximum prediction discrepancy between two task classifiers to learn domain-invariant and class-discriminative features.
RCA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p2.1.12.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib14" title="" class="ltx_ref">14</a><span id="A9.SS0.SSS0.Px3.p2.1.13.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p2.1.14" class="ltx_text" style="font-size:173%;"> implements the domain-adversarial training based on a joint domain-category classifier to learn class-level aligned features, i.e., invariant at corresponding classes of the two domains.
Differently, works of another emerging strategy </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p2.1.15.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib54" title="" class="ltx_ref">54</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="A9.SS0.SSS0.Px3.p2.1.16.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p2.1.17" class="ltx_text" style="font-size:173%;"> take steps towards implicit domain adaptation, without explicit feature alignment that could hurt the intrinsic discriminative structures of target data.
SRDC </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p2.1.18.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib76" title="" class="ltx_ref">76</a><span id="A9.SS0.SSS0.Px3.p2.1.19.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p2.1.20" class="ltx_text" style="font-size:173%;"> uncovers the intrinsic target discrimination via deep discriminative target clustering in both the output and feature spaces with structural source regularization hinging on the assumption of structural similarity across domains.
DisClusterDA </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p2.1.21.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib78" title="" class="ltx_ref">78</a><span id="A9.SS0.SSS0.Px3.p2.1.22.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p2.1.23" class="ltx_text" style="font-size:173%;"> proposes a new clustering objective for discriminative clustering of target data with distilled informative source knowledge, based on a robust variant of entropy minimization, a soft Fisher-like criterion, and the cluster ordering via centroid classification.
In this work, we consider these representative DA methods for the empirical study, and broader introductions to the rich literature are provided in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px3.p2.1.24.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib89" title="" class="ltx_ref">89</a>, <a href="#bib.bib68" title="" class="ltx_ref">68</a><span id="A9.SS0.SSS0.Px3.p2.1.25.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px3.p2.1.26" class="ltx_text" style="font-size:173%;">.</span></p>
</div>
</section>
<section id="A9.SS0.SSS0.Px4" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:173%;">OOD Generalization.</h5>

<div id="A9.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="A9.SS0.SSS0.Px4.p1.1" class="ltx_p"><span id="A9.SS0.SSS0.Px4.p1.1.1" class="ltx_text" style="font-size:173%;">Out-of-distribution (OOD) generalization, i.e., domain generalization, assumes the access to single or multiple different but related domains and aims to generalize the learned model to an unseen test domain. A detailed review for recent advances in domain generalization is presented in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px4.p1.1.2.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib87" title="" class="ltx_ref">87</a><span id="A9.SS0.SSS0.Px4.p1.1.3.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px4.p1.1.4" class="ltx_text" style="font-size:173%;">, which categorizes the popular algorithms into three classes: data manipulation </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px4.p1.1.5.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a><span id="A9.SS0.SSS0.Px4.p1.1.6.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px4.p1.1.7" class="ltx_text" style="font-size:173%;">, representation learning </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px4.p1.1.8.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a><span id="A9.SS0.SSS0.Px4.p1.1.9.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px4.p1.1.10" class="ltx_text" style="font-size:173%;">, and learning strategy </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px4.p1.1.11.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a><span id="A9.SS0.SSS0.Px4.p1.1.12.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px4.p1.1.13" class="ltx_text" style="font-size:173%;">. For example, adversarial examples are generated to learn robust models in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px4.p1.1.14.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib86" title="" class="ltx_ref">86</a><span id="A9.SS0.SSS0.Px4.p1.1.15.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px4.p1.1.16" class="ltx_text" style="font-size:173%;">; a progressive domain expansion subnetwork and a domain-invariant representation learning subnetwork are jointly learned to mutually benefit from each other in </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px4.p1.1.17.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="A9.SS0.SSS0.Px4.p1.1.18.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px4.p1.1.19" class="ltx_text" style="font-size:173%;">; Balaji et al. </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A9.SS0.SSS0.Px4.p1.1.20.1" class="ltx_text" style="font-size:173%;">[</span><a href="#bib.bib4" title="" class="ltx_ref">4</a><span id="A9.SS0.SSS0.Px4.p1.1.21.2" class="ltx_text" style="font-size:173%;">]</span></cite><span id="A9.SS0.SSS0.Px4.p1.1.22" class="ltx_text" style="font-size:173%;"> adopt the meta-learning strategy to learn a regularizer that the model trained on one domain can well generalize to another domain.</span></p>
</div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2303.09164" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2303.09165" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2303.09165">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2303.09165" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2303.09166" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 19:35:57 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
