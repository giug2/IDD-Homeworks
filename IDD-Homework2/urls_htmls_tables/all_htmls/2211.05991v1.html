<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2211.05991] MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering</title><meta property="og:description" content="There is a key problem in the medical visual question answering task that how to effectively realize the feature fusion of language and medical images with limited datasets. In order to better utilize multi-scale inforâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2211.05991">

<!--Generated on Thu Mar 14 08:05:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MF<sup id="id4.id1" class="ltx_sup"><span id="id4.id1.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.2" class="ltx_p">There is a key problem in the medical visual question answering task that how to effectively realize the feature fusion of language and medical images with limited datasets. In order to better utilize multi-scale information of medical images, previous methods directly embed the multi-stage visual feature maps as tokens of same size respectively and fuse them with text representation. However, this will cause the confusion of visual features at different stages. To this end, we propose a simple but powerful multi-stage feature fusion method, MF<sup id="id3.2.1" class="ltx_sup"><span id="id3.2.1.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA, which stage-wise fuses multi-level visual features with textual semantics. MF<sup id="id3.2.2" class="ltx_sup"><span id="id3.2.2.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA achieves the State-Of-The-Art performance on VQA-Med 2019 and VQA-RAD dataset. The results of visualization also verify that our model outperforms previous work.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">â€”â€‰</span></span>
Medical visual question answering, Multimodal feature fusion, Transformer</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Medical visual question answering (Med-VQA) is a new exploration in visual question answering, focusing on answering relevant questions raised by specific medical images. Different from natural image datasets, professional medical prior knowledge is needed in medical dataset collection. Therefore, realizing effective Med-VQA systems in limited datasets are endowed with great importance for improving the diagnostic processing and reducing the burden of professionals, such as helping patients have basic understanding of their conditions and assisting doctors in obtaining a more precise diagnosis.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2211.05991/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="227" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 1</span>: </span>Structure comparison of our model and previous work for medical visual question answering task</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Recent works on Med-VQA mainly consist of two categories. One way is by manually designing various modules to enhance the featureâ€™s reasoning capability, including conditional reasoning framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> aiming at improving the reasoning parts of Med-VQA and cross-modal self-attention module <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to capture the long-range contextual information. Considering the strong capability of Transformer to fuse multimodal information, the other type of work makes modifications and innovations on the basis of Transformer to integrate the visual features and linguistic semantics. Ren et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> proposed a multimodal Transformer architecture named CGMVQA, which is the first model utilizing different convolutional layer outputs from the pre-trained CNN-based model as the input image tokens. Khare et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> pre-trained MMBERT model on a set of medical image-text pairs for the masked task and achieved great performance on VQA-Med 2019 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and VQA-RAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite> datasets. By encoding visual features from different stages to image tokens, such methods take advantage of the transformer and realize multi-scale fusion to a certain extent.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, due to visual features of different stages containing diverse and specific information respectively, encoding them into the same resolution and then sending them together may lead to the loss of informative data. As shown in Fig.<a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, based on the aforementioned problems, our work focuses on improving the fusion parts for better integration of images and language information in Transformer architecture. We propose a new Transformer-based framework for Med-VQA named MF<sup id="S1.p3.1.1" class="ltx_sup"><span id="S1.p3.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA. By designing a specific masked method and layer-wise addition of image tokens for fusing multi-stage image features, our new method is capable of fully fusing multi-stage image semantics and questions. We conduct experiments on VQA-Med 2019 and VQA-RAD datasets, the results demonstrate the effectiveness of our method and the superiority over other previous methods.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2211.05991/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="514" height="185" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.5.1.1" class="ltx_text ltx_font_bold">Fig.Â 2</span>: </span>Overall architecture of our proposed MF<sup id="S1.F2.6.2" class="ltx_sup"><span id="S1.F2.6.2.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA.The medical image is fed into the CNN encoder to obtain feature maps of different stages, and then those are added to each layer of transformer stage by stage. The visual token spaces without image feature input will be set to [MASK] tokens. The question is embedded firstly, and then the textual tokens are directly fed to the transformer structure. Finally, the classifier outputs the predicted result from candidate answers.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Transformer</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">MF<sup id="S2.SS1.p1.1.1" class="ltx_sup"><span id="S2.SS1.p1.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA utilizes BERT-like layers as the basis of cross-modal fusion module. BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a multi-layer bidirectional Transformer encoder capable of modeling the dependencies of all input tokens.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">For vanilla transformer structure<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, it is a popular sequence transduction model. By stacking transformer encoders and decoders, models are conducted with strong sequence learning capability and efficient parallel computing. The self-attention mechanism in transformer is utilized for extracting interactions between tokens, hence the inter-modality features and the relationship between visual and linguistic representations could be obtained in our tasks.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">In each attention module, Query (Q), Key (K) and Value (V) are three crucial vectors for attention calculation. The output is a weighted sum of V, where the weight assigned to each value is the similarity between Q and its corresponding K. Specifically, the weights on V are computed by scaling the dot product of Q and V vectors following a softmax function. Meanwhile, transformers use the ensemble of self-attention modules to conduct multi-head attention, allowing the model to learn the interactions from various representation subspaces.</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.4" class="ltx_Math" alttext="Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V" display="block"><semantics id="S2.E1.m1.4a"><mrow id="S2.E1.m1.4.5" xref="S2.E1.m1.4.5.cmml"><mrow id="S2.E1.m1.4.5.2" xref="S2.E1.m1.4.5.2.cmml"><mi id="S2.E1.m1.4.5.2.2" xref="S2.E1.m1.4.5.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.2.3" xref="S2.E1.m1.4.5.2.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1a" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.2.4" xref="S2.E1.m1.4.5.2.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1b" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.2.5" xref="S2.E1.m1.4.5.2.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1c" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.2.6" xref="S2.E1.m1.4.5.2.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1d" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.2.7" xref="S2.E1.m1.4.5.2.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1e" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.2.8" xref="S2.E1.m1.4.5.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1f" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.2.9" xref="S2.E1.m1.4.5.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1g" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.2.10" xref="S2.E1.m1.4.5.2.10.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.2.1h" xref="S2.E1.m1.4.5.2.1.cmml">â€‹</mo><mrow id="S2.E1.m1.4.5.2.11.2" xref="S2.E1.m1.4.5.2.11.1.cmml"><mo stretchy="false" id="S2.E1.m1.4.5.2.11.2.1" xref="S2.E1.m1.4.5.2.11.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">Q</mi><mo id="S2.E1.m1.4.5.2.11.2.2" xref="S2.E1.m1.4.5.2.11.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">K</mi><mo id="S2.E1.m1.4.5.2.11.2.3" xref="S2.E1.m1.4.5.2.11.1.cmml">,</mo><mi id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">V</mi><mo stretchy="false" id="S2.E1.m1.4.5.2.11.2.4" xref="S2.E1.m1.4.5.2.11.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.4.5.1" xref="S2.E1.m1.4.5.1.cmml">=</mo><mrow id="S2.E1.m1.4.5.3" xref="S2.E1.m1.4.5.3.cmml"><mi id="S2.E1.m1.4.5.3.2" xref="S2.E1.m1.4.5.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.1" xref="S2.E1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.3.3" xref="S2.E1.m1.4.5.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.1a" xref="S2.E1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.3.4" xref="S2.E1.m1.4.5.3.4.cmml">f</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.1b" xref="S2.E1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.3.5" xref="S2.E1.m1.4.5.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.1c" xref="S2.E1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.3.6" xref="S2.E1.m1.4.5.3.6.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.1d" xref="S2.E1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.3.7" xref="S2.E1.m1.4.5.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.1e" xref="S2.E1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.3.8" xref="S2.E1.m1.4.5.3.8.cmml">x</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.1f" xref="S2.E1.m1.4.5.3.1.cmml">â€‹</mo><mrow id="S2.E1.m1.4.5.3.9.2" xref="S2.E1.m1.4.4.cmml"><mo stretchy="false" id="S2.E1.m1.4.5.3.9.2.1" xref="S2.E1.m1.4.4.cmml">(</mo><mfrac id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml"><mrow id="S2.E1.m1.4.4.2" xref="S2.E1.m1.4.4.2.cmml"><mi id="S2.E1.m1.4.4.2.2" xref="S2.E1.m1.4.4.2.2.cmml">Q</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.4.2.1" xref="S2.E1.m1.4.4.2.1.cmml">â€‹</mo><msup id="S2.E1.m1.4.4.2.3" xref="S2.E1.m1.4.4.2.3.cmml"><mi id="S2.E1.m1.4.4.2.3.2" xref="S2.E1.m1.4.4.2.3.2.cmml">K</mi><mi id="S2.E1.m1.4.4.2.3.3" xref="S2.E1.m1.4.4.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S2.E1.m1.4.4.3" xref="S2.E1.m1.4.4.3.cmml"><msub id="S2.E1.m1.4.4.3.2" xref="S2.E1.m1.4.4.3.2.cmml"><mi id="S2.E1.m1.4.4.3.2.2" xref="S2.E1.m1.4.4.3.2.2.cmml">d</mi><mi id="S2.E1.m1.4.4.3.2.3" xref="S2.E1.m1.4.4.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo stretchy="false" id="S2.E1.m1.4.5.3.9.2.2" xref="S2.E1.m1.4.4.cmml">)</mo></mrow><mo lspace="0em" rspace="0em" id="S2.E1.m1.4.5.3.1g" xref="S2.E1.m1.4.5.3.1.cmml">â€‹</mo><mi id="S2.E1.m1.4.5.3.10" xref="S2.E1.m1.4.5.3.10.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.4b"><apply id="S2.E1.m1.4.5.cmml" xref="S2.E1.m1.4.5"><eq id="S2.E1.m1.4.5.1.cmml" xref="S2.E1.m1.4.5.1"></eq><apply id="S2.E1.m1.4.5.2.cmml" xref="S2.E1.m1.4.5.2"><times id="S2.E1.m1.4.5.2.1.cmml" xref="S2.E1.m1.4.5.2.1"></times><ci id="S2.E1.m1.4.5.2.2.cmml" xref="S2.E1.m1.4.5.2.2">ğ´</ci><ci id="S2.E1.m1.4.5.2.3.cmml" xref="S2.E1.m1.4.5.2.3">ğ‘¡</ci><ci id="S2.E1.m1.4.5.2.4.cmml" xref="S2.E1.m1.4.5.2.4">ğ‘¡</ci><ci id="S2.E1.m1.4.5.2.5.cmml" xref="S2.E1.m1.4.5.2.5">ğ‘’</ci><ci id="S2.E1.m1.4.5.2.6.cmml" xref="S2.E1.m1.4.5.2.6">ğ‘›</ci><ci id="S2.E1.m1.4.5.2.7.cmml" xref="S2.E1.m1.4.5.2.7">ğ‘¡</ci><ci id="S2.E1.m1.4.5.2.8.cmml" xref="S2.E1.m1.4.5.2.8">ğ‘–</ci><ci id="S2.E1.m1.4.5.2.9.cmml" xref="S2.E1.m1.4.5.2.9">ğ‘œ</ci><ci id="S2.E1.m1.4.5.2.10.cmml" xref="S2.E1.m1.4.5.2.10">ğ‘›</ci><vector id="S2.E1.m1.4.5.2.11.1.cmml" xref="S2.E1.m1.4.5.2.11.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">ğ‘„</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ¾</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">ğ‘‰</ci></vector></apply><apply id="S2.E1.m1.4.5.3.cmml" xref="S2.E1.m1.4.5.3"><times id="S2.E1.m1.4.5.3.1.cmml" xref="S2.E1.m1.4.5.3.1"></times><ci id="S2.E1.m1.4.5.3.2.cmml" xref="S2.E1.m1.4.5.3.2">ğ‘ </ci><ci id="S2.E1.m1.4.5.3.3.cmml" xref="S2.E1.m1.4.5.3.3">ğ‘œ</ci><ci id="S2.E1.m1.4.5.3.4.cmml" xref="S2.E1.m1.4.5.3.4">ğ‘“</ci><ci id="S2.E1.m1.4.5.3.5.cmml" xref="S2.E1.m1.4.5.3.5">ğ‘¡</ci><ci id="S2.E1.m1.4.5.3.6.cmml" xref="S2.E1.m1.4.5.3.6">ğ‘š</ci><ci id="S2.E1.m1.4.5.3.7.cmml" xref="S2.E1.m1.4.5.3.7">ğ‘</ci><ci id="S2.E1.m1.4.5.3.8.cmml" xref="S2.E1.m1.4.5.3.8">ğ‘¥</ci><apply id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.5.3.9.2"><divide id="S2.E1.m1.4.4.1.cmml" xref="S2.E1.m1.4.5.3.9.2"></divide><apply id="S2.E1.m1.4.4.2.cmml" xref="S2.E1.m1.4.4.2"><times id="S2.E1.m1.4.4.2.1.cmml" xref="S2.E1.m1.4.4.2.1"></times><ci id="S2.E1.m1.4.4.2.2.cmml" xref="S2.E1.m1.4.4.2.2">ğ‘„</ci><apply id="S2.E1.m1.4.4.2.3.cmml" xref="S2.E1.m1.4.4.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.2.3.1.cmml" xref="S2.E1.m1.4.4.2.3">superscript</csymbol><ci id="S2.E1.m1.4.4.2.3.2.cmml" xref="S2.E1.m1.4.4.2.3.2">ğ¾</ci><ci id="S2.E1.m1.4.4.2.3.3.cmml" xref="S2.E1.m1.4.4.2.3.3">ğ‘‡</ci></apply></apply><apply id="S2.E1.m1.4.4.3.cmml" xref="S2.E1.m1.4.4.3"><root id="S2.E1.m1.4.4.3a.cmml" xref="S2.E1.m1.4.4.3"></root><apply id="S2.E1.m1.4.4.3.2.cmml" xref="S2.E1.m1.4.4.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.4.4.3.2.1.cmml" xref="S2.E1.m1.4.4.3.2">subscript</csymbol><ci id="S2.E1.m1.4.4.3.2.2.cmml" xref="S2.E1.m1.4.4.3.2.2">ğ‘‘</ci><ci id="S2.E1.m1.4.4.3.2.3.cmml" xref="S2.E1.m1.4.4.3.2.3">ğ‘˜</ci></apply></apply></apply><ci id="S2.E1.m1.4.5.3.10.cmml" xref="S2.E1.m1.4.5.3.10">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.4c">Attention(Q,K,V)=softmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.2" class="ltx_p">With the aforementioned structures, transformer and its variants are given the advantages of cross-modal interactions. By sending feature representation tokens of two categories together into transformer, the models could achieve fusion of vision and text information.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<table id="S2.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S2.T1.2.1.1" class="ltx_tr">
<th id="S2.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" rowspan="2"><span id="S2.T1.2.1.1.1.1" class="ltx_text">Method</span></th>
<th id="S2.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Modality</th>
<th id="S2.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Plane</th>
<th id="S2.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Organ</th>
<th id="S2.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Abnormality</th>
<th id="S2.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Binary</th>
<th id="S2.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2">Overall</th>
</tr>
<tr id="S2.T1.2.2.2" class="ltx_tr">
<th id="S2.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc</th>
<th id="S2.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
<th id="S2.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc</th>
<th id="S2.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
<th id="S2.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc</th>
<th id="S2.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
<th id="S2.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc</th>
<th id="S2.T1.2.2.2.8" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
<th id="S2.T1.2.2.2.9" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc</th>
<th id="S2.T1.2.2.2.10" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
<th id="S2.T1.2.2.2.11" class="ltx_td ltx_align_center ltx_th ltx_th_column">Acc</th>
<th id="S2.T1.2.2.2.12" class="ltx_td ltx_align_center ltx_th ltx_th_column">BLEU</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S2.T1.2.3.1" class="ltx_tr">
<td id="S2.T1.2.3.1.1" class="ltx_td ltx_align_center ltx_border_t">CGMVQA</td>
<td id="S2.T1.2.3.1.2" class="ltx_td ltx_align_center ltx_border_t">80.5</td>
<td id="S2.T1.2.3.1.3" class="ltx_td ltx_align_center ltx_border_t">85.6</td>
<td id="S2.T1.2.3.1.4" class="ltx_td ltx_align_center ltx_border_t">80.8</td>
<td id="S2.T1.2.3.1.5" class="ltx_td ltx_align_center ltx_border_t">81.3</td>
<td id="S2.T1.2.3.1.6" class="ltx_td ltx_align_center ltx_border_t">72.8</td>
<td id="S2.T1.2.3.1.7" class="ltx_td ltx_align_center ltx_border_t">76.9</td>
<td id="S2.T1.2.3.1.8" class="ltx_td ltx_align_center ltx_border_t">1.7</td>
<td id="S2.T1.2.3.1.9" class="ltx_td ltx_align_center ltx_border_t">1.7</td>
<td id="S2.T1.2.3.1.10" class="ltx_td ltx_align_center ltx_border_t">75.0</td>
<td id="S2.T1.2.3.1.11" class="ltx_td ltx_align_center ltx_border_t">75.0</td>
<td id="S2.T1.2.3.1.12" class="ltx_td ltx_align_center ltx_border_t">60.0</td>
<td id="S2.T1.2.3.1.13" class="ltx_td ltx_align_center ltx_border_t">61.9</td>
</tr>
<tr id="S2.T1.2.4.2" class="ltx_tr">
<td id="S2.T1.2.4.2.1" class="ltx_td ltx_align_center">MMBERT NP</td>
<td id="S2.T1.2.4.2.2" class="ltx_td ltx_align_center">80.6</td>
<td id="S2.T1.2.4.2.3" class="ltx_td ltx_align_center">85.6</td>
<td id="S2.T1.2.4.2.4" class="ltx_td ltx_align_center">81.6</td>
<td id="S2.T1.2.4.2.5" class="ltx_td ltx_align_center">82.1</td>
<td id="S2.T1.2.4.2.6" class="ltx_td ltx_align_center">71.2</td>
<td id="S2.T1.2.4.2.7" class="ltx_td ltx_align_center">74.4</td>
<td id="S2.T1.2.4.2.8" class="ltx_td ltx_align_center">4.3</td>
<td id="S2.T1.2.4.2.9" class="ltx_td ltx_align_center">5.7</td>
<td id="S2.T1.2.4.2.10" class="ltx_td ltx_align_center">78.1</td>
<td id="S2.T1.2.4.2.11" class="ltx_td ltx_align_center">78.1</td>
<td id="S2.T1.2.4.2.12" class="ltx_td ltx_align_center">60.2</td>
<td id="S2.T1.2.4.2.13" class="ltx_td ltx_align_center">62.7</td>
</tr>
<tr id="S2.T1.2.5.3" class="ltx_tr" style="background-color:#E8E8E8;">
<td id="S2.T1.2.5.3.1" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.1.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">Ours NP</span></td>
<td id="S2.T1.2.5.3.2" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.2.1" class="ltx_text" style="background-color:#E8E8E8;">73.6</span></td>
<td id="S2.T1.2.5.3.3" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.3.1" class="ltx_text" style="background-color:#E8E8E8;">80.4</span></td>
<td id="S2.T1.2.5.3.4" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.4.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">86.4</span></td>
<td id="S2.T1.2.5.3.5" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.5.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">86.4</span></td>
<td id="S2.T1.2.5.3.6" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.6.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">72.8</span></td>
<td id="S2.T1.2.5.3.7" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.7.1" class="ltx_text" style="background-color:#E8E8E8;">76.1</span></td>
<td id="S2.T1.2.5.3.8" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.8.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">6.1</span></td>
<td id="S2.T1.2.5.3.9" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.9.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">7.8</span></td>
<td id="S2.T1.2.5.3.10" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.10.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">85.9</span></td>
<td id="S2.T1.2.5.3.11" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.11.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">85.9</span></td>
<td id="S2.T1.2.5.3.12" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.12.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">62.8</span></td>
<td id="S2.T1.2.5.3.13" class="ltx_td ltx_align_center"><span id="S2.T1.2.5.3.13.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">65.0</span></td>
</tr>
<tr id="S2.T1.2.6.4" class="ltx_tr">
<td id="S2.T1.2.6.4.1" class="ltx_td ltx_align_center ltx_border_t">MMBERT P</td>
<td id="S2.T1.2.6.4.2" class="ltx_td ltx_align_center ltx_border_t">77.7</td>
<td id="S2.T1.2.6.4.3" class="ltx_td ltx_align_center ltx_border_t">81.8</td>
<td id="S2.T1.2.6.4.4" class="ltx_td ltx_align_center ltx_border_t">82.4</td>
<td id="S2.T1.2.6.4.5" class="ltx_td ltx_align_center ltx_border_t">82.9</td>
<td id="S2.T1.2.6.4.6" class="ltx_td ltx_align_center ltx_border_t">73.6</td>
<td id="S2.T1.2.6.4.7" class="ltx_td ltx_align_center ltx_border_t">76.6</td>
<td id="S2.T1.2.6.4.8" class="ltx_td ltx_align_center ltx_border_t">5.2</td>
<td id="S2.T1.2.6.4.9" class="ltx_td ltx_align_center ltx_border_t">6.7</td>
<td id="S2.T1.2.6.4.10" class="ltx_td ltx_align_center ltx_border_t">85.9</td>
<td id="S2.T1.2.6.4.11" class="ltx_td ltx_align_center ltx_border_t">85.9</td>
<td id="S2.T1.2.6.4.12" class="ltx_td ltx_align_center ltx_border_t">62.4</td>
<td id="S2.T1.2.6.4.13" class="ltx_td ltx_align_center ltx_border_t">64.2</td>
</tr>
<tr id="S2.T1.2.7.5" class="ltx_tr" style="background-color:#E8E8E8;">
<td id="S2.T1.2.7.5.1" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.1.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">Ours P</span></td>
<td id="S2.T1.2.7.5.2" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.2.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">79.2</span></td>
<td id="S2.T1.2.7.5.3" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.3.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">84.4</span></td>
<td id="S2.T1.2.7.5.4" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.4.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">83.2</span></td>
<td id="S2.T1.2.7.5.5" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.5.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">83.2</span></td>
<td id="S2.T1.2.7.5.6" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.6.1" class="ltx_text" style="background-color:#E8E8E8;">72.8</span></td>
<td id="S2.T1.2.7.5.7" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.7.1" class="ltx_text" style="background-color:#E8E8E8;">75.6</span></td>
<td id="S2.T1.2.7.5.8" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.8.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">10.5</span></td>
<td id="S2.T1.2.7.5.9" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.9.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">12.1</span></td>
<td id="S2.T1.2.7.5.10" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.10.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">89.1</span></td>
<td id="S2.T1.2.7.5.11" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.11.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">89.1</span></td>
<td id="S2.T1.2.7.5.12" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.12.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">64.2</span></td>
<td id="S2.T1.2.7.5.13" class="ltx_td ltx_align_center"><span id="S2.T1.2.7.5.13.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">66.0</span></td>
</tr>
<tr id="S2.T1.2.8.6" class="ltx_tr">
<td id="S2.T1.2.8.6.1" class="ltx_td ltx_align_center ltx_border_t">CGMVQA Ens</td>
<td id="S2.T1.2.8.6.2" class="ltx_td ltx_align_center ltx_border_t">81.9</td>
<td id="S2.T1.2.8.6.3" class="ltx_td ltx_align_center ltx_border_t">88.0</td>
<td id="S2.T1.2.8.6.4" class="ltx_td ltx_align_center ltx_border_t">86.4</td>
<td id="S2.T1.2.8.6.5" class="ltx_td ltx_align_center ltx_border_t">86.4</td>
<td id="S2.T1.2.8.6.6" class="ltx_td ltx_align_center ltx_border_t">78.4</td>
<td id="S2.T1.2.8.6.7" class="ltx_td ltx_align_center ltx_border_t">79.7</td>
<td id="S2.T1.2.8.6.8" class="ltx_td ltx_align_center ltx_border_t">4.4</td>
<td id="S2.T1.2.8.6.9" class="ltx_td ltx_align_center ltx_border_t">7.6</td>
<td id="S2.T1.2.8.6.10" class="ltx_td ltx_align_center ltx_border_t">78.1</td>
<td id="S2.T1.2.8.6.11" class="ltx_td ltx_align_center ltx_border_t">78.1</td>
<td id="S2.T1.2.8.6.12" class="ltx_td ltx_align_center ltx_border_t">64.0</td>
<td id="S2.T1.2.8.6.13" class="ltx_td ltx_align_center ltx_border_t">65.9</td>
</tr>
<tr id="S2.T1.2.9.7" class="ltx_tr">
<td id="S2.T1.2.9.7.1" class="ltx_td ltx_align_center">MMBERT Ex</td>
<td id="S2.T1.2.9.7.2" class="ltx_td ltx_align_center">83.3</td>
<td id="S2.T1.2.9.7.3" class="ltx_td ltx_align_center">86.2</td>
<td id="S2.T1.2.9.7.4" class="ltx_td ltx_align_center">86.4</td>
<td id="S2.T1.2.9.7.5" class="ltx_td ltx_align_center">86.4</td>
<td id="S2.T1.2.9.7.6" class="ltx_td ltx_align_center">76.8</td>
<td id="S2.T1.2.9.7.7" class="ltx_td ltx_align_center">80.7</td>
<td id="S2.T1.2.9.7.8" class="ltx_td ltx_align_center">14.0</td>
<td id="S2.T1.2.9.7.9" class="ltx_td ltx_align_center">16.0</td>
<td id="S2.T1.2.9.7.10" class="ltx_td ltx_align_center">87.5</td>
<td id="S2.T1.2.9.7.11" class="ltx_td ltx_align_center">87.5</td>
<td id="S2.T1.2.9.7.12" class="ltx_td ltx_align_center">67.2</td>
<td id="S2.T1.2.9.7.13" class="ltx_td ltx_align_center">69.0</td>
</tr>
<tr id="S2.T1.2.10.8" class="ltx_tr" style="background-color:#E8E8E8;">
<td id="S2.T1.2.10.8.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.1.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">Ours Ex</span></td>
<td id="S2.T1.2.10.8.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.2.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">84.7</span></td>
<td id="S2.T1.2.10.8.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.3.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">88.6</span></td>
<td id="S2.T1.2.10.8.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.4.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">86.4</span></td>
<td id="S2.T1.2.10.8.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.5.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">86.4</span></td>
<td id="S2.T1.2.10.8.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.6.1" class="ltx_text" style="background-color:#E8E8E8;">76.8</span></td>
<td id="S2.T1.2.10.8.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.7.1" class="ltx_text" style="background-color:#E8E8E8;">80.2</span></td>
<td id="S2.T1.2.10.8.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.8.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">16.7</span></td>
<td id="S2.T1.2.10.8.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.9.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">17.2</span></td>
<td id="S2.T1.2.10.8.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.10.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">89.1</span></td>
<td id="S2.T1.2.10.8.11" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.11.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">89.1</span></td>
<td id="S2.T1.2.10.8.12" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.12.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">68.2</span></td>
<td id="S2.T1.2.10.8.13" class="ltx_td ltx_align_center ltx_border_bb"><span id="S2.T1.2.10.8.13.1" class="ltx_text ltx_font_bold" style="background-color:#E8E8E8;">69.7</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T1.3.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Results of our method and other comparison methods on VQA-Med 2019 dataset. NP, P, Ens and Ex represent non-pretrained model, pretrained model, ensemble models and exclusive models for different categories respectively.</figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Overall Architecture</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The overall architecture of our proposed MF<sup id="S2.SS2.p1.1.1" class="ltx_sup"><span id="S2.SS2.p1.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA is illustrated in Fig.<a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Image-question pairs are taken as input to multi-scale feature fusion transformer layer and pass through a classification layer to produce corresponding answers. The whole network can be pre-trained by MLM task and acquire better performance in Med-VQA.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Visual Feature Embedding.</span> A CNN-based image encoder, such as ResNet152 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> or EfficientNetv2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> , is used to extract visual feature. We still choose the previous method to extract different convolutional layer outputs from image encoder because this method can obtain multi-scale information of medical image efficiently, then the feature map of each stage is resized into a token respectively through convolution layer and global average pooling layer.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.13" class="ltx_p"><span id="S2.SS2.p3.13.1" class="ltx_text ltx_font_bold">Question Embedding.</span> We refer to the methods of Uniter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and VilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> which tokenize each word in the question and use the BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> embedding layer to embed each token as a vector. To be more specific, the embedded sequence is represented by <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\mathbf{w}" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">ğ°</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">ğ°</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\mathbf{w}</annotation></semantics></math> = <math id="S2.SS2.p3.2.m2.1" class="ltx_math_unparsed" alttext="\{w_{1}" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mrow id="S2.SS2.p3.2.m2.1b"><mo stretchy="false" id="S2.SS2.p3.2.m2.1.1">{</mo><msub id="S2.SS2.p3.2.m2.1.2"><mi id="S2.SS2.p3.2.m2.1.2.2">w</mi><mn id="S2.SS2.p3.2.m2.1.2.3">1</mn></msub></mrow><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">\{w_{1}</annotation></semantics></math>, <math id="S2.SS2.p3.3.m3.1" class="ltx_Math" alttext="w_{2}" display="inline"><semantics id="S2.SS2.p3.3.m3.1a"><msub id="S2.SS2.p3.3.m3.1.1" xref="S2.SS2.p3.3.m3.1.1.cmml"><mi id="S2.SS2.p3.3.m3.1.1.2" xref="S2.SS2.p3.3.m3.1.1.2.cmml">w</mi><mn id="S2.SS2.p3.3.m3.1.1.3" xref="S2.SS2.p3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.1b"><apply id="S2.SS2.p3.3.m3.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m3.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p3.3.m3.1.1.2.cmml" xref="S2.SS2.p3.3.m3.1.1.2">ğ‘¤</ci><cn type="integer" id="S2.SS2.p3.3.m3.1.1.3.cmml" xref="S2.SS2.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.1c">w_{2}</annotation></semantics></math>, <math id="S2.SS2.p3.4.m4.1" class="ltx_Math" alttext="\cdots" display="inline"><semantics id="S2.SS2.p3.4.m4.1a"><mi mathvariant="normal" id="S2.SS2.p3.4.m4.1.1" xref="S2.SS2.p3.4.m4.1.1.cmml">â‹¯</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.4.m4.1b"><ci id="S2.SS2.p3.4.m4.1.1.cmml" xref="S2.SS2.p3.4.m4.1.1">â‹¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.4.m4.1c">\cdots</annotation></semantics></math>, <math id="S2.SS2.p3.5.m5.1" class="ltx_math_unparsed" alttext="w_{n}\}" display="inline"><semantics id="S2.SS2.p3.5.m5.1a"><mrow id="S2.SS2.p3.5.m5.1b"><msub id="S2.SS2.p3.5.m5.1.1"><mi id="S2.SS2.p3.5.m5.1.1.2">w</mi><mi id="S2.SS2.p3.5.m5.1.1.3">n</mi></msub><mo stretchy="false" id="S2.SS2.p3.5.m5.1.2">}</mo></mrow><annotation encoding="application/x-tex" id="S2.SS2.p3.5.m5.1c">w_{n}\}</annotation></semantics></math> <math id="S2.SS2.p3.6.m6.1" class="ltx_Math" alttext="\in" display="inline"><semantics id="S2.SS2.p3.6.m6.1a"><mo id="S2.SS2.p3.6.m6.1.1" xref="S2.SS2.p3.6.m6.1.1.cmml">âˆˆ</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.6.m6.1b"><in id="S2.SS2.p3.6.m6.1.1.cmml" xref="S2.SS2.p3.6.m6.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.6.m6.1c">\in</annotation></semantics></math> <math id="S2.SS2.p3.7.m7.1" class="ltx_Math" alttext="\mathbb{R}^{d}" display="inline"><semantics id="S2.SS2.p3.7.m7.1a"><msup id="S2.SS2.p3.7.m7.1.1" xref="S2.SS2.p3.7.m7.1.1.cmml"><mi id="S2.SS2.p3.7.m7.1.1.2" xref="S2.SS2.p3.7.m7.1.1.2.cmml">â„</mi><mi id="S2.SS2.p3.7.m7.1.1.3" xref="S2.SS2.p3.7.m7.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.7.m7.1b"><apply id="S2.SS2.p3.7.m7.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p3.7.m7.1.1.1.cmml" xref="S2.SS2.p3.7.m7.1.1">superscript</csymbol><ci id="S2.SS2.p3.7.m7.1.1.2.cmml" xref="S2.SS2.p3.7.m7.1.1.2">â„</ci><ci id="S2.SS2.p3.7.m7.1.1.3.cmml" xref="S2.SS2.p3.7.m7.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.7.m7.1c">\mathbb{R}^{d}</annotation></semantics></math>, where <em id="S2.SS2.p3.13.2" class="ltx_emph ltx_font_italic">n</em> indicates the sequence length, and <em id="S2.SS2.p3.13.3" class="ltx_emph ltx_font_italic">d</em> is the embedding dimension. After that the positional embedding is added to encode the position information. The final language representation of the question is <math id="S2.SS2.p3.8.m8.1" class="ltx_Math" alttext="\{" display="inline"><semantics id="S2.SS2.p3.8.m8.1a"><mo stretchy="false" id="S2.SS2.p3.8.m8.1.1" xref="S2.SS2.p3.8.m8.1.1.cmml">{</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.8.m8.1b"><ci id="S2.SS2.p3.8.m8.1.1.cmml" xref="S2.SS2.p3.8.m8.1.1">{</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.8.m8.1c">\{</annotation></semantics></math><math id="S2.SS2.p3.9.m9.1" class="ltx_Math" alttext="\hat{w_{1}}" display="inline"><semantics id="S2.SS2.p3.9.m9.1a"><mover accent="true" id="S2.SS2.p3.9.m9.1.1" xref="S2.SS2.p3.9.m9.1.1.cmml"><msub id="S2.SS2.p3.9.m9.1.1.2" xref="S2.SS2.p3.9.m9.1.1.2.cmml"><mi id="S2.SS2.p3.9.m9.1.1.2.2" xref="S2.SS2.p3.9.m9.1.1.2.2.cmml">w</mi><mn id="S2.SS2.p3.9.m9.1.1.2.3" xref="S2.SS2.p3.9.m9.1.1.2.3.cmml">1</mn></msub><mo id="S2.SS2.p3.9.m9.1.1.1" xref="S2.SS2.p3.9.m9.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.9.m9.1b"><apply id="S2.SS2.p3.9.m9.1.1.cmml" xref="S2.SS2.p3.9.m9.1.1"><ci id="S2.SS2.p3.9.m9.1.1.1.cmml" xref="S2.SS2.p3.9.m9.1.1.1">^</ci><apply id="S2.SS2.p3.9.m9.1.1.2.cmml" xref="S2.SS2.p3.9.m9.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p3.9.m9.1.1.2.1.cmml" xref="S2.SS2.p3.9.m9.1.1.2">subscript</csymbol><ci id="S2.SS2.p3.9.m9.1.1.2.2.cmml" xref="S2.SS2.p3.9.m9.1.1.2.2">ğ‘¤</ci><cn type="integer" id="S2.SS2.p3.9.m9.1.1.2.3.cmml" xref="S2.SS2.p3.9.m9.1.1.2.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.9.m9.1c">\hat{w_{1}}</annotation></semantics></math>, <math id="S2.SS2.p3.10.m10.1" class="ltx_Math" alttext="\hat{w_{2}}" display="inline"><semantics id="S2.SS2.p3.10.m10.1a"><mover accent="true" id="S2.SS2.p3.10.m10.1.1" xref="S2.SS2.p3.10.m10.1.1.cmml"><msub id="S2.SS2.p3.10.m10.1.1.2" xref="S2.SS2.p3.10.m10.1.1.2.cmml"><mi id="S2.SS2.p3.10.m10.1.1.2.2" xref="S2.SS2.p3.10.m10.1.1.2.2.cmml">w</mi><mn id="S2.SS2.p3.10.m10.1.1.2.3" xref="S2.SS2.p3.10.m10.1.1.2.3.cmml">2</mn></msub><mo id="S2.SS2.p3.10.m10.1.1.1" xref="S2.SS2.p3.10.m10.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.10.m10.1b"><apply id="S2.SS2.p3.10.m10.1.1.cmml" xref="S2.SS2.p3.10.m10.1.1"><ci id="S2.SS2.p3.10.m10.1.1.1.cmml" xref="S2.SS2.p3.10.m10.1.1.1">^</ci><apply id="S2.SS2.p3.10.m10.1.1.2.cmml" xref="S2.SS2.p3.10.m10.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p3.10.m10.1.1.2.1.cmml" xref="S2.SS2.p3.10.m10.1.1.2">subscript</csymbol><ci id="S2.SS2.p3.10.m10.1.1.2.2.cmml" xref="S2.SS2.p3.10.m10.1.1.2.2">ğ‘¤</ci><cn type="integer" id="S2.SS2.p3.10.m10.1.1.2.3.cmml" xref="S2.SS2.p3.10.m10.1.1.2.3">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.10.m10.1c">\hat{w_{2}}</annotation></semantics></math>, <math id="S2.SS2.p3.11.m11.1" class="ltx_Math" alttext="\cdots" display="inline"><semantics id="S2.SS2.p3.11.m11.1a"><mi mathvariant="normal" id="S2.SS2.p3.11.m11.1.1" xref="S2.SS2.p3.11.m11.1.1.cmml">â‹¯</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.11.m11.1b"><ci id="S2.SS2.p3.11.m11.1.1.cmml" xref="S2.SS2.p3.11.m11.1.1">â‹¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.11.m11.1c">\cdots</annotation></semantics></math>, <math id="S2.SS2.p3.12.m12.1" class="ltx_Math" alttext="\hat{w_{n}}" display="inline"><semantics id="S2.SS2.p3.12.m12.1a"><mover accent="true" id="S2.SS2.p3.12.m12.1.1" xref="S2.SS2.p3.12.m12.1.1.cmml"><msub id="S2.SS2.p3.12.m12.1.1.2" xref="S2.SS2.p3.12.m12.1.1.2.cmml"><mi id="S2.SS2.p3.12.m12.1.1.2.2" xref="S2.SS2.p3.12.m12.1.1.2.2.cmml">w</mi><mi id="S2.SS2.p3.12.m12.1.1.2.3" xref="S2.SS2.p3.12.m12.1.1.2.3.cmml">n</mi></msub><mo id="S2.SS2.p3.12.m12.1.1.1" xref="S2.SS2.p3.12.m12.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.12.m12.1b"><apply id="S2.SS2.p3.12.m12.1.1.cmml" xref="S2.SS2.p3.12.m12.1.1"><ci id="S2.SS2.p3.12.m12.1.1.1.cmml" xref="S2.SS2.p3.12.m12.1.1.1">^</ci><apply id="S2.SS2.p3.12.m12.1.1.2.cmml" xref="S2.SS2.p3.12.m12.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p3.12.m12.1.1.2.1.cmml" xref="S2.SS2.p3.12.m12.1.1.2">subscript</csymbol><ci id="S2.SS2.p3.12.m12.1.1.2.2.cmml" xref="S2.SS2.p3.12.m12.1.1.2.2">ğ‘¤</ci><ci id="S2.SS2.p3.12.m12.1.1.2.3.cmml" xref="S2.SS2.p3.12.m12.1.1.2.3">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.12.m12.1c">\hat{w_{n}}</annotation></semantics></math><math id="S2.SS2.p3.13.m13.1" class="ltx_Math" alttext="\}" display="inline"><semantics id="S2.SS2.p3.13.m13.1a"><mo stretchy="false" id="S2.SS2.p3.13.m13.1.1" xref="S2.SS2.p3.13.m13.1.1.cmml">}</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.13.m13.1b"><ci id="S2.SS2.p3.13.m13.1.1.cmml" xref="S2.SS2.p3.13.m13.1.1">}</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.13.m13.1c">\}</annotation></semantics></math>. For each of the representation at position <em id="S2.SS2.p3.13.4" class="ltx_emph ltx_font_italic">i</em>, it is calculated by</p>
<table id="S2.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E2.m1.1" class="ltx_Math" alttext="\hat{W_{i}}=LayerNorm(w_{i}+p_{i})" display="block"><semantics id="S2.E2.m1.1a"><mrow id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml"><mover accent="true" id="S2.E2.m1.1.1.3" xref="S2.E2.m1.1.1.3.cmml"><msub id="S2.E2.m1.1.1.3.2" xref="S2.E2.m1.1.1.3.2.cmml"><mi id="S2.E2.m1.1.1.3.2.2" xref="S2.E2.m1.1.1.3.2.2.cmml">W</mi><mi id="S2.E2.m1.1.1.3.2.3" xref="S2.E2.m1.1.1.3.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.3.1" xref="S2.E2.m1.1.1.3.1.cmml">^</mo></mover><mo id="S2.E2.m1.1.1.2" xref="S2.E2.m1.1.1.2.cmml">=</mo><mrow id="S2.E2.m1.1.1.1" xref="S2.E2.m1.1.1.1.cmml"><mi id="S2.E2.m1.1.1.1.3" xref="S2.E2.m1.1.1.1.3.cmml">L</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.1.1.1.4" xref="S2.E2.m1.1.1.1.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2a" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.1.1.1.5" xref="S2.E2.m1.1.1.1.5.cmml">y</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2b" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.1.1.1.6" xref="S2.E2.m1.1.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2c" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.1.1.1.7" xref="S2.E2.m1.1.1.1.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2d" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.1.1.1.8" xref="S2.E2.m1.1.1.1.8.cmml">N</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2e" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.1.1.1.9" xref="S2.E2.m1.1.1.1.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2f" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.1.1.1.10" xref="S2.E2.m1.1.1.1.10.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2g" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mi id="S2.E2.m1.1.1.1.11" xref="S2.E2.m1.1.1.1.11.cmml">m</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.1.1.1.2h" xref="S2.E2.m1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.E2.m1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.cmml"><msub id="S2.E2.m1.1.1.1.1.1.1.2" xref="S2.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.2.2" xref="S2.E2.m1.1.1.1.1.1.1.2.2.cmml">w</mi><mi id="S2.E2.m1.1.1.1.1.1.1.2.3" xref="S2.E2.m1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S2.E2.m1.1.1.1.1.1.1.1" xref="S2.E2.m1.1.1.1.1.1.1.1.cmml">+</mo><msub id="S2.E2.m1.1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E2.m1.1.1.1.1.1.1.3.2" xref="S2.E2.m1.1.1.1.1.1.1.3.2.cmml">p</mi><mi id="S2.E2.m1.1.1.1.1.1.1.3.3" xref="S2.E2.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo stretchy="false" id="S2.E2.m1.1.1.1.1.1.3" xref="S2.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.1b"><apply id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1"><eq id="S2.E2.m1.1.1.2.cmml" xref="S2.E2.m1.1.1.2"></eq><apply id="S2.E2.m1.1.1.3.cmml" xref="S2.E2.m1.1.1.3"><ci id="S2.E2.m1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.3.1">^</ci><apply id="S2.E2.m1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.3.2.1.cmml" xref="S2.E2.m1.1.1.3.2">subscript</csymbol><ci id="S2.E2.m1.1.1.3.2.2.cmml" xref="S2.E2.m1.1.1.3.2.2">ğ‘Š</ci><ci id="S2.E2.m1.1.1.3.2.3.cmml" xref="S2.E2.m1.1.1.3.2.3">ğ‘–</ci></apply></apply><apply id="S2.E2.m1.1.1.1.cmml" xref="S2.E2.m1.1.1.1"><times id="S2.E2.m1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.2"></times><ci id="S2.E2.m1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.3">ğ¿</ci><ci id="S2.E2.m1.1.1.1.4.cmml" xref="S2.E2.m1.1.1.1.4">ğ‘</ci><ci id="S2.E2.m1.1.1.1.5.cmml" xref="S2.E2.m1.1.1.1.5">ğ‘¦</ci><ci id="S2.E2.m1.1.1.1.6.cmml" xref="S2.E2.m1.1.1.1.6">ğ‘’</ci><ci id="S2.E2.m1.1.1.1.7.cmml" xref="S2.E2.m1.1.1.1.7">ğ‘Ÿ</ci><ci id="S2.E2.m1.1.1.1.8.cmml" xref="S2.E2.m1.1.1.1.8">ğ‘</ci><ci id="S2.E2.m1.1.1.1.9.cmml" xref="S2.E2.m1.1.1.1.9">ğ‘œ</ci><ci id="S2.E2.m1.1.1.1.10.cmml" xref="S2.E2.m1.1.1.1.10">ğ‘Ÿ</ci><ci id="S2.E2.m1.1.1.1.11.cmml" xref="S2.E2.m1.1.1.1.11">ğ‘š</ci><apply id="S2.E2.m1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1"><plus id="S2.E2.m1.1.1.1.1.1.1.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.1"></plus><apply id="S2.E2.m1.1.1.1.1.1.1.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.2">ğ‘¤</ci><ci id="S2.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.2.3">ğ‘–</ci></apply><apply id="S2.E2.m1.1.1.1.1.1.1.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.2">ğ‘</ci><ci id="S2.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S2.E2.m1.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.1c">\hat{W_{i}}=LayerNorm(w_{i}+p_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S2.SS2.p3.14" class="ltx_p">where <em id="S2.SS2.p3.14.1" class="ltx_emph ltx_font_italic">p<sub id="S2.SS2.p3.14.1.1" class="ltx_sub">i</sub></em> indicates the embedding vector at position <em id="S2.SS2.p3.14.2" class="ltx_emph ltx_font_italic">i</em> and
<br class="ltx_break"><em id="S2.SS2.p3.14.3" class="ltx_emph ltx_font_italic">LayerNorm</em> is a normalization function.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">After the above process, the low-dimensional vector representation of the two modalities data are obtained respectively. Then the two parts feature tokens are integrated with the corresponding segment embedding and mask matrix to form a low-dimensional token sequence. After that two special tokens [CLS] and [SEP] are added for learning joint classiï¬cation feature and specifying token length.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">Cross-modal feature fusion.</span> Multi-layer transformers, whose layers have the same numbers with visual tokens, are adopted to conduct cross-modality feature fusion between medical image and question representation. We use a simple fusion method to fully learn cross-modal information, the specific details will be described in detail in Sec.<a href="#S2.SS3" title="2.3 Multi-stage Feature Fusion â€£ 2 Methods â€£ MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>. After sufficient feature fusion, the output pass through a classifier. The class is predicted by the output h[cls] and finally get the predicted answer from candidate items.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Multi-stage Feature Fusion</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Recent methods directly sent whole tokens of image and text to the multi-layer transformer for feature fusion. However, such methods do not consider the difference in feature dimensions as well as the diverse and specific information in visual features from different stages. Whatâ€™s more, integrating modalities together cannot effectively focus on specific image features at different stages.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">Instead of feeding all visual tokens with text tokens to the five-layer transformer together, every visual token from each encoder stage is added gradually for inputting different visual features as shown in Fig.<a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. To be more specific, we first set 5 tokens for all 5 visual features. In the first transformer layer, only one visual token which is from the lowest encoder stage and 4 mask tokens are sent to the first transformer layer together. In the second layer, we add a new visual token from the next stage and the number of mask tokens is 3. The remaining layers add visual tokens step by step in this way.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">Therefore, the lowest visual feature is through all transformer layers and the highest feature maps only go through 1 layer. With this simple method, we can better integrate the multi-scale information of the image and the high-level semantic information of the question.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiment and discussion</h2>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2211.05991/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="484" height="238" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text ltx_font_bold">Fig.Â 3</span>: </span>Visualized examples of comparing MMBERT and our method on VQA-Med 2019 dataset. The first line is original images, the second line is MMBERTâ€™s heatmaps, and the third line is our visualization. Q means input question, A1 is MMBERTâ€™s prediction of answer, and A2 is our model output. Both methods are compared without loading pre-training.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Dataset</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Radiology Objects in COntext (ROCO) dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> is chosen to pre-train our model. The ROCO dataset consists of more than 81,000 medical image-caption pairs. We fine-tune MF<sup id="S3.SS1.p1.1.1" class="ltx_sup"><span id="S3.SS1.p1.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA on two classical datasets. The first one is VQA-Med 2019. It contains 4200 medical images and 15,292 Question-Answer pairs, we split the dataset in the same way as previous work.Another dataset is VQA-RAD which contains 315 images and 3,515 corresponding questions. All questions are divided into 11 categories.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implementation details</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.3" class="ltx_p">The input medical images are resized to <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1.2" xref="S3.SS2.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S3.SS2.p1.1.m1.1.1.3" xref="S3.SS2.p1.1.m1.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><times id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1"></times><cn type="integer" id="S3.SS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S3.SS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.p1.1.m1.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">224\times 224</annotation></semantics></math>, and then we use EfficientNetv2 to obtain 5-stage visual feature maps. This is because even if we utilize the Efficientnetv2 block that obtains the feature map with parameter sharing, we can still reach on-par performance compared with resnet152 without parameter sharing. The feature fusion model consists of 5 transformer layers corresponding to the number of visual tokens, the hidden size is set to 768. The pretraining learning rate is 2e<sup id="S3.SS2.p1.3.1" class="ltx_sup"><span id="S3.SS2.p1.3.1.1" class="ltx_text ltx_font_italic">-5</span></sup>. For fine-tuning, we use the Adam optimizer with a learning rate of 5e<sup id="S3.SS2.p1.3.2" class="ltx_sup"><span id="S3.SS2.p1.3.2.1" class="ltx_text ltx_font_italic">-5</span></sup>. The rest of the parameter settings are consistent with MMBERT. All experiments were conducted on a GPU of NVIDIA 3090.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">We use masked language modeling task (MLM) to pre-train our model by predicting original token of corresponding text or visual feature, which is replaced by [mask] token. For the VQA-Med 2019 dataset, we compare our methods with others under three settings respectively. First, there is no pre-training or dedicated setting, and the answer prediction is directly performed on all categories. Second, we load the pre-trained weights to predict all classes together as well. Finally, we load pre-training but train models separately for 5 different classes.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.2.1.1" class="ltx_tr">
<th id="S3.T2.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Method</th>
<th id="S3.T2.2.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Open</th>
<th id="S3.T2.2.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Closed</th>
<th id="S3.T2.2.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">Overall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.2.2.1" class="ltx_tr">
<td id="S3.T2.2.2.1.1" class="ltx_td ltx_align_center ltx_border_t">CGMVQA</td>
<td id="S3.T2.2.2.1.2" class="ltx_td ltx_align_center ltx_border_t">54.2</td>
<td id="S3.T2.2.2.1.3" class="ltx_td ltx_align_center ltx_border_t">78.3</td>
<td id="S3.T2.2.2.1.4" class="ltx_td ltx_align_center ltx_border_t">68.7</td>
</tr>
<tr id="S3.T2.2.3.2" class="ltx_tr">
<td id="S3.T2.2.3.2.1" class="ltx_td ltx_align_center">MMBERT P</td>
<td id="S3.T2.2.3.2.2" class="ltx_td ltx_align_center">63.1</td>
<td id="S3.T2.2.3.2.3" class="ltx_td ltx_align_center">77.9</td>
<td id="S3.T2.2.3.2.4" class="ltx_td ltx_align_center">72.0</td>
</tr>
<tr id="S3.T2.2.4.3" class="ltx_tr">
<td id="S3.T2.2.4.3.1" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.4.3.1.1" class="ltx_text ltx_font_bold">Ours P</span></td>
<td id="S3.T2.2.4.3.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.4.3.2.1" class="ltx_text ltx_font_bold">63.7</span></td>
<td id="S3.T2.2.4.3.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.4.3.3.1" class="ltx_text ltx_font_bold">80.1</span></td>
<td id="S3.T2.2.4.3.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S3.T2.2.4.3.4.1" class="ltx_text ltx_font_bold">73.6</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T2.3.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Accuracy of our method and other comparison methods on VQA-RAD dataset.</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Results and Analysis</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We conduct experiments on the VQA-Med 2019 dataset and compare our results with previous approaches. The results are presented in Table.<a href="#S2.T1" title="Table 1 â€£ 2.1 Transformer â€£ 2 Methods â€£ MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. Without data augmentation and pre-training, we considerably outperform previous methods. Compared with MMBERT, MF<sup id="S3.SS3.p1.1.1" class="ltx_sup"><span id="S3.SS3.p1.1.1.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA increases the accuracy by 2.6% and the BLEU by 2.3%. This experiment fully demonstrates the effectiveness of our proposed multi-stage feature fusion method. Especially on the three categories of Plane, Abnormality, Yes/No, our results are even better than pre-trained MMBERT model performance.
With pretrained weights loaded, we achieve the accuracy of 64.2% which makes a 1.8% improvement over MMBERT and the BLEU scores of 66.0. Better results in all categories can be seen except for the organ category. Yes/No even surpassed the highest scores of other methods. In addition, we achieve state-of-the-art performance with accuracy of 68.2% and BLEU of 69.7% by training 5 models separately for different categories, which fully prove the effect of our Multi-stage feature fusion method.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Qualitative Results</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The heatmaps of both MMBERT and ours are visualized to gain qualitative insight of the feature fusion performance between question tokens and visual features as shown in Fig.<a href="#S3.F3" title="Figure 3 â€£ 3 Experiment and discussion â€£ MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. It obviously can be seen that our model (the third line) focuses on the more important regions of the medical image, which is better than MMBERT(the second line). Even if both predicting the correct answer, our method can be relatively accurate focusing on the significant part of the medical image. This suggests that our fusion method effectively fuse semantics of multi-stage feature maps with question tokens.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.3" class="ltx_p">Furthermore, we also evaluate our model on the VQA-RAD dataset without setting dedicated models for different question categories, Table.<a href="#S3.T2" title="Table 2 â€£ 3.2 Implementation details â€£ 3 Experiment and discussion â€£ MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the performance comparison between our method and others. Our accuracy is up to 73.6%, which makes a 1.6% improvement. It is obvious that our method achieves a considerable improvement in terms of closed-ended and open-ended questions (<math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mo stretchy="false" id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\uparrow</annotation></semantics></math> 2.2% and <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="\uparrow" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mo stretchy="false" id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">â†‘</mo><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">â†‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\uparrow</annotation></semantics></math> 0.6% respectively). These results illustrate that MF<sup id="S3.SS4.p2.3.1" class="ltx_sup"><span id="S3.SS4.p2.3.1.1" class="ltx_text ltx_font_italic">2</span></sup>-MVQA can get the best performance on different datasets and reveals our benefit of cross-modal inference for Med-VQA task.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">For cross-modal fusion in Med-VQA, we propose a multi-stage feature fusion model that can use transformers to better fuse medical image features and question semantics at different scales. Experiments show that our method achieves state-of-the-art on various medical question answering datasets.
</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Acknowledgments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work was supported in part by the Natural Science Foundation of China under Grant 42201386, in part by the International Exchange Growth Program for Young Teachers of USTB under Grant QNXM20220033, and Scientific and Technological Innovation Foundation of Shunde Innovation School, USTB (BK20BE014).</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Li-Ming Zhan, BoÂ Liu, LuÂ Fan, Jiaxin Chen, and Xiao-Ming Wu,

</span>
<span class="ltx_bibblock">â€œMedical visual question answering via conditional reasoning,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th ACM International Conference on
Multimedia</span>, 2020, pp. 2345â€“2354.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Haifan Gong, Guanqi Chen, Sishuo Liu, Yizhou Yu, and Guanbin Li,

</span>
<span class="ltx_bibblock">â€œCross-modal self-attention with multi-task pre-training for medical
visual question answering,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2021 International Conference on
Multimedia Retrieval</span>, 2021, pp. 456â€“460.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Fuji Ren and Yangyang Zhou,

</span>
<span class="ltx_bibblock">â€œCgmvqa: A new classification and generative model for medical
visual question answering,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, vol. 8, pp. 50626â€“50636, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, UÂ Deva Priyakumar, and
CVÂ Jawahar,

</span>
<span class="ltx_bibblock">â€œMmbert: multimodal bert pretraining for improved medical vqa,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2021 IEEE 18th International Symposium on Biomedical Imaging
(ISBI)</span>. IEEE, 2021, pp. 1033â€“1036.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
AsmaÂ Ben Abacha, SadidÂ A Hasan, VivekÂ V Datla, Joey Liu, Dina Demner-Fushman,
and Henning MÃ¼ller,

</span>
<span class="ltx_bibblock">â€œVqa-med: Overview of the medical visual question answering task at
imageclef 2019.,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">CLEF (Working Notes)</span>, vol. 2, no. 6, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
JasonÂ J Lau, Soumya Gayen, Asma BenÂ Abacha, and Dina Demner-Fushman,

</span>
<span class="ltx_bibblock">â€œA dataset of clinically generated visual questions and answers
about radiology images,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Scientific data</span>, vol. 5, no. 1, pp. 1â€“10, 2018.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova,

</span>
<span class="ltx_bibblock">â€œBert: Pre-training of deep bidirectional transformers for language
understanding,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1810.04805</span>, 2018.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, <span id="bib.bib8.1.1" class="ltx_text ltx_font_caligraphic">L</span>ukasz Kaiser, and Illia Polosukhin,

</span>
<span class="ltx_bibblock">â€œAttention is all you need,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 30,
2017.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,

</span>
<span class="ltx_bibblock">â€œDeep residual learning for image recognition,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</span>, 2016, pp. 770â€“778.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Mingxing Tan and Quoc Le,

</span>
<span class="ltx_bibblock">â€œEfficientnetv2: Smaller models and faster training,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>. PMLR, 2021,
pp. 10096â€“10106.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed ElÂ Kholy, Faisal Ahmed, Zhe Gan,
YuÂ Cheng, and Jingjing Liu,

</span>
<span class="ltx_bibblock">â€œUniter: Learning universal image-text representations,â€

</span>
<span class="ltx_bibblock">2019.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee,

</span>
<span class="ltx_bibblock">â€œVilbert: Pretraining task-agnostic visiolinguistic representations
for vision-and-language tasks,â€

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, vol. 32,
2019.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Obioma Pelka, Sven Koitka, Johannes RÃ¼ckert, Felix Nensa, and ChristophÂ M
Friedrich,

</span>
<span class="ltx_bibblock">â€œRadiology objects in context (roco): a multimodal image dataset,â€

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Intravascular Imaging and Computer Assisted Stenting and
Large-Scale Annotation of Biomedical Data and Expert Label Synthesis</span>, pp.
180â€“189. Springer, 2018.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2211.05989" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2211.05991" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2211.05991">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2211.05991" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2211.05992" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 08:05:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
