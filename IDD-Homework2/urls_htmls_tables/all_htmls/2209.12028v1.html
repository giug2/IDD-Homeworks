<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2209.12028] Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline</title><meta property="og:description" content="Recently, 3D vision-and-language tasks have attracted increasing research interest. Compared to other vision-and-language tasks, the 3D visual question answering (VQA) task is less exploited and is more susceptible to …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2209.12028">

<!--Generated on Thu Mar 14 00:49:24 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
grounded visual question answering,  vision and language on 3D scenes
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lichen Zhao,
Daigang Cai,
Jing Zhang,
Lu Sheng,
Dong Xu, , 
<br class="ltx_break">Rui Zheng, Yinjie Zhao,
Lipeng Wang and Xibo Fan
</span><span class="ltx_author_notes">Lichen Zhao, Daigang Cai, Jing Zhang, Lu Sheng, Rui Zheng, Yinjie Zhao,
Lipeng Wang and Xibo Fan are with the College of Software, Beihang University, China. E-mail: {zlc1114, caidaigang, zhang_jing, lsheng, rzheng, yjzhao, wanglipeng, fanxibo}@buaa.edu.cn.Dong Xu is with the School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, 2008 Australia. E-mail: {dong.xu}@sydney.edu.au.Corresponding author: Jing Zhang.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Recently, 3D vision-and-language tasks have attracted increasing research interest. Compared to other vision-and-language tasks, the 3D visual question answering (VQA) task is less exploited and is more susceptible to language priors and co-reference ambiguity.
Meanwhile, a couple of recently proposed 3D VQA datasets do not well support 3D VQA task due to their limited scale and annotation methods.
In this work, we formally define and address a 3D grounded VQA task by collecting a new 3D VQA dataset, referred to as FE-3DGQA, with diverse and <span id="id1.id1.1" class="ltx_text">relatively<span id="id1.id1.1.1" class="ltx_text"> free-form question-answer pairs, as well as dense and completely grounded bounding box annotations.
<span id="id1.id1.1.1.1" class="ltx_text">To achieve more explainable answers, we labeled the objects appeared in the complex QA pairs with different semantic types, including answer-grounded objects (both appeared and not appeared in the questions), and contextual objects for answer-grounded objects.<span id="id1.id1.1.1.1.1" class="ltx_text">
We also propose a new 3D VQA framework to effectively predict the completely visually grounded and explainable answer. Extensive experiments verify that our newly collected benchmark datasets can be effectively used to evaluate various 3D VQA methods from different aspects and our newly proposed framework also achieves the state-of-the-art performance on the new benchmark dataset.</span></span></span></span></p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
grounded visual question answering, vision and language on 3D scenes

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In
recent years, there has been increasing research interest in various vision-and-language tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Among these tasks, visual question answering (VQA) systems aim to answer free-form questions based on the visual content in images or other types of data like 3D point cloud.
<span id="S1.p1.1.1" class="ltx_text">Compared to other vision-language tasks, the VQA task is more susceptible to language priors and co-reference ambiguity.
Without the visual data, it is not feasible to obtain the final results for other vision-language tasks.
For example, the captioning task merely takes visual data as the input and to generate textual descriptions. The grounding task aim to predict bounding boxes on the visual data.
And thus these tasks heavily rely on the understanding of the visual data. However, as shown in many previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for the VQA task, even without the visual data, we can still achieve reasonable results by merely relying on textual questions, since some questions can be answered by common sense (<em id="S1.p1.1.1.1" class="ltx_emph ltx_font_italic">i.e</em>., language priors).
Thus the VQA task requires more consideration for both the design of the benchmark datasets and the corresponding methods.<span id="S1.p1.1.1.2" class="ltx_text"></span></span></p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Compared to the 2D VQA tasks, the VQA task based on the 3D visual scenes is less exploited, which introduces more opportunities and challenges to the 3D vision-and-language field. <span id="S1.p2.1.1" class="ltx_text">As shown in Fig. <a href="#S1.F1" title="Figure 1 ‣ I Introduction ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, extending the 2D VQA task to 3D brings more accurate relative directions and distances in the original 3D scenes, which cannot be well preserved in 2D images.<span id="S1.p2.1.1.1" class="ltx_text"> On one hand, compared to the 2D images, the 3D point cloud-based visual scenes contain rich, complex, and less biased geometric and relation information of different objects without the issues of occlusion.
<span id="S1.p2.1.1.1.1" class="ltx_text">Thus, more <em id="S1.p2.1.1.1.1.1" class="ltx_emph ltx_font_italic">complex and relatively free-form questions</em> can potentially be correctly answered with the aid of the rich 3D point cloud data. On the other hand, to correctly answer these complicated questions, more information related to the objects appeared in the question should be involved to obtain <em id="S1.p2.1.1.1.1.2" class="ltx_emph ltx_font_italic">reliable and explainable results</em><span id="S1.p2.1.1.1.1.3" class="ltx_text">. For example, not only the answer-grounded objects should be grounded by the model, the contextual objects related to the answer-grounded objects should also be identified to precisely predict the correct answer to the questions that involve complex object relationships. Therefore, a <em id="S1.p2.1.1.1.1.3.1" class="ltx_emph ltx_font_italic">complex and explainable</em> 3D grounded visual question answering task needs to be introduced and formulated to boost the development of the 3D visual-and-language field.</span></span></span></span></p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2209.12028/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of our collected dataset and other 2D VQA and 3D VQA datasets. When compared with ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, our dataset contains complex relations with finely-annotated 3 types of related objects. When compared to 2D data, the 3D VQA task can avoid the inherent spatial ambiguity (<em id="S1.F1.2.1" class="ltx_emph ltx_font_italic">e.g</em>., “front”).</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.2" class="ltx_p">To tackle the 3D VQA task, the model needs to first ground to the related objects and then identify how they are related to the answers in the 3D scene based on the textual question, and then infer the correct answer based on the question and the grounded objects. In this way, the model can achieve the explainable 3D VQA results.
In this work, we formulate and address the relatively free-form and explainable 3D VQA task by defining a quintuple (<em id="S1.p3.2.1" class="ltx_emph ltx_font_italic">i.e</em>., <math id="S1.p3.1.m1.1" class="ltx_Math" alttext="\langle" display="inline"><semantics id="S1.p3.1.m1.1a"><mo stretchy="false" id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml">⟨</mo><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><ci id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1">⟨</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">\langle</annotation></semantics></math>question, 3D point cloud data, answer, completely grounded object labels, completely grounded object location<math id="S1.p3.2.m2.1" class="ltx_Math" alttext="\rangle" display="inline"><semantics id="S1.p3.2.m2.1a"><mo stretchy="false" id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml">⟩</mo><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><ci id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1">⟩</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">\rangle</annotation></semantics></math>) for the 3D VQA model. The inputs to the model include a 3D point cloud together with a complex question based on the 3D scene. The outputs from the model contain not only the correct answer to the question, but also the class labels and localization of the related objects (as well as how they are related to the answer) in the scene to assist the reasoning and evaluation of the correct answer.
It is worth mentioning that more complex relationships could be involved for the 3D VQA task when compared to the 2D VQA task. Thanks to the rich geometric and relation information contained in 3D point cloud data,
we formally and fine-grainedly define three types of objects required to reliably assist the inference and evaluation of the correct answer to the questions in 3D VQA: 1) AG-In-Q: the answer-grounded objects appeared in the question, 2) AG-NotIn-Q: the answer-grounded objects not appeared in the question, and 3) Context-Of-AG: the contextual objects related to the answer-grounded objects appeared in the question. Given the question and the 3D scene, the three types of objects jointly provide the reasoning evidence and explanation to the predicted answers. Thus, if all the three types of objects are grounded (including their class labels and localizations as well as their correct object types), we call the 3D VQA task would be completely grounded and explainable.
</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Though three 3D VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> are proposed recently, none of these datasets could well support the free-form and explainable 3D grounded VQA task due to their limited annotations of the question-answer pairs and incomplete annotations to the grounded objects (see <span id="S1.p4.1.1" class="ltx_text">Fig. <a href="#S1.F2" title="Figure 2 ‣ I Introduction ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and<span id="S1.p4.1.1.1" class="ltx_text"> Sec. <a href="#S2.SS2" title="II-B Visual Question Answering (VQA). ‣ II Related Work ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a> for more details).
To this end, in this work, we collect a new 3D visual question answering dataset referred to as FE-3DGQA with diverse and complex question-answer pairs together with dense and complete grounded object bounding box annotations. Based on the ScanNet dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, our dataset consists of 20k manually annotated question-answer pairs with an average of 2.1 completely grounded objects’ bounding box annotations for each QA pair from 703 indoor scenes. Different from the previous works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> without or with only one type of grounded objects,
we annotate the aforementioned three different types of grounded objects for each QA pair.
In addition, observing that all the existing 3D VQA datasets are limited in scale when compared with their 2D counterparts, we further expand our FE-3DGQA dataset by using the existing ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> (<em id="S1.p4.1.1.1.1" class="ltx_emph ltx_font_italic">e.g</em>., ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Referit3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>) datasets based on question templates, resulting in an Extended FE-3DGQA dataset with 112k additional question-answer pairs.</span></span></p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2209.12028/assets/x2.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="307" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The distribution of the number of annotated QA-related objects of our FE-3DGQA dataset and existing AutoGen+HumanEdit dataset ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. <span id="S1.F2.2.1" class="ltx_text">Our proposed dataset includes complete object annotations for different types of objects in each QA, providing explainability for 3D VQA tasks.</span></figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text">Building upon the defined free-form and explainable 3D VQA task and the two newly collected FE-3DGQA datasets, we for the first time propose a strong baseline.
<span id="S1.p5.1.1.1" class="ltx_text">Even though much progress has been made in VQA on mult-view 2D images from 3D scenes, it is non-trivial to fuse the results from different views as these results may be inconsistent due to the missing relative directions and occlusions in images.<span id="S1.p5.1.1.1.1" class="ltx_text">
<span id="S1.p5.1.1.1.1.1" class="ltx_text">Existing 3D VQA works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> encode and fuse the questions and visual data by using different methods, such as Bi-LSTM, BERT, and other encoder-decoder networks. In contrast, we explicitly enhance the within-object attribute features and model the across-object relations by using an enhanced self-attention module, and carefully design our framework to effectively explore three types of related-objects.<span id="S1.p5.1.1.1.1.1.1" class="ltx_text">
Specifically, our method tackles the problem by three objectives: 1) grounding and attending to all the related objects in the 3D scene based on the question, 2) identifying how these objects are related to the answer, and 3) inferring the correct answer based on the question and different types of grounded objects. To achieve the three objectives, we design an end-to-end optimized 3D VQA method with a language branch, a 3D vision branch, and a fusion module.
The language branch is based on a pre-trained T5 model to handle the free-form question-answer pairs.
It provides the object labels of “AG-In-Q” and “Context-Of-AG” objects to guide the corresponding attentions in the 3D scene.
Moreover, it also limits the scope of the candidate answers and links to the potential answers of the question. The 3D vision branch includes a 3D object detector to tokenize the 3D scene with the object proposals as the object tokens, followed by a token enhancement module to extensively encode the within-object and across-object features of all the related objects, which can effectively help additional identify the “AG-NotIn-Q” objects,
and thus ground and attend to all three different types of related objects and eventually provide the visual evidence to the answer. The co-attention-based fusion module enables the interactions between the question and 3D point clouds, which establishes various semantic links between textual question-answer pairs and the corresponding object regions to filter out unrelated objects (and attend to the related objects) in the 3D point clouds, and thus allows the correct predictions of the final answer based on the joint textual and visual features in the subsequent grounded answer generation module.
</span></span></span></span></span></p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">The contributions of our work can be summarized as: (1) We define a new 3D vision and language task called free-form and explainable grounded 3D VQA. To this task, we also collect the new dataset called FE-3DGQA with 20k free-form question-answer pairs, in which we additionally annotate three types of corresponding grounded objects from 703 3D scenes. (2) A new framework is proposed, which can be used as a strong baseline for the free-form and explainable grounded 3D VQA task. (3) The proposed framework is extensively evaluated on the proposed FE-3DGQA dataset and achieves the state-of-the-art results.

</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Work</span>
</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS1.4.1.1" class="ltx_text">II-A</span> </span><span id="S2.SS1.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Vision And Language on 3D scenes.</span>
</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Deep learning in various 3D point cloud based vision tasks has attracted a great deal of interest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
Unlike 2D datasets, the data collection with 3D annotations are expensive, which limits the development of the unified 3D framework. Some recent works are also proposed to explore the 3D vision+language tasks for scene understanding, such as visual grounding <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, visual captioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, 3D scene graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, <em id="S2.SS1.p1.1.1" class="ltx_emph ltx_font_italic">etc</em>.
ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> and Referit3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> are two datasets to first introduce the task to localize 3D objects on RGB-D indoor Scans. Based on the ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> dataset, Scan2Cap <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is recently proposed for dense captioning on 3D scenes, which focuses on describing the attribute and relation information of objects in the scene. 3DSSG <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> is proposed for 3D scene graph generation, which is built based on several instances from the 3RScan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> dataset.
Different methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> are proposed to tackle these problems by joint modeling the language and vision data.
However, compared to these 3D vision-and-language tasks, the 3D visual question answering is more prone to ignore the visual data and overfit to the language priors. Thus, both the collection of 3D VQA dataset and design of 3D VQA method require more consideration compared to other 3D vision-and-language tasks.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S2.SS2.4.1.1" class="ltx_text">II-B</span> </span><span id="S2.SS2.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Visual Question Answering (VQA).</span>
</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Several 2D image-based VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> and methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> are proposed in recent years. However, the VQA task based on 3D point clouds-based visual scene is far less exploited, which introduces more opportunities and challenges to the 3D vision-and-language field.
Recently, three datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> were collected for 3D visual question answering. For example, 3DQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> only contains 6k manually annotated questions, and they do not provide the related object labels and bounding boxes. CLEVR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> is built upon the 3D scene graph datasets and their question-answer pairs are created based on a set of fixed templates from the scene graph annotations. In other words, their annotations are not fully free-form and highly rely on the scene-graph annotations, and thus cannot be easily expanded to other 3D scenes without scene-graph annotations. ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> is another 3D VQA dataset, which contains 41k question-answer pairs. However, their questions are automatically generated from the question generation model based on the fixed form grounding annotations. Though they have annotated the corresponding 3D bounding boxes related to the answers, the annotations of objects are not elaborately classified by how they are related to the questions or answers,
which limits their full reasoning capability on how the answers are generated by the VQA models.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">In summary, none of the 3D VQA datasets could well support the free-form and explainable 3D grounded VQA task due to their limited annotation methods of the question-answer pairs and incomplete annotations to the grounded objects.
In contrast, we collect a new FE-3DGQA dataset, which is a well-annotated dataset with various annotations for object-of-interest for free-form and explainable QA.
We argue that our dataset could help evaluate language-based 3D scene understanding methods, and help reduce the bias issue from the language prior. Moreover, we design a new completely grounded and explainable 3D VQA framework by using a joint training strategy with a specifically designed language branch, an enhanced 3D vision branch, and a fusion module for both VQA and visual grounding tasks.

</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">FE-3DGQA Dataset</span>
</h2>

<figure id="S3.F3" class="ltx_figure"><img src="/html/2209.12028/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="443" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualizations of the newly collected FE-3DGQA dataset. The related objects of AG-In-Q, AG-NotIn-Q, and Context-Of-AG are colored in green, cyan, and red, respectively.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Compared to other vision-language tasks, the VQA task is more susceptible to language priors and co-reference ambiguity and thus requires more consideration for both the design of the benchmark datasets and the corresponding methods. For example, if the questions and answers are highly correlated, the models tend to make predictions based on the co-occurrence patterns between textual questions and answers (<em id="S3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>., the language priors) instead of reasoning upon visual contents. In addition, if the question-answer pairs are not designed properly, the co-reference ambiguity issue arises when a referred object in the questions or answers has multiple correspondences in an image, and the context of the questions or answers is insufficient to distinguish them.
When compared to the 2D VQA task, the VQA task based on the 3D scenes introduces more opportunities and challenges, which motivates us to collect our FE-3DGQA dataset. Fig. <a href="#S3.F3" title="Figure 3 ‣ III FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows some representative examples from our FE-3DGQA dataset. Each 3D VQA sample contains a question based on a 3D scene, the corresponding answer, and the completely grounded object annotations in the 3D scene to facilitate the subsequent research to solve the relatively free-form and explainable 3D grounded VQA task.
Our FE-3DGQA dataset consists of two parts: 1) the relatively free-form question-answer pairs, 2) the completely grounded object annotations.

</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">FE-3DGQA Dataset Collection</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our dataset is annotated based on ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which contains the RGB-D Scans of various indoor scenes with fine-grained instance-level segmentation annotations.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Complex and Relatively Free-Form Question-Answer Pairs.</span>
Different from the 2D image, the 3D point cloud-based visual scenes contain rich, complex, and less biased geometric and related information of different objects without the occlusion issue. Thus, more complex <em id="S3.SS1.p2.1.2" class="ltx_emph ltx_font_italic">free-form</em> questions can potentially be correctly answered with the aid of the rich 3D point cloud data. As shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, our FE-3DGQA dataset contains more complex and free-form questions, where full exploitation and understanding of the corresponding 3D scene is often required to answer those questions.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">Unlike the previous template-based or auto-generated question-answer pairs based on the existing ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> dataset, our datasets are manually annotated with proper instructions on the design of non-trivial questions. Thus, our annotations are completely free-form and potentially go beyond the ScanRefer grounding dataset and ScanNet detection datasets with more complex multi-object relationships. In addition, our annotators are instructed to carefully design the questions customized to each individual scene in a whole to maximally avoid co-reference ambiguity with respect to both questions and answers.
Specifically, to encourage the diversity of QA pairs to reflect a broad coverage of 3D scene details, we require the annotators to annotate each object in a scene with at least 2 different questions. We do not provide any question template to avoid limiting the richness of the questions. However, the questions in each scene should cover all the four different aspects in free-form: 1) the local object-oriented questions with attributes and local relations of a specific object (<em id="S3.SS1.p3.1.1" class="ltx_emph ltx_font_italic">e.g</em>., “what color is the table behind the l-shaped couch?”, “what shape is the table between two white chairs?”, “what is between two printers on the desk?”), 2) the global context-aware questions to understand the whole scene (<em id="S3.SS1.p3.1.2" class="ltx_emph ltx_font_italic">e.g</em>., “what is this room?”, “how many printers in the room?”), 3) the complex relationships among multiple objects (<em id="S3.SS1.p3.1.3" class="ltx_emph ltx_font_italic">e.g</em>., “which is closer to the curtain, the suitcase or the nightstand?”), and 4) the direction or location related questions potentially for navigation (<em id="S3.SS1.p3.1.4" class="ltx_emph ltx_font_italic">e.g</em>., “where is the couch?”, “what kind of furniture is facing two green chairs?”).</p>
</div>
<div id="S3.SS1.p4" class="ltx_para ltx_noindent">
<p id="S3.SS1.p4.1" class="ltx_p"><span id="S3.SS1.p4.1.1" class="ltx_text ltx_font_bold">Completely Grounded Objects Annotations.</span> To support and enable the development of fully explainable 3D VQA models, we not only design and annotate the question-answer pairs, but also annotate all the grounded objects related to the question-answer pairs. Though the ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> dataset also provides the annotations of the answer-grounded objects, we take one step further, and fully annotate all the objects (with object classes and locations) related to both questions and answers.
Moreover, we fine-grainedly categorize the grounded objects into the following three types:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p"><em id="S3.I1.i1.p1.1.1" class="ltx_emph ltx_font_italic">AG-In-Q: the answer-grounded objects appeared in the question</em> (<em id="S3.I1.i1.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>., the ‘table’ in the question ‘what color is the table behind the l-shaped couch?’).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p"><em id="S3.I1.i2.p1.1.1" class="ltx_emph ltx_font_italic">AG-NotIn-Q: the answer-grounded objects not appeared in the question</em> (<em id="S3.I1.i2.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>., the ‘chair’ in the question ‘what is under the white table behind the l-shaped couch?’ and the answer ‘chair’).</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p"><em id="S3.I1.i3.p1.1.1" class="ltx_emph ltx_font_italic">Context-Of-AG: the contextual objects related to the answer-grounded objects appeared in the question</em> (<em id="S3.I1.i3.p1.1.2" class="ltx_emph ltx_font_italic">e.g</em>., the ‘l-shaped couch’ in the question ‘what color is the table behind the l-shaped couch?’).</p>
</div>
</li>
</ol>
<p id="S3.SS1.p4.2" class="ltx_p">The three types of objects jointly assist the design and evaluation of the explainable 3D VQA methods. To be specific, the “AG-In-Q” and “AG-NotIn-Q” objects are answer-grounded. As long as these two types of objects are correctly grounded, the answer to the question will be more explainable. Otherwise, the question may be obtained merely from the language priors in the dataset or by the common sense based on the questions. Moreover, the “AG-In-Q” and “AG-NotIn-Q” annotations also enable the visual answers in the form of bounding boxes. In addition, the “Context-Of-AG” objects appeared in the questions are related to the answer-grounded objects, which are also the key to predict the correct answer. None of the existing 3D VQA datasets annotate this type of objects. However, we argue that the grounding results of the “Context-Of-AG” objects in the visual data provide the key reasoning evidence of the correct answer. For example, for the question: “what is between the two printers on the desk?”, both of the “two printers” and the “desk” belong to the “Context-Of-AG” objects. Thus, the “Context-Of-AG” objects provide the rich cues to the correct answer. So the model needs to precisely ground to the “Context-Of-AG” objects before correctly answering the question.
To aid the prediction and explanation of the correct answer, we annotate the bounding boxes of three different types of objects related to the answers as shown in Fig. <a href="#S3.F3" title="Figure 3 ‣ III FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The “AG-In-Q” objects are marked in green, which indicates the answer-grounded objects that appeared in the question. The “AG-NotIn-Q” objects marked in cyan are answer-grounded objects not appeared in the question. The contextual objects of the answer-grounded objects are categorized as “Context-Of-AG”, which are marked in red. To precisely answer the complex questions with a reasonable explanations, all the three potential types of objects should be correctly grounded.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparison between our dataset and other 3D VQA datasets.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<td id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Datasets</span></td>
<td id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Source</span></td>
<td id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold">Amount</span></td>
<td id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">Object Annotations</span></td>
<td id="S3.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.1.1.5.1" class="ltx_text ltx_font_bold">Collection</span></td>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<td id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S3.T1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">41k</td>
<td id="S3.T1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Unknown Types</td>
<td id="S3.T1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">AutoGen+HumanEdit</td>
</tr>
<tr id="S3.T1.1.3.3" class="ltx_tr">
<td id="S3.T1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r">ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S3.T1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r">ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S3.T1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r">6k</td>
<td id="S3.T1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r">Not annotated</td>
<td id="S3.T1.1.3.3.5" class="ltx_td ltx_align_center">Human</td>
</tr>
<tr id="S3.T1.1.4.4" class="ltx_tr">
<td id="S3.T1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r">CLEVR3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>
</td>
<td id="S3.T1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r">3RScan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>
</td>
<td id="S3.T1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r">60k</td>
<td id="S3.T1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r">Not annotated</td>
<td id="S3.T1.1.4.4.5" class="ltx_td ltx_align_center">Template</td>
</tr>
<tr id="S3.T1.1.5.5" class="ltx_tr">
<td id="S3.T1.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FE-3DGQA (Ours)</td>
<td id="S3.T1.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S3.T1.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20k</td>
<td id="S3.T1.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">AG In / NotIn Q &amp; Context of AG</td>
<td id="S3.T1.1.5.5.5" class="ltx_td ltx_align_center ltx_border_t">Human</td>
</tr>
<tr id="S3.T1.1.6.6" class="ltx_tr">
<td id="S3.T1.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Ext. FE-3DGQA (Ours)</td>
<td id="S3.T1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>
</td>
<td id="S3.T1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">112k</td>
<td id="S3.T1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">AG In / NotIn Q</td>
<td id="S3.T1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b">Template</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">FE-3DGQA Dataset Statistics.</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.11" class="ltx_p">We collected <math id="S3.SS2.p1.1.m1.2" class="ltx_Math" alttext="20,215" display="inline"><semantics id="S3.SS2.p1.1.m1.2a"><mrow id="S3.SS2.p1.1.m1.2.3.2" xref="S3.SS2.p1.1.m1.2.3.1.cmml"><mn id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">20</mn><mo id="S3.SS2.p1.1.m1.2.3.2.1" xref="S3.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.1.m1.2.2" xref="S3.SS2.p1.1.m1.2.2.cmml">215</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.2b"><list id="S3.SS2.p1.1.m1.2.3.1.cmml" xref="S3.SS2.p1.1.m1.2.3.2"><cn type="integer" id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">20</cn><cn type="integer" id="S3.SS2.p1.1.m1.2.2.cmml" xref="S3.SS2.p1.1.m1.2.2">215</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.2c">20,215</annotation></semantics></math> question-answer pairs for <math id="S3.SS2.p1.2.m2.1" class="ltx_Math" alttext="703" display="inline"><semantics id="S3.SS2.p1.2.m2.1a"><mn id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">703</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><cn type="integer" id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">703</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">703</annotation></semantics></math> ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> scenes
<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">‡1subscript‡1\ddagger_{1}</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">‡1subscript‡1\ddagger_{1}</sup><span class="ltx_tag ltx_tag_note"><math id="footnote1.1.1.m1.1" class="ltx_Math" alttext="\ddagger_{1}" display="inline"><semantics id="footnote1.1.1.m1.1c"><msub id="footnote1.1.1.m1.1.1" xref="footnote1.1.1.m1.1.1.cmml"><mo mathcolor="#000000" id="footnote1.1.1.m1.1.1.2" xref="footnote1.1.1.m1.1.1.2.cmml">‡</mo><mn mathcolor="#000000" id="footnote1.1.1.m1.1.1.3" xref="footnote1.1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="footnote1.1.1.m1.1d"><apply id="footnote1.1.1.m1.1.1.cmml" xref="footnote1.1.1.m1.1.1"><csymbol cd="ambiguous" id="footnote1.1.1.m1.1.1.1.cmml" xref="footnote1.1.1.m1.1.1">subscript</csymbol><ci id="footnote1.1.1.m1.1.1.2.cmml" xref="footnote1.1.1.m1.1.1.2">‡</ci><cn type="integer" id="footnote1.1.1.m1.1.1.3.cmml" xref="footnote1.1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote1.1.1.m1.1e">\ddagger_{1}</annotation></semantics></math></span>After excluding 97 test set scenes (due to Copyright issues) from the total 800 scenes, we follow ScanRefer[6] to only annotate one single scan for each of 703 scenes.</span></span></span>
with <math id="S3.SS2.p1.3.m3.2" class="ltx_Math" alttext="42,456" display="inline"><semantics id="S3.SS2.p1.3.m3.2a"><mrow id="S3.SS2.p1.3.m3.2.3.2" xref="S3.SS2.p1.3.m3.2.3.1.cmml"><mn id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml">42</mn><mo id="S3.SS2.p1.3.m3.2.3.2.1" xref="S3.SS2.p1.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.3.m3.2.2" xref="S3.SS2.p1.3.m3.2.2.cmml">456</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.2b"><list id="S3.SS2.p1.3.m3.2.3.1.cmml" xref="S3.SS2.p1.3.m3.2.3.2"><cn type="integer" id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">42</cn><cn type="integer" id="S3.SS2.p1.3.m3.2.2.cmml" xref="S3.SS2.p1.3.m3.2.2">456</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.2c">42,456</annotation></semantics></math> annotated grounded objects. The average numbers of three different types of object in each 3D scene are <math id="S3.SS2.p1.4.m4.1" class="ltx_Math" alttext="1.53" display="inline"><semantics id="S3.SS2.p1.4.m4.1a"><mn id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">1.53</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><cn type="float" id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">1.53</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">1.53</annotation></semantics></math>, <math id="S3.SS2.p1.5.m5.1" class="ltx_Math" alttext="0.18" display="inline"><semantics id="S3.SS2.p1.5.m5.1a"><mn id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml">0.18</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><cn type="float" id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">0.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">0.18</annotation></semantics></math>, and <math id="S3.SS2.p1.6.m6.1" class="ltx_Math" alttext="0.39" display="inline"><semantics id="S3.SS2.p1.6.m6.1a"><mn id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml">0.39</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><cn type="float" id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">0.39</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">0.39</annotation></semantics></math>, respectively. Overall, we annotated <math id="S3.SS2.p1.7.m7.1" class="ltx_Math" alttext="2.1" display="inline"><semantics id="S3.SS2.p1.7.m7.1a"><mn id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">2.1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><cn type="float" id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">2.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">2.1</annotation></semantics></math> objects on average for each scene. And we follow the official ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> setting to split FE-3DGQA as the train/val set, with <math id="S3.SS2.p1.8.m8.2" class="ltx_Math" alttext="16,245" display="inline"><semantics id="S3.SS2.p1.8.m8.2a"><mrow id="S3.SS2.p1.8.m8.2.3.2" xref="S3.SS2.p1.8.m8.2.3.1.cmml"><mn id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">16</mn><mo id="S3.SS2.p1.8.m8.2.3.2.1" xref="S3.SS2.p1.8.m8.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.8.m8.2.2" xref="S3.SS2.p1.8.m8.2.2.cmml">245</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.2b"><list id="S3.SS2.p1.8.m8.2.3.1.cmml" xref="S3.SS2.p1.8.m8.2.3.2"><cn type="integer" id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">16</cn><cn type="integer" id="S3.SS2.p1.8.m8.2.2.cmml" xref="S3.SS2.p1.8.m8.2.2">245</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.2c">16,245</annotation></semantics></math> and <math id="S3.SS2.p1.9.m9.2" class="ltx_Math" alttext="3,970" display="inline"><semantics id="S3.SS2.p1.9.m9.2a"><mrow id="S3.SS2.p1.9.m9.2.3.2" xref="S3.SS2.p1.9.m9.2.3.1.cmml"><mn id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">3</mn><mo id="S3.SS2.p1.9.m9.2.3.2.1" xref="S3.SS2.p1.9.m9.2.3.1.cmml">,</mo><mn id="S3.SS2.p1.9.m9.2.2" xref="S3.SS2.p1.9.m9.2.2.cmml">970</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.2b"><list id="S3.SS2.p1.9.m9.2.3.1.cmml" xref="S3.SS2.p1.9.m9.2.3.2"><cn type="integer" id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">3</cn><cn type="integer" id="S3.SS2.p1.9.m9.2.2.cmml" xref="S3.SS2.p1.9.m9.2.2">970</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.2c">3,970</annotation></semantics></math> samples, respectively. On average, there are <math id="S3.SS2.p1.10.m10.1" class="ltx_Math" alttext="9.10" display="inline"><semantics id="S3.SS2.p1.10.m10.1a"><mn id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml">9.10</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><cn type="float" id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">9.10</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">9.10</annotation></semantics></math> words per question and <math id="S3.SS2.p1.11.m11.1" class="ltx_Math" alttext="28.76" display="inline"><semantics id="S3.SS2.p1.11.m11.1a"><mn id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml">28.76</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><cn type="float" id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">28.76</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">28.76</annotation></semantics></math> question-answer pairs per scene.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Our manually labeled FE-3DGQA dataset enjoys the advantage of free-form and completely grounded objects. However, it is still limited by relatively small scale when compared to other 2D VQA datasets. To enrich our FE-3DGQA dataset, we further extend our 3D VQA dataset by including the original ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> dataset, ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> dataset, and Referit3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> dataset into their QA-like versions based on the fixed templates. We name the extended FE-3DGQA dataset as Ext. FE-3DGQA, which contains around 112k additional template-based QA pairs and 100k additional grounded objects.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">Table <a href="#S3.T1" title="TABLE I ‣ III-A FE-3DGQA Dataset Collection ‣ III FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a> shows the comparison between the existing concurrent 3D VQA datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and our newly collected FE-3DGQA and the Ext. FE-3DGQA datasets. It can be seen that our FE-3DGQA dataset is the largest manually annotated 3D VQA dataset, with complete grounded object annotations. Moreover, by using additional template-based QAs, our Ext. FE-3DGQA dataset is the largest 3D VQA dataset.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2209.12028/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="453" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Question distribution in terms of their first four words in FE-3DGQA.</figcaption>
</figure>
<figure id="S3.F5" class="ltx_figure"><img src="/html/2209.12028/assets/x5.png" id="S3.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="156" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Question distribution in terms of the number of words in each question from FE-3DGQA.</figcaption>
</figure>
<figure id="S3.F6" class="ltx_figure"><img src="/html/2209.12028/assets/x6.png" id="S3.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="442" height="191" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Answer distribution in terms of different type of questions in FE-3DGQA.</figcaption>
</figure>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">As shown in Fig. <a href="#S3.F4" title="Figure 4 ‣ III-B FE-3DGQA Dataset Statistics. ‣ III FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we report the distribution of the question types based on the first four words of the questions in FE-3DGQA. It can be seen that all four aspects are involved in the questions. For example, 1) the local object-oriented questions (<em id="S3.SS2.p4.1.1" class="ltx_emph ltx_font_italic">e.g</em>., ‘what color is the’, ‘what kind of furniture’), 2) the global context-aware questions (<em id="S3.SS2.p4.1.2" class="ltx_emph ltx_font_italic">e.g</em>., ‘what is this room?’, ‘are there more than’, ‘how many chairs are’), 3) the complex relationships among multiple objects (<em id="S3.SS2.p4.1.3" class="ltx_emph ltx_font_italic">e.g</em>., ‘which is more/less/taller’, ‘what is between the’), and 4) the direction or location related questions potentially for navigation (<em id="S3.SS2.p4.1.4" class="ltx_emph ltx_font_italic">e.g</em>., ‘where is the’, ‘facing the’).</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p">In Fig. <a href="#S3.F5" title="Figure 5 ‣ III-B FE-3DGQA Dataset Statistics. ‣ III FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>, we show the distribution of the question types based on the number of words in each type. We can see that the questions have varied lengths with 9.10 words on average, which further demonstrates the diversity of our questions.
Fig. <a href="#S3.F6" title="Figure 6 ‣ III-B FE-3DGQA Dataset Statistics. ‣ III FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the distribution of answers
<span id="S3.SS2.p5.1.1" class="ltx_text">to the question types by the first few words with the highest numbers of samples. We can observe that the answers to most of the questions are relatively balanced.<span id="S3.SS2.p5.1.1.1" class="ltx_text"></span></span></p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">FE-3DGQA dataset collection details</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p"><span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_bold">Annotation.</span>
To ensure the quality and diversity of the grounded questions, especially the correctness of bounding box annotations of various related objects, we ask well-educated university students to annotate the scenes based on an online visualizer website, on which annotators could interactively rotate, move, zoom in/out, and obtain object names and ids. We also developed the scripts to visualize different types of related objects to check the annotations offline.
Specifically, the students are divided into six groups, and each group is assigned with a well-trained leader annotator to ensure the quality of the annotations.
Furthermore, we provide more than 200 samples based on 12 different 3D point cloud scenes to provide concrete instructions for the annotators.
Each object in a scene is annotated with at least two different questions, and each scene should cover all four aspects to achieve the diversity of QA pairs and reflect a broad coverage of 3D scene details. In addition, our annotators are instructed to carefully design the questions customized to each individual scene in a whole to maximally avoid co-reference ambiguity with respect to both questions and answers.
To avoid the view dependency issue in the question (<em id="S3.SS3.p1.1.2" class="ltx_emph ltx_font_italic">i.e</em>., spatial relations such as “left” in the 3D-scene is unclear when facing different directions), we inspire the annotators to add the detailed direction constraints when there are complex relationships in the question.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Filtering.</span>
After collecting the QA pairs together with the objects, each question is refined and filtered more than twice by two different additional annotators.
A different annotator first checks and filters the annotated questions and objects, and the group leader will double-check all the annotations again.
In the filtering stage, the question, the answer, and the related objects are checked and filtered simultaneously.

</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Extended FE-3DGQA Dataset</span>
</h2>

<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Data statistics of our FE-3DGQA dataset and the extended FE-3DGQA dataset.
The numbers in the columns ‘AG-In-Q’, ‘AG-NotIn-Q’, and ‘Context-Of-AG’ indicate the average number of ‘the answer-referred objects appeared in the question’, ‘the answer-referred objects not appeared in the question’, and ‘the contextual objects related to the answer-referred objects appeared in the question’ in each question, respectively. The numbers in the column “Overall” indicates the average number of all types of related object annotations in each question.
<math id="S4.T2.4.m1.1" class="ltx_Math" alttext="{[+]}" display="inline"><semantics id="S4.T2.4.m1.1b"><mrow id="S4.T2.4.m1.1.2.2" xref="S4.T2.4.m1.1.2.1.cmml"><mo stretchy="false" id="S4.T2.4.m1.1.2.2.1" xref="S4.T2.4.m1.1.2.1.1.cmml">[</mo><mo lspace="0em" rspace="0em" id="S4.T2.4.m1.1.1" xref="S4.T2.4.m1.1.1.cmml">+</mo><mo stretchy="false" id="S4.T2.4.m1.1.2.2.2" xref="S4.T2.4.m1.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.m1.1c"><apply id="S4.T2.4.m1.1.2.1.cmml" xref="S4.T2.4.m1.1.2.2"><csymbol cd="latexml" id="S4.T2.4.m1.1.2.1.1.cmml" xref="S4.T2.4.m1.1.2.2.1">delimited-[]</csymbol><plus id="S4.T2.4.m1.1.1.cmml" xref="S4.T2.4.m1.1.1"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.m1.1d">{[+]}</annotation></semantics></math>: Transformed by the extending method described in Section <a href="#S4.SS1" title="IV-A Extending existing datasets to GQA-like version ‣ IV Extended FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-A</span></span></a>.
<math id="S4.T2.5.m2.1" class="ltx_Math" alttext="{[*]}" display="inline"><semantics id="S4.T2.5.m2.1b"><mrow id="S4.T2.5.m2.1.2.2" xref="S4.T2.5.m2.1.2.1.cmml"><mo stretchy="false" id="S4.T2.5.m2.1.2.2.1" xref="S4.T2.5.m2.1.2.1.1.cmml">[</mo><mo lspace="0em" rspace="0em" id="S4.T2.5.m2.1.1" xref="S4.T2.5.m2.1.1.cmml">∗</mo><mo stretchy="false" id="S4.T2.5.m2.1.2.2.2" xref="S4.T2.5.m2.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.m2.1c"><apply id="S4.T2.5.m2.1.2.1.cmml" xref="S4.T2.5.m2.1.2.2"><csymbol cd="latexml" id="S4.T2.5.m2.1.2.1.1.cmml" xref="S4.T2.5.m2.1.2.2.1">delimited-[]</csymbol><times id="S4.T2.5.m2.1.1.cmml" xref="S4.T2.5.m2.1.1"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.m2.1d">{[*]}</annotation></semantics></math>:
Since the transformed grounding dataset and the transformed masked grounding dataset are based on the same dataset, we count the total number of QA pairs and the total number of related objects transformed from the same dataset only once in the Ext. FE-3DGQA dataset.
<math id="S4.T2.6.m3.1" class="ltx_Math" alttext="{[-]}" display="inline"><semantics id="S4.T2.6.m3.1b"><mrow id="S4.T2.6.m3.1.2.2" xref="S4.T2.6.m3.1.2.1.cmml"><mo stretchy="false" id="S4.T2.6.m3.1.2.2.1" xref="S4.T2.6.m3.1.2.1.1.cmml">[</mo><mo lspace="0em" rspace="0em" id="S4.T2.6.m3.1.1" xref="S4.T2.6.m3.1.1.cmml">−</mo><mo stretchy="false" id="S4.T2.6.m3.1.2.2.2" xref="S4.T2.6.m3.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.m3.1c"><apply id="S4.T2.6.m3.1.2.1.cmml" xref="S4.T2.6.m3.1.2.2"><csymbol cd="latexml" id="S4.T2.6.m3.1.2.1.1.cmml" xref="S4.T2.6.m3.1.2.2.1">delimited-[]</csymbol><minus id="S4.T2.6.m3.1.1.cmml" xref="S4.T2.6.m3.1.1"></minus></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.m3.1d">{[-]}</annotation></semantics></math>: Not annotated.
</figcaption>
<table id="S4.T2.13" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T2.13.8.1" class="ltx_tr">
<td id="S4.T2.13.8.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Datasets</td>
<td id="S4.T2.13.8.1.2" class="ltx_td ltx_align_center ltx_border_t">#QA</td>
<td id="S4.T2.13.8.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">#Related Objects</td>
<td id="S4.T2.13.8.1.4" class="ltx_td ltx_align_center ltx_border_t">AG-In-Q</td>
<td id="S4.T2.13.8.1.5" class="ltx_td ltx_align_center ltx_border_t">AG-NotIn-Q</td>
<td id="S4.T2.13.8.1.6" class="ltx_td ltx_align_center ltx_border_t">Context-Of-AG</td>
<td id="S4.T2.13.8.1.7" class="ltx_td ltx_align_center ltx_border_t">Overall</td>
</tr>
<tr id="S4.T2.13.9.2" class="ltx_tr">
<td id="S4.T2.13.9.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">FE-3DGQA</td>
<td id="S4.T2.13.9.2.2" class="ltx_td ltx_align_center ltx_border_t">20,215</td>
<td id="S4.T2.13.9.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">42,456</td>
<td id="S4.T2.13.9.2.4" class="ltx_td ltx_align_center ltx_border_t">1.530</td>
<td id="S4.T2.13.9.2.5" class="ltx_td ltx_align_center ltx_border_t">0.178</td>
<td id="S4.T2.13.9.2.6" class="ltx_td ltx_align_center ltx_border_t">0.392</td>
<td id="S4.T2.13.9.2.7" class="ltx_td ltx_align_center ltx_border_t">2.100</td>
</tr>
<tr id="S4.T2.7.1" class="ltx_tr">
<td id="S4.T2.7.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ScanNet<sup id="S4.T2.7.1.1.1" class="ltx_sup"><span id="S4.T2.7.1.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">+</span></sup>
</td>
<td id="S4.T2.7.1.2" class="ltx_td ltx_align_center ltx_border_t">24,539</td>
<td id="S4.T2.7.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">12,930</td>
<td id="S4.T2.7.1.4" class="ltx_td ltx_align_center ltx_border_t">0.527</td>
<td id="S4.T2.7.1.5" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T2.7.1.6" class="ltx_td ltx_align_center ltx_border_t">0</td>
<td id="S4.T2.7.1.7" class="ltx_td ltx_align_center ltx_border_t">0.527</td>
</tr>
<tr id="S4.T2.8.2" class="ltx_tr">
<td id="S4.T2.8.2.1" class="ltx_td ltx_align_center ltx_border_r">ScanRefer<sup id="S4.T2.8.2.1.1" class="ltx_sup"><span id="S4.T2.8.2.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">+</span></sup>
</td>
<td id="S4.T2.8.2.2" class="ltx_td ltx_align_center">46,173</td>
<td id="S4.T2.8.2.3" class="ltx_td ltx_align_center ltx_border_r">46,173</td>
<td id="S4.T2.8.2.4" class="ltx_td ltx_align_center">1</td>
<td id="S4.T2.8.2.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.8.2.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.8.2.7" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="S4.T2.9.3" class="ltx_tr">
<td id="S4.T2.9.3.1" class="ltx_td ltx_align_center ltx_border_r">Nr3D<sup id="S4.T2.9.3.1.1" class="ltx_sup"><span id="S4.T2.9.3.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">+</span></sup>
</td>
<td id="S4.T2.9.3.2" class="ltx_td ltx_align_center">41,475</td>
<td id="S4.T2.9.3.3" class="ltx_td ltx_align_center ltx_border_r">41,475</td>
<td id="S4.T2.9.3.4" class="ltx_td ltx_align_center">1</td>
<td id="S4.T2.9.3.5" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.9.3.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.9.3.7" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="S4.T2.10.4" class="ltx_tr">
<td id="S4.T2.10.4.1" class="ltx_td ltx_align_center ltx_border_r">Masked ScanRefer<sup id="S4.T2.10.4.1.1" class="ltx_sup"><span id="S4.T2.10.4.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">+</span></sup>
</td>
<td id="S4.T2.10.4.2" class="ltx_td ltx_align_center">46,173</td>
<td id="S4.T2.10.4.3" class="ltx_td ltx_align_center ltx_border_r">46,173</td>
<td id="S4.T2.10.4.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.10.4.5" class="ltx_td ltx_align_center">1</td>
<td id="S4.T2.10.4.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.10.4.7" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="S4.T2.11.5" class="ltx_tr">
<td id="S4.T2.11.5.1" class="ltx_td ltx_align_center ltx_border_r">Masked Nr3D<sup id="S4.T2.11.5.1.1" class="ltx_sup"><span id="S4.T2.11.5.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">+</span></sup>
</td>
<td id="S4.T2.11.5.2" class="ltx_td ltx_align_center">41,475</td>
<td id="S4.T2.11.5.3" class="ltx_td ltx_align_center ltx_border_r">41,475</td>
<td id="S4.T2.11.5.4" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.11.5.5" class="ltx_td ltx_align_center">1</td>
<td id="S4.T2.11.5.6" class="ltx_td ltx_align_center">-</td>
<td id="S4.T2.11.5.7" class="ltx_td ltx_align_center">1</td>
</tr>
<tr id="S4.T2.13.7" class="ltx_tr">
<td id="S4.T2.13.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Ext. FE-3DGQA</td>
<td id="S4.T2.12.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">112,187<sup id="S4.T2.12.6.1.1" class="ltx_sup"><span id="S4.T2.12.6.1.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">∗</span></sup>
</td>
<td id="S4.T2.13.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">100,578<sup id="S4.T2.13.7.2.1" class="ltx_sup"><span id="S4.T2.13.7.2.1.1" class="ltx_text ltx_font_italic" style="color:#000000;">∗</span></sup>
</td>
<td id="S4.T2.13.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.503</td>
<td id="S4.T2.13.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.439</td>
<td id="S4.T2.13.7.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0</td>
<td id="S4.T2.13.7.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">0.942</td>
</tr>
</tbody>
</table>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S4.SS1.4.1.1" class="ltx_text">IV-A</span> </span><span id="S4.SS1.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Extending existing datasets to GQA-like version</span>
</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">To enrich our FE-3DGQA dataset, we further extend our 3D VQA dataset by transforming the original ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> dataset, the ScanRefer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> dataset, and the Referit3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> dataset into their QA-like versions based on the fixed templates for better scene understanding.
The statistics of the extended FE-3DGQA dataset are shown in Tab. <a href="#S4.T2" title="TABLE II ‣ IV Extended FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a>.
Below we describe how we transform each dataset into our Ext. FE-3DGQA in detail.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS1.p2.2" class="ltx_p"><span id="S4.SS1.p2.2.1" class="ltx_text ltx_font_bold">Transforming 3D Detection Dataset to 3D GQA.</span>
The detection dataset consists of the dense object class annotations, which can be transformed to some reasonable questions with fixed templates. Here, we use the same detection annotations from the ScanRefer dataset, in which the class names are slightly different from the original ScanNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
To be specific, we use some templates to generate a new QA dataset based on the annotations from the original detection dataset (<em id="S4.SS1.p2.2.2" class="ltx_emph ltx_font_italic">e.g</em>., “how many <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="&lt;object&gt;" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mrow id="S4.SS1.p2.1.m1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.SS1.p2.1.m1.1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.2.1.cmml">&lt;</mo><mrow id="S4.SS1.p2.1.m1.1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.1.cmml"><mi mathcolor="#000000" id="S4.SS1.p2.1.m1.1.1.1.1.2" xref="S4.SS1.p2.1.m1.1.1.1.1.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.1.1.1" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.1.m1.1.1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.1.1.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.1.1.1a" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.1.m1.1.1.1.1.4" xref="S4.SS1.p2.1.m1.1.1.1.1.4.cmml">j</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.1.1.1b" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.1.m1.1.1.1.1.5" xref="S4.SS1.p2.1.m1.1.1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.1.1.1c" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.1.m1.1.1.1.1.6" xref="S4.SS1.p2.1.m1.1.1.1.1.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.1.m1.1.1.1.1.1d" xref="S4.SS1.p2.1.m1.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.1.m1.1.1.1.1.7" xref="S4.SS1.p2.1.m1.1.1.1.1.7.cmml">t</mi></mrow><mo fence="true" lspace="0em" id="S4.SS1.p2.1.m1.1.1.1.3" xref="S4.SS1.p2.1.m1.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.1"><csymbol cd="latexml" id="S4.SS1.p2.1.m1.1.1.2.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.2">expectation</csymbol><apply id="S4.SS1.p2.1.m1.1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1"><times id="S4.SS1.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.1"></times><ci id="S4.SS1.p2.1.m1.1.1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.2">𝑜</ci><ci id="S4.SS1.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.3">𝑏</ci><ci id="S4.SS1.p2.1.m1.1.1.1.1.4.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.4">𝑗</ci><ci id="S4.SS1.p2.1.m1.1.1.1.1.5.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.5">𝑒</ci><ci id="S4.SS1.p2.1.m1.1.1.1.1.6.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.6">𝑐</ci><ci id="S4.SS1.p2.1.m1.1.1.1.1.7.cmml" xref="S4.SS1.p2.1.m1.1.1.1.1.7">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">&lt;object&gt;</annotation></semantics></math> are there in the room?”, or “is there a <math id="S4.SS1.p2.2.m2.1" class="ltx_Math" alttext="&lt;object&gt;" display="inline"><semantics id="S4.SS1.p2.2.m2.1a"><mrow id="S4.SS1.p2.2.m2.1.1.1" xref="S4.SS1.p2.2.m2.1.1.2.cmml"><mo fence="true" rspace="0em" id="S4.SS1.p2.2.m2.1.1.1.2" xref="S4.SS1.p2.2.m2.1.1.2.1.cmml">&lt;</mo><mrow id="S4.SS1.p2.2.m2.1.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.1.cmml"><mi mathcolor="#000000" id="S4.SS1.p2.2.m2.1.1.1.1.2" xref="S4.SS1.p2.2.m2.1.1.1.1.2.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.1.1.1" xref="S4.SS1.p2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.2.m2.1.1.1.1.3" xref="S4.SS1.p2.2.m2.1.1.1.1.3.cmml">b</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.1.1.1a" xref="S4.SS1.p2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.2.m2.1.1.1.1.4" xref="S4.SS1.p2.2.m2.1.1.1.1.4.cmml">j</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.1.1.1b" xref="S4.SS1.p2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.2.m2.1.1.1.1.5" xref="S4.SS1.p2.2.m2.1.1.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.1.1.1c" xref="S4.SS1.p2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.2.m2.1.1.1.1.6" xref="S4.SS1.p2.2.m2.1.1.1.1.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p2.2.m2.1.1.1.1.1d" xref="S4.SS1.p2.2.m2.1.1.1.1.1.cmml">​</mo><mi mathcolor="#000000" id="S4.SS1.p2.2.m2.1.1.1.1.7" xref="S4.SS1.p2.2.m2.1.1.1.1.7.cmml">t</mi></mrow><mo fence="true" lspace="0em" id="S4.SS1.p2.2.m2.1.1.1.3" xref="S4.SS1.p2.2.m2.1.1.2.1.cmml">&gt;</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.2.m2.1b"><apply id="S4.SS1.p2.2.m2.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.1"><csymbol cd="latexml" id="S4.SS1.p2.2.m2.1.1.2.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1.2">expectation</csymbol><apply id="S4.SS1.p2.2.m2.1.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1"><times id="S4.SS1.p2.2.m2.1.1.1.1.1.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.1"></times><ci id="S4.SS1.p2.2.m2.1.1.1.1.2.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.2">𝑜</ci><ci id="S4.SS1.p2.2.m2.1.1.1.1.3.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.3">𝑏</ci><ci id="S4.SS1.p2.2.m2.1.1.1.1.4.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.4">𝑗</ci><ci id="S4.SS1.p2.2.m2.1.1.1.1.5.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.5">𝑒</ci><ci id="S4.SS1.p2.2.m2.1.1.1.1.6.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.6">𝑐</ci><ci id="S4.SS1.p2.2.m2.1.1.1.1.7.cmml" xref="S4.SS1.p2.2.m2.1.1.1.1.7">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.2.m2.1c">&lt;object&gt;</annotation></semantics></math> in the room?”). The blanks in the template are filled with the 17 candidate object classes contained in the ScanNet dataset. In this way, all the referred objects are marked as “AG-In-Q” objects. Considering that for many answers, we have “zero” object to be referred to in this extended dataset, we eventually obtain 24,539 QAs with an average of 0.527 annotated objects for each query in the scenes.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text ltx_font_bold">Transforming 3D Visual Grounding Datasets to 3D GQA.</span>
Except for the detection annotations, the ScanRefer dataset and the Nr3D dataset for the visual grounding task also provide rich grounding annotations with dense descriptions of object attributes and relationships.
Thus, we treat the descriptions from the object grounding task as the input question, the referred object name as the answer, and the corresponding object bounding box annotation as the visually grounded “AG-In-Q” object.
</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text ltx_font_bold">Transforming Masked 3D Visual Grounding Datasets to 3D GQA.</span>
Since the answers (<em id="S4.SS1.p4.1.2" class="ltx_emph ltx_font_italic">e.g</em>., the grounded objects) in the transformed visual grounding datasets can always be found in the questions (<em id="S4.SS1.p4.1.3" class="ltx_emph ltx_font_italic">e.g</em>., the grounding description), the VQA model may be biased by the language priors. To alleviate the issue, we propose to mask the referred object in the grounding description to form a new question.
We treat the masked object name as the answer and the corresponding object bounding box annotation as the visually grounded “AG-NotIn-Q” object.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Methodology</span>
</h2>

<figure id="S5.F7" class="ltx_figure"><img src="/html/2209.12028/assets/x7.png" id="S5.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Overview of our framework. The inputs (<em id="S5.F7.2.1" class="ltx_emph ltx_font_italic">i.e</em>., the question and the point cloud) are first encoded by the tokenization module. Then, the word tokens and the object tokens are enhanced and fused by using several transformer-based layers, which consist of both self-attention and co-attention modules. Finally, the grounded answers together with the predicted bounding boxes as well as their types and labels are generated by using separate decoders.</figcaption>
</figure>
<div id="S5.p1" class="ltx_para">
<p id="S5.p1.2" class="ltx_p">With our completely grounded 3D VQA dataset, we formulate the FE-3DGQA problem as follows: given the point cloud <math id="S5.p1.1.m1.1" class="ltx_Math" alttext="\bm{P}" display="inline"><semantics id="S5.p1.1.m1.1a"><mi id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">𝑷</mi><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><ci id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">𝑷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\bm{P}</annotation></semantics></math> and the question <math id="S5.p1.2.m2.1" class="ltx_Math" alttext="\bm{Q}" display="inline"><semantics id="S5.p1.2.m2.1a"><mi id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">𝑸</mi><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">𝑸</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\bm{Q}</annotation></semantics></math> about the 3D scene, we introduce a new framework to predict the visually grounded answer together with the corresponding bounding boxes and labels of all the related objects.
Our method tackles the problem by three objectives: 1) grounding and attending to all the related objects in the 3D scene based on the question, 2) identifying how these objects are related to the answer, and 3) inferring the correct answer based on the question and different types of grounded objects. To achieve these objectives, as shown in Fig. <a href="#S5.F7" title="Figure 7 ‣ V Methodology ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, our framework consists of a language branch, a 3D vison branch, and a fusion module. We further split our model into the tokenization module to tokenize both the 3D point clouds and the textual question respectively, the token encoding and fusion modules to encode and exchange the tokenized features from different modalities, and the grounded answer generation modules to predict the final visually grounded answer.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS1.4.1.1" class="ltx_text">V-A</span> </span><span id="S5.SS1.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Tokenization module.</span>
</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.2" class="ltx_p">We tokenize the input question <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="\bm{Q}" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mi id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">𝑸</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><ci id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">𝑸</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\bm{Q}</annotation></semantics></math> and 3D point cloud <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="\bm{P}" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mi id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">𝑷</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><ci id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">𝑷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\bm{P}</annotation></semantics></math> into word-level tokens and object-level tokens by using a language tokenizer and a 3D point cloud tokenizer, respectively.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para ltx_noindent">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text ltx_font_bold">Language Tokenizer.</span>
Our language tokenizer is a standard T5 tokenizer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite>, which is widely-used in natural language processing. We also follow T5 to add a task-specific prefix before feeding the question into the T5-tokenizer ( <em id="S5.SS1.p2.1.2" class="ltx_emph ltx_font_italic">e.g</em>., “[visual question answering:] is there a chair in this room?”). The outputs of our language tokenizer are the list of IDs of all the words appeared in the question.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para ltx_noindent">
<p id="S5.SS1.p3.3" class="ltx_p"><span id="S5.SS1.p3.3.1" class="ltx_text ltx_font_bold">3D Point Cloud Tokenizer (<em id="S5.SS1.p3.3.1.1" class="ltx_emph ltx_font_italic">i.e</em>., 3D Object Detector).</span>
The input of the 3D detector is the Point Cloud <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="\bm{P}\in\mathbb{R}^{N\times(3+K)}" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.2" xref="S5.SS1.p3.1.m1.1.2.cmml"><mi id="S5.SS1.p3.1.m1.1.2.2" xref="S5.SS1.p3.1.m1.1.2.2.cmml">𝑷</mi><mo id="S5.SS1.p3.1.m1.1.2.1" xref="S5.SS1.p3.1.m1.1.2.1.cmml">∈</mo><msup id="S5.SS1.p3.1.m1.1.2.3" xref="S5.SS1.p3.1.m1.1.2.3.cmml"><mi id="S5.SS1.p3.1.m1.1.2.3.2" xref="S5.SS1.p3.1.m1.1.2.3.2.cmml">ℝ</mi><mrow id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml"><mi mathcolor="#000000" id="S5.SS1.p3.1.m1.1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.1.3.cmml">N</mi><mo lspace="0.222em" mathcolor="#000000" rspace="0.222em" id="S5.SS1.p3.1.m1.1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.1.2.cmml">×</mo><mrow id="S5.SS1.p3.1.m1.1.1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.cmml"><mo mathcolor="#000000" stretchy="false" id="S5.SS1.p3.1.m1.1.1.1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.SS1.p3.1.m1.1.1.1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.cmml"><mn mathcolor="#000000" id="S5.SS1.p3.1.m1.1.1.1.1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.2.cmml">3</mn><mo mathcolor="#000000" id="S5.SS1.p3.1.m1.1.1.1.1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.1.cmml">+</mo><mi mathcolor="#000000" id="S5.SS1.p3.1.m1.1.1.1.1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.3.cmml">K</mi></mrow><mo mathcolor="#000000" stretchy="false" id="S5.SS1.p3.1.m1.1.1.1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.2"><in id="S5.SS1.p3.1.m1.1.2.1.cmml" xref="S5.SS1.p3.1.m1.1.2.1"></in><ci id="S5.SS1.p3.1.m1.1.2.2.cmml" xref="S5.SS1.p3.1.m1.1.2.2">𝑷</ci><apply id="S5.SS1.p3.1.m1.1.2.3.cmml" xref="S5.SS1.p3.1.m1.1.2.3"><csymbol cd="ambiguous" id="S5.SS1.p3.1.m1.1.2.3.1.cmml" xref="S5.SS1.p3.1.m1.1.2.3">superscript</csymbol><ci id="S5.SS1.p3.1.m1.1.2.3.2.cmml" xref="S5.SS1.p3.1.m1.1.2.3.2">ℝ</ci><apply id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"><times id="S5.SS1.p3.1.m1.1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.1.2"></times><ci id="S5.SS1.p3.1.m1.1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.1.3">𝑁</ci><apply id="S5.SS1.p3.1.m1.1.1.1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1.1.1"><plus id="S5.SS1.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.1"></plus><cn type="integer" id="S5.SS1.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.2">3</cn><ci id="S5.SS1.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.1.1.1.1.3">𝐾</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">\bm{P}\in\mathbb{R}^{N\times(3+K)}</annotation></semantics></math>, which represents the whole scene as <math id="S5.SS1.p3.2.m2.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS1.p3.2.m2.1a"><mi mathcolor="#000000" id="S5.SS1.p3.2.m2.1.1" xref="S5.SS1.p3.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.2.m2.1b"><ci id="S5.SS1.p3.2.m2.1.1.cmml" xref="S5.SS1.p3.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.2.m2.1c">N</annotation></semantics></math> 3D coordinates with <math id="S5.SS1.p3.3.m3.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS1.p3.3.m3.1a"><mi mathcolor="#000000" id="S5.SS1.p3.3.m3.1.1" xref="S5.SS1.p3.3.m3.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.3.m3.1b"><ci id="S5.SS1.p3.3.m3.1.1.cmml" xref="S5.SS1.p3.3.m3.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.3.m3.1c">K</annotation></semantics></math> dimensional hidden features, which is a combination of the color, the normal vector, the height, and the 128-dimensional multi-view appearance features for each point in the 3D scene.
For the object detector, we use a variant of VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, in which we do not use the pre-defined anchors but predict the distance between the voted 3D points and the bounding box boundaries, inspired by FCOS <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>. The outputs of our point cloud tokeinzer are the predicted object proposals for all the objects in the 3D scene.</p>
</div>
<figure id="S5.F8" class="ltx_figure"><img src="/html/2209.12028/assets/x8.png" id="S5.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="151" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The detailed structure of the enhanced self-attention module for the object branch. We additionally encode within-object attribute features and across-object spatial distances to enhance 3D object token features.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS2.4.1.1" class="ltx_text">V-B</span> </span><span id="S5.SS2.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Token Encoding and Fusion Module</span>
</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.2" class="ltx_p">We use <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mi mathcolor="#000000" id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><ci id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">k</annotation></semantics></math> interlaced self-attention and cross-attention layers for token-level feature encoding and fusion. The self-attention layers aim to extract high-level semantic information and encode rich relation information (<em id="S5.SS2.p1.2.1" class="ltx_emph ltx_font_italic">i.e</em>., word-to-word relations for the question, and object-to-object relations for the point clouds) respectively, while the cross-attention layers focus on how to model complex interactions between different modalities and exchange word-object information.
In our experiments, we empirically set <math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="k=2" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">k</mi><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S5.SS2.p1.2.m2.1.1.3" xref="S5.SS2.p1.2.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><eq id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1"></eq><ci id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">𝑘</ci><cn type="integer" id="S5.SS2.p1.2.m2.1.1.3.cmml" xref="S5.SS2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">k=2</annotation></semantics></math>.
With the baseline framework in hand, we propose to enhance the 3D VQA performance by approaching to the three objectives more closely via the improved language encoding branch, enhanced point cloud encoding branch, as well as the enriched fusion modules.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.1" class="ltx_p">Firstly, for the language encoding, we use the encoder of a pre-trained T5 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> (<em id="S5.SS2.p2.1.1" class="ltx_emph ltx_font_italic">i.e</em>., a 12-layer transformer encoder) to extract high-level semantic features and effectively encode the free-form questions. The motivation is that, the self-attention layers in the T5 model could encode rich word-to-word relations. Moreover, the questions in our FE-3DGQA dataset are free-form, and simply encoding the original free-form questions without using any pre-trained language model may be overfited by the questions. Then the language encoding modules not only help predict the labels of the answer-grounded objects that appeared in the question(“AG-In-Q”) and the contextual objects (“Context-Of-AG”) related to the “AG-In-Q” objects, but also
help select the potential answer types.
</p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.3" class="ltx_p">Secondly, for the 3D object encoding branch, we propose an enhanced self-attention module to extensively encode the within-object features (<em id="S5.SS2.p3.3.1" class="ltx_emph ltx_font_italic">i.e</em>., the shape, color, texture, and materials) and across-object features (<em id="S5.SS2.p3.3.2" class="ltx_emph ltx_font_italic">i.e</em>., the complex global &amp; local relations among different objects) to precisely attend and ground to all of the answer related objects. As shown in Fig. <a href="#S5.F8" title="Figure 8 ‣ V-A Tokenization module. ‣ V Methodology ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>, for the within-object feature encoding, we apply a 2-layer linear projection to the concatenation of the bounding box corners and the inputted <math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mi mathcolor="#000000" id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">K</annotation></semantics></math> dimensional multi-view RGB features in the corresponding bounding box.
For across-object feature encoding, motivated by the success of relative position bias in the state-of-the-art 2D and 3D methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, we encode the pairwise distances between any two object tokens (<em id="S5.SS2.p3.3.3" class="ltx_emph ltx_font_italic">i.e</em>., the distance between any two object centers along <math id="S5.SS2.p3.2.m2.2" class="ltx_Math" alttext="x,y" display="inline"><semantics id="S5.SS2.p3.2.m2.2a"><mrow id="S5.SS2.p3.2.m2.2.3.2" xref="S5.SS2.p3.2.m2.2.3.1.cmml"><mi mathcolor="#000000" id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml">x</mi><mo id="S5.SS2.p3.2.m2.2.3.2.1" xref="S5.SS2.p3.2.m2.2.3.1.cmml">,</mo><mi mathcolor="#000000" id="S5.SS2.p3.2.m2.2.2" xref="S5.SS2.p3.2.m2.2.2.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.2b"><list id="S5.SS2.p3.2.m2.2.3.1.cmml" xref="S5.SS2.p3.2.m2.2.3.2"><ci id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">𝑥</ci><ci id="S5.SS2.p3.2.m2.2.2.cmml" xref="S5.SS2.p3.2.m2.2.2">𝑦</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.2c">x,y</annotation></semantics></math> and <math id="S5.SS2.p3.3.m3.1" class="ltx_Math" alttext="z" display="inline"><semantics id="S5.SS2.p3.3.m3.1a"><mi mathcolor="#000000" id="S5.SS2.p3.3.m3.1.1" xref="S5.SS2.p3.3.m3.1.1.cmml">z</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.3.m3.1b"><ci id="S5.SS2.p3.3.m3.1.1.cmml" xref="S5.SS2.p3.3.m3.1.1">𝑧</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.3.m3.1c">z</annotation></semantics></math> direction) by using a 2-layer linear projection and then add it to the attention map of each self-attention module. The well encoded object features not only provide within-object and across-object features of all the three types of objects, but also help identify the answer-grounded objects not appeared in the question (“AG-NotIn-Q”). Moreover, it also encodes the visual evidence to the correct answers.</p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p">Thirdly, the cross-attention-based fusion modules between the language and visual branches focus on how to model complex interactions between different modalities and exchange word-object information. In our model, we use co-attention layers by adding cross-attention layers to both branches, respectively. Specifically, the cross-attention layer in the language branch helps the model to predict the labels of all the three types of answer related objects, because the language branch itself is by no means to predict the answer-grounded object not appeared in the question (“AG-NotIn-Q”) without visual information. It also allows the prediction of the correct answer based on the joint textual and visual features. For the cross-attention layer in the visual branch, the encoded word features in the question provide a guidance on where to attend the feature in the 3D scene for filtering out the unrelated objects features to the potential answer.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS3.4.1.1" class="ltx_text">V-C</span> </span><span id="S5.SS3.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Grounded Answer Generation Module</span>
</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">With the encoded and fused features from the question and 3D point clouds, we generate the free-form grounded answers together with the corresponding labels, types, and bounding boxes of the related objects. For answer generation, we use the decoder of a pre-trained T5 model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> (<em id="S5.SS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>., a 12-layer transformer decoder)
with two more parallel linear layers for answer prediction and related objects label and type prediction, respectively. Specifically, for simplicity, we cast the answer generation task as a classification problem similar to the previous 3D VQA methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Besides, to improve the explainability and reliability of the predicted answer, we also propose an auxiliary task by predicting the QA-related object classes and the corresponding object types for each class,
which attempt to link the answer to specific related objects. Besides, the 3D vision branch not only locates and classifies the bounding boxes of all the related objects but also predicts the corresponding object types, such that the predicted answer is completely grounded.
</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S5.SS4.4.1.1" class="ltx_text">V-D</span> </span><span id="S5.SS4.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Loss Function</span>
</h3>

<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.9" class="ltx_p">The loss function of our method is a combination of detection loss <math id="S5.SS4.p1.1.m1.1" class="ltx_Math" alttext="L_{\text{detection}}" display="inline"><semantics id="S5.SS4.p1.1.m1.1a"><msub id="S5.SS4.p1.1.m1.1.1" xref="S5.SS4.p1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.1.m1.1.1.2" xref="S5.SS4.p1.1.m1.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S5.SS4.p1.1.m1.1.1.3" xref="S5.SS4.p1.1.m1.1.1.3a.cmml">detection</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.1.m1.1b"><apply id="S5.SS4.p1.1.m1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.1.m1.1.1.1.cmml" xref="S5.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.p1.1.m1.1.1.2.cmml" xref="S5.SS4.p1.1.m1.1.1.2">𝐿</ci><ci id="S5.SS4.p1.1.m1.1.1.3a.cmml" xref="S5.SS4.p1.1.m1.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S5.SS4.p1.1.m1.1.1.3.cmml" xref="S5.SS4.p1.1.m1.1.1.3">detection</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.1.m1.1c">L_{\text{detection}}</annotation></semantics></math>, question answering loss <math id="S5.SS4.p1.2.m2.1" class="ltx_Math" alttext="L_{\text{QA}}" display="inline"><semantics id="S5.SS4.p1.2.m2.1a"><msub id="S5.SS4.p1.2.m2.1.1" xref="S5.SS4.p1.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.2.m2.1.1.2" xref="S5.SS4.p1.2.m2.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S5.SS4.p1.2.m2.1.1.3" xref="S5.SS4.p1.2.m2.1.1.3a.cmml">QA</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.2.m2.1b"><apply id="S5.SS4.p1.2.m2.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.2.m2.1.1.1.cmml" xref="S5.SS4.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS4.p1.2.m2.1.1.2.cmml" xref="S5.SS4.p1.2.m2.1.1.2">𝐿</ci><ci id="S5.SS4.p1.2.m2.1.1.3a.cmml" xref="S5.SS4.p1.2.m2.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S5.SS4.p1.2.m2.1.1.3.cmml" xref="S5.SS4.p1.2.m2.1.1.3">QA</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.2.m2.1c">L_{\text{QA}}</annotation></semantics></math>, the object type classification loss <math id="S5.SS4.p1.3.m3.1" class="ltx_Math" alttext="L_{\text{type}}" display="inline"><semantics id="S5.SS4.p1.3.m3.1a"><msub id="S5.SS4.p1.3.m3.1.1" xref="S5.SS4.p1.3.m3.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.3.m3.1.1.2" xref="S5.SS4.p1.3.m3.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S5.SS4.p1.3.m3.1.1.3" xref="S5.SS4.p1.3.m3.1.1.3a.cmml">type</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.3.m3.1b"><apply id="S5.SS4.p1.3.m3.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.3.m3.1.1.1.cmml" xref="S5.SS4.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS4.p1.3.m3.1.1.2.cmml" xref="S5.SS4.p1.3.m3.1.1.2">𝐿</ci><ci id="S5.SS4.p1.3.m3.1.1.3a.cmml" xref="S5.SS4.p1.3.m3.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S5.SS4.p1.3.m3.1.1.3.cmml" xref="S5.SS4.p1.3.m3.1.1.3">type</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.3.m3.1c">L_{\text{type}}</annotation></semantics></math> and an auxiliary classification loss <math id="S5.SS4.p1.4.m4.1" class="ltx_Math" alttext="L_{\text{sem-cls}}" display="inline"><semantics id="S5.SS4.p1.4.m4.1a"><msub id="S5.SS4.p1.4.m4.1.1" xref="S5.SS4.p1.4.m4.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.4.m4.1.1.2" xref="S5.SS4.p1.4.m4.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S5.SS4.p1.4.m4.1.1.3" xref="S5.SS4.p1.4.m4.1.1.3a.cmml">sem-cls</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.4.m4.1b"><apply id="S5.SS4.p1.4.m4.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.4.m4.1.1.1.cmml" xref="S5.SS4.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS4.p1.4.m4.1.1.2.cmml" xref="S5.SS4.p1.4.m4.1.1.2">𝐿</ci><ci id="S5.SS4.p1.4.m4.1.1.3a.cmml" xref="S5.SS4.p1.4.m4.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S5.SS4.p1.4.m4.1.1.3.cmml" xref="S5.SS4.p1.4.m4.1.1.3">sem-cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.4.m4.1c">L_{\text{sem-cls}}</annotation></semantics></math> to predict the related object semantic classes and the types to each class.
The detection loss <math id="S5.SS4.p1.5.m5.1" class="ltx_Math" alttext="L_{\text{detection}}" display="inline"><semantics id="S5.SS4.p1.5.m5.1a"><msub id="S5.SS4.p1.5.m5.1.1" xref="S5.SS4.p1.5.m5.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.5.m5.1.1.2" xref="S5.SS4.p1.5.m5.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S5.SS4.p1.5.m5.1.1.3" xref="S5.SS4.p1.5.m5.1.1.3a.cmml">detection</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.5.m5.1b"><apply id="S5.SS4.p1.5.m5.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.5.m5.1.1.1.cmml" xref="S5.SS4.p1.5.m5.1.1">subscript</csymbol><ci id="S5.SS4.p1.5.m5.1.1.2.cmml" xref="S5.SS4.p1.5.m5.1.1.2">𝐿</ci><ci id="S5.SS4.p1.5.m5.1.1.3a.cmml" xref="S5.SS4.p1.5.m5.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S5.SS4.p1.5.m5.1.1.3.cmml" xref="S5.SS4.p1.5.m5.1.1.3">detection</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.5.m5.1c">L_{\text{detection}}</annotation></semantics></math> is almost the same as used in VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>, except that the anchor-based bounding box classification loss and the regression loss are replaced with the boundary regression loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.
We formulate the grounded question answering problem as a classical multi-class classification problem for answers and a multi-label grounding problem for the grounded objects.
We use the focal loss as the question answering loss <math id="S5.SS4.p1.6.m6.1" class="ltx_Math" alttext="L_{\text{QA}}" display="inline"><semantics id="S5.SS4.p1.6.m6.1a"><msub id="S5.SS4.p1.6.m6.1.1" xref="S5.SS4.p1.6.m6.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.6.m6.1.1.2" xref="S5.SS4.p1.6.m6.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S5.SS4.p1.6.m6.1.1.3" xref="S5.SS4.p1.6.m6.1.1.3a.cmml">QA</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.6.m6.1b"><apply id="S5.SS4.p1.6.m6.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.6.m6.1.1.1.cmml" xref="S5.SS4.p1.6.m6.1.1">subscript</csymbol><ci id="S5.SS4.p1.6.m6.1.1.2.cmml" xref="S5.SS4.p1.6.m6.1.1.2">𝐿</ci><ci id="S5.SS4.p1.6.m6.1.1.3a.cmml" xref="S5.SS4.p1.6.m6.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S5.SS4.p1.6.m6.1.1.3.cmml" xref="S5.SS4.p1.6.m6.1.1.3">QA</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.6.m6.1c">L_{\text{QA}}</annotation></semantics></math> to better handle the long-tailed answers.
The object type classification loss <math id="S5.SS4.p1.7.m7.1" class="ltx_Math" alttext="L_{\text{type}}" display="inline"><semantics id="S5.SS4.p1.7.m7.1a"><msub id="S5.SS4.p1.7.m7.1.1" xref="S5.SS4.p1.7.m7.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.7.m7.1.1.2" xref="S5.SS4.p1.7.m7.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S5.SS4.p1.7.m7.1.1.3" xref="S5.SS4.p1.7.m7.1.1.3a.cmml">type</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.7.m7.1b"><apply id="S5.SS4.p1.7.m7.1.1.cmml" xref="S5.SS4.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.7.m7.1.1.1.cmml" xref="S5.SS4.p1.7.m7.1.1">subscript</csymbol><ci id="S5.SS4.p1.7.m7.1.1.2.cmml" xref="S5.SS4.p1.7.m7.1.1.2">𝐿</ci><ci id="S5.SS4.p1.7.m7.1.1.3a.cmml" xref="S5.SS4.p1.7.m7.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S5.SS4.p1.7.m7.1.1.3.cmml" xref="S5.SS4.p1.7.m7.1.1.3">type</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.7.m7.1c">L_{\text{type}}</annotation></semantics></math> from the 3D vision branch is a focal loss for the object proposals generated by the detector, where
we regard the object proposals whose IoUs with the ground-truth objects larger than 0.5
as positive samples and other object proposals as negative samples during the training process.
The auxiliary related object semantic label and type classification loss <math id="S5.SS4.p1.8.m8.1" class="ltx_Math" alttext="L_{\text{sem-cls}}" display="inline"><semantics id="S5.SS4.p1.8.m8.1a"><msub id="S5.SS4.p1.8.m8.1.1" xref="S5.SS4.p1.8.m8.1.1.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.8.m8.1.1.2" xref="S5.SS4.p1.8.m8.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S5.SS4.p1.8.m8.1.1.3" xref="S5.SS4.p1.8.m8.1.1.3a.cmml">sem-cls</mtext></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.8.m8.1b"><apply id="S5.SS4.p1.8.m8.1.1.cmml" xref="S5.SS4.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS4.p1.8.m8.1.1.1.cmml" xref="S5.SS4.p1.8.m8.1.1">subscript</csymbol><ci id="S5.SS4.p1.8.m8.1.1.2.cmml" xref="S5.SS4.p1.8.m8.1.1.2">𝐿</ci><ci id="S5.SS4.p1.8.m8.1.1.3a.cmml" xref="S5.SS4.p1.8.m8.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S5.SS4.p1.8.m8.1.1.3.cmml" xref="S5.SS4.p1.8.m8.1.1.3">sem-cls</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.8.m8.1c">L_{\text{sem-cls}}</annotation></semantics></math> of the language branch is a focal loss to predict both object labels and corresponding types.
The final loss function is a combination of these items, <em id="S5.SS4.p1.9.1" class="ltx_emph ltx_font_italic">i.e</em>., <math id="S5.SS4.p1.9.m9.1" class="ltx_Math" alttext="L_{\text{All}}=L_{\text{detection}}+0.5L_{\text{QA}}+0.5L_{\text{type}}+0.5L_{\text{sem-cls}}" display="inline"><semantics id="S5.SS4.p1.9.m9.1a"><mrow id="S5.SS4.p1.9.m9.1.1" xref="S5.SS4.p1.9.m9.1.1.cmml"><msub id="S5.SS4.p1.9.m9.1.1.2" xref="S5.SS4.p1.9.m9.1.1.2.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.9.m9.1.1.2.2" xref="S5.SS4.p1.9.m9.1.1.2.2.cmml">L</mi><mtext id="S5.SS4.p1.9.m9.1.1.2.3" xref="S5.SS4.p1.9.m9.1.1.2.3a.cmml">All</mtext></msub><mo id="S5.SS4.p1.9.m9.1.1.1" xref="S5.SS4.p1.9.m9.1.1.1.cmml">=</mo><mrow id="S5.SS4.p1.9.m9.1.1.3" xref="S5.SS4.p1.9.m9.1.1.3.cmml"><msub id="S5.SS4.p1.9.m9.1.1.3.2" xref="S5.SS4.p1.9.m9.1.1.3.2.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.9.m9.1.1.3.2.2" xref="S5.SS4.p1.9.m9.1.1.3.2.2.cmml">L</mi><mtext id="S5.SS4.p1.9.m9.1.1.3.2.3" xref="S5.SS4.p1.9.m9.1.1.3.2.3a.cmml">detection</mtext></msub><mo id="S5.SS4.p1.9.m9.1.1.3.1" xref="S5.SS4.p1.9.m9.1.1.3.1.cmml">+</mo><mrow id="S5.SS4.p1.9.m9.1.1.3.3" xref="S5.SS4.p1.9.m9.1.1.3.3.cmml"><mn id="S5.SS4.p1.9.m9.1.1.3.3.2" xref="S5.SS4.p1.9.m9.1.1.3.3.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p1.9.m9.1.1.3.3.1" xref="S5.SS4.p1.9.m9.1.1.3.3.1.cmml">​</mo><msub id="S5.SS4.p1.9.m9.1.1.3.3.3" xref="S5.SS4.p1.9.m9.1.1.3.3.3.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.9.m9.1.1.3.3.3.2" xref="S5.SS4.p1.9.m9.1.1.3.3.3.2.cmml">L</mi><mtext id="S5.SS4.p1.9.m9.1.1.3.3.3.3" xref="S5.SS4.p1.9.m9.1.1.3.3.3.3a.cmml">QA</mtext></msub></mrow><mo id="S5.SS4.p1.9.m9.1.1.3.1a" xref="S5.SS4.p1.9.m9.1.1.3.1.cmml">+</mo><mrow id="S5.SS4.p1.9.m9.1.1.3.4" xref="S5.SS4.p1.9.m9.1.1.3.4.cmml"><mn id="S5.SS4.p1.9.m9.1.1.3.4.2" xref="S5.SS4.p1.9.m9.1.1.3.4.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p1.9.m9.1.1.3.4.1" xref="S5.SS4.p1.9.m9.1.1.3.4.1.cmml">​</mo><msub id="S5.SS4.p1.9.m9.1.1.3.4.3" xref="S5.SS4.p1.9.m9.1.1.3.4.3.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.9.m9.1.1.3.4.3.2" xref="S5.SS4.p1.9.m9.1.1.3.4.3.2.cmml">L</mi><mtext id="S5.SS4.p1.9.m9.1.1.3.4.3.3" xref="S5.SS4.p1.9.m9.1.1.3.4.3.3a.cmml">type</mtext></msub></mrow><mo id="S5.SS4.p1.9.m9.1.1.3.1b" xref="S5.SS4.p1.9.m9.1.1.3.1.cmml">+</mo><mrow id="S5.SS4.p1.9.m9.1.1.3.5" xref="S5.SS4.p1.9.m9.1.1.3.5.cmml"><mn id="S5.SS4.p1.9.m9.1.1.3.5.2" xref="S5.SS4.p1.9.m9.1.1.3.5.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S5.SS4.p1.9.m9.1.1.3.5.1" xref="S5.SS4.p1.9.m9.1.1.3.5.1.cmml">​</mo><msub id="S5.SS4.p1.9.m9.1.1.3.5.3" xref="S5.SS4.p1.9.m9.1.1.3.5.3.cmml"><mi mathcolor="#000000" id="S5.SS4.p1.9.m9.1.1.3.5.3.2" xref="S5.SS4.p1.9.m9.1.1.3.5.3.2.cmml">L</mi><mtext id="S5.SS4.p1.9.m9.1.1.3.5.3.3" xref="S5.SS4.p1.9.m9.1.1.3.5.3.3a.cmml">sem-cls</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS4.p1.9.m9.1b"><apply id="S5.SS4.p1.9.m9.1.1.cmml" xref="S5.SS4.p1.9.m9.1.1"><eq id="S5.SS4.p1.9.m9.1.1.1.cmml" xref="S5.SS4.p1.9.m9.1.1.1"></eq><apply id="S5.SS4.p1.9.m9.1.1.2.cmml" xref="S5.SS4.p1.9.m9.1.1.2"><csymbol cd="ambiguous" id="S5.SS4.p1.9.m9.1.1.2.1.cmml" xref="S5.SS4.p1.9.m9.1.1.2">subscript</csymbol><ci id="S5.SS4.p1.9.m9.1.1.2.2.cmml" xref="S5.SS4.p1.9.m9.1.1.2.2">𝐿</ci><ci id="S5.SS4.p1.9.m9.1.1.2.3a.cmml" xref="S5.SS4.p1.9.m9.1.1.2.3"><mtext mathsize="70%" id="S5.SS4.p1.9.m9.1.1.2.3.cmml" xref="S5.SS4.p1.9.m9.1.1.2.3">All</mtext></ci></apply><apply id="S5.SS4.p1.9.m9.1.1.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3"><plus id="S5.SS4.p1.9.m9.1.1.3.1.cmml" xref="S5.SS4.p1.9.m9.1.1.3.1"></plus><apply id="S5.SS4.p1.9.m9.1.1.3.2.cmml" xref="S5.SS4.p1.9.m9.1.1.3.2"><csymbol cd="ambiguous" id="S5.SS4.p1.9.m9.1.1.3.2.1.cmml" xref="S5.SS4.p1.9.m9.1.1.3.2">subscript</csymbol><ci id="S5.SS4.p1.9.m9.1.1.3.2.2.cmml" xref="S5.SS4.p1.9.m9.1.1.3.2.2">𝐿</ci><ci id="S5.SS4.p1.9.m9.1.1.3.2.3a.cmml" xref="S5.SS4.p1.9.m9.1.1.3.2.3"><mtext mathsize="70%" id="S5.SS4.p1.9.m9.1.1.3.2.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3.2.3">detection</mtext></ci></apply><apply id="S5.SS4.p1.9.m9.1.1.3.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3.3"><times id="S5.SS4.p1.9.m9.1.1.3.3.1.cmml" xref="S5.SS4.p1.9.m9.1.1.3.3.1"></times><cn type="float" id="S5.SS4.p1.9.m9.1.1.3.3.2.cmml" xref="S5.SS4.p1.9.m9.1.1.3.3.2">0.5</cn><apply id="S5.SS4.p1.9.m9.1.1.3.3.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3.3.3"><csymbol cd="ambiguous" id="S5.SS4.p1.9.m9.1.1.3.3.3.1.cmml" xref="S5.SS4.p1.9.m9.1.1.3.3.3">subscript</csymbol><ci id="S5.SS4.p1.9.m9.1.1.3.3.3.2.cmml" xref="S5.SS4.p1.9.m9.1.1.3.3.3.2">𝐿</ci><ci id="S5.SS4.p1.9.m9.1.1.3.3.3.3a.cmml" xref="S5.SS4.p1.9.m9.1.1.3.3.3.3"><mtext mathsize="70%" id="S5.SS4.p1.9.m9.1.1.3.3.3.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3.3.3.3">QA</mtext></ci></apply></apply><apply id="S5.SS4.p1.9.m9.1.1.3.4.cmml" xref="S5.SS4.p1.9.m9.1.1.3.4"><times id="S5.SS4.p1.9.m9.1.1.3.4.1.cmml" xref="S5.SS4.p1.9.m9.1.1.3.4.1"></times><cn type="float" id="S5.SS4.p1.9.m9.1.1.3.4.2.cmml" xref="S5.SS4.p1.9.m9.1.1.3.4.2">0.5</cn><apply id="S5.SS4.p1.9.m9.1.1.3.4.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3.4.3"><csymbol cd="ambiguous" id="S5.SS4.p1.9.m9.1.1.3.4.3.1.cmml" xref="S5.SS4.p1.9.m9.1.1.3.4.3">subscript</csymbol><ci id="S5.SS4.p1.9.m9.1.1.3.4.3.2.cmml" xref="S5.SS4.p1.9.m9.1.1.3.4.3.2">𝐿</ci><ci id="S5.SS4.p1.9.m9.1.1.3.4.3.3a.cmml" xref="S5.SS4.p1.9.m9.1.1.3.4.3.3"><mtext mathsize="70%" id="S5.SS4.p1.9.m9.1.1.3.4.3.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3.4.3.3">type</mtext></ci></apply></apply><apply id="S5.SS4.p1.9.m9.1.1.3.5.cmml" xref="S5.SS4.p1.9.m9.1.1.3.5"><times id="S5.SS4.p1.9.m9.1.1.3.5.1.cmml" xref="S5.SS4.p1.9.m9.1.1.3.5.1"></times><cn type="float" id="S5.SS4.p1.9.m9.1.1.3.5.2.cmml" xref="S5.SS4.p1.9.m9.1.1.3.5.2">0.5</cn><apply id="S5.SS4.p1.9.m9.1.1.3.5.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3.5.3"><csymbol cd="ambiguous" id="S5.SS4.p1.9.m9.1.1.3.5.3.1.cmml" xref="S5.SS4.p1.9.m9.1.1.3.5.3">subscript</csymbol><ci id="S5.SS4.p1.9.m9.1.1.3.5.3.2.cmml" xref="S5.SS4.p1.9.m9.1.1.3.5.3.2">𝐿</ci><ci id="S5.SS4.p1.9.m9.1.1.3.5.3.3a.cmml" xref="S5.SS4.p1.9.m9.1.1.3.5.3.3"><mtext mathsize="70%" id="S5.SS4.p1.9.m9.1.1.3.5.3.3.cmml" xref="S5.SS4.p1.9.m9.1.1.3.5.3.3">sem-cls</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p1.9.m9.1c">L_{\text{All}}=L_{\text{detection}}+0.5L_{\text{QA}}+0.5L_{\text{type}}+0.5L_{\text{sem-cls}}</annotation></semantics></math>.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>
Comparison of the 3D visual question answering (VQA) and grounding results from different methods on the FE-3DGQA dataset. [*]: Results based on our re-implementation. In ours(manual+Ext.), we combine the FE-3DGQA and Ext. FE-3DGQA as the training dataset.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<td id="S5.T3.1.1.1.1" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.1.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="5">3D VQA Results (Acc)</td>
<td id="S5.T3.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t" colspan="4">Grounding Results (AP@0.5)</td>
</tr>
<tr id="S5.T3.1.2.2" class="ltx_tr">
<td id="S5.T3.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r">Methods</td>
<td id="S5.T3.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r">Input Modality</td>
<td id="S5.T3.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">number</td>
<td id="S5.T3.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">color</td>
<td id="S5.T3.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">yes/no</td>
<td id="S5.T3.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">other</td>
<td id="S5.T3.1.2.2.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">overall</td>
<td id="S5.T3.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t">AG-In-Q</td>
<td id="S5.T3.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t">AG-NotIn-Q</td>
<td id="S5.T3.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t">Context-Of-AG</td>
<td id="S5.T3.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t">mean</td>
</tr>
<tr id="S5.T3.1.3.3" class="ltx_tr">
<td id="S5.T3.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">QA w/o Scene</td>
<td id="S5.T3.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">-</td>
<td id="S5.T3.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">29.21</td>
<td id="S5.T3.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">32.31</td>
<td id="S5.T3.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">55.65</td>
<td id="S5.T3.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">17.59</td>
<td id="S5.T3.1.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">37.25</td>
<td id="S5.T3.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.4.4" class="ltx_tr">
<td id="S5.T3.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Multiview+MCAN</td>
<td id="S5.T3.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2D</td>
<td id="S5.T3.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">35.08</td>
<td id="S5.T3.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">44.42</td>
<td id="S5.T3.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">59.73</td>
<td id="S5.T3.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">28.99</td>
<td id="S5.T3.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.50</td>
<td id="S5.T3.1.4.4.8" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.4.4.10" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T3.1.4.4.11" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T3.1.5.5" class="ltx_tr">
<td id="S5.T3.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r">Multiview+T5&amp;Co-Attn</td>
<td id="S5.T3.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r">2D</td>
<td id="S5.T3.1.5.5.3" class="ltx_td ltx_align_center">35.07</td>
<td id="S5.T3.1.5.5.4" class="ltx_td ltx_align_center">41.25</td>
<td id="S5.T3.1.5.5.5" class="ltx_td ltx_align_center">64.44</td>
<td id="S5.T3.1.5.5.6" class="ltx_td ltx_align_center">26.81</td>
<td id="S5.T3.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r">45.16</td>
<td id="S5.T3.1.5.5.8" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.5.5.9" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.5.5.10" class="ltx_td ltx_align_center">-</td>
<td id="S5.T3.1.5.5.11" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T3.1.6.6" class="ltx_tr">
<td id="S5.T3.1.6.6.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">ScanQA* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</td>
<td id="S5.T3.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3D</td>
<td id="S5.T3.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">40.30</td>
<td id="S5.T3.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">35.47</td>
<td id="S5.T3.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">62.30</td>
<td id="S5.T3.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">29.64</td>
<td id="S5.T3.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">44.93</td>
<td id="S5.T3.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">20.92</td>
<td id="S5.T3.1.6.6.9" class="ltx_td ltx_align_center ltx_border_t">4.66</td>
<td id="S5.T3.1.6.6.10" class="ltx_td ltx_align_center ltx_border_t">7.26</td>
<td id="S5.T3.1.6.6.11" class="ltx_td ltx_align_center ltx_border_t">10.95</td>
</tr>
<tr id="S5.T3.1.7.7" class="ltx_tr">
<td id="S5.T3.1.7.7.1" class="ltx_td ltx_align_center ltx_border_r">3DQA-TR* <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>
</td>
<td id="S5.T3.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">3D</td>
<td id="S5.T3.1.7.7.3" class="ltx_td ltx_align_center">34.48</td>
<td id="S5.T3.1.7.7.4" class="ltx_td ltx_align_center">36.27</td>
<td id="S5.T3.1.7.7.5" class="ltx_td ltx_align_center">66.98</td>
<td id="S5.T3.1.7.7.6" class="ltx_td ltx_align_center">29.39</td>
<td id="S5.T3.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r">45.95</td>
<td id="S5.T3.1.7.7.8" class="ltx_td ltx_align_center">13.71</td>
<td id="S5.T3.1.7.7.9" class="ltx_td ltx_align_center">5.10</td>
<td id="S5.T3.1.7.7.10" class="ltx_td ltx_align_center">10.23</td>
<td id="S5.T3.1.7.7.11" class="ltx_td ltx_align_center">9.68</td>
</tr>
<tr id="S5.T3.1.8.8" class="ltx_tr">
<td id="S5.T3.1.8.8.1" class="ltx_td ltx_align_center ltx_border_r">Ours</td>
<td id="S5.T3.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r">3D</td>
<td id="S5.T3.1.8.8.3" class="ltx_td ltx_align_center">34.90</td>
<td id="S5.T3.1.8.8.4" class="ltx_td ltx_align_center">40.17</td>
<td id="S5.T3.1.8.8.5" class="ltx_td ltx_align_center">68.27</td>
<td id="S5.T3.1.8.8.6" class="ltx_td ltx_align_center">31.08</td>
<td id="S5.T3.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r">47.88</td>
<td id="S5.T3.1.8.8.8" class="ltx_td ltx_align_center">28.20</td>
<td id="S5.T3.1.8.8.9" class="ltx_td ltx_align_center">11.38</td>
<td id="S5.T3.1.8.8.10" class="ltx_td ltx_align_center">19.72</td>
<td id="S5.T3.1.8.8.11" class="ltx_td ltx_align_center">19.77</td>
</tr>
<tr id="S5.T3.1.9.9" class="ltx_tr">
<td id="S5.T3.1.9.9.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Ours (manual+Ext.)</td>
<td id="S5.T3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3D</td>
<td id="S5.T3.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t">37.59</td>
<td id="S5.T3.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">44.70</td>
<td id="S5.T3.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t">67.42</td>
<td id="S5.T3.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t">32.98</td>
<td id="S5.T3.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">49.11</td>
<td id="S5.T3.1.9.9.8" class="ltx_td ltx_align_center ltx_border_t">34.97</td>
<td id="S5.T3.1.9.9.9" class="ltx_td ltx_align_center ltx_border_t">15.41</td>
<td id="S5.T3.1.9.9.10" class="ltx_td ltx_align_center ltx_border_t">25.44</td>
<td id="S5.T3.1.9.9.11" class="ltx_td ltx_align_center ltx_border_t">25.28</td>
</tr>
<tr id="S5.T3.1.10.10" class="ltx_tr">
<td id="S5.T3.1.10.10.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Human</td>
<td id="S5.T3.1.10.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">3D</td>
<td id="S5.T3.1.10.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">85.25</td>
<td id="S5.T3.1.10.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">70.15</td>
<td id="S5.T3.1.10.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">90.26</td>
<td id="S5.T3.1.10.10.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">50.85</td>
<td id="S5.T3.1.10.10.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">74.13</td>
<td id="S5.T3.1.10.10.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">81.94</td>
<td id="S5.T3.1.10.10.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">53.10</td>
<td id="S5.T3.1.10.10.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">52.87</td>
<td id="S5.T3.1.10.10.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">62.34</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>
Comparison of the results from different methods on validation set of the ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> dataset.</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t"></th>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t">EM@1</td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t">EM@10</td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">Acc@0.25</td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">Acc@0.5</td>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">
<table id="S5.T4.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.2.2.1.1.1" class="ltx_tr">
<td id="S5.T4.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">RandomImage</td>
</tr>
<tr id="S5.T4.1.2.2.1.1.2" class="ltx_tr">
<td id="S5.T4.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">+Oscar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> (real)</td>
</tr>
</table>
</th>
<td id="S5.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">19.38</td>
<td id="S5.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t">46.37</td>
<td id="S5.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t">-</td>
<td id="S5.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">-</td>
</tr>
<tr id="S5.T4.1.3.3" class="ltx_tr">
<th id="S5.T4.1.3.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">
<table id="S5.T4.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T4.1.3.3.1.1.1" class="ltx_tr">
<td id="S5.T4.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">TopDownImage</td>
</tr>
<tr id="S5.T4.1.3.3.1.1.2" class="ltx_tr">
<td id="S5.T4.1.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">+Oscar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>
</td>
</tr>
</table>
</th>
<td id="S5.T4.1.3.3.2" class="ltx_td ltx_align_center">17.20</td>
<td id="S5.T4.1.3.3.3" class="ltx_td ltx_align_center">43.81</td>
<td id="S5.T4.1.3.3.4" class="ltx_td ltx_align_center">-</td>
<td id="S5.T4.1.3.3.5" class="ltx_td ltx_align_center">-</td>
</tr>
<tr id="S5.T4.1.4.4" class="ltx_tr">
<th id="S5.T4.1.4.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">ScanRefer+MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
</th>
<td id="S5.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_t">18.59</td>
<td id="S5.T4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_t">46.76</td>
<td id="S5.T4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_t">23.53</td>
<td id="S5.T4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">11.76</td>
</tr>
<tr id="S5.T4.1.5.5" class="ltx_tr">
<th id="S5.T4.1.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<td id="S5.T4.1.5.5.2" class="ltx_td ltx_align_center">21.05</td>
<td id="S5.T4.1.5.5.3" class="ltx_td ltx_align_center">51.23</td>
<td id="S5.T4.1.5.5.4" class="ltx_td ltx_align_center">24.96</td>
<td id="S5.T4.1.5.5.5" class="ltx_td ltx_align_center">15.42</td>
</tr>
<tr id="S5.T4.1.6.6" class="ltx_tr">
<th id="S5.T4.1.6.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Ours</th>
<td id="S5.T4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b">22.26</td>
<td id="S5.T4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b">54.51</td>
<td id="S5.T4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b">26.62</td>
<td id="S5.T4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b">18.83</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span id="S6.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">To evaluate the newly proposed method, we conduct the experiments based on our newly collected FE-3DGQA dataset (as well as its extended version).</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS1.4.1.1" class="ltx_text">VI-A</span> </span><span id="S6.SS1.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">3D Grounded Question Answering Results.</span>
</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">In Table <a href="#S5.T3" title="TABLE III ‣ V-D Loss Function ‣ V Methodology ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, we report the completely grounded 3D visual question answering (VQA) results including not only the QA results, but also the grounding results from three different types of objects. According to different types of answers, we split the validation set into 4 sub-classes: “number”, “color”, “yes/no”, and “other”.
Since most recent methods and datasets are not open-sourced yet, we compare the results of our method with few state-of-the-art methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> based on our implementation.
For fair comparison, we also add a grounding head to their methods and also report the grounding results of the three types of objects.
Note that we do not report the results of TransVQA3D <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> since this baseline method requires additional relation graph annotations.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.3" class="ltx_p">In general, all the 3D VQA methods outperform the pure QA results without the visual data (<em id="S6.SS1.p2.3.1" class="ltx_emph ltx_font_italic">i.e</em>., the results from random guess or the language priors), and our method achieves the best overall QA results, as well as the grounding results for all types of objects.
Specifically, our method achieves around <math id="S6.SS1.p2.1.m1.1" class="ltx_Math" alttext="1.93" display="inline"><semantics id="S6.SS1.p2.1.m1.1a"><mn id="S6.SS1.p2.1.m1.1.1" xref="S6.SS1.p2.1.m1.1.1.cmml">1.93</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.1.m1.1b"><cn type="float" id="S6.SS1.p2.1.m1.1.1.cmml" xref="S6.SS1.p2.1.m1.1.1">1.93</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.1.m1.1c">1.93</annotation></semantics></math>% improvements in the “overall” QA results when compared with the recent 3DQA-TR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> method on the validation set.
For the grounding results, our method achieves remarkable performance gain of <math id="S6.SS1.p2.2.m2.1" class="ltx_Math" alttext="8.82" display="inline"><semantics id="S6.SS1.p2.2.m2.1a"><mn id="S6.SS1.p2.2.m2.1.1" xref="S6.SS1.p2.2.m2.1.1.cmml">8.82</mn><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.2.m2.1b"><cn type="float" id="S6.SS1.p2.2.m2.1.1.cmml" xref="S6.SS1.p2.2.m2.1.1">8.82</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.2.m2.1c">8.82</annotation></semantics></math>% in terms of the mAP<math id="S6.SS1.p2.3.m3.1" class="ltx_Math" alttext="@" display="inline"><semantics id="S6.SS1.p2.3.m3.1a"><mi mathvariant="normal" id="S6.SS1.p2.3.m3.1.1" xref="S6.SS1.p2.3.m3.1.1.cmml">@</mi><annotation-xml encoding="MathML-Content" id="S6.SS1.p2.3.m3.1b"><ci id="S6.SS1.p2.3.m3.1.1.cmml" xref="S6.SS1.p2.3.m3.1.1">@</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p2.3.m3.1c">@</annotation></semantics></math>0.5 when compared with the ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> method.
<span id="S6.SS1.p2.3.2" class="ltx_text">We also compare our method with the methods using multiview 2D images with various vision-language fusion methods (<em id="S6.SS1.p2.3.2.1" class="ltx_emph ltx_font_italic">i.e</em>., MCAN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite> or pretrained T5 + co-attention). We found that the method using multiview images performs better for the “color” subset, but worse for the “yes/no” and “other” subsets, which requires 3D scene understanding.<span id="S6.SS1.p2.3.2.2" class="ltx_text">
The results show that our newly proposed method could effectively ground to the answer related objects and thus predict more accurate answers.
For the baseline methods ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and 3DQA-TR from 3DQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, the grounding results are poor, which indicate that their methods could not ground to the answer related objects precisely and thus are not well explainable.
We also conduct the experiments by combining the manually annotated dataset (<em id="S6.SS1.p2.3.2.2.1" class="ltx_emph ltx_font_italic">i.e</em>., FE-3DGQA) and the extended dataset (<em id="S6.SS1.p2.3.2.2.2" class="ltx_emph ltx_font_italic">i.e</em>., Ext. FE-3DGQA) in the training stage to further enhance our performance (see the 5th row in Table. <a href="#S5.T3" title="TABLE III ‣ V-D Loss Function ‣ V Methodology ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) for the 3D VQA task. We examine human performance by asking 4 human subjects to answer 400 randomly selected questions and identify the three types of related objects in the 3D scene. The average results are shown in the last row of Table. <a href="#S5.T3" title="TABLE III ‣ V-D Loss Function ‣ V Methodology ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>. It can be seen that the performance gaps between all the algorithms and human are still large, which allows further exploitation of the 3D GQA task by using our dataset.
As shown in Table. <a href="#S5.T4" title="TABLE IV ‣ V-D Loss Function ‣ V Methodology ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">IV</span></a>, we also report the results of our methods on the ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> dataset.
Results show that our method outperforms the baseline methods.</span></span></p>
</div>
<div id="S6.SS1.p3" class="ltx_para ltx_noindent">
<p id="S6.SS1.p3.1" class="ltx_p"><span id="S6.SS1.p3.1.1" class="ltx_text ltx_font_bold">Training Details.</span>
The model is trained in an end-to-end manner on a machine with a single 32G V100 GPU. For the FE-3DGQA dataset, we train our model for 100 epochs with 15 GPU hours.
When we use the large Extended FE-3DGQA dataset, we train our model for 50 epochs with about 60 GPU hours. We set the learning rate as 1e-3 for the VoteNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> detector, and 1e-4 for other transformer-based modules.</p>
</div>
<figure id="S6.T5" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE V: </span>3D Grounded VQA results of our alternative methods when using different vision modules, language modules, and losses. “Enhanced SA” <em id="S6.T5.11.1" class="ltx_emph ltx_font_italic">vs</em>.“Standard SA” means whether we use the enhanced or the standard self-attention module in the vision branch. “Bi-GRU” <em id="S6.T5.12.2" class="ltx_emph ltx_font_italic">vs</em>.“T5” means whether we use a pretrained T5 module or a randomly initialized Bi-GRU module for language modeling. “<math id="S6.T5.3.m1.1" class="ltx_Math" alttext="L_{\text{All}}" display="inline"><semantics id="S6.T5.3.m1.1b"><msub id="S6.T5.3.m1.1.1" xref="S6.T5.3.m1.1.1.cmml"><mi mathcolor="#000000" id="S6.T5.3.m1.1.1.2" xref="S6.T5.3.m1.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S6.T5.3.m1.1.1.3" xref="S6.T5.3.m1.1.1.3a.cmml">All</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T5.3.m1.1c"><apply id="S6.T5.3.m1.1.1.cmml" xref="S6.T5.3.m1.1.1"><csymbol cd="ambiguous" id="S6.T5.3.m1.1.1.1.cmml" xref="S6.T5.3.m1.1.1">subscript</csymbol><ci id="S6.T5.3.m1.1.1.2.cmml" xref="S6.T5.3.m1.1.1.2">𝐿</ci><ci id="S6.T5.3.m1.1.1.3a.cmml" xref="S6.T5.3.m1.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S6.T5.3.m1.1.1.3.cmml" xref="S6.T5.3.m1.1.1.3">All</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.3.m1.1d">L_{\text{All}}</annotation></semantics></math>” and “<math id="S6.T5.4.m2.1" class="ltx_Math" alttext="L_{\text{detection}}+0.5L_{\text{QA}}" display="inline"><semantics id="S6.T5.4.m2.1b"><mrow id="S6.T5.4.m2.1.1" xref="S6.T5.4.m2.1.1.cmml"><msub id="S6.T5.4.m2.1.1.2" xref="S6.T5.4.m2.1.1.2.cmml"><mi mathcolor="#000000" id="S6.T5.4.m2.1.1.2.2" xref="S6.T5.4.m2.1.1.2.2.cmml">L</mi><mtext id="S6.T5.4.m2.1.1.2.3" xref="S6.T5.4.m2.1.1.2.3a.cmml">detection</mtext></msub><mo id="S6.T5.4.m2.1.1.1" xref="S6.T5.4.m2.1.1.1.cmml">+</mo><mrow id="S6.T5.4.m2.1.1.3" xref="S6.T5.4.m2.1.1.3.cmml"><mn id="S6.T5.4.m2.1.1.3.2" xref="S6.T5.4.m2.1.1.3.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S6.T5.4.m2.1.1.3.1" xref="S6.T5.4.m2.1.1.3.1.cmml">​</mo><msub id="S6.T5.4.m2.1.1.3.3" xref="S6.T5.4.m2.1.1.3.3.cmml"><mi mathcolor="#000000" id="S6.T5.4.m2.1.1.3.3.2" xref="S6.T5.4.m2.1.1.3.3.2.cmml">L</mi><mtext id="S6.T5.4.m2.1.1.3.3.3" xref="S6.T5.4.m2.1.1.3.3.3a.cmml">QA</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.4.m2.1c"><apply id="S6.T5.4.m2.1.1.cmml" xref="S6.T5.4.m2.1.1"><plus id="S6.T5.4.m2.1.1.1.cmml" xref="S6.T5.4.m2.1.1.1"></plus><apply id="S6.T5.4.m2.1.1.2.cmml" xref="S6.T5.4.m2.1.1.2"><csymbol cd="ambiguous" id="S6.T5.4.m2.1.1.2.1.cmml" xref="S6.T5.4.m2.1.1.2">subscript</csymbol><ci id="S6.T5.4.m2.1.1.2.2.cmml" xref="S6.T5.4.m2.1.1.2.2">𝐿</ci><ci id="S6.T5.4.m2.1.1.2.3a.cmml" xref="S6.T5.4.m2.1.1.2.3"><mtext mathsize="70%" id="S6.T5.4.m2.1.1.2.3.cmml" xref="S6.T5.4.m2.1.1.2.3">detection</mtext></ci></apply><apply id="S6.T5.4.m2.1.1.3.cmml" xref="S6.T5.4.m2.1.1.3"><times id="S6.T5.4.m2.1.1.3.1.cmml" xref="S6.T5.4.m2.1.1.3.1"></times><cn type="float" id="S6.T5.4.m2.1.1.3.2.cmml" xref="S6.T5.4.m2.1.1.3.2">0.5</cn><apply id="S6.T5.4.m2.1.1.3.3.cmml" xref="S6.T5.4.m2.1.1.3.3"><csymbol cd="ambiguous" id="S6.T5.4.m2.1.1.3.3.1.cmml" xref="S6.T5.4.m2.1.1.3.3">subscript</csymbol><ci id="S6.T5.4.m2.1.1.3.3.2.cmml" xref="S6.T5.4.m2.1.1.3.3.2">𝐿</ci><ci id="S6.T5.4.m2.1.1.3.3.3a.cmml" xref="S6.T5.4.m2.1.1.3.3.3"><mtext mathsize="70%" id="S6.T5.4.m2.1.1.3.3.3.cmml" xref="S6.T5.4.m2.1.1.3.3.3">QA</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.4.m2.1d">L_{\text{detection}}+0.5L_{\text{QA}}</annotation></semantics></math>” mean our overall joint training method and our alternative method without the grounding task.</figcaption>
<table id="S6.T5.8" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S6.T5.8.5.1" class="ltx_tr">
<th id="S6.T5.8.5.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Vision</th>
<th id="S6.T5.8.5.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Language</th>
<th id="S6.T5.8.5.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Loss</th>
<th id="S6.T5.8.5.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">VQA Acc</th>
<th id="S6.T5.8.5.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">mAP@0.5</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S6.T5.5.1" class="ltx_tr">
<td id="S6.T5.5.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Standard SA</td>
<td id="S6.T5.5.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">T5</td>
<td id="S6.T5.5.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><math id="S6.T5.5.1.1.m1.1" class="ltx_Math" alttext="L_{\text{All}}" display="inline"><semantics id="S6.T5.5.1.1.m1.1a"><msub id="S6.T5.5.1.1.m1.1.1" xref="S6.T5.5.1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S6.T5.5.1.1.m1.1.1.2" xref="S6.T5.5.1.1.m1.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S6.T5.5.1.1.m1.1.1.3" xref="S6.T5.5.1.1.m1.1.1.3a.cmml">All</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T5.5.1.1.m1.1b"><apply id="S6.T5.5.1.1.m1.1.1.cmml" xref="S6.T5.5.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T5.5.1.1.m1.1.1.1.cmml" xref="S6.T5.5.1.1.m1.1.1">subscript</csymbol><ci id="S6.T5.5.1.1.m1.1.1.2.cmml" xref="S6.T5.5.1.1.m1.1.1.2">𝐿</ci><ci id="S6.T5.5.1.1.m1.1.1.3a.cmml" xref="S6.T5.5.1.1.m1.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S6.T5.5.1.1.m1.1.1.3.cmml" xref="S6.T5.5.1.1.m1.1.1.3">All</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.5.1.1.m1.1c">L_{\text{All}}</annotation></semantics></math></td>
<td id="S6.T5.5.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">47.04</td>
<td id="S6.T5.5.1.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">19.03</td>
</tr>
<tr id="S6.T5.6.2" class="ltx_tr">
<td id="S6.T5.6.2.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">Enhanced SA</td>
<td id="S6.T5.6.2.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">Bi-GRU</td>
<td id="S6.T5.6.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><math id="S6.T5.6.2.1.m1.1" class="ltx_Math" alttext="L_{\text{All}}" display="inline"><semantics id="S6.T5.6.2.1.m1.1a"><msub id="S6.T5.6.2.1.m1.1.1" xref="S6.T5.6.2.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S6.T5.6.2.1.m1.1.1.2" xref="S6.T5.6.2.1.m1.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S6.T5.6.2.1.m1.1.1.3" xref="S6.T5.6.2.1.m1.1.1.3a.cmml">All</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T5.6.2.1.m1.1b"><apply id="S6.T5.6.2.1.m1.1.1.cmml" xref="S6.T5.6.2.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T5.6.2.1.m1.1.1.1.cmml" xref="S6.T5.6.2.1.m1.1.1">subscript</csymbol><ci id="S6.T5.6.2.1.m1.1.1.2.cmml" xref="S6.T5.6.2.1.m1.1.1.2">𝐿</ci><ci id="S6.T5.6.2.1.m1.1.1.3a.cmml" xref="S6.T5.6.2.1.m1.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S6.T5.6.2.1.m1.1.1.3.cmml" xref="S6.T5.6.2.1.m1.1.1.3">All</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.6.2.1.m1.1c">L_{\text{All}}</annotation></semantics></math></td>
<td id="S6.T5.6.2.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">44.99</td>
<td id="S6.T5.6.2.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">12.23</td>
</tr>
<tr id="S6.T5.7.3" class="ltx_tr">
<td id="S6.T5.7.3.2" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">Enhanced SA</td>
<td id="S6.T5.7.3.3" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">T5</td>
<td id="S6.T5.7.3.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><math id="S6.T5.7.3.1.m1.1" class="ltx_Math" alttext="L_{\text{detection}}+0.5L_{\text{QA}}" display="inline"><semantics id="S6.T5.7.3.1.m1.1a"><mrow id="S6.T5.7.3.1.m1.1.1" xref="S6.T5.7.3.1.m1.1.1.cmml"><msub id="S6.T5.7.3.1.m1.1.1.2" xref="S6.T5.7.3.1.m1.1.1.2.cmml"><mi mathcolor="#000000" id="S6.T5.7.3.1.m1.1.1.2.2" xref="S6.T5.7.3.1.m1.1.1.2.2.cmml">L</mi><mtext id="S6.T5.7.3.1.m1.1.1.2.3" xref="S6.T5.7.3.1.m1.1.1.2.3a.cmml">detection</mtext></msub><mo id="S6.T5.7.3.1.m1.1.1.1" xref="S6.T5.7.3.1.m1.1.1.1.cmml">+</mo><mrow id="S6.T5.7.3.1.m1.1.1.3" xref="S6.T5.7.3.1.m1.1.1.3.cmml"><mn id="S6.T5.7.3.1.m1.1.1.3.2" xref="S6.T5.7.3.1.m1.1.1.3.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S6.T5.7.3.1.m1.1.1.3.1" xref="S6.T5.7.3.1.m1.1.1.3.1.cmml">​</mo><msub id="S6.T5.7.3.1.m1.1.1.3.3" xref="S6.T5.7.3.1.m1.1.1.3.3.cmml"><mi mathcolor="#000000" id="S6.T5.7.3.1.m1.1.1.3.3.2" xref="S6.T5.7.3.1.m1.1.1.3.3.2.cmml">L</mi><mtext id="S6.T5.7.3.1.m1.1.1.3.3.3" xref="S6.T5.7.3.1.m1.1.1.3.3.3a.cmml">QA</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.T5.7.3.1.m1.1b"><apply id="S6.T5.7.3.1.m1.1.1.cmml" xref="S6.T5.7.3.1.m1.1.1"><plus id="S6.T5.7.3.1.m1.1.1.1.cmml" xref="S6.T5.7.3.1.m1.1.1.1"></plus><apply id="S6.T5.7.3.1.m1.1.1.2.cmml" xref="S6.T5.7.3.1.m1.1.1.2"><csymbol cd="ambiguous" id="S6.T5.7.3.1.m1.1.1.2.1.cmml" xref="S6.T5.7.3.1.m1.1.1.2">subscript</csymbol><ci id="S6.T5.7.3.1.m1.1.1.2.2.cmml" xref="S6.T5.7.3.1.m1.1.1.2.2">𝐿</ci><ci id="S6.T5.7.3.1.m1.1.1.2.3a.cmml" xref="S6.T5.7.3.1.m1.1.1.2.3"><mtext mathsize="70%" id="S6.T5.7.3.1.m1.1.1.2.3.cmml" xref="S6.T5.7.3.1.m1.1.1.2.3">detection</mtext></ci></apply><apply id="S6.T5.7.3.1.m1.1.1.3.cmml" xref="S6.T5.7.3.1.m1.1.1.3"><times id="S6.T5.7.3.1.m1.1.1.3.1.cmml" xref="S6.T5.7.3.1.m1.1.1.3.1"></times><cn type="float" id="S6.T5.7.3.1.m1.1.1.3.2.cmml" xref="S6.T5.7.3.1.m1.1.1.3.2">0.5</cn><apply id="S6.T5.7.3.1.m1.1.1.3.3.cmml" xref="S6.T5.7.3.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S6.T5.7.3.1.m1.1.1.3.3.1.cmml" xref="S6.T5.7.3.1.m1.1.1.3.3">subscript</csymbol><ci id="S6.T5.7.3.1.m1.1.1.3.3.2.cmml" xref="S6.T5.7.3.1.m1.1.1.3.3.2">𝐿</ci><ci id="S6.T5.7.3.1.m1.1.1.3.3.3a.cmml" xref="S6.T5.7.3.1.m1.1.1.3.3.3"><mtext mathsize="70%" id="S6.T5.7.3.1.m1.1.1.3.3.3.cmml" xref="S6.T5.7.3.1.m1.1.1.3.3.3">QA</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.7.3.1.m1.1c">L_{\text{detection}}+0.5L_{\text{QA}}</annotation></semantics></math></td>
<td id="S6.T5.7.3.4" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;">45.99</td>
<td id="S6.T5.7.3.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;">-</td>
</tr>
<tr id="S6.T5.8.4" class="ltx_tr">
<td id="S6.T5.8.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">Enhanced SA</td>
<td id="S6.T5.8.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">T5</td>
<td id="S6.T5.8.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><math id="S6.T5.8.4.1.m1.1" class="ltx_Math" alttext="L_{\text{All}}" display="inline"><semantics id="S6.T5.8.4.1.m1.1a"><msub id="S6.T5.8.4.1.m1.1.1" xref="S6.T5.8.4.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S6.T5.8.4.1.m1.1.1.2" xref="S6.T5.8.4.1.m1.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S6.T5.8.4.1.m1.1.1.3" xref="S6.T5.8.4.1.m1.1.1.3a.cmml">All</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.T5.8.4.1.m1.1b"><apply id="S6.T5.8.4.1.m1.1.1.cmml" xref="S6.T5.8.4.1.m1.1.1"><csymbol cd="ambiguous" id="S6.T5.8.4.1.m1.1.1.1.cmml" xref="S6.T5.8.4.1.m1.1.1">subscript</csymbol><ci id="S6.T5.8.4.1.m1.1.1.2.cmml" xref="S6.T5.8.4.1.m1.1.1.2">𝐿</ci><ci id="S6.T5.8.4.1.m1.1.1.3a.cmml" xref="S6.T5.8.4.1.m1.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S6.T5.8.4.1.m1.1.1.3.cmml" xref="S6.T5.8.4.1.m1.1.1.3">All</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.8.4.1.m1.1c">L_{\text{All}}</annotation></semantics></math></td>
<td id="S6.T5.8.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">47.88</td>
<td id="S6.T5.8.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;">19.77</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS2.4.1.1" class="ltx_text">VI-B</span> </span><span id="S6.SS2.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Ablation Studies and Analysis</span>
</h3>

<figure id="S6.F9" class="ltx_figure"><img src="/html/2209.12028/assets/x9.png" id="S6.F9.g1" class="ltx_graphics ltx_centering ltx_img_square" width="424" height="426" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Visualization of the results from our method and other baseline methods on the newly proposed FE-3DGQA dataset. The related objects of AG-In-Q, AG-NotIn-Q, and Context-Of-AG are colored in green, cyan, and red, respectively.</figcaption>
</figure>
<figure id="S6.F10" class="ltx_figure"><img src="/html/2209.12028/assets/x10.png" id="S6.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="433" height="473" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>More visualization results from our method on the newly collected FE-3DGQA dataset.</figcaption>
</figure>
<figure id="S6.T6" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>Grounded question answering results of our newly proposed method when using different training datasets.</figcaption>
<table id="S6.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T6.1.1.1" class="ltx_tr">
<td id="S6.T6.1.1.1.1" class="ltx_td ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"></td>
<td id="S6.T6.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="5"><span id="S6.T6.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Question Answering Results (Acc)</span></td>
<td id="S6.T6.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;" colspan="4"><span id="S6.T6.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Grounding Results (AP@0.5)</span></td>
</tr>
<tr id="S6.T6.1.2.2" class="ltx_tr">
<td id="S6.T6.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.1.1" class="ltx_text" style="font-size:90%;">Training Dataset</span></td>
<td id="S6.T6.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.2.1" class="ltx_text" style="font-size:90%;">number</span></td>
<td id="S6.T6.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.3.1" class="ltx_text" style="font-size:90%;">color</span></td>
<td id="S6.T6.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.4.1" class="ltx_text" style="font-size:90%;">yes/no</span></td>
<td id="S6.T6.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.5.1" class="ltx_text" style="font-size:90%;">other</span></td>
<td id="S6.T6.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.6.1" class="ltx_text" style="font-size:90%;">overall</span></td>
<td id="S6.T6.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.7.1" class="ltx_text" style="font-size:90%;">AG-In-Q</span></td>
<td id="S6.T6.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.8.1" class="ltx_text" style="font-size:90%;">AG-NotIn-Q</span></td>
<td id="S6.T6.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.9.1" class="ltx_text" style="font-size:90%;">Context-Of-AG</span></td>
<td id="S6.T6.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.2.2.10.1" class="ltx_text" style="font-size:90%;">mean</span></td>
</tr>
<tr id="S6.T6.1.3.3" class="ltx_tr">
<td id="S6.T6.1.3.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.1.1" class="ltx_text" style="font-size:90%;">manual</span></td>
<td id="S6.T6.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.2.1" class="ltx_text" style="font-size:90%;">34.90</span></td>
<td id="S6.T6.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.3.1" class="ltx_text" style="font-size:90%;">40.17</span></td>
<td id="S6.T6.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.4.1" class="ltx_text" style="font-size:90%;">68.27</span></td>
<td id="S6.T6.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.5.1" class="ltx_text" style="font-size:90%;">31.08</span></td>
<td id="S6.T6.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.6.1" class="ltx_text" style="font-size:90%;">47.88</span></td>
<td id="S6.T6.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.7.1" class="ltx_text" style="font-size:90%;">28.20</span></td>
<td id="S6.T6.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.8.1" class="ltx_text" style="font-size:90%;">11.38</span></td>
<td id="S6.T6.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.9.1" class="ltx_text" style="font-size:90%;">19.72</span></td>
<td id="S6.T6.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.3.3.10.1" class="ltx_text" style="font-size:90%;">19.77</span></td>
</tr>
<tr id="S6.T6.1.4.4" class="ltx_tr">
<td id="S6.T6.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.1.1" class="ltx_text" style="font-size:90%;">manual+det.</span></td>
<td id="S6.T6.1.4.4.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.2.1" class="ltx_text" style="font-size:90%;">35.80</span></td>
<td id="S6.T6.1.4.4.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.3.1" class="ltx_text" style="font-size:90%;">40.04</span></td>
<td id="S6.T6.1.4.4.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">69.08</span></td>
<td id="S6.T6.1.4.4.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.5.1" class="ltx_text" style="font-size:90%;">30.10</span></td>
<td id="S6.T6.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.6.1" class="ltx_text" style="font-size:90%;">48.12</span></td>
<td id="S6.T6.1.4.4.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.7.1" class="ltx_text" style="font-size:90%;">31.50</span></td>
<td id="S6.T6.1.4.4.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.8.1" class="ltx_text" style="font-size:90%;">12.12</span></td>
<td id="S6.T6.1.4.4.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.9.1" class="ltx_text" style="font-size:90%;">21.61</span></td>
<td id="S6.T6.1.4.4.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.4.4.10.1" class="ltx_text" style="font-size:90%;">21.74</span></td>
</tr>
<tr id="S6.T6.1.5.5" class="ltx_tr">
<td id="S6.T6.1.5.5.1" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.1.1" class="ltx_text" style="font-size:90%;">manual+det.+ground.</span></td>
<td id="S6.T6.1.5.5.2" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.2.1" class="ltx_text" style="font-size:90%;">36.60</span></td>
<td id="S6.T6.1.5.5.3" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.3.1" class="ltx_text" style="font-size:90%;">42.15</span></td>
<td id="S6.T6.1.5.5.4" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.4.1" class="ltx_text" style="font-size:90%;">67.35</span></td>
<td id="S6.T6.1.5.5.5" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">33.30</span></td>
<td id="S6.T6.1.5.5.6" class="ltx_td ltx_align_center ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.6.1" class="ltx_text" style="font-size:90%;">48.78</span></td>
<td id="S6.T6.1.5.5.7" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">35.04</span></td>
<td id="S6.T6.1.5.5.8" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.8.1" class="ltx_text" style="font-size:90%;">14.34</span></td>
<td id="S6.T6.1.5.5.9" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.44</span></td>
<td id="S6.T6.1.5.5.10" class="ltx_td ltx_align_center" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.5.5.10.1" class="ltx_text" style="font-size:90%;">24.94</span></td>
</tr>
<tr id="S6.T6.1.6.6" class="ltx_tr">
<td id="S6.T6.1.6.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.1.1" class="ltx_text" style="font-size:90%;">manual+Ext.</span></td>
<td id="S6.T6.1.6.6.2" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">37.59</span></td>
<td id="S6.T6.1.6.6.3" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">44.70</span></td>
<td id="S6.T6.1.6.6.4" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.4.1" class="ltx_text" style="font-size:90%;">67.42</span></td>
<td id="S6.T6.1.6.6.5" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.5.1" class="ltx_text" style="font-size:90%;">32.98</span></td>
<td id="S6.T6.1.6.6.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">49.11</span></td>
<td id="S6.T6.1.6.6.7" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.7.1" class="ltx_text" style="font-size:90%;">34.97</span></td>
<td id="S6.T6.1.6.6.8" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">15.41</span></td>
<td id="S6.T6.1.6.6.9" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.44</span></td>
<td id="S6.T6.1.6.6.10" class="ltx_td ltx_align_center ltx_border_b" style="padding-left:3.0pt;padding-right:3.0pt;"><span id="S6.T6.1.6.6.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">25.28</span></td>
</tr>
</tbody>
</table>
</figure>
<section id="S6.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS2.SSS1.4.1.1" class="ltx_text">VI-B</span>1 </span>How does the auxiliary grounding task affect the QA results?</h4>

<div id="S6.SS2.SSS1.p1" class="ltx_para">
<p id="S6.SS2.SSS1.p1.4" class="ltx_p">We argue that the visually grounded results are more reliable and explainable. Here, we compare the QA results between our alternative method
called “Ours (w/o Grounding)”
without considering the grounding task (<em id="S6.SS2.SSS1.p1.4.1" class="ltx_emph ltx_font_italic">i.e</em>., “ <math id="S6.SS2.SSS1.p1.1.m1.1" class="ltx_Math" alttext="L_{\text{detection}}+0.5L_{\text{QA}}" display="inline"><semantics id="S6.SS2.SSS1.p1.1.m1.1a"><mrow id="S6.SS2.SSS1.p1.1.m1.1.1" xref="S6.SS2.SSS1.p1.1.m1.1.1.cmml"><msub id="S6.SS2.SSS1.p1.1.m1.1.1.2" xref="S6.SS2.SSS1.p1.1.m1.1.1.2.cmml"><mi mathcolor="#000000" id="S6.SS2.SSS1.p1.1.m1.1.1.2.2" xref="S6.SS2.SSS1.p1.1.m1.1.1.2.2.cmml">L</mi><mtext id="S6.SS2.SSS1.p1.1.m1.1.1.2.3" xref="S6.SS2.SSS1.p1.1.m1.1.1.2.3a.cmml">detection</mtext></msub><mo id="S6.SS2.SSS1.p1.1.m1.1.1.1" xref="S6.SS2.SSS1.p1.1.m1.1.1.1.cmml">+</mo><mrow id="S6.SS2.SSS1.p1.1.m1.1.1.3" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.cmml"><mn id="S6.SS2.SSS1.p1.1.m1.1.1.3.2" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.2.cmml">0.5</mn><mo lspace="0em" rspace="0em" id="S6.SS2.SSS1.p1.1.m1.1.1.3.1" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.1.cmml">​</mo><msub id="S6.SS2.SSS1.p1.1.m1.1.1.3.3" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.3.cmml"><mi mathcolor="#000000" id="S6.SS2.SSS1.p1.1.m1.1.1.3.3.2" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml">L</mi><mtext id="S6.SS2.SSS1.p1.1.m1.1.1.3.3.3" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.3.3a.cmml">QA</mtext></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.1.m1.1b"><apply id="S6.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1"><plus id="S6.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.1"></plus><apply id="S6.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.2">subscript</csymbol><ci id="S6.SS2.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.2.2">𝐿</ci><ci id="S6.SS2.SSS1.p1.1.m1.1.1.2.3a.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.2.3"><mtext mathsize="70%" id="S6.SS2.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.2.3">detection</mtext></ci></apply><apply id="S6.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3"><times id="S6.SS2.SSS1.p1.1.m1.1.1.3.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.1"></times><cn type="float" id="S6.SS2.SSS1.p1.1.m1.1.1.3.2.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.2">0.5</cn><apply id="S6.SS2.SSS1.p1.1.m1.1.1.3.3.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p1.1.m1.1.1.3.3.1.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.3">subscript</csymbol><ci id="S6.SS2.SSS1.p1.1.m1.1.1.3.3.2.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.3.2">𝐿</ci><ci id="S6.SS2.SSS1.p1.1.m1.1.1.3.3.3a.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.3.3"><mtext mathsize="70%" id="S6.SS2.SSS1.p1.1.m1.1.1.3.3.3.cmml" xref="S6.SS2.SSS1.p1.1.m1.1.1.3.3.3">QA</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.1.m1.1c">L_{\text{detection}}+0.5L_{\text{QA}}</annotation></semantics></math>”) and our complete method based on the joint QA and grounding training strategy (<em id="S6.SS2.SSS1.p1.4.2" class="ltx_emph ltx_font_italic">i.e</em>., “<math id="S6.SS2.SSS1.p1.2.m2.1" class="ltx_Math" alttext="L_{\text{All}}" display="inline"><semantics id="S6.SS2.SSS1.p1.2.m2.1a"><msub id="S6.SS2.SSS1.p1.2.m2.1.1" xref="S6.SS2.SSS1.p1.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S6.SS2.SSS1.p1.2.m2.1.1.2" xref="S6.SS2.SSS1.p1.2.m2.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S6.SS2.SSS1.p1.2.m2.1.1.3" xref="S6.SS2.SSS1.p1.2.m2.1.1.3a.cmml">All</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.2.m2.1b"><apply id="S6.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S6.SS2.SSS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S6.SS2.SSS1.p1.2.m2.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S6.SS2.SSS1.p1.2.m2.1.1.2">𝐿</ci><ci id="S6.SS2.SSS1.p1.2.m2.1.1.3a.cmml" xref="S6.SS2.SSS1.p1.2.m2.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S6.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S6.SS2.SSS1.p1.2.m2.1.1.3">All</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.2.m2.1c">L_{\text{All}}</annotation></semantics></math>”).
As shown in Lines 3 and 4 of Table <a href="#S6.T5" title="TABLE V ‣ VI-A 3D Grounded Question Answering Results. ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, we can see that the grounding results not only provide potential explanation capability to the QA results, but also improve the overall QA results to a certain degree. Moreover, with the additional detection loss <math id="S6.SS2.SSS1.p1.3.m3.1" class="ltx_Math" alttext="L_{\text{detection}}" display="inline"><semantics id="S6.SS2.SSS1.p1.3.m3.1a"><msub id="S6.SS2.SSS1.p1.3.m3.1.1" xref="S6.SS2.SSS1.p1.3.m3.1.1.cmml"><mi mathcolor="#000000" id="S6.SS2.SSS1.p1.3.m3.1.1.2" xref="S6.SS2.SSS1.p1.3.m3.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S6.SS2.SSS1.p1.3.m3.1.1.3" xref="S6.SS2.SSS1.p1.3.m3.1.1.3a.cmml">detection</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.3.m3.1b"><apply id="S6.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S6.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S6.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S6.SS2.SSS1.p1.3.m3.1.1.2">𝐿</ci><ci id="S6.SS2.SSS1.p1.3.m3.1.1.3a.cmml" xref="S6.SS2.SSS1.p1.3.m3.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S6.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S6.SS2.SSS1.p1.3.m3.1.1.3">detection</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.3.m3.1c">L_{\text{detection}}</annotation></semantics></math> from the vision branch, our alternative method “Ours (w/o Grounding)” (<em id="S6.SS2.SSS1.p1.4.3" class="ltx_emph ltx_font_italic">i.e</em>., with 45.99% Acc.) also outperforms the “QA w/o Scene” method (<em id="S6.SS2.SSS1.p1.4.4" class="ltx_emph ltx_font_italic">i.e</em>., “ <math id="S6.SS2.SSS1.p1.4.m4.1" class="ltx_Math" alttext="L_{\text{QA}}" display="inline"><semantics id="S6.SS2.SSS1.p1.4.m4.1a"><msub id="S6.SS2.SSS1.p1.4.m4.1.1" xref="S6.SS2.SSS1.p1.4.m4.1.1.cmml"><mi mathcolor="#000000" id="S6.SS2.SSS1.p1.4.m4.1.1.2" xref="S6.SS2.SSS1.p1.4.m4.1.1.2.cmml">L</mi><mtext mathcolor="#000000" id="S6.SS2.SSS1.p1.4.m4.1.1.3" xref="S6.SS2.SSS1.p1.4.m4.1.1.3a.cmml">QA</mtext></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS1.p1.4.m4.1b"><apply id="S6.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S6.SS2.SSS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS1.p1.4.m4.1.1.1.cmml" xref="S6.SS2.SSS1.p1.4.m4.1.1">subscript</csymbol><ci id="S6.SS2.SSS1.p1.4.m4.1.1.2.cmml" xref="S6.SS2.SSS1.p1.4.m4.1.1.2">𝐿</ci><ci id="S6.SS2.SSS1.p1.4.m4.1.1.3a.cmml" xref="S6.SS2.SSS1.p1.4.m4.1.1.3"><mtext mathcolor="#000000" mathsize="70%" id="S6.SS2.SSS1.p1.4.m4.1.1.3.cmml" xref="S6.SS2.SSS1.p1.4.m4.1.1.3">QA</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS1.p1.4.m4.1c">L_{\text{QA}}</annotation></semantics></math>”, with 37.25% Acc. as shown in Line 1 of Table <a href="#S5.T3" title="TABLE III ‣ V-D Loss Function ‣ V Methodology ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>) by 8.74%, which also verifies that our 3D vision branch and the fusion method can significantly improve the 3D VQA results.</p>
</div>
</section>
<section id="S6.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS2.SSS2.4.1.1" class="ltx_text">VI-B</span>2 </span>Effectiveness of the pre-trained language model in the language branch.</h4>

<div id="S6.SS2.SSS2.p1" class="ltx_para">
<p id="S6.SS2.SSS2.p1.1" class="ltx_p">In Lines 2 and 4 of Table <a href="#S6.T5" title="TABLE V ‣ VI-A 3D Grounded Question Answering Results. ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, we report the results by using different language modules (<em id="S6.SS2.SSS2.p1.1.1" class="ltx_emph ltx_font_italic">e.g</em>., randomly initialized Bi-GRU and pre-trained T5). When compared to the randomly initialized Bi-GRU, our method using the pre-trained T5 language module achieves much better QA results as well as the grounding results. The results verify that our newly collected dataset contains more free-form questions. Without the pre-trained language module, the model is more prone to overfit to the questions and ground to the wrong objects due to insufficient language modeling capability in the Bi-GRU method, leading to poor QA results.</p>
</div>
</section>
<section id="S6.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span id="S6.SS2.SSS3.4.1.1" class="ltx_text">VI-B</span>3 </span>Ablation study for the enhanced self-attention module in the 3D vision branch.</h4>

<div id="S6.SS2.SSS3.p1" class="ltx_para">
<p id="S6.SS2.SSS3.p1.1" class="ltx_p">As discussed in Sec. <a href="#S5.SS2" title="V-B Token Encoding and Fusion Module ‣ V Methodology ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">V-B</span></span></a>, we propose an enhanced self-attention module in the 3D vision branch to extensively encode the within-object features and across-object spatial distances to help the model attend and ground to all the answer related objects.
In Lines 1 and 4 of Table <a href="#S6.T5" title="TABLE V ‣ VI-A 3D Grounded Question Answering Results. ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">V</span></a>, we compare the results of our method and the alternative method that uses the standard self-attention modules. We can see that the enhanced self-attention module contributes to both the final QA results and the grounding results.
Specifically, the grounding results of the “AG-NotIn-Q” objects (<em id="S6.SS2.SSS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>., the answer-grounded objects not appeared in the question) are improved by 2.7%,
which verifies that a good visual encoding module helps us identify the challenging but essential “AG-NotIn-Q” objects.</p>
</div>
</section>
</section>
<section id="S6.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS3.4.1.1" class="ltx_text">VI-C</span> </span><span id="S6.SS3.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">Visualization of the Grounded VQA Results.</span>
</h3>

<div id="S6.SS3.p1" class="ltx_para">
<p id="S6.SS3.p1.1" class="ltx_p">Fig. <a href="#S6.F9" title="Figure 9 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> compares the visualization results (<em id="S6.SS3.p1.1.1" class="ltx_emph ltx_font_italic">i.e</em>., the grounded VQA results) by using the proposed method and two baseline methods 3DQA-TR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> and ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> on the FE-3DGQA dataset. Given the question and the 3D point clouds, we show not only the predicted answer, but also the grounding results of the three types of answer-related objects. The related objects of AG-In-Q, AG-NotIn-Q, and Context-Of-AG are colored in green, cyan, and red, respectively. Fig. <a href="#S6.F9" title="Figure 9 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (a) shows that the “AG-In-Q” object is successfully grounded and classified by our method and the correct answer is also predicted accordingly. While 3DQA-TR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> correctly answers the question in Fig. <a href="#S6.F9" title="Figure 9 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (b), it fails to detect all the “Context-Of-AG” objects and the “AG-NotIn-Q” objects, and thus the correct answer is not explainable. By contrast, our method predicts the correct answer together with the completely grounded related objects. In another example (see Fig. <a href="#S6.F9" title="Figure 9 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (c)), both ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> and our method predict the right answer. However, when compared to our completely grounded results, the ScanQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> method predicts the correct answer even without correctly localizing the “AG-In-Q” object and thus the result is not reliable.
For Fig. <a href="#S6.F9" title="Figure 9 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> (d), while all the methods predict the answer correctly, only our method is completely grounded by not only identifying the “AG-In-Q” object, but also most of the “Context-Of-AG” objects in the 3D scene.</p>
</div>
<div id="S6.SS3.p2" class="ltx_para">
<p id="S6.SS3.p2.1" class="ltx_p">Fig. <a href="#S6.F10" title="Figure 10 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> shows more qualitative results of the ground-truth annotation and the outputs from our method. As mentioned in Section <a href="#S3.SS1" title="III-A FE-3DGQA Dataset Collection ‣ III FE-3DGQA Dataset ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>, our dataset covers the free-form questions from different aspects, including
1) the local object-oriented questions with attributes and local relations of a specific object, 2) the global context-aware questions to understand the whole scene, 3) the complex relationships among multiple objects, and 4) the direction or location related questions potentially for navigation.</p>
</div>
<div id="S6.SS3.p3" class="ltx_para">
<p id="S6.SS3.p3.1" class="ltx_p">In Fig. <a href="#S6.F10" title="Figure 10 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>, we show more results corresponding to all the four aspects.
Fig. <a href="#S6.F10" title="Figure 10 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (a) shows the correct and explainable answer predicted by our method. In this example, our method not only predicts the correct answer to the question, but also completely grounds all the objects related to the answer.
By contrast, as shown in Fig. <a href="#S6.F10" title="Figure 10 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (b), there are still some failure cases from our method.
For example, in the 1st row of Fig. <a href="#S6.F10" title="Figure 10 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (b), though the answer to the question is correct, the grounding results are not complete.
For the 2nd and 3rd row of Fig. <a href="#S6.F10" title="Figure 10 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (b), the answer to the question is wrong due to the incorrect number of grounded objects in the grounding results.
For the 4th row of Fig. <a href="#S6.F10" title="Figure 10 ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a> (b), the incorrect answer might be caused by the incorrect grounding results to the related objects. The failure cases verify the challenges of our dataset, and more advanced techniques are required to deal with the challenging cases in our future works.

</p>
</div>
</section>
<section id="S6.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S6.SS4.4.1.1" class="ltx_text">VI-D</span> </span><span id="S6.SS4.5.2" class="ltx_text ltx_font_italic" style="color:#000000;">The effect of three data extension strategies.</span>
</h3>

<div id="S6.SS4.p1" class="ltx_para">
<p id="S6.SS4.p1.2" class="ltx_p">In Tab. <a href="#S6.T6" title="TABLE VI ‣ VI-B Ablation Studies and Analysis ‣ VI Experiments ‣ Towards Explainable 3D Grounded Visual Question Answering: A New Benchmark and Strong Baseline" class="ltx_ref"><span class="ltx_text ltx_ref_tag">VI</span></a>, we report the grounded visual question answering results of our newly proposed method based on the test set of our newly collected FE-3DGQA dataset together with different extended training datasets.
In this table, “manual” means our original manually annotated FE-3DGQA dataset, “det.” represents the transformed 3D detection dataset (<em id="S6.SS4.p1.2.3" class="ltx_emph ltx_font_italic">i.e</em>., ScanNet), “ground.” represents the transformed 3D visual grounding datasets (<em id="S6.SS4.p1.2.4" class="ltx_emph ltx_font_italic">i.e</em>., ScanRefer and Nr3D), and “Ext.” represents the full extended FE-3DGQA dataset, which consists of the transformed detection dataset, the transformed visual grounding dataset, and the transformed masked visual grounding dataset.
Note that as the annotations of the related objects for <span id="S6.SS4.p1.2.2" class="ltx_text">the samples from both the transformed 3D grounding datasets and the transformed masked 3D grounding datasets<span id="S6.SS4.p1.2.2.2" class="ltx_text"> are incomplete,
<span id="S6.SS4.p1.2.2.2.2" class="ltx_text">we remove the corresponding loss functions for “AG-NotIn-Q” and “Context-Of-AG” (<span id="S6.SS4.p1.2.2.2.2.1" class="ltx_text ltx_font_italic" style="color:#000000;">resp.</span> “AG-In-Q” and “Context-Of-AG”) in the object type classification loss <math id="S6.SS4.p1.1.1.1.1.m1.1" class="ltx_Math" alttext="L_{type}" display="inline"><semantics id="S6.SS4.p1.1.1.1.1.m1.1a"><msub id="S6.SS4.p1.1.1.1.1.m1.1.1" xref="S6.SS4.p1.1.1.1.1.m1.1.1.cmml"><mi mathcolor="#000000" id="S6.SS4.p1.1.1.1.1.m1.1.1.2" xref="S6.SS4.p1.1.1.1.1.m1.1.1.2.cmml">L</mi><mrow id="S6.SS4.p1.1.1.1.1.m1.1.1.3" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.cmml"><mi mathcolor="#000000" id="S6.SS4.p1.1.1.1.1.m1.1.1.3.2" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.1.1.1.m1.1.1.3.1" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathcolor="#000000" id="S6.SS4.p1.1.1.1.1.m1.1.1.3.3" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.3.cmml">y</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.1.1.1.m1.1.1.3.1a" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathcolor="#000000" id="S6.SS4.p1.1.1.1.1.m1.1.1.3.4" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.1.1.1.1.m1.1.1.3.1b" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.1.cmml">​</mo><mi mathcolor="#000000" id="S6.SS4.p1.1.1.1.1.m1.1.1.3.5" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.1.1.1.1.m1.1b"><apply id="S6.SS4.p1.1.1.1.1.m1.1.1.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS4.p1.1.1.1.1.m1.1.1.1.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S6.SS4.p1.1.1.1.1.m1.1.1.2.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1.2">𝐿</ci><apply id="S6.SS4.p1.1.1.1.1.m1.1.1.3.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3"><times id="S6.SS4.p1.1.1.1.1.m1.1.1.3.1.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.1"></times><ci id="S6.SS4.p1.1.1.1.1.m1.1.1.3.2.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.2">𝑡</ci><ci id="S6.SS4.p1.1.1.1.1.m1.1.1.3.3.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.3">𝑦</ci><ci id="S6.SS4.p1.1.1.1.1.m1.1.1.3.4.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.4">𝑝</ci><ci id="S6.SS4.p1.1.1.1.1.m1.1.1.3.5.cmml" xref="S6.SS4.p1.1.1.1.1.m1.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.1.1.1.1.m1.1c">L_{type}</annotation></semantics></math> for the vision branch, and the semantic label and type classification loss <math id="S6.SS4.p1.2.2.2.2.m2.1" class="ltx_Math" alttext="L_{sem-cls}" display="inline"><semantics id="S6.SS4.p1.2.2.2.2.m2.1a"><msub id="S6.SS4.p1.2.2.2.2.m2.1.1" xref="S6.SS4.p1.2.2.2.2.m2.1.1.cmml"><mi mathcolor="#000000" id="S6.SS4.p1.2.2.2.2.m2.1.1.2" xref="S6.SS4.p1.2.2.2.2.m2.1.1.2.cmml">L</mi><mrow id="S6.SS4.p1.2.2.2.2.m2.1.1.3" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.cmml"><mrow id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.cmml"><mi mathcolor="#000000" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.2" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.1" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathcolor="#000000" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.3" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.1a" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.1.cmml">​</mo><mi mathcolor="#000000" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.4" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.4.cmml">m</mi></mrow><mo mathcolor="#000000" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.1" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.1.cmml">−</mo><mrow id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.cmml"><mi mathcolor="#000000" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.2" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.1" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.1.cmml">​</mo><mi mathcolor="#000000" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.3" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.3.cmml">l</mi><mo lspace="0em" rspace="0em" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.1a" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.1.cmml">​</mo><mi mathcolor="#000000" id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.4" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.4.cmml">s</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S6.SS4.p1.2.2.2.2.m2.1b"><apply id="S6.SS4.p1.2.2.2.2.m2.1.1.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS4.p1.2.2.2.2.m2.1.1.1.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1">subscript</csymbol><ci id="S6.SS4.p1.2.2.2.2.m2.1.1.2.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.2">𝐿</ci><apply id="S6.SS4.p1.2.2.2.2.m2.1.1.3.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3"><minus id="S6.SS4.p1.2.2.2.2.m2.1.1.3.1.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.1"></minus><apply id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2"><times id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.1.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.1"></times><ci id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.2.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.2">𝑠</ci><ci id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.3.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.3">𝑒</ci><ci id="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.4.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.2.4">𝑚</ci></apply><apply id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3"><times id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.1.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.1"></times><ci id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.2.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.2">𝑐</ci><ci id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.3.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.3">𝑙</ci><ci id="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.4.cmml" xref="S6.SS4.p1.2.2.2.2.m2.1.1.3.3.4">𝑠</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS4.p1.2.2.2.2.m2.1c">L_{sem-cls}</annotation></semantics></math> of the language branch for the samples from the transformed 3D grounding datasets (<span id="S6.SS4.p1.2.2.2.2.2" class="ltx_text ltx_font_italic" style="color:#000000;">resp.</span> the transformed masked 3D grounding datasets).<span id="S6.SS4.p1.2.2.2.2.3" class="ltx_text"></span></span></span></span></p>
</div>
<div id="S6.SS4.p2" class="ltx_para">
<p id="S6.SS4.p2.1" class="ltx_p">The results show that the overall QA results and the grounding results of our method can be generally improved by extending the manually annotated FE-3DGQA dataset through adding more transformed datasets. The results also verify that our FE-3DGQA dataset can be easily extended by transforming more 3D datasets based on the template-based annotations.

</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span id="S7.1.1" class="ltx_text ltx_font_smallcaps">Conclusion</span>
</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">This work formally defines and addresses the free-form and explainable 3D VQA task. First, we collect a new FE-3DGQA dataset with manually annotated diverse and free-form question-answer pairs and dense and complete grounded bounding box annotations for three different types of answer-related objects. Then, we propose a new 3D VQA framework that effectively predicts the completely visually grounded and explainable answer. The experimental results verify the effectiveness of the collected benchmark dataset for evaluating 3D VQA methods, as well as validate that our proposed framework achieves the state-of-the-art 3D VQA performance by predicting the completely visually grounded answer.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg, “Referitgame: Referring to
objects in photographs of natural scenes,” in <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2014
conference on empirical methods in natural language processing (EMNLP)</em>,
2014, pp. 787–798.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. Moura, D. Parikh, and
D. Batra, “Visual dialog,” in <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on
computer vision and pattern recognition</em>, 2017, pp. 326–335.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dollár, and C. L.
Zitnick, “Microsoft coco captions: Data collection and evaluation server,”
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1504.00325</em>, 2015.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra, “Embodied
question answering,” in <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition</em>, 2018, pp. 1–10.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sünderhauf, I. Reid,
S. Gould, and A. Van Den Hengel, “Vision-and-language navigation:
Interpreting visually-grounded navigation instructions in real
environments,” in <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>, 2018, pp. 3674–3683.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and
D. Parikh, “VQA: visual question answering,” in <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh, “Making the V
in VQA matter: Elevating the role of image understanding in visual question
answering,” in <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
D. Z. Chen, A. X. Chang, and M. Nießner, “ScanRefer: 3D object
localization in RGB-D scans using natural language,” in <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Z. Chen, A. Gholami, M. Nießner, and A. X. Chang, “Scan2Cap:
Context-aware dense captioning in rgb-d scans,” in <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
M. Feng, Z. Li, Q. Li, L. Zhang, X. Zhang, G. Zhu, H. Zhang, Y. Wang, and
A. Mian, “Free-form description guided 3d visual graph network for object
grounding in point cloud,” in <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. J. Shih, S. Singh, and D. Hoiem, “Where to look: Focus regions for visual
question answering,” in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer
vision and pattern recognition</em>, 2016, pp. 4613–4621.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
D. Azuma, T. Miyanishi, S. Kurita, and M. Kawanabe, “Scanqa: 3d question
answering for spatial scene understanding,” <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, vol. 2112.10482,
2021.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
S. Ye, D. Chen, S. Han, and J. Liao, “3d question answering,” <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">arXiv</em>,
vol. 2112.08359, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
X. Yan, Z. Yuan, Y. Du, Y. Liao, Y. Guo, Z. Li, and S. Cui, “CLEVR3D:
compositional language and elementary visual reasoning for question answering
in 3d real-world scenes,” <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, vol. 2112.11691, 2021.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner,
“ScanNet: Richly-annotated 3d reconstructions of indoor scenes,” in
<em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas,
“ReferIt3D: Neural listeners for fine-grained 3d object identification in
real-world scenes,” in <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">ECCV</em>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Guo, J. Liu, and D. Xu, “JointPruning: Pruning networks along multiple
dimensions for efficient point cloud processing,” <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">TCSVT</em>, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
K. Wang, L. Sheng, S. Gu, and D. Xu, “Sequential point cloud upsampling by
exploiting multi-scale temporal dependency,” <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">TCSVT</em>, pp. 4686–4696,
2021.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Liu and D. Xu, “GeometryMotion-Net: A strong two-stream baseline for 3d
action recognition,” <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">TCSVT</em>, pp. 4711–4721, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
L. Zhao, J. Guo, D. Xu, and L. Sheng, “Transformer3D-Det: Improving 3d
object detection by vote refinement,” <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">TCSVT</em>, pp. 4735–4746, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “PointNet: Deep learning on point
sets for 3d classification and segmentation,” in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2017.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Z. Song, L. Zhao, and J. Zhou, “Learning hybrid semantic affinity for point
cloud segmentation,” <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">TCSVT</em>, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Wang, H. Zhu, H. Liu, and Z. Ma, “Lossy point cloud geometry compression
via end-to-end learning,” <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">TCSVT</em>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
R. Mekuria, K. Blom, and P. César, “Design, implementation, and
evaluation of a point cloud codec for tele-immersive video,” <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">TCSVT</em>,
2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
F. Song, Y. Shao, W. Gao, H. Wang, and T. Li, “Layer-wise geometry aggregation
framework for lossless lidar point cloud compression,” <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">TCSVT</em>, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
L. Zhao, D. Cai, L. Sheng, and D. Xu, “3dvg-transformer: Relation modeling for
visual grounding on point clouds,” in <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
D. Z. Chen, Q. Wu, M. Nießner, and A. X. Chang, “D3net: A
speaker-listener architecture for semi-supervised dense captioning and visual
grounding in RGB-D scans,” <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv</em>, vol. 2112.01551, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
J. Wald, H. Dhamo, N. Navab, and F. Tombari, “Learning 3D Semantic Scene
Graphs from 3D Indoor Reconstructions,” in <em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Wald, A. Avetisyan, F. T. Nassir Navab, and M. Niessner, “RIO: 3d object
instance re-localization in changing indoor environments,” in <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">ICCV</em>,
2019.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Z. Yuan, X. Yan, Y. Liao, R. Zhang, Z. Li, and S. Cui, “InstanceRefer:
Cooperative holistic understanding for visual grounding on point clouds
through instance multi-level contextual referring,” <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
P.-H. Huang, H.-H. Lee, H.-T. Chen, and T.-L. Liu, “Text-guided graph neural
networks for referring 3d instance segmentation,” in <em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">AAAI</em>, 2021.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. Roh, K. Desingh, A. Farhadi, and D. Fox, “Languagerefer: Spatial-language
model for 3d visual grounding,” in <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">PMLR</em>, 2021.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Z. Yang, S. Zhang, L. Wang, and J. Luo, “SAT: 2d semantics assisted training
for 3d visual grounding,” <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
H. Liu, A. Lin, X. Han, L. Yang, Y. Yu, and S. Cui, “Refer-it-in-rgbd: A
bottom-up approach for 3d visual grounding in RGBD images,” in
<em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">CVPR</em>, 2021.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
D. He, Y. Zhao, J. Luo, T. Hui, S. Huang, A. Zhang, and S. Liu, “Transrefer3d:
Entity-and-relation aware transformer for fine-grained 3d visual grounding,”
in <em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">ACM MM</em>, 2021.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
A. Abdelreheem, U. Upadhyay, I. Skorokhodov, R. A. Yahya, J. Chen, and
M. Elhoseiny, “3dreftransformer: Fine-grained object identification in
real-world scenes using natural language,” in <em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">WACV</em>, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M. Malinowski and M. Fritz, “A multi-world approach to question answering
about real-world scenes based on uncertain input,” in <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">NIPS</em>, 2014.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
B. Zhou, Y. Tian, S. Sukhbaatar, A. Szlam, and R. Fergus, “Simple baseline for
visual question answering,” <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1512.02167</em>, 2015.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
D. Teney, P. Anderson, X. He, and A. Van Den Hengel, “Tips and tricks for
visual question answering: Learnings from the 2017 challenge,” in
<em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and pattern
recognition</em>, 2018, pp. 4223–4232.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-image
co-attention for visual question answering,” <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol. 29, 2016.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu, L. Dong,
F. Wei <em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">et al.</em>, “Oscar: Object-semantics aligned pre-training for
vision-language tasks,” in <em id="bib.bib41.2.2" class="ltx_emph ltx_font_italic">European Conference on Computer
Vision</em>.   Springer, 2020, pp. 121–137.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
J. Lu, D. Batra, D. Parikh, and S. Lee, “Vilbert: Pretraining task-agnostic
visiolinguistic representations for vision-and-language tasks,”
<em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, vol. 32, 2019.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou,
W. Li, and P. J. Liu, “Exploring the limits of transfer learning with a
unified text-to-text transformer,” <em id="bib.bib43.1.1" class="ltx_emph ltx_font_italic">J. Mach. Learn. Res.</em>, 2020.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
C. R. Qi, O. Litany, K. He, and L. J. Guibas, “Deep hough voting for 3d object
detection in point clouds,” in <em id="bib.bib44.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Z. Tian, C. Shen, H. Chen, and T. He, “Fcos: Fully convolutional one-stage
object detection,” in <em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2019, pp. 9627–9636.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
transformer: Hierarchical vision transformer using shifted windows,” in
<em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">ICCV</em>, 2021.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Z. Yu, J. Yu, Y. Cui, D. Tao, and Q. Tian, “Deep modular co-attention networks
for visual question answering,” in <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition</em>, 2019, pp. 6281–6290.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2209.12026" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2209.12028" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2209.12028">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2209.12028" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2209.12029" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 00:49:24 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
