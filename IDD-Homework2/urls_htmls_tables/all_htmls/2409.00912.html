<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.00912] Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation</title><meta property="og:description" content="Multiple datasets have been created for training and testing appearance-based gaze estimators. Intuitively, more data should lead to better performance. However, combining datasets to train a single estimator rarely imâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.00912">

<!--Generated on Sun Oct  6 01:26:59 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="gaze estimation transformers feature fusion multi-dataset training.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Hong Kong University of Science and Technology, Hong Kong
<span id="id1.1" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>liang.wu@connect.ust.hk</span></span></span>
<br class="ltx_break"><span id="id1.2" class="ltx_note ltx_role_email"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>eebert@ust.hk</span></span></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liang Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-5214-7715" title="ORCID identifier" class="ltx_ref">0000-0001-5214-7715</a></span>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bertram E. Shi
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-9167-7495" title="ORCID identifier" class="ltx_ref">0000-0001-9167-7495</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Multiple datasets have been created for training and testing appearance-based gaze estimators. Intuitively, more data should lead to better performance. However, combining datasets to train a single estimator rarely improves gaze estimation performance. One reason may be differences in the experimental protocols used to obtain the gaze samples, resulting in differences in the distributions of head poses, gaze angles, illumination, etc. Another reason may be the inconsistency between methods used to define gaze angles (label mismatch). We propose two innovations to improve the performance of gaze estimation by leveraging multiple datasets, a change in the estimator architecture and the introduction of a gaze adaptation module. Most state-of-the-art estimators merge information extracted from images of the two eyes and the entire face either in parallel or combine information from the eyes first then with the face. Our proposed <span id="id1.id1.1" class="ltx_text ltx_font_italic">Two-stage Transformer-based Gaze-feature Fusion</span> (TTGF) method uses transformers to merge information from each eye and the face separately and then merge across the two eyes. We argue that this improves head pose invariance since changes in head pose affect left and right eye images in different ways. Our proposed <span id="id1.id1.2" class="ltx_text ltx_font_italic">Gaze Adaptation Module</span> (GAM) method handles annotation inconsistency by applying a Gaze Adaption Module for each dataset to correct gaze estimates from a single shared estimator. This enables us to combine information across datasets despite differences in labeling. Our experiments show that these innovations improve gaze estimation performance over the SOTA both individually and collectively (by 10% - 20%). Our code is available at <a target="_blank" href="https://github.com/HKUST-NISL/GazeSetMerge" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/HKUST-NISL/GazeSetMerge</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>gaze estimation transformers feature fusion multi-dataset training.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Estimation of human gaze plays important roles in many applications, such as human-computer interaction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, virtual reality <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, attention analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and psychological studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Conventional methods, such as those based on pupil center corneal reflections (PCCR), use 3D eye models to compute the gaze direction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. These require special measurement setups, such as active infrared illumination, to estimate model geometry. In contrast, appearance-based gaze estimators use input from commonly available RGB web cameras, which are more convenient and less expensive. Unfortunately, estimates from them are less accurate than those from PCCR-based systems. The current lowest reported within-person error of gaze estimation is <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="4.04^{\circ}" display="inline"><semantics id="S1.p2.1.m1.1a"><msup id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml"><mn id="S1.p2.1.m1.1.1.2" xref="S1.p2.1.m1.1.1.2.cmml">4.04</mn><mo id="S1.p2.1.m1.1.1.3" xref="S1.p2.1.m1.1.1.3.cmml">âˆ˜</mo></msup><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><apply id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S1.p2.1.m1.1.1.1.cmml" xref="S1.p2.1.m1.1.1">superscript</csymbol><cn type="float" id="S1.p2.1.m1.1.1.2.cmml" xref="S1.p2.1.m1.1.1.2">4.04</cn><compose id="S1.p2.1.m1.1.1.3.cmml" xref="S1.p2.1.m1.1.1.3"></compose></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">4.04^{\circ}</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> on MPIIFaceGaze. In contrast, manufacturers of PCCR-based systems typically report accuracies of less than one degree.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">However, the gap between the two continues to shrink, most recently due to the use of Convolutional Neural Networks (CNN) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and transformers. Many CNN architectures have been proposed for appearance-based gaze estimation. Zhang et al. employed a multi-modal model that used eye images and an estimated head pose vector as inputs to estimate gaze direction <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Later, they applied spatial weighting to feature maps from the face image to enhance information from eye regions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Other studies used three separate pipelines to extract features from images of the head and the two eyes and then fused them to predict the gaze <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. Merging information from the eyes and the face improves estimation accuracy.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Since appearance-based gaze estimators rely heavily on training data, many datasets have been proposed to train gaze estimators. Initial datasets were collected under fairly well-controlled and limited conditions (e.g., ranges of head poses and gaze angles). More recent datasets have been collected on conditions of increased diversity. The availability of more data can potentially increase the performance of appearance-based gaze estimators, but can also introduce new challenges. This paper seeks to address two of these challenges.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">First, increases in the head pose range have spurred the development of new architectures that combine information from images of the two eye regions (which primarily indicate gaze direction in head-centric coordinates) and an image of the entire face (which primarily indicates head pose). Many SOTA (state-of-the-art) methods combine this information in parallel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, or combine information from the eyes first followed by the face image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To improve upon these approaches, we propose a Two-stage Transformer-based Gaze-feature Fusion (TTGF) architecture, which combines information from each eye image with the face image separately and then integrates information across the two eyes. This approach is motivated by the fact that the head-centric gaze directions of the two eyes differ and should thus each be merged with the face image. This may also compensate for situations where the reliability of information from the two eyes may differ, e.g., due to occlusion.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Second, although intuitively increasing the amount of data by combining datasets should improve performance, inconsistencies in annotation among datasets make it difficult to improve accuracy by simply combining multiple gaze datasets. To provide a normalized gaze annotation, a common scheme is to rotate the gaze vector from the gaze origin to the target point by a rotation matrix that depends upon the head pose <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>. Differences between the methods for head pose estimation and target point estimation lead to inconsistency among different datasets. Even when the subjectâ€™s head is constrained by a chin rest <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, head pose estimation error can still exist due to the placement of the subjectâ€™s head in the chin rest.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p">To address this, we propose the use of a Gaze Adaption Module (GAMs) for each dataset, which adjusts the gaze label from a shared estimator so it is consistent with the dataset of the source image. This enables multi-dataset training by simply adding GAM to the modelâ€™s gaze regression head.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">Our experimental results demonstrate that these two innovations lead to state-of-the-art performance on multiple datasets, under training with both single datasets and mixed datasets.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Gaze Estimation Methods</span>â€ƒGaze estimation methods can typically be categorized as either model-based or appearance-based. Model-based methods usually construct the 3D model of the head and eyes. The gaze direction is calculated by utilizing geometric information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. Model-based methods usually require time-consuming personal calibration to fit the subject-specific parameters, such as cornea radius and kappa angles.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In contrast, appearance-based methods directly learn mapping functions from a large number of image-gaze sample pairs.
Early approaches used conventional regression to perform the mapping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. More recently, CNNs have significantly improved the performance of appearance-based gaze estimation. Zhang et al. proposed the first CNN-based network to regress the gaze direction from a cropped eye image, and a head pose vector <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. They later proposed to use the learnable spatial weights to enhance the information from the eye regions in the face image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Krafka et al. proposed iTracker, a multi-region CNN model, which takes both the head and eye images as input. To further improve the accuracy, Chen et al. investigated the dilated convolution layers to efficiently increase the receptive field sizes of the features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Researchers have now started to use transformer-based networks, which can further improve gaze estimation accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_noindent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Transformers</span>â€ƒThe Transformer architecture was first introduced by Vaswani et al. for natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. It consists of self-attention layers, layer normalization, and multi-layer perceptron layers. Compared with recurrent networks, the global computations and efficient memory of self-attention layers make transformers more suitable for long sequences.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The Vision Transformer (ViT) was proposed by Dosovitskiy et al. for image classification tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite>. ViT divides one image into non-overlapping patches. A transformer encoder is applied to the features extracted from the patches. Transformers have achieved state-of-the-art in large-scale image classification tasks, leading to their application to many other vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite>.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Recently, a few researchers have explored the capability of transformers in gaze estimation. Cheng et al. proposed GazeTR-Hybrid where they used convolutional neural networks to extract the feature map of an input head image, then treated the features at different positions as a sequence of features input to a transformer encoder <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>. Cai et al. proposed iTracker-MHSH <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>. Inspired by iTracker, it uses a transformer to integrate the features of the head and eye images.</p>
</div>
<div id="S2.p6" class="ltx_para ltx_noindent">
<p id="S2.p6.1" class="ltx_p"><span id="S2.p6.1.1" class="ltx_text ltx_font_bold">Mixed Dataset Training</span>â€ƒThere are two main advantages to mixed dataset training. First, it provides a single model applicable to multiple datasets. Second, model training may benefit from the increased amount of data. Mixed dataset training has been applied to many computer vision tasks, such as person reidentification <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>, monocular depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>, semantic image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, video quality assessment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and 3D object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>. Addressing the challenges of mixed dataset training is task-specific. For example, to mix image segmentation datasets, category merging needs were conducted before training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. For video quality assessment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, the challenge was to resolve inconsistent ranges of subjective quality scores across datasets.</p>
</div>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">To the best of our knowledge, we are the first to propose mixed dataset training for gaze estimation. There are two challenges that must be addressed. First, the distribution of gaze vectors and head poses varies between different gaze datasets. Second, there exists annotation inconsistency in gaze vectors from different gaze datasets.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Annotation Inconsistency</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.3" class="ltx_p">The gaze vector is defined as the vector starting from the gaze origin to the gaze target. Gaze dataset collection requires an experimental setup to capture three types of information in camera coordinates: 1) the position of the visual target <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="P_{t}" display="inline"><semantics id="S3.p1.1.m1.1a"><msub id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">P</mi><mi id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">ğ‘ƒ</ci><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">P_{t}</annotation></semantics></math>, 2) the position of gaze origin <math id="S3.p1.2.m2.1" class="ltx_Math" alttext="P_{o}" display="inline"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">P</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">ğ‘ƒ</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">ğ‘œ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">P_{o}</annotation></semantics></math>, and 3) the head pose <math id="S3.p1.3.m3.1" class="ltx_Math" alttext="R" display="inline"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">ğ‘…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">R</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. However, different datasets utilize different methods to get these values, leading to different annotations.</p>
</div>
<div id="S3.p2" class="ltx_para ltx_noindent">
<p id="S3.p2.1" class="ltx_p"><span id="S3.p2.1.1" class="ltx_text ltx_font_bold">Inconsistency in gaze target estimation</span>â€ƒUsually, the visual target is indicated by a moving dot on a screens. To determine the position of the dot target, the intrinsic parameters of the camera must be obtained beforehand. MPIIGaze uses a mirror-based calibration method <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> to estimate the 3D positions of each screen plane. Finally, the position of the moving dot is computed based on the screen size and resolution. In addition to a moving dot on the screen, EYEDIAP has an additional floating ball visual target. Its position is estimated first in an RGB-D sensor coordinate system and then transformed to the camera coordinate system. Imprecision in the RGB-D sensor, errors in the screen-to-camera calibration and RGB-D to-camera calibration will all contribute to the inconsistency of the gaze target position <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="p_{t}" display="inline"><semantics id="S3.p2.1.m1.1a"><msub id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">p</mi><mi id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">ğ‘</ci><ci id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3">ğ‘¡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">p_{t}</annotation></semantics></math>.</p>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Inconsistency in gaze origin and head pose estimation</span>â€ƒThere are inconsistencies between datasets in the selection of gaze origin and the estimation of head pose. In early work, gaze was estimated eye images, where the eye center defined the gaze origin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. More recently, people estimate gaze from the whole head image, where the gaze origin is usually set at the center of the head <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. To get the 3D head pose, MPIIGaze and ETH-XGaze detect landmarks from the 2D head image and fit a 3D morphable model of the head to the detected landmarks. EYEDIAP directly uses the depth data from the RGB-D sensor to fit a 3D Morphable Model.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Method</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Fig.<a href="#S4.F1" title="Figure 1 â€£ 4 Method â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> shows our framework, which consists of an eye-head transformer-based feature fusion module for gaze estimation followed by a set of gaze adaptation modules. We described these in more detail below.</p>
</div>
<figure id="S4.F1" class="ltx_figure"><img src="/html/2409.00912/assets/x1.png" id="S4.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="232" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The proposed framework contains two modules: 1) TTGF and 2) GAM. The TTGF applies two-stage feature fusion to the features of the head and eyes with transformers, and the GAM produces a gaze offset to adjust the predicted gaze for mixed datasets training.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Feature Fusion with Transformers</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.8" class="ltx_p">A typical transformer encoder contains <math id="S4.SS1.p1.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">L</annotation></semantics></math> transformer blocks, each containing multi-head self-attention (MHSA) layers, layer normalization (LN), and multi-layer perceptron layers (MLP). To process an input feature matrix <math id="S4.SS1.p1.2.m2.1" class="ltx_Math" alttext="\boldsymbol{Z}\in\mathbb{R}^{n\times d}" display="inline"><semantics id="S4.SS1.p1.2.m2.1a"><mrow id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">ğ’</mi><mo id="S4.SS1.p1.2.m2.1.1.1" xref="S4.SS1.p1.2.m2.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml"><mi id="S4.SS1.p1.2.m2.1.1.3.2" xref="S4.SS1.p1.2.m2.1.1.3.2.cmml">â„</mi><mrow id="S4.SS1.p1.2.m2.1.1.3.3" xref="S4.SS1.p1.2.m2.1.1.3.3.cmml"><mi id="S4.SS1.p1.2.m2.1.1.3.3.2" xref="S4.SS1.p1.2.m2.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.2.m2.1.1.3.3.1" xref="S4.SS1.p1.2.m2.1.1.3.3.1.cmml">Ã—</mo><mi id="S4.SS1.p1.2.m2.1.1.3.3.3" xref="S4.SS1.p1.2.m2.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><in id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1.1"></in><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">ğ’</ci><apply id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.3.1.cmml" xref="S4.SS1.p1.2.m2.1.1.3">superscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.3.2.cmml" xref="S4.SS1.p1.2.m2.1.1.3.2">â„</ci><apply id="S4.SS1.p1.2.m2.1.1.3.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3"><times id="S4.SS1.p1.2.m2.1.1.3.3.1.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3.1"></times><ci id="S4.SS1.p1.2.m2.1.1.3.3.2.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3.2">ğ‘›</ci><ci id="S4.SS1.p1.2.m2.1.1.3.3.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3.3.3">ğ‘‘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\boldsymbol{Z}\in\mathbb{R}^{n\times d}</annotation></semantics></math>, MHSA projects <math id="S4.SS1.p1.3.m3.1" class="ltx_Math" alttext="\boldsymbol{Z}" display="inline"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml">ğ’</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">ğ’</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">\boldsymbol{Z}</annotation></semantics></math> into <math id="S4.SS1.p1.4.m4.1" class="ltx_Math" alttext="\boldsymbol{Q}\in\mathbb{R}^{n\times d_{k}}" display="inline"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml">ğ‘¸</mi><mo id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.p1.4.m4.1.1.3" xref="S4.SS1.p1.4.m4.1.1.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.2" xref="S4.SS1.p1.4.m4.1.1.3.2.cmml">â„</mi><mrow id="S4.SS1.p1.4.m4.1.1.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.3.2" xref="S4.SS1.p1.4.m4.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.4.m4.1.1.3.3.1" xref="S4.SS1.p1.4.m4.1.1.3.3.1.cmml">Ã—</mo><msub id="S4.SS1.p1.4.m4.1.1.3.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.3.3.3.2" xref="S4.SS1.p1.4.m4.1.1.3.3.3.2.cmml">d</mi><mi id="S4.SS1.p1.4.m4.1.1.3.3.3.3" xref="S4.SS1.p1.4.m4.1.1.3.3.3.3.cmml">k</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><in id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></in><ci id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2">ğ‘¸</ci><apply id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.2">â„</ci><apply id="S4.SS1.p1.4.m4.1.1.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3"><times id="S4.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.1"></times><ci id="S4.SS1.p1.4.m4.1.1.3.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.2">ğ‘›</ci><apply id="S4.SS1.p1.4.m4.1.1.3.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.3.3.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.3.3.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.3.2">ğ‘‘</ci><ci id="S4.SS1.p1.4.m4.1.1.3.3.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3.3.3.3">ğ‘˜</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\boldsymbol{Q}\in\mathbb{R}^{n\times d_{k}}</annotation></semantics></math>, keys <math id="S4.SS1.p1.5.m5.1" class="ltx_Math" alttext="\boldsymbol{K}\in\mathbb{R}^{n\times d_{k}}" display="inline"><semantics id="S4.SS1.p1.5.m5.1a"><mrow id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml"><mi id="S4.SS1.p1.5.m5.1.1.2" xref="S4.SS1.p1.5.m5.1.1.2.cmml">ğ‘²</mi><mo id="S4.SS1.p1.5.m5.1.1.1" xref="S4.SS1.p1.5.m5.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.p1.5.m5.1.1.3" xref="S4.SS1.p1.5.m5.1.1.3.cmml"><mi id="S4.SS1.p1.5.m5.1.1.3.2" xref="S4.SS1.p1.5.m5.1.1.3.2.cmml">â„</mi><mrow id="S4.SS1.p1.5.m5.1.1.3.3" xref="S4.SS1.p1.5.m5.1.1.3.3.cmml"><mi id="S4.SS1.p1.5.m5.1.1.3.3.2" xref="S4.SS1.p1.5.m5.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.5.m5.1.1.3.3.1" xref="S4.SS1.p1.5.m5.1.1.3.3.1.cmml">Ã—</mo><msub id="S4.SS1.p1.5.m5.1.1.3.3.3" xref="S4.SS1.p1.5.m5.1.1.3.3.3.cmml"><mi id="S4.SS1.p1.5.m5.1.1.3.3.3.2" xref="S4.SS1.p1.5.m5.1.1.3.3.3.2.cmml">d</mi><mi id="S4.SS1.p1.5.m5.1.1.3.3.3.3" xref="S4.SS1.p1.5.m5.1.1.3.3.3.3.cmml">k</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"><in id="S4.SS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1.1"></in><ci id="S4.SS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2">ğ‘²</ci><apply id="S4.SS1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.1.1.3.1.cmml" xref="S4.SS1.p1.5.m5.1.1.3">superscript</csymbol><ci id="S4.SS1.p1.5.m5.1.1.3.2.cmml" xref="S4.SS1.p1.5.m5.1.1.3.2">â„</ci><apply id="S4.SS1.p1.5.m5.1.1.3.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3"><times id="S4.SS1.p1.5.m5.1.1.3.3.1.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.1"></times><ci id="S4.SS1.p1.5.m5.1.1.3.3.2.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.2">ğ‘›</ci><apply id="S4.SS1.p1.5.m5.1.1.3.3.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.1.1.3.3.3.1.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.5.m5.1.1.3.3.3.2.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.3.2">ğ‘‘</ci><ci id="S4.SS1.p1.5.m5.1.1.3.3.3.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.3.3">ğ‘˜</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">\boldsymbol{K}\in\mathbb{R}^{n\times d_{k}}</annotation></semantics></math> and values <math id="S4.SS1.p1.6.m6.1" class="ltx_Math" alttext="\boldsymbol{V}\in\mathbb{R}^{n\times d_{v}}" display="inline"><semantics id="S4.SS1.p1.6.m6.1a"><mrow id="S4.SS1.p1.6.m6.1.1" xref="S4.SS1.p1.6.m6.1.1.cmml"><mi id="S4.SS1.p1.6.m6.1.1.2" xref="S4.SS1.p1.6.m6.1.1.2.cmml">ğ‘½</mi><mo id="S4.SS1.p1.6.m6.1.1.1" xref="S4.SS1.p1.6.m6.1.1.1.cmml">âˆˆ</mo><msup id="S4.SS1.p1.6.m6.1.1.3" xref="S4.SS1.p1.6.m6.1.1.3.cmml"><mi id="S4.SS1.p1.6.m6.1.1.3.2" xref="S4.SS1.p1.6.m6.1.1.3.2.cmml">â„</mi><mrow id="S4.SS1.p1.6.m6.1.1.3.3" xref="S4.SS1.p1.6.m6.1.1.3.3.cmml"><mi id="S4.SS1.p1.6.m6.1.1.3.3.2" xref="S4.SS1.p1.6.m6.1.1.3.3.2.cmml">n</mi><mo lspace="0.222em" rspace="0.222em" id="S4.SS1.p1.6.m6.1.1.3.3.1" xref="S4.SS1.p1.6.m6.1.1.3.3.1.cmml">Ã—</mo><msub id="S4.SS1.p1.6.m6.1.1.3.3.3" xref="S4.SS1.p1.6.m6.1.1.3.3.3.cmml"><mi id="S4.SS1.p1.6.m6.1.1.3.3.3.2" xref="S4.SS1.p1.6.m6.1.1.3.3.3.2.cmml">d</mi><mi id="S4.SS1.p1.6.m6.1.1.3.3.3.3" xref="S4.SS1.p1.6.m6.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.6.m6.1b"><apply id="S4.SS1.p1.6.m6.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1"><in id="S4.SS1.p1.6.m6.1.1.1.cmml" xref="S4.SS1.p1.6.m6.1.1.1"></in><ci id="S4.SS1.p1.6.m6.1.1.2.cmml" xref="S4.SS1.p1.6.m6.1.1.2">ğ‘½</ci><apply id="S4.SS1.p1.6.m6.1.1.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.3.1.cmml" xref="S4.SS1.p1.6.m6.1.1.3">superscript</csymbol><ci id="S4.SS1.p1.6.m6.1.1.3.2.cmml" xref="S4.SS1.p1.6.m6.1.1.3.2">â„</ci><apply id="S4.SS1.p1.6.m6.1.1.3.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3.3"><times id="S4.SS1.p1.6.m6.1.1.3.3.1.cmml" xref="S4.SS1.p1.6.m6.1.1.3.3.1"></times><ci id="S4.SS1.p1.6.m6.1.1.3.3.2.cmml" xref="S4.SS1.p1.6.m6.1.1.3.3.2">ğ‘›</ci><apply id="S4.SS1.p1.6.m6.1.1.3.3.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3.3.3"><csymbol cd="ambiguous" id="S4.SS1.p1.6.m6.1.1.3.3.3.1.cmml" xref="S4.SS1.p1.6.m6.1.1.3.3.3">subscript</csymbol><ci id="S4.SS1.p1.6.m6.1.1.3.3.3.2.cmml" xref="S4.SS1.p1.6.m6.1.1.3.3.3.2">ğ‘‘</ci><ci id="S4.SS1.p1.6.m6.1.1.3.3.3.3.cmml" xref="S4.SS1.p1.6.m6.1.1.3.3.3.3">ğ‘£</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.6.m6.1c">\boldsymbol{V}\in\mathbb{R}^{n\times d_{v}}</annotation></semantics></math> where <math id="S4.SS1.p1.7.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS1.p1.7.m7.1a"><mi id="S4.SS1.p1.7.m7.1.1" xref="S4.SS1.p1.7.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.7.m7.1b"><ci id="S4.SS1.p1.7.m7.1.1.cmml" xref="S4.SS1.p1.7.m7.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.7.m7.1c">n</annotation></semantics></math> is the number of tokens and <math id="S4.SS1.p1.8.m8.3" class="ltx_Math" alttext="d,d_{k},d_{v}" display="inline"><semantics id="S4.SS1.p1.8.m8.3a"><mrow id="S4.SS1.p1.8.m8.3.3.2" xref="S4.SS1.p1.8.m8.3.3.3.cmml"><mi id="S4.SS1.p1.8.m8.1.1" xref="S4.SS1.p1.8.m8.1.1.cmml">d</mi><mo id="S4.SS1.p1.8.m8.3.3.2.3" xref="S4.SS1.p1.8.m8.3.3.3.cmml">,</mo><msub id="S4.SS1.p1.8.m8.2.2.1.1" xref="S4.SS1.p1.8.m8.2.2.1.1.cmml"><mi id="S4.SS1.p1.8.m8.2.2.1.1.2" xref="S4.SS1.p1.8.m8.2.2.1.1.2.cmml">d</mi><mi id="S4.SS1.p1.8.m8.2.2.1.1.3" xref="S4.SS1.p1.8.m8.2.2.1.1.3.cmml">k</mi></msub><mo id="S4.SS1.p1.8.m8.3.3.2.4" xref="S4.SS1.p1.8.m8.3.3.3.cmml">,</mo><msub id="S4.SS1.p1.8.m8.3.3.2.2" xref="S4.SS1.p1.8.m8.3.3.2.2.cmml"><mi id="S4.SS1.p1.8.m8.3.3.2.2.2" xref="S4.SS1.p1.8.m8.3.3.2.2.2.cmml">d</mi><mi id="S4.SS1.p1.8.m8.3.3.2.2.3" xref="S4.SS1.p1.8.m8.3.3.2.2.3.cmml">v</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.8.m8.3b"><list id="S4.SS1.p1.8.m8.3.3.3.cmml" xref="S4.SS1.p1.8.m8.3.3.2"><ci id="S4.SS1.p1.8.m8.1.1.cmml" xref="S4.SS1.p1.8.m8.1.1">ğ‘‘</ci><apply id="S4.SS1.p1.8.m8.2.2.1.1.cmml" xref="S4.SS1.p1.8.m8.2.2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.8.m8.2.2.1.1.1.cmml" xref="S4.SS1.p1.8.m8.2.2.1.1">subscript</csymbol><ci id="S4.SS1.p1.8.m8.2.2.1.1.2.cmml" xref="S4.SS1.p1.8.m8.2.2.1.1.2">ğ‘‘</ci><ci id="S4.SS1.p1.8.m8.2.2.1.1.3.cmml" xref="S4.SS1.p1.8.m8.2.2.1.1.3">ğ‘˜</ci></apply><apply id="S4.SS1.p1.8.m8.3.3.2.2.cmml" xref="S4.SS1.p1.8.m8.3.3.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.8.m8.3.3.2.2.1.cmml" xref="S4.SS1.p1.8.m8.3.3.2.2">subscript</csymbol><ci id="S4.SS1.p1.8.m8.3.3.2.2.2.cmml" xref="S4.SS1.p1.8.m8.3.3.2.2.2">ğ‘‘</ci><ci id="S4.SS1.p1.8.m8.3.3.2.2.3.cmml" xref="S4.SS1.p1.8.m8.3.3.2.2.3">ğ‘£</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.8.m8.3c">d,d_{k},d_{v}</annotation></semantics></math> are the dimension of the feature, key/query and value.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.2" class="ltx_p">The attention is computed through the following equation:</p>
<table id="S4.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E1.m1.4" class="ltx_Math" alttext="\text{Attention}(\boldsymbol{Q};\boldsymbol{K};\boldsymbol{V})=\text{softmax}(\frac{\boldsymbol{Q}\boldsymbol{K}^{T}}{\sqrt{d_{k}}}\boldsymbol{V})." display="block"><semantics id="S4.E1.m1.4a"><mrow id="S4.E1.m1.4.4.1" xref="S4.E1.m1.4.4.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1" xref="S4.E1.m1.4.4.1.1.cmml"><mrow id="S4.E1.m1.4.4.1.1.3" xref="S4.E1.m1.4.4.1.1.3.cmml"><mtext id="S4.E1.m1.4.4.1.1.3.2" xref="S4.E1.m1.4.4.1.1.3.2a.cmml">Attention</mtext><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.3.1" xref="S4.E1.m1.4.4.1.1.3.1.cmml">â€‹</mo><mrow id="S4.E1.m1.4.4.1.1.3.3.2" xref="S4.E1.m1.4.4.1.1.3.3.1.cmml"><mo stretchy="false" id="S4.E1.m1.4.4.1.1.3.3.2.1" xref="S4.E1.m1.4.4.1.1.3.3.1.cmml">(</mo><mi id="S4.E1.m1.1.1" xref="S4.E1.m1.1.1.cmml">ğ‘¸</mi><mo id="S4.E1.m1.4.4.1.1.3.3.2.2" xref="S4.E1.m1.4.4.1.1.3.3.1.cmml">;</mo><mi id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml">ğ‘²</mi><mo id="S4.E1.m1.4.4.1.1.3.3.2.3" xref="S4.E1.m1.4.4.1.1.3.3.1.cmml">;</mo><mi id="S4.E1.m1.3.3" xref="S4.E1.m1.3.3.cmml">ğ‘½</mi><mo stretchy="false" id="S4.E1.m1.4.4.1.1.3.3.2.4" xref="S4.E1.m1.4.4.1.1.3.3.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.4.4.1.1.2" xref="S4.E1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S4.E1.m1.4.4.1.1.1" xref="S4.E1.m1.4.4.1.1.1.cmml"><mtext id="S4.E1.m1.4.4.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.3a.cmml">softmax</mtext><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E1.m1.4.4.1.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.4.4.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml"><mfrac id="S4.E1.m1.4.4.1.1.1.1.1.1.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.cmml"><mrow id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml">ğ‘¸</mi><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.1.cmml">â€‹</mo><msup id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.2.cmml">ğ‘²</mi><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.cmml"><msub id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.cmml"><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.2" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.2.cmml">d</mi><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo lspace="0em" rspace="0em" id="S4.E1.m1.4.4.1.1.1.1.1.1.1" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1.cmml">â€‹</mo><mi id="S4.E1.m1.4.4.1.1.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3.cmml">ğ‘½</mi></mrow><mo stretchy="false" id="S4.E1.m1.4.4.1.1.1.1.1.3" xref="S4.E1.m1.4.4.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E1.m1.4.4.1.2" xref="S4.E1.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.4b"><apply id="S4.E1.m1.4.4.1.1.cmml" xref="S4.E1.m1.4.4.1"><eq id="S4.E1.m1.4.4.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.2"></eq><apply id="S4.E1.m1.4.4.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.3"><times id="S4.E1.m1.4.4.1.1.3.1.cmml" xref="S4.E1.m1.4.4.1.1.3.1"></times><ci id="S4.E1.m1.4.4.1.1.3.2a.cmml" xref="S4.E1.m1.4.4.1.1.3.2"><mtext id="S4.E1.m1.4.4.1.1.3.2.cmml" xref="S4.E1.m1.4.4.1.1.3.2">Attention</mtext></ci><list id="S4.E1.m1.4.4.1.1.3.3.1.cmml" xref="S4.E1.m1.4.4.1.1.3.3.2"><ci id="S4.E1.m1.1.1.cmml" xref="S4.E1.m1.1.1">ğ‘¸</ci><ci id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2">ğ‘²</ci><ci id="S4.E1.m1.3.3.cmml" xref="S4.E1.m1.3.3">ğ‘½</ci></list></apply><apply id="S4.E1.m1.4.4.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1"><times id="S4.E1.m1.4.4.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.2"></times><ci id="S4.E1.m1.4.4.1.1.1.3a.cmml" xref="S4.E1.m1.4.4.1.1.1.3"><mtext id="S4.E1.m1.4.4.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.3">softmax</mtext></ci><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1"><times id="S4.E1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.1"></times><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2"><divide id="S4.E1.m1.4.4.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2"></divide><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2"><times id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.1"></times><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.2">ğ‘¸</ci><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3">superscript</csymbol><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.2">ğ‘²</ci><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.2.3.3">ğ‘‡</ci></apply></apply><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3"><root id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3a.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3"></root><apply id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.1.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2">subscript</csymbol><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.2.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.2">ğ‘‘</ci><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.2.3.2.3">ğ‘˜</ci></apply></apply></apply><ci id="S4.E1.m1.4.4.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.4.4.1.1.1.1.1.1.3">ğ‘½</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.4c">\text{Attention}(\boldsymbol{Q};\boldsymbol{K};\boldsymbol{V})=\text{softmax}(\frac{\boldsymbol{Q}\boldsymbol{K}^{T}}{\sqrt{d_{k}}}\boldsymbol{V}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p2.1" class="ltx_p">Combined with LN and MLP, the overall equations for the transformer encoder with <math id="S4.SS1.p2.1.m1.1" class="ltx_Math" alttext="L" display="inline"><semantics id="S4.SS1.p2.1.m1.1a"><mi id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><ci id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">L</annotation></semantics></math> transformer blocks are</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<table id="S6.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S4.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m1.1" class="ltx_Math" alttext="\displaystyle z^{\prime}_{l}" display="inline"><semantics id="S4.E2.m1.1a"><msubsup id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mi id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">z</mi><mi id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml">l</mi><mo id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml">â€²</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1">subscript</csymbol><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1">superscript</csymbol><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">ğ‘§</ci><ci id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3">â€²</ci></apply><ci id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">\displaystyle z^{\prime}_{l}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E2.m2.1" class="ltx_Math" alttext="\displaystyle=\text{MSA}(\text{LN}(z_{l-1}))+z_{l-1}," display="inline"><semantics id="S4.E2.m2.1a"><mrow id="S4.E2.m2.1.1.1" xref="S4.E2.m2.1.1.1.1.cmml"><mrow id="S4.E2.m2.1.1.1.1" xref="S4.E2.m2.1.1.1.1.cmml"><mi id="S4.E2.m2.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.3.cmml"></mi><mo id="S4.E2.m2.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.2.cmml">=</mo><mrow id="S4.E2.m2.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.cmml"><mrow id="S4.E2.m2.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.cmml"><mtext id="S4.E2.m2.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.3a.cmml">MSA</mtext><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E2.m2.1.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E2.m2.1.1.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S4.E2.m2.1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.3a.cmml">LN</mtext><mo lspace="0em" rspace="0em" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mrow id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mo id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.1" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml">âˆ’</mo><mn id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E2.m2.1.1.1.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E2.m2.1.1.1.1.1.2" xref="S4.E2.m2.1.1.1.1.1.2.cmml">+</mo><msub id="S4.E2.m2.1.1.1.1.1.3" xref="S4.E2.m2.1.1.1.1.1.3.cmml"><mi id="S4.E2.m2.1.1.1.1.1.3.2" xref="S4.E2.m2.1.1.1.1.1.3.2.cmml">z</mi><mrow id="S4.E2.m2.1.1.1.1.1.3.3" xref="S4.E2.m2.1.1.1.1.1.3.3.cmml"><mi id="S4.E2.m2.1.1.1.1.1.3.3.2" xref="S4.E2.m2.1.1.1.1.1.3.3.2.cmml">l</mi><mo id="S4.E2.m2.1.1.1.1.1.3.3.1" xref="S4.E2.m2.1.1.1.1.1.3.3.1.cmml">âˆ’</mo><mn id="S4.E2.m2.1.1.1.1.1.3.3.3" xref="S4.E2.m2.1.1.1.1.1.3.3.3.cmml">1</mn></mrow></msub></mrow></mrow><mo id="S4.E2.m2.1.1.1.2" xref="S4.E2.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m2.1b"><apply id="S4.E2.m2.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1"><eq id="S4.E2.m2.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S4.E2.m2.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.3">absent</csymbol><apply id="S4.E2.m2.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1"><plus id="S4.E2.m2.1.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.1.2"></plus><apply id="S4.E2.m2.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1"><times id="S4.E2.m2.1.1.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.2"></times><ci id="S4.E2.m2.1.1.1.1.1.1.3a.cmml" xref="S4.E2.m2.1.1.1.1.1.1.3"><mtext id="S4.E2.m2.1.1.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.3">MSA</mtext></ci><apply id="S4.E2.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1"><times id="S4.E2.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E2.m2.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.3"><mtext id="S4.E2.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.3">LN</mtext></ci><apply id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘§</ci><apply id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3"><minus id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.1"></minus><ci id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.2">ğ‘™</ci><cn type="integer" id="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E2.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply><apply id="S4.E2.m2.1.1.1.1.1.3.cmml" xref="S4.E2.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m2.1.1.1.1.1.3.1.cmml" xref="S4.E2.m2.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m2.1.1.1.1.1.3.2.cmml" xref="S4.E2.m2.1.1.1.1.1.3.2">ğ‘§</ci><apply id="S4.E2.m2.1.1.1.1.1.3.3.cmml" xref="S4.E2.m2.1.1.1.1.1.3.3"><minus id="S4.E2.m2.1.1.1.1.1.3.3.1.cmml" xref="S4.E2.m2.1.1.1.1.1.3.3.1"></minus><ci id="S4.E2.m2.1.1.1.1.1.3.3.2.cmml" xref="S4.E2.m2.1.1.1.1.1.3.3.2">ğ‘™</ci><cn type="integer" id="S4.E2.m2.1.1.1.1.1.3.3.3.cmml" xref="S4.E2.m2.1.1.1.1.1.3.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m2.1c">\displaystyle=\text{MSA}(\text{LN}(z_{l-1}))+z_{l-1},</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E2.m3.1" class="ltx_Math" alttext="\displaystyle l=1...L," display="inline"><semantics id="S4.E2.m3.1a"><mrow id="S4.E2.m3.1.1.1" xref="S4.E2.m3.1.1.1.1.cmml"><mrow id="S4.E2.m3.1.1.1.1" xref="S4.E2.m3.1.1.1.1.cmml"><mi id="S4.E2.m3.1.1.1.1.2" xref="S4.E2.m3.1.1.1.1.2.cmml">l</mi><mo id="S4.E2.m3.1.1.1.1.1" xref="S4.E2.m3.1.1.1.1.1.cmml">=</mo><mrow id="S4.E2.m3.1.1.1.1.3" xref="S4.E2.m3.1.1.1.1.3.cmml"><mn id="S4.E2.m3.1.1.1.1.3.2" xref="S4.E2.m3.1.1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.E2.m3.1.1.1.1.3.1" xref="S4.E2.m3.1.1.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S4.E2.m3.1.1.1.1.3.3" xref="S4.E2.m3.1.1.1.1.3.3.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S4.E2.m3.1.1.1.1.3.1a" xref="S4.E2.m3.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E2.m3.1.1.1.1.3.4" xref="S4.E2.m3.1.1.1.1.3.4.cmml">L</mi></mrow></mrow><mo id="S4.E2.m3.1.1.1.2" xref="S4.E2.m3.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m3.1b"><apply id="S4.E2.m3.1.1.1.1.cmml" xref="S4.E2.m3.1.1.1"><eq id="S4.E2.m3.1.1.1.1.1.cmml" xref="S4.E2.m3.1.1.1.1.1"></eq><ci id="S4.E2.m3.1.1.1.1.2.cmml" xref="S4.E2.m3.1.1.1.1.2">ğ‘™</ci><apply id="S4.E2.m3.1.1.1.1.3.cmml" xref="S4.E2.m3.1.1.1.1.3"><times id="S4.E2.m3.1.1.1.1.3.1.cmml" xref="S4.E2.m3.1.1.1.1.3.1"></times><cn type="integer" id="S4.E2.m3.1.1.1.1.3.2.cmml" xref="S4.E2.m3.1.1.1.1.3.2">1</cn><ci id="S4.E2.m3.1.1.1.1.3.3.cmml" xref="S4.E2.m3.1.1.1.1.3.3">â€¦</ci><ci id="S4.E2.m3.1.1.1.1.3.4.cmml" xref="S4.E2.m3.1.1.1.1.3.4">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m3.1c">\displaystyle l=1...L,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
<tbody id="S4.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E3.m1.1" class="ltx_Math" alttext="\displaystyle z_{l}" display="inline"><semantics id="S4.E3.m1.1a"><msub id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mi id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml">z</mi><mi id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><csymbol cd="ambiguous" id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1">subscript</csymbol><ci id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2">ğ‘§</ci><ci id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3">ğ‘™</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\displaystyle z_{l}</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E3.m2.1" class="ltx_Math" alttext="\displaystyle=\text{MLP}(\text{LN}(z^{\prime}_{l}))+z^{\prime}_{l}," display="inline"><semantics id="S4.E3.m2.1a"><mrow id="S4.E3.m2.1.1.1" xref="S4.E3.m2.1.1.1.1.cmml"><mrow id="S4.E3.m2.1.1.1.1" xref="S4.E3.m2.1.1.1.1.cmml"><mi id="S4.E3.m2.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.3.cmml"></mi><mo id="S4.E3.m2.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.2.cmml">=</mo><mrow id="S4.E3.m2.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.cmml"><mrow id="S4.E3.m2.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.cmml"><mtext id="S4.E3.m2.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m2.1.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E3.m2.1.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m2.1.1.1.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E3.m2.1.1.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml"><mtext id="S4.E3.m2.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.3a.cmml">LN</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">z</mi><mi id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">l</mi><mo id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">â€²</mo></msubsup><mo stretchy="false" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E3.m2.1.1.1.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E3.m2.1.1.1.1.1.2" xref="S4.E3.m2.1.1.1.1.1.2.cmml">+</mo><msubsup id="S4.E3.m2.1.1.1.1.1.3" xref="S4.E3.m2.1.1.1.1.1.3.cmml"><mi id="S4.E3.m2.1.1.1.1.1.3.2.2" xref="S4.E3.m2.1.1.1.1.1.3.2.2.cmml">z</mi><mi id="S4.E3.m2.1.1.1.1.1.3.3" xref="S4.E3.m2.1.1.1.1.1.3.3.cmml">l</mi><mo id="S4.E3.m2.1.1.1.1.1.3.2.3" xref="S4.E3.m2.1.1.1.1.1.3.2.3.cmml">â€²</mo></msubsup></mrow></mrow><mo id="S4.E3.m2.1.1.1.2" xref="S4.E3.m2.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m2.1b"><apply id="S4.E3.m2.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1"><eq id="S4.E3.m2.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S4.E3.m2.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.3">absent</csymbol><apply id="S4.E3.m2.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1"><plus id="S4.E3.m2.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.2"></plus><apply id="S4.E3.m2.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1"><times id="S4.E3.m2.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.2"></times><ci id="S4.E3.m2.1.1.1.1.1.1.3a.cmml" xref="S4.E3.m2.1.1.1.1.1.1.3"><mtext id="S4.E3.m2.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.3">MLP</mtext></ci><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1"><times id="S4.E3.m2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.2"></times><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.3a.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.3"><mtext id="S4.E3.m2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.3">LN</mtext></ci><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.2">ğ‘§</ci><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.2.3">â€²</ci></apply><ci id="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.1.1.1.1.1.1.1.3">ğ‘™</ci></apply></apply></apply><apply id="S4.E3.m2.1.1.1.1.1.3.cmml" xref="S4.E3.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.3.1.cmml" xref="S4.E3.m2.1.1.1.1.1.3">subscript</csymbol><apply id="S4.E3.m2.1.1.1.1.1.3.2.cmml" xref="S4.E3.m2.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3.m2.1.1.1.1.1.3.2.1.cmml" xref="S4.E3.m2.1.1.1.1.1.3">superscript</csymbol><ci id="S4.E3.m2.1.1.1.1.1.3.2.2.cmml" xref="S4.E3.m2.1.1.1.1.1.3.2.2">ğ‘§</ci><ci id="S4.E3.m2.1.1.1.1.1.3.2.3.cmml" xref="S4.E3.m2.1.1.1.1.1.3.2.3">â€²</ci></apply><ci id="S4.E3.m2.1.1.1.1.1.3.3.cmml" xref="S4.E3.m2.1.1.1.1.1.3.3">ğ‘™</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m2.1c">\displaystyle=\text{MLP}(\text{LN}(z^{\prime}_{l}))+z^{\prime}_{l},</annotation></semantics></math></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E3.m3.1" class="ltx_Math" alttext="\displaystyle l=1...L," display="inline"><semantics id="S4.E3.m3.1a"><mrow id="S4.E3.m3.1.1.1" xref="S4.E3.m3.1.1.1.1.cmml"><mrow id="S4.E3.m3.1.1.1.1" xref="S4.E3.m3.1.1.1.1.cmml"><mi id="S4.E3.m3.1.1.1.1.2" xref="S4.E3.m3.1.1.1.1.2.cmml">l</mi><mo id="S4.E3.m3.1.1.1.1.1" xref="S4.E3.m3.1.1.1.1.1.cmml">=</mo><mrow id="S4.E3.m3.1.1.1.1.3" xref="S4.E3.m3.1.1.1.1.3.cmml"><mn id="S4.E3.m3.1.1.1.1.3.2" xref="S4.E3.m3.1.1.1.1.3.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.E3.m3.1.1.1.1.3.1" xref="S4.E3.m3.1.1.1.1.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S4.E3.m3.1.1.1.1.3.3" xref="S4.E3.m3.1.1.1.1.3.3.cmml">â€¦</mi><mo lspace="0em" rspace="0em" id="S4.E3.m3.1.1.1.1.3.1a" xref="S4.E3.m3.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E3.m3.1.1.1.1.3.4" xref="S4.E3.m3.1.1.1.1.3.4.cmml">L</mi></mrow></mrow><mo id="S4.E3.m3.1.1.1.2" xref="S4.E3.m3.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m3.1b"><apply id="S4.E3.m3.1.1.1.1.cmml" xref="S4.E3.m3.1.1.1"><eq id="S4.E3.m3.1.1.1.1.1.cmml" xref="S4.E3.m3.1.1.1.1.1"></eq><ci id="S4.E3.m3.1.1.1.1.2.cmml" xref="S4.E3.m3.1.1.1.1.2">ğ‘™</ci><apply id="S4.E3.m3.1.1.1.1.3.cmml" xref="S4.E3.m3.1.1.1.1.3"><times id="S4.E3.m3.1.1.1.1.3.1.cmml" xref="S4.E3.m3.1.1.1.1.3.1"></times><cn type="integer" id="S4.E3.m3.1.1.1.1.3.2.cmml" xref="S4.E3.m3.1.1.1.1.3.2">1</cn><ci id="S4.E3.m3.1.1.1.1.3.3.cmml" xref="S4.E3.m3.1.1.1.1.3.3">â€¦</ci><ci id="S4.E3.m3.1.1.1.1.3.4.cmml" xref="S4.E3.m3.1.1.1.1.3.4">ğ¿</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m3.1c">\displaystyle l=1...L,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
<tbody id="S4.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S4.E4.m1.1" class="ltx_Math" alttext="\displaystyle y" display="inline"><semantics id="S4.E4.m1.1a"><mi id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">ğ‘¦</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">\displaystyle y</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S4.E4.m2.1" class="ltx_Math" alttext="\displaystyle=\text{LN}(z_{L})." display="inline"><semantics id="S4.E4.m2.1a"><mrow id="S4.E4.m2.1.1.1" xref="S4.E4.m2.1.1.1.1.cmml"><mrow id="S4.E4.m2.1.1.1.1" xref="S4.E4.m2.1.1.1.1.cmml"><mi id="S4.E4.m2.1.1.1.1.3" xref="S4.E4.m2.1.1.1.1.3.cmml"></mi><mo id="S4.E4.m2.1.1.1.1.2" xref="S4.E4.m2.1.1.1.1.2.cmml">=</mo><mrow id="S4.E4.m2.1.1.1.1.1" xref="S4.E4.m2.1.1.1.1.1.cmml"><mtext id="S4.E4.m2.1.1.1.1.1.3" xref="S4.E4.m2.1.1.1.1.1.3a.cmml">LN</mtext><mo lspace="0em" rspace="0em" id="S4.E4.m2.1.1.1.1.1.2" xref="S4.E4.m2.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E4.m2.1.1.1.1.1.1.1" xref="S4.E4.m2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E4.m2.1.1.1.1.1.1.1.2" xref="S4.E4.m2.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E4.m2.1.1.1.1.1.1.1.1" xref="S4.E4.m2.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E4.m2.1.1.1.1.1.1.1.1.2" xref="S4.E4.m2.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S4.E4.m2.1.1.1.1.1.1.1.1.3" xref="S4.E4.m2.1.1.1.1.1.1.1.1.3.cmml">L</mi></msub><mo stretchy="false" id="S4.E4.m2.1.1.1.1.1.1.1.3" xref="S4.E4.m2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E4.m2.1.1.1.2" xref="S4.E4.m2.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m2.1b"><apply id="S4.E4.m2.1.1.1.1.cmml" xref="S4.E4.m2.1.1.1"><eq id="S4.E4.m2.1.1.1.1.2.cmml" xref="S4.E4.m2.1.1.1.1.2"></eq><csymbol cd="latexml" id="S4.E4.m2.1.1.1.1.3.cmml" xref="S4.E4.m2.1.1.1.1.3">absent</csymbol><apply id="S4.E4.m2.1.1.1.1.1.cmml" xref="S4.E4.m2.1.1.1.1.1"><times id="S4.E4.m2.1.1.1.1.1.2.cmml" xref="S4.E4.m2.1.1.1.1.1.2"></times><ci id="S4.E4.m2.1.1.1.1.1.3a.cmml" xref="S4.E4.m2.1.1.1.1.1.3"><mtext id="S4.E4.m2.1.1.1.1.1.3.cmml" xref="S4.E4.m2.1.1.1.1.1.3">LN</mtext></ci><apply id="S4.E4.m2.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E4.m2.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E4.m2.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.1.2">ğ‘§</ci><ci id="S4.E4.m2.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E4.m2.1.1.1.1.1.1.1.1.3">ğ¿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m2.1c">\displaystyle=\text{LN}(z_{L}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright" colspan="2"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p">Krafka et al. proposed iTracker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to estimate gaze by integrating the features of the head and eyes using several fully connected layers. To better fuse features, we propose the two-stage transformer-based gaze-feature fusion (TTGF) architecture shown in Fig. <a href="#S4.F1" title="Figure 1 â€£ 4 Method â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
This architecture applies three transformer encoders to fuse the features from the head and eye images in two fusion steps, 1) head-eye fusion and 2) left-right fusion. The idea of using two-step fusion is based on the intuition combining information of the head and one eye enable rough inference of the personâ€™s gaze direction. The second step combines the two rough estimates into a single more precise estimate.</p>
</div>
<div id="S4.SS1.p5" class="ltx_para">
<p id="S4.SS1.p5.7" class="ltx_p">In our design, the architectures of all three fusion modules are identical. One TGF module accepts two gaze-related features and produces a fused feature. We describe the computation in ta TGF formally with the following equation:</p>
<table id="S4.E5" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E5.m1.1" class="ltx_Math" alttext="\text{TGF}(f^{\ast},f^{\dagger})=\text{CAT}(\text{FC}(\text{Trans}([f^{\ast};f^{\dagger}])))," display="block"><semantics id="S4.E5.m1.1a"><mrow id="S4.E5.m1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.2.cmml"><mtext id="S4.E5.m1.1.1.1.1.2.4" xref="S4.E5.m1.1.1.1.1.2.4a.cmml">TGF</mtext><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.2.3" xref="S4.E5.m1.1.1.1.1.2.3.cmml">â€‹</mo><mrow id="S4.E5.m1.1.1.1.1.2.2.2" xref="S4.E5.m1.1.1.1.1.2.2.3.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.2.2.2.3" xref="S4.E5.m1.1.1.1.1.2.2.3.cmml">(</mo><msup id="S4.E5.m1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mo id="S4.E5.m1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3.cmml">âˆ—</mo></msup><mo id="S4.E5.m1.1.1.1.1.2.2.2.4" xref="S4.E5.m1.1.1.1.1.2.2.3.cmml">,</mo><msup id="S4.E5.m1.1.1.1.1.2.2.2.2" xref="S4.E5.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S4.E5.m1.1.1.1.1.2.2.2.2.2" xref="S4.E5.m1.1.1.1.1.2.2.2.2.2.cmml">f</mi><mo id="S4.E5.m1.1.1.1.1.2.2.2.2.3" xref="S4.E5.m1.1.1.1.1.2.2.2.2.3.cmml">â€ </mo></msup><mo stretchy="false" id="S4.E5.m1.1.1.1.1.2.2.2.5" xref="S4.E5.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo id="S4.E5.m1.1.1.1.1.4" xref="S4.E5.m1.1.1.1.1.4.cmml">=</mo><mrow id="S4.E5.m1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.cmml"><mtext id="S4.E5.m1.1.1.1.1.3.3" xref="S4.E5.m1.1.1.1.1.3.3a.cmml">CAT</mtext><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.3.2" xref="S4.E5.m1.1.1.1.1.3.2.cmml">â€‹</mo><mrow id="S4.E5.m1.1.1.1.1.3.1.1" xref="S4.E5.m1.1.1.1.1.3.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.3.1.1.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.3.1.1.1" xref="S4.E5.m1.1.1.1.1.3.1.1.1.cmml"><mtext id="S4.E5.m1.1.1.1.1.3.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.1.1.1.3a.cmml">FC</mtext><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.3.1.1.1.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.cmml"><mtext id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.3a.cmml">Trans</mtext><mo lspace="0em" rspace="0em" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.3" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.3.cmml">[</mo><msup id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mo id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml">âˆ—</mo></msup><mo id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.4" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.3.cmml">;</mo><msup id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.2" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.2.cmml">f</mi><mo id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.3" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.3.cmml">â€ </mo></msup><mo stretchy="false" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.5" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S4.E5.m1.1.1.1.1.3.1.1.3" xref="S4.E5.m1.1.1.1.1.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.E5.m1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1"><eq id="S4.E5.m1.1.1.1.1.4.cmml" xref="S4.E5.m1.1.1.1.1.4"></eq><apply id="S4.E5.m1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.2"><times id="S4.E5.m1.1.1.1.1.2.3.cmml" xref="S4.E5.m1.1.1.1.1.2.3"></times><ci id="S4.E5.m1.1.1.1.1.2.4a.cmml" xref="S4.E5.m1.1.1.1.1.2.4"><mtext id="S4.E5.m1.1.1.1.1.2.4.cmml" xref="S4.E5.m1.1.1.1.1.2.4">TGF</mtext></ci><interval closure="open" id="S4.E5.m1.1.1.1.1.2.2.3.cmml" xref="S4.E5.m1.1.1.1.1.2.2.2"><apply id="S4.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.2">ğ‘“</ci><ci id="S4.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1.1.3">âˆ—</ci></apply><apply id="S4.E5.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.E5.m1.1.1.1.1.2.2.2.2">superscript</csymbol><ci id="S4.E5.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.2.2.2.2.2">ğ‘“</ci><ci id="S4.E5.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S4.E5.m1.1.1.1.1.2.2.2.2.3">â€ </ci></apply></interval></apply><apply id="S4.E5.m1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3"><times id="S4.E5.m1.1.1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.1.1.3.2"></times><ci id="S4.E5.m1.1.1.1.1.3.3a.cmml" xref="S4.E5.m1.1.1.1.1.3.3"><mtext id="S4.E5.m1.1.1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.1.1.3.3">CAT</mtext></ci><apply id="S4.E5.m1.1.1.1.1.3.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1"><times id="S4.E5.m1.1.1.1.1.3.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.2"></times><ci id="S4.E5.m1.1.1.1.1.3.1.1.1.3a.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.3"><mtext id="S4.E5.m1.1.1.1.1.3.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.3">FC</mtext></ci><apply id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1"><times id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.2"></times><ci id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.3a.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.3"><mtext id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.3">Trans</mtext></ci><list id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2"><apply id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.2">ğ‘“</ci><ci id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.1.1.3">âˆ—</ci></apply><apply id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.2">ğ‘“</ci><ci id="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E5.m1.1.1.1.1.3.1.1.1.1.1.1.1.1.1.2.2.3">â€ </ci></apply></list></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">\text{TGF}(f^{\ast},f^{\dagger})=\text{CAT}(\text{FC}(\text{Trans}([f^{\ast};f^{\dagger}]))),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p5.6" class="ltx_p">where <math id="S4.SS1.p5.1.m1.1" class="ltx_Math" alttext="\text{Trans}([f^{\ast};f^{\dagger}])" display="inline"><semantics id="S4.SS1.p5.1.m1.1a"><mrow id="S4.SS1.p5.1.m1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mtext id="S4.SS1.p5.1.m1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.3a.cmml">Trans</mtext><mo lspace="0em" rspace="0em" id="S4.SS1.p5.1.m1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS1.p5.1.m1.1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.cmml"><mo stretchy="false" id="S4.SS1.p5.1.m1.1.1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.cmml">(</mo><mrow id="S4.SS1.p5.1.m1.1.1.1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.1.1.1.3.cmml"><mo stretchy="false" id="S4.SS1.p5.1.m1.1.1.1.1.1.2.3" xref="S4.SS1.p5.1.m1.1.1.1.1.1.3.cmml">[</mo><msup id="S4.SS1.p5.1.m1.1.1.1.1.1.1.1" xref="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.2" xref="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.2.cmml">f</mi><mo id="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.3.cmml">âˆ—</mo></msup><mo id="S4.SS1.p5.1.m1.1.1.1.1.1.2.4" xref="S4.SS1.p5.1.m1.1.1.1.1.1.3.cmml">;</mo><msup id="S4.SS1.p5.1.m1.1.1.1.1.1.2.2" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.cmml"><mi id="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.2" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.2.cmml">f</mi><mo id="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.3" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.3.cmml">â€ </mo></msup><mo stretchy="false" id="S4.SS1.p5.1.m1.1.1.1.1.1.2.5" xref="S4.SS1.p5.1.m1.1.1.1.1.1.3.cmml">]</mo></mrow><mo stretchy="false" id="S4.SS1.p5.1.m1.1.1.1.1.3" xref="S4.SS1.p5.1.m1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.1.m1.1b"><apply id="S4.SS1.p5.1.m1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1"><times id="S4.SS1.p5.1.m1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.2"></times><ci id="S4.SS1.p5.1.m1.1.1.3a.cmml" xref="S4.SS1.p5.1.m1.1.1.3"><mtext id="S4.SS1.p5.1.m1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.3">Trans</mtext></ci><list id="S4.SS1.p5.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2"><apply id="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.2">ğ‘“</ci><ci id="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.1.1.3">âˆ—</ci></apply><apply id="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.1.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2.2">superscript</csymbol><ci id="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.2.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.2">ğ‘“</ci><ci id="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.3.cmml" xref="S4.SS1.p5.1.m1.1.1.1.1.1.2.2.3">â€ </ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.1.m1.1c">\text{Trans}([f^{\ast};f^{\dagger}])</annotation></semantics></math> is the transformer used for fusing the head-eye features or eye-eye features, <span id="S4.SS1.p5.6.1" class="ltx_text ltx_markedasmath">FC</span> is a linear layer used to project the features to a specific size, and <span id="S4.SS1.p5.6.2" class="ltx_text ltx_markedasmath">CAT</span> concatenates the outputs of the transformer to generate fused features. In the head-eye fusion stage, each eye feature <math id="S4.SS1.p5.4.m4.1" class="ltx_Math" alttext="f^{le}" display="inline"><semantics id="S4.SS1.p5.4.m4.1a"><msup id="S4.SS1.p5.4.m4.1.1" xref="S4.SS1.p5.4.m4.1.1.cmml"><mi id="S4.SS1.p5.4.m4.1.1.2" xref="S4.SS1.p5.4.m4.1.1.2.cmml">f</mi><mrow id="S4.SS1.p5.4.m4.1.1.3" xref="S4.SS1.p5.4.m4.1.1.3.cmml"><mi id="S4.SS1.p5.4.m4.1.1.3.2" xref="S4.SS1.p5.4.m4.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p5.4.m4.1.1.3.1" xref="S4.SS1.p5.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p5.4.m4.1.1.3.3" xref="S4.SS1.p5.4.m4.1.1.3.3.cmml">e</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.4.m4.1b"><apply id="S4.SS1.p5.4.m4.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.4.m4.1.1.1.cmml" xref="S4.SS1.p5.4.m4.1.1">superscript</csymbol><ci id="S4.SS1.p5.4.m4.1.1.2.cmml" xref="S4.SS1.p5.4.m4.1.1.2">ğ‘“</ci><apply id="S4.SS1.p5.4.m4.1.1.3.cmml" xref="S4.SS1.p5.4.m4.1.1.3"><times id="S4.SS1.p5.4.m4.1.1.3.1.cmml" xref="S4.SS1.p5.4.m4.1.1.3.1"></times><ci id="S4.SS1.p5.4.m4.1.1.3.2.cmml" xref="S4.SS1.p5.4.m4.1.1.3.2">ğ‘™</ci><ci id="S4.SS1.p5.4.m4.1.1.3.3.cmml" xref="S4.SS1.p5.4.m4.1.1.3.3">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.4.m4.1c">f^{le}</annotation></semantics></math> or <math id="S4.SS1.p5.5.m5.1" class="ltx_Math" alttext="f^{re}" display="inline"><semantics id="S4.SS1.p5.5.m5.1a"><msup id="S4.SS1.p5.5.m5.1.1" xref="S4.SS1.p5.5.m5.1.1.cmml"><mi id="S4.SS1.p5.5.m5.1.1.2" xref="S4.SS1.p5.5.m5.1.1.2.cmml">f</mi><mrow id="S4.SS1.p5.5.m5.1.1.3" xref="S4.SS1.p5.5.m5.1.1.3.cmml"><mi id="S4.SS1.p5.5.m5.1.1.3.2" xref="S4.SS1.p5.5.m5.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p5.5.m5.1.1.3.1" xref="S4.SS1.p5.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p5.5.m5.1.1.3.3" xref="S4.SS1.p5.5.m5.1.1.3.3.cmml">e</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.5.m5.1b"><apply id="S4.SS1.p5.5.m5.1.1.cmml" xref="S4.SS1.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.5.m5.1.1.1.cmml" xref="S4.SS1.p5.5.m5.1.1">superscript</csymbol><ci id="S4.SS1.p5.5.m5.1.1.2.cmml" xref="S4.SS1.p5.5.m5.1.1.2">ğ‘“</ci><apply id="S4.SS1.p5.5.m5.1.1.3.cmml" xref="S4.SS1.p5.5.m5.1.1.3"><times id="S4.SS1.p5.5.m5.1.1.3.1.cmml" xref="S4.SS1.p5.5.m5.1.1.3.1"></times><ci id="S4.SS1.p5.5.m5.1.1.3.2.cmml" xref="S4.SS1.p5.5.m5.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.SS1.p5.5.m5.1.1.3.3.cmml" xref="S4.SS1.p5.5.m5.1.1.3.3">ğ‘’</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.5.m5.1c">f^{re}</annotation></semantics></math> is fused with the head feature <math id="S4.SS1.p5.6.m6.1" class="ltx_Math" alttext="f^{h}" display="inline"><semantics id="S4.SS1.p5.6.m6.1a"><msup id="S4.SS1.p5.6.m6.1.1" xref="S4.SS1.p5.6.m6.1.1.cmml"><mi id="S4.SS1.p5.6.m6.1.1.2" xref="S4.SS1.p5.6.m6.1.1.2.cmml">f</mi><mi id="S4.SS1.p5.6.m6.1.1.3" xref="S4.SS1.p5.6.m6.1.1.3.cmml">h</mi></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p5.6.m6.1b"><apply id="S4.SS1.p5.6.m6.1.1.cmml" xref="S4.SS1.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S4.SS1.p5.6.m6.1.1.1.cmml" xref="S4.SS1.p5.6.m6.1.1">superscript</csymbol><ci id="S4.SS1.p5.6.m6.1.1.2.cmml" xref="S4.SS1.p5.6.m6.1.1.2">ğ‘“</ci><ci id="S4.SS1.p5.6.m6.1.1.3.cmml" xref="S4.SS1.p5.6.m6.1.1.3">â„</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p5.6.m6.1c">f^{h}</annotation></semantics></math>:</p>
</div>
<div id="S4.SS1.p6" class="ltx_para">
<table id="S4.E6" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E6.m1.2" class="ltx_Math" alttext="f^{lh}=\text{TGF}^{lh}(f^{le},f^{h})" display="block"><semantics id="S4.E6.m1.2a"><mrow id="S4.E6.m1.2.2" xref="S4.E6.m1.2.2.cmml"><msup id="S4.E6.m1.2.2.4" xref="S4.E6.m1.2.2.4.cmml"><mi id="S4.E6.m1.2.2.4.2" xref="S4.E6.m1.2.2.4.2.cmml">f</mi><mrow id="S4.E6.m1.2.2.4.3" xref="S4.E6.m1.2.2.4.3.cmml"><mi id="S4.E6.m1.2.2.4.3.2" xref="S4.E6.m1.2.2.4.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.4.3.1" xref="S4.E6.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E6.m1.2.2.4.3.3" xref="S4.E6.m1.2.2.4.3.3.cmml">h</mi></mrow></msup><mo id="S4.E6.m1.2.2.3" xref="S4.E6.m1.2.2.3.cmml">=</mo><mrow id="S4.E6.m1.2.2.2" xref="S4.E6.m1.2.2.2.cmml"><msup id="S4.E6.m1.2.2.2.4" xref="S4.E6.m1.2.2.2.4.cmml"><mtext id="S4.E6.m1.2.2.2.4.2" xref="S4.E6.m1.2.2.2.4.2a.cmml">TGF</mtext><mrow id="S4.E6.m1.2.2.2.4.3" xref="S4.E6.m1.2.2.2.4.3.cmml"><mi id="S4.E6.m1.2.2.2.4.3.2" xref="S4.E6.m1.2.2.2.4.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.2.4.3.1" xref="S4.E6.m1.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E6.m1.2.2.2.4.3.3" xref="S4.E6.m1.2.2.2.4.3.3.cmml">h</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S4.E6.m1.2.2.2.3" xref="S4.E6.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S4.E6.m1.2.2.2.2.2" xref="S4.E6.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E6.m1.2.2.2.2.2.3" xref="S4.E6.m1.2.2.2.2.3.cmml">(</mo><msup id="S4.E6.m1.1.1.1.1.1.1" xref="S4.E6.m1.1.1.1.1.1.1.cmml"><mi id="S4.E6.m1.1.1.1.1.1.1.2" xref="S4.E6.m1.1.1.1.1.1.1.2.cmml">f</mi><mrow id="S4.E6.m1.1.1.1.1.1.1.3" xref="S4.E6.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E6.m1.1.1.1.1.1.1.3.2" xref="S4.E6.m1.1.1.1.1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E6.m1.1.1.1.1.1.1.3.1" xref="S4.E6.m1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E6.m1.1.1.1.1.1.1.3.3" xref="S4.E6.m1.1.1.1.1.1.1.3.3.cmml">e</mi></mrow></msup><mo id="S4.E6.m1.2.2.2.2.2.4" xref="S4.E6.m1.2.2.2.2.3.cmml">,</mo><msup id="S4.E6.m1.2.2.2.2.2.2" xref="S4.E6.m1.2.2.2.2.2.2.cmml"><mi id="S4.E6.m1.2.2.2.2.2.2.2" xref="S4.E6.m1.2.2.2.2.2.2.2.cmml">f</mi><mi id="S4.E6.m1.2.2.2.2.2.2.3" xref="S4.E6.m1.2.2.2.2.2.2.3.cmml">h</mi></msup><mo stretchy="false" id="S4.E6.m1.2.2.2.2.2.5" xref="S4.E6.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.2b"><apply id="S4.E6.m1.2.2.cmml" xref="S4.E6.m1.2.2"><eq id="S4.E6.m1.2.2.3.cmml" xref="S4.E6.m1.2.2.3"></eq><apply id="S4.E6.m1.2.2.4.cmml" xref="S4.E6.m1.2.2.4"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.4.1.cmml" xref="S4.E6.m1.2.2.4">superscript</csymbol><ci id="S4.E6.m1.2.2.4.2.cmml" xref="S4.E6.m1.2.2.4.2">ğ‘“</ci><apply id="S4.E6.m1.2.2.4.3.cmml" xref="S4.E6.m1.2.2.4.3"><times id="S4.E6.m1.2.2.4.3.1.cmml" xref="S4.E6.m1.2.2.4.3.1"></times><ci id="S4.E6.m1.2.2.4.3.2.cmml" xref="S4.E6.m1.2.2.4.3.2">ğ‘™</ci><ci id="S4.E6.m1.2.2.4.3.3.cmml" xref="S4.E6.m1.2.2.4.3.3">â„</ci></apply></apply><apply id="S4.E6.m1.2.2.2.cmml" xref="S4.E6.m1.2.2.2"><times id="S4.E6.m1.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.3"></times><apply id="S4.E6.m1.2.2.2.4.cmml" xref="S4.E6.m1.2.2.2.4"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.4.1.cmml" xref="S4.E6.m1.2.2.2.4">superscript</csymbol><ci id="S4.E6.m1.2.2.2.4.2a.cmml" xref="S4.E6.m1.2.2.2.4.2"><mtext id="S4.E6.m1.2.2.2.4.2.cmml" xref="S4.E6.m1.2.2.2.4.2">TGF</mtext></ci><apply id="S4.E6.m1.2.2.2.4.3.cmml" xref="S4.E6.m1.2.2.2.4.3"><times id="S4.E6.m1.2.2.2.4.3.1.cmml" xref="S4.E6.m1.2.2.2.4.3.1"></times><ci id="S4.E6.m1.2.2.2.4.3.2.cmml" xref="S4.E6.m1.2.2.2.4.3.2">ğ‘™</ci><ci id="S4.E6.m1.2.2.2.4.3.3.cmml" xref="S4.E6.m1.2.2.2.4.3.3">â„</ci></apply></apply><interval closure="open" id="S4.E6.m1.2.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.2.2"><apply id="S4.E6.m1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E6.m1.1.1.1.1.1.1.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E6.m1.1.1.1.1.1.1.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.2">ğ‘“</ci><apply id="S4.E6.m1.1.1.1.1.1.1.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.3"><times id="S4.E6.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E6.m1.1.1.1.1.1.1.3.1"></times><ci id="S4.E6.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E6.m1.1.1.1.1.1.1.3.2">ğ‘™</ci><ci id="S4.E6.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E6.m1.1.1.1.1.1.1.3.3">ğ‘’</ci></apply></apply><apply id="S4.E6.m1.2.2.2.2.2.2.cmml" xref="S4.E6.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E6.m1.2.2.2.2.2.2.1.cmml" xref="S4.E6.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S4.E6.m1.2.2.2.2.2.2.2.cmml" xref="S4.E6.m1.2.2.2.2.2.2.2">ğ‘“</ci><ci id="S4.E6.m1.2.2.2.2.2.2.3.cmml" xref="S4.E6.m1.2.2.2.2.2.2.3">â„</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.2c">f^{lh}=\text{TGF}^{lh}(f^{le},f^{h})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<table id="S4.E7" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E7.m1.2" class="ltx_Math" alttext="f^{rh}=\text{TGF}^{rh}(f^{re},f^{h})" display="block"><semantics id="S4.E7.m1.2a"><mrow id="S4.E7.m1.2.2" xref="S4.E7.m1.2.2.cmml"><msup id="S4.E7.m1.2.2.4" xref="S4.E7.m1.2.2.4.cmml"><mi id="S4.E7.m1.2.2.4.2" xref="S4.E7.m1.2.2.4.2.cmml">f</mi><mrow id="S4.E7.m1.2.2.4.3" xref="S4.E7.m1.2.2.4.3.cmml"><mi id="S4.E7.m1.2.2.4.3.2" xref="S4.E7.m1.2.2.4.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.4.3.1" xref="S4.E7.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E7.m1.2.2.4.3.3" xref="S4.E7.m1.2.2.4.3.3.cmml">h</mi></mrow></msup><mo id="S4.E7.m1.2.2.3" xref="S4.E7.m1.2.2.3.cmml">=</mo><mrow id="S4.E7.m1.2.2.2" xref="S4.E7.m1.2.2.2.cmml"><msup id="S4.E7.m1.2.2.2.4" xref="S4.E7.m1.2.2.2.4.cmml"><mtext id="S4.E7.m1.2.2.2.4.2" xref="S4.E7.m1.2.2.2.4.2a.cmml">TGF</mtext><mrow id="S4.E7.m1.2.2.2.4.3" xref="S4.E7.m1.2.2.2.4.3.cmml"><mi id="S4.E7.m1.2.2.2.4.3.2" xref="S4.E7.m1.2.2.2.4.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.2.4.3.1" xref="S4.E7.m1.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E7.m1.2.2.2.4.3.3" xref="S4.E7.m1.2.2.2.4.3.3.cmml">h</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S4.E7.m1.2.2.2.3" xref="S4.E7.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S4.E7.m1.2.2.2.2.2" xref="S4.E7.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E7.m1.2.2.2.2.2.3" xref="S4.E7.m1.2.2.2.2.3.cmml">(</mo><msup id="S4.E7.m1.1.1.1.1.1.1" xref="S4.E7.m1.1.1.1.1.1.1.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.2" xref="S4.E7.m1.1.1.1.1.1.1.2.cmml">f</mi><mrow id="S4.E7.m1.1.1.1.1.1.1.3" xref="S4.E7.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E7.m1.1.1.1.1.1.1.3.2" xref="S4.E7.m1.1.1.1.1.1.1.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E7.m1.1.1.1.1.1.1.3.1" xref="S4.E7.m1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E7.m1.1.1.1.1.1.1.3.3" xref="S4.E7.m1.1.1.1.1.1.1.3.3.cmml">e</mi></mrow></msup><mo id="S4.E7.m1.2.2.2.2.2.4" xref="S4.E7.m1.2.2.2.2.3.cmml">,</mo><msup id="S4.E7.m1.2.2.2.2.2.2" xref="S4.E7.m1.2.2.2.2.2.2.cmml"><mi id="S4.E7.m1.2.2.2.2.2.2.2" xref="S4.E7.m1.2.2.2.2.2.2.2.cmml">f</mi><mi id="S4.E7.m1.2.2.2.2.2.2.3" xref="S4.E7.m1.2.2.2.2.2.2.3.cmml">h</mi></msup><mo stretchy="false" id="S4.E7.m1.2.2.2.2.2.5" xref="S4.E7.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E7.m1.2b"><apply id="S4.E7.m1.2.2.cmml" xref="S4.E7.m1.2.2"><eq id="S4.E7.m1.2.2.3.cmml" xref="S4.E7.m1.2.2.3"></eq><apply id="S4.E7.m1.2.2.4.cmml" xref="S4.E7.m1.2.2.4"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.4.1.cmml" xref="S4.E7.m1.2.2.4">superscript</csymbol><ci id="S4.E7.m1.2.2.4.2.cmml" xref="S4.E7.m1.2.2.4.2">ğ‘“</ci><apply id="S4.E7.m1.2.2.4.3.cmml" xref="S4.E7.m1.2.2.4.3"><times id="S4.E7.m1.2.2.4.3.1.cmml" xref="S4.E7.m1.2.2.4.3.1"></times><ci id="S4.E7.m1.2.2.4.3.2.cmml" xref="S4.E7.m1.2.2.4.3.2">ğ‘Ÿ</ci><ci id="S4.E7.m1.2.2.4.3.3.cmml" xref="S4.E7.m1.2.2.4.3.3">â„</ci></apply></apply><apply id="S4.E7.m1.2.2.2.cmml" xref="S4.E7.m1.2.2.2"><times id="S4.E7.m1.2.2.2.3.cmml" xref="S4.E7.m1.2.2.2.3"></times><apply id="S4.E7.m1.2.2.2.4.cmml" xref="S4.E7.m1.2.2.2.4"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.2.4.1.cmml" xref="S4.E7.m1.2.2.2.4">superscript</csymbol><ci id="S4.E7.m1.2.2.2.4.2a.cmml" xref="S4.E7.m1.2.2.2.4.2"><mtext id="S4.E7.m1.2.2.2.4.2.cmml" xref="S4.E7.m1.2.2.2.4.2">TGF</mtext></ci><apply id="S4.E7.m1.2.2.2.4.3.cmml" xref="S4.E7.m1.2.2.2.4.3"><times id="S4.E7.m1.2.2.2.4.3.1.cmml" xref="S4.E7.m1.2.2.2.4.3.1"></times><ci id="S4.E7.m1.2.2.2.4.3.2.cmml" xref="S4.E7.m1.2.2.2.4.3.2">ğ‘Ÿ</ci><ci id="S4.E7.m1.2.2.2.4.3.3.cmml" xref="S4.E7.m1.2.2.2.4.3.3">â„</ci></apply></apply><interval closure="open" id="S4.E7.m1.2.2.2.2.3.cmml" xref="S4.E7.m1.2.2.2.2.2"><apply id="S4.E7.m1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E7.m1.1.1.1.1.1.1.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E7.m1.1.1.1.1.1.1.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.2">ğ‘“</ci><apply id="S4.E7.m1.1.1.1.1.1.1.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3"><times id="S4.E7.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.1"></times><ci id="S4.E7.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.2">ğ‘Ÿ</ci><ci id="S4.E7.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E7.m1.1.1.1.1.1.1.3.3">ğ‘’</ci></apply></apply><apply id="S4.E7.m1.2.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E7.m1.2.2.2.2.2.2.1.cmml" xref="S4.E7.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S4.E7.m1.2.2.2.2.2.2.2.cmml" xref="S4.E7.m1.2.2.2.2.2.2.2">ğ‘“</ci><ci id="S4.E7.m1.2.2.2.2.2.2.3.cmml" xref="S4.E7.m1.2.2.2.2.2.2.3">â„</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E7.m1.2c">f^{rh}=\text{TGF}^{rh}(f^{re},f^{h})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p6.1" class="ltx_p">In the second stage, the two fused eye-head features are fed into a third TGF module to fuse features from left and right:</p>
</div>
<div id="S4.SS1.p7" class="ltx_para">
<table id="S4.E8" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E8.m1.2" class="ltx_Math" alttext="f^{lr}=\text{TGF}^{lr}(f^{lh},f^{rh})" display="block"><semantics id="S4.E8.m1.2a"><mrow id="S4.E8.m1.2.2" xref="S4.E8.m1.2.2.cmml"><msup id="S4.E8.m1.2.2.4" xref="S4.E8.m1.2.2.4.cmml"><mi id="S4.E8.m1.2.2.4.2" xref="S4.E8.m1.2.2.4.2.cmml">f</mi><mrow id="S4.E8.m1.2.2.4.3" xref="S4.E8.m1.2.2.4.3.cmml"><mi id="S4.E8.m1.2.2.4.3.2" xref="S4.E8.m1.2.2.4.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.2.2.4.3.1" xref="S4.E8.m1.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E8.m1.2.2.4.3.3" xref="S4.E8.m1.2.2.4.3.3.cmml">r</mi></mrow></msup><mo id="S4.E8.m1.2.2.3" xref="S4.E8.m1.2.2.3.cmml">=</mo><mrow id="S4.E8.m1.2.2.2" xref="S4.E8.m1.2.2.2.cmml"><msup id="S4.E8.m1.2.2.2.4" xref="S4.E8.m1.2.2.2.4.cmml"><mtext id="S4.E8.m1.2.2.2.4.2" xref="S4.E8.m1.2.2.2.4.2a.cmml">TGF</mtext><mrow id="S4.E8.m1.2.2.2.4.3" xref="S4.E8.m1.2.2.2.4.3.cmml"><mi id="S4.E8.m1.2.2.2.4.3.2" xref="S4.E8.m1.2.2.2.4.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.2.2.2.4.3.1" xref="S4.E8.m1.2.2.2.4.3.1.cmml">â€‹</mo><mi id="S4.E8.m1.2.2.2.4.3.3" xref="S4.E8.m1.2.2.2.4.3.3.cmml">r</mi></mrow></msup><mo lspace="0em" rspace="0em" id="S4.E8.m1.2.2.2.3" xref="S4.E8.m1.2.2.2.3.cmml">â€‹</mo><mrow id="S4.E8.m1.2.2.2.2.2" xref="S4.E8.m1.2.2.2.2.3.cmml"><mo stretchy="false" id="S4.E8.m1.2.2.2.2.2.3" xref="S4.E8.m1.2.2.2.2.3.cmml">(</mo><msup id="S4.E8.m1.1.1.1.1.1.1" xref="S4.E8.m1.1.1.1.1.1.1.cmml"><mi id="S4.E8.m1.1.1.1.1.1.1.2" xref="S4.E8.m1.1.1.1.1.1.1.2.cmml">f</mi><mrow id="S4.E8.m1.1.1.1.1.1.1.3" xref="S4.E8.m1.1.1.1.1.1.1.3.cmml"><mi id="S4.E8.m1.1.1.1.1.1.1.3.2" xref="S4.E8.m1.1.1.1.1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.1.1.1.1.1.1.3.1" xref="S4.E8.m1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E8.m1.1.1.1.1.1.1.3.3" xref="S4.E8.m1.1.1.1.1.1.1.3.3.cmml">h</mi></mrow></msup><mo id="S4.E8.m1.2.2.2.2.2.4" xref="S4.E8.m1.2.2.2.2.3.cmml">,</mo><msup id="S4.E8.m1.2.2.2.2.2.2" xref="S4.E8.m1.2.2.2.2.2.2.cmml"><mi id="S4.E8.m1.2.2.2.2.2.2.2" xref="S4.E8.m1.2.2.2.2.2.2.2.cmml">f</mi><mrow id="S4.E8.m1.2.2.2.2.2.2.3" xref="S4.E8.m1.2.2.2.2.2.2.3.cmml"><mi id="S4.E8.m1.2.2.2.2.2.2.3.2" xref="S4.E8.m1.2.2.2.2.2.2.3.2.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E8.m1.2.2.2.2.2.2.3.1" xref="S4.E8.m1.2.2.2.2.2.2.3.1.cmml">â€‹</mo><mi id="S4.E8.m1.2.2.2.2.2.2.3.3" xref="S4.E8.m1.2.2.2.2.2.2.3.3.cmml">h</mi></mrow></msup><mo stretchy="false" id="S4.E8.m1.2.2.2.2.2.5" xref="S4.E8.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E8.m1.2b"><apply id="S4.E8.m1.2.2.cmml" xref="S4.E8.m1.2.2"><eq id="S4.E8.m1.2.2.3.cmml" xref="S4.E8.m1.2.2.3"></eq><apply id="S4.E8.m1.2.2.4.cmml" xref="S4.E8.m1.2.2.4"><csymbol cd="ambiguous" id="S4.E8.m1.2.2.4.1.cmml" xref="S4.E8.m1.2.2.4">superscript</csymbol><ci id="S4.E8.m1.2.2.4.2.cmml" xref="S4.E8.m1.2.2.4.2">ğ‘“</ci><apply id="S4.E8.m1.2.2.4.3.cmml" xref="S4.E8.m1.2.2.4.3"><times id="S4.E8.m1.2.2.4.3.1.cmml" xref="S4.E8.m1.2.2.4.3.1"></times><ci id="S4.E8.m1.2.2.4.3.2.cmml" xref="S4.E8.m1.2.2.4.3.2">ğ‘™</ci><ci id="S4.E8.m1.2.2.4.3.3.cmml" xref="S4.E8.m1.2.2.4.3.3">ğ‘Ÿ</ci></apply></apply><apply id="S4.E8.m1.2.2.2.cmml" xref="S4.E8.m1.2.2.2"><times id="S4.E8.m1.2.2.2.3.cmml" xref="S4.E8.m1.2.2.2.3"></times><apply id="S4.E8.m1.2.2.2.4.cmml" xref="S4.E8.m1.2.2.2.4"><csymbol cd="ambiguous" id="S4.E8.m1.2.2.2.4.1.cmml" xref="S4.E8.m1.2.2.2.4">superscript</csymbol><ci id="S4.E8.m1.2.2.2.4.2a.cmml" xref="S4.E8.m1.2.2.2.4.2"><mtext id="S4.E8.m1.2.2.2.4.2.cmml" xref="S4.E8.m1.2.2.2.4.2">TGF</mtext></ci><apply id="S4.E8.m1.2.2.2.4.3.cmml" xref="S4.E8.m1.2.2.2.4.3"><times id="S4.E8.m1.2.2.2.4.3.1.cmml" xref="S4.E8.m1.2.2.2.4.3.1"></times><ci id="S4.E8.m1.2.2.2.4.3.2.cmml" xref="S4.E8.m1.2.2.2.4.3.2">ğ‘™</ci><ci id="S4.E8.m1.2.2.2.4.3.3.cmml" xref="S4.E8.m1.2.2.2.4.3.3">ğ‘Ÿ</ci></apply></apply><interval closure="open" id="S4.E8.m1.2.2.2.2.3.cmml" xref="S4.E8.m1.2.2.2.2.2"><apply id="S4.E8.m1.1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E8.m1.1.1.1.1.1.1.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E8.m1.1.1.1.1.1.1.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.2">ğ‘“</ci><apply id="S4.E8.m1.1.1.1.1.1.1.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3"><times id="S4.E8.m1.1.1.1.1.1.1.3.1.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.1"></times><ci id="S4.E8.m1.1.1.1.1.1.1.3.2.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.2">ğ‘™</ci><ci id="S4.E8.m1.1.1.1.1.1.1.3.3.cmml" xref="S4.E8.m1.1.1.1.1.1.1.3.3">â„</ci></apply></apply><apply id="S4.E8.m1.2.2.2.2.2.2.cmml" xref="S4.E8.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E8.m1.2.2.2.2.2.2.1.cmml" xref="S4.E8.m1.2.2.2.2.2.2">superscript</csymbol><ci id="S4.E8.m1.2.2.2.2.2.2.2.cmml" xref="S4.E8.m1.2.2.2.2.2.2.2">ğ‘“</ci><apply id="S4.E8.m1.2.2.2.2.2.2.3.cmml" xref="S4.E8.m1.2.2.2.2.2.2.3"><times id="S4.E8.m1.2.2.2.2.2.2.3.1.cmml" xref="S4.E8.m1.2.2.2.2.2.2.3.1"></times><ci id="S4.E8.m1.2.2.2.2.2.2.3.2.cmml" xref="S4.E8.m1.2.2.2.2.2.2.3.2">ğ‘Ÿ</ci><ci id="S4.E8.m1.2.2.2.2.2.2.3.3.cmml" xref="S4.E8.m1.2.2.2.2.2.2.3.3">â„</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E8.m1.2c">f^{lr}=\text{TGF}^{lr}(f^{lh},f^{rh})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(8)</span></td>
</tr></tbody>
</table>
<p id="S4.SS1.p7.2" class="ltx_p">Finally, the fused feature <math id="S4.SS1.p7.1.m1.1" class="ltx_Math" alttext="f^{lr}" display="inline"><semantics id="S4.SS1.p7.1.m1.1a"><msup id="S4.SS1.p7.1.m1.1.1" xref="S4.SS1.p7.1.m1.1.1.cmml"><mi id="S4.SS1.p7.1.m1.1.1.2" xref="S4.SS1.p7.1.m1.1.1.2.cmml">f</mi><mrow id="S4.SS1.p7.1.m1.1.1.3" xref="S4.SS1.p7.1.m1.1.1.3.cmml"><mi id="S4.SS1.p7.1.m1.1.1.3.2" xref="S4.SS1.p7.1.m1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS1.p7.1.m1.1.1.3.1" xref="S4.SS1.p7.1.m1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS1.p7.1.m1.1.1.3.3" xref="S4.SS1.p7.1.m1.1.1.3.3.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.1.m1.1b"><apply id="S4.SS1.p7.1.m1.1.1.cmml" xref="S4.SS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p7.1.m1.1.1.1.cmml" xref="S4.SS1.p7.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p7.1.m1.1.1.2.cmml" xref="S4.SS1.p7.1.m1.1.1.2">ğ‘“</ci><apply id="S4.SS1.p7.1.m1.1.1.3.cmml" xref="S4.SS1.p7.1.m1.1.1.3"><times id="S4.SS1.p7.1.m1.1.1.3.1.cmml" xref="S4.SS1.p7.1.m1.1.1.3.1"></times><ci id="S4.SS1.p7.1.m1.1.1.3.2.cmml" xref="S4.SS1.p7.1.m1.1.1.3.2">ğ‘™</ci><ci id="S4.SS1.p7.1.m1.1.1.3.3.cmml" xref="S4.SS1.p7.1.m1.1.1.3.3">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.1.m1.1c">f^{lr}</annotation></semantics></math> is fed to an MLP to get the predicted gaze <math id="S4.SS1.p7.2.m2.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S4.SS1.p7.2.m2.1a"><mi id="S4.SS1.p7.2.m2.1.1" xref="S4.SS1.p7.2.m2.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p7.2.m2.1b"><ci id="S4.SS1.p7.2.m2.1.1.cmml" xref="S4.SS1.p7.2.m2.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p7.2.m2.1c">g</annotation></semantics></math>:</p>
</div>
<div id="S4.SS1.p8" class="ltx_para">
<table id="S4.E9" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E9.m1.1" class="ltx_Math" alttext="g=\text{MLP}(f^{lr})." display="block"><semantics id="S4.E9.m1.1a"><mrow id="S4.E9.m1.1.1.1" xref="S4.E9.m1.1.1.1.1.cmml"><mrow id="S4.E9.m1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.cmml"><mi id="S4.E9.m1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.3.cmml">g</mi><mo id="S4.E9.m1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.2.cmml">=</mo><mrow id="S4.E9.m1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.cmml"><mtext id="S4.E9.m1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.3a.cmml">MLP</mtext><mo lspace="0em" rspace="0em" id="S4.E9.m1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S4.E9.m1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.E9.m1.1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S4.E9.m1.1.1.1.1.1.1.1.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2.cmml">f</mi><mrow id="S4.E9.m1.1.1.1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.3.2" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E9.m1.1.1.1.1.1.1.1.1.3.1" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.cmml">r</mi></mrow></msup><mo stretchy="false" id="S4.E9.m1.1.1.1.1.1.1.1.3" xref="S4.E9.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E9.m1.1.1.1.2" xref="S4.E9.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E9.m1.1b"><apply id="S4.E9.m1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1"><eq id="S4.E9.m1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.2"></eq><ci id="S4.E9.m1.1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.1.3">ğ‘”</ci><apply id="S4.E9.m1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1"><times id="S4.E9.m1.1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.1.2"></times><ci id="S4.E9.m1.1.1.1.1.1.3a.cmml" xref="S4.E9.m1.1.1.1.1.1.3"><mtext id="S4.E9.m1.1.1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.1.1.3">MLP</mtext></ci><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E9.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.2">ğ‘“</ci><apply id="S4.E9.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3"><times id="S4.E9.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.2">ğ‘™</ci><ci id="S4.E9.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E9.m1.1.1.1.1.1.1.1.1.3.3">ğ‘Ÿ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E9.m1.1c">g=\text{MLP}(f^{lr}).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(9)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Gaze Adaptation Module</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.6" class="ltx_p">Suppose we have <math id="S4.SS2.p1.1.m1.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">M</annotation></semantics></math> gaze datasets, <math id="S4.SS2.p1.2.m2.4" class="ltx_Math" alttext="D=\{D_{0},D_{2},...,D_{M-1}\}" display="inline"><semantics id="S4.SS2.p1.2.m2.4a"><mrow id="S4.SS2.p1.2.m2.4.4" xref="S4.SS2.p1.2.m2.4.4.cmml"><mi id="S4.SS2.p1.2.m2.4.4.5" xref="S4.SS2.p1.2.m2.4.4.5.cmml">D</mi><mo id="S4.SS2.p1.2.m2.4.4.4" xref="S4.SS2.p1.2.m2.4.4.4.cmml">=</mo><mrow id="S4.SS2.p1.2.m2.4.4.3.3" xref="S4.SS2.p1.2.m2.4.4.3.4.cmml"><mo stretchy="false" id="S4.SS2.p1.2.m2.4.4.3.3.4" xref="S4.SS2.p1.2.m2.4.4.3.4.cmml">{</mo><msub id="S4.SS2.p1.2.m2.2.2.1.1.1" xref="S4.SS2.p1.2.m2.2.2.1.1.1.cmml"><mi id="S4.SS2.p1.2.m2.2.2.1.1.1.2" xref="S4.SS2.p1.2.m2.2.2.1.1.1.2.cmml">D</mi><mn id="S4.SS2.p1.2.m2.2.2.1.1.1.3" xref="S4.SS2.p1.2.m2.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S4.SS2.p1.2.m2.4.4.3.3.5" xref="S4.SS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p1.2.m2.3.3.2.2.2" xref="S4.SS2.p1.2.m2.3.3.2.2.2.cmml"><mi id="S4.SS2.p1.2.m2.3.3.2.2.2.2" xref="S4.SS2.p1.2.m2.3.3.2.2.2.2.cmml">D</mi><mn id="S4.SS2.p1.2.m2.3.3.2.2.2.3" xref="S4.SS2.p1.2.m2.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S4.SS2.p1.2.m2.4.4.3.3.6" xref="S4.SS2.p1.2.m2.4.4.3.4.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">â€¦</mi><mo id="S4.SS2.p1.2.m2.4.4.3.3.7" xref="S4.SS2.p1.2.m2.4.4.3.4.cmml">,</mo><msub id="S4.SS2.p1.2.m2.4.4.3.3.3" xref="S4.SS2.p1.2.m2.4.4.3.3.3.cmml"><mi id="S4.SS2.p1.2.m2.4.4.3.3.3.2" xref="S4.SS2.p1.2.m2.4.4.3.3.3.2.cmml">D</mi><mrow id="S4.SS2.p1.2.m2.4.4.3.3.3.3" xref="S4.SS2.p1.2.m2.4.4.3.3.3.3.cmml"><mi id="S4.SS2.p1.2.m2.4.4.3.3.3.3.2" xref="S4.SS2.p1.2.m2.4.4.3.3.3.3.2.cmml">M</mi><mo id="S4.SS2.p1.2.m2.4.4.3.3.3.3.1" xref="S4.SS2.p1.2.m2.4.4.3.3.3.3.1.cmml">âˆ’</mo><mn id="S4.SS2.p1.2.m2.4.4.3.3.3.3.3" xref="S4.SS2.p1.2.m2.4.4.3.3.3.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S4.SS2.p1.2.m2.4.4.3.3.8" xref="S4.SS2.p1.2.m2.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.4b"><apply id="S4.SS2.p1.2.m2.4.4.cmml" xref="S4.SS2.p1.2.m2.4.4"><eq id="S4.SS2.p1.2.m2.4.4.4.cmml" xref="S4.SS2.p1.2.m2.4.4.4"></eq><ci id="S4.SS2.p1.2.m2.4.4.5.cmml" xref="S4.SS2.p1.2.m2.4.4.5">ğ·</ci><set id="S4.SS2.p1.2.m2.4.4.3.4.cmml" xref="S4.SS2.p1.2.m2.4.4.3.3"><apply id="S4.SS2.p1.2.m2.2.2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.2.2.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.2.2.1.1.1.1.cmml" xref="S4.SS2.p1.2.m2.2.2.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.2.m2.2.2.1.1.1.2.cmml" xref="S4.SS2.p1.2.m2.2.2.1.1.1.2">ğ·</ci><cn type="integer" id="S4.SS2.p1.2.m2.2.2.1.1.1.3.cmml" xref="S4.SS2.p1.2.m2.2.2.1.1.1.3">0</cn></apply><apply id="S4.SS2.p1.2.m2.3.3.2.2.2.cmml" xref="S4.SS2.p1.2.m2.3.3.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.3.3.2.2.2.1.cmml" xref="S4.SS2.p1.2.m2.3.3.2.2.2">subscript</csymbol><ci id="S4.SS2.p1.2.m2.3.3.2.2.2.2.cmml" xref="S4.SS2.p1.2.m2.3.3.2.2.2.2">ğ·</ci><cn type="integer" id="S4.SS2.p1.2.m2.3.3.2.2.2.3.cmml" xref="S4.SS2.p1.2.m2.3.3.2.2.2.3">2</cn></apply><ci id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1">â€¦</ci><apply id="S4.SS2.p1.2.m2.4.4.3.3.3.cmml" xref="S4.SS2.p1.2.m2.4.4.3.3.3"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.4.4.3.3.3.1.cmml" xref="S4.SS2.p1.2.m2.4.4.3.3.3">subscript</csymbol><ci id="S4.SS2.p1.2.m2.4.4.3.3.3.2.cmml" xref="S4.SS2.p1.2.m2.4.4.3.3.3.2">ğ·</ci><apply id="S4.SS2.p1.2.m2.4.4.3.3.3.3.cmml" xref="S4.SS2.p1.2.m2.4.4.3.3.3.3"><minus id="S4.SS2.p1.2.m2.4.4.3.3.3.3.1.cmml" xref="S4.SS2.p1.2.m2.4.4.3.3.3.3.1"></minus><ci id="S4.SS2.p1.2.m2.4.4.3.3.3.3.2.cmml" xref="S4.SS2.p1.2.m2.4.4.3.3.3.3.2">ğ‘€</ci><cn type="integer" id="S4.SS2.p1.2.m2.4.4.3.3.3.3.3.cmml" xref="S4.SS2.p1.2.m2.4.4.3.3.3.3.3">1</cn></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.4c">D=\{D_{0},D_{2},...,D_{M-1}\}</annotation></semantics></math>. Typically, we need to train <math id="S4.SS2.p1.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS2.p1.3.m3.1a"><mi id="S4.SS2.p1.3.m3.1.1" xref="S4.SS2.p1.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.3.m3.1b"><ci id="S4.SS2.p1.3.m3.1.1.cmml" xref="S4.SS2.p1.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.3.m3.1c">M</annotation></semantics></math> models: one for each dataset to get good performance. A model trained on <math id="S4.SS2.p1.4.m4.1" class="ltx_Math" alttext="D_{i}" display="inline"><semantics id="S4.SS2.p1.4.m4.1a"><msub id="S4.SS2.p1.4.m4.1.1" xref="S4.SS2.p1.4.m4.1.1.cmml"><mi id="S4.SS2.p1.4.m4.1.1.2" xref="S4.SS2.p1.4.m4.1.1.2.cmml">D</mi><mi id="S4.SS2.p1.4.m4.1.1.3" xref="S4.SS2.p1.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.4.m4.1b"><apply id="S4.SS2.p1.4.m4.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.4.m4.1.1.1.cmml" xref="S4.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS2.p1.4.m4.1.1.2.cmml" xref="S4.SS2.p1.4.m4.1.1.2">ğ·</ci><ci id="S4.SS2.p1.4.m4.1.1.3.cmml" xref="S4.SS2.p1.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.4.m4.1c">D_{i}</annotation></semantics></math> typically performs poorly on <math id="S4.SS2.p1.5.m5.1" class="ltx_Math" alttext="D_{j}" display="inline"><semantics id="S4.SS2.p1.5.m5.1a"><msub id="S4.SS2.p1.5.m5.1.1" xref="S4.SS2.p1.5.m5.1.1.cmml"><mi id="S4.SS2.p1.5.m5.1.1.2" xref="S4.SS2.p1.5.m5.1.1.2.cmml">D</mi><mi id="S4.SS2.p1.5.m5.1.1.3" xref="S4.SS2.p1.5.m5.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.5.m5.1b"><apply id="S4.SS2.p1.5.m5.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.5.m5.1.1.1.cmml" xref="S4.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S4.SS2.p1.5.m5.1.1.2.cmml" xref="S4.SS2.p1.5.m5.1.1.2">ğ·</ci><ci id="S4.SS2.p1.5.m5.1.1.3.cmml" xref="S4.SS2.p1.5.m5.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.5.m5.1c">D_{j}</annotation></semantics></math> where <math id="S4.SS2.p1.6.m6.1" class="ltx_Math" alttext="i\neq j" display="inline"><semantics id="S4.SS2.p1.6.m6.1a"><mrow id="S4.SS2.p1.6.m6.1.1" xref="S4.SS2.p1.6.m6.1.1.cmml"><mi id="S4.SS2.p1.6.m6.1.1.2" xref="S4.SS2.p1.6.m6.1.1.2.cmml">i</mi><mo id="S4.SS2.p1.6.m6.1.1.1" xref="S4.SS2.p1.6.m6.1.1.1.cmml">â‰ </mo><mi id="S4.SS2.p1.6.m6.1.1.3" xref="S4.SS2.p1.6.m6.1.1.3.cmml">j</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.6.m6.1b"><apply id="S4.SS2.p1.6.m6.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1"><neq id="S4.SS2.p1.6.m6.1.1.1.cmml" xref="S4.SS2.p1.6.m6.1.1.1"></neq><ci id="S4.SS2.p1.6.m6.1.1.2.cmml" xref="S4.SS2.p1.6.m6.1.1.2">ğ‘–</ci><ci id="S4.SS2.p1.6.m6.1.1.3.cmml" xref="S4.SS2.p1.6.m6.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.6.m6.1c">i\neq j</annotation></semantics></math>.</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.11" class="ltx_p">Instead, our approach trains only one model and <math id="S4.SS2.p2.1.m1.1" class="ltx_Math" alttext="M-1" display="inline"><semantics id="S4.SS2.p2.1.m1.1a"><mrow id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml"><mi id="S4.SS2.p2.1.m1.1.1.2" xref="S4.SS2.p2.1.m1.1.1.2.cmml">M</mi><mo id="S4.SS2.p2.1.m1.1.1.1" xref="S4.SS2.p2.1.m1.1.1.1.cmml">âˆ’</mo><mn id="S4.SS2.p2.1.m1.1.1.3" xref="S4.SS2.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><apply id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1"><minus id="S4.SS2.p2.1.m1.1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1.1"></minus><ci id="S4.SS2.p2.1.m1.1.1.2.cmml" xref="S4.SS2.p2.1.m1.1.1.2">ğ‘€</ci><cn type="integer" id="S4.SS2.p2.1.m1.1.1.3.cmml" xref="S4.SS2.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">M-1</annotation></semantics></math> Gaze Adaptation Modules (GAMs). The GAM is a module consisting of a <math id="S4.SS2.p2.2.m2.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S4.SS2.p2.2.m2.1a"><mi id="S4.SS2.p2.2.m2.1.1" xref="S4.SS2.p2.2.m2.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.2.m2.1b"><ci id="S4.SS2.p2.2.m2.1.1.cmml" xref="S4.SS2.p2.2.m2.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.2.m2.1c">M</annotation></semantics></math> MLPs, one for each dataset <math id="S4.SS2.p2.3.m3.3" class="ltx_Math" alttext="i\in\{0,\ldots,M-1\}" display="inline"><semantics id="S4.SS2.p2.3.m3.3a"><mrow id="S4.SS2.p2.3.m3.3.3" xref="S4.SS2.p2.3.m3.3.3.cmml"><mi id="S4.SS2.p2.3.m3.3.3.3" xref="S4.SS2.p2.3.m3.3.3.3.cmml">i</mi><mo id="S4.SS2.p2.3.m3.3.3.2" xref="S4.SS2.p2.3.m3.3.3.2.cmml">âˆˆ</mo><mrow id="S4.SS2.p2.3.m3.3.3.1.1" xref="S4.SS2.p2.3.m3.3.3.1.2.cmml"><mo stretchy="false" id="S4.SS2.p2.3.m3.3.3.1.1.2" xref="S4.SS2.p2.3.m3.3.3.1.2.cmml">{</mo><mn id="S4.SS2.p2.3.m3.1.1" xref="S4.SS2.p2.3.m3.1.1.cmml">0</mn><mo id="S4.SS2.p2.3.m3.3.3.1.1.3" xref="S4.SS2.p2.3.m3.3.3.1.2.cmml">,</mo><mi mathvariant="normal" id="S4.SS2.p2.3.m3.2.2" xref="S4.SS2.p2.3.m3.2.2.cmml">â€¦</mi><mo id="S4.SS2.p2.3.m3.3.3.1.1.4" xref="S4.SS2.p2.3.m3.3.3.1.2.cmml">,</mo><mrow id="S4.SS2.p2.3.m3.3.3.1.1.1" xref="S4.SS2.p2.3.m3.3.3.1.1.1.cmml"><mi id="S4.SS2.p2.3.m3.3.3.1.1.1.2" xref="S4.SS2.p2.3.m3.3.3.1.1.1.2.cmml">M</mi><mo id="S4.SS2.p2.3.m3.3.3.1.1.1.1" xref="S4.SS2.p2.3.m3.3.3.1.1.1.1.cmml">âˆ’</mo><mn id="S4.SS2.p2.3.m3.3.3.1.1.1.3" xref="S4.SS2.p2.3.m3.3.3.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S4.SS2.p2.3.m3.3.3.1.1.5" xref="S4.SS2.p2.3.m3.3.3.1.2.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.3.m3.3b"><apply id="S4.SS2.p2.3.m3.3.3.cmml" xref="S4.SS2.p2.3.m3.3.3"><in id="S4.SS2.p2.3.m3.3.3.2.cmml" xref="S4.SS2.p2.3.m3.3.3.2"></in><ci id="S4.SS2.p2.3.m3.3.3.3.cmml" xref="S4.SS2.p2.3.m3.3.3.3">ğ‘–</ci><set id="S4.SS2.p2.3.m3.3.3.1.2.cmml" xref="S4.SS2.p2.3.m3.3.3.1.1"><cn type="integer" id="S4.SS2.p2.3.m3.1.1.cmml" xref="S4.SS2.p2.3.m3.1.1">0</cn><ci id="S4.SS2.p2.3.m3.2.2.cmml" xref="S4.SS2.p2.3.m3.2.2">â€¦</ci><apply id="S4.SS2.p2.3.m3.3.3.1.1.1.cmml" xref="S4.SS2.p2.3.m3.3.3.1.1.1"><minus id="S4.SS2.p2.3.m3.3.3.1.1.1.1.cmml" xref="S4.SS2.p2.3.m3.3.3.1.1.1.1"></minus><ci id="S4.SS2.p2.3.m3.3.3.1.1.1.2.cmml" xref="S4.SS2.p2.3.m3.3.3.1.1.1.2">ğ‘€</ci><cn type="integer" id="S4.SS2.p2.3.m3.3.3.1.1.1.3.cmml" xref="S4.SS2.p2.3.m3.3.3.1.1.1.3">1</cn></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.3.m3.3c">i\in\{0,\ldots,M-1\}</annotation></semantics></math>. Each MLP, <math id="S4.SS2.p2.4.m4.1" class="ltx_Math" alttext="\text{MLP}_{i}(\cdot)" display="inline"><semantics id="S4.SS2.p2.4.m4.1a"><mrow id="S4.SS2.p2.4.m4.1.2" xref="S4.SS2.p2.4.m4.1.2.cmml"><msub id="S4.SS2.p2.4.m4.1.2.2" xref="S4.SS2.p2.4.m4.1.2.2.cmml"><mtext id="S4.SS2.p2.4.m4.1.2.2.2" xref="S4.SS2.p2.4.m4.1.2.2.2a.cmml">MLP</mtext><mi id="S4.SS2.p2.4.m4.1.2.2.3" xref="S4.SS2.p2.4.m4.1.2.2.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.4.m4.1.2.1" xref="S4.SS2.p2.4.m4.1.2.1.cmml">â€‹</mo><mrow id="S4.SS2.p2.4.m4.1.2.3.2" xref="S4.SS2.p2.4.m4.1.2.cmml"><mo stretchy="false" id="S4.SS2.p2.4.m4.1.2.3.2.1" xref="S4.SS2.p2.4.m4.1.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS2.p2.4.m4.1.1" xref="S4.SS2.p2.4.m4.1.1.cmml">â‹…</mo><mo stretchy="false" id="S4.SS2.p2.4.m4.1.2.3.2.2" xref="S4.SS2.p2.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.4.m4.1b"><apply id="S4.SS2.p2.4.m4.1.2.cmml" xref="S4.SS2.p2.4.m4.1.2"><times id="S4.SS2.p2.4.m4.1.2.1.cmml" xref="S4.SS2.p2.4.m4.1.2.1"></times><apply id="S4.SS2.p2.4.m4.1.2.2.cmml" xref="S4.SS2.p2.4.m4.1.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.4.m4.1.2.2.1.cmml" xref="S4.SS2.p2.4.m4.1.2.2">subscript</csymbol><ci id="S4.SS2.p2.4.m4.1.2.2.2a.cmml" xref="S4.SS2.p2.4.m4.1.2.2.2"><mtext id="S4.SS2.p2.4.m4.1.2.2.2.cmml" xref="S4.SS2.p2.4.m4.1.2.2.2">MLP</mtext></ci><ci id="S4.SS2.p2.4.m4.1.2.2.3.cmml" xref="S4.SS2.p2.4.m4.1.2.2.3">ğ‘–</ci></apply><ci id="S4.SS2.p2.4.m4.1.1.cmml" xref="S4.SS2.p2.4.m4.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.4.m4.1c">\text{MLP}_{i}(\cdot)</annotation></semantics></math>, accepts the extracted feature <math id="S4.SS2.p2.5.m5.1" class="ltx_Math" alttext="f^{lr}" display="inline"><semantics id="S4.SS2.p2.5.m5.1a"><msup id="S4.SS2.p2.5.m5.1.1" xref="S4.SS2.p2.5.m5.1.1.cmml"><mi id="S4.SS2.p2.5.m5.1.1.2" xref="S4.SS2.p2.5.m5.1.1.2.cmml">f</mi><mrow id="S4.SS2.p2.5.m5.1.1.3" xref="S4.SS2.p2.5.m5.1.1.3.cmml"><mi id="S4.SS2.p2.5.m5.1.1.3.2" xref="S4.SS2.p2.5.m5.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.5.m5.1.1.3.1" xref="S4.SS2.p2.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS2.p2.5.m5.1.1.3.3" xref="S4.SS2.p2.5.m5.1.1.3.3.cmml">r</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.5.m5.1b"><apply id="S4.SS2.p2.5.m5.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.5.m5.1.1.1.cmml" xref="S4.SS2.p2.5.m5.1.1">superscript</csymbol><ci id="S4.SS2.p2.5.m5.1.1.2.cmml" xref="S4.SS2.p2.5.m5.1.1.2">ğ‘“</ci><apply id="S4.SS2.p2.5.m5.1.1.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3"><times id="S4.SS2.p2.5.m5.1.1.3.1.cmml" xref="S4.SS2.p2.5.m5.1.1.3.1"></times><ci id="S4.SS2.p2.5.m5.1.1.3.2.cmml" xref="S4.SS2.p2.5.m5.1.1.3.2">ğ‘™</ci><ci id="S4.SS2.p2.5.m5.1.1.3.3.cmml" xref="S4.SS2.p2.5.m5.1.1.3.3">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.5.m5.1c">f^{lr}</annotation></semantics></math> and produces a gaze offset assuming the sample comes from dataset <math id="S4.SS2.p2.6.m6.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p2.6.m6.1a"><mi id="S4.SS2.p2.6.m6.1.1" xref="S4.SS2.p2.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.6.m6.1b"><ci id="S4.SS2.p2.6.m6.1.1.cmml" xref="S4.SS2.p2.6.m6.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.6.m6.1c">i</annotation></semantics></math>. <math id="S4.SS2.p2.7.m7.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S4.SS2.p2.7.m7.1a"><msub id="S4.SS2.p2.7.m7.1.1" xref="S4.SS2.p2.7.m7.1.1.cmml"><mi id="S4.SS2.p2.7.m7.1.1.2" xref="S4.SS2.p2.7.m7.1.1.2.cmml">D</mi><mn id="S4.SS2.p2.7.m7.1.1.3" xref="S4.SS2.p2.7.m7.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.7.m7.1b"><apply id="S4.SS2.p2.7.m7.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.7.m7.1.1.1.cmml" xref="S4.SS2.p2.7.m7.1.1">subscript</csymbol><ci id="S4.SS2.p2.7.m7.1.1.2.cmml" xref="S4.SS2.p2.7.m7.1.1.2">ğ·</ci><cn type="integer" id="S4.SS2.p2.7.m7.1.1.3.cmml" xref="S4.SS2.p2.7.m7.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.7.m7.1c">D_{0}</annotation></semantics></math> is regarded as the anchor dataset, so its offset is always zero, i.e., <math id="S4.SS2.p2.8.m8.1" class="ltx_Math" alttext="\text{MLP}_{0}(\cdot)=\boldsymbol{0}" display="inline"><semantics id="S4.SS2.p2.8.m8.1a"><mrow id="S4.SS2.p2.8.m8.1.2" xref="S4.SS2.p2.8.m8.1.2.cmml"><mrow id="S4.SS2.p2.8.m8.1.2.2" xref="S4.SS2.p2.8.m8.1.2.2.cmml"><msub id="S4.SS2.p2.8.m8.1.2.2.2" xref="S4.SS2.p2.8.m8.1.2.2.2.cmml"><mtext id="S4.SS2.p2.8.m8.1.2.2.2.2" xref="S4.SS2.p2.8.m8.1.2.2.2.2a.cmml">MLP</mtext><mn id="S4.SS2.p2.8.m8.1.2.2.2.3" xref="S4.SS2.p2.8.m8.1.2.2.2.3.cmml">0</mn></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.8.m8.1.2.2.1" xref="S4.SS2.p2.8.m8.1.2.2.1.cmml">â€‹</mo><mrow id="S4.SS2.p2.8.m8.1.2.2.3.2" xref="S4.SS2.p2.8.m8.1.2.2.cmml"><mo stretchy="false" id="S4.SS2.p2.8.m8.1.2.2.3.2.1" xref="S4.SS2.p2.8.m8.1.2.2.cmml">(</mo><mo lspace="0em" rspace="0em" id="S4.SS2.p2.8.m8.1.1" xref="S4.SS2.p2.8.m8.1.1.cmml">â‹…</mo><mo stretchy="false" id="S4.SS2.p2.8.m8.1.2.2.3.2.2" xref="S4.SS2.p2.8.m8.1.2.2.cmml">)</mo></mrow></mrow><mo id="S4.SS2.p2.8.m8.1.2.1" xref="S4.SS2.p2.8.m8.1.2.1.cmml">=</mo><mn id="S4.SS2.p2.8.m8.1.2.3" xref="S4.SS2.p2.8.m8.1.2.3.cmml">ğŸ</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.8.m8.1b"><apply id="S4.SS2.p2.8.m8.1.2.cmml" xref="S4.SS2.p2.8.m8.1.2"><eq id="S4.SS2.p2.8.m8.1.2.1.cmml" xref="S4.SS2.p2.8.m8.1.2.1"></eq><apply id="S4.SS2.p2.8.m8.1.2.2.cmml" xref="S4.SS2.p2.8.m8.1.2.2"><times id="S4.SS2.p2.8.m8.1.2.2.1.cmml" xref="S4.SS2.p2.8.m8.1.2.2.1"></times><apply id="S4.SS2.p2.8.m8.1.2.2.2.cmml" xref="S4.SS2.p2.8.m8.1.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p2.8.m8.1.2.2.2.1.cmml" xref="S4.SS2.p2.8.m8.1.2.2.2">subscript</csymbol><ci id="S4.SS2.p2.8.m8.1.2.2.2.2a.cmml" xref="S4.SS2.p2.8.m8.1.2.2.2.2"><mtext id="S4.SS2.p2.8.m8.1.2.2.2.2.cmml" xref="S4.SS2.p2.8.m8.1.2.2.2.2">MLP</mtext></ci><cn type="integer" id="S4.SS2.p2.8.m8.1.2.2.2.3.cmml" xref="S4.SS2.p2.8.m8.1.2.2.2.3">0</cn></apply><ci id="S4.SS2.p2.8.m8.1.1.cmml" xref="S4.SS2.p2.8.m8.1.1">â‹…</ci></apply><cn type="integer" id="S4.SS2.p2.8.m8.1.2.3.cmml" xref="S4.SS2.p2.8.m8.1.2.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.8.m8.1c">\text{MLP}_{0}(\cdot)=\boldsymbol{0}</annotation></semantics></math> and does not need to be trained. For the others, the MLP has two layers with GELU nonlinearities. If the sample comes from dataset <math id="S4.SS2.p2.9.m9.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS2.p2.9.m9.1a"><mi id="S4.SS2.p2.9.m9.1.1" xref="S4.SS2.p2.9.m9.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.9.m9.1b"><ci id="S4.SS2.p2.9.m9.1.1.cmml" xref="S4.SS2.p2.9.m9.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.9.m9.1c">i</annotation></semantics></math>, the corrected gaze vector is given by <math id="S4.SS2.p2.10.m10.1" class="ltx_Math" alttext="\hat{g}=g+{\Delta{g}}" display="inline"><semantics id="S4.SS2.p2.10.m10.1a"><mrow id="S4.SS2.p2.10.m10.1.1" xref="S4.SS2.p2.10.m10.1.1.cmml"><mover accent="true" id="S4.SS2.p2.10.m10.1.1.2" xref="S4.SS2.p2.10.m10.1.1.2.cmml"><mi id="S4.SS2.p2.10.m10.1.1.2.2" xref="S4.SS2.p2.10.m10.1.1.2.2.cmml">g</mi><mo id="S4.SS2.p2.10.m10.1.1.2.1" xref="S4.SS2.p2.10.m10.1.1.2.1.cmml">^</mo></mover><mo id="S4.SS2.p2.10.m10.1.1.1" xref="S4.SS2.p2.10.m10.1.1.1.cmml">=</mo><mrow id="S4.SS2.p2.10.m10.1.1.3" xref="S4.SS2.p2.10.m10.1.1.3.cmml"><mi id="S4.SS2.p2.10.m10.1.1.3.2" xref="S4.SS2.p2.10.m10.1.1.3.2.cmml">g</mi><mo id="S4.SS2.p2.10.m10.1.1.3.1" xref="S4.SS2.p2.10.m10.1.1.3.1.cmml">+</mo><mrow id="S4.SS2.p2.10.m10.1.1.3.3" xref="S4.SS2.p2.10.m10.1.1.3.3.cmml"><mi mathvariant="normal" id="S4.SS2.p2.10.m10.1.1.3.3.2" xref="S4.SS2.p2.10.m10.1.1.3.3.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.10.m10.1.1.3.3.1" xref="S4.SS2.p2.10.m10.1.1.3.3.1.cmml">â€‹</mo><mi id="S4.SS2.p2.10.m10.1.1.3.3.3" xref="S4.SS2.p2.10.m10.1.1.3.3.3.cmml">g</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.10.m10.1b"><apply id="S4.SS2.p2.10.m10.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1"><eq id="S4.SS2.p2.10.m10.1.1.1.cmml" xref="S4.SS2.p2.10.m10.1.1.1"></eq><apply id="S4.SS2.p2.10.m10.1.1.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2"><ci id="S4.SS2.p2.10.m10.1.1.2.1.cmml" xref="S4.SS2.p2.10.m10.1.1.2.1">^</ci><ci id="S4.SS2.p2.10.m10.1.1.2.2.cmml" xref="S4.SS2.p2.10.m10.1.1.2.2">ğ‘”</ci></apply><apply id="S4.SS2.p2.10.m10.1.1.3.cmml" xref="S4.SS2.p2.10.m10.1.1.3"><plus id="S4.SS2.p2.10.m10.1.1.3.1.cmml" xref="S4.SS2.p2.10.m10.1.1.3.1"></plus><ci id="S4.SS2.p2.10.m10.1.1.3.2.cmml" xref="S4.SS2.p2.10.m10.1.1.3.2">ğ‘”</ci><apply id="S4.SS2.p2.10.m10.1.1.3.3.cmml" xref="S4.SS2.p2.10.m10.1.1.3.3"><times id="S4.SS2.p2.10.m10.1.1.3.3.1.cmml" xref="S4.SS2.p2.10.m10.1.1.3.3.1"></times><ci id="S4.SS2.p2.10.m10.1.1.3.3.2.cmml" xref="S4.SS2.p2.10.m10.1.1.3.3.2">Î”</ci><ci id="S4.SS2.p2.10.m10.1.1.3.3.3.cmml" xref="S4.SS2.p2.10.m10.1.1.3.3.3">ğ‘”</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.10.m10.1c">\hat{g}=g+{\Delta{g}}</annotation></semantics></math>, where <math id="S4.SS2.p2.11.m11.1" class="ltx_Math" alttext="\Delta{g}=\text{MLP}_{i}(f^{lr})" display="inline"><semantics id="S4.SS2.p2.11.m11.1a"><mrow id="S4.SS2.p2.11.m11.1.1" xref="S4.SS2.p2.11.m11.1.1.cmml"><mrow id="S4.SS2.p2.11.m11.1.1.3" xref="S4.SS2.p2.11.m11.1.1.3.cmml"><mi mathvariant="normal" id="S4.SS2.p2.11.m11.1.1.3.2" xref="S4.SS2.p2.11.m11.1.1.3.2.cmml">Î”</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.11.m11.1.1.3.1" xref="S4.SS2.p2.11.m11.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS2.p2.11.m11.1.1.3.3" xref="S4.SS2.p2.11.m11.1.1.3.3.cmml">g</mi></mrow><mo id="S4.SS2.p2.11.m11.1.1.2" xref="S4.SS2.p2.11.m11.1.1.2.cmml">=</mo><mrow id="S4.SS2.p2.11.m11.1.1.1" xref="S4.SS2.p2.11.m11.1.1.1.cmml"><msub id="S4.SS2.p2.11.m11.1.1.1.3" xref="S4.SS2.p2.11.m11.1.1.1.3.cmml"><mtext id="S4.SS2.p2.11.m11.1.1.1.3.2" xref="S4.SS2.p2.11.m11.1.1.1.3.2a.cmml">MLP</mtext><mi id="S4.SS2.p2.11.m11.1.1.1.3.3" xref="S4.SS2.p2.11.m11.1.1.1.3.3.cmml">i</mi></msub><mo lspace="0em" rspace="0em" id="S4.SS2.p2.11.m11.1.1.1.2" xref="S4.SS2.p2.11.m11.1.1.1.2.cmml">â€‹</mo><mrow id="S4.SS2.p2.11.m11.1.1.1.1.1" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS2.p2.11.m11.1.1.1.1.1.2" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.cmml">(</mo><msup id="S4.SS2.p2.11.m11.1.1.1.1.1.1" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.cmml"><mi id="S4.SS2.p2.11.m11.1.1.1.1.1.1.2" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.2.cmml">f</mi><mrow id="S4.SS2.p2.11.m11.1.1.1.1.1.1.3" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.cmml"><mi id="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.2" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.1" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.1.cmml">â€‹</mo><mi id="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.3" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.3.cmml">r</mi></mrow></msup><mo stretchy="false" id="S4.SS2.p2.11.m11.1.1.1.1.1.3" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.11.m11.1b"><apply id="S4.SS2.p2.11.m11.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1"><eq id="S4.SS2.p2.11.m11.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.2"></eq><apply id="S4.SS2.p2.11.m11.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3"><times id="S4.SS2.p2.11.m11.1.1.3.1.cmml" xref="S4.SS2.p2.11.m11.1.1.3.1"></times><ci id="S4.SS2.p2.11.m11.1.1.3.2.cmml" xref="S4.SS2.p2.11.m11.1.1.3.2">Î”</ci><ci id="S4.SS2.p2.11.m11.1.1.3.3.cmml" xref="S4.SS2.p2.11.m11.1.1.3.3">ğ‘”</ci></apply><apply id="S4.SS2.p2.11.m11.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1.1"><times id="S4.SS2.p2.11.m11.1.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.1.2"></times><apply id="S4.SS2.p2.11.m11.1.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p2.11.m11.1.1.1.3.1.cmml" xref="S4.SS2.p2.11.m11.1.1.1.3">subscript</csymbol><ci id="S4.SS2.p2.11.m11.1.1.1.3.2a.cmml" xref="S4.SS2.p2.11.m11.1.1.1.3.2"><mtext id="S4.SS2.p2.11.m11.1.1.1.3.2.cmml" xref="S4.SS2.p2.11.m11.1.1.1.3.2">MLP</mtext></ci><ci id="S4.SS2.p2.11.m11.1.1.1.3.3.cmml" xref="S4.SS2.p2.11.m11.1.1.1.3.3">ğ‘–</ci></apply><apply id="S4.SS2.p2.11.m11.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p2.11.m11.1.1.1.1.1.1.1.cmml" xref="S4.SS2.p2.11.m11.1.1.1.1.1">superscript</csymbol><ci id="S4.SS2.p2.11.m11.1.1.1.1.1.1.2.cmml" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.2">ğ‘“</ci><apply id="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.cmml" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.3"><times id="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.1"></times><ci id="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.2.cmml" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.2">ğ‘™</ci><ci id="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.3.cmml" xref="S4.SS2.p2.11.m11.1.1.1.1.1.1.3.3">ğ‘Ÿ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.11.m11.1c">\Delta{g}=\text{MLP}_{i}(f^{lr})</annotation></semantics></math>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Architecture Details</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.2" class="ltx_p">The whole architecture contains three pipelines for the face and two eye images. All the backbones are ResNet18 networks, which are initialized from the model trained on ImageNet. The input face image size is <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="224\times 224\times 3" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><mn id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.1.m1.1.1.1a" xref="S4.SS3.p1.1.m1.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p1.1.m1.1.1.4" xref="S4.SS3.p1.1.m1.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><times id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2">224</cn><cn type="integer" id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3">224</cn><cn type="integer" id="S4.SS3.p1.1.m1.1.1.4.cmml" xref="S4.SS3.p1.1.m1.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">224\times 224\times 3</annotation></semantics></math>. We crop the eye patches according to the landmarks and use RoI align to resize the cropped patches to <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="128\times 128\times 3" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><mrow id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mn id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.2.m2.1.1.1" xref="S4.SS3.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S4.SS3.p1.2.m2.1.1.1a" xref="S4.SS3.p1.2.m2.1.1.1.cmml">Ã—</mo><mn id="S4.SS3.p1.2.m2.1.1.4" xref="S4.SS3.p1.2.m2.1.1.4.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><times id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">128</cn><cn type="integer" id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">128</cn><cn type="integer" id="S4.SS3.p1.2.m2.1.1.4.cmml" xref="S4.SS3.p1.2.m2.1.1.4">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">128\times 128\times 3</annotation></semantics></math>. The estimated gaze contains the yaw and pitch representing the 3D gaze direction in the camera coordinate system. We chose L1 loss as the loss function for gaze estimation.</p>
</div>
<div id="S4.SS3.p2" class="ltx_para">
<p id="S4.SS3.p2.1" class="ltx_p">For TTGF, we set the number of heads of all MSAs as 8 and the hidden size of the MLP is 2048. We use 8 repeated blocks in each transformer encoder. After each transformer encoder, the features are projected with a linear layer whose output size is 128. For the MLPs for both gaze regression and the GAMs, the sizes of the hidden layers are identically set to 128.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we introduce the experimental settings and the evaluation datasets we selected and evaluate our proposed TTGF and GAM in two types of experiments. We first compare our method with the state-of-the-art methods for gaze estimation performance. Then we perform ablation studies to determine the effects due to TTGF and GAM respectively and study the effect of multiple dataset training.</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p"><span id="S5.p2.1.1" class="ltx_text ltx_font_bold">Dataset for evaluation</span>â€ƒFor evaluating gaze estimation performance, we used three gaze datasets to evaluate the gaze estimation performance as shown in Table <a href="#S5.T1" title="Table 1 â€£ 5 Experiments â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: MPIIFaceGaze <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, RT-GENE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and EYEDIAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. MPIIFaceGaze dataset is based on MPIIGaze, but includes face and eye images. It contains 45K images collected from 15 subjects. We used leave-one-person-out cross-validation with this dataset. The RT-GENE dataset consists of 123K samples from 15 participants. We used three-fold cross-validation with this dataset. The raw data of the EYEDIAP dataset has 94 videos collected from 16 subjects. We used the sampling scheme from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> to extract face images and four-fold cross-validation. For our experiments on multi-dataset training, we trained 15 models (one for each subject left out from MPIIFaceGaze), where each person was assigned to one of the folds in the other two datasets. Performance for each fold in the other two datasets was computed by averaging the performance of the models from the MPIIFaceGaze subjects assigned to that fold.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overview of the datasets used for evaluation and anchor dataset in our experiments. We show the number of subjects, the range of gaze, and the head pose in both horizontal and vertical directions in the camera coordinate systems.</figcaption>
<table id="S5.T1.15" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T1.15.16.1" class="ltx_tr">
<th id="S5.T1.15.16.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Dataset</th>
<th id="S5.T1.15.16.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"># Subjects</th>
<th id="S5.T1.15.16.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Gaze</th>
<th id="S5.T1.15.16.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Head Pose</th>
<th id="S5.T1.15.16.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"># Data</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T1.4.4" class="ltx_tr">
<th id="S5.T1.4.4.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">MPIIFaceGaze <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<td id="S5.T1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">15</td>
<td id="S5.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T1.1.1.1.m1.1" class="ltx_Math" alttext="\pm 20" display="inline"><semantics id="S5.T1.1.1.1.m1.1a"><mrow id="S5.T1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.m1.1.1.cmml"><mo id="S5.T1.1.1.1.m1.1.1a" xref="S5.T1.1.1.1.m1.1.1.cmml">Â±</mo><mn id="S5.T1.1.1.1.m1.1.1.2" xref="S5.T1.1.1.1.m1.1.1.2.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b"><apply id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.1.1.1.m1.1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.1.1.1.m1.1.1.2.cmml" xref="S5.T1.1.1.1.m1.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">\pm 20</annotation></semantics></math>Â°, <math id="S5.T1.2.2.2.m2.1" class="ltx_Math" alttext="\pm 20" display="inline"><semantics id="S5.T1.2.2.2.m2.1a"><mrow id="S5.T1.2.2.2.m2.1.1" xref="S5.T1.2.2.2.m2.1.1.cmml"><mo id="S5.T1.2.2.2.m2.1.1a" xref="S5.T1.2.2.2.m2.1.1.cmml">Â±</mo><mn id="S5.T1.2.2.2.m2.1.1.2" xref="S5.T1.2.2.2.m2.1.1.2.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.m2.1b"><apply id="S5.T1.2.2.2.m2.1.1.cmml" xref="S5.T1.2.2.2.m2.1.1"><csymbol cd="latexml" id="S5.T1.2.2.2.m2.1.1.1.cmml" xref="S5.T1.2.2.2.m2.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.2.2.2.m2.1.1.2.cmml" xref="S5.T1.2.2.2.m2.1.1.2">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.m2.1c">\pm 20</annotation></semantics></math>Â°</td>
<td id="S5.T1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">
<math id="S5.T1.3.3.3.m1.1" class="ltx_Math" alttext="\pm 15" display="inline"><semantics id="S5.T1.3.3.3.m1.1a"><mrow id="S5.T1.3.3.3.m1.1.1" xref="S5.T1.3.3.3.m1.1.1.cmml"><mo id="S5.T1.3.3.3.m1.1.1a" xref="S5.T1.3.3.3.m1.1.1.cmml">Â±</mo><mn id="S5.T1.3.3.3.m1.1.1.2" xref="S5.T1.3.3.3.m1.1.1.2.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.m1.1b"><apply id="S5.T1.3.3.3.m1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1"><csymbol cd="latexml" id="S5.T1.3.3.3.m1.1.1.1.cmml" xref="S5.T1.3.3.3.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.3.3.3.m1.1.1.2.cmml" xref="S5.T1.3.3.3.m1.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.m1.1c">\pm 15</annotation></semantics></math>Â°, <math id="S5.T1.4.4.4.m2.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S5.T1.4.4.4.m2.1a"><mn id="S5.T1.4.4.4.m2.1.1" xref="S5.T1.4.4.4.m2.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.m2.1b"><cn type="integer" id="S5.T1.4.4.4.m2.1.1.cmml" xref="S5.T1.4.4.4.m2.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.m2.1c">30</annotation></semantics></math>Â°</td>
<td id="S5.T1.4.4.7" class="ltx_td ltx_align_center ltx_border_t">45K images</td>
</tr>
<tr id="S5.T1.8.8" class="ltx_tr">
<th id="S5.T1.8.8.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">RT-GENE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S5.T1.8.8.6" class="ltx_td ltx_align_center ltx_border_r">15</td>
<td id="S5.T1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T1.5.5.1.m1.1" class="ltx_Math" alttext="\pm 40" display="inline"><semantics id="S5.T1.5.5.1.m1.1a"><mrow id="S5.T1.5.5.1.m1.1.1" xref="S5.T1.5.5.1.m1.1.1.cmml"><mo id="S5.T1.5.5.1.m1.1.1a" xref="S5.T1.5.5.1.m1.1.1.cmml">Â±</mo><mn id="S5.T1.5.5.1.m1.1.1.2" xref="S5.T1.5.5.1.m1.1.1.2.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.5.5.1.m1.1b"><apply id="S5.T1.5.5.1.m1.1.1.cmml" xref="S5.T1.5.5.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.5.5.1.m1.1.1.1.cmml" xref="S5.T1.5.5.1.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.5.5.1.m1.1.1.2.cmml" xref="S5.T1.5.5.1.m1.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.5.5.1.m1.1c">\pm 40</annotation></semantics></math>Â°, <math id="S5.T1.6.6.2.m2.1" class="ltx_Math" alttext="-40" display="inline"><semantics id="S5.T1.6.6.2.m2.1a"><mrow id="S5.T1.6.6.2.m2.1.1" xref="S5.T1.6.6.2.m2.1.1.cmml"><mo id="S5.T1.6.6.2.m2.1.1a" xref="S5.T1.6.6.2.m2.1.1.cmml">âˆ’</mo><mn id="S5.T1.6.6.2.m2.1.1.2" xref="S5.T1.6.6.2.m2.1.1.2.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.6.6.2.m2.1b"><apply id="S5.T1.6.6.2.m2.1.1.cmml" xref="S5.T1.6.6.2.m2.1.1"><minus id="S5.T1.6.6.2.m2.1.1.1.cmml" xref="S5.T1.6.6.2.m2.1.1"></minus><cn type="integer" id="S5.T1.6.6.2.m2.1.1.2.cmml" xref="S5.T1.6.6.2.m2.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.6.6.2.m2.1c">-40</annotation></semantics></math>Â°</td>
<td id="S5.T1.8.8.4" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T1.7.7.3.m1.1" class="ltx_Math" alttext="\pm 40" display="inline"><semantics id="S5.T1.7.7.3.m1.1a"><mrow id="S5.T1.7.7.3.m1.1.1" xref="S5.T1.7.7.3.m1.1.1.cmml"><mo id="S5.T1.7.7.3.m1.1.1a" xref="S5.T1.7.7.3.m1.1.1.cmml">Â±</mo><mn id="S5.T1.7.7.3.m1.1.1.2" xref="S5.T1.7.7.3.m1.1.1.2.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.7.7.3.m1.1b"><apply id="S5.T1.7.7.3.m1.1.1.cmml" xref="S5.T1.7.7.3.m1.1.1"><csymbol cd="latexml" id="S5.T1.7.7.3.m1.1.1.1.cmml" xref="S5.T1.7.7.3.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.7.7.3.m1.1.1.2.cmml" xref="S5.T1.7.7.3.m1.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.7.7.3.m1.1c">\pm 40</annotation></semantics></math>Â°, <math id="S5.T1.8.8.4.m2.1" class="ltx_Math" alttext="\pm 40" display="inline"><semantics id="S5.T1.8.8.4.m2.1a"><mrow id="S5.T1.8.8.4.m2.1.1" xref="S5.T1.8.8.4.m2.1.1.cmml"><mo id="S5.T1.8.8.4.m2.1.1a" xref="S5.T1.8.8.4.m2.1.1.cmml">Â±</mo><mn id="S5.T1.8.8.4.m2.1.1.2" xref="S5.T1.8.8.4.m2.1.1.2.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.8.8.4.m2.1b"><apply id="S5.T1.8.8.4.m2.1.1.cmml" xref="S5.T1.8.8.4.m2.1.1"><csymbol cd="latexml" id="S5.T1.8.8.4.m2.1.1.1.cmml" xref="S5.T1.8.8.4.m2.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.8.8.4.m2.1.1.2.cmml" xref="S5.T1.8.8.4.m2.1.1.2">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.8.4.m2.1c">\pm 40</annotation></semantics></math>Â°</td>
<td id="S5.T1.8.8.7" class="ltx_td ltx_align_center">123K images</td>
</tr>
<tr id="S5.T1.11.11" class="ltx_tr">
<th id="S5.T1.11.11.4" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r">EYEDIAP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>
</th>
<td id="S5.T1.11.11.5" class="ltx_td ltx_align_center ltx_border_r">16</td>
<td id="S5.T1.10.10.2" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T1.9.9.1.m1.1" class="ltx_Math" alttext="\pm 25" display="inline"><semantics id="S5.T1.9.9.1.m1.1a"><mrow id="S5.T1.9.9.1.m1.1.1" xref="S5.T1.9.9.1.m1.1.1.cmml"><mo id="S5.T1.9.9.1.m1.1.1a" xref="S5.T1.9.9.1.m1.1.1.cmml">Â±</mo><mn id="S5.T1.9.9.1.m1.1.1.2" xref="S5.T1.9.9.1.m1.1.1.2.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.9.9.1.m1.1b"><apply id="S5.T1.9.9.1.m1.1.1.cmml" xref="S5.T1.9.9.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.9.9.1.m1.1.1.1.cmml" xref="S5.T1.9.9.1.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.9.9.1.m1.1.1.2.cmml" xref="S5.T1.9.9.1.m1.1.1.2">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.9.1.m1.1c">\pm 25</annotation></semantics></math>Â°, <math id="S5.T1.10.10.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S5.T1.10.10.2.m2.1a"><mn id="S5.T1.10.10.2.m2.1.1" xref="S5.T1.10.10.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S5.T1.10.10.2.m2.1b"><cn type="integer" id="S5.T1.10.10.2.m2.1.1.cmml" xref="S5.T1.10.10.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.10.2.m2.1c">20</annotation></semantics></math>Â°</td>
<td id="S5.T1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">
<math id="S5.T1.11.11.3.m1.1" class="ltx_Math" alttext="\pm 15" display="inline"><semantics id="S5.T1.11.11.3.m1.1a"><mrow id="S5.T1.11.11.3.m1.1.1" xref="S5.T1.11.11.3.m1.1.1.cmml"><mo id="S5.T1.11.11.3.m1.1.1a" xref="S5.T1.11.11.3.m1.1.1.cmml">Â±</mo><mn id="S5.T1.11.11.3.m1.1.1.2" xref="S5.T1.11.11.3.m1.1.1.2.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.11.11.3.m1.1b"><apply id="S5.T1.11.11.3.m1.1.1.cmml" xref="S5.T1.11.11.3.m1.1.1"><csymbol cd="latexml" id="S5.T1.11.11.3.m1.1.1.1.cmml" xref="S5.T1.11.11.3.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.11.11.3.m1.1.1.2.cmml" xref="S5.T1.11.11.3.m1.1.1.2">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.11.11.3.m1.1c">\pm 15</annotation></semantics></math>Â°, 30Â°</td>
<td id="S5.T1.11.11.6" class="ltx_td ltx_align_center">94 videos</td>
</tr>
<tr id="S5.T1.15.15" class="ltx_tr">
<th id="S5.T1.15.15.5" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r">ETH-XGaze <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>
</th>
<td id="S5.T1.15.15.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">110</td>
<td id="S5.T1.13.13.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T1.12.12.1.m1.1" class="ltx_Math" alttext="\pm 120" display="inline"><semantics id="S5.T1.12.12.1.m1.1a"><mrow id="S5.T1.12.12.1.m1.1.1" xref="S5.T1.12.12.1.m1.1.1.cmml"><mo id="S5.T1.12.12.1.m1.1.1a" xref="S5.T1.12.12.1.m1.1.1.cmml">Â±</mo><mn id="S5.T1.12.12.1.m1.1.1.2" xref="S5.T1.12.12.1.m1.1.1.2.cmml">120</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.12.12.1.m1.1b"><apply id="S5.T1.12.12.1.m1.1.1.cmml" xref="S5.T1.12.12.1.m1.1.1"><csymbol cd="latexml" id="S5.T1.12.12.1.m1.1.1.1.cmml" xref="S5.T1.12.12.1.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.12.12.1.m1.1.1.2.cmml" xref="S5.T1.12.12.1.m1.1.1.2">120</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.12.12.1.m1.1c">\pm 120</annotation></semantics></math>Â°, <math id="S5.T1.13.13.2.m2.1" class="ltx_Math" alttext="\pm 70" display="inline"><semantics id="S5.T1.13.13.2.m2.1a"><mrow id="S5.T1.13.13.2.m2.1.1" xref="S5.T1.13.13.2.m2.1.1.cmml"><mo id="S5.T1.13.13.2.m2.1.1a" xref="S5.T1.13.13.2.m2.1.1.cmml">Â±</mo><mn id="S5.T1.13.13.2.m2.1.1.2" xref="S5.T1.13.13.2.m2.1.1.2.cmml">70</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.13.13.2.m2.1b"><apply id="S5.T1.13.13.2.m2.1.1.cmml" xref="S5.T1.13.13.2.m2.1.1"><csymbol cd="latexml" id="S5.T1.13.13.2.m2.1.1.1.cmml" xref="S5.T1.13.13.2.m2.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.13.13.2.m2.1.1.2.cmml" xref="S5.T1.13.13.2.m2.1.1.2">70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.13.13.2.m2.1c">\pm 70</annotation></semantics></math>Â°</td>
<td id="S5.T1.15.15.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">
<math id="S5.T1.14.14.3.m1.1" class="ltx_Math" alttext="\pm 80" display="inline"><semantics id="S5.T1.14.14.3.m1.1a"><mrow id="S5.T1.14.14.3.m1.1.1" xref="S5.T1.14.14.3.m1.1.1.cmml"><mo id="S5.T1.14.14.3.m1.1.1a" xref="S5.T1.14.14.3.m1.1.1.cmml">Â±</mo><mn id="S5.T1.14.14.3.m1.1.1.2" xref="S5.T1.14.14.3.m1.1.1.2.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.14.14.3.m1.1b"><apply id="S5.T1.14.14.3.m1.1.1.cmml" xref="S5.T1.14.14.3.m1.1.1"><csymbol cd="latexml" id="S5.T1.14.14.3.m1.1.1.1.cmml" xref="S5.T1.14.14.3.m1.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.14.14.3.m1.1.1.2.cmml" xref="S5.T1.14.14.3.m1.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.14.14.3.m1.1c">\pm 80</annotation></semantics></math>Â°, <math id="S5.T1.15.15.4.m2.1" class="ltx_Math" alttext="\pm 80" display="inline"><semantics id="S5.T1.15.15.4.m2.1a"><mrow id="S5.T1.15.15.4.m2.1.1" xref="S5.T1.15.15.4.m2.1.1.cmml"><mo id="S5.T1.15.15.4.m2.1.1a" xref="S5.T1.15.15.4.m2.1.1.cmml">Â±</mo><mn id="S5.T1.15.15.4.m2.1.1.2" xref="S5.T1.15.15.4.m2.1.1.2.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.15.15.4.m2.1b"><apply id="S5.T1.15.15.4.m2.1.1.cmml" xref="S5.T1.15.15.4.m2.1.1"><csymbol cd="latexml" id="S5.T1.15.15.4.m2.1.1.1.cmml" xref="S5.T1.15.15.4.m2.1.1">plus-or-minus</csymbol><cn type="integer" id="S5.T1.15.15.4.m2.1.1.2.cmml" xref="S5.T1.15.15.4.m2.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.15.15.4.m2.1c">\pm 80</annotation></semantics></math>Â°</td>
<td id="S5.T1.15.15.7" class="ltx_td ltx_align_center ltx_border_b">1.1M images</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p"><span id="S5.p3.1.1" class="ltx_text ltx_font_bold">Anchor dataset and pre-training</span>â€ƒETH-XGaze <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> is a large-scale gaze dataset that consists of 1,083,492 image samples from 110 participants (47 female and 63 male). It has the largest range of head poses compared to the evaluation dataset and the gaze direction is evenly sampled both horizontally and vertically as shown in Table <a href="#S5.T1" title="Table 1 â€£ 5 Experiments â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The large variation and scale make it a suitable dataset as the anchor dataset <math id="S5.p3.1.m1.1" class="ltx_Math" alttext="D_{0}" display="inline"><semantics id="S5.p3.1.m1.1a"><msub id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml"><mi id="S5.p3.1.m1.1.1.2" xref="S5.p3.1.m1.1.1.2.cmml">D</mi><mn id="S5.p3.1.m1.1.1.3" xref="S5.p3.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><apply id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p3.1.m1.1.1.1.cmml" xref="S5.p3.1.m1.1.1">subscript</csymbol><ci id="S5.p3.1.m1.1.1.2.cmml" xref="S5.p3.1.m1.1.1.2">ğ·</ci><cn type="integer" id="S5.p3.1.m1.1.1.3.cmml" xref="S5.p3.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">D_{0}</annotation></semantics></math> and for pre-training. The whole dataset contains three parts: the training set, the within-dataset, and the person-specific evaluation set. The training set has 765K images of 80 subjects. We use this part as the anchor set and also for pre-training. The person-specific evaluation consists of 15 subjects but is not related to this task. The within-dataset which includes 15 subjects is used for validation of multiple datasets training and the pre-training model.</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<p id="S5.p4.1" class="ltx_p"><span id="S5.p4.1.1" class="ltx_text ltx_font_bold">Experimental settings</span>â€ƒThe optimizer applied for model training is AdamW with a linear scheduled warm-up strategy. The initial learning rate is set to 0.0001 for all the training and uses the exponential schedule to update it. For multiple-set training, in each iteration, we randomly sample the same number of samples from each set to form a batch fed to the model. The batch size is set to 64. The number of iterations in one training epoch is determined by the size of the dataset with the smallest number of samples. The number of epochs is 50 and gamma is 0.96. For single-set training for the TTGF-only model, the batch size is also set to 64. For ETH-XGaze, we train the model for 50 epochs with the exponential gamma setting to 0.95. For MPIIFaceGaze and RT-GENe, the total number of epochs is 30 epochs with the exponential gamma setting to 0.95. For EYEDIAP, the number of epochs is 50 and gamma is 0.096. Our experiments are all conducted on a single GeForce RTX 3090 GPU.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison with state-of-the-art methods</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">In this part, we compare the gaze estimation performance of our proposed model with state-of-the-art methods. Our model is a single model trained on multiple datasets: one anchor dataset and three evaluation datasets, while the existing methods were tested with separated models for different evaluation datasets. We trained our TTGF-only model on ETH-XGaze and got a testing error of 3.58Â°and the proposed TTGF+GAM trained on multiple datasets achieved a slightly better error of 3.54Â°.</p>
</div>
<figure id="S5.F2" class="ltx_figure"><img src="/html/2409.00912/assets/x2.png" id="S5.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="438" height="198" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Four types of feature fusion for gaze estimation models: (a) two-eyes model uses the cropped eye patches as inputs. (b) PAR indicates left eye, right eye, and head features are combined in parallel. (c) LR-EH indicates that left and right eye features are combined first then combined head features. (d) EH-LR indicates that single eye and head features are combined first followed by a combination across the left
and right..</figcaption>
</figure>
<figure id="S5.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison with the state-of-the-art methods. The proposed method outperforms state-of-the-art results in estimation error.</figcaption>
<table id="S5.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Model</th>
<td id="S5.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Transformer</td>
<td id="S5.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Feature Fusion</td>
<td id="S5.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MPIIFaceGaze</td>
<td id="S5.T2.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RT-GENE</td>
<td id="S5.T2.1.1.1.6" class="ltx_td ltx_align_center ltx_border_t">EYEDIAP</td>
</tr>
<tr id="S5.T2.1.2.2" class="ltx_tr">
<th id="S5.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">FullFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>
</th>
<td id="S5.T2.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NO</td>
<td id="S5.T2.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Face Only</td>
<td id="S5.T2.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.93Â°</td>
<td id="S5.T2.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">10.00Â°</td>
<td id="S5.T2.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t">6.53Â°</td>
</tr>
<tr id="S5.T2.1.3.3" class="ltx_tr">
<th id="S5.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">RT-GENE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S5.T2.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NO</td>
<td id="S5.T2.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Two Eyes</td>
<td id="S5.T2.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.66Â°</td>
<td id="S5.T2.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.00Â°</td>
<td id="S5.T2.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">6.02Â°</td>
</tr>
<tr id="S5.T2.1.4.4" class="ltx_tr">
<th id="S5.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">DilatedNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>
</th>
<td id="S5.T2.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NO</td>
<td id="S5.T2.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">PAR</td>
<td id="S5.T2.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.42Â°</td>
<td id="S5.T2.1.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">8.38Â°</td>
<td id="S5.T2.1.4.4.6" class="ltx_td ltx_align_center ltx_border_t">6.19Â°</td>
</tr>
<tr id="S5.T2.1.5.5" class="ltx_tr">
<th id="S5.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">iTracker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>
</th>
<td id="S5.T2.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NO</td>
<td id="S5.T2.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LR-EH</td>
<td id="S5.T2.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.33Â°</td>
<td id="S5.T2.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.12Â°</td>
<td id="S5.T2.1.5.5.6" class="ltx_td ltx_align_center ltx_border_t">5.28Â°</td>
</tr>
<tr id="S5.T2.1.6.6" class="ltx_tr">
<th id="S5.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">iTracker-MHSA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<td id="S5.T2.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">YES</td>
<td id="S5.T2.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">LR-EH</td>
<td id="S5.T2.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.05Â°</td>
<td id="S5.T2.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.06Â°</td>
<td id="S5.T2.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">5.17Â°</td>
</tr>
<tr id="S5.T2.1.7.7" class="ltx_tr">
<th id="S5.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GazeTR-Hybrid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<td id="S5.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">YES</td>
<td id="S5.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Face Only</td>
<td id="S5.T2.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.18Â°</td>
<td id="S5.T2.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.12Â°</td>
<td id="S5.T2.1.7.7.6" class="ltx_td ltx_align_center ltx_border_t">5.33Â°</td>
</tr>
<tr id="S5.T2.1.8.8" class="ltx_tr">
<th id="S5.T2.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GazeCADSE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</th>
<td id="S5.T2.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">YES</td>
<td id="S5.T2.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Face Only</td>
<td id="S5.T2.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.04Â°</td>
<td id="S5.T2.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.00Â°</td>
<td id="S5.T2.1.8.8.6" class="ltx_td ltx_align_center ltx_border_t">5.25Â°</td>
</tr>
<tr id="S5.T2.1.9.9" class="ltx_tr">
<th id="S5.T2.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T2.1.9.9.1.1" class="ltx_text ltx_font_bold">Proposed</span></th>
<td id="S5.T2.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">YES</td>
<td id="S5.T2.1.9.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">EH-LR</td>
<td id="S5.T2.1.9.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T2.1.9.9.4.1" class="ltx_text ltx_font_bold">3.88</span>Â°</td>
<td id="S5.T2.1.9.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T2.1.9.9.5.1" class="ltx_text ltx_font_bold">6.46</span>Â°</td>
<td id="S5.T2.1.9.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S5.T2.1.9.9.6.1" class="ltx_text ltx_font_bold">4.89</span>Â°</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Table <a href="#S5.T2" title="Table 2 â€£ 5.1 Comparison with state-of-the-art methods â€£ 5 Experiments â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the angular errors of each method on the evaluation datasets: MPIIFaceGaze, RT-GENE, and EYEDIAP. As iTracker and iTracker-MHSA did not provide the performance on the evaluation datasets, we re-implemented them by replacing their backbones with ResNet18 for fair comparison. In the table, among existing models, FullFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, GazeTR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and GazeCADSE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> only use the full face image as the input for gaze estimation. RT-GENE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> feeds two cropped eyes to a VGG16 model. DilatedNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> fuses the features of the left eye, right eye, and head directly. iTracker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, iTracker-MHSA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> fuse the features of the left and right eyes first then with the head features. Our proposed method also uses both the face and the eye images as inputs but has different ways of feature fusion we fuse the features of each eye and head in the first stage and then fuse the left and right features in the second stage. In addition, GazeTR, GazeCADSE, and our proposed methods utilize the transformers in the model. We show different types of gaze estimation models in Fig. <a href="#S5.F2" title="Figure 2 â€£ 5.1 Comparison with state-of-the-art methods â€£ 5 Experiments â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">As shown in Table <a href="#S5.T2" title="Table 2 â€£ 5.1 Comparison with state-of-the-art methods â€£ 5 Experiments â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, our proposed methods TTGF with GAM achieved the state-of-the-art performance of gaze estimation on all the selected evaluation datasets. Among the methods using the feature fusing, our eye-head first then left-right combination shows the best performance. Overall the transformer-based methods show advantages in the performance of gaze estimation compared with non-transformer methods. Among the transformer-based methods, our model uses both the face and eye images, we used RoI alignment to resize the eye region to <math id="S5.SS1.p3.1.m1.1" class="ltx_Math" alttext="128\times 128" display="inline"><semantics id="S5.SS1.p3.1.m1.1a"><mrow id="S5.SS1.p3.1.m1.1.1" xref="S5.SS1.p3.1.m1.1.1.cmml"><mn id="S5.SS1.p3.1.m1.1.1.2" xref="S5.SS1.p3.1.m1.1.1.2.cmml">128</mn><mo lspace="0.222em" rspace="0.222em" id="S5.SS1.p3.1.m1.1.1.1" xref="S5.SS1.p3.1.m1.1.1.1.cmml">Ã—</mo><mn id="S5.SS1.p3.1.m1.1.1.3" xref="S5.SS1.p3.1.m1.1.1.3.cmml">128</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p3.1.m1.1b"><apply id="S5.SS1.p3.1.m1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1"><times id="S5.SS1.p3.1.m1.1.1.1.cmml" xref="S5.SS1.p3.1.m1.1.1.1"></times><cn type="integer" id="S5.SS1.p3.1.m1.1.1.2.cmml" xref="S5.SS1.p3.1.m1.1.1.2">128</cn><cn type="integer" id="S5.SS1.p3.1.m1.1.1.3.cmml" xref="S5.SS1.p3.1.m1.1.1.3">128</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p3.1.m1.1c">128\times 128</annotation></semantics></math>, which enables the model to extract features directly from the eye patches.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparison of Computational Costs.</figcaption>
<table id="S5.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.1.1.1" class="ltx_tr">
<th id="S5.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t">Model</th>
<th id="S5.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Params</th>
<th id="S5.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">FLOPs</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.1.2.1" class="ltx_tr">
<th id="S5.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">RT-GENE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>
</th>
<td id="S5.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">82.0M</td>
<td id="S5.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">30.81G</td>
</tr>
<tr id="S5.T3.1.3.2" class="ltx_tr">
<th id="S5.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GazeTR-Hybrid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>
</th>
<td id="S5.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">11.4M</td>
<td id="S5.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">1.82G</td>
</tr>
<tr id="S5.T3.1.4.3" class="ltx_tr">
<th id="S5.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">GazeCADSE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite>
</th>
<td id="S5.T3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.8M</td>
<td id="S5.T3.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t">12.78G</td>
</tr>
<tr id="S5.T3.1.5.4" class="ltx_tr">
<th id="S5.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t"><span id="S5.T3.1.5.4.1.1" class="ltx_text ltx_font_bold">proposed method</span></th>
<td id="S5.T3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">65.3M</td>
<td id="S5.T3.1.5.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">3.03G</td>
</tr>
</tbody>
</table>
</figure>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.10" class="ltx_p">By using GAM, our proposed model achieves better performance on multiple datasets using only a single main model. This results in a smaller number of parameters compared with other methods. Suppose the number of parameters of the feature extractor is <math id="S5.SS1.p4.1.m1.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS1.p4.1.m1.1a"><mi id="S5.SS1.p4.1.m1.1.1" xref="S5.SS1.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.1.m1.1b"><ci id="S5.SS1.p4.1.m1.1.1.cmml" xref="S5.SS1.p4.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.1.m1.1c">N</annotation></semantics></math> and that of each gaze regressor is <math id="S5.SS1.p4.2.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS1.p4.2.m2.1a"><mi id="S5.SS1.p4.2.m2.1.1" xref="S5.SS1.p4.2.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.2.m2.1b"><ci id="S5.SS1.p4.2.m2.1.1.cmml" xref="S5.SS1.p4.2.m2.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.2.m2.1c">K</annotation></semantics></math>. For <math id="S5.SS1.p4.3.m3.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS1.p4.3.m3.1a"><mi id="S5.SS1.p4.3.m3.1.1" xref="S5.SS1.p4.3.m3.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.3.m3.1b"><ci id="S5.SS1.p4.3.m3.1.1.cmml" xref="S5.SS1.p4.3.m3.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.3.m3.1c">M</annotation></semantics></math> datasets, without GAM we need to train <math id="S5.SS1.p4.4.m4.1" class="ltx_Math" alttext="M" display="inline"><semantics id="S5.SS1.p4.4.m4.1a"><mi id="S5.SS1.p4.4.m4.1.1" xref="S5.SS1.p4.4.m4.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.4.m4.1b"><ci id="S5.SS1.p4.4.m4.1.1.cmml" xref="S5.SS1.p4.4.m4.1.1">ğ‘€</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.4.m4.1c">M</annotation></semantics></math> models for each dataset resulting in total <math id="S5.SS1.p4.5.m5.1" class="ltx_Math" alttext="MN" display="inline"><semantics id="S5.SS1.p4.5.m5.1a"><mrow id="S5.SS1.p4.5.m5.1.1" xref="S5.SS1.p4.5.m5.1.1.cmml"><mi id="S5.SS1.p4.5.m5.1.1.2" xref="S5.SS1.p4.5.m5.1.1.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.5.m5.1.1.1" xref="S5.SS1.p4.5.m5.1.1.1.cmml">â€‹</mo><mi id="S5.SS1.p4.5.m5.1.1.3" xref="S5.SS1.p4.5.m5.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.5.m5.1b"><apply id="S5.SS1.p4.5.m5.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1"><times id="S5.SS1.p4.5.m5.1.1.1.cmml" xref="S5.SS1.p4.5.m5.1.1.1"></times><ci id="S5.SS1.p4.5.m5.1.1.2.cmml" xref="S5.SS1.p4.5.m5.1.1.2">ğ‘€</ci><ci id="S5.SS1.p4.5.m5.1.1.3.cmml" xref="S5.SS1.p4.5.m5.1.1.3">ğ‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.5.m5.1c">MN</annotation></semantics></math> parameters. On the contrary, by applying GAM to train on multiple datasets, we only need one single model with one feature extractor, one gaze regressor and <math id="S5.SS1.p4.6.m6.1" class="ltx_Math" alttext="M-1" display="inline"><semantics id="S5.SS1.p4.6.m6.1a"><mrow id="S5.SS1.p4.6.m6.1.1" xref="S5.SS1.p4.6.m6.1.1.cmml"><mi id="S5.SS1.p4.6.m6.1.1.2" xref="S5.SS1.p4.6.m6.1.1.2.cmml">M</mi><mo id="S5.SS1.p4.6.m6.1.1.1" xref="S5.SS1.p4.6.m6.1.1.1.cmml">âˆ’</mo><mn id="S5.SS1.p4.6.m6.1.1.3" xref="S5.SS1.p4.6.m6.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.6.m6.1b"><apply id="S5.SS1.p4.6.m6.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1"><minus id="S5.SS1.p4.6.m6.1.1.1.cmml" xref="S5.SS1.p4.6.m6.1.1.1"></minus><ci id="S5.SS1.p4.6.m6.1.1.2.cmml" xref="S5.SS1.p4.6.m6.1.1.2">ğ‘€</ci><cn type="integer" id="S5.SS1.p4.6.m6.1.1.3.cmml" xref="S5.SS1.p4.6.m6.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.6.m6.1c">M-1</annotation></semantics></math> MLPs as the gaze offset for the anchor set is always <math id="S5.SS1.p4.7.m7.1" class="ltx_Math" alttext="\boldsymbol{0}" display="inline"><semantics id="S5.SS1.p4.7.m7.1a"><mn id="S5.SS1.p4.7.m7.1.1" xref="S5.SS1.p4.7.m7.1.1.cmml">ğŸ</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.7.m7.1b"><cn type="integer" id="S5.SS1.p4.7.m7.1.1.cmml" xref="S5.SS1.p4.7.m7.1.1">0</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.7.m7.1c">\boldsymbol{0}</annotation></semantics></math>. So the total number of parameters for our proposed model is <math id="S5.SS1.p4.8.m8.1" class="ltx_Math" alttext="N+MK" display="inline"><semantics id="S5.SS1.p4.8.m8.1a"><mrow id="S5.SS1.p4.8.m8.1.1" xref="S5.SS1.p4.8.m8.1.1.cmml"><mi id="S5.SS1.p4.8.m8.1.1.2" xref="S5.SS1.p4.8.m8.1.1.2.cmml">N</mi><mo id="S5.SS1.p4.8.m8.1.1.1" xref="S5.SS1.p4.8.m8.1.1.1.cmml">+</mo><mrow id="S5.SS1.p4.8.m8.1.1.3" xref="S5.SS1.p4.8.m8.1.1.3.cmml"><mi id="S5.SS1.p4.8.m8.1.1.3.2" xref="S5.SS1.p4.8.m8.1.1.3.2.cmml">M</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p4.8.m8.1.1.3.1" xref="S5.SS1.p4.8.m8.1.1.3.1.cmml">â€‹</mo><mi id="S5.SS1.p4.8.m8.1.1.3.3" xref="S5.SS1.p4.8.m8.1.1.3.3.cmml">K</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.8.m8.1b"><apply id="S5.SS1.p4.8.m8.1.1.cmml" xref="S5.SS1.p4.8.m8.1.1"><plus id="S5.SS1.p4.8.m8.1.1.1.cmml" xref="S5.SS1.p4.8.m8.1.1.1"></plus><ci id="S5.SS1.p4.8.m8.1.1.2.cmml" xref="S5.SS1.p4.8.m8.1.1.2">ğ‘</ci><apply id="S5.SS1.p4.8.m8.1.1.3.cmml" xref="S5.SS1.p4.8.m8.1.1.3"><times id="S5.SS1.p4.8.m8.1.1.3.1.cmml" xref="S5.SS1.p4.8.m8.1.1.3.1"></times><ci id="S5.SS1.p4.8.m8.1.1.3.2.cmml" xref="S5.SS1.p4.8.m8.1.1.3.2">ğ‘€</ci><ci id="S5.SS1.p4.8.m8.1.1.3.3.cmml" xref="S5.SS1.p4.8.m8.1.1.3.3">ğ¾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.8.m8.1c">N+MK</annotation></semantics></math>. As <math id="S5.SS1.p4.9.m9.1" class="ltx_Math" alttext="K" display="inline"><semantics id="S5.SS1.p4.9.m9.1a"><mi id="S5.SS1.p4.9.m9.1.1" xref="S5.SS1.p4.9.m9.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.9.m9.1b"><ci id="S5.SS1.p4.9.m9.1.1.cmml" xref="S5.SS1.p4.9.m9.1.1">ğ¾</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.9.m9.1c">K</annotation></semantics></math> is much smaller than <math id="S5.SS1.p4.10.m10.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S5.SS1.p4.10.m10.1a"><mi id="S5.SS1.p4.10.m10.1.1" xref="S5.SS1.p4.10.m10.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.p4.10.m10.1b"><ci id="S5.SS1.p4.10.m10.1.1.cmml" xref="S5.SS1.p4.10.m10.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p4.10.m10.1c">N</annotation></semantics></math>, our method needs fewer parameters to achieve better performance.</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">Table <a href="#S5.T3" title="Table 3 â€£ 5.1 Comparison with state-of-the-art methods â€£ 5 Experiments â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows the number of parameters and the flops for each model. We can see that our proposed method has a fairly low computational cost which we believe is related to two reasons: 1) a relatively smaller model ResNet18 is applied as the backbone, and 2) a smaller size for the two eye patches as inputs.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Ablation Study</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">To study the individual contributions of the TTGF and GAM modules, we conducted ablation experiments by removing one of them from the entire framework.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Ablation study.</figcaption>
<table id="S5.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T4.1.1.1" class="ltx_tr">
<th id="S5.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">Model</th>
<td id="S5.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Multiple Sets</td>
<td id="S5.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">MPIIFaceGaze</td>
<td id="S5.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">RT-GENE</td>
<td id="S5.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t">EYEDIAP</td>
</tr>
<tr id="S5.T4.1.2.2" class="ltx_tr">
<th id="S5.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">itracker-MHSA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>
</th>
<td id="S5.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NO</td>
<td id="S5.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.05 Â°</td>
<td id="S5.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.06Â°</td>
<td id="S5.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t">5.17Â°</td>
</tr>
<tr id="S5.T4.1.3.3" class="ltx_tr">
<th id="S5.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">TTGF-Only</th>
<td id="S5.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">NO</td>
<td id="S5.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">3.98Â°</td>
<td id="S5.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">6.89Â°</td>
<td id="S5.T4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">5.11Â°</td>
</tr>
<tr id="S5.T4.1.4.4" class="ltx_tr">
<th id="S5.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t">TTGF-Only</th>
<td id="S5.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">YES</td>
<td id="S5.T4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">4.12 Â°</td>
<td id="S5.T4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">7.14 Â°</td>
<td id="S5.T4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_t">5.20Â°</td>
</tr>
<tr id="S5.T4.1.5.5" class="ltx_tr">
<th id="S5.T4.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">proposed method (TTGF+GAM)</th>
<td id="S5.T4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">YES</td>
<td id="S5.T4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T4.1.5.5.3.1" class="ltx_text ltx_font_bold">3.88</span>Â°</td>
<td id="S5.T4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">
<span id="S5.T4.1.5.5.4.1" class="ltx_text ltx_font_bold">6.46</span>Â°</td>
<td id="S5.T4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">
<span id="S5.T4.1.5.5.5.1" class="ltx_text ltx_font_bold">4.89</span>Â°</td>
</tr>
</tbody>
</table>
</figure>
<section id="S5.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1 </span>Effect of TTGF</h4>

<div id="S5.SS2.SSS1.p1" class="ltx_para">
<p id="S5.SS2.SSS1.p1.1" class="ltx_p">To study the TTGF, we trained a TTGF-only model on each evaluation dataset and compared the results with itracker-MHSA. We compare with itracker-MHSA because it also uses a transformer encoder to combine eye and head features in a different order.
The itracker-MHSA fuses features first from the left and right eyes and then with the head feature. TTGF fuses features from each eye with head features and then across the two eyesAs we mentioned before, we re-implemented itracker-MHSA with the same backbone as our model for a fair comparison.
As we mentioned before, we re-implemented itracker-MHSA with the same backbone as our model for a fair comparison.</p>
</div>
<div id="S5.SS2.SSS1.p2" class="ltx_para">
<p id="S5.SS2.SSS1.p2.1" class="ltx_p">Table <a href="#S5.T4" title="Table 4 â€£ 5.2 Ablation Study â€£ 5 Experiments â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the angular errors of each method on the evaluation datasets.
The TTGF-only model outperforms the itracker-MHSA on all evaluation datasets.</p>
</div>
</section>
<section id="S5.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2 </span>Effect of GAM</h4>

<div id="S5.SS2.SSS2.p1" class="ltx_para">
<p id="S5.SS2.SSS2.p1.1" class="ltx_p">We compared our proposed model with GAM with the TTGF-only model trained on multiple datasets. Table <a href="#S5.T4" title="Table 4 â€£ 5.2 Ablation Study â€£ 5 Experiments â€£ Merging Multiple Datasets for Improved Appearance-Based Gaze Estimation" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows that with GAM the accuracy of the TTGF-only model without multiple sets of training is improved on all three datasets from 0.1Â°to 0.43Â°respectively.</p>
</div>
<div id="S5.SS2.SSS2.p2" class="ltx_para">
<p id="S5.SS2.SSS2.p2.1" class="ltx_p">To confirm the performance gain in multiple dataset training is due to the use of GAM, we trained the TTGF-only model with the combination of the ETH-XGaze and the evaluation datasets. The TTGF-only model trained on mixed datasets performed even worse than the TTGF-only model trained on each single evaluation set. This supports our claim that GAM can address the inconsistency in annotation across different datasets.</p>
</div>
</section>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We proposed a Two-stage Transformer-based Gaze-future Fusion (TTGF) and the use of Gaze Adaption Modules (GAMs) for improving gaze estimation accuracy. The TTGF uses two-stage fusion for the features of the head and eye images through three transformer encoders. The proposed GAM generates gaze corrections to gaze estimates for one dataset (chosen here to be ETH-Gaze) to create estimates for images from other datasets. Our experiments show that our method surpasses the state-of-the-art by a significant margin. Ablation studies show that both innovations result in improvements when applied in isolation and that improvements compound when they are applied together. However, our proposed model still has some limitations. For example, the proposed TTGF needs cropped eye patches as input. The GAM does not address all issues arising from annotation inconsistency among gaze datasets.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">Patney, A., Salvi, M., Kim, J., Kaplanyan, A., Wyman, C., Benty, N., Luebke, D. &amp; Lefohn, A. Towards foveated rendering for gaze-tracked virtual reality. <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">ACM Transactions On Graphics (TOG)</span>. <span id="bib.bib1.2.2" class="ltx_text ltx_font_bold">35</span>, 1-12 (2016)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">Chen, Z. &amp; Shi, B. Using variable dwell time to accelerate gaze-based web browsing with two-step selection. <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">International Journal Of Humanâ€“Computer Interaction</span>. <span id="bib.bib2.2.2" class="ltx_text ltx_font_bold">35</span>, 240-255 (2019)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">Pi, J. &amp; Shi, B. Probabilistic adjustment of dwell time for eye typing. <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2017 10th International Conference On Human System Interactions (HSI)</span>. pp. 251-257 (2017)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">Recasens, A., Khosla, A., Vondrick, C. &amp; Torralba, A. Where are they looking?. <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Advances In Neural Information Processing Systems</span>. <span id="bib.bib4.2.2" class="ltx_text ltx_font_bold">28</span> (2015)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">Chong, E., Wang, Y., Ruiz, N. &amp; Rehg, J. Detecting attended visual targets in video. <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</span>. pp. 5396-5406 (2020)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">Gehrer, N., SchÃ¶nenberg, M., Duchowski, A. &amp; Krejtz, K. Implementing innovative gaze analytic mods in clinical psychology: A study on eye movements in antisocial violent offenders. <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings Of The 2018 ACM Symposium On Eye Tracking Research &amp; Applications</span>. pp. 1-9 (2018)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">Zhang, X., Sugano, Y., Fritz, M. &amp; Bulling, A. Appearance-based gaze estimation in the wild. <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</span>. pp. 4511-4520 (2015)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">Zhang, X., Sugano, Y., Fritz, M. &amp; Bulling, A. Itâ€™s written all over your face: Full-face appearance-based gaze estimation. <span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition Workshops</span>. pp. 51-60 (2017)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">Chen, Z. &amp; Shi, B. Appearance-based gaze estimation using dilated-convolutions. <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Asian Conference On Computer Vision</span>. pp. 309-324 (2018)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">Fischer, T., Chang, H. &amp; Demiris, Y. Rt-gene: Real-time eye gaze estimation in natural environments. <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings Of The European Conference On Computer Vision (ECCV)</span>. pp. 334-352 (2018)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">Guestrin, E. &amp; Eizenman, M. General theory of remote gaze estimation using the pupil center and corneal reflections. <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">IEEE Transactions On Biomedical Engineering</span>. <span id="bib.bib11.2.2" class="ltx_text ltx_font_bold">53</span>, 1124-1133 (2006)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">Krafka, K., Khosla, A., Kellnhofer, P., Kannan, H., Bhandarkar, S., Matusik, W. &amp; Torralba, A. Eye tracking for everyone. <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</span>. pp. 2176-2184 (2016)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">Chen, J. &amp; Ji, Q. 3D gaze estimation with a single camera without IR illumination. <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">2008 19th International Conference On Pattern Recognition</span>. pp. 1-4 (2008)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">Valenti, R., Sebe, N. &amp; Gevers, T. Combining head pose and eye location information for gaze estimation. <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">IEEE Transactions On Image Processing</span>. <span id="bib.bib14.2.2" class="ltx_text ltx_font_bold">21</span>, 802-815 (2011)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">Wood, E. &amp; Bulling, A. Eyetab: Model-based gaze estimation on unmodified tablet computers. <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings Of The Symposium On Eye Tracking Research And Applications</span>. pp. 207-210 (2014)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">Tan, K., Kriegman, D. &amp; Ahuja, N. Appearance-based eye gaze estimation. <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Sixth IEEE Workshop On Applications Of Computer Vision, 2002. (WACV 2002). Proceedings.</span>. pp. 191-195 (2002)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">Lu, F., Sugano, Y., Okabe, T. &amp; Sato, Y. Adaptive linear regression for appearance-based gaze estimation. <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">IEEE Transactions On Pattern Analysis And Machine Intelligence</span>. <span id="bib.bib17.2.2" class="ltx_text ltx_font_bold">36</span>, 2033-2046 (2014)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">Williams, O., Blake, A. &amp; Cipolla, R. Sparse and Semi-supervised Visual Mapping with the Sundefined 3GP. <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">2006 IEEE Computer Society Conference On Computer Vision And Pattern Recognition (CVPR)</span>. <span id="bib.bib18.2.2" class="ltx_text ltx_font_bold">1</span> pp. 230-237 (2006)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">Cheng, Y. &amp; Lu, F. Gaze estimation using transformer. <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">2022 26th International Conference On Pattern Recognition (ICPR)</span>. pp. 3341-3347 (2022)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">Tu, D., Min, X., Duan, H., Guo, G., Zhai, G. &amp; Shen, W. End-to-End Human-Gaze-Target Detection with Transformers. <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:2203.10433</span>. (2022)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">Cai, X., Chen, B., Zeng, J., Zhang, J., Sun, Y., Wang, X., Ji, Z., Liu, X., Chen, X. &amp; Shan, S. Gaze Estimation with an Ensemble of Four Architectures. <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:2107.01980</span>. (2021)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">Lv, J., Chen, W., Li, Q. &amp; Yang, C. Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns. <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE Conference On Computer Vision And Pattern Recognition</span>. pp. 7948-7956 (2018)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">Li, Y., Lin, C., Lin, Y. &amp; Wang, Y. Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation. <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE/CVF International Conference On Computer Vision</span>. pp. 7919-7929 (2019)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">Ranftl, R., Lasinger, K., Hafner, D., Schindler, K. &amp; Koltun, V. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">IEEE Transactions On Pattern Analysis And Machine Intelligence</span>. (2020)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">He, H., Zhang, J., Zhang, Q. &amp; Tao, D. Grapy-ML: Graph Pyramid Mutual Learning for Cross-Dataset Human Parsing. <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">The Thirty-Fourth AAAI Conference On Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications Of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium On Educational Advances In Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020</span>. pp. 10949-10956 (2020), https://ojs.aaai.org/index.php/AAAI/article/view/6728

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">Lambert, J., Liu, Z., Sener, O., Hays, J. &amp; Koltun, V. MSeg: A composite dataset for multi-domain semantic segmentation. <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</span>. pp. 2879-2888 (2020)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">Li, D., Jiang, T. &amp; Jiang, M. Unified quality assessment of in-the-wild videos with mixed datasets training. <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">International Journal Of Computer Vision</span>. <span id="bib.bib27.2.2" class="ltx_text ltx_font_bold">129</span>, 1238-1257 (2021)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">Zhang, W., Li, W. &amp; Xu, D. SRDAN: Scale-aware and range-aware domain adaptation network for cross-dataset 3D object detection. <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</span>. pp. 6769-6779 (2021)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">Smith, B., Yin, Q., Feiner, S. &amp; Nayar, S. Gaze locking: passive eye contact detection for human-object interaction. <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings Of The 26th Annual ACM Symposium On User Interface Software And Technology</span>. pp. 271-280 (2013)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">Korhonen, J. Two-level approach for no-reference consumer video quality assessment. <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE Transactions On Image Processing</span>. <span id="bib.bib30.2.2" class="ltx_text ltx_font_bold">28</span>, 5923-5938 (2019)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">Zhang, X., Sugano, Y. &amp; Bulling, A. Revisiting data normalization for appearance-based gaze estimation. <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proceedings Of The 2018 ACM Symposium On Eye Tracking Research &amp; Applications</span>. pp. 1-9 (2018)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">Rodrigues, R., Barreto, J. &amp; Nunes, U. Camera pose estimation using images of planar mirror reflections. <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">European Conference On Computer Vision</span>. pp. 382-395 (2010)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">Zhang, X., Park, S., Beeler, T., Bradley, D., Tang, S. &amp; Hilliges, O. Eth-xgaze: A large scale dataset for gaze estimation under extreme head pose and gaze variation. <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">European Conference On Computer Vision</span>. pp. 365-381 (2020)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">Funes Mora, K., Monay, F. &amp; Odobez, J. Eyediap: A database for the development and evaluation of gaze estimation algorithms from rgb and rgb-d cameras. <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings Of The Symposium On Eye Tracking Research And Applications</span>. pp. 255-258 (2014)

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">Park, S., Spurr, A. &amp; Hilliges, O. Deep pictorial gaze estimation. <span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Proceedings Of The European Conference On Computer Vision (ECCV)</span>. pp. 721-738 (2018)

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">Liu, G., Yu, Y., Mora, K. &amp; Odobez, J. A differential approach for gaze estimation. <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">IEEE Transactions On Pattern Analysis And Machine Intelligence</span>. <span id="bib.bib36.2.2" class="ltx_text ltx_font_bold">43</span>, 1092-1099 (2019)

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S. &amp; Others An image is worth 16x16 words: Transformers for image recognition at scale. <span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">ArXiv Preprint ArXiv:2010.11929</span>. (2020)

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, Å. &amp; Polosukhin, I. Attention is all you need. <span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Advances In Neural Information Processing Systems</span>. <span id="bib.bib38.2.2" class="ltx_text ltx_font_bold">30</span> (2017)

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. &amp; Luo, P. SegFormer: Simple and efficient design for semantic segmentation with transformers. <span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Advances In Neural Information Processing Systems</span>. <span id="bib.bib39.2.2" class="ltx_text ltx_font_bold">34</span> pp. 12077-12090 (2021)

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">Kim, B., Lee, J., Kang, J., Kim, E. &amp; Kim, H. Hotr: End-to-end human-object interaction detection with transformers. <span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</span>. pp. 74-83 (2021)

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">Zhang, W., Qiu, F., Wang, S., Zeng, H., Zhang, Z., An, R., Ma, B. &amp; Ding, Y. Transformer-based Multimodal Information Fusion for Facial Expression Analysis. <span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</span>. pp. 2428-2437 (2022)

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">O Oh, J., Chang, H. &amp; Choi, S. Self-Attention With Convolution and Deconvolution for Efficient Eye Gaze Estimation From a Full Face Image. <span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">Proceedings Of The IEEE/CVF Conference On Computer Vision And Pattern Recognition</span>. pp. 4992-5000 (2022)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.00911" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.00912" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.00912">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.00912" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.00913" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:26:59 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
