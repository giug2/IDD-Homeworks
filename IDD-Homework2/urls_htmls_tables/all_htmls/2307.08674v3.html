<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.08674] TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT</title><meta property="og:description" content="Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate.
The advancements in large language models (LLMs) have made it possible to interact with tables u…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.08674">

<!--Generated on Wed Feb 28 17:32:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Liangyu Zha<sup id="id20.20.id1" class="ltx_sup"><span id="id20.20.id1.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Junlin Zhou<sup id="id21.21.id2" class="ltx_sup"><span id="id21.21.id2.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Liyao Li<sup id="id22.22.id3" class="ltx_sup"><span id="id22.22.id3.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Rui Wang<sup id="id23.23.id4" class="ltx_sup"><span id="id23.23.id4.1" class="ltx_text ltx_font_italic">1,2</span></sup>  Qingyi Huang<sup id="id24.24.id5" class="ltx_sup"><span id="id24.24.id5.1" class="ltx_text ltx_font_italic">3</span></sup> 
<br class="ltx_break"><span id="id11.11.6" class="ltx_text ltx_font_bold"> Saisai Yang<sup id="id11.11.6.1" class="ltx_sup"><span id="id11.11.6.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup>  Jing Yuan<sup id="id11.11.6.2" class="ltx_sup"><span id="id11.11.6.2.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup>  Changbao Su<sup id="id11.11.6.3" class="ltx_sup"><span id="id11.11.6.3.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup>  Xiang Li<sup id="id11.11.6.4" class="ltx_sup"><span id="id11.11.6.4.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup>  Aofeng Su<sup id="id11.11.6.5" class="ltx_sup"><span id="id11.11.6.5.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup>  Tao Zhang<sup id="id11.11.6.6" class="ltx_sup"><span id="id11.11.6.6.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup></span> 
<br class="ltx_break">
<span id="id12.12.7" class="ltx_text ltx_font_bold">Chen Zhou<sup id="id12.12.7.1" class="ltx_sup"><span id="id12.12.7.1.1" class="ltx_text ltx_font_medium ltx_font_italic">3</span></sup>  Kaizhe Shou  Miao Wang  Wufang Zhu  Guoshan Lu  Chao Ye</span> 
<br class="ltx_break"><span id="id15.15.10" class="ltx_text ltx_font_bold">Yali Ye  Wentao Ye  Yiming Zhang  Xinglong Deng  Jie Xu 
<br class="ltx_break">
Haobo Wang<sup id="id15.15.10.1" class="ltx_sup"><span id="id15.15.10.1.1" class="ltx_text ltx_font_medium">4</span></sup>  Gang Chen<sup id="id15.15.10.2" class="ltx_sup"><span id="id15.15.10.2.1" class="ltx_text ltx_font_medium">4</span></sup>  Junbo Zhao<sup id="id15.15.10.3" class="ltx_sup"><span id="id15.15.10.3.1" class="ltx_text ltx_font_medium">4</span></sup></span> 
<br class="ltx_break"><sup id="id25.25.id6" class="ltx_sup"><span id="id25.25.id6.1" class="ltx_text" style="font-size:90%;">1</span></sup><span id="id19.19.13" class="ltx_text" style="font-size:90%;">directional lead  <sup id="id19.19.13.1" class="ltx_sup">2</sup>joint first author  <sup id="id19.19.13.2" class="ltx_sup">3</sup>equal contribution  <sup id="id19.19.13.3" class="ltx_sup">4</sup>project lead 
<br class="ltx_break">
<br class="ltx_break">Zhejiang University
<br class="ltx_break"></span>
</span><span class="ltx_author_notes"><span id="id26.26.id1" class="ltx_text ltx_font_bold">Correspondence to </span><span id="id27.27.id2" class="ltx_text ltx_font_typewriter ltx_font_bold">j.zhao@zju.edu.cn</span><span id="id28.28.id3" class="ltx_text ltx_font_bold">.</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id29.id1" class="ltx_p">Tables are prevalent in real-world databases, requiring significant time and effort for humans to analyze and manipulate.
The advancements in large language models (LLMs) have made it possible to interact with tables using natural language input, bringing this capability closer to reality.
In this paper, we present TableGPT, a unified fine-tuned framework that enables LLMs to understand and operate on tables using external functional commands.
It introduces the capability to seamlessly interact with tables, enabling a wide range of functionalities such as question answering, data manipulation (e.g., insert, delete, query, and modify operations), data visualization, analysis report generation, and automated prediction. TableGPT aims to provide convenience and accessibility to users by empowering them to effortlessly leverage tabular data.
At the core of TableGPT lies the novel concept of global tabular representations, which empowers LLMs to gain a comprehensive understanding of the entire table beyond meta-information.
By jointly training LLMs on both table and text modalities, TableGPT achieves a deep understanding of tabular data and the ability to perform complex operations on tables through chain-of-command instructions. Importantly, TableGPT offers the advantage of being a self-contained system rather than relying on external API interfaces. Moreover, it supports efficient data process flow, query rejection (when appropriate) and private deployment, enabling faster domain data fine-tuning and ensuring data privacy, which enhances the framework’s adaptability to specific use cases.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The vast and intricate world of data is often encapsulated in tables, being a foundation for data-driven decision-making in a wide spectrum of applications, including financial analysis, supply chain management, and healthcare analytics.
It enables stakeholders to analyze trends, patterns, and relationships, leading to informed business decisions, process improvements, and resource optimization.
For years, data scientists have struggled to process tables using complicated Excel formulas or handcrafted programming <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>; <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
Consequently, there has been an urgent need to understand and interpret tabular data in a more efficient fashion.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In the field of natural language processing, Generative Pre-trained Transformers (GPTs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>; <a href="#bib.bib25" title="" class="ltx_ref">25</a>; <a href="#bib.bib2" title="" class="ltx_ref">2</a>; <a href="#bib.bib22" title="" class="ltx_ref">22</a>; <a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> or Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>; <a href="#bib.bib36" title="" class="ltx_ref">36</a>; <a href="#bib.bib27" title="" class="ltx_ref">27</a>; <a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> have <span id="S1.p2.1.1" class="ltx_text ltx_font_bold">revolutionized</span> the paradigm of language data mining.
Following this line of works, researchers have also explored large models for various modalities like vision <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>; <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, and speech <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
From a technical standpoint, their ability to generate human-like text has opened new vistas of possibilities for processing tabular data.
Nevertheless, it is non-trivial to directly employ the vanilla ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> model in the tabular area for two reasons:
(i)-<span id="S1.p2.1.2" class="ltx_text ltx_font_bold">Global Table Understanding</span>: the GPTs are known to suffer from the limited token length and thus, they can not read a whole large table, making them hard to understand the global tabular information.
(ii)-<span id="S1.p2.1.3" class="ltx_text ltx_font_bold">Generalized to Tabular Domain: </span>
Second, their training processes are tailored for natural languages and thus, they are less generalizable when handling tabular data.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparisons with previous command-using LLMs for tabular data. (See details in Sec <a href="#S3.SS2" title="3.2 Comparison with previous command-using LLMs ‣ 3 Evaluation ‣ TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>)</figcaption>
<div id="S1.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:106.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-151.2pt,37.3pt) scale(0.589083958164138,0.589083958164138) ;">
<table id="S1.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S1.T1.1.1.1" class="ltx_tr">
<th id="S1.T1.1.1.1.1" class="ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt"><svg version="1.1" height="19.58" width="112.02" overflow="visible"><g transform="translate(0,19.58) scale(1,-1)"><path d="M 0,19.58 112.02,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,10.93) scale(1, -1)"><foreignObject width="56.01" height="10.93" overflow="visible">
<span id="S1.T1.1.1.1.1.pic1.1.1" class="ltx_inline-block">
<span id="S1.T1.1.1.1.1.pic1.1.1.1" class="ltx_inline-block ltx_align_left">
<span id="S1.T1.1.1.1.1.pic1.1.1.1.1" class="ltx_p"><span id="S1.T1.1.1.1.1.pic1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Properties</span></span>
</span>
</span></foreignObject></g></g><g class="ltx_svg_fog" transform="translate(64.9,10.93)"><g transform="translate(0,8.65) scale(1, -1)"><foreignObject width="47.12" height="8.65" overflow="visible">
<span id="S1.T1.1.1.1.1.pic1.2.1" class="ltx_inline-block">
<span id="S1.T1.1.1.1.1.pic1.2.1.1" class="ltx_inline-block ltx_align_right">
<span id="S1.T1.1.1.1.1.pic1.2.1.1.1" class="ltx_p"><span id="S1.T1.1.1.1.1.pic1.2.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Methods</span></span>
</span>
</span></foreignObject></g></g></g></svg></th>
<th id="S1.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">ChatExcel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite></span></th>
<th id="S1.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">SheetCopilot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite></span></th>
<th id="S1.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S1.T1.1.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Data-Copilot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite></span></th>
<th id="S1.T1.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt">
<span id="S1.T1.1.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">TableGPT</span><span id="S1.T1.1.1.1.5.2" class="ltx_text" style="font-size:90%;"> (ours)</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S1.T1.1.1.2.1" class="ltx_tr">
<td id="S1.T1.1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S1.T1.1.1.2.1.1.1" class="ltx_text" style="font-size:90%;">Nature Language Operations</span></td>
<td id="S1.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.1.2.1.3.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.1.2.1.4.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S1.T1.1.1.2.1.5.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
</tr>
<tr id="S1.T1.1.1.3.2" class="ltx_tr">
<td id="S1.T1.1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.1.3.2.1.1" class="ltx_text" style="font-size:90%;">Generalization to Arbitrary Tables</span></td>
<td id="S1.T1.1.1.3.2.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.3.2.2.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.3.2.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.3.2.3.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.3.2.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.3.2.4.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.3.2.5" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.3.2.5.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
</tr>
<tr id="S1.T1.1.1.4.3" class="ltx_tr">
<td id="S1.T1.1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.1.4.3.1.1" class="ltx_text" style="font-size:90%;">Visualization</span></td>
<td id="S1.T1.1.1.4.3.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.4.3.2.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.4.3.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.4.3.3.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.4.3.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.4.3.4.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.4.3.5" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.4.3.5.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
</tr>
<tr id="S1.T1.1.1.5.4" class="ltx_tr">
<td id="S1.T1.1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Analysis &amp; Report</span></td>
<td id="S1.T1.1.1.5.4.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.5.4.2.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.5.4.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.5.4.3.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.5.4.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.5.4.4.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.5.4.5" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.5.4.5.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
</tr>
<tr id="S1.T1.1.1.6.5" class="ltx_tr">
<td id="S1.T1.1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Prediction</span></td>
<td id="S1.T1.1.1.6.5.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.6.5.2.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.6.5.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.6.5.3.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.6.5.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.6.5.4.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.6.5.5" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.6.5.5.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
</tr>
<tr id="S1.T1.1.1.7.6" class="ltx_tr">
<td id="S1.T1.1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.1.7.6.1.1" class="ltx_text" style="font-size:90%;">Chain-of-command</span></td>
<td id="S1.T1.1.1.7.6.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.7.6.2.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.7.6.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.7.6.3.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.7.6.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.7.6.4.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
<td id="S1.T1.1.1.7.6.5" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.7.6.5.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
</tr>
<tr id="S1.T1.1.1.8.7" class="ltx_tr">
<td id="S1.T1.1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.1.8.7.1.1" class="ltx_text" style="font-size:90%;">Base Model</span></td>
<td id="S1.T1.1.1.8.7.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.8.7.2.1" class="ltx_text" style="font-size:90%;">Unknown</span></td>
<td id="S1.T1.1.1.8.7.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.8.7.3.1" class="ltx_text" style="font-size:90%;">API</span></td>
<td id="S1.T1.1.1.8.7.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.8.7.4.1" class="ltx_text" style="font-size:90%;">API</span></td>
<td id="S1.T1.1.1.8.7.5" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.8.7.5.1" class="ltx_text" style="font-size:90%;">Fine-tuned</span></td>
</tr>
<tr id="S1.T1.1.1.9.8" class="ltx_tr">
<td id="S1.T1.1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_r"><span id="S1.T1.1.1.9.8.1.1" class="ltx_text" style="font-size:90%;">Vague Input Rejection</span></td>
<td id="S1.T1.1.1.9.8.2" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.9.8.2.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.9.8.3" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.9.8.3.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.9.8.4" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.9.8.4.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.9.8.5" class="ltx_td ltx_align_center"><span id="S1.T1.1.1.9.8.5.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
</tr>
<tr id="S1.T1.1.1.10.9" class="ltx_tr">
<td id="S1.T1.1.1.10.9.1" class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span id="S1.T1.1.1.10.9.1.1" class="ltx_text" style="font-size:90%;">Private Deployment</span></td>
<td id="S1.T1.1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.1.1.10.9.2.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.1.1.10.9.3.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.10.9.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.1.1.10.9.4.1" class="ltx_text" style="font-size:90%;color:#FF9400;">✗</span></td>
<td id="S1.T1.1.1.10.9.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="S1.T1.1.1.10.9.5.1" class="ltx_text" style="font-size:90%;color:#009900;">✓</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">There have been several works <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>; <a href="#bib.bib39" title="" class="ltx_ref">39</a>; <a href="#bib.bib18" title="" class="ltx_ref">18</a>; <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> developed to integrate natural language for tabular data analysis. NL2SQL (Nature language to SQL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>; <a href="#bib.bib39" title="" class="ltx_ref">39</a>; <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> is a long-standing research topic that converts natural language to SQL commands that manipulate the relational database. Recently, SheetCopilot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> explored languages to VBA (Visual Basic for Applications, an embedded script language for Microsoft Excel) command such that benefit from a rich set of spreadsheet software functionalities.
However, we found that both solutions demonstrate unsatisfactory performance. We speculate that these forms of programming code, which is fundamentally unstructured, adds another layer of complexity, making automated post-processing almost insurmountable.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">In this work, we develop TableGPT that pushes the boundaries of what is possible in data analysis empowered by LLM techniques, marking an important step forward in our pursuit of making data more accessible and understandable.
Our TableGPT framework unifies tables, natural language, and commands into a single GPT model, making data interpretation and manipulation more intuitive and user-friendly.
By rethinking the interaction of tables, natural language, and commands, we integrate several core components into TableGPT:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Global Table Representation: </span> We make the first attempt to develop a global representation learning paradigm for tables that encodes the whole table into one vector. By jointly training the LLM and a table encoder on vast amounts of text and table data, we equip the encoder to adequately capture the global information in the input table. This enables the LLM to perceive and understand the table data effectively, thereby providing a more global and enhanced comprehension of tables.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Chain-of-Command: </span> We introduce this concept to emphasize the essential idea of a structured and hierarchical execution of tasks. Just like a well-coordinated organization where each directive is cascaded from a higher level to its lower counterpart, TableGPT follows a similar chain of commands, breaking down complex tasks into simpler ones and executing them step-by-step. Moreover, it fosters the ability to refuse ambiguous or inappropriate commands, much like an actual data scientist, instead of blindly following any potential erroneous instruction, thereby improving the interaction between humans and LLM systems in the field of data science. Our proposed command set is not only easier to control but also reduces the uncertainty that often accompanies traditional methods of handling table data.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Domain-aware Fine-Tuning: </span> To foster the ability to adapt to specific domains of tables and corresponding textual materials, domain-aware fine-tuning hinges on customizing training in a way that the model generates text embodying similar stylistic and logical elements found in a given domain, thereby augmenting its understanding of specific domain table data. To make this approach scalable and feasible, we have also developed a data processing pipeline that yields notable improvements with only a small amount of data, hence alleviating the resource-demanding aspect of training LLMs and supporting private deployment.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">From a real-world production standpoint, the unstructured code outputted by NL2SQL poses significant challenges for preemptive checks and error corrections. Hence, we advocate for the use of structured command sequences, simplifying post-processing.
Data-Copilot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> also embraces this command-based approach with self-instruct <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, but its reliance on API-called native LLMs to comprehend tabular data’s processing and analysis logic directly presents limitations. Given the intrinsic data variability and task-specificity of tabular data, we believe an effective product should be custom-built for tabular data while maintaining general applicability to broader downstream tasks. This conviction underscores the imperative of introducing a LLM specifically pre-trained for tabular data.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">To sum up, this work presents a pioneering TableGPT framework, which is a unified, well-fledged holistic solution, enabling efficient tabular data processing, analysis and visualization, driven all by natural languages.
We summarize several important advantages of TableGPT as follows:</p>
<ul id="S1.I2" class="ltx_itemize">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p"><span id="S1.I2.i1.p1.1.1" class="ltx_text ltx_font_bold">Language-driven EDA: </span> TableGPT understands user intent from natural language, dissects the desired actions, and executes external commands on the table. It subsequently returns the processed results in both tabular and textual explanations to the user. This novel approach simplifies the way users engage with table data, bringing an intuitive instantiation to Exploratory Data Analysis (EDA).</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p"><span id="S1.I2.i2.p1.1.1" class="ltx_text ltx_font_bold">Unified Cross-modal Framework: </span> Innovatively, we devise a global table encoder for understanding the whole table. TableGPT is able to fully understand the user query, metaknowledge, and whole tabular data, which leads to much more reliable execution commands for table manipulation.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p"><span id="S1.I2.i3.p1.1.1" class="ltx_text ltx_font_bold">Generalization and Privacy: </span> By domain-aware fine-tuning, our TableGPT can better handle data variability of tables and generalize to different domains. Further, our framework supports private deployment, offering robust data privacy protections. This aspect is critical in the modern age where data privacy and protection are just paramount.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2307.08674/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="296" height="166" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An architecture of TableGPT framework.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>TableGPT</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Model Design</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">The development of TableGPT begins with the foundation provided by pre-trained LLMs. The advancements in the field of natural language processing have led to the development of a number of exceptional open-source LLMs, such as LLaMa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>, Phoenix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, ChatGLM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>, Ziya <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>, and Baichuan <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
In designing TableGPT, we opted to use Phoenix <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> with 7B parameters as our base model for fine-tuning, owing to its excellent capabilities in handling both Chinese and English languages. This choice is not, however, exclusive. Our model design supports adaptation with other LLMs, providing versatility and flexibility in its implementation.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">What sets TableGPT apart from its predecessors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>; <a href="#bib.bib17" title="" class="ltx_ref">17</a>; <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> is the novel approach to its fine-tuning process. We performed the fine-tuning on a vast corpus, comprising 2T tokens of textual data and 0.3M tables. This corpus offers a diverse landscape for the model to learn from, including but not limited to user query-command sequence pairs and publicly available domain-specific data for table analysis reports.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The overall architecture of TableGPT is shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. When a user inputs a table and a query, these are received by TableGPT, which consists of a table encoder and an LLM.
The table encoder serves to extract vector representations from the input table. These representations, coupled with the text query, are then fed into the LLM for inference. The LLM discerns the user’s query intent and generates an output that includes both a command sequence and a textual reply.
The command sequence undergoes error correction in the command system’s corrector before it is fed into the executor for execution. The final output, provided to the user, includes the manipulated table and a textual reply. This streamlined process delivers efficient, reliable responses to table data queries, enhancing user experience and simplifying data analysis.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Global Representation of Table</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">The rapid development of large language models (LLMs) has seen them interfacing with a multitude of modalities such as vision, and audio. For instance, the integration of vision and LLMs has led to models like CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> (Contrastive Language–Image Pretraining) from OpenAI that connects images and text through shared latent space. The combination of audio and LLMs gave rise to models like Wave2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> and Tacotron <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> that employ the representation of audio in the form of spectrograms to generate or understand speech.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Despite these advancements, the exploration of LLMs interfacing with tabular data remains limited. The question of how to enable LLMs to comprehend and interpret tables is essential. Some studies have attempted to convert sample rows of table data directly into a sentence-like text description <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, while others have attempted to artificially define a global representation of table data through the template-based extraction of column names, industry background, and other metadata schema <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. However, these approaches only extract partial information from table data for LLMs, consequently overlooking the global information and industry background inherent in the data.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p">Notably, for the tables, it is required to embed the whole table into one single vector, instead of generating sample-wise embedding. This can be non-trivial and challenging because, unlike images, videos, and audio, table data is inherently a highly abstract structured data type.
Furthermore, it possesses a dual permutation invariance structure where shuffling rows or columns does not affect the information contained within the table, a distinct contrast to images and audio, which carry inductive bias in adjacent positions or sequences. Moreover, tables from different domains vary in size and format, such as having different numbers of discrete and continuous columns, making it challenging to extract features from diverse tables using a unified neural network architecture <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">Yet, it remains an open problem to effectively extract global representations from tables for LLMs to achieve comprehensive table understanding. To this end, we present a Cascaded Table Encoder that jointly extracts knowledge from metadata and whole numerical entries.</p>
</div>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Cascaded Table Encoder. </h4>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">Consider the approach of an experienced data scientist encountering a table. They typically examine the structure of the table data, such as the table headers and distribution of feature columns, to understand the meaning of different cells based on their position, without focusing too much on the numeric information of each cell.
Following this biologically plausible approach, we propose a novel cascading table encoder. It divides the information in the table data into two main parts. The first part learns the metadata representation of the table, such as schema, industry background, and the meanings of column names, which can help LLMs understand the global information of the table structure. The second part learns the numerical information representation of the table, such as the distribution and trends of values in different columns, helping LLMs understand the global information of the table numbers like human experts.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p2" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p2.1" class="ltx_p">We consider the rows and columns of the table as elements of a set and learn the overall representation of the entire set. We use a modified set transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> as the backbone of the table encoder. The set transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, originally designed for dealing with permutation invariant problems, aligns well with the inherent structure of tabular data. We enhance it with an attention mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> that can capture the interdependencies between different rows or columns of the table, enabling the model to understand the relations between different parts of the table data.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p3" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p3.1" class="ltx_p">This encoder is pre-trained on ten thousand table datasets using a masked table modeling approach, similar to the masked language modeling used in BERT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> but adapted to tabular data. The learned table representation not only can be used for table understanding but also can enhance the predictive performance of downstream classifiers.</p>
</div>
<div id="S2.SS2.SSS0.Px1.p4" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p4.1" class="ltx_p">Our proposed method presents a significant step forward in the integration of tables, natural language, and commands into LLMs. It provides a comprehensive approach for extracting global representations from tables and enables LLMs to understand and manipulate.</p>
</div>
</section>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Chain-of-Command</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In recognition of the fact that Large Language Models (LLMs) like GPT can struggle with numerical reasoning, prone to computational errors and hallucinations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, our approach does not require them to operate and calculate within the tables in their latent space. Instead, we provide a series of pre-packaged function commands for LLMs to call upon. LLMs, understanding the global representation of the table and user input, generate a sequence of commands for the backend system to execute, resulting in a modified table. Compared to the SQL statements generated by text2SQL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>; <a href="#bib.bib39" title="" class="ltx_ref">39</a>; <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, these command sequences are more easily examined and error-located by the backend parsing system, while SQL statements can be challenging to diagnose and correct for specific errors.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">However, user queries are often vague and complex, and we can only encapsulate and provide some basic table operation commands. Teaching the LLM to deconstruct complex and vague queries is crucial. For example, a user’s query for a specified object column could be a synonym or translation of a column in the original table, or the user may only have a vague intent and cannot express the demand clearly.</p>
</div>
<div id="S2.SS3.p3" class="ltx_para">
<p id="S2.SS3.p3.1" class="ltx_p">The Chain-of-thought <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>; <a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> approach emphasizes breaking down complex reasoning into a series of intermediate steps. We introduce the concept of Chain-of-command (CoC), an approach that enhances the chain-of-thought by providing a mechanism for step-by-step instructions associated with these intermediate steps. For instance, when a user asks, "Show me the five movies with the highest profit margin," the LLM first checks if a profit margin column exists in the table. If not, it generates arithmetic instructions to calculate the profit margin using box office and cost data; next, it executes instructions to sort by profit margin in descending order and slice to select the top five movies. When user queries are too vague, like "Give me some numbers," the LLM might struggle to decompose and could refuse execution, instead, it would ask the user for more specific intent.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para">
<p id="S2.SS3.p4.1" class="ltx_p">The aim of the Chain-of-command is to enhance LLM’s reasoning capabilities and robustness when operating table data. This approach involves translating user inputs into a sequence of intermediate command operations, enabling LLMs to manipulate tables more accurately and efficiently symbolically. The ability to manipulate symbolic instructions is particularly valuable for real-world applications involving complex and accurate interactions with historical data, such as record-keeping and data analysis in management environments.</p>
</div>
<div id="S2.SS3.p5" class="ltx_para">
<p id="S2.SS3.p5.1" class="ltx_p">To enhance the performance and stability of our approach, we constructed a substantial dataset of command chain instructions while fine-tuning LLMs to adapt to commands, and employed contextual learning to provide prompts for multiple steps in the command chain sequence. A strong and accurate command chain process allows LLMs to better reason about table data and handle more complex scenarios.</p>
</div>
<div id="S2.SS3.p6" class="ltx_para">
<p id="S2.SS3.p6.1" class="ltx_p">The Chain-of-command approach has three main advantages. First, it enables LLMs to execute complex table instructions accurately, thereby enhancing their multi-hop reasoning capabilities for table operations. Second, by breaking down complex operations into a series of intermediate table operations, the chain-of-command method enhances the LLM’s ability to handle complex multi-table interactions. Lastly, it enables LLMs to refuse overly vague instructions and ask users for more specific intent. This approach allows LLMs to handle edge cases and unexpected scenarios better, making it a promising method for real-world applications.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Domain Data Processing Pipeline</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">Despite the broad knowledge and dialogue capabilities of large language models (LLMs) due to extensive pre-training on a diverse corpus, their performance often falls short in addressing the nuanced language styles and logic of specific industries. This is primarily due to the lack of exposure to proprietary domain data during their training phase. To mitigate this issue, we have developed an efficient domain data processing pipeline <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>; <a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>.</p>
</div>
<div id="S2.SS4.p2" class="ltx_para">
<p id="S2.SS4.p2.1" class="ltx_p">Motivated by the goal to streamline the fine-tuning process of LLMs with minimal computational overhead and accelerated model iteration, our pipeline is designed to harness the power of active learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>. Through this, we curate a carefully selected set of fine-tuning examples from the domain data, allowing LLMs to achieve superior fine-tuning results with a reduced number of examples. This strategic utilization of resources expedites the model’s learning process, thereby speeding up its iteration.</p>
</div>
<div id="S2.SS4.p3" class="ltx_para">
<p id="S2.SS4.p3.1" class="ltx_p">Additionally, we have fortified the document retrieval capabilities of LLMs. We utilize technologies like vector databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and LangChain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to facilitate the retrieval of pertinent information from a plethora of proprietary documents, further enriching the context that LLMs learn from.</p>
</div>
<div id="S2.SS4.p4" class="ltx_para">
<p id="S2.SS4.p4.1" class="ltx_p">In essence, our pipeline serves as a catalyst for the rapid and cost-effective adaptation of LLMs to the data needs of various specific industries. This pipeline not only addresses the challenges of industry-specific language styles and logic but also empowers LLMs to handle commands that interact with tables, integrating the realms of natural language, tables, and commands.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Evaluation</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Commands supported by TableGPT</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">To unleash the power of TableGPT, we have designed and supported a rich set of commands.
Firstly, TableGPT enables natural language interaction with tables, empowering users to intuitively query, filter, sort, and aggregate data using everyday language. It also facilitates tasks such as data visualization and report generation, enhancing the interpretability and presentation of tabular information. Lastly, TableGPT facilitates automated decision-making processes, empowering users to make predictions, forecast trends, and estimate outcomes using table data and natural language instructions.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">Note that when the intent of the user query is too vague, TableGPT will reject to generate commands and instead ask the user for more detailed intent. This is one of the benefits of chain-of-command, the ability to think about the rationality of commands like a human expert, rather than a rigid command translator.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Comparison with previous command-using LLMs</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Several existing solutions attempt to combine tables and language models, such as ChatExcel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, SheetCopilot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and Data-Copilot <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite>. These approaches typically rely on using prompts to invoke pre-defined external commands through inference API of LLMs, such as OpenAI API<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://openai.com/blog/openai-api</span></span></span>. In contrast, TableGPT takes a different approach by fine-tuning LLM specifically for table-related tasks. This key distinction allows us to harness the inherent capabilities of the LLM architecture while tailoring it to excel in table processing tasks.
A detailed comparison of TableGPT with the previous command-using LLMs is shown in Table <a href="#S1.T1" title="Table 1 ‣ 1 Introduction ‣ TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Case Study</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We show some cases in Figure <a href="#S4.F2" title="Figure 2 ‣ 4 Conclusion ‣ TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> - <a href="#S4.F8" title="Figure 8 ‣ 4 Conclusion ‣ TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a>. More examples will be released soon.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We present TableGPT, a large language model designed for table analysis, unifying tables, nature language, and commands. It enables a variety of functions like answering questions, manipulating data, visualizing information, generating analysis reports, and making predictions.
Technically, TableGPT addresses several major challenges in developing a natural language-driven framework for table data processing, including comprehensive table understanding, instruction chain generation, and domain-specific fine-tuning.
We believe TableGPT has the potential to reshape the landscape of tabular data processing, accelerating the efficiency of table modeling and exploratory data analysis (EDA), and empowering various domains like finance, transportation, scientific research, etc.</p>
</div>
<figure id="S4.F2" class="ltx_figure"><img src="/html/2307.08674/assets/figures/1.png" id="S4.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="653" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Cases of TableGPT.</figcaption>
</figure>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2307.08674/assets/figures/2.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="653" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Cases of TableGPT.</figcaption>
</figure>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2307.08674/assets/figures/3.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="653" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Cases of TableGPT.</figcaption>
</figure>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2307.08674/assets/figures/4.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="653" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Cases of TableGPT.</figcaption>
</figure>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2307.08674/assets/figures/5.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="653" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Cases of TableGPT.</figcaption>
</figure>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2307.08674/assets/figures/6.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="653" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Cases of TableGPT.</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure"><img src="/html/2307.08674/assets/figures/7.png" id="S4.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="653" height="368" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Cases of TableGPT.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech
representations, 2020.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
33:1877–1901, 2020.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Hao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan
Yanggong, and Junbo Zhao.

</span>
<span class="ltx_bibblock">Maybe only 0.5% data is needed: A preliminary exploration of low
training data instruction tuning, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Zhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen,
Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, et al.

</span>
<span class="ltx_bibblock">Phoenix: Democratizing chatgpt across languages.

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.10453</span>, 2023.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding, 2019.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,
Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.

</span>
<span class="ltx_bibblock">Multimodal-gpt: A vision and language model for dialogue with humans.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.04790</span>, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi
Jiang, and David Sontag.

</span>
<span class="ltx_bibblock">Tabllm: Few-shot classification of tabular data with large language
models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</span>, pages 5549–5581. PMLR, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao.

</span>
<span class="ltx_bibblock">Chatdb: Augmenting llms with databases as their symbolic memory.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.03901</span>, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui
Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, et al.

</span>
<span class="ltx_bibblock">Audiogpt: Understanding and generating speech, music, sound, and
talking head.

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.12995</span>, 2023.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
IDEA-CCNL.

</span>
<span class="ltx_bibblock">Fengshenbang-lm.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/IDEA-CCNL/Fengshenbang-LM" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/IDEA-CCNL/Fengshenbang-LM</a>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Shima Imani, Liang Du, and Harsh Shrivastava.

</span>
<span class="ltx_bibblock">Mathprompter: Mathematical reasoning using large language models,
2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Baichuan Intelligence.

</span>
<span class="ltx_bibblock">Baichuan-7b.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/baichuan-inc/baichuan-7B" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/baichuan-inc/baichuan-7B</a>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura
Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.

</span>
<span class="ltx_bibblock">Segment anything.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2304.02643</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke
Iwasawa.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
35:22199–22213, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
LangChain.

</span>
<span class="ltx_bibblock">Langchain.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://blog.langchain.dev/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://blog.langchain.dev/</a>, 2022.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye
Teh.

</span>
<span class="ltx_bibblock">Set transformer: A framework for attention-based
permutation-invariant neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
3744–3753. PMLR, 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, and Zhaoxiang Zhang.

</span>
<span class="ltx_bibblock">Sheetcopilot: Bringing software productivity to the next level
through large language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.19308</span>, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Jinyang Li, Binyuan Hui, Reynold Cheng, Bowen Qin, Chenhao Ma, Nan Huo, Fei
Huang, Wenyu Du, Luo Si, and Yongbin Li.

</span>
<span class="ltx_bibblock">Graphix-t5: Mixing pre-trained transformers with graph-aware layers
for text-to-sql parsing.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2301.07507</span>, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Liyao Li, Haobo Wang, Liangyu Zha, Qingyi Huang, Sai Wu, Gang Chen, and Junbo
Zhao.

</span>
<span class="ltx_bibblock">Learning a data-driven policy network for pre-training automated
feature engineering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">The Eleventh International Conference on Learning
Representations</span>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Guoshan Lu, Haobo Wang, Saisai Yang, Jing Yuan, Guozheng Yang, Cheng Zang, Gang
Chen, and Junbo Zhao.

</span>
<span class="ltx_bibblock">Catch: Collaborative feature set search for automated feature
engineering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of the ACM Web Conference 2023</span>, pages
1886–1896, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openai.com/blog/chatgpt" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openai.com/blog/chatgpt</a>, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
8748–8763. PMLR, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
<span class="ltx_bibblock">2018.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, et al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">OpenAI blog</span>, 1(8):9, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Pengzhen Ren, Yun Xiao, Xiaojun Chang, Po-Yao Huang, Zhihui Li, Brij B. Gupta,
Xiaojiang Chen, and Xin Wang.

</span>
<span class="ltx_bibblock">A survey of deep active learning, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Peking University.

</span>
<span class="ltx_bibblock">Chatexcel.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://chatexcel.com/" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://chatexcel.com/</a>, 2023.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Jianguo Wang, Xiaomeng Yi, Rentong Guo, Hai Jin, Peng Xu, Shengjun Li, Xiangyu
Wang, Xiangzhou Guo, Chengming Li, Xiaohai Xu, et al.

</span>
<span class="ltx_bibblock">Milvus: A purpose-built vector data management system.

</span>
<span class="ltx_bibblock">In <span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2021 International Conference on
Management of Data</span>, pages 2614–2627, 2021.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel
Khashabi, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">Self-instruct: Aligning language model with self generated
instructions.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2212.10560</span>, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yuxuan Wang, RJ Skerry-Ryan, Daisy Stanton, Yonghui Wu, Ron J. Weiss, Navdeep
Jaitly, Zongheng Yang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc Le, Yannis
Agiomyrgiannakis, Rob Clark, and Rif A. Saurous.

</span>
<span class="ltx_bibblock">Tacotron: Towards end-to-end speech synthesis, 2017.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
35:24824–24837, 2022.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Chao Ye, Guoshan Lu, Haobo Wang, Liyao Li, Sai Wu, Gang Chen, and Junbo Zhao.

</span>
<span class="ltx_bibblock">Ct-bert: Learning better tabular representations through cross-table
pre-training.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2307.04308</span>, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Wentao Ye, Mingfeng Ou, Tianyi Li, Xuetao Ma, Yifan Yanggong, Sai Wu, Jie Fu,
Gang Chen, Junbo Zhao, et al.

</span>
<span class="ltx_bibblock">Assessing hidden risks of llms: An empirical study on robustness,
consistency, and credibility.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2305.10235</span>, 2023.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi
Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al.

</span>
<span class="ltx_bibblock">Glm-130b: An open bilingual pre-trained model.

</span>
<span class="ltx_bibblock">In <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">The Eleventh International Conference on Learning
Representations</span>, 2022.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2205.01068</span>, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Wenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang.

</span>
<span class="ltx_bibblock">Data-copilot: Bridging billions of data and humans with autonomous
workflow.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2306.07209</span>, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Victor Zhong, Caiming Xiong, and Richard Socher.

</span>
<span class="ltx_bibblock">Seq2sql: Generating structured queries from natural language using
reinforcement learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1709.00103</span>, 2017.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.08672" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.08674" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.08674">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.08674" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.08675" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 17:32:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
