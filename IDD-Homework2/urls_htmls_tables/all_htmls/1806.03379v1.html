<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[1806.03379] CS-VQA: Visual Question Answering with Compressively Sensed Images</title><meta property="og:description" content="Visual Question Answering (VQA) is a complex semantic task requiring both natural language processing and visual recognition. In this paper, we explore whether VQA is solvable when images are captured in a sub-Nyquist ‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS-VQA: Visual Question Answering with Compressively Sensed Images">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CS-VQA: Visual Question Answering with Compressively Sensed Images">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/1806.03379">

<!--Generated on Sat Mar 16 04:16:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">CS-VQA: Visual Question Answering with Compressively Sensed Images</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Visual Question Answering (VQA) is a complex semantic task requiring both natural language processing and visual recognition. In this paper, we explore whether VQA is solvable when images are captured in a sub-Nyquist compressive paradigm. We develop a series of deep-network architectures that exploit available compressive data to increasing degrees of accuracy, and show that VQA is indeed solvable in the compressed domain. Our results show that there is nominal degradation in VQA performance when using compressive measurements, but that accuracy can be recovered when VQA pipelines are used in conjunction with state-of-the-art deep neural networks for CS reconstruction. The results presented yield important implications for resource-constrained VQA applications.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">‚Äî‚Äâ</span></span>
Computer vision, compressed sensing, multi-layer neural network, image reconstruction</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">The Visual Question Answering (VQA) problem has recently gained significant research attention in the computer vision and machine learning communities¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The VQA task consists of answering an open-ended question for a given image, which requires the ability to parse a question expressed in natural language, computationally analyze the image based on the question‚Äôs requirement, and present an answer in natural language. For example, given an image depicting a family reunion, representative questions might include ‚ÄúHow many people are there?‚Äù, ‚ÄúWhat is the color of the table?‚Äù etc. Due to the contextual analysis required to answer these questions, VQA has been considered an AI complete task¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Contemporary VQA research has utilized deep neural-networks trained jointly on images and natural language ‚Äòvectors‚Äô computed from the questions. However, in this paper, we explore whether the underlying representation of visual data in 2D images is even critical for VQA performance. In particular, we explore whether sub-Nyquist rate sensed measurements of natural images can be an effective substitute for fully-sampled images in a VQA architecture.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The answer to the above question can have significant implications for adapting VQA techniques to resource-constrained platforms, such as a Google Glass, a Hololens, mobile computing platforms, field robotics etc. For instance, the Google Glass continuously running an off-the-shelf face-detection algorithm drains its battery in only <math id="S1.p2.1.m1.1" class="ltx_Math" alttext="45" display="inline"><semantics id="S1.p2.1.m1.1a"><mn id="S1.p2.1.m1.1.1" xref="S1.p2.1.m1.1.1.cmml">45</mn><annotation-xml encoding="MathML-Content" id="S1.p2.1.m1.1b"><cn type="integer" id="S1.p2.1.m1.1.1.cmml" xref="S1.p2.1.m1.1.1">45</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.p2.1.m1.1c">45</annotation></semantics></math> minutes¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Sub-Nyquist imagers hold the potential to save imaging energy, reducing data-bandwidth, storage, etc. all of which can result in sustaining performance under resource constraints.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The most popular sub-Nyquist, or, compressive sensing (CS) framework for imaging has utilized a sampling framework where incoming light-rays are multiplexed onto a smaller set of pixels (even a single pixel¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>). Via multiple coded projections of a scene, the original image can be reconstructed using post-processing ¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. This allows CS techniques to satisfy resource constraints in real imaging systems including decreasing energy consumption, computation, bandwidth, and latency. Working with CS data requires rethinking the computer vision pipeline, as even basic operations like convolutions require non-trivial computation such as smashed filtering <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. We term this new task CS-VQA and present new approaches to solve this task.
<span id="S1.p3.1.1" class="ltx_text ltx_font_bold">Contributions:</span> This paper is a first investigation of the CS-VQA task. We design a series of deep neural-network architectures to solve CS-VQA. While some of the proposed modules are inspired from past work in CS reconstruction, we do not require explicit reconstruction. We also investigate whether CS imaging is more suited for answering certain types of questions, more so than others. Finally, we explore the tradeoffs between performance, computational time, size of models, etc., and show that it is indeed possible to achieve near state-of-the-art VQA performance, even while working with compressively sensed imagery.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.8" class="ltx_p"><span id="S2.p1.8.1" class="ltx_text ltx_font_bold">Compressive Sensing:</span>
Compressive Sensing (CS) is a signal acquisition paradigm which samples
a signal at sub-Nyquist rates using random linear measurements, and then recovers the original signal in post-processing¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. The measurements are given by <math id="S2.p1.1.m1.1" class="ltx_Math" alttext="\mathbf{y}=\Phi\mathbf{x}+\mathbf{e}" display="inline"><semantics id="S2.p1.1.m1.1a"><mrow id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml"><mi id="S2.p1.1.m1.1.1.2" xref="S2.p1.1.m1.1.1.2.cmml">ùê≤</mi><mo id="S2.p1.1.m1.1.1.1" xref="S2.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S2.p1.1.m1.1.1.3" xref="S2.p1.1.m1.1.1.3.cmml"><mrow id="S2.p1.1.m1.1.1.3.2" xref="S2.p1.1.m1.1.1.3.2.cmml"><mi mathvariant="normal" id="S2.p1.1.m1.1.1.3.2.2" xref="S2.p1.1.m1.1.1.3.2.2.cmml">Œ¶</mi><mo lspace="0em" rspace="0em" id="S2.p1.1.m1.1.1.3.2.1" xref="S2.p1.1.m1.1.1.3.2.1.cmml">‚Äã</mo><mi id="S2.p1.1.m1.1.1.3.2.3" xref="S2.p1.1.m1.1.1.3.2.3.cmml">ùê±</mi></mrow><mo id="S2.p1.1.m1.1.1.3.1" xref="S2.p1.1.m1.1.1.3.1.cmml">+</mo><mi id="S2.p1.1.m1.1.1.3.3" xref="S2.p1.1.m1.1.1.3.3.cmml">ùêû</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.1b"><apply id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1"><eq id="S2.p1.1.m1.1.1.1.cmml" xref="S2.p1.1.m1.1.1.1"></eq><ci id="S2.p1.1.m1.1.1.2.cmml" xref="S2.p1.1.m1.1.1.2">ùê≤</ci><apply id="S2.p1.1.m1.1.1.3.cmml" xref="S2.p1.1.m1.1.1.3"><plus id="S2.p1.1.m1.1.1.3.1.cmml" xref="S2.p1.1.m1.1.1.3.1"></plus><apply id="S2.p1.1.m1.1.1.3.2.cmml" xref="S2.p1.1.m1.1.1.3.2"><times id="S2.p1.1.m1.1.1.3.2.1.cmml" xref="S2.p1.1.m1.1.1.3.2.1"></times><ci id="S2.p1.1.m1.1.1.3.2.2.cmml" xref="S2.p1.1.m1.1.1.3.2.2">Œ¶</ci><ci id="S2.p1.1.m1.1.1.3.2.3.cmml" xref="S2.p1.1.m1.1.1.3.2.3">ùê±</ci></apply><ci id="S2.p1.1.m1.1.1.3.3.cmml" xref="S2.p1.1.m1.1.1.3.3">ùêû</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.1c">\mathbf{y}=\Phi\mathbf{x}+\mathbf{e}</annotation></semantics></math> , with image <math id="S2.p1.2.m2.1" class="ltx_Math" alttext="\mathbf{x}\in\mathbb{R}^{n}" display="inline"><semantics id="S2.p1.2.m2.1a"><mrow id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml"><mi id="S2.p1.2.m2.1.1.2" xref="S2.p1.2.m2.1.1.2.cmml">ùê±</mi><mo id="S2.p1.2.m2.1.1.1" xref="S2.p1.2.m2.1.1.1.cmml">‚àà</mo><msup id="S2.p1.2.m2.1.1.3" xref="S2.p1.2.m2.1.1.3.cmml"><mi id="S2.p1.2.m2.1.1.3.2" xref="S2.p1.2.m2.1.1.3.2.cmml">‚Ñù</mi><mi id="S2.p1.2.m2.1.1.3.3" xref="S2.p1.2.m2.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><apply id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1"><in id="S2.p1.2.m2.1.1.1.cmml" xref="S2.p1.2.m2.1.1.1"></in><ci id="S2.p1.2.m2.1.1.2.cmml" xref="S2.p1.2.m2.1.1.2">ùê±</ci><apply id="S2.p1.2.m2.1.1.3.cmml" xref="S2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S2.p1.2.m2.1.1.3.1.cmml" xref="S2.p1.2.m2.1.1.3">superscript</csymbol><ci id="S2.p1.2.m2.1.1.3.2.cmml" xref="S2.p1.2.m2.1.1.3.2">‚Ñù</ci><ci id="S2.p1.2.m2.1.1.3.3.cmml" xref="S2.p1.2.m2.1.1.3.3">ùëõ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\mathbf{x}\in\mathbb{R}^{n}</annotation></semantics></math>, measurement vector <math id="S2.p1.3.m3.1" class="ltx_Math" alttext="\mathbf{y}\in\mathbb{R}^{m}" display="inline"><semantics id="S2.p1.3.m3.1a"><mrow id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml"><mi id="S2.p1.3.m3.1.1.2" xref="S2.p1.3.m3.1.1.2.cmml">ùê≤</mi><mo id="S2.p1.3.m3.1.1.1" xref="S2.p1.3.m3.1.1.1.cmml">‚àà</mo><msup id="S2.p1.3.m3.1.1.3" xref="S2.p1.3.m3.1.1.3.cmml"><mi id="S2.p1.3.m3.1.1.3.2" xref="S2.p1.3.m3.1.1.3.2.cmml">‚Ñù</mi><mi id="S2.p1.3.m3.1.1.3.3" xref="S2.p1.3.m3.1.1.3.3.cmml">m</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><apply id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1"><in id="S2.p1.3.m3.1.1.1.cmml" xref="S2.p1.3.m3.1.1.1"></in><ci id="S2.p1.3.m3.1.1.2.cmml" xref="S2.p1.3.m3.1.1.2">ùê≤</ci><apply id="S2.p1.3.m3.1.1.3.cmml" xref="S2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S2.p1.3.m3.1.1.3.1.cmml" xref="S2.p1.3.m3.1.1.3">superscript</csymbol><ci id="S2.p1.3.m3.1.1.3.2.cmml" xref="S2.p1.3.m3.1.1.3.2">‚Ñù</ci><ci id="S2.p1.3.m3.1.1.3.3.cmml" xref="S2.p1.3.m3.1.1.3.3">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathbf{y}\in\mathbb{R}^{m}</annotation></semantics></math>, measurement/projection matrix <math id="S2.p1.4.m4.1" class="ltx_Math" alttext="\Phi\in\mathbb{R}^{m\times n}" display="inline"><semantics id="S2.p1.4.m4.1a"><mrow id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml"><mi mathvariant="normal" id="S2.p1.4.m4.1.1.2" xref="S2.p1.4.m4.1.1.2.cmml">Œ¶</mi><mo id="S2.p1.4.m4.1.1.1" xref="S2.p1.4.m4.1.1.1.cmml">‚àà</mo><msup id="S2.p1.4.m4.1.1.3" xref="S2.p1.4.m4.1.1.3.cmml"><mi id="S2.p1.4.m4.1.1.3.2" xref="S2.p1.4.m4.1.1.3.2.cmml">‚Ñù</mi><mrow id="S2.p1.4.m4.1.1.3.3" xref="S2.p1.4.m4.1.1.3.3.cmml"><mi id="S2.p1.4.m4.1.1.3.3.2" xref="S2.p1.4.m4.1.1.3.3.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S2.p1.4.m4.1.1.3.3.1" xref="S2.p1.4.m4.1.1.3.3.1.cmml">√ó</mo><mi id="S2.p1.4.m4.1.1.3.3.3" xref="S2.p1.4.m4.1.1.3.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><apply id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1"><in id="S2.p1.4.m4.1.1.1.cmml" xref="S2.p1.4.m4.1.1.1"></in><ci id="S2.p1.4.m4.1.1.2.cmml" xref="S2.p1.4.m4.1.1.2">Œ¶</ci><apply id="S2.p1.4.m4.1.1.3.cmml" xref="S2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.p1.4.m4.1.1.3.1.cmml" xref="S2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S2.p1.4.m4.1.1.3.2.cmml" xref="S2.p1.4.m4.1.1.3.2">‚Ñù</ci><apply id="S2.p1.4.m4.1.1.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3"><times id="S2.p1.4.m4.1.1.3.3.1.cmml" xref="S2.p1.4.m4.1.1.3.3.1"></times><ci id="S2.p1.4.m4.1.1.3.3.2.cmml" xref="S2.p1.4.m4.1.1.3.3.2">ùëö</ci><ci id="S2.p1.4.m4.1.1.3.3.3.cmml" xref="S2.p1.4.m4.1.1.3.3.3">ùëõ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">\Phi\in\mathbb{R}^{m\times n}</annotation></semantics></math> and additive noise <math id="S2.p1.5.m5.1" class="ltx_Math" alttext="\mathbf{e}\in\mathbb{R}^{m}" display="inline"><semantics id="S2.p1.5.m5.1a"><mrow id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml"><mi id="S2.p1.5.m5.1.1.2" xref="S2.p1.5.m5.1.1.2.cmml">ùêû</mi><mo id="S2.p1.5.m5.1.1.1" xref="S2.p1.5.m5.1.1.1.cmml">‚àà</mo><msup id="S2.p1.5.m5.1.1.3" xref="S2.p1.5.m5.1.1.3.cmml"><mi id="S2.p1.5.m5.1.1.3.2" xref="S2.p1.5.m5.1.1.3.2.cmml">‚Ñù</mi><mi id="S2.p1.5.m5.1.1.3.3" xref="S2.p1.5.m5.1.1.3.3.cmml">m</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><apply id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1"><in id="S2.p1.5.m5.1.1.1.cmml" xref="S2.p1.5.m5.1.1.1"></in><ci id="S2.p1.5.m5.1.1.2.cmml" xref="S2.p1.5.m5.1.1.2">ùêû</ci><apply id="S2.p1.5.m5.1.1.3.cmml" xref="S2.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.p1.5.m5.1.1.3.1.cmml" xref="S2.p1.5.m5.1.1.3">superscript</csymbol><ci id="S2.p1.5.m5.1.1.3.2.cmml" xref="S2.p1.5.m5.1.1.3.2">‚Ñù</ci><ci id="S2.p1.5.m5.1.1.3.3.cmml" xref="S2.p1.5.m5.1.1.3.3">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\mathbf{e}\in\mathbb{R}^{m}</annotation></semantics></math>. To solve this ill-posed problem when <math id="S2.p1.6.m6.1" class="ltx_Math" alttext="m&lt;&lt;n" display="inline"><semantics id="S2.p1.6.m6.1a"><mrow id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml"><mi id="S2.p1.6.m6.1.1.2" xref="S2.p1.6.m6.1.1.2.cmml">m</mi><mo id="S2.p1.6.m6.1.1.1" xref="S2.p1.6.m6.1.1.1.cmml">&lt;&lt;</mo><mi id="S2.p1.6.m6.1.1.3" xref="S2.p1.6.m6.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><apply id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1"><csymbol cd="latexml" id="S2.p1.6.m6.1.1.1.cmml" xref="S2.p1.6.m6.1.1.1">much-less-than</csymbol><ci id="S2.p1.6.m6.1.1.2.cmml" xref="S2.p1.6.m6.1.1.2">ùëö</ci><ci id="S2.p1.6.m6.1.1.3.cmml" xref="S2.p1.6.m6.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">m&lt;&lt;n</annotation></semantics></math>, one can solve the following optimization problem in equation¬†<a href="#S2.E1" title="In 2 Background and Related Work ‚Ä£ CS-VQA: Visual Question Answering with Compressively Sensed Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, provided the signal is <math id="S2.p1.7.m7.1" class="ltx_Math" alttext="s" display="inline"><semantics id="S2.p1.7.m7.1a"><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.1b"><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">ùë†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.1c">s</annotation></semantics></math>-sparse in some sparsifying domain, <math id="S2.p1.8.m8.1" class="ltx_Math" alttext="\Psi" display="inline"><semantics id="S2.p1.8.m8.1a"><mi mathvariant="normal" id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml">Œ®</mi><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.1b"><ci id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1">Œ®</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.1c">\Psi</annotation></semantics></math>,</p>
<table id="S2.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.E1.m1.1" class="ltx_Math" alttext="\min_{\mathbf{x}}||\Psi\mathbf{x}||_{1}\text{ s.t. }||\mathbf{y}-\Phi\mathbf{x}||_{2}\leq\epsilon." display="block"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.cmml"><munder id="S2.E1.m1.1.1.1.1.2.4" xref="S2.E1.m1.1.1.1.1.2.4.cmml"><mi id="S2.E1.m1.1.1.1.1.2.4.2" xref="S2.E1.m1.1.1.1.1.2.4.2.cmml">min</mi><mi id="S2.E1.m1.1.1.1.1.2.4.3" xref="S2.E1.m1.1.1.1.1.2.4.3.cmml">ùê±</mi></munder><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.2.3" xref="S2.E1.m1.1.1.1.1.2.3.cmml">‚Äã</mo><msub id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo><mrow id="S2.E1.m1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi mathvariant="normal" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">Œ®</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml">‚Äã</mo><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">ùê±</mi></mrow><mo stretchy="false" id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.2.3a" xref="S2.E1.m1.1.1.1.1.2.3.cmml">‚Äã</mo><mtext id="S2.E1.m1.1.1.1.1.2.5" xref="S2.E1.m1.1.1.1.1.2.5a.cmml">¬†s.t.¬†</mtext><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.2.3b" xref="S2.E1.m1.1.1.1.1.2.3.cmml">‚Äã</mo><msub id="S2.E1.m1.1.1.1.1.2.2" xref="S2.E1.m1.1.1.1.1.2.2.cmml"><mrow id="S2.E1.m1.1.1.1.1.2.2.1.1" xref="S2.E1.m1.1.1.1.1.2.2.1.2.cmml"><mo stretchy="false" id="S2.E1.m1.1.1.1.1.2.2.1.1.2" xref="S2.E1.m1.1.1.1.1.2.2.1.2.1.cmml">‚Äñ</mo><mrow id="S2.E1.m1.1.1.1.1.2.2.1.1.1" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.cmml">ùê≤</mi><mo id="S2.E1.m1.1.1.1.1.2.2.1.1.1.1" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.1.cmml">‚àí</mo><mrow id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.cmml"><mi mathvariant="normal" id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.2.cmml">Œ¶</mi><mo lspace="0em" rspace="0em" id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.1" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.1.cmml">‚Äã</mo><mi id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.cmml">ùê±</mi></mrow></mrow><mo stretchy="false" id="S2.E1.m1.1.1.1.1.2.2.1.1.3" xref="S2.E1.m1.1.1.1.1.2.2.1.2.1.cmml">‚Äñ</mo></mrow><mn id="S2.E1.m1.1.1.1.1.2.2.3" xref="S2.E1.m1.1.1.1.1.2.2.3.cmml">2</mn></msub></mrow><mo id="S2.E1.m1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.3.cmml">‚â§</mo><mi id="S2.E1.m1.1.1.1.1.4" xref="S2.E1.m1.1.1.1.1.4.cmml">œµ</mi></mrow><mo lspace="0em" id="S2.E1.m1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1"><leq id="S2.E1.m1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.3"></leq><apply id="S2.E1.m1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2"><times id="S2.E1.m1.1.1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.3"></times><apply id="S2.E1.m1.1.1.1.1.2.4.cmml" xref="S2.E1.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.4.1.cmml" xref="S2.E1.m1.1.1.1.1.2.4">subscript</csymbol><min id="S2.E1.m1.1.1.1.1.2.4.2.cmml" xref="S2.E1.m1.1.1.1.1.2.4.2"></min><ci id="S2.E1.m1.1.1.1.1.2.4.3.cmml" xref="S2.E1.m1.1.1.1.1.2.4.3">ùê±</ci></apply><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1">subscript</csymbol><apply id="S2.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S2.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1"><times id="S2.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.2">Œ®</ci><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.1.3">ùê±</ci></apply></apply><cn type="integer" id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">1</cn></apply><ci id="S2.E1.m1.1.1.1.1.2.5a.cmml" xref="S2.E1.m1.1.1.1.1.2.5"><mtext id="S2.E1.m1.1.1.1.1.2.5.cmml" xref="S2.E1.m1.1.1.1.1.2.5">¬†s.t.¬†</mtext></ci><apply id="S2.E1.m1.1.1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.2.2.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2">subscript</csymbol><apply id="S2.E1.m1.1.1.1.1.2.2.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1"><csymbol cd="latexml" id="S2.E1.m1.1.1.1.1.2.2.1.2.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.2">norm</csymbol><apply id="S2.E1.m1.1.1.1.1.2.2.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1"><minus id="S2.E1.m1.1.1.1.1.2.2.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.1"></minus><ci id="S2.E1.m1.1.1.1.1.2.2.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.2">ùê≤</ci><apply id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3"><times id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.1"></times><ci id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.2">Œ¶</ci><ci id="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.1.1.1.3.3">ùê±</ci></apply></apply></apply><cn type="integer" id="S2.E1.m1.1.1.1.1.2.2.3.cmml" xref="S2.E1.m1.1.1.1.1.2.2.3">2</cn></apply></apply><ci id="S2.E1.m1.1.1.1.1.4.cmml" xref="S2.E1.m1.1.1.1.1.4">italic-œµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">\min_{\mathbf{x}}||\Psi\mathbf{x}||_{1}\text{ s.t. }||\mathbf{y}-\Phi\mathbf{x}||_{2}\leq\epsilon.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">To solve (<a href="#S2.E1" title="In 2 Background and Related Work ‚Ä£ CS-VQA: Visual Question Answering with Compressively Sensed Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>), many iterative algorithms have been proposed in the literature¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> but they are not conducive for fast reconstruction or low measurement rates (MRs). For faster reconstruction and better recovery at low MRs (<math id="S2.p2.1.m1.1" class="ltx_Math" alttext="&lt;0.10" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml"></mi><mo id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">0.10</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><lt id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"></lt><csymbol cd="latexml" id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2">absent</csymbol><cn type="float" id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">0.10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">&lt;0.10</annotation></semantics></math>), deep learning networks have been proposed that achieve state-of-the-art performance¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para ltx_indent">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Compressive Inference:</span> The goal of compressive inference is to infer semantic information directly from compressed measurements without reconstruction. Direct inference has been shown to be feasible in applications like action recognition¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, image classification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, and object tracking¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. This paper explores the tradeoffs in compressive VQA, which has not been attempted in the past.</p>
</div>
<div id="S2.p4" class="ltx_para ltx_indent">
<p id="S2.p4.1" class="ltx_p"><span id="S2.p4.1.1" class="ltx_text ltx_font_bold">Visual Question Answering:</span> Current approaches to solve VQA rely heavily on deep-learning methods, for fusing image and text features. Image features are typically extracted using pre-trained or fine-tuned convolutional neural networks (CNNs) such as GoogleNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> or ResNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Textual questions are converted into vector sequences using methods such as Word2vec¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, and further processed using recurrent neural networks (RNNs) or Long Short-Term Memory cells (LSTMs)¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite> to encode temporal structure within the questions. A late-stage fusion is done for the image and question features, and a final classifier provides an answer from a set of specified answers. Recent approaches have utilized attention mechanisms to spatially localize image features for improved performance¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. State-of-the-art VQA models use an ensemble of methods¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. In this paper, we do not seek to improve VQA performance, but investigate the effect of sub-Nyquist sensing of images on VQA performance.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Problem Formulation and Approach</h2>

<figure id="S3.F1" class="ltx_figure"><img src="/html/1806.03379/assets/x1.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="142" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text ltx_font_bold">Fig.¬†1</span>: </span>An overview of the proposed CS-VQA architecture. An image is compressively sensed via random projections. The measurement vector is used to reconstruct either a surrogate image using linear inversion or full-reconstruction via ReconNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. The reconstructed image is used to extract visual features using GoogleNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. The given question is encoded using Word2vec¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> and fed into a LSTM to form a question feature vector. The two features are concatenated and fed into a fully connected network to generate the final answer. </figcaption>
</figure>
<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our goal is to answer questions posed with respect to a scene, given its CS measurements. The existing VQA dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> consists of image-question pairs, thus we must convert the images into measurement vectors. We simulate compressive sensing with either a random Gaussian or column-permuted Hadamard measurement matrix, operating at a measurement rate (MR) = <math id="S3.p1.1.m1.1" class="ltx_Math" alttext="m/n" display="inline"><semantics id="S3.p1.1.m1.1a"><mrow id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">m</mi><mo id="S3.p1.1.m1.1.1.1" xref="S3.p1.1.m1.1.1.1.cmml">/</mo><mi id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><divide id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1.1"></divide><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">ùëö</ci><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">m/n</annotation></semantics></math>. The choice of the measurement matrix is motivated by the following reasons, a) it is task-agnostic, yet generalizable to many tasks, b) it is a theoretically supported method for compressive acquisition of natural image data. However, learning a measurement matrix may result in improved performance, but we leave this avenue for future work. For our experiments, we simulate compressive sensing two different ways, each yielding a CS-VQA dataset, and conduct experiments on both the CS-VQA datasets.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.9" class="ltx_p">In the first method, the measurements <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.p2.1.m1.1a"><mi id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">ùê≤</mi><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><ci id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1">ùê≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">\mathbf{y}</annotation></semantics></math> are obtained by pre-multiplying the image vector, <math id="S3.p2.2.m2.1" class="ltx_Math" alttext="\mathbf{x}\in\mathbb{R}^{n}" display="inline"><semantics id="S3.p2.2.m2.1a"><mrow id="S3.p2.2.m2.1.1" xref="S3.p2.2.m2.1.1.cmml"><mi id="S3.p2.2.m2.1.1.2" xref="S3.p2.2.m2.1.1.2.cmml">ùê±</mi><mo id="S3.p2.2.m2.1.1.1" xref="S3.p2.2.m2.1.1.1.cmml">‚àà</mo><msup id="S3.p2.2.m2.1.1.3" xref="S3.p2.2.m2.1.1.3.cmml"><mi id="S3.p2.2.m2.1.1.3.2" xref="S3.p2.2.m2.1.1.3.2.cmml">‚Ñù</mi><mi id="S3.p2.2.m2.1.1.3.3" xref="S3.p2.2.m2.1.1.3.3.cmml">n</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.2.m2.1b"><apply id="S3.p2.2.m2.1.1.cmml" xref="S3.p2.2.m2.1.1"><in id="S3.p2.2.m2.1.1.1.cmml" xref="S3.p2.2.m2.1.1.1"></in><ci id="S3.p2.2.m2.1.1.2.cmml" xref="S3.p2.2.m2.1.1.2">ùê±</ci><apply id="S3.p2.2.m2.1.1.3.cmml" xref="S3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p2.2.m2.1.1.3.1.cmml" xref="S3.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.p2.2.m2.1.1.3.2.cmml" xref="S3.p2.2.m2.1.1.3.2">‚Ñù</ci><ci id="S3.p2.2.m2.1.1.3.3.cmml" xref="S3.p2.2.m2.1.1.3.3">ùëõ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.2.m2.1c">\mathbf{x}\in\mathbb{R}^{n}</annotation></semantics></math> by a column-permuted Hadamard matrix (<math id="S3.p2.3.m3.1" class="ltx_Math" alttext="\Phi" display="inline"><semantics id="S3.p2.3.m3.1a"><mi mathvariant="normal" id="S3.p2.3.m3.1.1" xref="S3.p2.3.m3.1.1.cmml">Œ¶</mi><annotation-xml encoding="MathML-Content" id="S3.p2.3.m3.1b"><ci id="S3.p2.3.m3.1.1.cmml" xref="S3.p2.3.m3.1.1">Œ¶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.3.m3.1c">\Phi</annotation></semantics></math>) of size <math id="S3.p2.4.m4.1" class="ltx_Math" alttext="m\times n" display="inline"><semantics id="S3.p2.4.m4.1a"><mrow id="S3.p2.4.m4.1.1" xref="S3.p2.4.m4.1.1.cmml"><mi id="S3.p2.4.m4.1.1.2" xref="S3.p2.4.m4.1.1.2.cmml">m</mi><mo lspace="0.222em" rspace="0.222em" id="S3.p2.4.m4.1.1.1" xref="S3.p2.4.m4.1.1.1.cmml">√ó</mo><mi id="S3.p2.4.m4.1.1.3" xref="S3.p2.4.m4.1.1.3.cmml">n</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.4.m4.1b"><apply id="S3.p2.4.m4.1.1.cmml" xref="S3.p2.4.m4.1.1"><times id="S3.p2.4.m4.1.1.1.cmml" xref="S3.p2.4.m4.1.1.1"></times><ci id="S3.p2.4.m4.1.1.2.cmml" xref="S3.p2.4.m4.1.1.2">ùëö</ci><ci id="S3.p2.4.m4.1.1.3.cmml" xref="S3.p2.4.m4.1.1.3">ùëõ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.4.m4.1c">m\times n</annotation></semantics></math>, mathematically written as <math id="S3.p2.5.m5.1" class="ltx_Math" alttext="\mathbf{y}=\Phi\mathbf{x}" display="inline"><semantics id="S3.p2.5.m5.1a"><mrow id="S3.p2.5.m5.1.1" xref="S3.p2.5.m5.1.1.cmml"><mi id="S3.p2.5.m5.1.1.2" xref="S3.p2.5.m5.1.1.2.cmml">ùê≤</mi><mo id="S3.p2.5.m5.1.1.1" xref="S3.p2.5.m5.1.1.1.cmml">=</mo><mrow id="S3.p2.5.m5.1.1.3" xref="S3.p2.5.m5.1.1.3.cmml"><mi mathvariant="normal" id="S3.p2.5.m5.1.1.3.2" xref="S3.p2.5.m5.1.1.3.2.cmml">Œ¶</mi><mo lspace="0em" rspace="0em" id="S3.p2.5.m5.1.1.3.1" xref="S3.p2.5.m5.1.1.3.1.cmml">‚Äã</mo><mi id="S3.p2.5.m5.1.1.3.3" xref="S3.p2.5.m5.1.1.3.3.cmml">ùê±</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.5.m5.1b"><apply id="S3.p2.5.m5.1.1.cmml" xref="S3.p2.5.m5.1.1"><eq id="S3.p2.5.m5.1.1.1.cmml" xref="S3.p2.5.m5.1.1.1"></eq><ci id="S3.p2.5.m5.1.1.2.cmml" xref="S3.p2.5.m5.1.1.2">ùê≤</ci><apply id="S3.p2.5.m5.1.1.3.cmml" xref="S3.p2.5.m5.1.1.3"><times id="S3.p2.5.m5.1.1.3.1.cmml" xref="S3.p2.5.m5.1.1.3.1"></times><ci id="S3.p2.5.m5.1.1.3.2.cmml" xref="S3.p2.5.m5.1.1.3.2">Œ¶</ci><ci id="S3.p2.5.m5.1.1.3.3.cmml" xref="S3.p2.5.m5.1.1.3.3">ùê±</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.5.m5.1c">\mathbf{y}=\Phi\mathbf{x}</annotation></semantics></math>. We call this ‚ÄòFF-CS-VQA‚Äô (FF <math id="S3.p2.6.m6.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S3.p2.6.m6.1a"><mo id="S3.p2.6.m6.1.1" xref="S3.p2.6.m6.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S3.p2.6.m6.1b"><eq id="S3.p2.6.m6.1.1.cmml" xref="S3.p2.6.m6.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.6.m6.1c">=</annotation></semantics></math> full-frame). In the second, the image is divided into non-overlapping blocks of a fixed size, <math id="S3.p2.7.m7.1" class="ltx_Math" alttext="33\times 33" display="inline"><semantics id="S3.p2.7.m7.1a"><mrow id="S3.p2.7.m7.1.1" xref="S3.p2.7.m7.1.1.cmml"><mn id="S3.p2.7.m7.1.1.2" xref="S3.p2.7.m7.1.1.2.cmml">33</mn><mo lspace="0.222em" rspace="0.222em" id="S3.p2.7.m7.1.1.1" xref="S3.p2.7.m7.1.1.1.cmml">√ó</mo><mn id="S3.p2.7.m7.1.1.3" xref="S3.p2.7.m7.1.1.3.cmml">33</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.7.m7.1b"><apply id="S3.p2.7.m7.1.1.cmml" xref="S3.p2.7.m7.1.1"><times id="S3.p2.7.m7.1.1.1.cmml" xref="S3.p2.7.m7.1.1.1"></times><cn type="integer" id="S3.p2.7.m7.1.1.2.cmml" xref="S3.p2.7.m7.1.1.2">33</cn><cn type="integer" id="S3.p2.7.m7.1.1.3.cmml" xref="S3.p2.7.m7.1.1.3">33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.7.m7.1c">33\times 33</annotation></semantics></math>, and independent measurements are obtained for each block, using a common random Gaussian measurement matrix, <math id="S3.p2.8.m8.1" class="ltx_Math" alttext="\mathbf{y}_{B}=\Phi_{B}\mathbf{x}_{B}" display="inline"><semantics id="S3.p2.8.m8.1a"><mrow id="S3.p2.8.m8.1.1" xref="S3.p2.8.m8.1.1.cmml"><msub id="S3.p2.8.m8.1.1.2" xref="S3.p2.8.m8.1.1.2.cmml"><mi id="S3.p2.8.m8.1.1.2.2" xref="S3.p2.8.m8.1.1.2.2.cmml">ùê≤</mi><mi id="S3.p2.8.m8.1.1.2.3" xref="S3.p2.8.m8.1.1.2.3.cmml">B</mi></msub><mo id="S3.p2.8.m8.1.1.1" xref="S3.p2.8.m8.1.1.1.cmml">=</mo><mrow id="S3.p2.8.m8.1.1.3" xref="S3.p2.8.m8.1.1.3.cmml"><msub id="S3.p2.8.m8.1.1.3.2" xref="S3.p2.8.m8.1.1.3.2.cmml"><mi mathvariant="normal" id="S3.p2.8.m8.1.1.3.2.2" xref="S3.p2.8.m8.1.1.3.2.2.cmml">Œ¶</mi><mi id="S3.p2.8.m8.1.1.3.2.3" xref="S3.p2.8.m8.1.1.3.2.3.cmml">B</mi></msub><mo lspace="0em" rspace="0em" id="S3.p2.8.m8.1.1.3.1" xref="S3.p2.8.m8.1.1.3.1.cmml">‚Äã</mo><msub id="S3.p2.8.m8.1.1.3.3" xref="S3.p2.8.m8.1.1.3.3.cmml"><mi id="S3.p2.8.m8.1.1.3.3.2" xref="S3.p2.8.m8.1.1.3.3.2.cmml">ùê±</mi><mi id="S3.p2.8.m8.1.1.3.3.3" xref="S3.p2.8.m8.1.1.3.3.3.cmml">B</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.8.m8.1b"><apply id="S3.p2.8.m8.1.1.cmml" xref="S3.p2.8.m8.1.1"><eq id="S3.p2.8.m8.1.1.1.cmml" xref="S3.p2.8.m8.1.1.1"></eq><apply id="S3.p2.8.m8.1.1.2.cmml" xref="S3.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.2.1.cmml" xref="S3.p2.8.m8.1.1.2">subscript</csymbol><ci id="S3.p2.8.m8.1.1.2.2.cmml" xref="S3.p2.8.m8.1.1.2.2">ùê≤</ci><ci id="S3.p2.8.m8.1.1.2.3.cmml" xref="S3.p2.8.m8.1.1.2.3">ùêµ</ci></apply><apply id="S3.p2.8.m8.1.1.3.cmml" xref="S3.p2.8.m8.1.1.3"><times id="S3.p2.8.m8.1.1.3.1.cmml" xref="S3.p2.8.m8.1.1.3.1"></times><apply id="S3.p2.8.m8.1.1.3.2.cmml" xref="S3.p2.8.m8.1.1.3.2"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.3.2.1.cmml" xref="S3.p2.8.m8.1.1.3.2">subscript</csymbol><ci id="S3.p2.8.m8.1.1.3.2.2.cmml" xref="S3.p2.8.m8.1.1.3.2.2">Œ¶</ci><ci id="S3.p2.8.m8.1.1.3.2.3.cmml" xref="S3.p2.8.m8.1.1.3.2.3">ùêµ</ci></apply><apply id="S3.p2.8.m8.1.1.3.3.cmml" xref="S3.p2.8.m8.1.1.3.3"><csymbol cd="ambiguous" id="S3.p2.8.m8.1.1.3.3.1.cmml" xref="S3.p2.8.m8.1.1.3.3">subscript</csymbol><ci id="S3.p2.8.m8.1.1.3.3.2.cmml" xref="S3.p2.8.m8.1.1.3.3.2">ùê±</ci><ci id="S3.p2.8.m8.1.1.3.3.3.cmml" xref="S3.p2.8.m8.1.1.3.3.3">ùêµ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.8.m8.1c">\mathbf{y}_{B}=\Phi_{B}\mathbf{x}_{B}</annotation></semantics></math>. We call this as ‚ÄòB-CS-VQA‚Äô (B <math id="S3.p2.9.m9.1" class="ltx_Math" alttext="=" display="inline"><semantics id="S3.p2.9.m9.1a"><mo id="S3.p2.9.m9.1.1" xref="S3.p2.9.m9.1.1.cmml">=</mo><annotation-xml encoding="MathML-Content" id="S3.p2.9.m9.1b"><eq id="S3.p2.9.m9.1.1.cmml" xref="S3.p2.9.m9.1.1"></eq></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.9.m9.1c">=</annotation></semantics></math> block).</p>
</div>
<div id="S3.p3" class="ltx_para ltx_indent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Reconstruction and Network Architecture:</span>
Most common VQA architectures consists of two streams ‚Äì one which operates on the given image and outputs a visual feature, and the other which operates on a word-embedding of the question and outputs a text feature. These feature vectors are concatenated, and further processed by a small network of fully-connected layers to obtain probability scores over the set of possible answers to the question. However, as described above, our CS-VQA dataset consists of modulated CS measurements. Hence, we need to redesign the image feature stream. We use four different approaches to recover surrogates of the image from its compressive measurement, each with different levels of sophistication. An overview of our CS-VQA architecture is shown in Figure¬†<a href="#S3.F1" title="Figure 1 ‚Ä£ 3 Problem Formulation and Approach ‚Ä£ CS-VQA: Visual Question Answering with Compressively Sensed Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We investigate the following CS surrogate-reconstruction approaches.</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Raw Multiplexed: This is when we <span id="S3.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">do not</span> perform any CS reconstruction, but use the raw CS measurement vectors as image features directly (i.e. no need for visual feature extraction using GoogleNet).</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.5" class="ltx_p"><math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="\Phi^{T}\mathbf{y}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><msup id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.I1.i2.p1.1.m1.1.1.2.2" xref="S3.I1.i2.p1.1.m1.1.1.2.2.cmml">Œ¶</mi><mi id="S3.I1.i2.p1.1.m1.1.1.2.3" xref="S3.I1.i2.p1.1.m1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.1.m1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml">ùê≤</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><times id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1"></times><apply id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">superscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2.2">Œ¶</ci><ci id="S3.I1.i2.p1.1.m1.1.1.2.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2.3">ùëá</ci></apply><ci id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">ùê≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\Phi^{T}\mathbf{y}</annotation></semantics></math>: For each image, <math id="S3.I1.i2.p1.2.m2.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.I1.i2.p1.2.m2.1a"><mi id="S3.I1.i2.p1.2.m2.1.1" xref="S3.I1.i2.p1.2.m2.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.2.m2.1b"><ci id="S3.I1.i2.p1.2.m2.1.1.cmml" xref="S3.I1.i2.p1.2.m2.1.1">ùêº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.2.m2.1c">I</annotation></semantics></math>, in the FF-CS-VQA dataset, we apply the transformation <math id="S3.I1.i2.p1.3.m3.1" class="ltx_Math" alttext="\Phi^{T}" display="inline"><semantics id="S3.I1.i2.p1.3.m3.1a"><msup id="S3.I1.i2.p1.3.m3.1.1" xref="S3.I1.i2.p1.3.m3.1.1.cmml"><mi mathvariant="normal" id="S3.I1.i2.p1.3.m3.1.1.2" xref="S3.I1.i2.p1.3.m3.1.1.2.cmml">Œ¶</mi><mi id="S3.I1.i2.p1.3.m3.1.1.3" xref="S3.I1.i2.p1.3.m3.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.3.m3.1b"><apply id="S3.I1.i2.p1.3.m3.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.3.m3.1.1.1.cmml" xref="S3.I1.i2.p1.3.m3.1.1">superscript</csymbol><ci id="S3.I1.i2.p1.3.m3.1.1.2.cmml" xref="S3.I1.i2.p1.3.m3.1.1.2">Œ¶</ci><ci id="S3.I1.i2.p1.3.m3.1.1.3.cmml" xref="S3.I1.i2.p1.3.m3.1.1.3">ùëá</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.3.m3.1c">\Phi^{T}</annotation></semantics></math> to the measurement vector, <math id="S3.I1.i2.p1.4.m4.1" class="ltx_Math" alttext="\mathbf{y}" display="inline"><semantics id="S3.I1.i2.p1.4.m4.1a"><mi id="S3.I1.i2.p1.4.m4.1.1" xref="S3.I1.i2.p1.4.m4.1.1.cmml">ùê≤</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.4.m4.1b"><ci id="S3.I1.i2.p1.4.m4.1.1.cmml" xref="S3.I1.i2.p1.4.m4.1.1">ùê≤</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.4.m4.1c">\mathbf{y}</annotation></semantics></math> to obtain <math id="S3.I1.i2.p1.5.m5.1" class="ltx_Math" alttext="\Phi^{T}\mathbf{y}" display="inline"><semantics id="S3.I1.i2.p1.5.m5.1a"><mrow id="S3.I1.i2.p1.5.m5.1.1" xref="S3.I1.i2.p1.5.m5.1.1.cmml"><msup id="S3.I1.i2.p1.5.m5.1.1.2" xref="S3.I1.i2.p1.5.m5.1.1.2.cmml"><mi mathvariant="normal" id="S3.I1.i2.p1.5.m5.1.1.2.2" xref="S3.I1.i2.p1.5.m5.1.1.2.2.cmml">Œ¶</mi><mi id="S3.I1.i2.p1.5.m5.1.1.2.3" xref="S3.I1.i2.p1.5.m5.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.5.m5.1.1.1" xref="S3.I1.i2.p1.5.m5.1.1.1.cmml">‚Äã</mo><mi id="S3.I1.i2.p1.5.m5.1.1.3" xref="S3.I1.i2.p1.5.m5.1.1.3.cmml">ùê≤</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.5.m5.1b"><apply id="S3.I1.i2.p1.5.m5.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1"><times id="S3.I1.i2.p1.5.m5.1.1.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1.1"></times><apply id="S3.I1.i2.p1.5.m5.1.1.2.cmml" xref="S3.I1.i2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i2.p1.5.m5.1.1.2.1.cmml" xref="S3.I1.i2.p1.5.m5.1.1.2">superscript</csymbol><ci id="S3.I1.i2.p1.5.m5.1.1.2.2.cmml" xref="S3.I1.i2.p1.5.m5.1.1.2.2">Œ¶</ci><ci id="S3.I1.i2.p1.5.m5.1.1.2.3.cmml" xref="S3.I1.i2.p1.5.m5.1.1.2.3">ùëá</ci></apply><ci id="S3.I1.i2.p1.5.m5.1.1.3.cmml" xref="S3.I1.i2.p1.5.m5.1.1.3">ùê≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.5.m5.1c">\Phi^{T}\mathbf{y}</annotation></semantics></math> which is reshaped to the image size.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.3" class="ltx_p">Block-wise linear inversion, <math id="S3.I1.i3.p1.1.m1.1" class="ltx_Math" alttext="\Phi_{B}^{T}\mathbf{y}_{B}" display="inline"><semantics id="S3.I1.i3.p1.1.m1.1a"><mrow id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><msubsup id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S3.I1.i3.p1.1.m1.1.1.2.2.2" xref="S3.I1.i3.p1.1.m1.1.1.2.2.2.cmml">Œ¶</mi><mi id="S3.I1.i3.p1.1.m1.1.1.2.2.3" xref="S3.I1.i3.p1.1.m1.1.1.2.2.3.cmml">B</mi><mi id="S3.I1.i3.p1.1.m1.1.1.2.3" xref="S3.I1.i3.p1.1.m1.1.1.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S3.I1.i3.p1.1.m1.1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.3.2" xref="S3.I1.i3.p1.1.m1.1.1.3.2.cmml">ùê≤</mi><mi id="S3.I1.i3.p1.1.m1.1.1.3.3" xref="S3.I1.i3.p1.1.m1.1.1.3.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><times id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.1"></times><apply id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">superscript</csymbol><apply id="S3.I1.i3.p1.1.m1.1.1.2.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.2.2.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.2.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2.2.2">Œ¶</ci><ci id="S3.I1.i3.p1.1.m1.1.1.2.2.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2.2.3">ùêµ</ci></apply><ci id="S3.I1.i3.p1.1.m1.1.1.2.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2.3">ùëá</ci></apply><apply id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3.2">ùê≤</ci><ci id="S3.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3.3">ùêµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\Phi_{B}^{T}\mathbf{y}_{B}</annotation></semantics></math>: In the B-CS-VQA dataset, we apply the transformation <math id="S3.I1.i3.p1.2.m2.1" class="ltx_Math" alttext="\Phi_{B}^{T}" display="inline"><semantics id="S3.I1.i3.p1.2.m2.1a"><msubsup id="S3.I1.i3.p1.2.m2.1.1" xref="S3.I1.i3.p1.2.m2.1.1.cmml"><mi mathvariant="normal" id="S3.I1.i3.p1.2.m2.1.1.2.2" xref="S3.I1.i3.p1.2.m2.1.1.2.2.cmml">Œ¶</mi><mi id="S3.I1.i3.p1.2.m2.1.1.2.3" xref="S3.I1.i3.p1.2.m2.1.1.2.3.cmml">B</mi><mi id="S3.I1.i3.p1.2.m2.1.1.3" xref="S3.I1.i3.p1.2.m2.1.1.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.2.m2.1b"><apply id="S3.I1.i3.p1.2.m2.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.2.m2.1.1.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">superscript</csymbol><apply id="S3.I1.i3.p1.2.m2.1.1.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.2.m2.1.1.2.1.cmml" xref="S3.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.2.m2.1.1.2.2.cmml" xref="S3.I1.i3.p1.2.m2.1.1.2.2">Œ¶</ci><ci id="S3.I1.i3.p1.2.m2.1.1.2.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.2.3">ùêµ</ci></apply><ci id="S3.I1.i3.p1.2.m2.1.1.3.cmml" xref="S3.I1.i3.p1.2.m2.1.1.3">ùëá</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.2.m2.1c">\Phi_{B}^{T}</annotation></semantics></math> to the measurement vectors for each non-overlapping block, and reshape the transformed vectors to the size of the image block. The reshaped blocks are arranged on a 2D grid, given by <math id="S3.I1.i3.p1.3.m3.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.I1.i3.p1.3.m3.1a"><mi id="S3.I1.i3.p1.3.m3.1.1" xref="S3.I1.i3.p1.3.m3.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.3.m3.1b"><ci id="S3.I1.i3.p1.3.m3.1.1.cmml" xref="S3.I1.i3.p1.3.m3.1.1">ùêº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.3.m3.1c">I</annotation></semantics></math>.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">ReconNet: For each image, <math id="S3.I1.i4.p1.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.I1.i4.p1.1.m1.1a"><mi id="S3.I1.i4.p1.1.m1.1.1" xref="S3.I1.i4.p1.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.I1.i4.p1.1.m1.1b"><ci id="S3.I1.i4.p1.1.m1.1.1.cmml" xref="S3.I1.i4.p1.1.m1.1.1">ùêº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i4.p1.1.m1.1c">I</annotation></semantics></math>, in the B-CS-VQA dataset, we use ReconNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, to obtain the reconstructed images. This corresponds to full reconstruction.</p>
</div>
</li>
</ul>
</div>
<div id="S3.p4" class="ltx_para ltx_indent">
<p id="S3.p4.6" class="ltx_p"><span id="S3.p4.6.1" class="ltx_text ltx_font_bold">Visual Feature Extraction:</span> After CS surrogate reconstruction, we use GoogleNet¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> to extract visual features. To train GoogleNet on these surrogates, we employ the following scheme: (1) initialize with pre-trained weights from the ImageNet dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>, and then (2) fine-tune the network by performing image classification on CS surrogate-reconstructions. Given an image <math id="S3.p4.1.m1.1" class="ltx_Math" alttext="I" display="inline"><semantics id="S3.p4.1.m1.1a"><mi id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml">I</mi><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><ci id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1">ùêº</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">I</annotation></semantics></math>, we obtain a 1024-length feature representation for the image by tapping the output of the penultimate layer of the GoogleNet, denoted by <math id="S3.p4.2.m2.1" class="ltx_Math" alttext="\mathbf{v}_{I}" display="inline"><semantics id="S3.p4.2.m2.1a"><msub id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mi id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">ùêØ</mi><mi id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml">I</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2">ùêØ</ci><ci id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3">ùêº</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">\mathbf{v}_{I}</annotation></semantics></math>.
<span id="S3.p4.6.2" class="ltx_text ltx_font_bold">Question Embedding:</span>
Questions are encoded using Word2vec <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>, such that the input to the LSTM is a sequence <math id="S3.p4.3.m3.3" class="ltx_Math" alttext="Q=\left(\mathbf{w}_{1},\ldots,\mathbf{w}_{N}\right)" display="inline"><semantics id="S3.p4.3.m3.3a"><mrow id="S3.p4.3.m3.3.3" xref="S3.p4.3.m3.3.3.cmml"><mi id="S3.p4.3.m3.3.3.4" xref="S3.p4.3.m3.3.3.4.cmml">Q</mi><mo id="S3.p4.3.m3.3.3.3" xref="S3.p4.3.m3.3.3.3.cmml">=</mo><mrow id="S3.p4.3.m3.3.3.2.2" xref="S3.p4.3.m3.3.3.2.3.cmml"><mo id="S3.p4.3.m3.3.3.2.2.3" xref="S3.p4.3.m3.3.3.2.3.cmml">(</mo><msub id="S3.p4.3.m3.2.2.1.1.1" xref="S3.p4.3.m3.2.2.1.1.1.cmml"><mi id="S3.p4.3.m3.2.2.1.1.1.2" xref="S3.p4.3.m3.2.2.1.1.1.2.cmml">ùê∞</mi><mn id="S3.p4.3.m3.2.2.1.1.1.3" xref="S3.p4.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p4.3.m3.3.3.2.2.4" xref="S3.p4.3.m3.3.3.2.3.cmml">,</mo><mi mathvariant="normal" id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml">‚Ä¶</mi><mo id="S3.p4.3.m3.3.3.2.2.5" xref="S3.p4.3.m3.3.3.2.3.cmml">,</mo><msub id="S3.p4.3.m3.3.3.2.2.2" xref="S3.p4.3.m3.3.3.2.2.2.cmml"><mi id="S3.p4.3.m3.3.3.2.2.2.2" xref="S3.p4.3.m3.3.3.2.2.2.2.cmml">ùê∞</mi><mi id="S3.p4.3.m3.3.3.2.2.2.3" xref="S3.p4.3.m3.3.3.2.2.2.3.cmml">N</mi></msub><mo id="S3.p4.3.m3.3.3.2.2.6" xref="S3.p4.3.m3.3.3.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.3b"><apply id="S3.p4.3.m3.3.3.cmml" xref="S3.p4.3.m3.3.3"><eq id="S3.p4.3.m3.3.3.3.cmml" xref="S3.p4.3.m3.3.3.3"></eq><ci id="S3.p4.3.m3.3.3.4.cmml" xref="S3.p4.3.m3.3.3.4">ùëÑ</ci><vector id="S3.p4.3.m3.3.3.2.3.cmml" xref="S3.p4.3.m3.3.3.2.2"><apply id="S3.p4.3.m3.2.2.1.1.1.cmml" xref="S3.p4.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p4.3.m3.2.2.1.1.1.1.cmml" xref="S3.p4.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.p4.3.m3.2.2.1.1.1.2.cmml" xref="S3.p4.3.m3.2.2.1.1.1.2">ùê∞</ci><cn type="integer" id="S3.p4.3.m3.2.2.1.1.1.3.cmml" xref="S3.p4.3.m3.2.2.1.1.1.3">1</cn></apply><ci id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1">‚Ä¶</ci><apply id="S3.p4.3.m3.3.3.2.2.2.cmml" xref="S3.p4.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p4.3.m3.3.3.2.2.2.1.cmml" xref="S3.p4.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.p4.3.m3.3.3.2.2.2.2.cmml" xref="S3.p4.3.m3.3.3.2.2.2.2">ùê∞</ci><ci id="S3.p4.3.m3.3.3.2.2.2.3.cmml" xref="S3.p4.3.m3.3.3.2.2.2.3">ùëÅ</ci></apply></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.3c">Q=\left(\mathbf{w}_{1},\ldots,\mathbf{w}_{N}\right)</annotation></semantics></math>. We employ an LSTM that is identical to that of¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. The LSTM states represent sequence embeddings, <math id="S3.p4.4.m4.2" class="ltx_Math" alttext="\mathbf{h}_{t}=LSTM(\mathbf{w}_{t},\mathbf{h}_{t-1})" display="inline"><semantics id="S3.p4.4.m4.2a"><mrow id="S3.p4.4.m4.2.2" xref="S3.p4.4.m4.2.2.cmml"><msub id="S3.p4.4.m4.2.2.4" xref="S3.p4.4.m4.2.2.4.cmml"><mi id="S3.p4.4.m4.2.2.4.2" xref="S3.p4.4.m4.2.2.4.2.cmml">ùê°</mi><mi id="S3.p4.4.m4.2.2.4.3" xref="S3.p4.4.m4.2.2.4.3.cmml">t</mi></msub><mo id="S3.p4.4.m4.2.2.3" xref="S3.p4.4.m4.2.2.3.cmml">=</mo><mrow id="S3.p4.4.m4.2.2.2" xref="S3.p4.4.m4.2.2.2.cmml"><mi id="S3.p4.4.m4.2.2.2.4" xref="S3.p4.4.m4.2.2.2.4.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.2.2.2.3" xref="S3.p4.4.m4.2.2.2.3.cmml">‚Äã</mo><mi id="S3.p4.4.m4.2.2.2.5" xref="S3.p4.4.m4.2.2.2.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.2.2.2.3a" xref="S3.p4.4.m4.2.2.2.3.cmml">‚Äã</mo><mi id="S3.p4.4.m4.2.2.2.6" xref="S3.p4.4.m4.2.2.2.6.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.2.2.2.3b" xref="S3.p4.4.m4.2.2.2.3.cmml">‚Äã</mo><mi id="S3.p4.4.m4.2.2.2.7" xref="S3.p4.4.m4.2.2.2.7.cmml">M</mi><mo lspace="0em" rspace="0em" id="S3.p4.4.m4.2.2.2.3c" xref="S3.p4.4.m4.2.2.2.3.cmml">‚Äã</mo><mrow id="S3.p4.4.m4.2.2.2.2.2" xref="S3.p4.4.m4.2.2.2.2.3.cmml"><mo stretchy="false" id="S3.p4.4.m4.2.2.2.2.2.3" xref="S3.p4.4.m4.2.2.2.2.3.cmml">(</mo><msub id="S3.p4.4.m4.1.1.1.1.1.1" xref="S3.p4.4.m4.1.1.1.1.1.1.cmml"><mi id="S3.p4.4.m4.1.1.1.1.1.1.2" xref="S3.p4.4.m4.1.1.1.1.1.1.2.cmml">ùê∞</mi><mi id="S3.p4.4.m4.1.1.1.1.1.1.3" xref="S3.p4.4.m4.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.p4.4.m4.2.2.2.2.2.4" xref="S3.p4.4.m4.2.2.2.2.3.cmml">,</mo><msub id="S3.p4.4.m4.2.2.2.2.2.2" xref="S3.p4.4.m4.2.2.2.2.2.2.cmml"><mi id="S3.p4.4.m4.2.2.2.2.2.2.2" xref="S3.p4.4.m4.2.2.2.2.2.2.2.cmml">ùê°</mi><mrow id="S3.p4.4.m4.2.2.2.2.2.2.3" xref="S3.p4.4.m4.2.2.2.2.2.2.3.cmml"><mi id="S3.p4.4.m4.2.2.2.2.2.2.3.2" xref="S3.p4.4.m4.2.2.2.2.2.2.3.2.cmml">t</mi><mo id="S3.p4.4.m4.2.2.2.2.2.2.3.1" xref="S3.p4.4.m4.2.2.2.2.2.2.3.1.cmml">‚àí</mo><mn id="S3.p4.4.m4.2.2.2.2.2.2.3.3" xref="S3.p4.4.m4.2.2.2.2.2.2.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="S3.p4.4.m4.2.2.2.2.2.5" xref="S3.p4.4.m4.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.4.m4.2b"><apply id="S3.p4.4.m4.2.2.cmml" xref="S3.p4.4.m4.2.2"><eq id="S3.p4.4.m4.2.2.3.cmml" xref="S3.p4.4.m4.2.2.3"></eq><apply id="S3.p4.4.m4.2.2.4.cmml" xref="S3.p4.4.m4.2.2.4"><csymbol cd="ambiguous" id="S3.p4.4.m4.2.2.4.1.cmml" xref="S3.p4.4.m4.2.2.4">subscript</csymbol><ci id="S3.p4.4.m4.2.2.4.2.cmml" xref="S3.p4.4.m4.2.2.4.2">ùê°</ci><ci id="S3.p4.4.m4.2.2.4.3.cmml" xref="S3.p4.4.m4.2.2.4.3">ùë°</ci></apply><apply id="S3.p4.4.m4.2.2.2.cmml" xref="S3.p4.4.m4.2.2.2"><times id="S3.p4.4.m4.2.2.2.3.cmml" xref="S3.p4.4.m4.2.2.2.3"></times><ci id="S3.p4.4.m4.2.2.2.4.cmml" xref="S3.p4.4.m4.2.2.2.4">ùêø</ci><ci id="S3.p4.4.m4.2.2.2.5.cmml" xref="S3.p4.4.m4.2.2.2.5">ùëÜ</ci><ci id="S3.p4.4.m4.2.2.2.6.cmml" xref="S3.p4.4.m4.2.2.2.6">ùëá</ci><ci id="S3.p4.4.m4.2.2.2.7.cmml" xref="S3.p4.4.m4.2.2.2.7">ùëÄ</ci><interval closure="open" id="S3.p4.4.m4.2.2.2.2.3.cmml" xref="S3.p4.4.m4.2.2.2.2.2"><apply id="S3.p4.4.m4.1.1.1.1.1.1.cmml" xref="S3.p4.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p4.4.m4.1.1.1.1.1.1.1.cmml" xref="S3.p4.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p4.4.m4.1.1.1.1.1.1.2.cmml" xref="S3.p4.4.m4.1.1.1.1.1.1.2">ùê∞</ci><ci id="S3.p4.4.m4.1.1.1.1.1.1.3.cmml" xref="S3.p4.4.m4.1.1.1.1.1.1.3">ùë°</ci></apply><apply id="S3.p4.4.m4.2.2.2.2.2.2.cmml" xref="S3.p4.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p4.4.m4.2.2.2.2.2.2.1.cmml" xref="S3.p4.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="S3.p4.4.m4.2.2.2.2.2.2.2.cmml" xref="S3.p4.4.m4.2.2.2.2.2.2.2">ùê°</ci><apply id="S3.p4.4.m4.2.2.2.2.2.2.3.cmml" xref="S3.p4.4.m4.2.2.2.2.2.2.3"><minus id="S3.p4.4.m4.2.2.2.2.2.2.3.1.cmml" xref="S3.p4.4.m4.2.2.2.2.2.2.3.1"></minus><ci id="S3.p4.4.m4.2.2.2.2.2.2.3.2.cmml" xref="S3.p4.4.m4.2.2.2.2.2.2.3.2">ùë°</ci><cn type="integer" id="S3.p4.4.m4.2.2.2.2.2.2.3.3.cmml" xref="S3.p4.4.m4.2.2.2.2.2.2.3.3">1</cn></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.4.m4.2c">\mathbf{h}_{t}=LSTM(\mathbf{w}_{t},\mathbf{h}_{t-1})</annotation></semantics></math>, <math id="S3.p4.5.m5.1" class="ltx_Math" alttext="\mathbf{h}_{0}" display="inline"><semantics id="S3.p4.5.m5.1a"><msub id="S3.p4.5.m5.1.1" xref="S3.p4.5.m5.1.1.cmml"><mi id="S3.p4.5.m5.1.1.2" xref="S3.p4.5.m5.1.1.2.cmml">ùê°</mi><mn id="S3.p4.5.m5.1.1.3" xref="S3.p4.5.m5.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S3.p4.5.m5.1b"><apply id="S3.p4.5.m5.1.1.cmml" xref="S3.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S3.p4.5.m5.1.1.1.cmml" xref="S3.p4.5.m5.1.1">subscript</csymbol><ci id="S3.p4.5.m5.1.1.2.cmml" xref="S3.p4.5.m5.1.1.2">ùê°</ci><cn type="integer" id="S3.p4.5.m5.1.1.3.cmml" xref="S3.p4.5.m5.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.5.m5.1c">\mathbf{h}_{0}</annotation></semantics></math> is an all-zero vector. The question embedding is the final state of the LSTM <math id="S3.p4.6.m6.1" class="ltx_Math" alttext="\mathbf{q}_{I}=\mathbf{h}_{T}" display="inline"><semantics id="S3.p4.6.m6.1a"><mrow id="S3.p4.6.m6.1.1" xref="S3.p4.6.m6.1.1.cmml"><msub id="S3.p4.6.m6.1.1.2" xref="S3.p4.6.m6.1.1.2.cmml"><mi id="S3.p4.6.m6.1.1.2.2" xref="S3.p4.6.m6.1.1.2.2.cmml">ùê™</mi><mi id="S3.p4.6.m6.1.1.2.3" xref="S3.p4.6.m6.1.1.2.3.cmml">I</mi></msub><mo id="S3.p4.6.m6.1.1.1" xref="S3.p4.6.m6.1.1.1.cmml">=</mo><msub id="S3.p4.6.m6.1.1.3" xref="S3.p4.6.m6.1.1.3.cmml"><mi id="S3.p4.6.m6.1.1.3.2" xref="S3.p4.6.m6.1.1.3.2.cmml">ùê°</mi><mi id="S3.p4.6.m6.1.1.3.3" xref="S3.p4.6.m6.1.1.3.3.cmml">T</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.6.m6.1b"><apply id="S3.p4.6.m6.1.1.cmml" xref="S3.p4.6.m6.1.1"><eq id="S3.p4.6.m6.1.1.1.cmml" xref="S3.p4.6.m6.1.1.1"></eq><apply id="S3.p4.6.m6.1.1.2.cmml" xref="S3.p4.6.m6.1.1.2"><csymbol cd="ambiguous" id="S3.p4.6.m6.1.1.2.1.cmml" xref="S3.p4.6.m6.1.1.2">subscript</csymbol><ci id="S3.p4.6.m6.1.1.2.2.cmml" xref="S3.p4.6.m6.1.1.2.2">ùê™</ci><ci id="S3.p4.6.m6.1.1.2.3.cmml" xref="S3.p4.6.m6.1.1.2.3">ùêº</ci></apply><apply id="S3.p4.6.m6.1.1.3.cmml" xref="S3.p4.6.m6.1.1.3"><csymbol cd="ambiguous" id="S3.p4.6.m6.1.1.3.1.cmml" xref="S3.p4.6.m6.1.1.3">subscript</csymbol><ci id="S3.p4.6.m6.1.1.3.2.cmml" xref="S3.p4.6.m6.1.1.3.2">ùê°</ci><ci id="S3.p4.6.m6.1.1.3.3.cmml" xref="S3.p4.6.m6.1.1.3.3">ùëá</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.6.m6.1c">\mathbf{q}_{I}=\mathbf{h}_{T}</annotation></semantics></math>¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
<span id="S3.p4.6.3" class="ltx_text ltx_font_bold">Fusing Visual and Language Features:</span> We use simple concatenation to fuse image and question feature vectors. This fused vector is fed into a fully connected network.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.4" class="ltx_p">In this section, we evaluate the proposed architectures for the proposed CS-VQA task. The VQA dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> uses images from the MS COCO dataset¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, which contains <math id="S4.p1.1.m1.1" class="ltx_Math" alttext="83783" display="inline"><semantics id="S4.p1.1.m1.1a"><mn id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml">83783</mn><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><cn type="integer" id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1">83783</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">83783</annotation></semantics></math> training images and <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="40504" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">40504</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="integer" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">40504</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">40504</annotation></semantics></math> validation images. The dataset includes three questions for each image, so there are a total of <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="248349" display="inline"><semantics id="S4.p1.3.m3.1a"><mn id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">248349</mn><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><cn type="integer" id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">248349</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">248349</annotation></semantics></math> questions for the training set and <math id="S4.p1.4.m4.1" class="ltx_Math" alttext="121512" display="inline"><semantics id="S4.p1.4.m4.1a"><mn id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml">121512</mn><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><cn type="integer" id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1">121512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">121512</annotation></semantics></math> questions for the validation set. Answers for questions are generated by Amazon Mechanical Turk (AMT) annotators, with 10 answers per question from unique annotators. Answers are generally open-ended, types of answers are generally classified as ‚Äúyes and no‚Äù, ‚Äúnumber‚Äù and ‚Äúother‚Äù answers. We adopt the validation set to test the performance of the proposed approach. The evaluation metric for the open-ended task in VQA dataset given a generated answer is as following:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.4" class="ltx_Math" alttext="\text{Accuracy}=\min\left(\frac{\text{\# of matches to ground truth}}{3},1\right)." display="block"><semantics id="S4.E2.m1.4a"><mrow id="S4.E2.m1.4.4.1" xref="S4.E2.m1.4.4.1.1.cmml"><mrow id="S4.E2.m1.4.4.1.1" xref="S4.E2.m1.4.4.1.1.cmml"><mtext id="S4.E2.m1.4.4.1.1.2" xref="S4.E2.m1.4.4.1.1.2a.cmml">Accuracy</mtext><mo id="S4.E2.m1.4.4.1.1.1" xref="S4.E2.m1.4.4.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.4.4.1.1.3.2" xref="S4.E2.m1.4.4.1.1.3.1.cmml"><mi id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml">min</mi><mo id="S4.E2.m1.4.4.1.1.3.2a" xref="S4.E2.m1.4.4.1.1.3.1.cmml">‚Å°</mo><mrow id="S4.E2.m1.4.4.1.1.3.2.1" xref="S4.E2.m1.4.4.1.1.3.1.cmml"><mo id="S4.E2.m1.4.4.1.1.3.2.1.1" xref="S4.E2.m1.4.4.1.1.3.1.cmml">(</mo><mfrac id="S4.E2.m1.2.2" xref="S4.E2.m1.2.2.cmml"><mtext id="S4.E2.m1.2.2.2" xref="S4.E2.m1.2.2.2a.cmml"># of matches to ground truth</mtext><mn id="S4.E2.m1.2.2.3" xref="S4.E2.m1.2.2.3.cmml">3</mn></mfrac><mo id="S4.E2.m1.4.4.1.1.3.2.1.2" xref="S4.E2.m1.4.4.1.1.3.1.cmml">,</mo><mn id="S4.E2.m1.3.3" xref="S4.E2.m1.3.3.cmml">1</mn><mo id="S4.E2.m1.4.4.1.1.3.2.1.3" xref="S4.E2.m1.4.4.1.1.3.1.cmml">)</mo></mrow></mrow></mrow><mo lspace="0em" id="S4.E2.m1.4.4.1.2" xref="S4.E2.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.4b"><apply id="S4.E2.m1.4.4.1.1.cmml" xref="S4.E2.m1.4.4.1"><eq id="S4.E2.m1.4.4.1.1.1.cmml" xref="S4.E2.m1.4.4.1.1.1"></eq><ci id="S4.E2.m1.4.4.1.1.2a.cmml" xref="S4.E2.m1.4.4.1.1.2"><mtext id="S4.E2.m1.4.4.1.1.2.cmml" xref="S4.E2.m1.4.4.1.1.2">Accuracy</mtext></ci><apply id="S4.E2.m1.4.4.1.1.3.1.cmml" xref="S4.E2.m1.4.4.1.1.3.2"><min id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"></min><apply id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2"><divide id="S4.E2.m1.2.2.1.cmml" xref="S4.E2.m1.2.2"></divide><ci id="S4.E2.m1.2.2.2a.cmml" xref="S4.E2.m1.2.2.2"><mtext id="S4.E2.m1.2.2.2.cmml" xref="S4.E2.m1.2.2.2"># of matches to ground truth</mtext></ci><cn type="integer" id="S4.E2.m1.2.2.3.cmml" xref="S4.E2.m1.2.2.3">3</cn></apply><cn type="integer" id="S4.E2.m1.3.3.cmml" xref="S4.E2.m1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.4c">\text{Accuracy}=\min\left(\frac{\text{\# of matches to ground truth}}{3},1\right).</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S4.p1.5" class="ltx_p">This metric gives the answer full credit if the generated answer matches with at least three (of ten) answers provided by AMT annotators. Otherwise, it is given partial credit.</p>
</div>
<div id="S4.p2" class="ltx_para ltx_indent">
<p id="S4.p2.1" class="ltx_p"><span id="S4.p2.1.1" class="ltx_text ltx_font_bold">Training Details:</span> GoogLeNet was finetuned on Caffe, whereas TensorFlow framework was used to train and test the LSTM unit. All training and testing was performed on an NVIDIA Titan X GPU. For finetuning GoogleNet a batch size of <math id="S4.p2.1.m1.1" class="ltx_Math" alttext="32" display="inline"><semantics id="S4.p2.1.m1.1a"><mn id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml">32</mn><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><cn type="integer" id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1">32</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">32</annotation></semantics></math> images was used, with data augmentation by mirror reflection of images. At MR = 0.25, stochastic gradient descent (SGD) was used with momentum 0.9, initial learning rate of 0.001 and learning rate decay of 0.8 for every 80000 iterations. A dropout of 0.4 was used on the last fully connected layer. For LSTM training, Adam optimizer was used with initial learning rate of 0.0003 and learning rate decay of 0.999 for every 5000 iterations. A dropout of 0.5 was used on each LSTM layer. Finetuning takes about 7 days when starting from pre-trained GoogleNet.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.2.3" class="ltx_tr">
<td id="S4.T1.2.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T1.2.3.1.1" class="ltx_text" style="font-size:90%;"><span id="S4.T1.2.3.1.1.1" class="ltx_text"></span> <span id="S4.T1.2.3.1.1.2" class="ltx_text">
<span id="S4.T1.2.3.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T1.2.3.1.1.2.1.1" class="ltx_tr">
<span id="S4.T1.2.3.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">CS Reconstruction</span></span>
<span id="S4.T1.2.3.1.1.2.1.2" class="ltx_tr">
<span id="S4.T1.2.3.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Method</span></span>
</span></span> <span id="S4.T1.2.3.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T1.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4"><span id="S4.T1.2.3.2.1" class="ltx_text" style="font-size:90%;">Question Type</span></td>
</tr>
<tr id="S4.T1.2.4" class="ltx_tr">
<td id="S4.T1.2.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.1.1" class="ltx_text" style="font-size:90%;">All</span></td>
<td id="S4.T1.2.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.2.1" class="ltx_text" style="font-size:90%;">Yes/No</span></td>
<td id="S4.T1.2.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.3.1" class="ltx_text" style="font-size:90%;">Number</span></td>
<td id="S4.T1.2.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.4.4.1" class="ltx_text" style="font-size:90%;">Other</span></td>
</tr>
<tr id="S4.T1.2.5" class="ltx_tr">
<td id="S4.T1.2.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.T1.2.5.1.1" class="ltx_text" style="font-size:90%;">None (Raw Multiplexed)</span></td>
<td id="S4.T1.2.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.5.2.1" class="ltx_text" style="font-size:90%;">47.95</span></td>
<td id="S4.T1.2.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.5.3.1" class="ltx_text" style="font-size:90%;">78.34</span></td>
<td id="S4.T1.2.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.5.4.1" class="ltx_text" style="font-size:90%;">32.45</span></td>
<td id="S4.T1.2.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.5.5.1" class="ltx_text" style="font-size:90%;">29.10</span></td>
</tr>
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math id="S4.T1.1.1.1.m1.1" class="ltx_Math" alttext="\Phi^{T}\Phi\mathbf{x}" display="inline"><semantics id="S4.T1.1.1.1.m1.1a"><mrow id="S4.T1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.m1.1.1.cmml"><msup id="S4.T1.1.1.1.m1.1.1.2" xref="S4.T1.1.1.1.m1.1.1.2.cmml"><mi mathsize="90%" mathvariant="normal" id="S4.T1.1.1.1.m1.1.1.2.2" xref="S4.T1.1.1.1.m1.1.1.2.2.cmml">Œ¶</mi><mi mathsize="90%" id="S4.T1.1.1.1.m1.1.1.2.3" xref="S4.T1.1.1.1.m1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.1" xref="S4.T1.1.1.1.m1.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" mathvariant="normal" id="S4.T1.1.1.1.m1.1.1.3" xref="S4.T1.1.1.1.m1.1.1.3.cmml">Œ¶</mi><mo lspace="0em" rspace="0em" id="S4.T1.1.1.1.m1.1.1.1a" xref="S4.T1.1.1.1.m1.1.1.1.cmml">‚Äã</mo><mi mathsize="90%" id="S4.T1.1.1.1.m1.1.1.4" xref="S4.T1.1.1.1.m1.1.1.4.cmml">ùê±</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.m1.1b"><apply id="S4.T1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1"><times id="S4.T1.1.1.1.m1.1.1.1.cmml" xref="S4.T1.1.1.1.m1.1.1.1"></times><apply id="S4.T1.1.1.1.m1.1.1.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T1.1.1.1.m1.1.1.2">superscript</csymbol><ci id="S4.T1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T1.1.1.1.m1.1.1.2.2">Œ¶</ci><ci id="S4.T1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T1.1.1.1.m1.1.1.2.3">ùëá</ci></apply><ci id="S4.T1.1.1.1.m1.1.1.3.cmml" xref="S4.T1.1.1.1.m1.1.1.3">Œ¶</ci><ci id="S4.T1.1.1.1.m1.1.1.4.cmml" xref="S4.T1.1.1.1.m1.1.1.4">ùê±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.m1.1c">\Phi^{T}\Phi\mathbf{x}</annotation></semantics></math></td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.2.1" class="ltx_text" style="font-size:90%;">51.10</span></td>
<td id="S4.T1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.3.1" class="ltx_text" style="font-size:90%;">78.82</span></td>
<td id="S4.T1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.4.1" class="ltx_text" style="font-size:90%;">33.30</span></td>
<td id="S4.T1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.1.1.5.1" class="ltx_text" style="font-size:90%;">34.82</span></td>
</tr>
<tr id="S4.T1.2.2" class="ltx_tr">
<td id="S4.T1.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><math id="S4.T1.2.2.1.m1.1" class="ltx_Math" alttext="\Phi_{B}^{T}\Phi_{B}\mathbf{x}_{B}" display="inline"><semantics id="S4.T1.2.2.1.m1.1a"><mrow id="S4.T1.2.2.1.m1.1.1" xref="S4.T1.2.2.1.m1.1.1.cmml"><msubsup id="S4.T1.2.2.1.m1.1.1.2" xref="S4.T1.2.2.1.m1.1.1.2.cmml"><mi mathsize="90%" mathvariant="normal" id="S4.T1.2.2.1.m1.1.1.2.2.2" xref="S4.T1.2.2.1.m1.1.1.2.2.2.cmml">Œ¶</mi><mi mathsize="90%" id="S4.T1.2.2.1.m1.1.1.2.2.3" xref="S4.T1.2.2.1.m1.1.1.2.2.3.cmml">B</mi><mi mathsize="90%" id="S4.T1.2.2.1.m1.1.1.2.3" xref="S4.T1.2.2.1.m1.1.1.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.T1.2.2.1.m1.1.1.1" xref="S4.T1.2.2.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.T1.2.2.1.m1.1.1.3" xref="S4.T1.2.2.1.m1.1.1.3.cmml"><mi mathsize="90%" mathvariant="normal" id="S4.T1.2.2.1.m1.1.1.3.2" xref="S4.T1.2.2.1.m1.1.1.3.2.cmml">Œ¶</mi><mi mathsize="90%" id="S4.T1.2.2.1.m1.1.1.3.3" xref="S4.T1.2.2.1.m1.1.1.3.3.cmml">B</mi></msub><mo lspace="0em" rspace="0em" id="S4.T1.2.2.1.m1.1.1.1a" xref="S4.T1.2.2.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.T1.2.2.1.m1.1.1.4" xref="S4.T1.2.2.1.m1.1.1.4.cmml"><mi mathsize="90%" id="S4.T1.2.2.1.m1.1.1.4.2" xref="S4.T1.2.2.1.m1.1.1.4.2.cmml">ùê±</mi><mi mathsize="90%" id="S4.T1.2.2.1.m1.1.1.4.3" xref="S4.T1.2.2.1.m1.1.1.4.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.1.m1.1b"><apply id="S4.T1.2.2.1.m1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1"><times id="S4.T1.2.2.1.m1.1.1.1.cmml" xref="S4.T1.2.2.1.m1.1.1.1"></times><apply id="S4.T1.2.2.1.m1.1.1.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T1.2.2.1.m1.1.1.2.1.cmml" xref="S4.T1.2.2.1.m1.1.1.2">superscript</csymbol><apply id="S4.T1.2.2.1.m1.1.1.2.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T1.2.2.1.m1.1.1.2.2.1.cmml" xref="S4.T1.2.2.1.m1.1.1.2">subscript</csymbol><ci id="S4.T1.2.2.1.m1.1.1.2.2.2.cmml" xref="S4.T1.2.2.1.m1.1.1.2.2.2">Œ¶</ci><ci id="S4.T1.2.2.1.m1.1.1.2.2.3.cmml" xref="S4.T1.2.2.1.m1.1.1.2.2.3">ùêµ</ci></apply><ci id="S4.T1.2.2.1.m1.1.1.2.3.cmml" xref="S4.T1.2.2.1.m1.1.1.2.3">ùëá</ci></apply><apply id="S4.T1.2.2.1.m1.1.1.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T1.2.2.1.m1.1.1.3.1.cmml" xref="S4.T1.2.2.1.m1.1.1.3">subscript</csymbol><ci id="S4.T1.2.2.1.m1.1.1.3.2.cmml" xref="S4.T1.2.2.1.m1.1.1.3.2">Œ¶</ci><ci id="S4.T1.2.2.1.m1.1.1.3.3.cmml" xref="S4.T1.2.2.1.m1.1.1.3.3">ùêµ</ci></apply><apply id="S4.T1.2.2.1.m1.1.1.4.cmml" xref="S4.T1.2.2.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T1.2.2.1.m1.1.1.4.1.cmml" xref="S4.T1.2.2.1.m1.1.1.4">subscript</csymbol><ci id="S4.T1.2.2.1.m1.1.1.4.2.cmml" xref="S4.T1.2.2.1.m1.1.1.4.2">ùê±</ci><ci id="S4.T1.2.2.1.m1.1.1.4.3.cmml" xref="S4.T1.2.2.1.m1.1.1.4.3">ùêµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.1.m1.1c">\Phi_{B}^{T}\Phi_{B}\mathbf{x}_{B}</annotation></semantics></math></td>
<td id="S4.T1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.2.1" class="ltx_text" style="font-size:90%;">52.98</span></td>
<td id="S4.T1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.3.1" class="ltx_text" style="font-size:90%;">79.50</span></td>
<td id="S4.T1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.4.1" class="ltx_text" style="font-size:90%;">33.03</span></td>
<td id="S4.T1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.2.5.1" class="ltx_text" style="font-size:90%;">38.15</span></td>
</tr>
<tr id="S4.T1.2.6" class="ltx_tr">
<td id="S4.T1.2.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.6.1.1" class="ltx_text" style="font-size:90%;">ReconNet</span></td>
<td id="S4.T1.2.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.6.2.1" class="ltx_text" style="font-size:90%;">54.22</span></td>
<td id="S4.T1.2.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.6.3.1" class="ltx_text" style="font-size:90%;">79.85</span></td>
<td id="S4.T1.2.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.6.4.1" class="ltx_text" style="font-size:90%;">33.28</span></td>
<td id="S4.T1.2.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.6.5.1" class="ltx_text" style="font-size:90%;">40.21</span></td>
</tr>
<tr id="S4.T1.2.7" class="ltx_tr">
<td id="S4.T1.2.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt" colspan="5">
<span id="S4.T1.2.7.1.1" class="ltx_text" style="font-size:90%;">Oracle VQA¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T1.2.7.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="S4.T1.2.7.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S4.T1.2.8" class="ltx_tr">
<td id="S4.T1.2.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.T1.2.8.1.1" class="ltx_text" style="font-size:90%;">LSTM + VGG</span></td>
<td id="S4.T1.2.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.8.2.1" class="ltx_text" style="font-size:90%;">57.75</span></td>
<td id="S4.T1.2.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.8.3.1" class="ltx_text" style="font-size:90%;">80.50</span></td>
<td id="S4.T1.2.8.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.8.4.1" class="ltx_text" style="font-size:90%;">36.77</span></td>
<td id="S4.T1.2.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T1.2.8.5.1" class="ltx_text" style="font-size:90%;">43.08</span></td>
</tr>
<tr id="S4.T1.2.9" class="ltx_tr">
<td id="S4.T1.2.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.9.1.1" class="ltx_text" style="font-size:90%;">Image Only</span></td>
<td id="S4.T1.2.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.2.1" class="ltx_text" style="font-size:90%;">28.13</span></td>
<td id="S4.T1.2.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.3.1" class="ltx_text" style="font-size:90%;">64.01</span></td>
<td id="S4.T1.2.9.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.4.1" class="ltx_text" style="font-size:90%;">0.42</span></td>
<td id="S4.T1.2.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T1.2.9.5.1" class="ltx_text" style="font-size:90%;">3.77</span></td>
</tr>
<tr id="S4.T1.2.10" class="ltx_tr">
<td id="S4.T1.2.10.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T1.2.10.1.1" class="ltx_text" style="font-size:90%;">Question Only</span></td>
<td id="S4.T1.2.10.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.10.2.1" class="ltx_text" style="font-size:90%;">50.39</span></td>
<td id="S4.T1.2.10.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.10.3.1" class="ltx_text" style="font-size:90%;">78.41</span></td>
<td id="S4.T1.2.10.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.10.4.1" class="ltx_text" style="font-size:90%;">34.68</span></td>
<td id="S4.T1.2.10.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T1.2.10.5.1" class="ltx_text" style="font-size:90%;">30.03</span></td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.4.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Open-ended VQA v1.0 results with various CS surrogate-reconstructions, and their corresponding accuracy (%). GoogleNet was used for visual feature extraction, and a LSTM for generating question features. The oracle VQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> performance is presented for comparison. </figcaption>
</figure>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><span id="S4.p3.1.1" class="ltx_text ltx_font_bold">Main Results:</span> In Table¬†<a href="#S4.T1" title="Table 1 ‚Ä£ 4 Experiments ‚Ä£ CS-VQA: Visual Question Answering with Compressively Sensed Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we show the results of open-ended VQA performance for various different CS reconstruction techniques at MR = 0.25. We compare this to the original results from the VQA paper¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, which we term the Oracle VQA. Note that training directly on CS measurements themselves (Raw Multiplexed) yields a 10% point drop in performance, and is mostly comparable to the question-only baseline (i.e. when no visual information is used). Each reconstruction technique, <math id="S4.p3.1.m1.1" class="ltx_Math" alttext="\Phi^{T}\Phi,\Phi^{T}_{B}\Phi_{B}," display="inline"><semantics id="S4.p3.1.m1.1a"><mrow id="S4.p3.1.m1.1.1.1"><mrow id="S4.p3.1.m1.1.1.1.1.2" xref="S4.p3.1.m1.1.1.1.1.3.cmml"><mrow id="S4.p3.1.m1.1.1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.1.1.cmml"><msup id="S4.p3.1.m1.1.1.1.1.1.1.2" xref="S4.p3.1.m1.1.1.1.1.1.1.2.cmml"><mi mathvariant="normal" id="S4.p3.1.m1.1.1.1.1.1.1.2.2" xref="S4.p3.1.m1.1.1.1.1.1.1.2.2.cmml">Œ¶</mi><mi id="S4.p3.1.m1.1.1.1.1.1.1.2.3" xref="S4.p3.1.m1.1.1.1.1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.1.1.1.1.1" xref="S4.p3.1.m1.1.1.1.1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.p3.1.m1.1.1.1.1.1.1.3" xref="S4.p3.1.m1.1.1.1.1.1.1.3.cmml">Œ¶</mi></mrow><mo id="S4.p3.1.m1.1.1.1.1.2.3" xref="S4.p3.1.m1.1.1.1.1.3.cmml">,</mo><mrow id="S4.p3.1.m1.1.1.1.1.2.2" xref="S4.p3.1.m1.1.1.1.1.2.2.cmml"><msubsup id="S4.p3.1.m1.1.1.1.1.2.2.2" xref="S4.p3.1.m1.1.1.1.1.2.2.2.cmml"><mi mathvariant="normal" id="S4.p3.1.m1.1.1.1.1.2.2.2.2.2" xref="S4.p3.1.m1.1.1.1.1.2.2.2.2.2.cmml">Œ¶</mi><mi id="S4.p3.1.m1.1.1.1.1.2.2.2.3" xref="S4.p3.1.m1.1.1.1.1.2.2.2.3.cmml">B</mi><mi id="S4.p3.1.m1.1.1.1.1.2.2.2.2.3" xref="S4.p3.1.m1.1.1.1.1.2.2.2.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.p3.1.m1.1.1.1.1.2.2.1" xref="S4.p3.1.m1.1.1.1.1.2.2.1.cmml">‚Äã</mo><msub id="S4.p3.1.m1.1.1.1.1.2.2.3" xref="S4.p3.1.m1.1.1.1.1.2.2.3.cmml"><mi mathvariant="normal" id="S4.p3.1.m1.1.1.1.1.2.2.3.2" xref="S4.p3.1.m1.1.1.1.1.2.2.3.2.cmml">Œ¶</mi><mi id="S4.p3.1.m1.1.1.1.1.2.2.3.3" xref="S4.p3.1.m1.1.1.1.1.2.2.3.3.cmml">B</mi></msub></mrow></mrow><mo id="S4.p3.1.m1.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><list id="S4.p3.1.m1.1.1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.1.1.2"><apply id="S4.p3.1.m1.1.1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1"><times id="S4.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.1"></times><apply id="S4.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.1.1.1.2.1.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S4.p3.1.m1.1.1.1.1.1.1.2.2.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.2.2">Œ¶</ci><ci id="S4.p3.1.m1.1.1.1.1.1.1.2.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.2.3">ùëá</ci></apply><ci id="S4.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S4.p3.1.m1.1.1.1.1.1.1.3">Œ¶</ci></apply><apply id="S4.p3.1.m1.1.1.1.1.2.2.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2"><times id="S4.p3.1.m1.1.1.1.1.2.2.1.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.1"></times><apply id="S4.p3.1.m1.1.1.1.1.2.2.2.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.1.2.2.2.1.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.2">subscript</csymbol><apply id="S4.p3.1.m1.1.1.1.1.2.2.2.2.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.2">superscript</csymbol><ci id="S4.p3.1.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.2.2.2">Œ¶</ci><ci id="S4.p3.1.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.2.2.3">ùëá</ci></apply><ci id="S4.p3.1.m1.1.1.1.1.2.2.2.3.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.2.3">ùêµ</ci></apply><apply id="S4.p3.1.m1.1.1.1.1.2.2.3.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S4.p3.1.m1.1.1.1.1.2.2.3.1.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.3">subscript</csymbol><ci id="S4.p3.1.m1.1.1.1.1.2.2.3.2.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.3.2">Œ¶</ci><ci id="S4.p3.1.m1.1.1.1.1.2.2.3.3.cmml" xref="S4.p3.1.m1.1.1.1.1.2.2.3.3">ùêµ</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">\Phi^{T}\Phi,\Phi^{T}_{B}\Phi_{B},</annotation></semantics></math> ReconNet yields improvement to their performance, particularly in the ‚Äúother‚Äù question category. Note that this question category seems to rely the most on visual data as evidenced by the Oracle VQA performance presented. ReconNet performs the best of the proposed methods, and is within 3% points of the oracle VQA.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">In Figure¬†<a href="#S4.F2" title="Figure 2 ‚Ä£ 4 Experiments ‚Ä£ CS-VQA: Visual Question Answering with Compressively Sensed Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we show the results of three different models: Raw Multiplexed, <math id="S4.p4.1.m1.1" class="ltx_Math" alttext="\Phi^{T}_{B}" display="inline"><semantics id="S4.p4.1.m1.1a"><msubsup id="S4.p4.1.m1.1.1" xref="S4.p4.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.p4.1.m1.1.1.2.2" xref="S4.p4.1.m1.1.1.2.2.cmml">Œ¶</mi><mi id="S4.p4.1.m1.1.1.3" xref="S4.p4.1.m1.1.1.3.cmml">B</mi><mi id="S4.p4.1.m1.1.1.2.3" xref="S4.p4.1.m1.1.1.2.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.p4.1.m1.1b"><apply id="S4.p4.1.m1.1.1.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.1.cmml" xref="S4.p4.1.m1.1.1">subscript</csymbol><apply id="S4.p4.1.m1.1.1.2.cmml" xref="S4.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p4.1.m1.1.1.2.1.cmml" xref="S4.p4.1.m1.1.1">superscript</csymbol><ci id="S4.p4.1.m1.1.1.2.2.cmml" xref="S4.p4.1.m1.1.1.2.2">Œ¶</ci><ci id="S4.p4.1.m1.1.1.2.3.cmml" xref="S4.p4.1.m1.1.1.2.3">ùëá</ci></apply><ci id="S4.p4.1.m1.1.1.3.cmml" xref="S4.p4.1.m1.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p4.1.m1.1c">\Phi^{T}_{B}</annotation></semantics></math>, and ReconNet with respect to the oracle VQA algorithm, sorted by question category. The questions where the oracle method outperforms the three CS-VQA methods, typically feature a <span id="S4.p4.1.1" class="ltx_text ltx_font_italic">specific</span> question about a subject/object in the picture, including ‚Äúwhat animal is‚Äù, ‚Äúwhat room is‚Äù, ‚Äúwhat is the person‚Äù, ‚Äúwhat sport is‚Äù. In contrast, questions such as ‚Äúwhat color‚Äù, ‚Äúis there‚Äù, ‚Äúdo you‚Äù are better answered by the CS algorithms.</p>
</div>
<div id="S4.p5" class="ltx_para ltx_indent">
<p id="S4.p5.1" class="ltx_p"><span id="S4.p5.1.1" class="ltx_text ltx_font_bold">Measurement Rate:</span> We also tested the effect of varying the measurement rate on the results. At MR = 0.10, ReconNet‚Äôs VQA accuracy is 51.40% with a breakdown of 79.13% yes/no, 33.20% number, and 35.21% other. At MR = 0.01, ReconNet‚Äôs VQA accuracy is 51.05% with a breakdown of 78.77% yes/no, 32.92% number, and 34.87% other. This validates that reconstructions at low measurement rates still perform well on the VQA task.</p>
</div>
<div id="S4.p6" class="ltx_para ltx_indent">
<p id="S4.p6.1" class="ltx_p"><span id="S4.p6.1.1" class="ltx_text ltx_font_bold">VQA v2.0:</span> We also compared the performances of <math id="S4.p6.1.m1.1" class="ltx_Math" alttext="\Phi^{T}_{B}" display="inline"><semantics id="S4.p6.1.m1.1a"><msubsup id="S4.p6.1.m1.1.1" xref="S4.p6.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.p6.1.m1.1.1.2.2" xref="S4.p6.1.m1.1.1.2.2.cmml">Œ¶</mi><mi id="S4.p6.1.m1.1.1.3" xref="S4.p6.1.m1.1.1.3.cmml">B</mi><mi id="S4.p6.1.m1.1.1.2.3" xref="S4.p6.1.m1.1.1.2.3.cmml">T</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.p6.1.m1.1b"><apply id="S4.p6.1.m1.1.1.cmml" xref="S4.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.1.cmml" xref="S4.p6.1.m1.1.1">subscript</csymbol><apply id="S4.p6.1.m1.1.1.2.cmml" xref="S4.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p6.1.m1.1.1.2.1.cmml" xref="S4.p6.1.m1.1.1">superscript</csymbol><ci id="S4.p6.1.m1.1.1.2.2.cmml" xref="S4.p6.1.m1.1.1.2.2">Œ¶</ci><ci id="S4.p6.1.m1.1.1.2.3.cmml" xref="S4.p6.1.m1.1.1.2.3">ùëá</ci></apply><ci id="S4.p6.1.m1.1.1.3.cmml" xref="S4.p6.1.m1.1.1.3">ùêµ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p6.1.m1.1c">\Phi^{T}_{B}</annotation></semantics></math>, and ReconNet based CS reconstruction models on the open-ended questions of VQA v2.0 dataset with that of the Oracle-VQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> and tabulated them in Table¬†<a href="#S4.T2" title="Table 2 ‚Ä£ 4 Experiments ‚Ä£ CS-VQA: Visual Question Answering with Compressively Sensed Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. Their comparable performances indicate that CS-VQA is also able to effectively handle the reduced language bias in VQA v2.0 dataset.
<br class="ltx_break"></p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S4.T2.1.2.1.1" class="ltx_text" style="font-size:90%;"><span id="S4.T2.1.2.1.1.1" class="ltx_text"></span> <span id="S4.T2.1.2.1.1.2" class="ltx_text">
<span id="S4.T2.1.2.1.1.2.1" class="ltx_tabular ltx_align_middle">
<span id="S4.T2.1.2.1.1.2.1.1" class="ltx_tr">
<span id="S4.T2.1.2.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center">CS Reconstruction</span></span>
<span id="S4.T2.1.2.1.1.2.1.2" class="ltx_tr">
<span id="S4.T2.1.2.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center">Method</span></span>
</span></span> <span id="S4.T2.1.2.1.1.3" class="ltx_text"></span></span></td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="4"><span id="S4.T2.1.2.2.1" class="ltx_text" style="font-size:90%;">Question Type</span></td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.1.1" class="ltx_text" style="font-size:90%;">All</span></td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.2.1" class="ltx_text" style="font-size:90%;">Yes/No</span></td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.3.1" class="ltx_text" style="font-size:90%;">Number</span></td>
<td id="S4.T2.1.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.3.4.1" class="ltx_text" style="font-size:90%;">Other</span></td>
</tr>
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><math id="S4.T2.1.1.1.m1.1" class="ltx_Math" alttext="\Phi_{B}^{T}\Phi_{B}\mathbf{x}_{B}" display="inline"><semantics id="S4.T2.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><msubsup id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2.cmml"><mi mathsize="90%" mathvariant="normal" id="S4.T2.1.1.1.m1.1.1.2.2.2" xref="S4.T2.1.1.1.m1.1.1.2.2.2.cmml">Œ¶</mi><mi mathsize="90%" id="S4.T2.1.1.1.m1.1.1.2.2.3" xref="S4.T2.1.1.1.m1.1.1.2.2.3.cmml">B</mi><mi mathsize="90%" id="S4.T2.1.1.1.m1.1.1.2.3" xref="S4.T2.1.1.1.m1.1.1.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml"><mi mathsize="90%" mathvariant="normal" id="S4.T2.1.1.1.m1.1.1.3.2" xref="S4.T2.1.1.1.m1.1.1.3.2.cmml">Œ¶</mi><mi mathsize="90%" id="S4.T2.1.1.1.m1.1.1.3.3" xref="S4.T2.1.1.1.m1.1.1.3.3.cmml">B</mi></msub><mo lspace="0em" rspace="0em" id="S4.T2.1.1.1.m1.1.1.1a" xref="S4.T2.1.1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.T2.1.1.1.m1.1.1.4" xref="S4.T2.1.1.1.m1.1.1.4.cmml"><mi mathsize="90%" id="S4.T2.1.1.1.m1.1.1.4.2" xref="S4.T2.1.1.1.m1.1.1.4.2.cmml">ùê±</mi><mi mathsize="90%" id="S4.T2.1.1.1.m1.1.1.4.3" xref="S4.T2.1.1.1.m1.1.1.4.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><times id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1.1"></times><apply id="S4.T2.1.1.1.m1.1.1.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.2.1.cmml" xref="S4.T2.1.1.1.m1.1.1.2">superscript</csymbol><apply id="S4.T2.1.1.1.m1.1.1.2.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.2.2.1.cmml" xref="S4.T2.1.1.1.m1.1.1.2">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.2.2.2.cmml" xref="S4.T2.1.1.1.m1.1.1.2.2.2">Œ¶</ci><ci id="S4.T2.1.1.1.m1.1.1.2.2.3.cmml" xref="S4.T2.1.1.1.m1.1.1.2.2.3">ùêµ</ci></apply><ci id="S4.T2.1.1.1.m1.1.1.2.3.cmml" xref="S4.T2.1.1.1.m1.1.1.2.3">ùëá</ci></apply><apply id="S4.T2.1.1.1.m1.1.1.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.3.1.cmml" xref="S4.T2.1.1.1.m1.1.1.3">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.3.2.cmml" xref="S4.T2.1.1.1.m1.1.1.3.2">Œ¶</ci><ci id="S4.T2.1.1.1.m1.1.1.3.3.cmml" xref="S4.T2.1.1.1.m1.1.1.3.3">ùêµ</ci></apply><apply id="S4.T2.1.1.1.m1.1.1.4.cmml" xref="S4.T2.1.1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S4.T2.1.1.1.m1.1.1.4.1.cmml" xref="S4.T2.1.1.1.m1.1.1.4">subscript</csymbol><ci id="S4.T2.1.1.1.m1.1.1.4.2.cmml" xref="S4.T2.1.1.1.m1.1.1.4.2">ùê±</ci><ci id="S4.T2.1.1.1.m1.1.1.4.3.cmml" xref="S4.T2.1.1.1.m1.1.1.4.3">ùêµ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">\Phi_{B}^{T}\Phi_{B}\mathbf{x}_{B}</annotation></semantics></math></td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.1.2.1" class="ltx_text" style="font-size:90%;">48.92</span></td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.1.3.1" class="ltx_text" style="font-size:90%;">70.61</span></td>
<td id="S4.T2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.1.4.1" class="ltx_text" style="font-size:90%;">33.13</span></td>
<td id="S4.T2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.1.5.1" class="ltx_text" style="font-size:90%;">36.58</span></td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.4.1.1" class="ltx_text" style="font-size:90%;">ReconNet</span></td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.4.2.1" class="ltx_text" style="font-size:90%;">49.85</span></td>
<td id="S4.T2.1.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.4.3.1" class="ltx_text" style="font-size:90%;">70.50</span></td>
<td id="S4.T2.1.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.4.4.1" class="ltx_text" style="font-size:90%;">33.32</span></td>
<td id="S4.T2.1.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S4.T2.1.4.5.1" class="ltx_text" style="font-size:90%;">38.52</span></td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="5">
<span id="S4.T2.1.5.1.1" class="ltx_text" style="font-size:90%;">Oracle VQA¬†</span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.T2.1.5.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib27" title="" class="ltx_ref">27</a><span id="S4.T2.1.5.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite>
</td>
</tr>
<tr id="S4.T2.1.6" class="ltx_tr">
<td id="S4.T2.1.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S4.T2.1.6.1.1" class="ltx_text" style="font-size:90%;">LSTM + VGG</span></td>
<td id="S4.T2.1.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.6.2.1" class="ltx_text" style="font-size:90%;">54.22</span></td>
<td id="S4.T2.1.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.6.3.1" class="ltx_text" style="font-size:90%;">73.46</span></td>
<td id="S4.T2.1.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.6.4.1" class="ltx_text" style="font-size:90%;">35.18</span></td>
<td id="S4.T2.1.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S4.T2.1.6.5.1" class="ltx_text" style="font-size:90%;">41.83</span></td>
</tr>
<tr id="S4.T2.1.7" class="ltx_tr">
<td id="S4.T2.1.7.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S4.T2.1.7.1.1" class="ltx_text" style="font-size:90%;">Question Only</span></td>
<td id="S4.T2.1.7.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.7.2.1" class="ltx_text" style="font-size:90%;">44.26</span></td>
<td id="S4.T2.1.7.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.7.3.1" class="ltx_text" style="font-size:90%;">67.01</span></td>
<td id="S4.T2.1.7.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.7.4.1" class="ltx_text" style="font-size:90%;">31.55</span></td>
<td id="S4.T2.1.7.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S4.T2.1.7.5.1" class="ltx_text" style="font-size:90%;">27.37</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.3.1.1" class="ltx_text ltx_font_bold">Table 2</span>: </span>Open-ended VQA v2.0 results with various CS reconstructions, and their corresponding accuracy(%) </figcaption>
</figure>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.2" class="ltx_p"><span id="S4.p7.2.1" class="ltx_text ltx_font_bold">Run-time Complexity of Models:</span> The average execution times for each model to answer a question, for one image, is presented in Table¬†<a href="#S4.T3" title="Table 3 ‚Ä£ 4 Experiments ‚Ä£ CS-VQA: Visual Question Answering with Compressively Sensed Images" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. We average the results of the Caffe ‚Äútime‚Äù command over 5 runs. The command uses random weights for measuring the time, and each computation time obtained for each of the 5 runs is itself the average over 100 iterations of forward pass through the network. All the numbers except for <math id="S4.p7.1.m1.1" class="ltx_Math" alttext="\Phi^{T}" display="inline"><semantics id="S4.p7.1.m1.1a"><msup id="S4.p7.1.m1.1.1" xref="S4.p7.1.m1.1.1.cmml"><mi mathvariant="normal" id="S4.p7.1.m1.1.1.2" xref="S4.p7.1.m1.1.1.2.cmml">Œ¶</mi><mi id="S4.p7.1.m1.1.1.3" xref="S4.p7.1.m1.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S4.p7.1.m1.1b"><apply id="S4.p7.1.m1.1.1.cmml" xref="S4.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p7.1.m1.1.1.1.cmml" xref="S4.p7.1.m1.1.1">superscript</csymbol><ci id="S4.p7.1.m1.1.1.2.cmml" xref="S4.p7.1.m1.1.1.2">Œ¶</ci><ci id="S4.p7.1.m1.1.1.3.cmml" xref="S4.p7.1.m1.1.1.3">ùëá</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.1.m1.1c">\Phi^{T}</annotation></semantics></math> are obtained using Caffe on Titan X GPU, with <math id="S4.p7.2.m2.1" class="ltx_Math" alttext="\Phi^{T}" display="inline"><semantics id="S4.p7.2.m2.1a"><msup id="S4.p7.2.m2.1.1" xref="S4.p7.2.m2.1.1.cmml"><mi mathvariant="normal" id="S4.p7.2.m2.1.1.2" xref="S4.p7.2.m2.1.1.2.cmml">Œ¶</mi><mi id="S4.p7.2.m2.1.1.3" xref="S4.p7.2.m2.1.1.3.cmml">T</mi></msup><annotation-xml encoding="MathML-Content" id="S4.p7.2.m2.1b"><apply id="S4.p7.2.m2.1.1.cmml" xref="S4.p7.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p7.2.m2.1.1.1.cmml" xref="S4.p7.2.m2.1.1">superscript</csymbol><ci id="S4.p7.2.m2.1.1.2.cmml" xref="S4.p7.2.m2.1.1.2">Œ¶</ci><ci id="S4.p7.2.m2.1.1.3.cmml" xref="S4.p7.2.m2.1.1.3">ùëá</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p7.2.m2.1c">\Phi^{T}</annotation></semantics></math> executed on a CPU with Matlab due to space considerations (too large to fit on the GPU). We can see from the table that all three methods are considerably faster than a traditional iterative CS solver, but ReconNet gives the best VQA performance with relatively fast execution time.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T3.2.2.3" class="ltx_tr">
<td id="S4.T3.2.2.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Reconstruction Method</td>
<td id="S4.T3.2.2.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Time (ms)</td>
</tr>
<tr id="S4.T3.1.1.1" class="ltx_tr">
<td id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><math id="S4.T3.1.1.1.1.m1.1" class="ltx_Math" alttext="\Phi^{T}\Phi\mathbf{x}" display="inline"><semantics id="S4.T3.1.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.1.m1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.cmml"><msup id="S4.T3.1.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S4.T3.1.1.1.1.m1.1.1.2.2" xref="S4.T3.1.1.1.1.m1.1.1.2.2.cmml">Œ¶</mi><mi id="S4.T3.1.1.1.1.m1.1.1.2.3" xref="S4.T3.1.1.1.1.m1.1.1.2.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S4.T3.1.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.1.m1.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="S4.T3.1.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.1.m1.1.1.3.cmml">Œ¶</mi><mo lspace="0em" rspace="0em" id="S4.T3.1.1.1.1.m1.1.1.1a" xref="S4.T3.1.1.1.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.T3.1.1.1.1.m1.1.1.4" xref="S4.T3.1.1.1.1.m1.1.1.4.cmml">ùê±</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1"><times id="S4.T3.1.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1.1"></times><apply id="S4.T3.1.1.1.1.m1.1.1.2.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T3.1.1.1.1.m1.1.1.2.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2">superscript</csymbol><ci id="S4.T3.1.1.1.1.m1.1.1.2.2.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2.2">Œ¶</ci><ci id="S4.T3.1.1.1.1.m1.1.1.2.3.cmml" xref="S4.T3.1.1.1.1.m1.1.1.2.3">ùëá</ci></apply><ci id="S4.T3.1.1.1.1.m1.1.1.3.cmml" xref="S4.T3.1.1.1.1.m1.1.1.3">Œ¶</ci><ci id="S4.T3.1.1.1.1.m1.1.1.4.cmml" xref="S4.T3.1.1.1.1.m1.1.1.4">ùê±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\Phi^{T}\Phi\mathbf{x}</annotation></semantics></math></td>
<td id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">29.36</td>
</tr>
<tr id="S4.T3.2.2.2" class="ltx_tr">
<td id="S4.T3.2.2.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Block-based <math id="S4.T3.2.2.2.1.m1.1" class="ltx_Math" alttext="\Phi^{T}_{B}\Phi_{B}\mathbf{x}" display="inline"><semantics id="S4.T3.2.2.2.1.m1.1a"><mrow id="S4.T3.2.2.2.1.m1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.cmml"><msubsup id="S4.T3.2.2.2.1.m1.1.1.2" xref="S4.T3.2.2.2.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S4.T3.2.2.2.1.m1.1.1.2.2.2" xref="S4.T3.2.2.2.1.m1.1.1.2.2.2.cmml">Œ¶</mi><mi id="S4.T3.2.2.2.1.m1.1.1.2.3" xref="S4.T3.2.2.2.1.m1.1.1.2.3.cmml">B</mi><mi id="S4.T3.2.2.2.1.m1.1.1.2.2.3" xref="S4.T3.2.2.2.1.m1.1.1.2.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.T3.2.2.2.1.m1.1.1.1" xref="S4.T3.2.2.2.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.T3.2.2.2.1.m1.1.1.3" xref="S4.T3.2.2.2.1.m1.1.1.3.cmml"><mi mathvariant="normal" id="S4.T3.2.2.2.1.m1.1.1.3.2" xref="S4.T3.2.2.2.1.m1.1.1.3.2.cmml">Œ¶</mi><mi id="S4.T3.2.2.2.1.m1.1.1.3.3" xref="S4.T3.2.2.2.1.m1.1.1.3.3.cmml">B</mi></msub><mo lspace="0em" rspace="0em" id="S4.T3.2.2.2.1.m1.1.1.1a" xref="S4.T3.2.2.2.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.T3.2.2.2.1.m1.1.1.4" xref="S4.T3.2.2.2.1.m1.1.1.4.cmml">ùê±</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.1.m1.1b"><apply id="S4.T3.2.2.2.1.m1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1"><times id="S4.T3.2.2.2.1.m1.1.1.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.1"></times><apply id="S4.T3.2.2.2.1.m1.1.1.2.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T3.2.2.2.1.m1.1.1.2.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2">subscript</csymbol><apply id="S4.T3.2.2.2.1.m1.1.1.2.2.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.T3.2.2.2.1.m1.1.1.2.2.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2">superscript</csymbol><ci id="S4.T3.2.2.2.1.m1.1.1.2.2.2.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2.2.2">Œ¶</ci><ci id="S4.T3.2.2.2.1.m1.1.1.2.2.3.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2.2.3">ùëá</ci></apply><ci id="S4.T3.2.2.2.1.m1.1.1.2.3.cmml" xref="S4.T3.2.2.2.1.m1.1.1.2.3">ùêµ</ci></apply><apply id="S4.T3.2.2.2.1.m1.1.1.3.cmml" xref="S4.T3.2.2.2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.T3.2.2.2.1.m1.1.1.3.1.cmml" xref="S4.T3.2.2.2.1.m1.1.1.3">subscript</csymbol><ci id="S4.T3.2.2.2.1.m1.1.1.3.2.cmml" xref="S4.T3.2.2.2.1.m1.1.1.3.2">Œ¶</ci><ci id="S4.T3.2.2.2.1.m1.1.1.3.3.cmml" xref="S4.T3.2.2.2.1.m1.1.1.3.3">ùêµ</ci></apply><ci id="S4.T3.2.2.2.1.m1.1.1.4.cmml" xref="S4.T3.2.2.2.1.m1.1.1.4">ùê±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.1.m1.1c">\Phi^{T}_{B}\Phi_{B}\mathbf{x}</annotation></semantics></math>
</td>
<td id="S4.T3.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">20.78</td>
</tr>
<tr id="S4.T3.2.2.4" class="ltx_tr">
<td id="S4.T3.2.2.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">ReconNet</td>
<td id="S4.T3.2.2.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">27.99</td>
</tr>
<tr id="S4.T3.2.2.5" class="ltx_tr">
<td id="S4.T3.2.2.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">TVAL3 (from¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>)</td>
<td id="S4.T3.2.2.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2963.00</td>
</tr>
</table>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.4.1.1" class="ltx_text ltx_font_bold">Table 3</span>: </span>Average execution time per image to generate answers, for various models.</figcaption>
</figure>
<figure id="S4.F2" class="ltx_figure"><img src="/html/1806.03379/assets/barchart2.eps" id="S4.F2.g1" class="ltx_graphics ltx_centering" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.F2.2.1.1" class="ltx_text ltx_font_bold">Fig.¬†2</span>: </span>Comparison of CS-VQA to the Oracle VQA¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> for different types of questions.</figcaption>
</figure>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.2" class="ltx_p">In addition to execution speed benefits, we also compare the memory requirements in terms of the number of parameters in each model. Using <math id="S4.p8.1.m1.1" class="ltx_Math" alttext="\Phi_{B}^{T}\Phi_{B}\mathbf{x}" display="inline"><semantics id="S4.p8.1.m1.1a"><mrow id="S4.p8.1.m1.1.1" xref="S4.p8.1.m1.1.1.cmml"><msubsup id="S4.p8.1.m1.1.1.2" xref="S4.p8.1.m1.1.1.2.cmml"><mi mathvariant="normal" id="S4.p8.1.m1.1.1.2.2.2" xref="S4.p8.1.m1.1.1.2.2.2.cmml">Œ¶</mi><mi id="S4.p8.1.m1.1.1.2.2.3" xref="S4.p8.1.m1.1.1.2.2.3.cmml">B</mi><mi id="S4.p8.1.m1.1.1.2.3" xref="S4.p8.1.m1.1.1.2.3.cmml">T</mi></msubsup><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1" xref="S4.p8.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S4.p8.1.m1.1.1.3" xref="S4.p8.1.m1.1.1.3.cmml"><mi mathvariant="normal" id="S4.p8.1.m1.1.1.3.2" xref="S4.p8.1.m1.1.1.3.2.cmml">Œ¶</mi><mi id="S4.p8.1.m1.1.1.3.3" xref="S4.p8.1.m1.1.1.3.3.cmml">B</mi></msub><mo lspace="0em" rspace="0em" id="S4.p8.1.m1.1.1.1a" xref="S4.p8.1.m1.1.1.1.cmml">‚Äã</mo><mi id="S4.p8.1.m1.1.1.4" xref="S4.p8.1.m1.1.1.4.cmml">ùê±</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.p8.1.m1.1b"><apply id="S4.p8.1.m1.1.1.cmml" xref="S4.p8.1.m1.1.1"><times id="S4.p8.1.m1.1.1.1.cmml" xref="S4.p8.1.m1.1.1.1"></times><apply id="S4.p8.1.m1.1.1.2.cmml" xref="S4.p8.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p8.1.m1.1.1.2.1.cmml" xref="S4.p8.1.m1.1.1.2">superscript</csymbol><apply id="S4.p8.1.m1.1.1.2.2.cmml" xref="S4.p8.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.p8.1.m1.1.1.2.2.1.cmml" xref="S4.p8.1.m1.1.1.2">subscript</csymbol><ci id="S4.p8.1.m1.1.1.2.2.2.cmml" xref="S4.p8.1.m1.1.1.2.2.2">Œ¶</ci><ci id="S4.p8.1.m1.1.1.2.2.3.cmml" xref="S4.p8.1.m1.1.1.2.2.3">ùêµ</ci></apply><ci id="S4.p8.1.m1.1.1.2.3.cmml" xref="S4.p8.1.m1.1.1.2.3">ùëá</ci></apply><apply id="S4.p8.1.m1.1.1.3.cmml" xref="S4.p8.1.m1.1.1.3"><csymbol cd="ambiguous" id="S4.p8.1.m1.1.1.3.1.cmml" xref="S4.p8.1.m1.1.1.3">subscript</csymbol><ci id="S4.p8.1.m1.1.1.3.2.cmml" xref="S4.p8.1.m1.1.1.3.2">Œ¶</ci><ci id="S4.p8.1.m1.1.1.3.3.cmml" xref="S4.p8.1.m1.1.1.3.3">ùêµ</ci></apply><ci id="S4.p8.1.m1.1.1.4.cmml" xref="S4.p8.1.m1.1.1.4">ùê±</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.1.m1.1c">\Phi_{B}^{T}\Phi_{B}\mathbf{x}</annotation></semantics></math> (at MR = 0.25) with GoogleNet and the LSTM (including the fusion layers) results in 12,610,768 parameters. Using ReconNet (at MR = 0.25) along with the same back-end of GoogleNet and LSTM results in 12,633,488 parameters. This is only a slight increase for an improvement of 1-2% points on the CS-VQA task, and an extra <math id="S4.p8.2.m2.1" class="ltx_Math" alttext="8" display="inline"><semantics id="S4.p8.2.m2.1a"><mn id="S4.p8.2.m2.1.1" xref="S4.p8.2.m2.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S4.p8.2.m2.1b"><cn type="integer" id="S4.p8.2.m2.1.1.cmml" xref="S4.p8.2.m2.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p8.2.m2.1c">8</annotation></semantics></math>ms of processing time. Using raw multiplexed measurements requires only 6,644,496 parameters.

<br class="ltx_break">
<br class="ltx_break"></p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In summary, we have presented the first study of the effectiveness of VQA on compressively sensed images. In particular, we show that VQA can achieve near-equivalent performance to natural images when using advanced compressive sensing (CS) reconstruction techniques such as ReconNet with a performance gap of only 3% points at measurement rate MR = 0.25, and 6% gap at MR = 0.01. Using direct inference approaches, we report reduced processing time over approaches that need full reconstruction, and reduced network parameters. Of course, using a full-reconstruction approach results in the best performance. We believe this work opens up a new avenue of research into VQA for derived or intermediate representations of visual data which are amenable to system considerations such as energy-efficiency and limited bandwidth for mobile and embedded AI platforms.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C¬†Lawrence¬†Zitnick, and Devi Parikh,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">‚ÄúVQA: Visual question answering,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE International Conference on Computer
Vision</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, 2015, pp. 2425‚Äì2433.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Robert LiKamWa, Zhen Wang, Aaron Carroll, Felix¬†Xiaozhu Lin, and Lin Zhong,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">‚ÄúDraining our glass: An energy and heat characterization of google
glass,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of 5th Asia-Pacific Workshop on Systems</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">. ACM,
2014, p.¬†10.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Marco¬†F Duarte, Mark¬†A Davenport, Dharmpal Takbar, Jason¬†N Laska, Ting Sun,
Kevin¬†F Kelly, and Richard¬†G Baraniuk,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">‚ÄúSingle-pixel imaging via compressive sampling,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Magazine</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, vol. 25, no. 2, pp. 83‚Äì91,
2008.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
David¬†L Donoho,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">‚ÄúCompressed sensing,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, vol. 52, no. 4, pp.
1289‚Äì1306, 2006.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Emmanuel¬†J Cand√®s, Justin Romberg, and Terence Tao,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">‚ÄúRobust uncertainty principles: Exact signal reconstruction from
highly incomplete frequency information,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</span><span id="bib.bib5.4.2" class="ltx_text" style="font-size:90%;">, vol. 52, no. 2, pp.
489‚Äì509, 2006.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Mark¬†A. Davenport, Marco¬†F. Duarte, Michael¬†B. Wakin, Jason¬†N. Laska, Dharmpal
Takhar, Kevin¬†F. Kelly, and Richard¬†G. Baraniuk,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">‚ÄúThe smashed filter for compressive classification and target
recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computational Imaging V, San Jose, CA, USA, January 29-31,
2007</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2007, p. 64980H.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Kuldeep Kulkarni and Pavan Turaga,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">‚ÄúReconstruction-free action inference from compressive imagers,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span id="bib.bib7.4.2" class="ltx_text" style="font-size:90%;">,
vol. 38, no. 4, pp. 772‚Äì784, 2016.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Emmanuel¬†J Candes and Terence Tao,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">‚ÄúNear-optimal signal recovery from random projections: Universal
encoding strategies?,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, vol. 52, no. 12, pp.
5406‚Äì5425, 2006.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Richard¬†G Baraniuk, Volkan Cevher, Marco¬†F Duarte, and Chinmay Hegde,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">‚ÄúModel-based compressive sensing,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Information Theory</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, vol. 56, no. 4, pp.
1982‚Äì2001, 2010.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Yookyung Kim, Mariappan¬†S Nadar, and Ali Bilgin,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">‚ÄúCompressed sensing using a Gaussian scale mixtures model in
wavelet domain,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Image Processing</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2010,
pp. 3365‚Äì3368.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
David¬†L Donoho, Arian Maleki, and Andrea Montanari,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">‚ÄúMessage-passing algorithms for compressed sensing,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the National Academy of Sciences</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, vol. 106, no.
45, pp. 18914‚Äì18919, 2009.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Ali Mousavi, Ankit¬†B Patel, and Richard¬†G Baraniuk,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">‚ÄúA deep learning approach to structured signal recovery,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib12.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">53rd Annual Allerton Conference on Communication, Control,
and Computing</span><span id="bib.bib12.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2015, pp. 1336‚Äì1343.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Michael Iliadis, Leonidas Spinoulas, and Aggelos¬†K Katsaggelos,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">‚ÄúDeep fully-connected networks for video compressive sensing,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Digital Signal Processing</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, vol. 72, pp. 9‚Äì18, 2018.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Kuldeep Kulkarni, Suhas Lohit, Pavan Turaga, Ronan Kerviche, and Amit Ashok,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">‚ÄúReconNet: Non-iterative reconstruction of images from
compressively sensed measurements,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib14.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib14.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 449‚Äì458.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Suhas Lohit, Kuldeep Kulkarni, and Pavan Turaga,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">‚ÄúDirect inference on compressive measurements using convolutional
neural networks,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib15.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Image Processing</span><span id="bib.bib15.5.3" class="ltx_text" style="font-size:90%;">. IEEE, 2016,
pp. 1913‚Äì1917.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Kuldeep Kulkarni and Pavan Turaga,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">‚ÄúFast integral image estimation at 1% measurement rate,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1601.07258</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">‚ÄúGoing deeper with convolutions,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2015, pp. 1‚Äì9.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">‚ÄúDeep residual learning for image recognition,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 770‚Äì778.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg¬†S Corrado, and Jeff Dean,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">‚ÄúDistributed representations of words and phrases and their
compositionality,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, 2013, pp.
3111‚Äì3119.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Sepp Hochreiter and J√ºrgen Schmidhuber,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text" style="font-size:90%;">‚ÄúLong short-term memory,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Neural Computation</span><span id="bib.bib20.4.2" class="ltx_text" style="font-size:90%;">, vol. 9, no. 8, pp. 1735‚Äì1780, 1997.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Huijuan Xu and Kate Saenko,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">‚ÄúAsk, attend and answer: Exploring question-guided spatial attention
for visual question answering,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib21.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib21.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2016, pp.
451‚Äì466.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Zichao Yang, Xiaodong He, Jianfeng Gao, Li¬†Deng, and Alex Smola,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">‚ÄúStacked attention networks for image question answering,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib22.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib22.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 21‚Äì29.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">‚ÄúHierarchical question-image co-attention for visual question
answering,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib23.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances In Neural Information Processing Systems</span><span id="bib.bib23.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp.
289‚Äì297.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Akira Fukui, Dong¬†Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and
Marcus Rohrbach,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">‚ÄúMultimodal compact bilinear pooling for visual question answering
and visual grounding,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2016 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2016, Austin, Texas, USA, November 1-4,
2016</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, 2016, pp. 457‚Äì468.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et¬†al.,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text" style="font-size:90%;">‚ÄúImagenet large scale visual recognition challenge,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Journal of Computer Vision</span><span id="bib.bib25.4.2" class="ltx_text" style="font-size:90%;">, vol. 115, no. 3, pp.
211‚Äì252, 2015.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Doll√°r, and C¬†Lawrence Zitnick,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">‚ÄúMicrosoft COCO: Common objects in context,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib26.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">European Conference on Computer Vision</span><span id="bib.bib26.5.3" class="ltx_text" style="font-size:90%;">. Springer, 2014, pp.
740‚Äì755.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh,
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">‚ÄúMaking the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering,‚Äù
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text" style="font-size:90%;">in </span><span id="bib.bib27.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition
(CVPR)</span><span id="bib.bib27.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/1806.03378" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/1806.03379" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+1806.03379">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/1806.03379" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/1806.03380" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sat Mar 16 04:16:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
