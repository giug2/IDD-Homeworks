<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?</title>
<!--Generated on Tue Sep  3 12:58:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.01864v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S1" title="In The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S2" title="In The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Pilot-survey: LLMs in musicology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S3" title="In The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Musicology Benchmark: TrustMus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S4" title="In The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S4.SS1" title="In 4 Results and Discussion ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Human validation insights</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S4.SS2" title="In 4 Results and Discussion ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>TrustMus evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S5" title="In The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">The Role of Large Language Models in Musicology: 
<br class="ltx_break"/>Are We Ready to Trust the Machines?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Pedro Ramoneda<sup class="ltx_sup" id="id7.7.id1"><span class="ltx_text ltx_font_italic" id="id7.7.id1.1">1</span></sup>   
Emilia Parada-Cabaleiro<sup class="ltx_sup" id="id8.8.id2">2</sup>   
Benno Weck<sup class="ltx_sup" id="id9.9.id3">1</sup>   
Xavier Serra<sup class="ltx_sup" id="id10.10.id4">1</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id11.11.id5">1</sup>Music Technology Group, Universitat Pompeu Fabra, Barcelona, Spain 
<br class="ltx_break"/><sup class="ltx_sup" id="id12.12.id6">2</sup>Department of Music Pedagogy, Nuremberg University of Music, Germany 
<br class="ltx_break"/>
</span><span class="ltx_author_notes">   Corresponding author: pedro.ramoneda@upf.edu</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id13.id1">In this work, we explore the use and reliability of Large Language Models (LLMs) in musicology.
From a discussion with experts and students, we assess
the current acceptance and concerns regarding this, nowadays ubiquitous, technology.
We aim to go one step further, proposing a semi-automatic method to create an initial benchmark using retrieval-augmented generation models and multiple-choice question generation, validated by human experts.
Our evaluation on 400 human-validated questions shows that current vanilla LLMs are less reliable than retrieval augmented generation from music dictionaries.
This paper suggests that the potential of LLMs in musicology requires musicology driven research that can specialized LLMs by including accurate and reliable domain knowledge.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years,
research
on Large Language Models (LLMs) has led to
notable advancements within the text generation domain <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib30" title="">2022a</a>); Minaee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib22" title="">2024</a>)</cite>. This is the result of training large models on vast non-domain-specific data
 <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib10" title="">2020</a>); Hoffmann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib13" title="">2022</a>)</cite>. Well-known families of models include Llama <cite class="ltx_cite ltx_citemacro_cite">AI@Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib3" title="">2024</a>)</cite> or GPT <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib2" title="">2023</a>)</cite>, which can generate coherent and
contextually relevant text, making them valuable tools in numerous applications
and professions such as healthcare <cite class="ltx_cite ltx_citemacro_cite">Thirunavukarasu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib28" title="">2023</a>)</cite>, journalism <cite class="ltx_cite ltx_citemacro_cite">Petridis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib23" title="">2023</a>)</cite>, customer support <cite class="ltx_cite ltx_citemacro_cite">Kolasani (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib17" title="">2023</a>)</cite> or education <cite class="ltx_cite ltx_citemacro_cite">Kasneci et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib16" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite their potential,
LLMs’ so-called
hallucinations <cite class="ltx_cite ltx_citemacro_cite">Alkaissi and McFarlane (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib4" title="">2023</a>)</cite>, i. e., the lack of confidence and accuracy in the text they generate, prevents the use of this technology
in most arts and humanities research tasks <cite class="ltx_cite ltx_citemacro_cite">Rane (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib24" title="">2023</a>); Lozić and Štular (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib21" title="">2023</a>); Rane and Choudhary (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib25" title="">2024</a>)</cite>.
Issues include a lack of contextual understanding, bias perpetuation <cite class="ltx_cite ltx_citemacro_cite">Gallegos et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib9" title="">2024</a>)</cite>, and ethical concerns such as generating misleading content <cite class="ltx_cite ltx_citemacro_cite">Weidinger et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib32" title="">2021</a>)</cite>.
The lack of credible source attribution <cite class="ltx_cite ltx_citemacro_cite">Rashkin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib26" title="">2023</a>)</cite> almost render them nugatory for fields like literature, history <cite class="ltx_cite ltx_citemacro_cite">Walters and Wilder (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib29" title="">2023</a>)</cite>, and law <cite class="ltx_cite ltx_citemacro_cite">Weiser (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib33" title="">2024</a>)</cite>.
However, LLMs can aid research through a variety of tasks, such as, translation, text analysis, data organization, historical context retrieval, or summarization.
In this regard,
interdisciplinary research involving the use and further development of LLMs within
the humanities should be carried out.
This will enable to constructively address existing risks and concerns while developing
LLMs’ full potential, by this delivering their benefits across disciplines.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.F1.1" style="width:182.1pt;height:88.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.7pt,14.0pt) scale(0.760013809669711,0.760013809669711) ;"><svg class="ltx_picture" height="160.96" id="S1.F1.1.pic1" overflow="visible" version="1.1" width="317.69"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,160.96) matrix(1 0 0 -1 0 0) translate(158.84,0) translate(0,75.08)"><path d="M -157.48 24.63 h 314.96 v 60.97 h -314.96 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -116.1 29.24)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 26.845)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 51.75)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject height="49.81" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="232.19">
<table class="ltx_tabular ltx_align_middle" id="S1.F1.1.pic1.1.1.1.1.1.1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.F1.1.pic1.1.1.1.1.1.1.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S1.F1.1.pic1.1.1.1.1.1.1.1.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S1.F1.1.pic1.1.1.1.1.1.1.1.1.1.1.1.1">Musicologist:</span> What’s the historical</td>
</tr>
<tr class="ltx_tr" id="S1.F1.1.pic1.1.1.1.1.1.1.1.1.2.2">
<td class="ltx_td ltx_align_center" id="S1.F1.1.pic1.1.1.1.1.1.1.1.1.2.2.1">context of this music piece?</td>
</tr>
</tbody>
</table></foreignobject></g></g></g></g><path d="M -158.57 -19.69 h 317.14 v 39.37 h -317.14 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -153.96 -12.45)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 12.455)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 24.91)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject height="24.91" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="307.91">
<table class="ltx_tabular ltx_align_middle" id="S1.F1.1.pic1.2.2.2.1.1.1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.F1.1.pic1.2.2.2.1.1.1.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S1.F1.1.pic1.2.2.2.1.1.1.1.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S1.F1.1.pic1.2.2.2.1.1.1.1.1.1.1.1.1">LLM:</span> It’s by Beethoven in 2025! Aliens helped!</td>
</tr>
</tbody>
</table></foreignobject></g></g></g></g><path d="M -157.48 -74.8 h 314.96 v 39.37 h -314.96 Z" style="fill:none"></path><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -141.81 -67.57)"><g class="ltx_tikzmatrix" transform="matrix(1 0 0 -1 0 12.455)"><g class="ltx_tikzmatrix_row" transform="matrix(1 0 0 1 0 24.91)"><g class="ltx_tikzmatrix_col ltx_nopad_l ltx_nopad_r" transform="matrix(1 0 0 -1 0 0)"><foreignobject height="24.91" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="283.62">
<table class="ltx_tabular ltx_align_middle" id="S1.F1.1.pic1.3.3.3.1.1.1.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S1.F1.1.pic1.3.3.3.1.1.1.1.1.1.1">
<td class="ltx_td ltx_align_center" id="S1.F1.1.pic1.3.3.3.1.1.1.1.1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S1.F1.1.pic1.3.3.3.1.1.1.1.1.1.1.1.1">Musicologist:</span> I’m not using THIS anymore.</td>
</tr>
</tbody>
</table></foreignobject></g></g></g></g></g></svg>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Fictitious interaction illustrating why LLMs’ hallucinations might prevent musicologists’ trust.</figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we focus on musicology,
a field where the impact of LLMs still needs to be explored.
Musicology, the scholarly study of music, spans from historical research to theoretical analysis <cite class="ltx_cite ltx_citemacro_cite">Harap (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib12" title="">1937</a>); Duckles et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib8" title="">2020</a>)</cite>.
Our research mainly focuses on the former,
an area which might be greatly supported by LLMs, e. g., by breaking language barriers, enhancing information retrieval, or supporting teaching and learning.
However, reliable sources, such as
music-specialized lexica, monographies, and research articles,
are often, unlike in more technical disciplines, not open-access, which prevents LLMs to access high quality information.
This knowledge deprivation further increases the risk of LLMs to hallucinate, which often leads to non-reliable text generation in musicology related topics.
</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Through a pilot-survey involving experts and students from the field of musicology, we gather initial insights into the acceptance and trustworthiness of
LLMs in domain-related tasks, and its potential impact for music professionals. Subsequently, we propose a methodology to measure to which extent such models posses domain expertise in the field of musicology, by this assessing their practical value for the discipline.
We adopt a Multiple-Choice Question Generation <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib20" title="">2024</a>)</cite> approach to semi-automatically construct a benchmark leveraging recent advancements in retrieval-augmented generation models <cite class="ltx_cite ltx_citemacro_cite">Lewis et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib18" title="">2020</a>)</cite>.
To automatically generate high-quality questions, we provide the generation model with domain-knowledge from <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">The New Grove Dictionary of Music and Musicians</span> <cite class="ltx_cite ltx_citemacro_cite">Sadie and Tyrrell (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib27" title="">2001</a>)</cite>, an established and reliable source.
The final benchmark, made up by 400 question-answer pairs validated by a human expert, is evaluated on several open-source models.
This dual approach—survey and benchmark—provides a comprehensive understanding of the challenges and potential solutions for meaningful integration of LLMs in musicology.
</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Pilot-survey: LLMs in musicology</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">We conducted a survey targeting professionals related to musicology. The survey included questions to identify the respondent’s domain of study (e.g., musicology, composition, music pedagogy, music performance), the highest level of music education completed or being pursued, and their familiarity with technologies known as LLMs such as ChatGPT. Additionally, the survey inquired about the frequency of interactions with LLMs, particularly in the context of musical topics like Music Theory and Music History. Participants were asked to rate the trustworthiness and usefulness of LLMs for these subjects, as well as to consider its revolutionary impact on the field of musicology. Lastly, the survey explored the possible consequences of LLMs on music professionals, both presently and in the future.
</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="456" id="S2.F2.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Survey’s answers about the usage of LLMs in general (left) and on music topics (right)</figcaption>
</figure>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">A total of 33 participants, having or pursuing a Bachelor’s degree in music, completed the survey:
20 students, 7 lecturers, 11 researchers, and 8 music educators (multiple areas can be selected). In terms of discipline, the respondents are distributed as the following:
22 Musicology and Related Studies, 10 Music Performance, 3 Music Pedagogy, 2 Composition, 1 Conduction, and 1 Music Therapy.
While only one participant (from the field of musicology) had not heard about LLMs before, in terms of the participants’ frequency of use
and trustworthiness, a noticeable gap between students and teachers<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For simplicity, with ‘teachers’ we refer to all the participants who did not identified themselves as student.</span></span></span> can be observed.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S2.F2" title="Figure 2 ‣ 2 Pilot-survey: LLMs in musicology ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_tag">2</span></a> illustrates how often students and teachers interact with LLMs in general and about music topics.
Teachers frequently or not at all, while most students use it weekly or monthly.
However, both groups tend to not use LLMs for music-related topics.
Participants’ judgement of LLMs’ trustworthiness is depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S2.F3" title="Figure 3 ‣ 2 Pilot-survey: LLMs in musicology ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_tag">3</span></a> and the trend of ratings is similar across both groups.
Additionally, confidence in LLMs is slightly higher for Music History than for Music Theory, indicating a nuanced perception of their reliability in different musicology subfields.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Most of the participants
(78%)
agreed that LLMs might revolutionize the field of musicology (cf. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S2.F4" title="Figure 4 ‣ 2 Pilot-survey: LLMs in musicology ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_tag">4</span></a>, left).
While the anticipated potential consequences of LLMs for the field are varied,
professional transformation seems to be
the most prominent (20 votes), as illustrated in the histogram (cf. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S2.F4" title="Figure 4 ‣ 2 Pilot-survey: LLMs in musicology ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_tag">4</span></a>, right).
In conclusion, despite limited current usage and trust, experts anticipate a significant future impact of LLMs on musicology, motivating current research on the topic.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="415" id="S2.F3.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Survey’s answers about the usage of LLMs in general (left) and on music topics (right).</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="312" id="S2.F4.g1" src="x3.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Survey answers about the revolutionary impact (right) and potential consequences (left) of LLMs.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Musicology Benchmark: TrustMus</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">This section outlines our strategy for evaluating how much LLMs
hallucinate in musicology. It summarizes the creation of the human-validated multiple-choice benchmark <span class="ltx_text ltx_font_italic" id="S3.p1.1.1">TrustMus</span>, i. e., a collection of reliable questions related to various musical topics and concepts, and analyzes the models’ performance on the benchmark.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Following previous works <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib19" title="">2023</a>)</cite>, multiple-choice questions are generated after extracting relevant information from a text source: here,
<span class="ltx_text ltx_font_italic" id="S3.p2.1.1">The Grove Dictionary Online</span> <cite class="ltx_cite ltx_citemacro_cite">Sadie and Tyrrell (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib27" title="">2001</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The Grove Dictionary is a copyrighted work. Using its content for generating questions is under fair use for research purposes. The EU Directive on Copyright in the Digital Single Market allows text and data mining for research purposes.</span></span></span> In order to identify the most relevant articles within the text source, we used a PageRank-like algorithm <cite class="ltx_cite ltx_citemacro_cite">Hagberg et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib11" title="">2008</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">To accelerate the creation of TrustMus, we designed a workflow inspired by recent works <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib35" title="">2024</a>); Jeong et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib14" title="">2024</a>); Asai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib5" title="">2023</a>); Dhuliawala et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib7" title="">2023</a>)</cite>, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S3.F5" title="Figure 5 ‣ 3 Musicology Benchmark: TrustMus ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_tag">5</span></a>.
First, we generated five questions from each article, each with four possible answer options, using a fine-tuned LLM for retrieval-augmented generation (RAG) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib20" title="">2024</a>)</cite>, resulting in 7 500 questions.
Second, we discard questions that did not have relation with musicology or a unique and unambiguous answer, by prompting the same LLM to decide based on the article, eliminating 2 632 questions.
Next, we attempted to answer the remaining questions using a RAG-like model that we term <span class="ltx_text ltx_font_italic" id="S3.p3.1.1">Llama Professor</span> by giving the article as context to the LLM.
Questions for which Llama Professor chooses the wrong answer option are considered ambiguous or unusable and are thus removed, resulting in 3 285 valid questions.
All previous prompts used the Chain of Thought (CoT) method to enhance the model’s reasoning skills <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib31" title="">2022b</a>)</cite>.
Before human intervention, we attempted to answer the questions with llama3-8B <cite class="ltx_cite ltx_citemacro_cite">AI@Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib3" title="">2024</a>)</cite> without RAG and in one shot, i. e., without the chain of thought (cf. the difficulty filter in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S3.F5" title="Figure 5 ‣ 3 Musicology Benchmark: TrustMus ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_tag">5</span></a>), which lead an accuracy of 67.4%.
Thus, arguably simple questions are eliminated, resulting in 1 081 domain-ones.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="695" id="S3.F5.g1" src="x4.png" width="705"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Language chain for generating the multi-choice questions.</figcaption>
</figure>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The resulting set of questions was automatically classified with a CoT prompt into four classes, according to their topic: People (Ppl); Instruments and Technology (I&amp;T); Genres, Forms, and Theory (Thr); Culture and history (C&amp;H).
An expert human
annotator validated questions until 100 valid ones per class were identified (on average, 17% of those assessed were discarded).<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>@: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zenodo.org/records/13644330" title="">https://zenodo.org/records/13644330</a></span></span></span></p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results and Discussion</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Human validation insights</h3>
<figure class="ltx_table" id="S4.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Quant</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.3.1">TrustMus</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.4.1">Rank</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.5.1">Ppl</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.6" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.6.1">I&amp;T</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.7" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.7.1">Thr</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.8" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.8.1">C&amp;H</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.9" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.9.1">LB</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.10" style="padding-left:4.0pt;padding-right:4.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.10.1">Rank</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">gpt-4o-2024-05-13 <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib2" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">API</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">58.75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.6" style="padding-left:4.0pt;padding-right:4.0pt;">44.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.7" style="padding-left:4.0pt;padding-right:4.0pt;">61.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.8" style="padding-left:4.0pt;padding-right:4.0pt;">70.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.9" style="padding-left:4.0pt;padding-right:4.0pt;">58.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.10" style="padding-left:4.0pt;padding-right:4.0pt;">1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.1" style="padding-left:4.0pt;padding-right:4.0pt;">mixtral-8x7b-instruct-v0.1 <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib15" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3" style="padding-left:4.0pt;padding-right:4.0pt;">40.5<span class="ltx_text ltx_phantom" id="S4.T1.1.3.2.3.1"><span style="visibility:hidden">0</span></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">2</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">41.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.6" style="padding-left:4.0pt;padding-right:4.0pt;">30.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.7" style="padding-left:4.0pt;padding-right:4.0pt;">43.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.8" style="padding-left:4.0pt;padding-right:4.0pt;">48.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.9" style="padding-left:4.0pt;padding-right:4.0pt;">37.24</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.10" style="padding-left:4.0pt;padding-right:4.0pt;">4</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.1" style="padding-left:4.0pt;padding-right:4.0pt;">gpt-3.5-turbo-0125 <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib2" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2" style="padding-left:4.0pt;padding-right:4.0pt;">API</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">39.75</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">3</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">39.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">25.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.7" style="padding-left:4.0pt;padding-right:4.0pt;">43.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.8" style="padding-left:4.0pt;padding-right:4.0pt;">52.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.9" style="padding-left:4.0pt;padding-right:4.0pt;">37.97</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.10" style="padding-left:4.0pt;padding-right:4.0pt;">5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">meta-llama-3-70b-instruct <cite class="ltx_cite ltx_citemacro_cite">AI@Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib3" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3" style="padding-left:4.0pt;padding-right:4.0pt;">37.75</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">41.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">23.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.7" style="padding-left:4.0pt;padding-right:4.0pt;">44.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.8" style="padding-left:4.0pt;padding-right:4.0pt;">43.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.9" style="padding-left:4.0pt;padding-right:4.0pt;">42.76</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.10" style="padding-left:4.0pt;padding-right:4.0pt;">2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.1" style="padding-left:4.0pt;padding-right:4.0pt;">qwen2-72b-instruct <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib6" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">35.5<span class="ltx_text ltx_phantom" id="S4.T1.1.6.5.3.1"><span style="visibility:hidden">0</span></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">39.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.6" style="padding-left:4.0pt;padding-right:4.0pt;">27.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.7" style="padding-left:4.0pt;padding-right:4.0pt;">37.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.8" style="padding-left:4.0pt;padding-right:4.0pt;">39.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.9" style="padding-left:4.0pt;padding-right:4.0pt;">41.43</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.10" style="padding-left:4.0pt;padding-right:4.0pt;">3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">qwen2-7b-instruct <cite class="ltx_cite ltx_citemacro_cite">Bai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib6" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">34.0<span class="ltx_text ltx_phantom" id="S4.T1.1.7.6.3.1"><span style="visibility:hidden">0</span></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">6</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.5" style="padding-left:4.0pt;padding-right:4.0pt;">29.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">43.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">36.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.8" style="padding-left:4.0pt;padding-right:4.0pt;">41.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.9" style="padding-left:4.0pt;padding-right:4.0pt;">25.92</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.10" style="padding-left:4.0pt;padding-right:4.0pt;">9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.1" style="padding-left:4.0pt;padding-right:4.0pt;">phi-3-medium-4k-instruct <cite class="ltx_cite ltx_citemacro_cite">Abdin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib1" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.3" style="padding-left:4.0pt;padding-right:4.0pt;">32.75</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.4" style="padding-left:4.0pt;padding-right:4.0pt;">7</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.5" style="padding-left:4.0pt;padding-right:4.0pt;">32.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.6" style="padding-left:4.0pt;padding-right:4.0pt;">27.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.7" style="padding-left:4.0pt;padding-right:4.0pt;">38.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.8" style="padding-left:4.0pt;padding-right:4.0pt;">34.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.9" style="padding-left:4.0pt;padding-right:4.0pt;">33.46</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.10" style="padding-left:4.0pt;padding-right:4.0pt;">6</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.1" style="padding-left:4.0pt;padding-right:4.0pt;">meta-llama-3-8b-instruct <cite class="ltx_cite ltx_citemacro_cite">AI@Meta (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib3" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.2" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.3" style="padding-left:4.0pt;padding-right:4.0pt;">32.75</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.4" style="padding-left:4.0pt;padding-right:4.0pt;">8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.5" style="padding-left:4.0pt;padding-right:4.0pt;">43.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.6" style="padding-left:4.0pt;padding-right:4.0pt;">22.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.7" style="padding-left:4.0pt;padding-right:4.0pt;">31.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.8" style="padding-left:4.0pt;padding-right:4.0pt;">35.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.9" style="padding-left:4.0pt;padding-right:4.0pt;">31.05</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.10" style="padding-left:4.0pt;padding-right:4.0pt;">7</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.1" style="padding-left:4.0pt;padding-right:4.0pt;">phi-3-small-128k-instruct <cite class="ltx_cite ltx_citemacro_cite">Abdin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib1" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.2" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.3" style="padding-left:4.0pt;padding-right:4.0pt;">31.5<span class="ltx_text ltx_phantom" id="S4.T1.1.10.9.3.1"><span style="visibility:hidden">0</span></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.4" style="padding-left:4.0pt;padding-right:4.0pt;">9</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.5" style="padding-left:4.0pt;padding-right:4.0pt;">20.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.6" style="padding-left:4.0pt;padding-right:4.0pt;">29.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.7" style="padding-left:4.0pt;padding-right:4.0pt;">41.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.8" style="padding-left:4.0pt;padding-right:4.0pt;">36.0</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.9" style="padding-left:4.0pt;padding-right:4.0pt;">28.12</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.10" style="padding-left:4.0pt;padding-right:4.0pt;">8</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.1" style="padding-left:4.0pt;padding-right:4.0pt;">Llama Professor (RAG)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.2" style="padding-left:4.0pt;padding-right:4.0pt;">✗</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.3" style="padding-left:4.0pt;padding-right:4.0pt;">100.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.4" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.5" style="padding-left:4.0pt;padding-right:4.0pt;">100.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.6" style="padding-left:4.0pt;padding-right:4.0pt;">100.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.7" style="padding-left:4.0pt;padding-right:4.0pt;">100.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.8" style="padding-left:4.0pt;padding-right:4.0pt;">100.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.9" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.11.10.10" style="padding-left:4.0pt;padding-right:4.0pt;">-</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Benchmark results (accuracy) on the 400 validated questions (TrustMus) and per category: People (Ppl); Instruments and Technology (I&amp;T); Genres, Forms, and Theory (Thr); Culture and History (C&amp;H). Whether the models are quantized (Quant), their rank, and LiveBench
average score (LB) excluding math ranking is also given.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Some examples of hallucinations of Llama3 without RAG and CoT, the difficulty filter, are as follows: <span class="ltx_text ltx_font_italic" id="S4.SS1.p1.1.1">What does the natural sign (<math alttext="\natural" class="ltx_Math" display="inline" id="S4.SS1.p1.1.1.m1.1"><semantics id="S4.SS1.p1.1.1.m1.1a"><mi id="S4.SS1.p1.1.1.m1.1.1" mathvariant="normal" xref="S4.SS1.p1.1.1.m1.1.1.cmml">♮</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.1.m1.1b"><ci id="S4.SS1.p1.1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.1.m1.1.1">♮</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.1.m1.1c">\natural</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.1.m1.1d">♮</annotation></semantics></math>) do in music notation? A) Raises a note by one semitone, B) Raises a note by two semitones, C) Lowers a note by one semitone, D) Cancels a previous sharp or flat.</span> The correct answer is D, but Llama3 chose A, which any musician should know is incorrect.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Another type of limitation of LLMs in the context of musicology, is the need of the models for interpreting the information.
This can be illustrated by how Llama3 handled the
article about <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">Adagio</span> in the <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">Grove Dictionary Online</span>, which summarizes the evolution of the term over centuries. In this regard, when interrogating Llama3 about the term as described by Rousseau, the model refers to the modern definition.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>TrustMus evaluation</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S4.T1" title="Table 1 ‣ 4.1 Human validation insights ‣ 4 Results and Discussion ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_tag">1</span></a> presents the benchmark results for various models evaluated on TrustMus.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Since we believe that open models are critical for transparency, reproducibility, and the advancement of knowledge, we use them in our research. We included ChatGPT in our comparison only because it is currently the most used LLM.</span></span></span>
The models tested include the best open source performing models in
LiveBench – LB <cite class="ltx_cite ltx_citemacro_cite">White et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib34" title="">2024</a>)</cite> excluding coding and math categories, i. e., a benchmark for LLMs without contamination and reduced biases containing non-musicology knowledge. Due to its’ leading performance, results of OpenAI’s GPT models are also given for comparison.
Models with less than 8B parameters were deployed in a computer with two RTX 2080ti GPUs with 16-bit precision, the largest models in a Colab A100 GPU with 4-bit quantization, and the
GPT models through their official API.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The model <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.1">gpt-4o-2024-05-13</span> clearly outperforms others with an accuracy of 58.75% (cf. TrustMus score in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#S4.T1" title="Table 1 ‣ 4.1 Human validation insights ‣ 4 Results and Discussion ‣ The Role of Large Language Models in Musicology: Are We Ready to Trust the Machines?"><span class="ltx_text ltx_ref_tag">1</span></a>), excelling in the categories
Ppl, Thr, and C&amp;H. This is not surprising as it is the leading model in LB as well, with a score of 58.38%.
However, comparing the LB and TrustMus rankings reveals important differences about how the models perform in terms of general and in domain-specific knowledge.
For instance, unlike in LB, the model <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.2">mixtral-8x22b-instruct-v0.1</span> performs well in our benchmark, ranking second with a score of 40.5%.
It is important to note the similar performance between <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.3">qwen</span> with 72B and 7B, the latter being the best performing of the ‘small’ LLMs in TrustMus while showing the worst performance in LB.
We also aim to acknowledge that comparing <span class="ltx_text ltx_font_typewriter" id="S4.SS2.p2.1.4">meta-llama3</span>
models with others is not entirely fair, as the benchmark was automatically generated by selecting questions from their specific blind spots, as detailed in Section 3.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">The lower performance of open-source models compared to LLama professor (RAG) <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.01864v1#bib.bib20" title="">2024</a>)</cite> highlights
the importance of reliable
domain-specific knowledge
for musicology-related applications.
This indicates considerable improvement possibilities with the potential of increasing the trustworthiness of LLMs
in the field.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Our paper shows that while current usage and trust in LLMs in musicology are low, there is a strong expectation of future impact.
However, LLMs are not yet at the required level for the field and do not meet the minimum quality, ethical and likely legal standards currently being discussed.
Through the proposed semi-automatic benchmark, we present a first attempt to measure LLMs hallucinations on musicology-related tasks.
This approach aims to facilitate the evaluation of future models, which promotes transparency and trustworthiness of the technology.
Despite the effort, this initial experiments are insufficient. Besides a more thorough evaluation, there is the need to specialize current models for musicology-related tasks, while reducing their environmental footprint.
Further research should focus on ensuring LLMs reliability to avoid misinformation, protecting user privacy and data security, and mitigating training data biases to promote responsible use in musicology. Collaboration between the technological, musicological, and content owner communities is essential for the proper development of this technology.
</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdin et al. (2024)</span>
<span class="ltx_bibblock">
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, , et al. 2024.

</span>
<span class="ltx_bibblock">Phi-3 technical report: A highly capable language model locally on your phone.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2404.14219</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, et al. 2023.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">AI@Meta (2024)</span>
<span class="ltx_bibblock">
AI@Meta. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md" title="">Llama 3 model card. Final report not published yet.</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alkaissi and McFarlane (2023)</span>
<span class="ltx_bibblock">
Hussam Alkaissi and Samy I McFarlane. 2023.

</span>
<span class="ltx_bibblock">Artificial hallucinations in ChatGPT: implications in scientific writing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Cureus</em>, 15(2).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al. (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2310.11511</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. 2023.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2309.16609</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dhuliawala et al. (2023)</span>
<span class="ltx_bibblock">
Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023.

</span>
<span class="ltx_bibblock">Chain-of-verification reduces hallucination in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2309.11495</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duckles et al. (2020)</span>
<span class="ltx_bibblock">
Vincent Duckles, Jann Pasler, Glenn Stanley, Thomas Christensen, Barbara H. Haggh, Robert Balchin, et al. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1093/gmo/9781561592630.article.46710" title="">Musicology</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">The Grove Music Online</em>. Oxford University Press.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gallegos et al. (2024)</span>
<span class="ltx_bibblock">
Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K Ahmed. 2024.

</span>
<span class="ltx_bibblock">Bias and fairness in large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2020)</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, et al. 2020.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2101.00027</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hagberg et al. (2008)</span>
<span class="ltx_bibblock">
Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. 2008.

</span>
<span class="ltx_bibblock">Exploring network structure, dynamics, and function using networkx.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 7th Python in Science Conference (SciPy2008)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harap (1937)</span>
<span class="ltx_bibblock">
Louis Harap. 1937.

</span>
<span class="ltx_bibblock">On the nature of musicology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">The Musical Quarterly</em>, 23(1).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hoffmann et al. (2022)</span>
<span class="ltx_bibblock">
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. 2022.

</span>
<span class="ltx_bibblock">Training compute-optimal large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2203.15556</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong et al. (2024)</span>
<span class="ltx_bibblock">
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C Park. 2024.

</span>
<span class="ltx_bibblock">Adaptive-RAG: Learning to adapt retrieval-augmented large language models through question complexity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2403.14403</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024.

</span>
<span class="ltx_bibblock">Mixtral of experts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2401.04088</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasneci et al. (2023)</span>
<span class="ltx_bibblock">
Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, et al. 2023.

</span>
<span class="ltx_bibblock">ChatGPT for good? on opportunities and challenges of large language models for education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Learning and individual differences</em>, 103:102274.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kolasani (2023)</span>
<span class="ltx_bibblock">
Saydulu Kolasani. 2023.

</span>
<span class="ltx_bibblock">Optimizing natural language processing, large language models (llms) for efficient customer service, and hyper-personalization to enable sustainable growth and revenue.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Transactions on Latest Trends in Artificial Intelligence</em>, 4(4).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, et al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of Advances in Neural Information Processing Systems, 33 (NeurIPS 2020)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock">HaluEval: A large-scale hallucination evaluation benchmark for large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Mohammad Shoeybi, and Bryan Catanzaro. 2024.

</span>
<span class="ltx_bibblock">ChatQA: Building GPT-4 Level Conversational QA Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">arXiv preprint arXiv:2401.10225</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lozić and Štular (2023)</span>
<span class="ltx_bibblock">
Edisa Lozić and Benjamin Štular. 2023.

</span>
<span class="ltx_bibblock">Fluent but not factual: A comparative analysis of ChatGPT and other ai chatbots’ proficiency and originality in scientific writing for humanities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Future Internet</em>, 15(10).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minaee et al. (2024)</span>
<span class="ltx_bibblock">
Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024.

</span>
<span class="ltx_bibblock">Large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2402.06196</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis et al. (2023)</span>
<span class="ltx_bibblock">
Savvas Petridis, Nicholas Diakopoulos, Kevin Crowston, Mark Hansen, Keren Henderson, Stan Jastrzebski, et al. 2023.

</span>
<span class="ltx_bibblock">Anglekindling: Supporting journalistic angle ideation with large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI 23)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rane (2023)</span>
<span class="ltx_bibblock">
Nitin Rane. 2023.

</span>
<span class="ltx_bibblock">Role and challenges of ChatGPT and similar generative artificial intelligence in arts and humanities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Available at SSRN 4603208</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rane and Choudhary (2024)</span>
<span class="ltx_bibblock">
Nitin Rane and Saurabh Choudhary. 2024.

</span>
<span class="ltx_bibblock">Role and challenges of ChatGPT, Google Bard, and similar generative Artificial Intelligence in Arts and Humanities.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Studies in Humanities and Education</em>, 5(1).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rashkin et al. (2023)</span>
<span class="ltx_bibblock">
Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023.

</span>
<span class="ltx_bibblock">Measuring attribution in natural language generation models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Computational Linguistics</em>, 49(4).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sadie and Tyrrell (2001)</span>
<span class="ltx_bibblock">
Stanley Sadie and John Tyrrell, editors. 2001.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">The New Grove Dictionary of Music and Musicians</em>, 2nd edition.

</span>
<span class="ltx_bibblock">Macmillan Publishers, London.

</span>
<span class="ltx_bibblock">Grove Music Online. Edited by Deane Root. Accessed 05-05-2024. http://www.oxfordmusiconline.com.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thirunavukarasu et al. (2023)</span>
<span class="ltx_bibblock">
Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.

</span>
<span class="ltx_bibblock">Large language models in medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Nature Medicine</em>, 29(8).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Walters and Wilder (2023)</span>
<span class="ltx_bibblock">
William H Walters and Esther Isabelle Wilder. 2023.

</span>
<span class="ltx_bibblock">Fabrication and errors in the bibliographic citations generated by chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Scientific Reports</em>, 13(1):14045.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022a)</span>
<span class="ltx_bibblock">
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, et al. 2022a.

</span>
<span class="ltx_bibblock">Emergent abilities of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Transactions Machine Learning Research</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2022b)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, et al. 2022b.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in Neural Information Processing Systems 35 (NeurIPS 2022)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger et al. (2021)</span>
<span class="ltx_bibblock">
Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, et al. 2021.

</span>
<span class="ltx_bibblock">Ethical and social risks of harm from language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2112.04359</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weiser (2024)</span>
<span class="ltx_bibblock">
Benjamin Weiser. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html" title="">Here’s what happens when your lawyer uses ChatGPT</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">New York Times</em>.

</span>
<span class="ltx_bibblock">Accessed 05-05-2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">White et al. (2024)</span>
<span class="ltx_bibblock">
Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, et al. 2024.

</span>
<span class="ltx_bibblock">LiveBench: A Challenging, Contamination-Free LLM Benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:2406.19314</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2024)</span>
<span class="ltx_bibblock">
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024.

</span>
<span class="ltx_bibblock">Corrective retrieval augmented generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2401.15884</em>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep  3 12:58:40 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
