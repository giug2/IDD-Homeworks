<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2204.05203] CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning</title><meta property="og:description" content="Federated learning enables building a shared model from multicentre data while storing the training data locally for privacy.
In this paper, we present an evaluation (called CXR-FL) of deep learning-based models for ch‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2204.05203">

<!--Generated on Mon Mar 11 12:07:03 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Deep learning Federated learning Medical Imaging">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Sano Centre for Computational Medicine, Krakow, Poland </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>AGH University of Science and Technology, Krakow, Poland </span></span></span><span id="id3" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_note_type">institutetext: </span>Warsaw University of Technology, Warsaw, Poland</span></span></span>
<h1 class="ltx_title ltx_title_document">CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Filip ≈ölazyk 
</span><span class="ltx_author_notes">11 2 2
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7270-7836" title="ORCID identifier" class="ltx_ref">0000-0002-7270-7836</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Przemys≈Çaw Jab≈Çecki
</span><span class="ltx_author_notes">11 2 2
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-7306-1467" title="ORCID identifier" class="ltx_ref">0000-0002-7306-1467</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aneta Lisowska
</span><span class="ltx_author_notes">11
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0002-4489-5956" title="ORCID identifier" class="ltx_ref">0000-0002-4489-5956</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maciej Malawski
</span><span class="ltx_author_notes">11 2 2
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-6005-0243" title="ORCID identifier" class="ltx_ref">0000-0001-6005-0243</a></span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Szymon P≈Çotka
</span><span class="ltx_author_notes">11 3 3
<span class="ltx_contact ltx_role_orcid"><a target="_blank" href="https://orcid.org/0000-0001-9411-820X" title="ORCID identifier" class="ltx_ref">0000-0001-9411-820X</a></span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Federated learning enables building a shared model from multicentre data while storing the training data locally for privacy.
In this paper, we present an evaluation (called CXR-FL) of deep learning-based models for chest X-ray image analysis using the federated learning method. We examine the impact of federated learning parameters on the performance of central models. Additionally, we show that classification models perform worse if trained on a region of interest reduced to segmentation of the lung compared to the full image. However, focusing training of the classification model on the lung area may result in improved pathology interpretability during inference. We also find that federated learning helps maintain model generalizability. The pre-trained weights and code are publicly available at (<a target="_blank" href="https://github.com/SanoScience/CXR-FL" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/SanoScience/CXR-FL</a>).</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Deep learning Federated learning Medical Imaging
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Federated Learning (FL) is an effective privacy-preserving machine learning technique used to train models across multiple decentralized devices. It enables using a large amount of labeled data in a secure and privacy-preserving process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> to improve the generalizability of the model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. Recent work on the application of federated learning in medical imaging shows promising results in dermoscopic diagnosis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, volumetric segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> and chest X-ray image analysis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. In this paper, we evaluate the application of deep learning-based models to medical image analysis using the FL method. To gain insight into the impact of FL-related parameters on the global model, we conduct experiments with a variable number of clients and local training epochs. We explore utilisation of cascading approach, where medical image segmentation is performed prior to classification, for increased pathology classification interpretability. We compare our results with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> in terms of explainable AI (XAI) and classification performance. We find faster convergence of the learning process for a greater fraction of selected clients and a greater number of local epochs in the segmentation task. We show that federated learning improves the generalizability of the model and helps avoid overfitting in the classification task. We show that Grad-CAM explanations for classification models trained on segmented images may be more focused on the lung area than those trained on full images.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2204.05203/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="181" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Methodology: combining segmentation and classification in FL setting</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Method: FL for Segmentation and Classification</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our method consists of federated training of segmentation and classification models. First, we train segmentation models in a federated manner. For this purpose, we utilize the UNet++ model (with an EfficientNet-B4 backbone) that is later used to prepare the input for classification models.
At the classification stage, we use the best segmentation model in terms of the chosen performance metric, and preprocess CXR images (from the training and testing set) to extract lung regions and reduce the impact of the background noise on the prediction.
We subsequently train one model on full images and the second on segmented ones independently, all in a federated fashion.
During each round of federated training, clients download the global model and fine-tune it with the use of locally stored data. Once all models are fine-tuned in the given round, the server aggregates weights and the next round begins.
After the training phase, both types of models pass through the visual explanation step using GradCAM, as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. We test two architectures: ResNet50 and DenseNet121, both commonly used in medical image data classification¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. An overview of the proposed method for classification stage is depicted in Fig¬†<a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments and Results</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Datasets</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p"><span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_bold">Chest X-Ray Dataset</span>: To train the UNet++ model in a federated manner, we use this data set, which is a union of two other data sets known as Chest X-Ray Images (Pneumonia) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. The dataset consists of 6380 CXR images.
<span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_bold">RSNA 2018 Dataset</span>: To evaluate our method, we use an open-source RSNA Pneumonia Detection Challenge 2018 chest X-ray data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. In total, the dataset consists of 26684 CXR images in the DICOM format. There are 3 classes in the dataset: "Normal" (8525 - train/326 - test), "No Lung Opacity / Not Normal" (11500 - train/321 - test) and "Lung Opacity" (5659 - train/353 - test).</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Implementation details</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We implement our models in Python 3.8 with the PyTorch v.1.10.1 and Flower v.0.17.0 frameworks, based on our previous experience¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. We train our models on 4 nodes of a cluster with 1 <math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\times" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">√ó</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><times id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\times</annotation></semantics></math> NVIDIA v100 GPU each.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.6" class="ltx_p">For the <span id="S3.SS2.p2.6.1" class="ltx_text ltx_font_bold">segmentation task</span>, we use UNet++ with EfficientNet-B4 backbone pretrained on ImageNet. Adagrad is utilised as an optimizer for clients. We use a batch size of 2 and set learning rate and weight decay to <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="lr=1\times 10^{-3}" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mrow id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2.2" xref="S3.SS2.p2.1.m1.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.1.m1.1.1.2.1" xref="S3.SS2.p2.1.m1.1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.1.m1.1.1.2.3" xref="S3.SS2.p2.1.m1.1.1.2.3.cmml">r</mi></mrow><mo id="S3.SS2.p2.1.m1.1.1.1" xref="S3.SS2.p2.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mn id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">√ó</mo><msup id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml"><mn id="S3.SS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.cmml">10</mn><mrow id="S3.SS2.p2.1.m1.1.1.3.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.3.cmml"><mo id="S3.SS2.p2.1.m1.1.1.3.3.3a" xref="S3.SS2.p2.1.m1.1.1.3.3.3.cmml">‚àí</mo><mn id="S3.SS2.p2.1.m1.1.1.3.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.3.2.cmml">3</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><eq id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></eq><apply id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2"><times id="S3.SS2.p2.1.m1.1.1.2.1.cmml" xref="S3.SS2.p2.1.m1.1.1.2.1"></times><ci id="S3.SS2.p2.1.m1.1.1.2.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2.2">ùëô</ci><ci id="S3.SS2.p2.1.m1.1.1.2.3.cmml" xref="S3.SS2.p2.1.m1.1.1.2.3">ùëü</ci></apply><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><times id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1"></times><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">1</cn><apply id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">superscript</csymbol><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.2">10</cn><apply id="S3.SS2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3"><minus id="S3.SS2.p2.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3"></minus><cn type="integer" id="S3.SS2.p2.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3.3.2">3</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">lr=1\times 10^{-3}</annotation></semantics></math>, <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="wd=0" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mrow id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mrow id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2.2" xref="S3.SS2.p2.2.m2.1.1.2.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.2.m2.1.1.2.1" xref="S3.SS2.p2.2.m2.1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.2.m2.1.1.2.3" xref="S3.SS2.p2.2.m2.1.1.2.3.cmml">d</mi></mrow><mo id="S3.SS2.p2.2.m2.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><eq id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1"></eq><apply id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2"><times id="S3.SS2.p2.2.m2.1.1.2.1.cmml" xref="S3.SS2.p2.2.m2.1.1.2.1"></times><ci id="S3.SS2.p2.2.m2.1.1.2.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2.2">ùë§</ci><ci id="S3.SS2.p2.2.m2.1.1.2.3.cmml" xref="S3.SS2.p2.2.m2.1.1.2.3">ùëë</ci></apply><cn type="integer" id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">wd=0</annotation></semantics></math> respectively. We assess Jaccard score and BCE-Dice loss on a test set on the central server. The data set used to train the segmentation model was split into a training set and a test set with a 9:1 ratio, maintaining IID distribution of samples. Images are rescaled to <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="1024\times 1024" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mn id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">1024</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">√ó</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">1024</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><times id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></times><cn type="integer" id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">1024</cn><cn type="integer" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">1024</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">1024\times 1024</annotation></semantics></math> px and augmented with random flip and random affine transformations. The central model is evaluated on a server-side test set after each training round.
For the <span id="S3.SS2.p2.6.2" class="ltx_text ltx_font_bold">classification task</span>, we use Adam optimizer with learning rate <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="lr=1\times 10^{-4}" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mrow id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2.2" xref="S3.SS2.p2.4.m4.1.1.2.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.4.m4.1.1.2.1" xref="S3.SS2.p2.4.m4.1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.4.m4.1.1.2.3" xref="S3.SS2.p2.4.m4.1.1.2.3.cmml">r</mi></mrow><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">=</mo><mrow id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml"><mn id="S3.SS2.p2.4.m4.1.1.3.2" xref="S3.SS2.p2.4.m4.1.1.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.4.m4.1.1.3.1" xref="S3.SS2.p2.4.m4.1.1.3.1.cmml">√ó</mo><msup id="S3.SS2.p2.4.m4.1.1.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.cmml"><mn id="S3.SS2.p2.4.m4.1.1.3.3.2" xref="S3.SS2.p2.4.m4.1.1.3.3.2.cmml">10</mn><mrow id="S3.SS2.p2.4.m4.1.1.3.3.3" xref="S3.SS2.p2.4.m4.1.1.3.3.3.cmml"><mo id="S3.SS2.p2.4.m4.1.1.3.3.3a" xref="S3.SS2.p2.4.m4.1.1.3.3.3.cmml">‚àí</mo><mn id="S3.SS2.p2.4.m4.1.1.3.3.3.2" xref="S3.SS2.p2.4.m4.1.1.3.3.3.2.cmml">4</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><eq id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></eq><apply id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2"><times id="S3.SS2.p2.4.m4.1.1.2.1.cmml" xref="S3.SS2.p2.4.m4.1.1.2.1"></times><ci id="S3.SS2.p2.4.m4.1.1.2.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2.2">ùëô</ci><ci id="S3.SS2.p2.4.m4.1.1.2.3.cmml" xref="S3.SS2.p2.4.m4.1.1.2.3">ùëü</ci></apply><apply id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><times id="S3.SS2.p2.4.m4.1.1.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.1"></times><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.2">1</cn><apply id="S3.SS2.p2.4.m4.1.1.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.3.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3">superscript</csymbol><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.2">10</cn><apply id="S3.SS2.p2.4.m4.1.1.3.3.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.3"><minus id="S3.SS2.p2.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.3"></minus><cn type="integer" id="S3.SS2.p2.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS2.p2.4.m4.1.1.3.3.3.2">4</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">lr=1\times 10^{-4}</annotation></semantics></math> and weight decay <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="wd=1\times 10^{-5}" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mrow id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mrow id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2.2" xref="S3.SS2.p2.5.m5.1.1.2.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p2.5.m5.1.1.2.1" xref="S3.SS2.p2.5.m5.1.1.2.1.cmml">‚Äã</mo><mi id="S3.SS2.p2.5.m5.1.1.2.3" xref="S3.SS2.p2.5.m5.1.1.2.3.cmml">d</mi></mrow><mo id="S3.SS2.p2.5.m5.1.1.1" xref="S3.SS2.p2.5.m5.1.1.1.cmml">=</mo><mrow id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mn id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">1</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.5.m5.1.1.3.1" xref="S3.SS2.p2.5.m5.1.1.3.1.cmml">√ó</mo><msup id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml"><mn id="S3.SS2.p2.5.m5.1.1.3.3.2" xref="S3.SS2.p2.5.m5.1.1.3.3.2.cmml">10</mn><mrow id="S3.SS2.p2.5.m5.1.1.3.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.3.cmml"><mo id="S3.SS2.p2.5.m5.1.1.3.3.3a" xref="S3.SS2.p2.5.m5.1.1.3.3.3.cmml">‚àí</mo><mn id="S3.SS2.p2.5.m5.1.1.3.3.3.2" xref="S3.SS2.p2.5.m5.1.1.3.3.3.2.cmml">5</mn></mrow></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><eq id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1.1"></eq><apply id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2"><times id="S3.SS2.p2.5.m5.1.1.2.1.cmml" xref="S3.SS2.p2.5.m5.1.1.2.1"></times><ci id="S3.SS2.p2.5.m5.1.1.2.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2.2">ùë§</ci><ci id="S3.SS2.p2.5.m5.1.1.2.3.cmml" xref="S3.SS2.p2.5.m5.1.1.2.3">ùëë</ci></apply><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><times id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.1"></times><cn type="integer" id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">1</cn><apply id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3">superscript</csymbol><cn type="integer" id="S3.SS2.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.2">10</cn><apply id="S3.SS2.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.3"><minus id="S3.SS2.p2.5.m5.1.1.3.3.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.3"></minus><cn type="integer" id="S3.SS2.p2.5.m5.1.1.3.3.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3.3.2">5</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">wd=1\times 10^{-5}</annotation></semantics></math>, and set batch size to 8. Images are rescaled to <math id="S3.SS2.p2.6.m6.1" class="ltx_Math" alttext="224\times 224" display="inline"><semantics id="S3.SS2.p2.6.m6.1a"><mrow id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml"><mn id="S3.SS2.p2.6.m6.1.1.2" xref="S3.SS2.p2.6.m6.1.1.2.cmml">224</mn><mo lspace="0.222em" rspace="0.222em" id="S3.SS2.p2.6.m6.1.1.1" xref="S3.SS2.p2.6.m6.1.1.1.cmml">√ó</mo><mn id="S3.SS2.p2.6.m6.1.1.3" xref="S3.SS2.p2.6.m6.1.1.3.cmml">224</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.1b"><apply id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1"><times id="S3.SS2.p2.6.m6.1.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1.1"></times><cn type="integer" id="S3.SS2.p2.6.m6.1.1.2.cmml" xref="S3.SS2.p2.6.m6.1.1.2">224</cn><cn type="integer" id="S3.SS2.p2.6.m6.1.1.3.cmml" xref="S3.SS2.p2.6.m6.1.1.3">224</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.1c">224\times 224</annotation></semantics></math> px and augmented with random flip and random affine transformations. We evaluate accuracy and CE loss on the test sets (segmented/non-segmented) on the central server. In both tasks, the models are pretrained on the ImageNet dataset. Such pretrained models are downloaded by clients during the first round of the process. We use the FedAvg¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> aggregation strategy and split data in the IID manner among FL clients both in segmentation and classification.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Segmentation results</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In order to find the optimal central segmentation model, we evaluate several configurations of parameters typical for FL such as the number of local epochs performed by each client during every training round and the fraction of clients selected by the server during each round. The process of training each model consists of 15 rounds. The Jaccard score and loss obtained by each model are presented in Fig <a href="#S3.F2" title="Figure 2 ‚Ä£ 3.3 Segmentation results ‚Ä£ 3 Experiments and Results ‚Ä£ CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. For each configuration, we check the number of rounds required to achieve a Jaccard score of 0.92 twice. Results are presented in Table <a href="#S3.T1" title="Table 1 ‚Ä£ 3.3 Segmentation results ‚Ä£ 3 Experiments and Results ‚Ä£ CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We identify that for a fixed number of local epochs, a greater fraction of selected clients results in a smaller number of rounds needed to exceed the score of 0.92, similarly to the trend observed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>. The highest score (0.924) is achieved by the model trained with 3 local epochs and 3 selected clients in the 15th round of training. This model is later used to generate masks for classification.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<table id="S3.F2.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F2.2.2" class="ltx_tr">
<td id="S3.F2.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x2.png" id="S3.F2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="198" height="173" alt="Refer to caption"></td>
<td id="S3.F2.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x3.png" id="S3.F2.2.2.2.g1" class="ltx_graphics ltx_img_square" width="198" height="173" alt="Refer to caption"></td>
</tr>
<tr id="S3.F2.2.3.1" class="ltx_tr">
<td id="S3.F2.2.3.1.1" class="ltx_td ltx_align_center">a)</td>
<td id="S3.F2.2.3.1.2" class="ltx_td ltx_align_center">b)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(a) Jaccard score for the test dataset, achieved by segmentation models, and (b) loss of segmentation models for the test dataset, in successive rounds of training. ‚Äùsc‚Äù - the number of clients selected by the server in each round, ‚Äùle‚Äù - the number of local epochs performed by each client per round.</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of rounds needed by the segmentation model to exceed a Jaccard Score of 0.92 for the serverside test dataset. "sc" - the number of the clients selected by the server in each round, "le" - the number of local epochs performed by each client per round.</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Configuration</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Experiment 1</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T1.1.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Experiment 2</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T1.1.2.1.1.1" class="ltx_text" style="font-size:70%;">le = 1 &amp; sc = 1</span></td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.1.2.1.2.1" class="ltx_text" style="font-size:70%;">13</span></td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.1.2.1.3.1" class="ltx_text" style="font-size:70%;">14</span></td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.3.2.1.1" class="ltx_text" style="font-size:70%;">le = 1 &amp; sc = 2</span></td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.3.2.2.1" class="ltx_text" style="font-size:70%;">11</span></td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.3.2.3.1" class="ltx_text" style="font-size:70%;">10</span></td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.4.3.1.1" class="ltx_text" style="font-size:70%;">le = 1 &amp; sc = 3</span></td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.4.3.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">9</span></td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.4.3.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">9</span></td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T1.1.5.4.1.1" class="ltx_text" style="font-size:70%;">le = 2 &amp; sc = 1</span></td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.1.5.4.2.1" class="ltx_text" style="font-size:70%;">9</span></td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.1.5.4.3.1" class="ltx_text" style="font-size:70%;">9</span></td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.6.5.1.1" class="ltx_text" style="font-size:70%;">le = 2 &amp; sc = 2</span></td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.6.5.2.1" class="ltx_text" style="font-size:70%;">7</span></td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.6.5.3.1" class="ltx_text" style="font-size:70%;">7</span></td>
</tr>
<tr id="S3.T1.1.7.6" class="ltx_tr">
<td id="S3.T1.1.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.7.6.1.1" class="ltx_text" style="font-size:70%;">le = 2 &amp; sc = 3</span></td>
<td id="S3.T1.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.7.6.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">6</span></td>
<td id="S3.T1.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">6</span></td>
</tr>
<tr id="S3.T1.1.8.7" class="ltx_tr">
<td id="S3.T1.1.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T1.1.8.7.1.1" class="ltx_text" style="font-size:70%;">le = 3 &amp; sc = 1</span></td>
<td id="S3.T1.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.1.8.7.2.1" class="ltx_text" style="font-size:70%;">6</span></td>
<td id="S3.T1.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T1.1.8.7.3.1" class="ltx_text" style="font-size:70%;">6</span></td>
</tr>
<tr id="S3.T1.1.9.8" class="ltx_tr">
<td id="S3.T1.1.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.9.8.1.1" class="ltx_text" style="font-size:70%;">le = 3 &amp; sc = 2</span></td>
<td id="S3.T1.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.9.8.2.1" class="ltx_text" style="font-size:70%;">5</span></td>
<td id="S3.T1.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T1.1.9.8.3.1" class="ltx_text" style="font-size:70%;">5</span></td>
</tr>
<tr id="S3.T1.1.10.9" class="ltx_tr">
<td id="S3.T1.1.10.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T1.1.10.9.1.1" class="ltx_text" style="font-size:70%;">le = 3 &amp; sc = 3</span></td>
<td id="S3.T1.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.10.9.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">5</span></td>
<td id="S3.T1.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T1.1.10.9.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">5</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Classification results</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">In the case of the classification task, we evaluate how splitting the same amount of training data between 1, 2 and 3 clients impacts global model quality. Additionally, we assess differences between results obtained with ResNet50 and DenseNet121 architectures on full and segmented images. The accuracy score and loss for 10 rounds of training are presented in Fig. <a href="#S3.F4" title="Figure 4 ‚Ä£ 3.4 Classification results ‚Ä£ 3 Experiments and Results ‚Ä£ CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. It can be noted that the training process overfits in the case of 1 client and DenseNet121 model, both for segmented and full images, which is represented by a high loss value in the two last rounds for those configurations. The degradation of the global model quality can be also observed for DenseNet121 trained with full images on 2 and 3 clients. The lowest and most stable loss values are obtained for the ResNet50 model trained with 2 and 3 clients for full images and 1 to 3 clients for segmented images. Table <a href="#S3.T2" title="Table 2 ‚Ä£ 3.4 Classification results ‚Ä£ 3 Experiments and Results ‚Ä£ CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> presents maximum accuracy and minimum loss values for each configuration of model architecture and dataset type.</p>
</div>
<figure id="S3.F3" class="ltx_figure">
<table id="S3.F3.8" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F3.4.4" class="ltx_tr">
<td id="S3.F3.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x4.png" id="S3.F3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="72" height="72" alt="Refer to caption"></td>
<td id="S3.F3.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x5.png" id="S3.F3.2.2.2.g1" class="ltx_graphics ltx_img_square" width="72" height="72" alt="Refer to caption"></td>
<td id="S3.F3.3.3.3" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x6.png" id="S3.F3.3.3.3.g1" class="ltx_graphics ltx_img_square" width="72" height="72" alt="Refer to caption"></td>
<td id="S3.F3.4.4.4" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x7.png" id="S3.F3.4.4.4.g1" class="ltx_graphics ltx_img_square" width="72" height="72" alt="Refer to caption"></td>
</tr>
<tr id="S3.F3.8.8" class="ltx_tr">
<td id="S3.F3.5.5.1" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x8.png" id="S3.F3.5.5.1.g1" class="ltx_graphics ltx_img_square" width="72" height="72" alt="Refer to caption"></td>
<td id="S3.F3.6.6.2" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x9.png" id="S3.F3.6.6.2.g1" class="ltx_graphics ltx_img_square" width="72" height="72" alt="Refer to caption"></td>
<td id="S3.F3.7.7.3" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x10.png" id="S3.F3.7.7.3.g1" class="ltx_graphics ltx_img_square" width="72" height="72" alt="Refer to caption"></td>
<td id="S3.F3.8.8.4" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x11.png" id="S3.F3.8.8.4.g1" class="ltx_graphics ltx_img_square" width="72" height="72" alt="Refer to caption"></td>
</tr>
<tr id="S3.F3.8.9.1" class="ltx_tr">
<td id="S3.F3.8.9.1.1" class="ltx_td ltx_align_center">a) Resnet50</td>
<td id="S3.F3.8.9.1.2" class="ltx_td ltx_align_center">b) Resnet50</td>
<td id="S3.F3.8.9.1.3" class="ltx_td ltx_align_center">c) DenseNet121</td>
<td id="S3.F3.8.9.1.4" class="ltx_td ltx_align_center">d) DenseNet121</td>
</tr>
<tr id="S3.F3.8.10.2" class="ltx_tr">
<td id="S3.F3.8.10.2.1" class="ltx_td ltx_align_center">¬†¬† (segmented)</td>
<td id="S3.F3.8.10.2.2" class="ltx_td"></td>
<td id="S3.F3.8.10.2.3" class="ltx_td ltx_align_center">¬†¬†(segmented)</td>
<td id="S3.F3.8.10.2.4" class="ltx_td"></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Grad-CAM visualisations of Lung Opacity samples. In some instances, segmentation resulted in activations focused more on the lung area (upper sample). However, for a majority of cases, visualisation was comparable for segmented and full images (lower sample).</figcaption>
</figure>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The best accuracy, <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="0.757" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mn id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">0.757</mn><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><cn type="float" id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">0.757</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">0.757</annotation></semantics></math>, is achieved for ResNet50 model trained on two clients. The worst-performing model is DenseNet121 trained on full images on a single client. In general, the evaluation shows that training on a single client results in overall worse accuracy compared to training with 2 and 3 clients, which is reflected in Fig¬†<a href="#S3.T2" title="Table 2 ‚Ä£ 3.4 Classification results ‚Ä£ 3 Experiments and Results ‚Ä£ CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This leads to the conclusion that in this case, splitting the data among distinct clients and training the model in the FL manner helps maintain generalizability and avoid overfitting.
We observe that models trained on segmented images perform consistently worse than models trained on full images, as is the case for¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. There is one exception: for the DenseNet121 model the best accuracy is achieved for segmented images (0.742).</p>
</div>
<figure id="S3.F4" class="ltx_figure">
<table id="S3.F4.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.F4.2.2" class="ltx_tr">
<td id="S3.F4.1.1.1" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x12.png" id="S3.F4.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="230" height="165" alt="Refer to caption"></td>
<td id="S3.F4.2.2.2" class="ltx_td ltx_align_center"><img src="/html/2204.05203/assets/x13.png" id="S3.F4.2.2.2.g1" class="ltx_graphics ltx_img_landscape" width="230" height="165" alt="Refer to caption"></td>
</tr>
<tr id="S3.F4.2.3.1" class="ltx_tr">
<td id="S3.F4.2.3.1.1" class="ltx_td ltx_align_center">a)</td>
<td id="S3.F4.2.3.1.2" class="ltx_td ltx_align_center">b)</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>(a) Accuracy score achieved for the test dataset by classification models, and (b) loss of classification models on test dataset, in successive rounds of training. "d" defines dataset type (f - full/s - segmented), "m" defines model (d - DenseNet121, r - ResNet50), "cc" indicates number of clients participating in training.</figcaption>
</figure>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">To understand qualitative differences in the classification of segmented and full images, we perform Grad-CAM visualisation for ResNet50 and DenseNet121 models. We identify samples that show that the use of segmented images leads to activations more focused on the lung area (as presented in the upper sample in Fig. <a href="#S3.F3" title="Figure 3 ‚Ä£ 3.4 Classification results ‚Ä£ 3 Experiments and Results ‚Ä£ CXR-FL: Deep Learning-based Chest X-ray Image Analysis Using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>), which is beneficial for model interpretability. However, it can be observed that samples in which the activations are already focused on regions with pathological lung changes, for both full and segmented images, are prevalent. We believe that the small difference in the quality of the models trained on full and segmented images can be explained by the common presence of that similarity.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Maximum accuracy and minimum loss values obtained for each classification model on the test set. "d" defines dataset type (f - full/s - segmented), "m" defines model (d - DenseNet121, r - ResNet50), "cc" indicates number of clients participating in training. Values listed in boldface correspond to extremes in each <span id="S3.T2.3.1" class="ltx_text ltx_font_italic">model</span> / <span id="S3.T2.4.2" class="ltx_text ltx_font_italic">dataset kind</span> subset.</figcaption>
<table id="S3.T2.5" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.5.1.1" class="ltx_tr">
<th id="S3.T2.5.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Configuration</span></th>
<th id="S3.T2.5.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T2.5.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Max. accuracy</span></th>
<th id="S3.T2.5.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="S3.T2.5.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">Min. loss</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.5.2.1" class="ltx_tr">
<td id="S3.T2.5.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T2.5.2.1.1.1" class="ltx_text" style="font-size:70%;">d=f &amp; m=d &amp; cc=1</span></td>
<td id="S3.T2.5.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.5.2.1.2.1" class="ltx_text" style="font-size:70%;">0.721</span></td>
<td id="S3.T2.5.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.5.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.599</span></td>
</tr>
<tr id="S3.T2.5.3.2" class="ltx_tr">
<td id="S3.T2.5.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.3.2.1.1" class="ltx_text" style="font-size:70%;">d=f &amp; m=d &amp; cc=2</span></td>
<td id="S3.T2.5.3.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.3.2.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.737</span></td>
<td id="S3.T2.5.3.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.3.2.3.1" class="ltx_text" style="font-size:70%;">0.620</span></td>
</tr>
<tr id="S3.T2.5.4.3" class="ltx_tr">
<td id="S3.T2.5.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.4.3.1.1" class="ltx_text" style="font-size:70%;">d=f &amp; m=d &amp; cc=3</span></td>
<td id="S3.T2.5.4.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.4.3.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.737</span></td>
<td id="S3.T2.5.4.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.4.3.3.1" class="ltx_text" style="font-size:70%;">0.606</span></td>
</tr>
<tr id="S3.T2.5.5.4" class="ltx_tr">
<td id="S3.T2.5.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T2.5.5.4.1.1" class="ltx_text" style="font-size:70%;">d=f &amp; m=r &amp; cc=1</span></td>
<td id="S3.T2.5.5.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.5.5.4.2.1" class="ltx_text" style="font-size:70%;">0.714</span></td>
<td id="S3.T2.5.5.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.5.5.4.3.1" class="ltx_text" style="font-size:70%;">0.623</span></td>
</tr>
<tr id="S3.T2.5.6.5" class="ltx_tr">
<td id="S3.T2.5.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.6.5.1.1" class="ltx_text" style="font-size:70%;">d=f &amp; m=r &amp; cc=2</span></td>
<td id="S3.T2.5.6.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.6.5.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.757</span></td>
<td id="S3.T2.5.6.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.6.5.3.1" class="ltx_text" style="font-size:70%;">0.601</span></td>
</tr>
<tr id="S3.T2.5.7.6" class="ltx_tr">
<td id="S3.T2.5.7.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.7.6.1.1" class="ltx_text" style="font-size:70%;">d=f &amp; m=r &amp; cc=3</span></td>
<td id="S3.T2.5.7.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.7.6.2.1" class="ltx_text" style="font-size:70%;">0.747</span></td>
<td id="S3.T2.5.7.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.7.6.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.579</span></td>
</tr>
<tr id="S3.T2.5.8.7" class="ltx_tr">
<td id="S3.T2.5.8.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T2.5.8.7.1.1" class="ltx_text" style="font-size:70%;">d=s &amp; m=d &amp; cc=1</span></td>
<td id="S3.T2.5.8.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.5.8.7.2.1" class="ltx_text" style="font-size:70%;">0.708</span></td>
<td id="S3.T2.5.8.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.5.8.7.3.1" class="ltx_text" style="font-size:70%;">0.631</span></td>
</tr>
<tr id="S3.T2.5.9.8" class="ltx_tr">
<td id="S3.T2.5.9.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.9.8.1.1" class="ltx_text" style="font-size:70%;">d=s &amp; m=d &amp; cc=2</span></td>
<td id="S3.T2.5.9.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.9.8.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.742</span></td>
<td id="S3.T2.5.9.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.9.8.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.612</span></td>
</tr>
<tr id="S3.T2.5.10.9" class="ltx_tr">
<td id="S3.T2.5.10.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.10.9.1.1" class="ltx_text" style="font-size:70%;">d=s &amp; m=d &amp; cc=3</span></td>
<td id="S3.T2.5.10.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.10.9.2.1" class="ltx_text" style="font-size:70%;">0.734</span></td>
<td id="S3.T2.5.10.9.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.10.9.3.1" class="ltx_text" style="font-size:70%;">0.618</span></td>
</tr>
<tr id="S3.T2.5.11.10" class="ltx_tr">
<td id="S3.T2.5.11.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt"><span id="S3.T2.5.11.10.1.1" class="ltx_text" style="font-size:70%;">d=s &amp; m=r &amp; cc=1</span></td>
<td id="S3.T2.5.11.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.5.11.10.2.1" class="ltx_text" style="font-size:70%;">0.704</span></td>
<td id="S3.T2.5.11.10.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt"><span id="S3.T2.5.11.10.3.1" class="ltx_text" style="font-size:70%;">0.643</span></td>
</tr>
<tr id="S3.T2.5.12.11" class="ltx_tr">
<td id="S3.T2.5.12.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.12.11.1.1" class="ltx_text" style="font-size:70%;">d=s &amp; m=r &amp; cc=2</span></td>
<td id="S3.T2.5.12.11.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.12.11.2.1" class="ltx_text" style="font-size:70%;">0.730</span></td>
<td id="S3.T2.5.12.11.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S3.T2.5.12.11.3.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.602</span></td>
</tr>
<tr id="S3.T2.5.13.12" class="ltx_tr">
<td id="S3.T2.5.13.12.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S3.T2.5.13.12.1.1" class="ltx_text" style="font-size:70%;">d=s &amp; m=r &amp; cc=3</span></td>
<td id="S3.T2.5.13.12.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.5.13.12.2.1" class="ltx_text ltx_font_bold" style="font-size:70%;">0.736</span></td>
<td id="S3.T2.5.13.12.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t"><span id="S3.T2.5.13.12.3.1" class="ltx_text" style="font-size:70%;">0.607</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusions</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this paper, we evaluated deep learning-based models in the context of CXR image analysis. We conducted experiments in a FL environment to understand the impact of FL-related parameters on the global model performance in segmentation and classification tasks. We also prepared Grad-CAM visualisations for classification models.
We found that in the segmentation task, when the number of local epochs is fixed, the model reaches the desired quality faster with a greater fraction of selected clients. In addition, setting a greater number of local epochs for each client also leads to the same behaviour, which may contribute to lower network traffic in FL processes. Moreover, we conclude that splitting the same dataset among distinct FL clients may lead to improvements in classification for the tested models. We observed a higher accuracy score for full images compared to segmented images in the classification task. However, models trained on segmented images may be characterized by improved interpretability.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This publication is partly supported by the EU H2020 grant Sano (No. 857533) and the IRAP Plus programme of the Foundation for Polish Science. This research was supported in part by the PL-Grid Infrastructure. We would like to thank Piotr Nowakowski for his assistance with proofreading the manuscript.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"> Teixeira, L.O., et al.
: Impact of Lung Segmentation on the Diagnosis and Explanation of COVID-19 in Chest X-ray Images. Sensors. 21, 7116 (2021). https://doi.org/10.3390/s21217116.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"> Sheller, M.J., et al.:
Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data. Sci Rep. 10, 12598 (2020). https://doi.org/10.1038/s41598-020-69250-1.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"> Chen, Z., et al.: Personalized Retrogress-Resilient Framework for Real-World Medical Federated Learning. In:
Medical Image Computing and Computer Assisted Intervention ‚Äì MICCAI 2021. pp. 347‚Äì356. Springer
(2021). https://doi.org/10.1007/978-3-030-87199-4_33.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"> Wu, Y., et al.: Federated Contrastive Learning for Volumetric Medical Image Segmentation. In:
Medical Image Computing and Computer Assisted Intervention ‚Äì MICCAI 2021. pp. 367‚Äì377. Springer
(2021). https://doi.org/10.1007/978-3-030-87199-4_35.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"> Dong, N., Voiculescu, I.: Federated Contrastive Learning for Decentralized Unlabeled Medical Images. In:
Medical Image Computing and Computer Assisted Intervention ‚Äì MICCAI 2021. pp. 378‚Äì387. Springer
(2021). https://doi.org/10.1007/978-3-030-87199-4_36.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"> Jab≈Çecki, P., et al.: Federated Learning in the Cloud for Analysis of Medical Images - Experience with Open Source Frameworks. In:
Clinical Image-Based Procedures, Distributed and Collaborative Learning, Artificial Intelligence for Combating COVID-19 and Secure and Privacy-Preserving Machine Learning. pp. 111‚Äì119. Springer
(2021). https://doi.org/10.1007/978-3-030-90874-4_11.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Shih, G.
et. al.
: Augmenting the National Institutes of Health Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia. Radiology: Artificial Intelligence. 1, e180041 (2019). https://doi.org/10.1148/ryai.2019180041.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"> Kermany, D., et al.:
Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification. 2, (2018). https://doi.org/10.17632/rscbjbr9sj.2.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"> Cohen, J.P., et al.: COVID-19 Image Data Collection: Prospective Predictions Are the Future. arXiv:2006.11988 [cs, eess, q-bio]. (2020).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"> Tang, Y.-X., et al.: Automated abnormality classification of chest radiographs using deep convolutional neural networks. npj Digit. Med. 3, 70 (2020). https://doi.org/10.1038/s41746-020-0273-z.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"> McMahan, H.B., et el.: Communication-Efficient Learning of Deep Networks from Decentralized Data. (2016). https://doi.org/10.48550/ARXIV.1602.05629.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"> Kaissis, G. A. et al: Secure, privacy-preserving and federated machine learning in medical imaging.
Nature Machine Intelligence 2 (6) (2020) 305‚Äì311. 25

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2204.05202" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2204.05203" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2204.05203">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2204.05203" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2204.05204" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 12:07:03 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
