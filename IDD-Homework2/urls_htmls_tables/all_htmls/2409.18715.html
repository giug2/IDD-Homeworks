<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2409.18715] Multi-modal Medical Image Fusion For Non-Small Cell Lung Cancer Classification</title><meta property="og:description" content="The early detection and nuanced subtype classification of non-small cell lung cancer (NSCLC), a predominant cause of cancer mortality worldwide, is a critical and complex issue. In this paper, we introduce an innovativ…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Multi-modal Medical Image Fusion For Non-Small Cell Lung Cancer Classification">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Multi-modal Medical Image Fusion For Non-Small Cell Lung Cancer Classification">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2409.18715">

<!--Generated on Sun Oct  6 01:02:32 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multi-modal Medical Image Fusion For Non-Small Cell Lung Cancer Classification</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The early detection and nuanced subtype classification of non-small cell lung cancer (NSCLC), a predominant cause of cancer mortality worldwide, is a critical and complex issue. In this paper, we introduce an innovative integration of multi-modal data, synthesizing fused medical imaging (CT and PET scans) with clinical health records and genomic data. This unique fusion methodology leverages advanced machine learning models, notably MedClip and BEiT, for sophisticated image feature extraction, setting a new standard in computational oncology. Our research surpasses existing approaches, as evidenced by a substantial enhancement in NSCLC detection and classification precision. The results showcase notable improvements across key performance metrics, including accuracy, precision, recall, and F1-score. Specifically, our leading multi-modal classifier model records an impressive accuracy of 94.04%. We believe that our approach has the potential to transform NSCLC diagnostics, facilitating earlier detection and more effective treatment planning and, ultimately, leading to superior patient outcomes in lung cancer care.</p>
</div>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
Lung Cancer, NSCLC, Medical Imaging Fusion, Multimodal data, CT Scans, PET Scans</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Lung cancer remains a significant health challenge, ranking as the second most prevalent cancer and the foremost cause of cancer-related mortality in both men and women <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Characteristically, lung cancer symptoms often remain undetected until the disease has progressed to an advanced stage, complicating treatment efforts. In this context, image screening emerges as a critical tool, particularly for asymptomatic individuals. Studies underscore the efficacy of imaging techniques such as CT and MRI in early lung cancer detection among high-risk groups, including smokers and individuals with genetic predispositions, significantly enhancing survival prospects <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, existing methods for NSCLC classification, such as those utilizing standalone CT or PET imaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, often face limitations in terms of sensitivity and specificity, and they may not fully capture the complexity of tumor heterogeneity. Additionally, existing approaches may lack the integration of comprehensive clinical and genetic data, which are crucial for a more precise diagnosis. These limitations underscore the need for more advanced diagnostic methodologies that can accurately pinpoint and characterize cancerous tissues, providing a strong motivation for our innovative multi-modal image fusion approach.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This paper aims to refine lung cancer diagnosis, particularly for non-small cell lung cancer (NSCLC), by integrating multi-modal data beyond traditional imaging scans. Our proposed method fuses CT and PET scans to leverage the strengths of both modalities. CT scans provide detailed anatomical information, while PET scans offer insights into metabolic activity, often indicative of tumor presence. By combining these scans, our fusion method creates a more comprehensive image that captures both structural and functional aspects of the lung. Such fusion techniques have shown promise in improving diagnostic accuracy in other organ scans and in combining CT with MRI <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Our approach addresses the limitations of existing diagnostic techniques that rely solely on single-modality imaging. CT or PET scans alone may not sufficiently differentiate between malignant and benign lesions, leading to diagnostic inaccuracies. Additionally, the standalone use of these modalities may only partially capture the tumor’s complexity and heterogeneity. Therefore, our method begins with the denoising of scans using a deep CNN auto-encoder for more precise image reconstruction. This step is followed by the fusion of CT and PET scans through wavelet decomposition and accurate image registration to create a fused image that provides a richer, dual perspective of both functional and anatomical information. Specifically, fusing PET and CT images facilitates the precise localization of abnormal metabolic activities within the lung’s structural framework <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>. This precise localization is crucial for identifying and characterizing cancerous tissues accurately. To complement this, we undertake rigorous pre-processing of both tabular and genetic data, encompassing missing value estimation, encoding, class balancing, scaling, and feature selection. The image processing phase includes optimal contrast enhancement and normalization.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">By integrating multi-modal imaging with advanced machine learning techniques, we aim to enhance the overall diagnostic performance, leading to earlier and more accurate detection of NSCLC, ultimately improving patient outcomes. Leveraging advanced models like MedClip and BEiT, we enhance feature extraction and diagnostic accuracy. Additionally, incorporating clinical health records and genetic data allows for a more personalized diagnosis and treatment plan. This integration not only improves the sensitivity and specificity of NSCLC detection but also paves the way for earlier diagnosis and better patient outcomes.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">This paper makes several key contributions to the field of oncological diagnostics, particularly in lung cancer:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p"><span id="S1.I1.i1.p1.1.1" class="ltx_text ltx_font_bold">Innovative Multi-Modal Data Fusion:</span> Introduces an advanced multi-modal data fusion approach that blends CT and PET imaging with clinical and genetic data. This methodology, relatively unexplored in lung cancer diagnostics, offers a more comprehensive diagnostic view, potentially leading to earlier and more accurate detection of NSCLC.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p"><span id="S1.I1.i2.p1.1.1" class="ltx_text ltx_font_bold">Novel Application of Deep Denoising CNN Auto-Encoders:</span> Presents a novel application of deep CNN auto-encoders for the denoising of medical images, setting a new precedent for image clarity and diagnostic precision.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p"><span id="S1.I1.i3.p1.1.1" class="ltx_text ltx_font_bold">Effective Integration of Diverse Data Types:</span> Demonstrates the effectiveness of integrating multiple data types through the use of diverse and sophisticated analytical models, significantly enhancing diagnostic accuracy.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Ultimately, the paper’s core goal is to substantially improve the precision of NSCLC diagnosis by merging cutting-edge medical imaging with exhaustive patient data. By harnessing the complementary strengths of CT and PET scans, we aspire to generate a comprehensive, unified image that delineates lung anomalies more effectively. The fusion of this imaging data with clinical and genetic information forms the cornerstone of our NSCLC classification strategy, aiming to transform and refine diagnostic procedures in oncology.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Recent advancements in AI for diagnosing and treating NSCLC have significantly transformed patient outcomes in oncology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Recent advancements in AI for diagnosing and treating NSCLC have significantly transformed patient outcomes in oncology <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. The application of AI in NSCLC diagnosis encompasses several key research domains: medical imaging analysis, survival prediction, recurrence analytics, and multi-modal data integration. These fields have been instrumental in advancing the diagnosis and treatment of NSCLC.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">In the realm of medical imaging analysis, significant strides have been made in tumor detection and classification using AI algorithms applied to CT scans, as evidenced by the works of Feng and Khoirunnisa <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. These advancements have facilitated earlier and more precise diagnoses, consequently improving patient outcomes. Additionally, AI analysis of patient data has enhanced survival and recurrence prediction, offering valuable insights into patient prognosis <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Recent research has also explored novel AI architectures for improving detection accuracy. Studies have investigated the potential of fully automated pipelines and advanced model architectures, such as CRNN, ViTs, AlexNet, and 3D reconstruction techniques, in enhancing diagnostic accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. However, the highest accuracy achieved to date in these studies was 84.1% using the VGG16 model on medical imaging data alone <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">The integration of multi-modal data represents a burgeoning area of research. This includes both image multi-modality, combining MRI and CT scans, and data multi-modality, integrating tabular and imaging data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Studies by Feng have underscored the potential of combining imaging modalities like CT and MRI scans with patient genomics data to enhance diagnostic accuracy. The fusion of diverse data types, such as imaging modalities with patient genomic data, has shown promise in improving diagnostic accuracy. Our paper builds upon these foundational studies, aiming to pioneer the combination of multi-modal fused imaging with clinical and genomic data for NSCLC subtype classification. No work combines multi-modal fused imaging with clinical data and genomics sequences for the classification of both NSCLC subtypes, which is the scope and aim of this paper.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Our paper contributes to this growing body of knowledge by not only applying these established techniques but also innovating in the way we integrate and process multi-modal data. The fusion of CT and PET scans, as proposed in our methodology, builds upon the foundational work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, who illustrated the benefits of CT and MRI image fusion in enhancing the clarity and informational value of medical scans for oncological application. Furthermore, our use of advanced models like MedClip and BEiT for image feature extraction extends the work of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>, who highlighted the strengths of these models in handling high-resolution medical images.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.1" class="ltx_p">In summary, this paper stands at the intersection of several key research areas within the field of medical imaging and cancer diagnosis. By synthesizing these diverse methodologies and building upon them, our work aims to contribute a novel approach to the early detection and classification of NSCLC, addressing some of the limitations identified in previous studies.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Proposed Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our methodology introduces several novel contributions to the field of NSCLC diagnosis through multi-modal data integration and advanced deep learning techniques. These contributions can be categorized into four key areas: comprehensive pre-processing, advanced image fusion, innovative model architectures, and holistic data integration.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2409.18715/assets/NSCLC_ARCH.png" id="S3.F1.g1" class="ltx_graphics ltx_img_landscape" width="699" height="311" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.2.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Multi-modal BEiT model showing model architecture that starts with pre-processing the CT and PET scan and then fusing them into a single scan that is then fused with the other data modalities, such as clinical and genetic, before applying feature selection and passing it to the BEiT model for classification.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Comprehensive Pre-processing</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Our approach to data pre-processing is a significant contribution with practical benefits. We address four key data modalities: clinical tabular data, genetic data, CT scans, and PET scans. For tabular and genetic data, we implement a series of rigorous pre-processing steps, including missing value imputation, categorical encoding, class balancing using SMOTE, and standardization. Feature importance is evaluated using XGBoost, ensuring that only the most relevant features are used in subsequent analyses. For imaging data, we develop a deep CNN auto-encoder trained on a clean PET scan dataset to de-noise PET scans, effectively reducing noise and enhancing image quality. CT scans undergo normalization and contrast enhancement. We also employ 3D Slicer software for precise image registration of CT and PET scans, ensuring accurate anatomical and functional alignment. These pre-processing steps are crucial for maximizing the performance of our fusion and classification models, leading to more accurate diagnoses and treatment plans in healthcare analytics.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Advanced Image Fusion</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">One of the primary contributions of our methodology is the development of an advanced image fusion technique that combines the strengths of CT and PET scans. After all the prepossessing of the scans was done, the new clean input was fed into our VGG19 fusion model. The fusion algorithm decomposes the CT and PET scans into four coefficients, each using the discrete wavelet transform. The coefficients are the coefficient LL1 and three detail coefficients: LH1(horizontal), LV1(vertical), LD1(diagonal). After the fusion of the four pairs, inverse wavelet transform was applied to the four bands to obtain the fused image, results shown in Figure <a href="#S3.F2" title="Figure 2 ‣ 3.2 Advanced Image Fusion ‣ 3 Proposed Methodology ‣ Multi-modal Medical Image Fusion For Non-Small Cell Lung Cancer Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The CT scan offers clear structural details of the lung tissues and surrounding areas, but it lacks metabolic information. The PET scan reveals areas of high metabolic activity indicative of tumor presence, but it lacks precise anatomical context. By merging these two modalities, the fused image not only overcomes the limitations of each modality but also provides a comprehensive view that is invaluable in medical imaging, highlighting both the detailed structures of the lung tissues and the areas of high metabolic activity.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2409.18715/assets/output.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="350" height="105" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.2.1.1" class="ltx_text ltx_font_bold">Fig. 2</span>: </span>Example of CT, PET images, and resulting fused image</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Baseline Models</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">The next stage involved implementing different baseline models to evaluate the effectiveness of the multi-modal approach. SVM and logistic regression were chosen for their robustness and simplicity in handling tabular and genetic data, providing reliable benchmarks for structured data analysis. For image classification, we selected 2D CNN, 3D CNN, VGG16, ResNet18, Inception, and Xception due to their proven efficacy in medical imaging tasks and their diverse architectures. These models represent a wide range of complexity and feature extraction capabilities. Testing these models on both CT images alone and fused PET and CT images offers a comprehensive evaluation, allowing us to demonstrate the advantages of our multi-modal approach over single-modality methods.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Multi-modal Classification Model</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">For the multi-modal classification model, we introduced two sophisticated model architectures: MedClip and BEiT (Bidirectional Encoder representation from Image Transformers).
MedClip employs a dual encoder structure for visual and textual data, using cross-modal contrastive learning to align these representations in a unified feature space. This approach is particularly novel and crucial as it facilitates the seamless integration of medical images with clinical and genetic data, enhancing the model’s ability to draw comprehensive insights from diverse data types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. The dual encoder structure allows MedClip to effectively capture the complementary information from both image and text data, making it a powerful tool for multi-modal medical diagnostics.
BEiT, based on the Vision Transformer (ViT) architecture, leverages bidirectional context modeling and masked image modeling to develop robust representations from images. This model is pre-trained on medical images, which enhances its capability to detect and classify tumors accurately <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>. The use of bidirectional context modeling allows BEiT to consider the entire context of an image, providing a more thorough understanding of the visual information. Masked image modeling, on the other hand, trains the model to predict missing parts of the image, which improves its ability to generalize and perform well on unseen data.
These models were tested with three different image input combinations: CT alone, CT and PET separately, and the fused CT and PET image, allowing us to evaluate the efficacy of our fusion method thoroughly. By doing so, we can determine the effectiveness of the proposed method and fusion model. The architecture of the best multi-modal model BEiT-based is shown in Figure <a href="#S3.F1" title="Figure 1 ‣ 3 Proposed Methodology ‣ Multi-modal Medical Image Fusion For Non-Small Cell Lung Cancer Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Details</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">This paper utilized three distinct datasets to optimize the analysis of NSCLC. The primary dataset was the NSCLC Radiogenomic collection from the Cancer Imaging Archive <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>. This comprehensive dataset includes 285,411 scan images from 303 studies of 211 NSCLC patients, encompassing both CT and PET lung images. It also provides extensive clinical data, covering variables such as age, smoking status, histology, treatment history, and cancer recurrence. Additionally, it includes RNA sequencing data from biopsied tumor tissues, linking genetic information with imaging.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">Given the primary dataset’s imbalance, predominantly featuring the Adenocarcinoma class, it was supplemented with the NSCLC Radiomics dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>, which contains images of 422 NSCLC patients. Notably, both datasets were generated using the same scanner, ensuring consistency in data quality. The Radiomics dataset was exclusively used to augment the training data, while validation was performed solely on the primary dataset. The third dataset employed was a Large-Scale CT and PET dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, featuring de-noised PET scans. These scans were used to train a deep CNN auto-encoder with added noise to enhance the model’s robustness. Post-training, this model was applied to our primary dataset for PET scan de-noising, thereby improving the quality of our input data for subsequent analysis.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Detail and Result Analysis</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">For the hardware requirement, computers equipped with NVIDIA Quadro RTX6000 were used to train the models, and the multi-core processors were used for efficient data processing and analysis. Powerful GPUs are necessary to speed up the computations significantly. Finally, given the large size of medical imaging datasets, storage solutions like cloud storage were used. In terms of software, 3D Slicer was used to view and pair the CT and PET DICOM scans and to register the image between the two scans. Moreover, several Python libraries were needed to aid in working with imaging scans, namely pydicom, pynrrd, skimage, lungmask <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and SimpleITK, to name a few. Besides these, we used deep learning frameworks such as PyTorch, TensorFlow, and Keras to develop and train the models for image classification. Also, the library Optuna was used to facilitate the hyperparameters search <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Model Training</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">For the deep learning models, the key hyperparameters included a learning rate of 0.001, a batch size of 96, a dropout of 0.5, and the Adam optimizer held constant across all models for fair comparison. For the XGBoost model for feature selection, the learning rate was set to 0.1, with a maximum depth of 5 for trees and 100 estimators. We employed a 5-fold cross-validation approach for testing and model evaluation. The baseline methods included a standard logistic regression model and support vector machine for tabular data comparison and several image classifiers such as 2D CNN, 3D CNN, VGG16, ResNet, Inception, and Xception. The optimal parameters for the tabular baseline models were achieved through GridSearch. The image baseline models were fed the CT images alone as well as the fused CT and PET, serving as a comparison point to assess the performance improvements offered by the more complex multi-modal models proposed. The assessment metrics focused on accuracy, precision, recall, and f1-score.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Evaluation Metrics</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">For the classification task, we prioritized evaluation metrics crucial for medical diagnostics: accuracy, precision, recall, and F1-score. Accuracy measures the overall correctness of the model, while precision assesses the proportion of true positives among positive predictions. Recall, or sensitivity, quantifies the model’s ability to identify true positives correctly. The F1-score, a balance of precision and recall, provides a holistic view of the model’s performance. Additionally, we analyzed the confusion matrix to scrutinize false positives and negatives, which are particularly critical in medical applications. These metrics were integral not only for assessing model performance but also for comparing the effectiveness of different imaging inputs, such as standalone CT scans versus integrated fused scans.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Quantitative Analysis</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">Our experimental analysis comprehensively evaluated various machine learning models, spanning tabular, image-based, and multi-modal approaches both separately and in a fused imaging modality. Key findings, detailed in Table 1, reveal distinct performance patterns across these models, evaluated on accuracy, precision, recall, and F1-score.</p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">Baseline tabular models, specifically SVM and Logistic Regression, exhibited solid performance with an accuracy of 77.0%. Image classifiers using solely CT scans showed varied effectiveness; 2D CNNs achieved a 70.0% accuracy, whereas Inception models reached up to 79.0%. Notably, the incorporation of fused CT and PET scans markedly boosted model performance. For instance, VGG16, using fused images, attained an 87.1% accuracy and an 82.5% F1-score, underlining the value of fused imaging.</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">The standout results were observed in multi-modal models. The MedClip model, integrating tabular, genetic, and fused imaging data, achieved an 84.9% accuracy. The BEiT-based model, also utilizing a multi-modal approach, demonstrated a remarkable accuracy of 94.04% with fused data. This represents a significant advancement over the performances achieved using separate CT and PET scans, underscoring the efficacy of our multi-modal, fused imaging methodology. These results not only highlight the potential of advanced AI models in medical diagnostics but also emphasize the transformative impact of integrating diverse data modalities.</p>
</div>
<figure id="S5.T1" class="ltx_table">
<table id="S5.T1.2" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T1.2.1.1" class="ltx_tr">
<th id="S5.T1.2.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T1.2.1.1.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.2.1.1.1.1.1" class="ltx_tr">
<td id="S5.T1.2.1.1.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></td>
</tr>
</table>
</th>
<td id="S5.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T1.2.1.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.2.1.1.2.1.1" class="ltx_tr">
<td id="S5.T1.2.1.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.2.1.1.1.1" class="ltx_text ltx_font_bold">Accuracy</span></td>
</tr>
<tr id="S5.T1.2.1.1.2.1.2" class="ltx_tr">
<td id="S5.T1.2.1.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.2.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</td>
<td id="S5.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T1.2.1.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.2.1.1.3.1.1" class="ltx_tr">
<td id="S5.T1.2.1.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.3.1.1.1.1" class="ltx_text ltx_font_bold">Precision</span></td>
</tr>
<tr id="S5.T1.2.1.1.3.1.2" class="ltx_tr">
<td id="S5.T1.2.1.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.3.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</td>
<td id="S5.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T1.2.1.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.2.1.1.4.1.1" class="ltx_tr">
<td id="S5.T1.2.1.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.4.1.1.1.1" class="ltx_text ltx_font_bold">Recall</span></td>
</tr>
<tr id="S5.T1.2.1.1.4.1.2" class="ltx_tr">
<td id="S5.T1.2.1.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.4.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</td>
<td id="S5.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" style="padding-left:2.0pt;padding-right:2.0pt;">
<table id="S5.T1.2.1.1.5.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T1.2.1.1.5.1.1" class="ltx_tr">
<td id="S5.T1.2.1.1.5.1.1.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.5.1.1.1.1" class="ltx_text ltx_font_bold">F1-Score</span></td>
</tr>
<tr id="S5.T1.2.1.1.5.1.2" class="ltx_tr">
<td id="S5.T1.2.1.1.5.1.2.1" class="ltx_td ltx_nopad_r ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.1.1.5.1.2.1.1" class="ltx_text ltx_font_bold">(%)</span></td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T1.2.2.2" class="ltx_tr">
<th id="S5.T1.2.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="5">Tabular Models</th>
</tr>
<tr id="S5.T1.2.3.3" class="ltx_tr">
<th id="S5.T1.2.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">SVM</th>
<td id="S5.T1.2.3.3.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.3.3.2.1" class="ltx_text ltx_font_bold">77.0 ± 1.7</span></td>
<td id="S5.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.3.3.3.1" class="ltx_text">76.5 ± 1.3</span></td>
<td id="S5.T1.2.3.3.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.3.3.4.1" class="ltx_text ltx_font_bold">76.5 ± 1.8</span></td>
<td id="S5.T1.2.3.3.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.3.3.5.1" class="ltx_text ltx_font_bold">76.0 ± 1.6</span></td>
</tr>
<tr id="S5.T1.2.4.4" class="ltx_tr">
<th id="S5.T1.2.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Logistic Reg.</th>
<td id="S5.T1.2.4.4.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.4.4.2.1" class="ltx_text ltx_font_bold">77.0 ± 1.6</span></td>
<td id="S5.T1.2.4.4.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.4.4.3.1" class="ltx_text ltx_font_bold">79.5 ± 1.6</span></td>
<td id="S5.T1.2.4.4.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.4.4.4.1" class="ltx_text">74.5 ± 1.9</span></td>
<td id="S5.T1.2.4.4.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.4.4.5.1" class="ltx_text">75.0 ± 1.7</span></td>
</tr>
<tr id="S5.T1.2.5.5" class="ltx_tr">
<th id="S5.T1.2.5.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="5">Image Classifier with CT images</th>
</tr>
<tr id="S5.T1.2.6.6" class="ltx_tr">
<th id="S5.T1.2.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2D CNN</th>
<td id="S5.T1.2.6.6.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.6.6.2.1" class="ltx_text">70.0 ± 2.7</span></td>
<td id="S5.T1.2.6.6.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.6.6.3.1" class="ltx_text">64.0 ± 1.8</span></td>
<td id="S5.T1.2.6.6.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.6.6.4.1" class="ltx_text">64.5 ± 2.1</span></td>
<td id="S5.T1.2.6.6.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.6.6.5.1" class="ltx_text">64.0 ± 2.2</span></td>
</tr>
<tr id="S5.T1.2.7.7" class="ltx_tr">
<th id="S5.T1.2.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">3D CNN</th>
<td id="S5.T1.2.7.7.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.7.7.2.1" class="ltx_text">55.5 ± 2.8</span></td>
<td id="S5.T1.2.7.7.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.7.7.3.1" class="ltx_text">44.0 ± 3.1</span></td>
<td id="S5.T1.2.7.7.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.7.7.4.1" class="ltx_text">48.5 ± 2.8</span></td>
<td id="S5.T1.2.7.7.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.7.7.5.1" class="ltx_text">40.0 ± 2.9</span></td>
</tr>
<tr id="S5.T1.2.8.8" class="ltx_tr">
<th id="S5.T1.2.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">VGG16</th>
<td id="S5.T1.2.8.8.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.8.8.2.1" class="ltx_text">75.0 ± 0.7</span></td>
<td id="S5.T1.2.8.8.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.8.8.3.1" class="ltx_text">70.0 ± 0.9</span></td>
<td id="S5.T1.2.8.8.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.8.8.4.1" class="ltx_text">71.5 ± 1.1</span></td>
<td id="S5.T1.2.8.8.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.8.8.5.1" class="ltx_text">70.5 ± 0.5</span></td>
</tr>
<tr id="S5.T1.2.9.9" class="ltx_tr">
<th id="S5.T1.2.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">ResNet</th>
<td id="S5.T1.2.9.9.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.9.9.2.1" class="ltx_text">74.0 ± 1.9</span></td>
<td id="S5.T1.2.9.9.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.9.9.3.1" class="ltx_text">74.1 ± 2.6</span></td>
<td id="S5.T1.2.9.9.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.9.9.4.1" class="ltx_text ltx_font_bold">73.7 ± 1.7</span></td>
<td id="S5.T1.2.9.9.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.9.9.5.1" class="ltx_text ltx_font_bold">71.6 ± 1.8</span></td>
</tr>
<tr id="S5.T1.2.10.10" class="ltx_tr">
<th id="S5.T1.2.10.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Inception</th>
<td id="S5.T1.2.10.10.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.10.10.2.1" class="ltx_text ltx_font_bold">79.0 ± 1.3</span></td>
<td id="S5.T1.2.10.10.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.10.10.3.1" class="ltx_text ltx_font_bold">76.0 ± 1.0</span></td>
<td id="S5.T1.2.10.10.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.10.10.4.1" class="ltx_text">68.0 ± 0.8</span></td>
<td id="S5.T1.2.10.10.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.10.10.5.1" class="ltx_text">70.0 ± 0.9</span></td>
</tr>
<tr id="S5.T1.2.11.11" class="ltx_tr">
<th id="S5.T1.2.11.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Xception</th>
<td id="S5.T1.2.11.11.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.11.11.2.1" class="ltx_text">78.0 ± 1.2</span></td>
<td id="S5.T1.2.11.11.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.11.11.3.1" class="ltx_text">74.0 ± 0.8</span></td>
<td id="S5.T1.2.11.11.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.11.11.4.1" class="ltx_text">70.0 ± 0.6</span></td>
<td id="S5.T1.2.11.11.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.11.11.5.1" class="ltx_text">71.0 ± 0.7</span></td>
</tr>
<tr id="S5.T1.2.12.12" class="ltx_tr">
<th id="S5.T1.2.12.12.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="5">Image Classifier with Fused CT + PET images</th>
</tr>
<tr id="S5.T1.2.13.13" class="ltx_tr">
<th id="S5.T1.2.13.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">2D CNN</th>
<td id="S5.T1.2.13.13.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.13.13.2.1" class="ltx_text">75.0 ± 2.1</span></td>
<td id="S5.T1.2.13.13.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.13.13.3.1" class="ltx_text">73.0 ± 2.6</span></td>
<td id="S5.T1.2.13.13.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.13.13.4.1" class="ltx_text">65.0 ± 2.2</span></td>
<td id="S5.T1.2.13.13.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.13.13.5.1" class="ltx_text">66.5 ± 2.4</span></td>
</tr>
<tr id="S5.T1.2.14.14" class="ltx_tr">
<th id="S5.T1.2.14.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">3D CNN</th>
<td id="S5.T1.2.14.14.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.14.14.2.1" class="ltx_text">60.0 ± 2.6</span></td>
<td id="S5.T1.2.14.14.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.14.14.3.1" class="ltx_text">79.0 ± 2.3</span></td>
<td id="S5.T1.2.14.14.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.14.14.4.1" class="ltx_text">55.0 ± 2.4</span></td>
<td id="S5.T1.2.14.14.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.14.14.5.1" class="ltx_text">45.0 ± 2.5</span></td>
</tr>
<tr id="S5.T1.2.15.15" class="ltx_tr">
<th id="S5.T1.2.15.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">VGG16</th>
<td id="S5.T1.2.15.15.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.15.15.2.1" class="ltx_text ltx_font_bold">87.1 ± 0.8</span></td>
<td id="S5.T1.2.15.15.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.15.15.3.1" class="ltx_text">83.0 ± 2.1</span></td>
<td id="S5.T1.2.15.15.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.15.15.4.1" class="ltx_text ltx_font_bold">82.5 ± 1.9</span></td>
<td id="S5.T1.2.15.15.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.15.15.5.1" class="ltx_text ltx_font_bold">82.5 ± 2.0</span></td>
</tr>
<tr id="S5.T1.2.16.16" class="ltx_tr">
<th id="S5.T1.2.16.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">ResNet</th>
<td id="S5.T1.2.16.16.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.16.16.2.1" class="ltx_text">82.0 ± 1.5</span></td>
<td id="S5.T1.2.16.16.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.16.16.3.1" class="ltx_text ltx_font_bold">86.8 ± 2.2</span></td>
<td id="S5.T1.2.16.16.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.16.16.4.1" class="ltx_text">81.9 ± 1.5</span></td>
<td id="S5.T1.2.16.16.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.16.16.5.1" class="ltx_text">81.3 ± 1.3</span></td>
</tr>
<tr id="S5.T1.2.17.17" class="ltx_tr">
<th id="S5.T1.2.17.17.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Inception</th>
<td id="S5.T1.2.17.17.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.17.17.2.1" class="ltx_text">81.8 ± 0.9</span></td>
<td id="S5.T1.2.17.17.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.17.17.3.1" class="ltx_text">86.5 ± 1.2</span></td>
<td id="S5.T1.2.17.17.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.17.17.4.1" class="ltx_text">82.0 ± 1.1</span></td>
<td id="S5.T1.2.17.17.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.17.17.5.1" class="ltx_text">81.0 ± 0.8</span></td>
</tr>
<tr id="S5.T1.2.18.18" class="ltx_tr">
<th id="S5.T1.2.18.18.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Xception</th>
<td id="S5.T1.2.18.18.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.18.18.2.1" class="ltx_text">81.8 ± 1.1</span></td>
<td id="S5.T1.2.18.18.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.18.18.3.1" class="ltx_text">86.5 ± 1.0</span></td>
<td id="S5.T1.2.18.18.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.18.18.4.1" class="ltx_text">82.0 ± 0.9</span></td>
<td id="S5.T1.2.18.18.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.18.18.5.1" class="ltx_text">81.0 ± 1.1</span></td>
</tr>
<tr id="S5.T1.2.19.19" class="ltx_tr">
<th id="S5.T1.2.19.19.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="5">Multi-modal MedClip Model (CT+PET+EHR+Genetic)</th>
</tr>
<tr id="S5.T1.2.20.20" class="ltx_tr">
<th id="S5.T1.2.20.20.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CT only</th>
<td id="S5.T1.2.20.20.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.20.20.2.1" class="ltx_text">78.2 ± 1.9</span></td>
<td id="S5.T1.2.20.20.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.20.20.3.1" class="ltx_text">79.0 ± 1.8</span></td>
<td id="S5.T1.2.20.20.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.20.20.4.1" class="ltx_text">77.5 ± 2.3</span></td>
<td id="S5.T1.2.20.20.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.20.20.5.1" class="ltx_text">78.0 ± 2.1</span></td>
</tr>
<tr id="S5.T1.2.21.21" class="ltx_tr">
<th id="S5.T1.2.21.21.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Separate CT/PET</th>
<td id="S5.T1.2.21.21.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.21.21.2.1" class="ltx_text">76.0 ± 1.7</span></td>
<td id="S5.T1.2.21.21.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.21.21.3.1" class="ltx_text">76.0 ± 2.1</span></td>
<td id="S5.T1.2.21.21.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.21.21.4.1" class="ltx_text">75.5 ± 1.3</span></td>
<td id="S5.T1.2.21.21.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.21.21.5.1" class="ltx_text">75.5 ± 1.5</span></td>
</tr>
<tr id="S5.T1.2.22.22" class="ltx_tr">
<th id="S5.T1.2.22.22.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Fused CT/PET</th>
<td id="S5.T1.2.22.22.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.22.22.2.1" class="ltx_text ltx_font_bold">84.9 ± 0.9</span></td>
<td id="S5.T1.2.22.22.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.22.22.3.1" class="ltx_text ltx_font_bold">89.5 ± 1.1</span></td>
<td id="S5.T1.2.22.22.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.22.22.4.1" class="ltx_text ltx_font_bold">83.0 ± 1.3</span></td>
<td id="S5.T1.2.22.22.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.22.22.5.1" class="ltx_text ltx_font_bold">83.5 ± 1.5</span></td>
</tr>
<tr id="S5.T1.2.23.23" class="ltx_tr">
<th id="S5.T1.2.23.23.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;" colspan="5">Multi-modal BEiT Model (CT+PET+EHR+Genetic)</th>
</tr>
<tr id="S5.T1.2.24.24" class="ltx_tr">
<th id="S5.T1.2.24.24.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;">CT only</th>
<td id="S5.T1.2.24.24.2" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.24.24.2.1" class="ltx_text">85.6 ± 1.7</span></td>
<td id="S5.T1.2.24.24.3" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.24.24.3.1" class="ltx_text">85.5 ± 2.1</span></td>
<td id="S5.T1.2.24.24.4" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.24.24.4.1" class="ltx_text">85.5 ± 2.3</span></td>
<td id="S5.T1.2.24.24.5" class="ltx_td ltx_align_center ltx_border_t" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.24.24.5.1" class="ltx_text">85.5 ± 2.6</span></td>
</tr>
<tr id="S5.T1.2.25.25" class="ltx_tr">
<th id="S5.T1.2.25.25.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" style="padding-left:2.0pt;padding-right:2.0pt;">Separate CT/PET</th>
<td id="S5.T1.2.25.25.2" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.25.25.2.1" class="ltx_text">76.0 ± 1.4</span></td>
<td id="S5.T1.2.25.25.3" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.25.25.3.1" class="ltx_text">76.0 ± 2.5</span></td>
<td id="S5.T1.2.25.25.4" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.25.25.4.1" class="ltx_text">75.5 ± 1.7</span></td>
<td id="S5.T1.2.25.25.5" class="ltx_td ltx_align_center" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.25.25.5.1" class="ltx_text">75.5 ± 1.9</span></td>
</tr>
<tr id="S5.T1.2.26.26" class="ltx_tr">
<th id="S5.T1.2.26.26.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;">Fused CT/PET</th>
<td id="S5.T1.2.26.26.2" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.26.26.2.1" class="ltx_text ltx_font_bold">94.0 ± 0.7</span></td>
<td id="S5.T1.2.26.26.3" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.26.26.3.1" class="ltx_text ltx_font_bold">95.0 ± 1.2</span></td>
<td id="S5.T1.2.26.26.4" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.26.26.4.1" class="ltx_text ltx_font_bold">94.0 ± 1.0</span></td>
<td id="S5.T1.2.26.26.5" class="ltx_td ltx_align_center ltx_border_bb" style="padding-left:2.0pt;padding-right:2.0pt;"><span id="S5.T1.2.26.26.5.1" class="ltx_text ltx_font_bold">94.0 ± 1.1</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T1.3.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Comparison of all models evaluated with varied input of CT alone versus fused CT/PET.</figcaption>
</figure>
<figure id="S5.F3" class="ltx_figure"><img src="/html/2409.18715/assets/F1_Models.png" id="S5.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="315" height="179" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F3.2.1.1" class="ltx_text ltx_font_bold">Fig. 3</span>: </span>F1-score comparison of different models using fused PET/CT images versus CT images alone.</figcaption>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Qualitative Analysis</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">Throughout all the assessments, the implementation of fused imaging data notably improved model performance metrics across the board, underscoring the value of integrating multiple imaging modalities for enhanced diagnostic inference, as illustrated in Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Quantitative Analysis ‣ 5 Results and Discussion ‣ Multi-modal Medical Image Fusion For Non-Small Cell Lung Cancer Classification" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. The deep learning models, particularly the one based on the BEiT architecture, effectively leveraged the rich, multi-modal data, translating into superior quantitative outcomes. It is also important to note that the multi-modal model surpasses the imaging and tabular model in performance, justifying the need to integrate data from different modalities to enhance the diagnosis accuracy. Moreover, even when trying the same multi-modal model but processing the CT and PET scans separately through feature extraction and then combining the results always performed worse than using the fused image generated by the VGG19 fusion model, as it can be concluded that the fusion process is necessary and aids in the improved performance of the overall model. Regarding the pre-processing of the imaging scans, the deep CNN auto-encoder helped a lot in de-noising the PET scan, and the results were very noticeable. Similarly, for the CT scans, the pre-processing was applied to filter only the slices containing lung pixels, and masking and improving the contrast helped improve the results.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Discussion</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">The quantitative leap in performance metrics with fused imaging data suggests that models benefit from the complementary information available in different imaging modalities. It can be inferred that the spatial resolution of CT images combined with the metabolic information from PET scans provides a more holistic view of the pathology, which is effectively exploited by the more complex architectures like VGG16, Inception, and the BEiT models. This illustrates that although the BEit model uses fewer parameters than MedClip and VGG16, it had a better performance.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">The SVM and Logistic Regression models, while less effective than image classifiers, offer competitive performance on tabular data, indicating that traditional machine learning models remain valuable for structured data analysis. The lower performance of 3D CNNs across all metrics suggests that for the dataset at hand, the additional complexity introduced by 3D convolution may not capture the essential features as effectively as other architectures, possibly due to overfitting or the need for more training data.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">The superior results of the BEiT-based model highlight the potential of transformer architectures in handling complex, multi-modal datasets. Transformers’ ability to model long-range dependencies and integrate disparate data sources is particularly beneficial for medical imaging tasks, where the context and subtlety of features are crucial for accurate diagnosis.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Limitations</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">The primary limitation of our multi-modal approach is the constrained dataset size, as comprehensive data across all modalities (clinical, imaging, and genomic) was available for only a limited subset of patients. This reduction in dataset size potentially affects the generalizability and robustness of our models and necessitates the use of supplemental data from a secondary source to address class imbalances. Additionally, the complexity of integrating diverse data sources, while beneficial for performance, leads to the ’black box’ issue, diminishing the interpretability of the model’s decision-making process. This aspect is particularly critical in clinical settings where transparency is essential for clinician trust and model applicability. Future efforts might focus on enhancing model transparency through explainable AI techniques.</p>
</div>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In conclusion, this paper made significant strides in NSCLC diagnostics by integrating fused CT and PET scans with genomics and clinical data. Our key contributions include the development of a novel image fusion technique that combines anatomical and metabolic information, the innovative application of advanced models like MedClip and BEiT for multi-modal data analysis, and the implementation of sophisticated pre-processing and denoising techniques for CT and PET scans. These contributions led to a substantial improvement in NSCLC classification, achieving a remarkable accuracy of 94.04%. This success demonstrates the transformative potential of integrating diverse data sources and leveraging state-of-the-art transformer-based architectures in medical diagnostics. The high accuracy achieved underscores the critical importance of fused imaging and comprehensive data integration in providing a more detailed and accurate lung cancer analysis. These findings mark a significant leap forward in the field, setting a new standard for future research to develop even more sophisticated models and richer datasets, ultimately leading to more precise and reliable diagnostic tools in oncology.</p>
</div>
<div id="S7.p2" class="ltx_para">
<p id="S7.p2.1" class="ltx_p"><span id="S7.p2.1.1" class="ltx_text ltx_font_bold">Future Work.</span> Future work will focus on expanding data sources by incorporating additional modalities such as proteomics and metabolomics, conducting longitudinal studies to track tumor progression, and validating model performance through real-world clinical trials. Enhancing model interpretability for clinical use, integrating our approach into existing clinical workflows, and optimizing for scalability and computational efficiency are also key areas for further research. These efforts aim to build on our findings to create more robust, accurate, and clinically applicable diagnostic tools. The ultimate goal is to improve patient outcomes in NSCLC and other cancers, a significant and impactful direction for future research.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
WHO,

</span>
<span class="ltx_bibblock">“World health organization: Lung cancer fact sheet,” https://www.who.int/news-room/fact-sheets/detail/lung-cancer.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Shubham Dodia, B Annappa, and Padukudru A Mahesh,

</span>
<span class="ltx_bibblock">“Recent advancements in deep learning based lung cancer detection: A systematic review,”

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Engineering Applications of Artificial Intelligence</span>, vol. 116, pp. 105490, 2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Iftikhar Naseer, Tehreem Masood, Sheeraz Akram, Arfan Jaffar, Muhammad Rashid, and Muhammad Amjad Iqbal,

</span>
<span class="ltx_bibblock">“Lung cancer detection using modified alexnet architecture and support vector machine.,”

</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Computers, Materials &amp; Continua</span>, vol. 74, no. 1, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Tafadzwa L Chaunzwa, Ahmed Hosny, Yiwen Xu, Andrea Shafer, Nancy Diao, Michael Lanuti, David C Christiani, Raymond H Mak, and Hugo JWL Aerts,

</span>
<span class="ltx_bibblock">“Deep learning classification of lung cancer histology using ct images,”

</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Scientific reports</span>, vol. 11, no. 1, pp. 1–12, 2021.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Mehdi Amini, Mostafa Nazari, Isaac Shiri, Ghasem Hajianfar, Mohammad Reza Deevband, Hamid Abdollahi, Hossein Arabi, Arman Rahmim, and Habib Zaidi,

</span>
<span class="ltx_bibblock">“Multi-level multi-modality (pet and ct) fusion radiomics: prognostic modeling for non-small cell lung carcinoma,”

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Physics in Medicine &amp; Biology</span>, vol. 66, no. 20, pp. 205017, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Yaser Alduais, Haijun Zhang, Fan Fan, Jing Chen, and Baoan Chen,

</span>
<span class="ltx_bibblock">“Non-small cell lung cancer (nsclc): A review of risk factors, diagnosis, and treatment,”

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Medicine</span>, vol. 102, no. 8, pp. e32899, 2023.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Jianxin Feng, Jun Jiang, et al.,

</span>
<span class="ltx_bibblock">“Deep learning-based chest ct image features in diagnosis of lung cancer,”

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Computational and Mathematical Methods in Medicine</span>, vol. 2022, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Azka Khoirunnisa, Didit Adytia, et al.,

</span>
<span class="ltx_bibblock">“Implementation of crnn method for lung cancer detection based on microarray data,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">JOIV: International Journal on Informatics Visualization</span>, vol. 7, no. 2, pp. 600–605, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jianjun Zhang, Kathryn A Gold, Heather Y Lin, Stephen G Swisher, Yan Xing, J Jack Lee, Edward S Kim, and William N William Jr,

</span>
<span class="ltx_bibblock">“Relationship between tumor size and survival in non–small-cell lung cancer (nsclc): an analysis of the surveillance, epidemiology, and end results (seer) registry,”

</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Journal of Thoracic Oncology</span>, vol. 10, no. 4, pp. 682–690, 2015.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Yong Han, Yuan Ma, Zhiyuan Wu, Feng Zhang, Deqiang Zheng, Xiangtong Liu, Lixin Tao, Zhigang Liang, Zhi Yang, Xia Li, et al.,

</span>
<span class="ltx_bibblock">“Histologic subtype classification of non-small cell lung cancer using pet/ct images,”

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">European journal of nuclear medicine and molecular imaging</span>, vol. 48, pp. 350–360, 2021.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ramesh Paudyal, Akash D Shah, Oguz Akin, Richard KG Do, Amaresha Shridhar Konar, Vaios Hatzoglou, Usman Mahmood, Nancy Lee, Richard J Wong, Suchandrima Banerjee, et al.,

</span>
<span class="ltx_bibblock">“Artificial intelligence in ct and mr imaging for oncological applications,”

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Cancers</span>, vol. 15, no. 9, pp. 2573, 2023.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun,

</span>
<span class="ltx_bibblock">“Medclip: Contrastive learning from unpaired medical images and text,”

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2210.10163</span>, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei,

</span>
<span class="ltx_bibblock">“Beit: Bert pre-training of image transformers,”

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.08254</span>, 2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Shaimaa Bakr, Olivier Gevaert, Sebastian Echegaray, Kelsey Ayers, Mu Zhou, Majid Shafiq, Hong Zheng, Jalen Anthony Benson, Weiruo Zhang, Ann NC Leung, et al.,

</span>
<span class="ltx_bibblock">“A radiogenomic dataset of non-small cell lung cancer,”

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Scientific data</span>, vol. 5, no. 1, pp. 1–9, 2018.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
HJWL Aerts, E Rios Velazquez, RT Leijenaar, Chintan Parmar, Patrick Grossmann, S Cavalho, Johan Bussink, René Monshouwer, Benjamin Haibe-Kains, Derek Rietveld, et al.,

</span>
<span class="ltx_bibblock">“Data from nsclc-radiomics,”

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">The cancer imaging archive</span>, 2015.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Ping Li, S Wang, T Li, J Lu, Y HuangFu, and D Wang,

</span>
<span class="ltx_bibblock">“A large-scale ct and pet/ct dataset for lung cancer diagnosis [dataset],”

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">The cancer imaging archive</span>, 2020.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Johannes Hofmanninger, Forian Prayer, Jeanny Pan, Sebastian Röhrich, Helmut Prosch, and Georg Langs,

</span>
<span class="ltx_bibblock">“Automatic lung segmentation in routine imaging is primarily a data diversity problem, not a methodology problem,”

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">European Radiology Experimental</span>, vol. 4, no. 1, Aug. 2020.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama,

</span>
<span class="ltx_bibblock">“Optuna: A next-generation hyperparameter optimization framework,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</span>, 2019, pp. 2623–2631.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2409.18714" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2409.18715" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2409.18715">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2409.18715" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2409.18716" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun Oct  6 01:02:32 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
