<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents</title>
<!--Generated on Tue Aug  6 09:02:26 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2408.03047v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S1" title="In OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S2" title="In OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S3" title="In OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>System design</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S3.SS1" title="In 3 System design ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Requirement analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S3.SS2" title="In 3 System design ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>System architecture</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S4" title="In OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Demonstration</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S4.SS1" title="In 4 Demonstration ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S4.SS2" title="In 4 Demonstration ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Can “AI” be your president?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S4.SS3" title="In 4 Demonstration ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Assist the visually impaired</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S5" title="In OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">OpenOmni: A Collaborative Open Source Tool for 
<br class="ltx_break"/>Building Future-Ready Multimodal Conversational Agents</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span class="ltx_text ltx_font_bold" id="id1.1.id1">Qiang Sun<sup class="ltx_sup" id="id1.1.id1.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id2.2.id2">Yuanyi Luo<sup class="ltx_sup" id="id2.2.id2.1">2</sup></span>,
<span class="ltx_text ltx_font_bold" id="id3.3.id3">Sirui Li<sup class="ltx_sup" id="id3.3.id3.1">3</sup></span>,
<span class="ltx_text ltx_font_bold" id="id4.4.id4">Wenxiao Zhang<sup class="ltx_sup" id="id4.4.id4.1">1</sup></span>,
<span class="ltx_text ltx_font_bold" id="id5.5.id5">Wei Liu<sup class="ltx_sup" id="id5.5.id5.1">1</sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id6.6.id6">1</sup>The University of Western Australia, Perth, WA, Australia,

<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.id7">2</sup>Harbin Institute of Technology, Harbin, China,
<sup class="ltx_sup" id="id8.8.id8">3</sup>Murdoch University, Perth, WA, Australia,

<br class="ltx_break"/>
<span class="ltx_text ltx_font_bold" id="id9.9.id9">Correspondence:</span> <a class="ltx_ref ltx_href" href="mailto:pascal.sun@research.uwa.edu.au" title="">pascal.sun@research.uwa.edu.au</a>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id10.id1">Multimodal conversational agents are highly desirable because they offer natural and human-like interaction.
However, there is a lack of comprehensive end-to-end solutions to support collaborative development and benchmarking.
While proprietary systems like GPT-4o and Gemini demonstrating impressive integration of audio, video, and text with response times of 200-250ms, challenges remain in balancing latency, accuracy, cost, and data privacy.
To better understand and quantify these issues, we developed <span class="ltx_text ltx_font_bold" id="id10.id1.1">OpenOmni</span>, an open-source, end-to-end pipeline benchmarking tool that integrates advanced technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented Generation, Large Language Models, along with the ability to integrate customized models.
OpenOmni supports local and cloud deployment, ensuring data privacy and supporting latency and accuracy benchmarking.
This flexible framework allows researchers to customize the pipeline, focusing on real bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can significantly enhance applications like indoor assistance for visually impaired individuals, advancing human-computer interaction.
Our demonstration video is available <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/watch?v=zaSiT3clWqY" title="">https://www.youtube.com/watch?v=zaSiT3clWqY</a>, demo is available via <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openomni.ai4wa.com" title="">https://openomni.ai4wa.com</a>, code is available via <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/AI4WA/OpenOmniFramework" title="">https://github.com/AI4WA/OpenOmniFramework</a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">OpenOmni: A Collaborative Open Source Tool for 
<br class="ltx_break"/>Building Future-Ready Multimodal Conversational Agents</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">
Qiang Sun<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">1</sup>,
Yuanyi Luo<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">2</sup>,
Sirui Li<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">3</sup>,
Wenxiao Zhang<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4">1</sup>,
Wei Liu<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.5">1</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1">1</sup>The University of Western Australia, Perth, WA, Australia,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">2</sup>Harbin Institute of Technology, Harbin, China,
<sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.2">3</sup>Murdoch University, Perth, WA, Australia,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1">
<span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.4.4.1.1">Correspondence:</span> <a class="ltx_ref ltx_href" href="mailto:pascal.sun@research.uwa.edu.au" title="">pascal.sun@research.uwa.edu.au</a></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<figure class="ltx_figure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="274" id="S0.F1.g1" src="x1.png" width="647"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture Design for OpenOmni Framework</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib14" title="">2023</a>); Minaee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib6" title="">2024</a>)</cite> demonstrated remarkable capabilities in understanding user intentions and following instructions.
However, text-only human-computer interaction (HCI) is often insufficient <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib13" title="">2023</a>)</cite>.
OpenAI recently released their new flagship model, GPT-4o, which can reason across audio, video, and text in real time.
The impressive performance is achieved with response times between 200-250ms, which is acceptable for large-scale applications<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/hello-gpt-4o/" title="">https://openai.com/index/hello-gpt-4o/</a></span></span></span>.
Google soon followed with their latest multimodal competitors, indicating a clear trend towards multimodal generative models and applications<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://blog.google/products/gemini/" title="">https://blog.google/products/gemini/</a></span></span></span>.
LLaVA <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib5" title="">2023</a>)</cite> is one of the early publicly available solutions for multimodal large models integrating text and images.
However, there is currently no open source end-to-end conversational agents implementation and demonstration publicly available online.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The ideal form of multimodal HCI should mirror human interactions, incorporating video and audio inputs with audio outputs.
Despite the availability of various modular components, there is no comprehensive integrated open-source implementation to foster research and innovation in this field.
Integrating existing models—such as audio speech recognition (<span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Speech2Text</span>), multimodal large models (MLMs), and text-to-speech synthesis (<span class="ltx_text ltx_font_italic" id="S1.p2.1.2">TTS</span>)—into a multimodal conversation system reveals significant challenges in balancing latency and accuracy.
Historically, accuracy has been a major hurdle; however, advancements in large language models (LLMs) have substantially improved contextual relevance.
The main challenge is reducing end-to-end latency while maintaining accuracy. While OpenAI and Google have shown it’s possible, the open-source community lacks alternatives that replicate this performance.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Another issue is data privacy.
The GPT-4 family of solutions also raise concerns about cost and data privacy.
Since GPT-4 is closed-source, users must upload their data to the server via a paid API, raising privacy issues. The privacy policy of GPT<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.gpt.com.au/privacy-policy" title="">https://www.gpt.com.au/privacy-policy</a></span></span></span> informs users that various forms of personal information, including account details, user content, communication information, and social media data, are collected when users create accounts to access ChatGPT services <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib12" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To support the rapid and responsible development of this new HCI format, establishing robust evaluation and benchmarking protocols is essential. For instance, if a user initiates a conversation in a sad and urgent tone, the system should respond appropriately with patient.
Evaluating this interaction is crucial and challenging for widespread adoption.
Our project aims to bridge these gaps by:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">Developing an open-source framework for end-to-end customizable conversational agents.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Providing options for a fully local or controllable end-to-end multimodal conversation solution, addressing privacy concerns.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Setting up tools to annotate and benchmark latency and accuracy performance, allowing rapid proof of concept development and research.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To achieve this goal, we propose the <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">OpenOmni</span> framework, an open-source, end-to-end multimodal pipeline that integrates advanced technologies such as Speech-to-Text (Speech2Text), Emotion Detection, Retrieval Augmented Generation (RAG), Large Language Models (LLMs), and Text-to-Speech (TTS). The framework gathers video and audio data from cameras and microphones, processes it through a customizable agents pipeline, and responds via a speaker, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S0.F1" title="Figure 1 ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">1</span></a>.
<span class="ltx_text ltx_font_bold" id="S1.p5.1.2">OpenOmni</span> can be deployed on a local server, ensuring secure data management and addressing privacy concerns.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">For research purposes, it includes tools for easy annotation and benchmarking, offering real-time monitoring and performance evaluation of latency.
Users can annotate individual components and entire conversations, generating comprehensive benchmark reports to identify bottlenecks.
The open-source nature of OpenOmni allows for adaptation across different application domains, such as aged care, personal assistant, etc.
Each pipeline component can be enabled or disabled based on specific use cases, facilitating flexible and efficient deployment.
Additionally, the framework supports the easy addition of extra models, enabling comparisons and further experimentation.
The <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">OpenOmni</span> framework allows researchers to focus on solving critical bottlenecks without reinventing the wheel, fostering innovation in multimodal conversational agents.
It enables rapid proof-of-concept development, such as indoor conversational robots assisting visually impaired individuals.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related works</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Solution options</span></p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="256" id="S2.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Traditional <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.F2.2.1">divide-and-conquer</span> end-to-end multimodal conversation system</figcaption>
</figure>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Traditional end-to-end multimodal conversation systems, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S2.F2" title="Figure 2 ‣ 2 Related works ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">2</span></a>, typically use a <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p2.1.1">divide-and-conquer</span> strategy, splitting the process into sub-tasks: speech-to-text (automatic speech recognition), image-to-text, text generation, and text-to-speech <cite class="ltx_cite ltx_citemacro_cite">Kusal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib2" title="">2022</a>)</cite>. Speech-to-text converts spoken language into text, while image-to-text generates textual descriptions of images.
Text generation, often powered by large language models, produces contextually appropriate responses, and text-to-speech converts these responses back into spoken language.
These core components form the backbone of the conversational pipeline.
Image-to-text adds essential context, enhancing natural human-computer interaction, and additional tasks like emotion detection tailor responses to the user’s emotional state.
A safe guard module can optionally be integrated to ensure responses are appropriate, non-harmful, and controllable, maintaining interaction integrity, especially in sensitive scenarios.
While this modular approach allows for optimization of individual components, the accumulated latency and accuracy errors can render the end-to-end system impractical for real-world use.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="359" id="S2.F3.g1" src="x3.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Our assumptions about how the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.F3.2.1">fully end-to-end</span> model: GPT-4o works</figcaption>
</figure>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">While GPT-4o is marketed as a <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p3.1.1">fully end-to-end</span> model, where inputs are video, audio or texts and outputs are audio, images or text, its technical details are unreleased.
We assume, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S2.F3" title="Figure 3 ‣ 2 Related works ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">3</span></a>, that audio and video frames are fed into modules generating text, audio, and image outputs.
The demonstration video suggests GPT-4o has memory capabilities, but specifics and limitations are unclear.
It is also unknown if the system can directly integrate external private data.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Unlike the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p4.1.1">divide-and-conquer</span> approach, a <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p4.1.2">fully end-to-end</span> neural network can incorporate more contextual information, such as tone, multiple speakers, and background noises, resulting in more flexible outputs.
This approach can theoretically reduce latency by eliminating orchestration bottlenecks.
However, both solutions face significant challenges due to immense data I/O, especially from video. Video files are large, straining servers and models, increasing computational costs, and causing latency from data transfer and model inference.
Real-time conversation requires streaming processing, posing further latency challenges.
In OpenAI’s demonstration<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/watch?v=RI-BxtCx32s" title="">https://www.youtube.com/watch?v=RI-BxtCx32s</a></span></span></span>, a USB-C connection to an iPhone was used to ensure a <span class="ltx_text ltx_font_bold" id="S2.p4.1.3">stable</span> internet connection, highlighting these issues.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="352" id="S2.F4.g1" src="x4.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span><span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.F4.3.1">Hybrid</span> solution via the combination of <span class="ltx_text ltx_font_italic" id="S2.F4.4.2">image2text</span> and end-to-end voice model</figcaption>
</figure>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">Recently, Kyutai, a technology company, from France released a planned open-source, fully end-to-end multimodal conversational AI called <span class="ltx_text ltx_font_italic" id="S2.p5.1.1">Moshi</span> <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://kyutai.org/" title="">https://kyutai.org/</a></span></span></span>.
This model supports text and audio modalities, excluding images, and claims to achieve an end-to-end latency of 200ms.
We can integrate the video modality via an <span class="ltx_text ltx_font_italic" id="S2.p5.1.2">Image2Text</span> <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib3" title="">2021</a>)</cite> module into <span class="ltx_text ltx_font_italic" id="S2.p5.1.3">Moshi</span>, creating a <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p5.1.4">Hybrid</span> solution, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S2.F4" title="Figure 4 ‣ 2 Related works ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">4</span></a>, that combines the <span class="ltx_text ltx_font_italic" id="S2.p5.1.5">divide-and-conquer</span> and <span class="ltx_text ltx_font_italic" id="S2.p5.1.6">fully end-to-end</span> approaches.
Another feasible <span class="ltx_text ltx_font_bold ltx_font_italic" id="S2.p5.1.7">Hybrid</span> solution is to use speech-to-text to convert audio into text, then feed this text along with video (processed into image sequences) to a vision language model, which generates text responses.
These responses can then be processed through text-to-speech, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S2.F2" title="Figure 2 ‣ 2 Related works ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">2</span></a> via the <span class="ltx_text ltx_font_italic" id="S2.p5.1.8">Vision LLM</span> line.</p>
</div>
<figure class="ltx_figure" id="S2.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="S2.F5.g1" src="x5.png" width="498"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Constraint triangle for real-world applicability in multimodal conversational agent development</figcaption>
</figure>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Multimodal end-to-end conversational agents, like OpenAI’s GPT-4, show promise, but large-scale application is challenging due to the need to balance latency, accuracy, and cost.
Generating real-time responses between 200-400 ms is difficult. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S2.F5" title="Figure 5 ‣ 2 Related works ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">5</span></a>, the primary goal is to reduce latency and cost while improving accuracy, enhancing the real-world applicability of conversational agents.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p7">
<p class="ltx_p" id="S2.p7.1"><span class="ltx_text ltx_font_bold" id="S2.p7.1.1">Evaluation metrics</span></p>
</div>
<div class="ltx_para" id="S2.p8">
<p class="ltx_p" id="S2.p8.1">To ensure efficient and effective collaboration, consistent and comparable evaluation metrics are essential. For speech-to-text, the Word Error Rate (WER) <cite class="ltx_cite ltx_citemacro_cite">Roy (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib8" title="">2021</a>)</cite> measures transcription accuracy, with a lower WER indicating better performance. Text-to-speech evaluation includes objective metrics like the Mean Opinion Score (MOS) <cite class="ltx_cite ltx_citemacro_cite">Streijl et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib10" title="">2016</a>)</cite> for naturalness and intelligibility, and the Signal-to-Noise Ratio (SNR) <cite class="ltx_cite ltx_citemacro_cite">Plapous et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib7" title="">2006</a>)</cite> for clarity, as well as subjective human ratings. Text generation is the most challenging to evaluate, using metrics like BLEU, ROUGE, and METEOR <cite class="ltx_cite ltx_citemacro_cite">Evtikhiev et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib1" title="">2023</a>)</cite>, which compare generated text to references but may not fully capture response quality and relevance. Evaluating text generation often requires large-scale datasets, which are not always available.
These metrics are widely adopted by the research community, including OpenAI. However, real-world applications require evaluation in production environments, considering diverse factors beyond these metrics. For instance, an aged care conversational agent should avoid sensitive topics that may be specific to each individual. Subjective opinions vary by region, highlighting the need for customizable and innovative automatic or semi-automatic evaluation approaches for conversational agents.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>System design</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Requirement analysis</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The system receives audio and video input, produces audio as the output.
Initially, we need two modules: one to collect audio and video data from the microphone and camera, and another to play audio through a speaker.
These <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Client</span> modules should support diverse devices, such as a smartphone, a laptop, or a Raspberry Pi.
The collected data will then be fed to a server.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The server, referred to as <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">API</span>, should manage audio, video data, and metadata. It should have access to a storage layer that includes a relational database, file management, and a graph database for potential GraphRAG integration. While the <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.2">API</span> can reside on the same instance as the <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.3">Client</span> module, we prefer them to be separate for better adaptability. This separation introduces the challenge of <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.4">sharing large volumes of data between modules</span>.
If the <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.5">API</span> is cloud-based, the audio and video data need to be uploaded to the cloud, for example using AWS S3, Azure Blob Storage, or Google Cloud Storage.
However, the upload process can become a bottleneck, making the data transfer time-consuming. If the server is local, within the same network as the <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.6">Client</span>, transfer latency will be reduced. However, this setup requires running the large language model locally, addressing data ownership and privacy concerns but potentially increasing model inference latency and compromising accuracy due to limited computing resources.
Another solution is edge computing, where video data is pre-processed on edge devices and summarized for the <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.7">API</span>. While this can be a research direction, data compression may cause information loss and reduce end-to-end performance.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">The pipeline components will need modification if developers want to adopt the framework and integrate with their work. To ensure flexibility, this part should be an independent module that can run locally or in the cloud. Researchers and developers should be able to easily integrate new components into this <span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Agent</span> module, further challenging the sharing of large datasets between modules.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">Lastly, we want to generate benchmarks to understand the latency and accuracy performance of the entire pipeline. For tasks that are hard to evaluate automatically, such as determining the appropriateness of the LLM response, we propose and develop an annotation module to allow human annotators to easily evaluate results and generate benchmark reports.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>System architecture</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Based on the requirements, we designed our system as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S0.F1" title="Figure 1 ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">1</span></a>. The system is divided into five modules: <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">Client</span>, <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.2">API</span>, <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.3">Storage</span>, <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.4">User Interface</span>, and <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.5">Agent</span>, all primarily developed in Python.
The <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.6">Client</span> module includes two submodules: the <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.7">Listener</span> for collecting video and audio data, and the <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.8">Responder</span> for playing audio. The <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.9">Storage</span> module consists of file storage for media, a relational database (PostgreSQL) for metadata, and a graph database (Neo4j) for potential GraphRAG integration.
The <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.10">API</span> module, built with the Django framework, extends Django’s admin interface and permission control system to develop the benchmark and annotation interface. Django’s maturity and large support community make it ideal for production development.
The <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.11">Agent</span> module, also in Python, includes all agent related submodules, allowing deployment on suitable compute nodes without altering the architecture. Communication between the <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.12">Client</span>, <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.13">API</span>, and <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.14">Agent</span> modules will be via RESTful endpoints.
For <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.15">sharing large data between modules</span>, local deployments (e.g., <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.16">Client</span> on Raspberry Pi, <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.17">API</span> and <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.18">Agent</span> on local servers) will use FTP for file synchronization. In cloud solutions (e.g., AWS), files will be uploaded to AWS S3<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/s3/" title="">https://aws.amazon.com/s3/</a></span></span></span>, triggering a Lambda function to download files to an AWS Elastic File Storage (EFS) <span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aws.amazon.com/efs/" title="">https://aws.amazon.com/efs/</a></span></span></span> shared by the <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.19">API</span> and <span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.20">Agent</span> modules.
We use <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.21">Docker</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.22">Docker Compose</span> to manage all modules, allowing easy setup with a single <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.23">docker compose up</span> command.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Demonstration</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Most multimodal question answering datasets focus on multiple-choice questions rather than open-ended conversations <cite class="ltx_cite ltx_citemacro_cite">Sundar and Heck (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib11" title="">2022</a>)</cite>.
Some, like Image-Chat <cite class="ltx_cite ltx_citemacro_cite">Shuster et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib9" title="">2018</a>)</cite>, involve multimodal conversations with images as extra input, but the output is often multiple-choice or text-based <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#bib.bib4" title="">2022</a>)</cite>.
A major hurdle in developing multimodal conversational agents is the lack of appropriate datasets.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">While there is no shortage of data from human-human interactions or extracted from movies and YouTube videos, we lack efficient methods to organize this data into structured datasets.
For specific domain applications, collecting data from human interactions and extracting datasets to train systems would be beneficial, allowing the agents to mimic human behavior.
Our OpenOmni Framework provides both capabilities: extracting conversational datasets from videos and testing them through the pipeline to evaluate agents’ responses, or collecting data from real-world scenarios to generate datasets for further research.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Can “AI” be your president?</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">One intensive conversational scenario is a debate. We extracted segments from the US Presidential Debate 2024 between Biden and Trump<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.youtube.com/watch?v=-v-8wJkmwBY" title="">https://www.youtube.com/watch?v=-v-8wJkmwBY</a></span></span></span>, focusing on Biden addressing the public and handling questions. These segments were fed into our pipeline to evaluate its performance under different configurations: OpenAI Whisper for speech-to-text, GPT-4o vision model, and text-to-speech (GPT4O_ETE); a locally deployed quantization LLM with Whisper, text-to-speech, and our emotion detection model for video input (QuantizationLLM_ETE); a version using HuggingFace LLM for inference (HF_ETE); and a version using only Whisper, GPT-3.5, and text-to-speech, ignoring the video modality (GPT35_ETE).
We ran the <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.1">Agent</span> modules on an NVIDIA-3080 GPU with 12GB memory.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="264" id="S4.F6.g1" src="extracted/5764442/assets/local_version.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Screenshot of the end-to-end latency benchmark statistics for the setup: <span class="ltx_text ltx_font_italic" id="S4.F6.2.1">Local Whisper, Emotion Detection, Quantization LLM, and OpenAI Text-to-Speech</span>. This visualization is one example of the generated benchmark report; you can customize it or explore more details within our demo.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The latency benchmark statistics are automatically generated. For example, the GPT4O_ETE configuration has an average latency of 45 seconds, with the GPT-4o vision model accounting for 31 seconds.
The fastest configuration is GPT35_ETE, averaging around 15 seconds, with most of the time consumed by the text-to-speech part, because the generated content is quite long and comprehensive.
The slowest configuration is HF_ETE, taking around 189 seconds, with the LLM model inference step taking the longest time.
QuantizationLLM_ETE takes an average of 60 seconds, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S4.F6" title="Figure 6 ‣ 4.2 Can “AI” be your president? ‣ 4 Demonstration ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">6</span></a>, with the LLM model inference averaging 28 seconds and our emotion detection model averaging around 10 seconds.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="288" id="S4.F7.g1" src="extracted/5764442/assets/gpt4oaccuracy.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Screenshot of annotated overall conversation accuracy statistics and comments for each conversation within GPT4O_ETE. Scores range from 0 to 5.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">After annotation with our interface, accuracy statistics are automatically generated. As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S4.F7" title="Figure 7 ‣ 4.2 Can “AI” be your president? ‣ 4 Demonstration ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">7</span></a>, the average score for each conversation is 2.4. Text-to-speech can be improved with more natural emotion or personality. The generated content is often too general and sometimes inappropriate. Biden’s responses are more in-context and evidence-supported. The pipeline excelled only in answering a subjective question about Biden’s age, where the GPT-4o pipeline performed well.
The GPT35_ETE pipeline had the best overall accuracy, but its responses were often in-context yet pompous. Thus, Biden still outperforms AI. In conclusion, “AI cannot be the President of the US just yet, considering both latency and accuracy.”☺</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Assist the visually impaired</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">While latency and the need for external information currently preventing AI from mission critical tasks, conversational agents can be production-ready and useful for non-latency-critical areas that do not require extensive external knowledge. Assisting indoor activities for the visually impaired is one such application.
We prepared questions for the visually impaired, including locating objects, navigating indoors, and inquiries about the surroundings. Six questions were sampled and fed to the GPT4O_ETE pipeline.
One scenario demonstration is included in our provided YouTube video.
The latency statistics in Figure <a class="ltx_ref" href="https://arxiv.org/html/2408.03047v1#S4.F8" title="Figure 8 ‣ 4.3 Assist the visually impaired ‣ 4 Demonstration ‣ OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents"><span class="ltx_text ltx_ref_tag">8</span></a> show responses within approximately 30 seconds.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="418" id="S4.F8.g1" src="extracted/5764442/assets/gpt-4o-assistance.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Screenshot visualizing detailed latency benchmark information for each conversation round</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Annotated results show a 4.7/5 accuracy, but the agent lacks specific skills for assisting the visually impaired. For example, ideally, it should provide step-by-step instructions on grabbing a coffee cup rather than just a general description.
This indicates that while conversational agents are nearly ready for assisting the visually impaired with indoor activities, improvements in latency and response quality are still needed.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Multimodal conversational agents offers a more natural human-computer interaction, exemplified by models like GPT-4o. However, real-world constraints necessitate balancing cost, latency, and accuracy, which may explain why GPT-4o’s full capabilities are not yet accessible.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">There are several technical options to achieve this, including traditional <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p2.1.1">divide-and-conquer</span> methods, <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p2.1.2">fully end-to-end</span> models like GPT-4o, and <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p2.1.3">Hybrid</span> approaches. The <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p2.1.4">fully end-to-end</span> approach inherently allows for lower latency, while the <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p2.1.5">divide-and-conquer</span> method faces latency issues when coordinating multiple components. Both approaches must address the challenge of handling large data I/O. If models are deployed locally, local network I/O issues can be more manageable. However, OpenAI’s models are closed-source, making local deployment impractical. While deploying other vision models locally is feasible, achieving high accuracy may be limited by local computational resources. <span class="ltx_text ltx_font_bold ltx_font_italic" id="S5.p2.1.6">Hybrid</span> solutions provides alternative approaches: pre-processing or compressing large data locally and then utilizing cloud-based models, or converting video to text and integrating it into the end-to-end voice model.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">We developed the OpenOmni framework to enable researchers to integrate their work into an end-to-end pipeline. The framework supports various solutions, allows for pipeline customization, generates latency performance reports, and provides an annotation interface for accuracy review. These features facilitate the creation of benchmark reports to identify and address key issues.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Testing with the US Presidential debate scenario highlighted latency as a critical issue, particularly with large video data. Integrating external knowledge remains a challenge, emphasizing the need for efficient Retrieval-Augmented Generation (RAG). For applications like indoor assistance for the visually impaired, latency improvements and model adaptation are both essential.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">The OpenOmni framework can significantly benefit the research community by facilitating the collection and management of new datasets, integrating various conversational agents approaches, and generating automatic latency benchmarks. Its annotation interface aids in accuracy performance review, making OpenOmni production-ready for suitable application scenarios and fostering further development in multimodal conversational agents.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Evtikhiev et al. (2023)</span>
<span class="ltx_bibblock">
Mikhail Evtikhiev, Egor Bogomolov, Yaroslav Sokolov, and Timofey Bryksin. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.jss.2023.111741" title="">Out of the bleu: How should we assess quality of the code generation models?</a>
</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Journal of Systems and Software</em>, 203:111741.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kusal et al. (2022)</span>
<span class="ltx_bibblock">
Sheetal Kusal, Shruti Patil, Jyoti Choudrie, Ketan Kotecha, Sashikala Mishra, and Ajith Abraham. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/ACCESS.2022.3201144" title="">Ai-based conversational agents: A scoping review from technologies to future directions</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">IEEE Access</em>, 10:92337–92356.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021)</span>
<span class="ltx_bibblock">
Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, and Lorenzo Torresani. 2021.

</span>
<span class="ltx_bibblock">Vx2text: End-to-end learning of video-based text generation from multimodal inputs.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>, pages 7005–7015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Guangya Liu, Shiqi Wang, Jianxing Yu, and Jian Yin. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/AEMCSE55572.2022.00170" title="">A survey on multimodal dialogue systems: Recent advances and new frontiers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)</em>, pages 845–853.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minaee et al. (2024)</span>
<span class="ltx_bibblock">
Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2402.06196" title="">Large language models: A survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Preprint</em>, arXiv:2402.06196.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Plapous et al. (2006)</span>
<span class="ltx_bibblock">
C. Plapous, C. Marro, and P. Scalart. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1109/TASL.2006.872621" title="">Improved signal-to-noise ratio estimation for speech enhancement</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">IEEE Transactions on Audio, Speech, and Language Processing</em>, 14(6):2098–2108.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roy (2021)</span>
<span class="ltx_bibblock">
Somnath Roy. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2106.02016" title="">Semantic-wer: A unified metric for the evaluation of asr transcript for end usability</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Preprint</em>, arXiv:2106.02016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shuster et al. (2018)</span>
<span class="ltx_bibblock">
Kurt Shuster, Samuel Humeau, Antoine Bordes, and Jason Weston. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/1811.00945" title="">Engaging image chat: Modeling personality in grounded dialogue</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">CoRR</em>, abs/1811.00945.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Streijl et al. (2016)</span>
<span class="ltx_bibblock">
Robert C. Streijl, Stefan Winkler, and David S. Hands. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://api.semanticscholar.org/CorpusID:15510814" title="">Mean opinion score (mos) revisited: methods and applications, limitations and alternatives</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Multimedia Systems</em>, 22:213–227.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sundar and Heck (2022)</span>
<span class="ltx_bibblock">
Anirudh Sundar and Larry Heck. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.nlp4convai-1.12" title="">Multimodal conversational AI: A survey of datasets and approaches</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 4th Workshop on NLP for Conversational AI</em>, pages 131–147, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2024)</span>
<span class="ltx_bibblock">
Xiaodong Wu, Ran Duan, and Jianbing Ni. 2024.

</span>
<span class="ltx_bibblock">Unveiling security, privacy, and ethical concerns of chatgpt.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Journal of Information and Intelligence</em>, 2(2):102–115.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/V1/2023.EMNLP-DEMO.49" title="">Video-llama: An instruction-tuned audio-visual language model for video understanding</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023 - System Demonstrations, Singapore, December 6-10, 2023</em>, pages 543–553. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.18223" title="">A survey of large language models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Preprint</em>, arXiv:2303.18223.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Aug  6 09:02:26 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
