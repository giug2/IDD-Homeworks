<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Language Models “Grok” to Copy</title>
<!--Generated on Sat Sep 14 03:10:42 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.09281v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S1" title="In Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S1.SS0.SSS0.Px1" title="In 1 Introduction ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title">Argument 1: Grokked Context Copying.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S1.SS0.SSS0.Px2" title="In 1 Introduction ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title">Argument 2: Token-Count-Independent Grokking Speed.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S1.SS0.SSS0.Px3" title="In 1 Introduction ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title">Argument 3: Deeper Circuit Formation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S2" title="In Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>General Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S2.SS0.SSS0.Px1" title="In 2 General Setup ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title">Model Architecture and Hyper-parameters.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S2.SS0.SSS0.Px2" title="In 2 General Setup ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title">Evaluating Context Copying.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S3" title="In Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Language Models “Grok” to Copy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S4" title="In Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Application</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S5" title="In Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Language Models “Grok” to Copy</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ang Lv<sup class="ltx_sup" id="id8.8.id1"><span class="ltx_text ltx_font_italic" id="id8.8.id1.1">1,2</span></sup>, Ruobing Xie<sup class="ltx_sup" id="id9.9.id2"><span class="ltx_text ltx_font_italic" id="id9.9.id2.1">2</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note">1</span></span></span></span>,  Xingwu Sun<sup class="ltx_sup" id="id10.10.id3"><span class="ltx_text ltx_font_italic" id="id10.10.id3.1">2</span></sup>, Zhanhui Kang<sup class="ltx_sup" id="id11.11.id4"><span class="ltx_text ltx_font_italic" id="id11.11.id4.1">2</span></sup>, <span class="ltx_text ltx_font_bold" id="id5.5.1">Rui Yan<sup class="ltx_sup" id="id5.5.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id5.5.1.1.1">1</span></sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id12.12.id5">1</sup>Gaoling School of Artificial Intelligence, Renmin University of China
<br class="ltx_break"/><sup class="ltx_sup" id="id13.13.id6">2</sup>Machine Learning Platform Department, Tencent
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id14.14.id7">{anglv, ruiyan}@ruc.edu.cn</span>  <span class="ltx_text ltx_font_typewriter" id="id15.15.id8">ruobingxie@tencent.com</span>
<br class="ltx_break"/>
</span><span class="ltx_author_notes">  Corresponding authors.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id16.id1">We examine the pre-training dynamics of language models, focusing on their ability to copy text from preceding context—a fundamental skill for various LLM applications, including in-context learning (ICL) and retrieval-augmented generation (RAG).
We propose a novel perspective that Transformer-based language models develop copying abilities similarly to grokking, which refers to sudden generalization on test set long after the model fit to the training set.
Our experiments yield three arguments: (1) The pre-training loss decreases rapidly, while the context copying ability of models initially lags and then abruptly saturates.
(2) The speed of developing copying ability is independent of the number of tokens trained, similarly to how grokking speed is unaffected by dataset size as long as the data distribution is preserved.
(3) Induction heads, the attention heads responsible for copying, form from shallow to deep layers during training, mirroring the development of circuits in deeper layers during grokking.
We contend that the connection between grokking and context copying can provide valuable insights for more effective language model training, ultimately improving in-context performance.
For example, we demonstrated that techniques that enhance grokking, such as regularization, either accelerate or enhance the development of context copying.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.7">
<p class="ltx_p" id="p1.7.8"><span class="ltx_text ltx_font_bold" id="p1.7.8.1">Language Models “Grok” to Copy</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.7.7" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.7.7.7" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.7.7.7.7">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.5"><span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.5.5">Ang Lv<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.1.1">1,2</span></sup>, Ruobing Xie<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.2.1">2</span></sup><span class="ltx_note ltx_role_footnotemark" id="footnotex2"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotemark: </span><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_medium" id="footnotex2.1.1.1">1</span></span></span></span></span>,  Xingwu Sun<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.3"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.3.1">2</span></sup>, Zhanhui Kang<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.4"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.4.1">2</span></sup>, Rui Yan<sup class="ltx_sup" id="p1.5.5.5.5.5.5.5.5"><span class="ltx_text ltx_font_medium ltx_font_italic" id="p1.5.5.5.5.5.5.5.5.1">1</span></sup><span class="ltx_note ltx_role_thanks" id="p1.5.5.5.5.5.5.5.6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>  Corresponding authors.</span></span></span></span></span></span>
<span class="ltx_tr" id="p1.6.6.6.6.6">
<span class="ltx_td ltx_align_center" id="p1.6.6.6.6.6.1"><sup class="ltx_sup" id="p1.6.6.6.6.6.1.1">1</sup>Gaoling School of Artificial Intelligence, Renmin University of China</span></span>
<span class="ltx_tr" id="p1.7.7.7.7.7">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.7.1"><sup class="ltx_sup" id="p1.7.7.7.7.7.1.1">2</sup>Machine Learning Platform Department, Tencent</span></span>
<span class="ltx_tr" id="p1.7.7.7.7.8.1">
<span class="ltx_td ltx_align_center" id="p1.7.7.7.7.8.1.1"><span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.8.1.1.1">{anglv, ruiyan}@ruc.edu.cn</span>  <span class="ltx_text ltx_font_typewriter" id="p1.7.7.7.7.8.1.1.2">ruobingxie@tencent.com</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large language models (LLMs) possess the capability to learn, retrieve, and reason from input context, facilitating various applications such as in-context learning (ICL, <cite class="ltx_cite ltx_citemacro_citep">Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib1" title="">2020</a></cite>) and retrieval-augmented generation (RAG, <cite class="ltx_cite ltx_citemacro_citep">Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib5" title="">2020</a></cite>).
Despite these achievements, several shortcomings have been reported regarding LLMs’ in-context capacities.
For instance, the order of ICL demonstrations matters <cite class="ltx_cite ltx_citemacro_cite">Lu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib9" title="">2022</a>); Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib17" title="">2024</a>)</cite> and LLMs’ awareness of different contextual positions fluctuates <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib7" title="">2023</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib2" title="">2024</a>); Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib6" title="">2024</a>)</cite>.
We believe that studying the mechanisms behind the development of in-context capabilities during pre-training offers valuable insights for enhancing LLMs from a novel perspective.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, we examine the pre-training dynamics of language models, focusing specifically on their <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">context copying</span> capabilities.
These capabilities are crucial for various LLM applications, including ICL and RAG.
For example, <cite class="ltx_cite ltx_citemacro_citet">Olsson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib12" title="">2022</a>)</cite> interpret ICL as a process that entails copying and then fuzzy pattern completion.
Similarly, RAG exhibits this characteristic, as it requires the in-context retrieval of key information, which is then copied (or integrated with additional paraphrasing and reasoning) as the output.
This paper presents empirical evidence demonstrating that Transformer-based language models <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib15" title="">2017</a>)</cite> develop context copying capabilities in a manner akin to “<span class="ltx_text ltx_font_bold" id="S1.p2.1.2">grokking</span>” <cite class="ltx_cite ltx_citemacro_cite">Power et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib13" title="">2022</a>)</cite>.
Grokking refers to the abrupt improvement in test set generalization long after models have overfit.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Our experimental method is summarized as follows:
We trained 12-layer Llama models <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib14" title="">2023</a>)</cite> using 40 billion tokens and saved checkpoints at regular intervals.
To evaluate context copying, we presented the models with an input context comprising multiple random token subsequences, each beginning with a unique prefix, and let them to complete one of the prefixes presented in the context.
The accuracy of these completions served as a measure of the models’ context copying abilities.
By analyzing the evolution of context copying accuracy and the development of <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">circuits</em> (i.e., the subnetworks responsible for completing the specific task) across the saved checkpoints, we argue a potential connection between grokking and the development of context copying capabilities, as outlined in the following arguments:</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Argument 1: Grokked Context Copying.</h3>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">We observed that context copying accuracy shows a sudden increase long after the training loss stabilizes, akin to “grokking” on the test set when neural networks trained on small training sets.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Argument 2: Token-Count-Independent Grokking Speed.</h3>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p1.1">We adjust the batch size to manage the number of tokens trained at specific update steps.
Results indicate that context copying is developed after certain updates, rather than after processing a specific quantity of tokens.
Similarly, the data-amount-independent (i.e., token-count-independent) generalization speed is a characteristic of grokking <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib16" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p2.1">We found that a higher learning rate speeds up grokked copying, suggesting it occurs at a specific optimization intensity, determined by the learning rate and update steps.
These experiments underscore the importance of careful hyperparameter selection in training language models for capacities like context copying, as their development isn’t necessarily reflected in pre-training loss reduction.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">Argument 3: Deeper Circuit Formation.</h3>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px3.p1.1">We noted that <em class="ltx_emph ltx_font_italic" id="S1.SS0.SSS0.Px3.p1.1.1">induction heads</em> <cite class="ltx_cite ltx_citemacro_cite">Olsson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib12" title="">2022</a>)</cite>, attention heads responsible for copying tokens, form from shallow to deep layers during training, consistent with research showing deeper circuits form in Transformers after grokking <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib16" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px3.p2">
<p class="ltx_p" id="S1.SS0.SSS0.Px3.p2.1">Based on the novel perspective that language models grok to copy, we pre-trained language models using regularization techniques, which are known to enhance grokking.
These techniques lead to either faster copying acquisition or higher accuracy.
Our findings highlight a promising and efficient research approach: developing improved language models with enhanced in-context performance by leveraging an understanding of grokking.
This efficiency arises from the fact that studies on grokking can utilize smaller, synthesized datasets, thereby avoiding the extensive and resource-intensive trials required for directly pre-training language models.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>General Setup</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="287" id="S2.F1.g1" src="x1.png" width="748"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An test input example when <math alttext="i=1" class="ltx_Math" display="inline" id="S2.F1.2.m1.1"><semantics id="S2.F1.2.m1.1b"><mrow id="S2.F1.2.m1.1.1" xref="S2.F1.2.m1.1.1.cmml"><mi id="S2.F1.2.m1.1.1.2" xref="S2.F1.2.m1.1.1.2.cmml">i</mi><mo id="S2.F1.2.m1.1.1.1" xref="S2.F1.2.m1.1.1.1.cmml">=</mo><mn id="S2.F1.2.m1.1.1.3" xref="S2.F1.2.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.F1.2.m1.1c"><apply id="S2.F1.2.m1.1.1.cmml" xref="S2.F1.2.m1.1.1"><eq id="S2.F1.2.m1.1.1.1.cmml" xref="S2.F1.2.m1.1.1.1"></eq><ci id="S2.F1.2.m1.1.1.2.cmml" xref="S2.F1.2.m1.1.1.2">𝑖</ci><cn id="S2.F1.2.m1.1.1.3.cmml" type="integer" xref="S2.F1.2.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.2.m1.1d">i=1</annotation><annotation encoding="application/x-llamapun" id="S2.F1.2.m1.1e">italic_i = 1</annotation></semantics></math>. The correct completion of this input should be <span class="ltx_text ltx_font_bold" id="S2.F1.4.1" style="color:#800080;">ba717e</span>.</figcaption>
</figure>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Model Architecture and Hyper-parameters.</h3>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">We train small Llama models <cite class="ltx_cite ltx_citemacro_cite">Touvron et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib14" title="">2023</a>)</cite> on a subset of the RedPajama dataset <cite class="ltx_cite ltx_citemacro_cite">Computer (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib3" title="">2023</a>)</cite>, comprising 40 billion tokens, with the task of next-token prediction.
Our model has 12 layers, each with 12 attention heads.
The hidden state dimension is 768, and the intermediate dimension of MLP layers is 3,072.
Our models contain 162M parameters.
We use Llama tokenizer with a vocabulary of 32,000 tokens, and set the model context length to 1,024 tokens.
Unless otherwise specified, the following hyperparameters are used:
The AdamW optimizer <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib8" title="">2019</a>)</cite> with <math alttext="(\beta_{1},\beta_{2})" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.1.m1.2"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.2a"><mrow id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.3.cmml"><mo id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.3" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.3.cmml">(</mo><msub id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml">β</mi><mn id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.4" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.3.cmml">,</mo><msub id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.cmml">β</mi><mn id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.5" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.2b"><interval closure="open" id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.3.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2"><apply id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.2">𝛽</ci><cn id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.1.1.3">1</cn></apply><apply id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.2">𝛽</ci><cn id="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.1.m1.2.2.2.2.3">2</cn></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.2c">(\beta_{1},\beta_{2})</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.1.m1.2d">( italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )</annotation></semantics></math> = (0.9, 0.999), a learning rate of 0.1, 2000 warmup steps, and the norm clip value of 1.
Our training is conducted on 8 A100 GPUs, with a batch size of 64 per GPU.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">Evaluating Context Copying.</h3>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.3">Each test sample consists of 50 random-token sequences, which are concatenated to form a single long sequence.
These sequences have an average length of 18 tokens, and we ensure that the
<math alttext="12" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">12</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.1.m1.1b"><cn id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1">12</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.1.m1.1c">12</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.1.m1.1d">12</annotation></semantics></math>-gram prefix and <math alttext="6" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px2.p1.2.m2.1a"><mn id="S2.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.2.m2.1b"><cn id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.2.m2.1c">6</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.2.m2.1d">6</annotation></semantics></math>-gram suffix of each sequence is unique.
We append the prefix of the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px2.p1.3.m3.1a"><mi id="S2.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.3.m3.1b"><ci id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.3.m3.1d">italic_i</annotation></semantics></math>-th sequence to the end of the concatenated sequences, which together serve as the model’s input.
An example input case is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S2.F1" title="Figure 1 ‣ 2 General Setup ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">1</span></a>.
Our test set includes 500 samples.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.2">The model’s objective is to continue the input.
We consider the model’s output correct if it copies the suffix of the queried prefix from the context, since random token sequences lack meaningful semantics and the most natural continuation is to generate a prefix that has appeared in the context <cite class="ltx_cite ltx_citemacro_cite">Olsson et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib12" title="">2022</a>)</cite>.
To comprehensively assess context copying capabilities across different contextual positions, we evaluate the model for every <math alttext="i" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p2.1.m1.1"><semantics id="S2.SS0.SSS0.Px2.p2.1.m1.1a"><mi id="S2.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p2.1.m1.1b"><ci id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p2.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p2.1.m1.1d">italic_i</annotation></semantics></math> mod <math alttext="5=0" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p2.2.m2.1"><semantics id="S2.SS0.SSS0.Px2.p2.2.m2.1a"><mrow id="S2.SS0.SSS0.Px2.p2.2.m2.1.1" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.cmml"><mn id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.2" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml">5</mn><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p2.2.m2.1b"><apply id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1"><eq id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1"></eq><cn id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml" type="integer" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.2">5</cn><cn id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p2.2.m2.1c">5=0</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p2.2.m2.1d">5 = 0</annotation></semantics></math>.
We report the average accuracy across these positions.
Unless specifically indicated, we report the average accuracy across these positions, from models trained with 3 different random seeds.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="548" id="S2.F2.g1" src="x2.png" width="763"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>We illustrate the average context copying accuracy by the bars, and the pre-training loss by the line.
The X-axis represents the number of tokens trained.
A clear grokked copying occurs at 15B tokens.</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="458" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
We manage the token count trained at specific steps by adjusting the batch size.
Three models trained with different batch size develop fundamental copying abilities after around 38,000 update steps, despite training on varying numbers of tokens.
</figcaption>
</figure>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="1144" id="S2.F4.g1" src="x4.png" width="764"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>With a fixed learning rate,the convergence rate on the training set, as indicated by the training loss, is related to the token count.
However, under similar convergence rates, the copying capacity varies significantly, which is influenced by the number of update steps.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Language Models “Grok” to Copy</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We propose that language models develop the context copying in a manner similar to “grokking”.
In this section, we present three arguments and present the corresponding experiments and analyses that provide evidence for these claims.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">For Argument 1</span>, we present the context copying accuracy and pre-training loss in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S2.F2" title="Figure 2 ‣ Evaluating Context Copying. ‣ 2 General Setup ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">2</span></a>.
The training loss stabilizes after 5B tokens, indicating that the fundamental language modeling has been established (i.e., fitted to the training distribution).
However, the accuracy is low until 10B tokens have been trained.
A surge in accuracy occurs at 15B tokens.
This pattern of developing robust context copying resembles grokking <cite class="ltx_cite ltx_citemacro_cite">Power et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib13" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.p3.1.1">For argument 2</span>, we trained another two models using the same setups and same initial weights as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S2" title="2 General Setup ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">2</span></a>, but with batch sizes of 32 and 128.
Our results indicate that grokked context copying is independent on the token count.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S2.F3" title="Figure 3 ‣ Evaluating Context Copying. ‣ 2 General Setup ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">3</span></a> shows that <span class="ltx_text ltx_font_italic" id="S3.p3.1.2">with a fixed learning rate</span>, to achieve similar accuracy to models using a batch size of 64, models trained with a batch size of 128 (32) require twice (half) the token count, as their update steps are equal.
This finding aligns with observations <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib16" title="">2024</a>)</cite> that data quantity does not affect the grokking speed.
The consistency enhances the connection between grokking and the development of context copying.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">Notably, we observed that the convergence on the training set is token-count-dependent, although copying performance is slowed down with larger batch sizes, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S2.F4" title="Figure 4 ‣ Evaluating Context Copying. ‣ 2 General Setup ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">4</span></a>.
We assume that using an appropriately smaller batch size to update the models with more steps within a single epoch may facilitate the development of capacities that are not reflected in the training loss reduction.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1">Moreover, we examine the impact of learning rates.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S3.F5" title="Figure 5 ‣ 3 Language Models “Grok” to Copy ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">5</span></a> indicates that an increased learning rate facilitates earlier and stronger grokking.
Consequently, we assume that the grokked context copying emerges until the optimization reaches a specific intensity, which is influenced by both the learning rate and the number of update steps.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="994" id="S3.F5.g1" src="x5.png" width="745"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>With a fixed batch size (64), a larger learning rate accelerates the grokking to copy.</figcaption>
</figure>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.9"><span class="ltx_text ltx_font_bold" id="S3.p6.9.1">For argument 3</span>, we examined the evolution of induction heads in our models.
Induction heads <cite class="ltx_cite ltx_citemacro_cite">Elhage et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib4" title="">2021</a>)</cite> are the primary circuit for conditional copying in Transformer-based language models and have been identified as a general mechanism across various models <cite class="ltx_cite ltx_citemacro_cite">Lv et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib10" title="">2024</a>)</cite>.
Consider a sequence “<math alttext="A,B,...,A" class="ltx_Math" display="inline" id="S3.p6.1.m1.4"><semantics id="S3.p6.1.m1.4a"><mrow id="S3.p6.1.m1.4.5.2" xref="S3.p6.1.m1.4.5.1.cmml"><mi id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">A</mi><mo id="S3.p6.1.m1.4.5.2.1" xref="S3.p6.1.m1.4.5.1.cmml">,</mo><mi id="S3.p6.1.m1.2.2" xref="S3.p6.1.m1.2.2.cmml">B</mi><mo id="S3.p6.1.m1.4.5.2.2" xref="S3.p6.1.m1.4.5.1.cmml">,</mo><mi id="S3.p6.1.m1.3.3" mathvariant="normal" xref="S3.p6.1.m1.3.3.cmml">…</mi><mo id="S3.p6.1.m1.4.5.2.3" xref="S3.p6.1.m1.4.5.1.cmml">,</mo><mi id="S3.p6.1.m1.4.4" xref="S3.p6.1.m1.4.4.cmml">A</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.4b"><list id="S3.p6.1.m1.4.5.1.cmml" xref="S3.p6.1.m1.4.5.2"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">𝐴</ci><ci id="S3.p6.1.m1.2.2.cmml" xref="S3.p6.1.m1.2.2">𝐵</ci><ci id="S3.p6.1.m1.3.3.cmml" xref="S3.p6.1.m1.3.3">…</ci><ci id="S3.p6.1.m1.4.4.cmml" xref="S3.p6.1.m1.4.4">𝐴</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.4c">A,B,...,A</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.4d">italic_A , italic_B , … , italic_A</annotation></semantics></math>” input to the language model, where <math alttext="A" class="ltx_Math" display="inline" id="S3.p6.2.m2.1"><semantics id="S3.p6.2.m2.1a"><mi id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><ci id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p6.2.m2.1d">italic_A</annotation></semantics></math> and <math alttext="B" class="ltx_Math" display="inline" id="S3.p6.3.m3.1"><semantics id="S3.p6.3.m3.1a"><mi id="S3.p6.3.m3.1.1" xref="S3.p6.3.m3.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.1b"><ci id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p6.3.m3.1d">italic_B</annotation></semantics></math> are arbitrary tokens.
Induction heads work based on collaboration across layers, enabling the model to output <math alttext="B" class="ltx_Math" display="inline" id="S3.p6.4.m4.1"><semantics id="S3.p6.4.m4.1a"><mi id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.1b"><ci id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p6.4.m4.1d">italic_B</annotation></semantics></math>.
In shallower layers, certain attention heads move each token’s information to its next position; in deeper layers, induction heads at the final position (i.e., the second <math alttext="A" class="ltx_Math" display="inline" id="S3.p6.5.m5.1"><semantics id="S3.p6.5.m5.1a"><mi id="S3.p6.5.m5.1.1" xref="S3.p6.5.m5.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p6.5.m5.1b"><ci id="S3.p6.5.m5.1.1.cmml" xref="S3.p6.5.m5.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.5.m5.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p6.5.m5.1d">italic_A</annotation></semantics></math>) attend to <math alttext="B" class="ltx_Math" display="inline" id="S3.p6.6.m6.1"><semantics id="S3.p6.6.m6.1a"><mi id="S3.p6.6.m6.1.1" xref="S3.p6.6.m6.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p6.6.m6.1b"><ci id="S3.p6.6.m6.1.1.cmml" xref="S3.p6.6.m6.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.6.m6.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p6.6.m6.1d">italic_B</annotation></semantics></math> (since a subspace of hidden states at <math alttext="B" class="ltx_Math" display="inline" id="S3.p6.7.m7.1"><semantics id="S3.p6.7.m7.1a"><mi id="S3.p6.7.m7.1.1" xref="S3.p6.7.m7.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p6.7.m7.1b"><ci id="S3.p6.7.m7.1.1.cmml" xref="S3.p6.7.m7.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.7.m7.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p6.7.m7.1d">italic_B</annotation></semantics></math>’s position contains information from the first <math alttext="A" class="ltx_Math" display="inline" id="S3.p6.8.m8.1"><semantics id="S3.p6.8.m8.1a"><mi id="S3.p6.8.m8.1.1" xref="S3.p6.8.m8.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S3.p6.8.m8.1b"><ci id="S3.p6.8.m8.1.1.cmml" xref="S3.p6.8.m8.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.8.m8.1c">A</annotation><annotation encoding="application/x-llamapun" id="S3.p6.8.m8.1d">italic_A</annotation></semantics></math>) and copy the attended <math alttext="B" class="ltx_Math" display="inline" id="S3.p6.9.m9.1"><semantics id="S3.p6.9.m9.1a"><mi id="S3.p6.9.m9.1.1" xref="S3.p6.9.m9.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.p6.9.m9.1b"><ci id="S3.p6.9.m9.1.1.cmml" xref="S3.p6.9.m9.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.9.m9.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.p6.9.m9.1d">italic_B</annotation></semantics></math> as the output.</p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.6">We introduce the induction score <math alttext="I^{(L,H)}" class="ltx_Math" display="inline" id="S3.p7.1.m1.2"><semantics id="S3.p7.1.m1.2a"><msup id="S3.p7.1.m1.2.3" xref="S3.p7.1.m1.2.3.cmml"><mi id="S3.p7.1.m1.2.3.2" xref="S3.p7.1.m1.2.3.2.cmml">I</mi><mrow id="S3.p7.1.m1.2.2.2.4" xref="S3.p7.1.m1.2.2.2.3.cmml"><mo id="S3.p7.1.m1.2.2.2.4.1" stretchy="false" xref="S3.p7.1.m1.2.2.2.3.cmml">(</mo><mi id="S3.p7.1.m1.1.1.1.1" xref="S3.p7.1.m1.1.1.1.1.cmml">L</mi><mo id="S3.p7.1.m1.2.2.2.4.2" xref="S3.p7.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.p7.1.m1.2.2.2.2" xref="S3.p7.1.m1.2.2.2.2.cmml">H</mi><mo id="S3.p7.1.m1.2.2.2.4.3" stretchy="false" xref="S3.p7.1.m1.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p7.1.m1.2b"><apply id="S3.p7.1.m1.2.3.cmml" xref="S3.p7.1.m1.2.3"><csymbol cd="ambiguous" id="S3.p7.1.m1.2.3.1.cmml" xref="S3.p7.1.m1.2.3">superscript</csymbol><ci id="S3.p7.1.m1.2.3.2.cmml" xref="S3.p7.1.m1.2.3.2">𝐼</ci><interval closure="open" id="S3.p7.1.m1.2.2.2.3.cmml" xref="S3.p7.1.m1.2.2.2.4"><ci id="S3.p7.1.m1.1.1.1.1.cmml" xref="S3.p7.1.m1.1.1.1.1">𝐿</ci><ci id="S3.p7.1.m1.2.2.2.2.cmml" xref="S3.p7.1.m1.2.2.2.2">𝐻</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.1.m1.2c">I^{(L,H)}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.1.m1.2d">italic_I start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT</annotation></semantics></math>, which quantifies the similarity between the behavior of the <math alttext="H" class="ltx_Math" display="inline" id="S3.p7.2.m2.1"><semantics id="S3.p7.2.m2.1a"><mi id="S3.p7.2.m2.1.1" xref="S3.p7.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.p7.2.m2.1b"><ci id="S3.p7.2.m2.1.1.cmml" xref="S3.p7.2.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.2.m2.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.p7.2.m2.1d">italic_H</annotation></semantics></math>-th head in layer <math alttext="L" class="ltx_Math" display="inline" id="S3.p7.3.m3.1"><semantics id="S3.p7.3.m3.1a"><mi id="S3.p7.3.m3.1.1" xref="S3.p7.3.m3.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.p7.3.m3.1b"><ci id="S3.p7.3.m3.1.1.cmml" xref="S3.p7.3.m3.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.3.m3.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.p7.3.m3.1d">italic_L</annotation></semantics></math>—referred to as <math alttext="(L,H)" class="ltx_Math" display="inline" id="S3.p7.4.m4.2"><semantics id="S3.p7.4.m4.2a"><mrow id="S3.p7.4.m4.2.3.2" xref="S3.p7.4.m4.2.3.1.cmml"><mo id="S3.p7.4.m4.2.3.2.1" stretchy="false" xref="S3.p7.4.m4.2.3.1.cmml">(</mo><mi id="S3.p7.4.m4.1.1" xref="S3.p7.4.m4.1.1.cmml">L</mi><mo id="S3.p7.4.m4.2.3.2.2" xref="S3.p7.4.m4.2.3.1.cmml">,</mo><mi id="S3.p7.4.m4.2.2" xref="S3.p7.4.m4.2.2.cmml">H</mi><mo id="S3.p7.4.m4.2.3.2.3" stretchy="false" xref="S3.p7.4.m4.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.4.m4.2b"><interval closure="open" id="S3.p7.4.m4.2.3.1.cmml" xref="S3.p7.4.m4.2.3.2"><ci id="S3.p7.4.m4.1.1.cmml" xref="S3.p7.4.m4.1.1">𝐿</ci><ci id="S3.p7.4.m4.2.2.cmml" xref="S3.p7.4.m4.2.2">𝐻</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.4.m4.2c">(L,H)</annotation><annotation encoding="application/x-llamapun" id="S3.p7.4.m4.2d">( italic_L , italic_H )</annotation></semantics></math>—and that of an ideal induction head.
We establish <math alttext="I^{(L,H)}" class="ltx_Math" display="inline" id="S3.p7.5.m5.2"><semantics id="S3.p7.5.m5.2a"><msup id="S3.p7.5.m5.2.3" xref="S3.p7.5.m5.2.3.cmml"><mi id="S3.p7.5.m5.2.3.2" xref="S3.p7.5.m5.2.3.2.cmml">I</mi><mrow id="S3.p7.5.m5.2.2.2.4" xref="S3.p7.5.m5.2.2.2.3.cmml"><mo id="S3.p7.5.m5.2.2.2.4.1" stretchy="false" xref="S3.p7.5.m5.2.2.2.3.cmml">(</mo><mi id="S3.p7.5.m5.1.1.1.1" xref="S3.p7.5.m5.1.1.1.1.cmml">L</mi><mo id="S3.p7.5.m5.2.2.2.4.2" xref="S3.p7.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.p7.5.m5.2.2.2.2" xref="S3.p7.5.m5.2.2.2.2.cmml">H</mi><mo id="S3.p7.5.m5.2.2.2.4.3" stretchy="false" xref="S3.p7.5.m5.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p7.5.m5.2b"><apply id="S3.p7.5.m5.2.3.cmml" xref="S3.p7.5.m5.2.3"><csymbol cd="ambiguous" id="S3.p7.5.m5.2.3.1.cmml" xref="S3.p7.5.m5.2.3">superscript</csymbol><ci id="S3.p7.5.m5.2.3.2.cmml" xref="S3.p7.5.m5.2.3.2">𝐼</ci><interval closure="open" id="S3.p7.5.m5.2.2.2.3.cmml" xref="S3.p7.5.m5.2.2.2.4"><ci id="S3.p7.5.m5.1.1.1.1.cmml" xref="S3.p7.5.m5.1.1.1.1">𝐿</ci><ci id="S3.p7.5.m5.2.2.2.2.cmml" xref="S3.p7.5.m5.2.2.2.2">𝐻</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.5.m5.2c">I^{(L,H)}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.5.m5.2d">italic_I start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT</annotation></semantics></math> as a value within the range of <math alttext="[-1,1]" class="ltx_Math" display="inline" id="S3.p7.6.m6.2"><semantics id="S3.p7.6.m6.2a"><mrow id="S3.p7.6.m6.2.2.1" xref="S3.p7.6.m6.2.2.2.cmml"><mo id="S3.p7.6.m6.2.2.1.2" stretchy="false" xref="S3.p7.6.m6.2.2.2.cmml">[</mo><mrow id="S3.p7.6.m6.2.2.1.1" xref="S3.p7.6.m6.2.2.1.1.cmml"><mo id="S3.p7.6.m6.2.2.1.1a" xref="S3.p7.6.m6.2.2.1.1.cmml">−</mo><mn id="S3.p7.6.m6.2.2.1.1.2" xref="S3.p7.6.m6.2.2.1.1.2.cmml">1</mn></mrow><mo id="S3.p7.6.m6.2.2.1.3" xref="S3.p7.6.m6.2.2.2.cmml">,</mo><mn id="S3.p7.6.m6.1.1" xref="S3.p7.6.m6.1.1.cmml">1</mn><mo id="S3.p7.6.m6.2.2.1.4" stretchy="false" xref="S3.p7.6.m6.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.6.m6.2b"><interval closure="closed" id="S3.p7.6.m6.2.2.2.cmml" xref="S3.p7.6.m6.2.2.1"><apply id="S3.p7.6.m6.2.2.1.1.cmml" xref="S3.p7.6.m6.2.2.1.1"><minus id="S3.p7.6.m6.2.2.1.1.1.cmml" xref="S3.p7.6.m6.2.2.1.1"></minus><cn id="S3.p7.6.m6.2.2.1.1.2.cmml" type="integer" xref="S3.p7.6.m6.2.2.1.1.2">1</cn></apply><cn id="S3.p7.6.m6.1.1.cmml" type="integer" xref="S3.p7.6.m6.1.1">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.6.m6.2c">[-1,1]</annotation><annotation encoding="application/x-llamapun" id="S3.p7.6.m6.2d">[ - 1 , 1 ]</annotation></semantics></math>, defined as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S5.EGx1">
<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle I^{(L,H)}=\bar{A}^{(L,H)}\cdot EP^{(L,H)}." class="ltx_Math" display="inline" id="S3.E1.m1.7"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.7.1" xref="S3.E1.m1.7.7.1.1.cmml"><mrow id="S3.E1.m1.7.7.1.1" xref="S3.E1.m1.7.7.1.1.cmml"><msup id="S3.E1.m1.7.7.1.1.2" xref="S3.E1.m1.7.7.1.1.2.cmml"><mi id="S3.E1.m1.7.7.1.1.2.2" xref="S3.E1.m1.7.7.1.1.2.2.cmml">I</mi><mrow id="S3.E1.m1.2.2.2.4" xref="S3.E1.m1.2.2.2.3.cmml"><mo id="S3.E1.m1.2.2.2.4.1" stretchy="false" xref="S3.E1.m1.2.2.2.3.cmml">(</mo><mi id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">L</mi><mo id="S3.E1.m1.2.2.2.4.2" xref="S3.E1.m1.2.2.2.3.cmml">,</mo><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">H</mi><mo id="S3.E1.m1.2.2.2.4.3" stretchy="false" xref="S3.E1.m1.2.2.2.3.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.7.7.1.1.1" xref="S3.E1.m1.7.7.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.7.7.1.1.3" xref="S3.E1.m1.7.7.1.1.3.cmml"><mrow id="S3.E1.m1.7.7.1.1.3.2" xref="S3.E1.m1.7.7.1.1.3.2.cmml"><msup id="S3.E1.m1.7.7.1.1.3.2.2" xref="S3.E1.m1.7.7.1.1.3.2.2.cmml"><mover accent="true" id="S3.E1.m1.7.7.1.1.3.2.2.2" xref="S3.E1.m1.7.7.1.1.3.2.2.2.cmml"><mi id="S3.E1.m1.7.7.1.1.3.2.2.2.2" xref="S3.E1.m1.7.7.1.1.3.2.2.2.2.cmml">A</mi><mo id="S3.E1.m1.7.7.1.1.3.2.2.2.1" xref="S3.E1.m1.7.7.1.1.3.2.2.2.1.cmml">¯</mo></mover><mrow id="S3.E1.m1.4.4.2.4" xref="S3.E1.m1.4.4.2.3.cmml"><mo id="S3.E1.m1.4.4.2.4.1" stretchy="false" xref="S3.E1.m1.4.4.2.3.cmml">(</mo><mi id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">L</mi><mo id="S3.E1.m1.4.4.2.4.2" xref="S3.E1.m1.4.4.2.3.cmml">,</mo><mi id="S3.E1.m1.4.4.2.2" xref="S3.E1.m1.4.4.2.2.cmml">H</mi><mo id="S3.E1.m1.4.4.2.4.3" stretchy="false" xref="S3.E1.m1.4.4.2.3.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.7.7.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.7.7.1.1.3.2.1.cmml">⋅</mo><mi id="S3.E1.m1.7.7.1.1.3.2.3" xref="S3.E1.m1.7.7.1.1.3.2.3.cmml">E</mi></mrow><mo id="S3.E1.m1.7.7.1.1.3.1" xref="S3.E1.m1.7.7.1.1.3.1.cmml">⁢</mo><msup id="S3.E1.m1.7.7.1.1.3.3" xref="S3.E1.m1.7.7.1.1.3.3.cmml"><mi id="S3.E1.m1.7.7.1.1.3.3.2" xref="S3.E1.m1.7.7.1.1.3.3.2.cmml">P</mi><mrow id="S3.E1.m1.6.6.2.4" xref="S3.E1.m1.6.6.2.3.cmml"><mo id="S3.E1.m1.6.6.2.4.1" stretchy="false" xref="S3.E1.m1.6.6.2.3.cmml">(</mo><mi id="S3.E1.m1.5.5.1.1" xref="S3.E1.m1.5.5.1.1.cmml">L</mi><mo id="S3.E1.m1.6.6.2.4.2" xref="S3.E1.m1.6.6.2.3.cmml">,</mo><mi id="S3.E1.m1.6.6.2.2" xref="S3.E1.m1.6.6.2.2.cmml">H</mi><mo id="S3.E1.m1.6.6.2.4.3" stretchy="false" xref="S3.E1.m1.6.6.2.3.cmml">)</mo></mrow></msup></mrow></mrow><mo id="S3.E1.m1.7.7.1.2" lspace="0em" xref="S3.E1.m1.7.7.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.7.1.1.cmml" xref="S3.E1.m1.7.7.1"><eq id="S3.E1.m1.7.7.1.1.1.cmml" xref="S3.E1.m1.7.7.1.1.1"></eq><apply id="S3.E1.m1.7.7.1.1.2.cmml" xref="S3.E1.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.2.1.cmml" xref="S3.E1.m1.7.7.1.1.2">superscript</csymbol><ci id="S3.E1.m1.7.7.1.1.2.2.cmml" xref="S3.E1.m1.7.7.1.1.2.2">𝐼</ci><interval closure="open" id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.4"><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">𝐿</ci><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">𝐻</ci></interval></apply><apply id="S3.E1.m1.7.7.1.1.3.cmml" xref="S3.E1.m1.7.7.1.1.3"><times id="S3.E1.m1.7.7.1.1.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.1"></times><apply id="S3.E1.m1.7.7.1.1.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2"><ci id="S3.E1.m1.7.7.1.1.3.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.2.1">⋅</ci><apply id="S3.E1.m1.7.7.1.1.3.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2">superscript</csymbol><apply id="S3.E1.m1.7.7.1.1.3.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2.2"><ci id="S3.E1.m1.7.7.1.1.3.2.2.2.1.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2.2.1">¯</ci><ci id="S3.E1.m1.7.7.1.1.3.2.2.2.2.cmml" xref="S3.E1.m1.7.7.1.1.3.2.2.2.2">𝐴</ci></apply><interval closure="open" id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.4"><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">𝐿</ci><ci id="S3.E1.m1.4.4.2.2.cmml" xref="S3.E1.m1.4.4.2.2">𝐻</ci></interval></apply><ci id="S3.E1.m1.7.7.1.1.3.2.3.cmml" xref="S3.E1.m1.7.7.1.1.3.2.3">𝐸</ci></apply><apply id="S3.E1.m1.7.7.1.1.3.3.cmml" xref="S3.E1.m1.7.7.1.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.1.1.3.3.1.cmml" xref="S3.E1.m1.7.7.1.1.3.3">superscript</csymbol><ci id="S3.E1.m1.7.7.1.1.3.3.2.cmml" xref="S3.E1.m1.7.7.1.1.3.3.2">𝑃</ci><interval closure="open" id="S3.E1.m1.6.6.2.3.cmml" xref="S3.E1.m1.6.6.2.4"><ci id="S3.E1.m1.5.5.1.1.cmml" xref="S3.E1.m1.5.5.1.1">𝐿</ci><ci id="S3.E1.m1.6.6.2.2.cmml" xref="S3.E1.m1.6.6.2.2">𝐻</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">\displaystyle I^{(L,H)}=\bar{A}^{(L,H)}\cdot EP^{(L,H)}.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.7d">italic_I start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT = over¯ start_ARG italic_A end_ARG start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT ⋅ italic_E italic_P start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.p7.14">In Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S3.E1" title="In 3 Language Models “Grok” to Copy ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">1</span></a>, <math alttext="\bar{A}^{(L,H)}\in[0,1]" class="ltx_Math" display="inline" id="S3.p7.7.m1.4"><semantics id="S3.p7.7.m1.4a"><mrow id="S3.p7.7.m1.4.5" xref="S3.p7.7.m1.4.5.cmml"><msup id="S3.p7.7.m1.4.5.2" xref="S3.p7.7.m1.4.5.2.cmml"><mover accent="true" id="S3.p7.7.m1.4.5.2.2" xref="S3.p7.7.m1.4.5.2.2.cmml"><mi id="S3.p7.7.m1.4.5.2.2.2" xref="S3.p7.7.m1.4.5.2.2.2.cmml">A</mi><mo id="S3.p7.7.m1.4.5.2.2.1" xref="S3.p7.7.m1.4.5.2.2.1.cmml">¯</mo></mover><mrow id="S3.p7.7.m1.2.2.2.4" xref="S3.p7.7.m1.2.2.2.3.cmml"><mo id="S3.p7.7.m1.2.2.2.4.1" stretchy="false" xref="S3.p7.7.m1.2.2.2.3.cmml">(</mo><mi id="S3.p7.7.m1.1.1.1.1" xref="S3.p7.7.m1.1.1.1.1.cmml">L</mi><mo id="S3.p7.7.m1.2.2.2.4.2" xref="S3.p7.7.m1.2.2.2.3.cmml">,</mo><mi id="S3.p7.7.m1.2.2.2.2" xref="S3.p7.7.m1.2.2.2.2.cmml">H</mi><mo id="S3.p7.7.m1.2.2.2.4.3" stretchy="false" xref="S3.p7.7.m1.2.2.2.3.cmml">)</mo></mrow></msup><mo id="S3.p7.7.m1.4.5.1" xref="S3.p7.7.m1.4.5.1.cmml">∈</mo><mrow id="S3.p7.7.m1.4.5.3.2" xref="S3.p7.7.m1.4.5.3.1.cmml"><mo id="S3.p7.7.m1.4.5.3.2.1" stretchy="false" xref="S3.p7.7.m1.4.5.3.1.cmml">[</mo><mn id="S3.p7.7.m1.3.3" xref="S3.p7.7.m1.3.3.cmml">0</mn><mo id="S3.p7.7.m1.4.5.3.2.2" xref="S3.p7.7.m1.4.5.3.1.cmml">,</mo><mn id="S3.p7.7.m1.4.4" xref="S3.p7.7.m1.4.4.cmml">1</mn><mo id="S3.p7.7.m1.4.5.3.2.3" stretchy="false" xref="S3.p7.7.m1.4.5.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.7.m1.4b"><apply id="S3.p7.7.m1.4.5.cmml" xref="S3.p7.7.m1.4.5"><in id="S3.p7.7.m1.4.5.1.cmml" xref="S3.p7.7.m1.4.5.1"></in><apply id="S3.p7.7.m1.4.5.2.cmml" xref="S3.p7.7.m1.4.5.2"><csymbol cd="ambiguous" id="S3.p7.7.m1.4.5.2.1.cmml" xref="S3.p7.7.m1.4.5.2">superscript</csymbol><apply id="S3.p7.7.m1.4.5.2.2.cmml" xref="S3.p7.7.m1.4.5.2.2"><ci id="S3.p7.7.m1.4.5.2.2.1.cmml" xref="S3.p7.7.m1.4.5.2.2.1">¯</ci><ci id="S3.p7.7.m1.4.5.2.2.2.cmml" xref="S3.p7.7.m1.4.5.2.2.2">𝐴</ci></apply><interval closure="open" id="S3.p7.7.m1.2.2.2.3.cmml" xref="S3.p7.7.m1.2.2.2.4"><ci id="S3.p7.7.m1.1.1.1.1.cmml" xref="S3.p7.7.m1.1.1.1.1">𝐿</ci><ci id="S3.p7.7.m1.2.2.2.2.cmml" xref="S3.p7.7.m1.2.2.2.2">𝐻</ci></interval></apply><interval closure="closed" id="S3.p7.7.m1.4.5.3.1.cmml" xref="S3.p7.7.m1.4.5.3.2"><cn id="S3.p7.7.m1.3.3.cmml" type="integer" xref="S3.p7.7.m1.3.3">0</cn><cn id="S3.p7.7.m1.4.4.cmml" type="integer" xref="S3.p7.7.m1.4.4">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.7.m1.4c">\bar{A}^{(L,H)}\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.p7.7.m1.4d">over¯ start_ARG italic_A end_ARG start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT ∈ [ 0 , 1 ]</annotation></semantics></math> measures the induction attention pattern: when inputting a random token sequence of length <math alttext="2s" class="ltx_Math" display="inline" id="S3.p7.8.m2.1"><semantics id="S3.p7.8.m2.1a"><mrow id="S3.p7.8.m2.1.1" xref="S3.p7.8.m2.1.1.cmml"><mn id="S3.p7.8.m2.1.1.2" xref="S3.p7.8.m2.1.1.2.cmml">2</mn><mo id="S3.p7.8.m2.1.1.1" xref="S3.p7.8.m2.1.1.1.cmml">⁢</mo><mi id="S3.p7.8.m2.1.1.3" xref="S3.p7.8.m2.1.1.3.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.8.m2.1b"><apply id="S3.p7.8.m2.1.1.cmml" xref="S3.p7.8.m2.1.1"><times id="S3.p7.8.m2.1.1.1.cmml" xref="S3.p7.8.m2.1.1.1"></times><cn id="S3.p7.8.m2.1.1.2.cmml" type="integer" xref="S3.p7.8.m2.1.1.2">2</cn><ci id="S3.p7.8.m2.1.1.3.cmml" xref="S3.p7.8.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.8.m2.1c">2s</annotation><annotation encoding="application/x-llamapun" id="S3.p7.8.m2.1d">2 italic_s</annotation></semantics></math> which contains two identical subsequences of length <math alttext="s" class="ltx_Math" display="inline" id="S3.p7.9.m3.1"><semantics id="S3.p7.9.m3.1a"><mi id="S3.p7.9.m3.1.1" xref="S3.p7.9.m3.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p7.9.m3.1b"><ci id="S3.p7.9.m3.1.1.cmml" xref="S3.p7.9.m3.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.9.m3.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.p7.9.m3.1d">italic_s</annotation></semantics></math> (set to 100), we denote the average attention weight assigned from position <math alttext="s+i-1" class="ltx_Math" display="inline" id="S3.p7.10.m4.1"><semantics id="S3.p7.10.m4.1a"><mrow id="S3.p7.10.m4.1.1" xref="S3.p7.10.m4.1.1.cmml"><mrow id="S3.p7.10.m4.1.1.2" xref="S3.p7.10.m4.1.1.2.cmml"><mi id="S3.p7.10.m4.1.1.2.2" xref="S3.p7.10.m4.1.1.2.2.cmml">s</mi><mo id="S3.p7.10.m4.1.1.2.1" xref="S3.p7.10.m4.1.1.2.1.cmml">+</mo><mi id="S3.p7.10.m4.1.1.2.3" xref="S3.p7.10.m4.1.1.2.3.cmml">i</mi></mrow><mo id="S3.p7.10.m4.1.1.1" xref="S3.p7.10.m4.1.1.1.cmml">−</mo><mn id="S3.p7.10.m4.1.1.3" xref="S3.p7.10.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.10.m4.1b"><apply id="S3.p7.10.m4.1.1.cmml" xref="S3.p7.10.m4.1.1"><minus id="S3.p7.10.m4.1.1.1.cmml" xref="S3.p7.10.m4.1.1.1"></minus><apply id="S3.p7.10.m4.1.1.2.cmml" xref="S3.p7.10.m4.1.1.2"><plus id="S3.p7.10.m4.1.1.2.1.cmml" xref="S3.p7.10.m4.1.1.2.1"></plus><ci id="S3.p7.10.m4.1.1.2.2.cmml" xref="S3.p7.10.m4.1.1.2.2">𝑠</ci><ci id="S3.p7.10.m4.1.1.2.3.cmml" xref="S3.p7.10.m4.1.1.2.3">𝑖</ci></apply><cn id="S3.p7.10.m4.1.1.3.cmml" type="integer" xref="S3.p7.10.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.10.m4.1c">s+i-1</annotation><annotation encoding="application/x-llamapun" id="S3.p7.10.m4.1d">italic_s + italic_i - 1</annotation></semantics></math> to <math alttext="i" class="ltx_Math" display="inline" id="S3.p7.11.m5.1"><semantics id="S3.p7.11.m5.1a"><mi id="S3.p7.11.m5.1.1" xref="S3.p7.11.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p7.11.m5.1b"><ci id="S3.p7.11.m5.1.1.cmml" xref="S3.p7.11.m5.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.11.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p7.11.m5.1d">italic_i</annotation></semantics></math> as <math alttext="\bar{A}^{(L,H)}" class="ltx_Math" display="inline" id="S3.p7.12.m6.2"><semantics id="S3.p7.12.m6.2a"><msup id="S3.p7.12.m6.2.3" xref="S3.p7.12.m6.2.3.cmml"><mover accent="true" id="S3.p7.12.m6.2.3.2" xref="S3.p7.12.m6.2.3.2.cmml"><mi id="S3.p7.12.m6.2.3.2.2" xref="S3.p7.12.m6.2.3.2.2.cmml">A</mi><mo id="S3.p7.12.m6.2.3.2.1" xref="S3.p7.12.m6.2.3.2.1.cmml">¯</mo></mover><mrow id="S3.p7.12.m6.2.2.2.4" xref="S3.p7.12.m6.2.2.2.3.cmml"><mo id="S3.p7.12.m6.2.2.2.4.1" stretchy="false" xref="S3.p7.12.m6.2.2.2.3.cmml">(</mo><mi id="S3.p7.12.m6.1.1.1.1" xref="S3.p7.12.m6.1.1.1.1.cmml">L</mi><mo id="S3.p7.12.m6.2.2.2.4.2" xref="S3.p7.12.m6.2.2.2.3.cmml">,</mo><mi id="S3.p7.12.m6.2.2.2.2" xref="S3.p7.12.m6.2.2.2.2.cmml">H</mi><mo id="S3.p7.12.m6.2.2.2.4.3" stretchy="false" xref="S3.p7.12.m6.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p7.12.m6.2b"><apply id="S3.p7.12.m6.2.3.cmml" xref="S3.p7.12.m6.2.3"><csymbol cd="ambiguous" id="S3.p7.12.m6.2.3.1.cmml" xref="S3.p7.12.m6.2.3">superscript</csymbol><apply id="S3.p7.12.m6.2.3.2.cmml" xref="S3.p7.12.m6.2.3.2"><ci id="S3.p7.12.m6.2.3.2.1.cmml" xref="S3.p7.12.m6.2.3.2.1">¯</ci><ci id="S3.p7.12.m6.2.3.2.2.cmml" xref="S3.p7.12.m6.2.3.2.2">𝐴</ci></apply><interval closure="open" id="S3.p7.12.m6.2.2.2.3.cmml" xref="S3.p7.12.m6.2.2.2.4"><ci id="S3.p7.12.m6.1.1.1.1.cmml" xref="S3.p7.12.m6.1.1.1.1">𝐿</ci><ci id="S3.p7.12.m6.2.2.2.2.cmml" xref="S3.p7.12.m6.2.2.2.2">𝐻</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.12.m6.2c">\bar{A}^{(L,H)}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.12.m6.2d">over¯ start_ARG italic_A end_ARG start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="i\in[1,s-1]" class="ltx_Math" display="inline" id="S3.p7.13.m7.2"><semantics id="S3.p7.13.m7.2a"><mrow id="S3.p7.13.m7.2.2" xref="S3.p7.13.m7.2.2.cmml"><mi id="S3.p7.13.m7.2.2.3" xref="S3.p7.13.m7.2.2.3.cmml">i</mi><mo id="S3.p7.13.m7.2.2.2" xref="S3.p7.13.m7.2.2.2.cmml">∈</mo><mrow id="S3.p7.13.m7.2.2.1.1" xref="S3.p7.13.m7.2.2.1.2.cmml"><mo id="S3.p7.13.m7.2.2.1.1.2" stretchy="false" xref="S3.p7.13.m7.2.2.1.2.cmml">[</mo><mn id="S3.p7.13.m7.1.1" xref="S3.p7.13.m7.1.1.cmml">1</mn><mo id="S3.p7.13.m7.2.2.1.1.3" xref="S3.p7.13.m7.2.2.1.2.cmml">,</mo><mrow id="S3.p7.13.m7.2.2.1.1.1" xref="S3.p7.13.m7.2.2.1.1.1.cmml"><mi id="S3.p7.13.m7.2.2.1.1.1.2" xref="S3.p7.13.m7.2.2.1.1.1.2.cmml">s</mi><mo id="S3.p7.13.m7.2.2.1.1.1.1" xref="S3.p7.13.m7.2.2.1.1.1.1.cmml">−</mo><mn id="S3.p7.13.m7.2.2.1.1.1.3" xref="S3.p7.13.m7.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S3.p7.13.m7.2.2.1.1.4" stretchy="false" xref="S3.p7.13.m7.2.2.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p7.13.m7.2b"><apply id="S3.p7.13.m7.2.2.cmml" xref="S3.p7.13.m7.2.2"><in id="S3.p7.13.m7.2.2.2.cmml" xref="S3.p7.13.m7.2.2.2"></in><ci id="S3.p7.13.m7.2.2.3.cmml" xref="S3.p7.13.m7.2.2.3">𝑖</ci><interval closure="closed" id="S3.p7.13.m7.2.2.1.2.cmml" xref="S3.p7.13.m7.2.2.1.1"><cn id="S3.p7.13.m7.1.1.cmml" type="integer" xref="S3.p7.13.m7.1.1">1</cn><apply id="S3.p7.13.m7.2.2.1.1.1.cmml" xref="S3.p7.13.m7.2.2.1.1.1"><minus id="S3.p7.13.m7.2.2.1.1.1.1.cmml" xref="S3.p7.13.m7.2.2.1.1.1.1"></minus><ci id="S3.p7.13.m7.2.2.1.1.1.2.cmml" xref="S3.p7.13.m7.2.2.1.1.1.2">𝑠</ci><cn id="S3.p7.13.m7.2.2.1.1.1.3.cmml" type="integer" xref="S3.p7.13.m7.2.2.1.1.1.3">1</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.13.m7.2c">i\in[1,s-1]</annotation><annotation encoding="application/x-llamapun" id="S3.p7.13.m7.2d">italic_i ∈ [ 1 , italic_s - 1 ]</annotation></semantics></math>.
Induction heads are expected to exhibit a high <math alttext="\bar{A}^{(L,H)}" class="ltx_Math" display="inline" id="S3.p7.14.m8.2"><semantics id="S3.p7.14.m8.2a"><msup id="S3.p7.14.m8.2.3" xref="S3.p7.14.m8.2.3.cmml"><mover accent="true" id="S3.p7.14.m8.2.3.2" xref="S3.p7.14.m8.2.3.2.cmml"><mi id="S3.p7.14.m8.2.3.2.2" xref="S3.p7.14.m8.2.3.2.2.cmml">A</mi><mo id="S3.p7.14.m8.2.3.2.1" xref="S3.p7.14.m8.2.3.2.1.cmml">¯</mo></mover><mrow id="S3.p7.14.m8.2.2.2.4" xref="S3.p7.14.m8.2.2.2.3.cmml"><mo id="S3.p7.14.m8.2.2.2.4.1" stretchy="false" xref="S3.p7.14.m8.2.2.2.3.cmml">(</mo><mi id="S3.p7.14.m8.1.1.1.1" xref="S3.p7.14.m8.1.1.1.1.cmml">L</mi><mo id="S3.p7.14.m8.2.2.2.4.2" xref="S3.p7.14.m8.2.2.2.3.cmml">,</mo><mi id="S3.p7.14.m8.2.2.2.2" xref="S3.p7.14.m8.2.2.2.2.cmml">H</mi><mo id="S3.p7.14.m8.2.2.2.4.3" stretchy="false" xref="S3.p7.14.m8.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p7.14.m8.2b"><apply id="S3.p7.14.m8.2.3.cmml" xref="S3.p7.14.m8.2.3"><csymbol cd="ambiguous" id="S3.p7.14.m8.2.3.1.cmml" xref="S3.p7.14.m8.2.3">superscript</csymbol><apply id="S3.p7.14.m8.2.3.2.cmml" xref="S3.p7.14.m8.2.3.2"><ci id="S3.p7.14.m8.2.3.2.1.cmml" xref="S3.p7.14.m8.2.3.2.1">¯</ci><ci id="S3.p7.14.m8.2.3.2.2.cmml" xref="S3.p7.14.m8.2.3.2.2">𝐴</ci></apply><interval closure="open" id="S3.p7.14.m8.2.2.2.3.cmml" xref="S3.p7.14.m8.2.2.2.4"><ci id="S3.p7.14.m8.1.1.1.1.cmml" xref="S3.p7.14.m8.1.1.1.1">𝐿</ci><ci id="S3.p7.14.m8.2.2.2.2.cmml" xref="S3.p7.14.m8.2.2.2.2">𝐻</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p7.14.m8.2c">\bar{A}^{(L,H)}</annotation><annotation encoding="application/x-llamapun" id="S3.p7.14.m8.2d">over¯ start_ARG italic_A end_ARG start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT</annotation></semantics></math> score.</p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.12"><math alttext="EP^{(L,H)}\in[-1,1]" class="ltx_Math" display="inline" id="S3.p8.1.m1.4"><semantics id="S3.p8.1.m1.4a"><mrow id="S3.p8.1.m1.4.4" xref="S3.p8.1.m1.4.4.cmml"><mrow id="S3.p8.1.m1.4.4.3" xref="S3.p8.1.m1.4.4.3.cmml"><mi id="S3.p8.1.m1.4.4.3.2" xref="S3.p8.1.m1.4.4.3.2.cmml">E</mi><mo id="S3.p8.1.m1.4.4.3.1" xref="S3.p8.1.m1.4.4.3.1.cmml">⁢</mo><msup id="S3.p8.1.m1.4.4.3.3" xref="S3.p8.1.m1.4.4.3.3.cmml"><mi id="S3.p8.1.m1.4.4.3.3.2" xref="S3.p8.1.m1.4.4.3.3.2.cmml">P</mi><mrow id="S3.p8.1.m1.2.2.2.4" xref="S3.p8.1.m1.2.2.2.3.cmml"><mo id="S3.p8.1.m1.2.2.2.4.1" stretchy="false" xref="S3.p8.1.m1.2.2.2.3.cmml">(</mo><mi id="S3.p8.1.m1.1.1.1.1" xref="S3.p8.1.m1.1.1.1.1.cmml">L</mi><mo id="S3.p8.1.m1.2.2.2.4.2" xref="S3.p8.1.m1.2.2.2.3.cmml">,</mo><mi id="S3.p8.1.m1.2.2.2.2" xref="S3.p8.1.m1.2.2.2.2.cmml">H</mi><mo id="S3.p8.1.m1.2.2.2.4.3" stretchy="false" xref="S3.p8.1.m1.2.2.2.3.cmml">)</mo></mrow></msup></mrow><mo id="S3.p8.1.m1.4.4.2" xref="S3.p8.1.m1.4.4.2.cmml">∈</mo><mrow id="S3.p8.1.m1.4.4.1.1" xref="S3.p8.1.m1.4.4.1.2.cmml"><mo id="S3.p8.1.m1.4.4.1.1.2" stretchy="false" xref="S3.p8.1.m1.4.4.1.2.cmml">[</mo><mrow id="S3.p8.1.m1.4.4.1.1.1" xref="S3.p8.1.m1.4.4.1.1.1.cmml"><mo id="S3.p8.1.m1.4.4.1.1.1a" xref="S3.p8.1.m1.4.4.1.1.1.cmml">−</mo><mn id="S3.p8.1.m1.4.4.1.1.1.2" xref="S3.p8.1.m1.4.4.1.1.1.2.cmml">1</mn></mrow><mo id="S3.p8.1.m1.4.4.1.1.3" xref="S3.p8.1.m1.4.4.1.2.cmml">,</mo><mn id="S3.p8.1.m1.3.3" xref="S3.p8.1.m1.3.3.cmml">1</mn><mo id="S3.p8.1.m1.4.4.1.1.4" stretchy="false" xref="S3.p8.1.m1.4.4.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p8.1.m1.4b"><apply id="S3.p8.1.m1.4.4.cmml" xref="S3.p8.1.m1.4.4"><in id="S3.p8.1.m1.4.4.2.cmml" xref="S3.p8.1.m1.4.4.2"></in><apply id="S3.p8.1.m1.4.4.3.cmml" xref="S3.p8.1.m1.4.4.3"><times id="S3.p8.1.m1.4.4.3.1.cmml" xref="S3.p8.1.m1.4.4.3.1"></times><ci id="S3.p8.1.m1.4.4.3.2.cmml" xref="S3.p8.1.m1.4.4.3.2">𝐸</ci><apply id="S3.p8.1.m1.4.4.3.3.cmml" xref="S3.p8.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S3.p8.1.m1.4.4.3.3.1.cmml" xref="S3.p8.1.m1.4.4.3.3">superscript</csymbol><ci id="S3.p8.1.m1.4.4.3.3.2.cmml" xref="S3.p8.1.m1.4.4.3.3.2">𝑃</ci><interval closure="open" id="S3.p8.1.m1.2.2.2.3.cmml" xref="S3.p8.1.m1.2.2.2.4"><ci id="S3.p8.1.m1.1.1.1.1.cmml" xref="S3.p8.1.m1.1.1.1.1">𝐿</ci><ci id="S3.p8.1.m1.2.2.2.2.cmml" xref="S3.p8.1.m1.2.2.2.2">𝐻</ci></interval></apply></apply><interval closure="closed" id="S3.p8.1.m1.4.4.1.2.cmml" xref="S3.p8.1.m1.4.4.1.1"><apply id="S3.p8.1.m1.4.4.1.1.1.cmml" xref="S3.p8.1.m1.4.4.1.1.1"><minus id="S3.p8.1.m1.4.4.1.1.1.1.cmml" xref="S3.p8.1.m1.4.4.1.1.1"></minus><cn id="S3.p8.1.m1.4.4.1.1.1.2.cmml" type="integer" xref="S3.p8.1.m1.4.4.1.1.1.2">1</cn></apply><cn id="S3.p8.1.m1.3.3.cmml" type="integer" xref="S3.p8.1.m1.3.3">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.1.m1.4c">EP^{(L,H)}\in[-1,1]</annotation><annotation encoding="application/x-llamapun" id="S3.p8.1.m1.4d">italic_E italic_P start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT ∈ [ - 1 , 1 ]</annotation></semantics></math> in Eq. <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S3.E1" title="In 3 Language Models “Grok” to Copy ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">1</span></a> is the eigenvalue positivity of the OV circuit <cite class="ltx_cite ltx_citemacro_cite">Elhage et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib4" title="">2021</a>)</cite> of the head: <math alttext="EP^{(L,H)}=\sum_{i}\lambda_{i}/\sum_{i}|\lambda_{i}|" class="ltx_Math" display="inline" id="S3.p8.2.m2.3"><semantics id="S3.p8.2.m2.3a"><mrow id="S3.p8.2.m2.3.3" xref="S3.p8.2.m2.3.3.cmml"><mrow id="S3.p8.2.m2.3.3.3" xref="S3.p8.2.m2.3.3.3.cmml"><mi id="S3.p8.2.m2.3.3.3.2" xref="S3.p8.2.m2.3.3.3.2.cmml">E</mi><mo id="S3.p8.2.m2.3.3.3.1" xref="S3.p8.2.m2.3.3.3.1.cmml">⁢</mo><msup id="S3.p8.2.m2.3.3.3.3" xref="S3.p8.2.m2.3.3.3.3.cmml"><mi id="S3.p8.2.m2.3.3.3.3.2" xref="S3.p8.2.m2.3.3.3.3.2.cmml">P</mi><mrow id="S3.p8.2.m2.2.2.2.4" xref="S3.p8.2.m2.2.2.2.3.cmml"><mo id="S3.p8.2.m2.2.2.2.4.1" stretchy="false" xref="S3.p8.2.m2.2.2.2.3.cmml">(</mo><mi id="S3.p8.2.m2.1.1.1.1" xref="S3.p8.2.m2.1.1.1.1.cmml">L</mi><mo id="S3.p8.2.m2.2.2.2.4.2" xref="S3.p8.2.m2.2.2.2.3.cmml">,</mo><mi id="S3.p8.2.m2.2.2.2.2" xref="S3.p8.2.m2.2.2.2.2.cmml">H</mi><mo id="S3.p8.2.m2.2.2.2.4.3" stretchy="false" xref="S3.p8.2.m2.2.2.2.3.cmml">)</mo></mrow></msup></mrow><mo id="S3.p8.2.m2.3.3.2" rspace="0.111em" xref="S3.p8.2.m2.3.3.2.cmml">=</mo><mrow id="S3.p8.2.m2.3.3.1" xref="S3.p8.2.m2.3.3.1.cmml"><msub id="S3.p8.2.m2.3.3.1.2" xref="S3.p8.2.m2.3.3.1.2.cmml"><mo id="S3.p8.2.m2.3.3.1.2.2" xref="S3.p8.2.m2.3.3.1.2.2.cmml">∑</mo><mi id="S3.p8.2.m2.3.3.1.2.3" xref="S3.p8.2.m2.3.3.1.2.3.cmml">i</mi></msub><mrow id="S3.p8.2.m2.3.3.1.1" xref="S3.p8.2.m2.3.3.1.1.cmml"><msub id="S3.p8.2.m2.3.3.1.1.3" xref="S3.p8.2.m2.3.3.1.1.3.cmml"><mi id="S3.p8.2.m2.3.3.1.1.3.2" xref="S3.p8.2.m2.3.3.1.1.3.2.cmml">λ</mi><mi id="S3.p8.2.m2.3.3.1.1.3.3" xref="S3.p8.2.m2.3.3.1.1.3.3.cmml">i</mi></msub><mo id="S3.p8.2.m2.3.3.1.1.2" rspace="0.055em" xref="S3.p8.2.m2.3.3.1.1.2.cmml">/</mo><mrow id="S3.p8.2.m2.3.3.1.1.1" xref="S3.p8.2.m2.3.3.1.1.1.cmml"><msub id="S3.p8.2.m2.3.3.1.1.1.2" xref="S3.p8.2.m2.3.3.1.1.1.2.cmml"><mo id="S3.p8.2.m2.3.3.1.1.1.2.2" rspace="0em" xref="S3.p8.2.m2.3.3.1.1.1.2.2.cmml">∑</mo><mi id="S3.p8.2.m2.3.3.1.1.1.2.3" xref="S3.p8.2.m2.3.3.1.1.1.2.3.cmml">i</mi></msub><mrow id="S3.p8.2.m2.3.3.1.1.1.1.1" xref="S3.p8.2.m2.3.3.1.1.1.1.2.cmml"><mo id="S3.p8.2.m2.3.3.1.1.1.1.1.2" stretchy="false" xref="S3.p8.2.m2.3.3.1.1.1.1.2.1.cmml">|</mo><msub id="S3.p8.2.m2.3.3.1.1.1.1.1.1" xref="S3.p8.2.m2.3.3.1.1.1.1.1.1.cmml"><mi id="S3.p8.2.m2.3.3.1.1.1.1.1.1.2" xref="S3.p8.2.m2.3.3.1.1.1.1.1.1.2.cmml">λ</mi><mi id="S3.p8.2.m2.3.3.1.1.1.1.1.1.3" xref="S3.p8.2.m2.3.3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p8.2.m2.3.3.1.1.1.1.1.3" stretchy="false" xref="S3.p8.2.m2.3.3.1.1.1.1.2.1.cmml">|</mo></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p8.2.m2.3b"><apply id="S3.p8.2.m2.3.3.cmml" xref="S3.p8.2.m2.3.3"><eq id="S3.p8.2.m2.3.3.2.cmml" xref="S3.p8.2.m2.3.3.2"></eq><apply id="S3.p8.2.m2.3.3.3.cmml" xref="S3.p8.2.m2.3.3.3"><times id="S3.p8.2.m2.3.3.3.1.cmml" xref="S3.p8.2.m2.3.3.3.1"></times><ci id="S3.p8.2.m2.3.3.3.2.cmml" xref="S3.p8.2.m2.3.3.3.2">𝐸</ci><apply id="S3.p8.2.m2.3.3.3.3.cmml" xref="S3.p8.2.m2.3.3.3.3"><csymbol cd="ambiguous" id="S3.p8.2.m2.3.3.3.3.1.cmml" xref="S3.p8.2.m2.3.3.3.3">superscript</csymbol><ci id="S3.p8.2.m2.3.3.3.3.2.cmml" xref="S3.p8.2.m2.3.3.3.3.2">𝑃</ci><interval closure="open" id="S3.p8.2.m2.2.2.2.3.cmml" xref="S3.p8.2.m2.2.2.2.4"><ci id="S3.p8.2.m2.1.1.1.1.cmml" xref="S3.p8.2.m2.1.1.1.1">𝐿</ci><ci id="S3.p8.2.m2.2.2.2.2.cmml" xref="S3.p8.2.m2.2.2.2.2">𝐻</ci></interval></apply></apply><apply id="S3.p8.2.m2.3.3.1.cmml" xref="S3.p8.2.m2.3.3.1"><apply id="S3.p8.2.m2.3.3.1.2.cmml" xref="S3.p8.2.m2.3.3.1.2"><csymbol cd="ambiguous" id="S3.p8.2.m2.3.3.1.2.1.cmml" xref="S3.p8.2.m2.3.3.1.2">subscript</csymbol><sum id="S3.p8.2.m2.3.3.1.2.2.cmml" xref="S3.p8.2.m2.3.3.1.2.2"></sum><ci id="S3.p8.2.m2.3.3.1.2.3.cmml" xref="S3.p8.2.m2.3.3.1.2.3">𝑖</ci></apply><apply id="S3.p8.2.m2.3.3.1.1.cmml" xref="S3.p8.2.m2.3.3.1.1"><divide id="S3.p8.2.m2.3.3.1.1.2.cmml" xref="S3.p8.2.m2.3.3.1.1.2"></divide><apply id="S3.p8.2.m2.3.3.1.1.3.cmml" xref="S3.p8.2.m2.3.3.1.1.3"><csymbol cd="ambiguous" id="S3.p8.2.m2.3.3.1.1.3.1.cmml" xref="S3.p8.2.m2.3.3.1.1.3">subscript</csymbol><ci id="S3.p8.2.m2.3.3.1.1.3.2.cmml" xref="S3.p8.2.m2.3.3.1.1.3.2">𝜆</ci><ci id="S3.p8.2.m2.3.3.1.1.3.3.cmml" xref="S3.p8.2.m2.3.3.1.1.3.3">𝑖</ci></apply><apply id="S3.p8.2.m2.3.3.1.1.1.cmml" xref="S3.p8.2.m2.3.3.1.1.1"><apply id="S3.p8.2.m2.3.3.1.1.1.2.cmml" xref="S3.p8.2.m2.3.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.p8.2.m2.3.3.1.1.1.2.1.cmml" xref="S3.p8.2.m2.3.3.1.1.1.2">subscript</csymbol><sum id="S3.p8.2.m2.3.3.1.1.1.2.2.cmml" xref="S3.p8.2.m2.3.3.1.1.1.2.2"></sum><ci id="S3.p8.2.m2.3.3.1.1.1.2.3.cmml" xref="S3.p8.2.m2.3.3.1.1.1.2.3">𝑖</ci></apply><apply id="S3.p8.2.m2.3.3.1.1.1.1.2.cmml" xref="S3.p8.2.m2.3.3.1.1.1.1.1"><abs id="S3.p8.2.m2.3.3.1.1.1.1.2.1.cmml" xref="S3.p8.2.m2.3.3.1.1.1.1.1.2"></abs><apply id="S3.p8.2.m2.3.3.1.1.1.1.1.1.cmml" xref="S3.p8.2.m2.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p8.2.m2.3.3.1.1.1.1.1.1.1.cmml" xref="S3.p8.2.m2.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p8.2.m2.3.3.1.1.1.1.1.1.2.cmml" xref="S3.p8.2.m2.3.3.1.1.1.1.1.1.2">𝜆</ci><ci id="S3.p8.2.m2.3.3.1.1.1.1.1.1.3.cmml" xref="S3.p8.2.m2.3.3.1.1.1.1.1.1.3">𝑖</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.2.m2.3c">EP^{(L,H)}=\sum_{i}\lambda_{i}/\sum_{i}|\lambda_{i}|</annotation><annotation encoding="application/x-llamapun" id="S3.p8.2.m2.3d">italic_E italic_P start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT = ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT | italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT |</annotation></semantics></math>.
<math alttext="\lambda_{i}" class="ltx_Math" display="inline" id="S3.p8.3.m3.1"><semantics id="S3.p8.3.m3.1a"><msub id="S3.p8.3.m3.1.1" xref="S3.p8.3.m3.1.1.cmml"><mi id="S3.p8.3.m3.1.1.2" xref="S3.p8.3.m3.1.1.2.cmml">λ</mi><mi id="S3.p8.3.m3.1.1.3" xref="S3.p8.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p8.3.m3.1b"><apply id="S3.p8.3.m3.1.1.cmml" xref="S3.p8.3.m3.1.1"><csymbol cd="ambiguous" id="S3.p8.3.m3.1.1.1.cmml" xref="S3.p8.3.m3.1.1">subscript</csymbol><ci id="S3.p8.3.m3.1.1.2.cmml" xref="S3.p8.3.m3.1.1.2">𝜆</ci><ci id="S3.p8.3.m3.1.1.3.cmml" xref="S3.p8.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.3.m3.1c">\lambda_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p8.3.m3.1d">italic_λ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the <math alttext="i" class="ltx_Math" display="inline" id="S3.p8.4.m4.1"><semantics id="S3.p8.4.m4.1a"><mi id="S3.p8.4.m4.1.1" xref="S3.p8.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p8.4.m4.1b"><ci id="S3.p8.4.m4.1.1.cmml" xref="S3.p8.4.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.4.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.p8.4.m4.1d">italic_i</annotation></semantics></math>-th eigenvalue of <math alttext="(W_{U}W^{(L,H)}_{O}W^{(L,H)}_{V}W_{E})" class="ltx_Math" display="inline" id="S3.p8.5.m5.5"><semantics id="S3.p8.5.m5.5a"><mrow id="S3.p8.5.m5.5.5.1" xref="S3.p8.5.m5.5.5.1.1.cmml"><mo id="S3.p8.5.m5.5.5.1.2" stretchy="false" xref="S3.p8.5.m5.5.5.1.1.cmml">(</mo><mrow id="S3.p8.5.m5.5.5.1.1" xref="S3.p8.5.m5.5.5.1.1.cmml"><msub id="S3.p8.5.m5.5.5.1.1.2" xref="S3.p8.5.m5.5.5.1.1.2.cmml"><mi id="S3.p8.5.m5.5.5.1.1.2.2" xref="S3.p8.5.m5.5.5.1.1.2.2.cmml">W</mi><mi id="S3.p8.5.m5.5.5.1.1.2.3" xref="S3.p8.5.m5.5.5.1.1.2.3.cmml">U</mi></msub><mo id="S3.p8.5.m5.5.5.1.1.1" xref="S3.p8.5.m5.5.5.1.1.1.cmml">⁢</mo><msubsup id="S3.p8.5.m5.5.5.1.1.3" xref="S3.p8.5.m5.5.5.1.1.3.cmml"><mi id="S3.p8.5.m5.5.5.1.1.3.2.2" xref="S3.p8.5.m5.5.5.1.1.3.2.2.cmml">W</mi><mi id="S3.p8.5.m5.5.5.1.1.3.3" xref="S3.p8.5.m5.5.5.1.1.3.3.cmml">O</mi><mrow id="S3.p8.5.m5.2.2.2.4" xref="S3.p8.5.m5.2.2.2.3.cmml"><mo id="S3.p8.5.m5.2.2.2.4.1" stretchy="false" xref="S3.p8.5.m5.2.2.2.3.cmml">(</mo><mi id="S3.p8.5.m5.1.1.1.1" xref="S3.p8.5.m5.1.1.1.1.cmml">L</mi><mo id="S3.p8.5.m5.2.2.2.4.2" xref="S3.p8.5.m5.2.2.2.3.cmml">,</mo><mi id="S3.p8.5.m5.2.2.2.2" xref="S3.p8.5.m5.2.2.2.2.cmml">H</mi><mo id="S3.p8.5.m5.2.2.2.4.3" stretchy="false" xref="S3.p8.5.m5.2.2.2.3.cmml">)</mo></mrow></msubsup><mo id="S3.p8.5.m5.5.5.1.1.1a" xref="S3.p8.5.m5.5.5.1.1.1.cmml">⁢</mo><msubsup id="S3.p8.5.m5.5.5.1.1.4" xref="S3.p8.5.m5.5.5.1.1.4.cmml"><mi id="S3.p8.5.m5.5.5.1.1.4.2.2" xref="S3.p8.5.m5.5.5.1.1.4.2.2.cmml">W</mi><mi id="S3.p8.5.m5.5.5.1.1.4.3" xref="S3.p8.5.m5.5.5.1.1.4.3.cmml">V</mi><mrow id="S3.p8.5.m5.4.4.2.4" xref="S3.p8.5.m5.4.4.2.3.cmml"><mo id="S3.p8.5.m5.4.4.2.4.1" stretchy="false" xref="S3.p8.5.m5.4.4.2.3.cmml">(</mo><mi id="S3.p8.5.m5.3.3.1.1" xref="S3.p8.5.m5.3.3.1.1.cmml">L</mi><mo id="S3.p8.5.m5.4.4.2.4.2" xref="S3.p8.5.m5.4.4.2.3.cmml">,</mo><mi id="S3.p8.5.m5.4.4.2.2" xref="S3.p8.5.m5.4.4.2.2.cmml">H</mi><mo id="S3.p8.5.m5.4.4.2.4.3" stretchy="false" xref="S3.p8.5.m5.4.4.2.3.cmml">)</mo></mrow></msubsup><mo id="S3.p8.5.m5.5.5.1.1.1b" xref="S3.p8.5.m5.5.5.1.1.1.cmml">⁢</mo><msub id="S3.p8.5.m5.5.5.1.1.5" xref="S3.p8.5.m5.5.5.1.1.5.cmml"><mi id="S3.p8.5.m5.5.5.1.1.5.2" xref="S3.p8.5.m5.5.5.1.1.5.2.cmml">W</mi><mi id="S3.p8.5.m5.5.5.1.1.5.3" xref="S3.p8.5.m5.5.5.1.1.5.3.cmml">E</mi></msub></mrow><mo id="S3.p8.5.m5.5.5.1.3" stretchy="false" xref="S3.p8.5.m5.5.5.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p8.5.m5.5b"><apply id="S3.p8.5.m5.5.5.1.1.cmml" xref="S3.p8.5.m5.5.5.1"><times id="S3.p8.5.m5.5.5.1.1.1.cmml" xref="S3.p8.5.m5.5.5.1.1.1"></times><apply id="S3.p8.5.m5.5.5.1.1.2.cmml" xref="S3.p8.5.m5.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.p8.5.m5.5.5.1.1.2.1.cmml" xref="S3.p8.5.m5.5.5.1.1.2">subscript</csymbol><ci id="S3.p8.5.m5.5.5.1.1.2.2.cmml" xref="S3.p8.5.m5.5.5.1.1.2.2">𝑊</ci><ci id="S3.p8.5.m5.5.5.1.1.2.3.cmml" xref="S3.p8.5.m5.5.5.1.1.2.3">𝑈</ci></apply><apply id="S3.p8.5.m5.5.5.1.1.3.cmml" xref="S3.p8.5.m5.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.p8.5.m5.5.5.1.1.3.1.cmml" xref="S3.p8.5.m5.5.5.1.1.3">subscript</csymbol><apply id="S3.p8.5.m5.5.5.1.1.3.2.cmml" xref="S3.p8.5.m5.5.5.1.1.3"><csymbol cd="ambiguous" id="S3.p8.5.m5.5.5.1.1.3.2.1.cmml" xref="S3.p8.5.m5.5.5.1.1.3">superscript</csymbol><ci id="S3.p8.5.m5.5.5.1.1.3.2.2.cmml" xref="S3.p8.5.m5.5.5.1.1.3.2.2">𝑊</ci><interval closure="open" id="S3.p8.5.m5.2.2.2.3.cmml" xref="S3.p8.5.m5.2.2.2.4"><ci id="S3.p8.5.m5.1.1.1.1.cmml" xref="S3.p8.5.m5.1.1.1.1">𝐿</ci><ci id="S3.p8.5.m5.2.2.2.2.cmml" xref="S3.p8.5.m5.2.2.2.2">𝐻</ci></interval></apply><ci id="S3.p8.5.m5.5.5.1.1.3.3.cmml" xref="S3.p8.5.m5.5.5.1.1.3.3">𝑂</ci></apply><apply id="S3.p8.5.m5.5.5.1.1.4.cmml" xref="S3.p8.5.m5.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.p8.5.m5.5.5.1.1.4.1.cmml" xref="S3.p8.5.m5.5.5.1.1.4">subscript</csymbol><apply id="S3.p8.5.m5.5.5.1.1.4.2.cmml" xref="S3.p8.5.m5.5.5.1.1.4"><csymbol cd="ambiguous" id="S3.p8.5.m5.5.5.1.1.4.2.1.cmml" xref="S3.p8.5.m5.5.5.1.1.4">superscript</csymbol><ci id="S3.p8.5.m5.5.5.1.1.4.2.2.cmml" xref="S3.p8.5.m5.5.5.1.1.4.2.2">𝑊</ci><interval closure="open" id="S3.p8.5.m5.4.4.2.3.cmml" xref="S3.p8.5.m5.4.4.2.4"><ci id="S3.p8.5.m5.3.3.1.1.cmml" xref="S3.p8.5.m5.3.3.1.1">𝐿</ci><ci id="S3.p8.5.m5.4.4.2.2.cmml" xref="S3.p8.5.m5.4.4.2.2">𝐻</ci></interval></apply><ci id="S3.p8.5.m5.5.5.1.1.4.3.cmml" xref="S3.p8.5.m5.5.5.1.1.4.3">𝑉</ci></apply><apply id="S3.p8.5.m5.5.5.1.1.5.cmml" xref="S3.p8.5.m5.5.5.1.1.5"><csymbol cd="ambiguous" id="S3.p8.5.m5.5.5.1.1.5.1.cmml" xref="S3.p8.5.m5.5.5.1.1.5">subscript</csymbol><ci id="S3.p8.5.m5.5.5.1.1.5.2.cmml" xref="S3.p8.5.m5.5.5.1.1.5.2">𝑊</ci><ci id="S3.p8.5.m5.5.5.1.1.5.3.cmml" xref="S3.p8.5.m5.5.5.1.1.5.3">𝐸</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.5.m5.5c">(W_{U}W^{(L,H)}_{O}W^{(L,H)}_{V}W_{E})</annotation><annotation encoding="application/x-llamapun" id="S3.p8.5.m5.5d">( italic_W start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT italic_W start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT italic_W start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT )</annotation></semantics></math>, and <math alttext="W^{(L,H)}_{O}" class="ltx_Math" display="inline" id="S3.p8.6.m6.2"><semantics id="S3.p8.6.m6.2a"><msubsup id="S3.p8.6.m6.2.3" xref="S3.p8.6.m6.2.3.cmml"><mi id="S3.p8.6.m6.2.3.2.2" xref="S3.p8.6.m6.2.3.2.2.cmml">W</mi><mi id="S3.p8.6.m6.2.3.3" xref="S3.p8.6.m6.2.3.3.cmml">O</mi><mrow id="S3.p8.6.m6.2.2.2.4" xref="S3.p8.6.m6.2.2.2.3.cmml"><mo id="S3.p8.6.m6.2.2.2.4.1" stretchy="false" xref="S3.p8.6.m6.2.2.2.3.cmml">(</mo><mi id="S3.p8.6.m6.1.1.1.1" xref="S3.p8.6.m6.1.1.1.1.cmml">L</mi><mo id="S3.p8.6.m6.2.2.2.4.2" xref="S3.p8.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.p8.6.m6.2.2.2.2" xref="S3.p8.6.m6.2.2.2.2.cmml">H</mi><mo id="S3.p8.6.m6.2.2.2.4.3" stretchy="false" xref="S3.p8.6.m6.2.2.2.3.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.p8.6.m6.2b"><apply id="S3.p8.6.m6.2.3.cmml" xref="S3.p8.6.m6.2.3"><csymbol cd="ambiguous" id="S3.p8.6.m6.2.3.1.cmml" xref="S3.p8.6.m6.2.3">subscript</csymbol><apply id="S3.p8.6.m6.2.3.2.cmml" xref="S3.p8.6.m6.2.3"><csymbol cd="ambiguous" id="S3.p8.6.m6.2.3.2.1.cmml" xref="S3.p8.6.m6.2.3">superscript</csymbol><ci id="S3.p8.6.m6.2.3.2.2.cmml" xref="S3.p8.6.m6.2.3.2.2">𝑊</ci><interval closure="open" id="S3.p8.6.m6.2.2.2.3.cmml" xref="S3.p8.6.m6.2.2.2.4"><ci id="S3.p8.6.m6.1.1.1.1.cmml" xref="S3.p8.6.m6.1.1.1.1">𝐿</ci><ci id="S3.p8.6.m6.2.2.2.2.cmml" xref="S3.p8.6.m6.2.2.2.2">𝐻</ci></interval></apply><ci id="S3.p8.6.m6.2.3.3.cmml" xref="S3.p8.6.m6.2.3.3">𝑂</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.6.m6.2c">W^{(L,H)}_{O}</annotation><annotation encoding="application/x-llamapun" id="S3.p8.6.m6.2d">italic_W start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="W^{(L,H)}_{V}" class="ltx_Math" display="inline" id="S3.p8.7.m7.2"><semantics id="S3.p8.7.m7.2a"><msubsup id="S3.p8.7.m7.2.3" xref="S3.p8.7.m7.2.3.cmml"><mi id="S3.p8.7.m7.2.3.2.2" xref="S3.p8.7.m7.2.3.2.2.cmml">W</mi><mi id="S3.p8.7.m7.2.3.3" xref="S3.p8.7.m7.2.3.3.cmml">V</mi><mrow id="S3.p8.7.m7.2.2.2.4" xref="S3.p8.7.m7.2.2.2.3.cmml"><mo id="S3.p8.7.m7.2.2.2.4.1" stretchy="false" xref="S3.p8.7.m7.2.2.2.3.cmml">(</mo><mi id="S3.p8.7.m7.1.1.1.1" xref="S3.p8.7.m7.1.1.1.1.cmml">L</mi><mo id="S3.p8.7.m7.2.2.2.4.2" xref="S3.p8.7.m7.2.2.2.3.cmml">,</mo><mi id="S3.p8.7.m7.2.2.2.2" xref="S3.p8.7.m7.2.2.2.2.cmml">H</mi><mo id="S3.p8.7.m7.2.2.2.4.3" stretchy="false" xref="S3.p8.7.m7.2.2.2.3.cmml">)</mo></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.p8.7.m7.2b"><apply id="S3.p8.7.m7.2.3.cmml" xref="S3.p8.7.m7.2.3"><csymbol cd="ambiguous" id="S3.p8.7.m7.2.3.1.cmml" xref="S3.p8.7.m7.2.3">subscript</csymbol><apply id="S3.p8.7.m7.2.3.2.cmml" xref="S3.p8.7.m7.2.3"><csymbol cd="ambiguous" id="S3.p8.7.m7.2.3.2.1.cmml" xref="S3.p8.7.m7.2.3">superscript</csymbol><ci id="S3.p8.7.m7.2.3.2.2.cmml" xref="S3.p8.7.m7.2.3.2.2">𝑊</ci><interval closure="open" id="S3.p8.7.m7.2.2.2.3.cmml" xref="S3.p8.7.m7.2.2.2.4"><ci id="S3.p8.7.m7.1.1.1.1.cmml" xref="S3.p8.7.m7.1.1.1.1">𝐿</ci><ci id="S3.p8.7.m7.2.2.2.2.cmml" xref="S3.p8.7.m7.2.2.2.2">𝐻</ci></interval></apply><ci id="S3.p8.7.m7.2.3.3.cmml" xref="S3.p8.7.m7.2.3.3">𝑉</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.7.m7.2c">W^{(L,H)}_{V}</annotation><annotation encoding="application/x-llamapun" id="S3.p8.7.m7.2d">italic_W start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT</annotation></semantics></math> are weights of the value and output projection in head (<math alttext="L,H" class="ltx_Math" display="inline" id="S3.p8.8.m8.2"><semantics id="S3.p8.8.m8.2a"><mrow id="S3.p8.8.m8.2.3.2" xref="S3.p8.8.m8.2.3.1.cmml"><mi id="S3.p8.8.m8.1.1" xref="S3.p8.8.m8.1.1.cmml">L</mi><mo id="S3.p8.8.m8.2.3.2.1" xref="S3.p8.8.m8.2.3.1.cmml">,</mo><mi id="S3.p8.8.m8.2.2" xref="S3.p8.8.m8.2.2.cmml">H</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.p8.8.m8.2b"><list id="S3.p8.8.m8.2.3.1.cmml" xref="S3.p8.8.m8.2.3.2"><ci id="S3.p8.8.m8.1.1.cmml" xref="S3.p8.8.m8.1.1">𝐿</ci><ci id="S3.p8.8.m8.2.2.cmml" xref="S3.p8.8.m8.2.2">𝐻</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.8.m8.2c">L,H</annotation><annotation encoding="application/x-llamapun" id="S3.p8.8.m8.2d">italic_L , italic_H</annotation></semantics></math>), while <math alttext="W_{E}" class="ltx_Math" display="inline" id="S3.p8.9.m9.1"><semantics id="S3.p8.9.m9.1a"><msub id="S3.p8.9.m9.1.1" xref="S3.p8.9.m9.1.1.cmml"><mi id="S3.p8.9.m9.1.1.2" xref="S3.p8.9.m9.1.1.2.cmml">W</mi><mi id="S3.p8.9.m9.1.1.3" xref="S3.p8.9.m9.1.1.3.cmml">E</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p8.9.m9.1b"><apply id="S3.p8.9.m9.1.1.cmml" xref="S3.p8.9.m9.1.1"><csymbol cd="ambiguous" id="S3.p8.9.m9.1.1.1.cmml" xref="S3.p8.9.m9.1.1">subscript</csymbol><ci id="S3.p8.9.m9.1.1.2.cmml" xref="S3.p8.9.m9.1.1.2">𝑊</ci><ci id="S3.p8.9.m9.1.1.3.cmml" xref="S3.p8.9.m9.1.1.3">𝐸</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.9.m9.1c">W_{E}</annotation><annotation encoding="application/x-llamapun" id="S3.p8.9.m9.1d">italic_W start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="W_{U}" class="ltx_Math" display="inline" id="S3.p8.10.m10.1"><semantics id="S3.p8.10.m10.1a"><msub id="S3.p8.10.m10.1.1" xref="S3.p8.10.m10.1.1.cmml"><mi id="S3.p8.10.m10.1.1.2" xref="S3.p8.10.m10.1.1.2.cmml">W</mi><mi id="S3.p8.10.m10.1.1.3" xref="S3.p8.10.m10.1.1.3.cmml">U</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p8.10.m10.1b"><apply id="S3.p8.10.m10.1.1.cmml" xref="S3.p8.10.m10.1.1"><csymbol cd="ambiguous" id="S3.p8.10.m10.1.1.1.cmml" xref="S3.p8.10.m10.1.1">subscript</csymbol><ci id="S3.p8.10.m10.1.1.2.cmml" xref="S3.p8.10.m10.1.1.2">𝑊</ci><ci id="S3.p8.10.m10.1.1.3.cmml" xref="S3.p8.10.m10.1.1.3">𝑈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.10.m10.1c">W_{U}</annotation><annotation encoding="application/x-llamapun" id="S3.p8.10.m10.1d">italic_W start_POSTSUBSCRIPT italic_U end_POSTSUBSCRIPT</annotation></semantics></math> are model’s embedding and unembedding matrices.
A high <math alttext="EP^{(L,H)}" class="ltx_Math" display="inline" id="S3.p8.11.m11.2"><semantics id="S3.p8.11.m11.2a"><mrow id="S3.p8.11.m11.2.3" xref="S3.p8.11.m11.2.3.cmml"><mi id="S3.p8.11.m11.2.3.2" xref="S3.p8.11.m11.2.3.2.cmml">E</mi><mo id="S3.p8.11.m11.2.3.1" xref="S3.p8.11.m11.2.3.1.cmml">⁢</mo><msup id="S3.p8.11.m11.2.3.3" xref="S3.p8.11.m11.2.3.3.cmml"><mi id="S3.p8.11.m11.2.3.3.2" xref="S3.p8.11.m11.2.3.3.2.cmml">P</mi><mrow id="S3.p8.11.m11.2.2.2.4" xref="S3.p8.11.m11.2.2.2.3.cmml"><mo id="S3.p8.11.m11.2.2.2.4.1" stretchy="false" xref="S3.p8.11.m11.2.2.2.3.cmml">(</mo><mi id="S3.p8.11.m11.1.1.1.1" xref="S3.p8.11.m11.1.1.1.1.cmml">L</mi><mo id="S3.p8.11.m11.2.2.2.4.2" xref="S3.p8.11.m11.2.2.2.3.cmml">,</mo><mi id="S3.p8.11.m11.2.2.2.2" xref="S3.p8.11.m11.2.2.2.2.cmml">H</mi><mo id="S3.p8.11.m11.2.2.2.4.3" stretchy="false" xref="S3.p8.11.m11.2.2.2.3.cmml">)</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.p8.11.m11.2b"><apply id="S3.p8.11.m11.2.3.cmml" xref="S3.p8.11.m11.2.3"><times id="S3.p8.11.m11.2.3.1.cmml" xref="S3.p8.11.m11.2.3.1"></times><ci id="S3.p8.11.m11.2.3.2.cmml" xref="S3.p8.11.m11.2.3.2">𝐸</ci><apply id="S3.p8.11.m11.2.3.3.cmml" xref="S3.p8.11.m11.2.3.3"><csymbol cd="ambiguous" id="S3.p8.11.m11.2.3.3.1.cmml" xref="S3.p8.11.m11.2.3.3">superscript</csymbol><ci id="S3.p8.11.m11.2.3.3.2.cmml" xref="S3.p8.11.m11.2.3.3.2">𝑃</ci><interval closure="open" id="S3.p8.11.m11.2.2.2.3.cmml" xref="S3.p8.11.m11.2.2.2.4"><ci id="S3.p8.11.m11.1.1.1.1.cmml" xref="S3.p8.11.m11.1.1.1.1">𝐿</ci><ci id="S3.p8.11.m11.2.2.2.2.cmml" xref="S3.p8.11.m11.2.2.2.2">𝐻</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.11.m11.2c">EP^{(L,H)}</annotation><annotation encoding="application/x-llamapun" id="S3.p8.11.m11.2d">italic_E italic_P start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT</annotation></semantics></math> implies that the head copies the tokens it attends to as output.
Overall, a higher <math alttext="I^{(L,H)}" class="ltx_Math" display="inline" id="S3.p8.12.m12.2"><semantics id="S3.p8.12.m12.2a"><msup id="S3.p8.12.m12.2.3" xref="S3.p8.12.m12.2.3.cmml"><mi id="S3.p8.12.m12.2.3.2" xref="S3.p8.12.m12.2.3.2.cmml">I</mi><mrow id="S3.p8.12.m12.2.2.2.4" xref="S3.p8.12.m12.2.2.2.3.cmml"><mo id="S3.p8.12.m12.2.2.2.4.1" stretchy="false" xref="S3.p8.12.m12.2.2.2.3.cmml">(</mo><mi id="S3.p8.12.m12.1.1.1.1" xref="S3.p8.12.m12.1.1.1.1.cmml">L</mi><mo id="S3.p8.12.m12.2.2.2.4.2" xref="S3.p8.12.m12.2.2.2.3.cmml">,</mo><mi id="S3.p8.12.m12.2.2.2.2" xref="S3.p8.12.m12.2.2.2.2.cmml">H</mi><mo id="S3.p8.12.m12.2.2.2.4.3" stretchy="false" xref="S3.p8.12.m12.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.p8.12.m12.2b"><apply id="S3.p8.12.m12.2.3.cmml" xref="S3.p8.12.m12.2.3"><csymbol cd="ambiguous" id="S3.p8.12.m12.2.3.1.cmml" xref="S3.p8.12.m12.2.3">superscript</csymbol><ci id="S3.p8.12.m12.2.3.2.cmml" xref="S3.p8.12.m12.2.3.2">𝐼</ci><interval closure="open" id="S3.p8.12.m12.2.2.2.3.cmml" xref="S3.p8.12.m12.2.2.2.4"><ci id="S3.p8.12.m12.1.1.1.1.cmml" xref="S3.p8.12.m12.1.1.1.1">𝐿</ci><ci id="S3.p8.12.m12.2.2.2.2.cmml" xref="S3.p8.12.m12.2.2.2.2">𝐻</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p8.12.m12.2c">I^{(L,H)}</annotation><annotation encoding="application/x-llamapun" id="S3.p8.12.m12.2d">italic_I start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT</annotation></semantics></math> indicates a stronger induction head.</p>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S3.F6" title="Figure 6 ‣ 3 Language Models “Grok” to Copy ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the evolution of induction heads during training, revealing that these heads develop from shallower to deeper layers.
This observation is consistent with <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib16" title="">2024</a>)</cite>, which observes that after grokking, models develop circuits in deeper layers than before grokking.</p>
</div>
<figure class="ltx_figure" id="S3.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="570" id="S3.F6.g1" src="x6.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The evolution of induction heads during training.
A bar’s height represents the <math alttext="I^{(L,H)}" class="ltx_Math" display="inline" id="S3.F6.2.m1.2"><semantics id="S3.F6.2.m1.2b"><msup id="S3.F6.2.m1.2.3" xref="S3.F6.2.m1.2.3.cmml"><mi id="S3.F6.2.m1.2.3.2" xref="S3.F6.2.m1.2.3.2.cmml">I</mi><mrow id="S3.F6.2.m1.2.2.2.4" xref="S3.F6.2.m1.2.2.2.3.cmml"><mo id="S3.F6.2.m1.2.2.2.4.1" stretchy="false" xref="S3.F6.2.m1.2.2.2.3.cmml">(</mo><mi id="S3.F6.2.m1.1.1.1.1" xref="S3.F6.2.m1.1.1.1.1.cmml">L</mi><mo id="S3.F6.2.m1.2.2.2.4.2" xref="S3.F6.2.m1.2.2.2.3.cmml">,</mo><mi id="S3.F6.2.m1.2.2.2.2" xref="S3.F6.2.m1.2.2.2.2.cmml">H</mi><mo id="S3.F6.2.m1.2.2.2.4.3" stretchy="false" xref="S3.F6.2.m1.2.2.2.3.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.F6.2.m1.2c"><apply id="S3.F6.2.m1.2.3.cmml" xref="S3.F6.2.m1.2.3"><csymbol cd="ambiguous" id="S3.F6.2.m1.2.3.1.cmml" xref="S3.F6.2.m1.2.3">superscript</csymbol><ci id="S3.F6.2.m1.2.3.2.cmml" xref="S3.F6.2.m1.2.3.2">𝐼</ci><interval closure="open" id="S3.F6.2.m1.2.2.2.3.cmml" xref="S3.F6.2.m1.2.2.2.4"><ci id="S3.F6.2.m1.1.1.1.1.cmml" xref="S3.F6.2.m1.1.1.1.1">𝐿</ci><ci id="S3.F6.2.m1.2.2.2.2.cmml" xref="S3.F6.2.m1.2.2.2.2">𝐻</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.2.m1.2d">I^{(L,H)}</annotation><annotation encoding="application/x-llamapun" id="S3.F6.2.m1.2e">italic_I start_POSTSUPERSCRIPT ( italic_L , italic_H ) end_POSTSUPERSCRIPT</annotation></semantics></math> value.
Bars exhibiting larger values positioned nearer to the X-axis.
The results in this figure are from a single model.</figcaption>
</figure>
<figure class="ltx_figure" id="S3.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="546" id="S3.F7.g1" src="x7.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Regularization positively impacts the grokked copying.
Compared with vanilla models, dropout accelerates the grokking process, advancing the abrupt accuracy increase from 15B tokens to 10B tokens, albeit with increased fluctuation in the evolutionary dynamics.
Both techniques improve the final accuracy.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Application</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Viewing the development of context copying as a special grokking inspires us to examine the impact of regularization, as it is known to enhance grokking <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib16" title="">2024</a>); Nanda et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#bib.bib11" title="">2023</a>)</cite>.
We train models using (1) attention dropout with a 10% probability and (2) weight decay with <math alttext="\lambda=0.1" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><mrow id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">λ</mi><mo id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1.cmml">=</mo><mn id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><eq id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1"></eq><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">𝜆</ci><cn id="S4.p1.1.m1.1.1.3.cmml" type="float" xref="S4.p1.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">\lambda=0.1</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_λ = 0.1</annotation></semantics></math>.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S3.F7" title="Figure 7 ‣ 3 Language Models “Grok” to Copy ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">7</span></a> shows that these techniques’ positive impact: with dropout, the model groks to copy earlier; both techniques improve the accuracy compared to the vanilla model. Future innovations on improving the in-context learning could also benefit from the correlations with grokking.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We introduce a novel perspective that the development of context copying is a special grokking.
It holds the potential to enhance the efficiency of language model training research because it enables us to explore grokking using small synthetic datasets, derive meaningful insights, and apply these insights to language models, as we did in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09281v1#S4" title="4 Application ‣ Language Models “Grok” to Copy"><span class="ltx_text ltx_ref_tag">4</span></a>.
We hope a better understanding of grokking in future works provide more insights for developing language models with improved in-context capability.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2005.14165" title="">Language models are few-shot learners</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Yuhan Chen, Ang Lv, Ting-En Lin, Changyu Chen, Yuchuan Wu, Fei Huang, Yongbin Li, and Rui Yan. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.acl-long.601" title="">Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 11160–11174, Bangkok, Thailand. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Computer (2023)</span>
<span class="ltx_bibblock">
Together Computer. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://github.com/togethercomputer/RedPajama-Data" title="">Redpajama: An open source recipe to reproduce llama training dataset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elhage et al. (2021)</span>
<span class="ltx_bibblock">
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2021.

</span>
<span class="ltx_bibblock">A mathematical framework for transformer circuits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Transformer Circuits Thread</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems</em>, 33:9459–9474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Hongzhan Lin, Ang Lv, Yuhan Chen, Chen Zhu, Yang Song, Hengshu Zhu, and Rui Yan. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2406.19598" title="">Mixture of in-context experts enhance llms’ long context awareness</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.03172" title="">Lost in the middle: How language models use long contexts</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2019)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1711.05101" title="">Decoupled weight decay regularization</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.556" title="">Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 8086–8098, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al. (2024)</span>
<span class="ltx_bibblock">
Ang Lv, Yuhan Chen, Kaiyi Zhang, Yulong Wang, Lifeng Liu, Ji-Rong Wen, Jian Xie, and Rui Yan. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.19521" title="">Interpreting key mechanisms of factual recall in transformer-based language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nanda et al. (2023)</span>
<span class="ltx_bibblock">
Neel Nanda, Lawrence Chan, Tom Lieberum, Jess Smith, and Jacob Steinhardt. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2301.05217" title="">Progress measures for grokking via mechanistic interpretability</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olsson et al. (2022)</span>
<span class="ltx_bibblock">
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah. 2022.

</span>
<span class="ltx_bibblock">In-context learning and induction heads.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Transformer Circuits Thread</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Power et al. (2022)</span>
<span class="ltx_bibblock">
Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2201.02177" title="">Grokking: Generalization beyond overfitting on small algorithmic datasets</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.13971" title="">Llama: Open and efficient foundation language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" title="">Attention is all you need</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Advances in Neural Information Processing Systems</em>, volume 30. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Boshi Wang, Xiang Yue, Yu Su, and Huan Sun. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2405.15071" title="">Grokked transformers are implicit reasoners: A mechanistic journey to the edge of generalization</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Kaiyi Zhang, Ang Lv, Yuhan Chen, Hansen Ha, Tao Xu, and Rui Yan. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2024.findings-acl.638" title="">Batch-ICL: Effective, efficient, and order-agnostic in-context learning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Findings of the Association for Computational Linguistics ACL 2024</em>, pages 10728–10739, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 14 03:10:42 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
