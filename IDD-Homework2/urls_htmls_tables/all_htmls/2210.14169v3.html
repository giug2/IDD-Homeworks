<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2210.14169] Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding</title><meta property="og:description" content="Dialogue understanding tasks often necessitate abundant annotated data to achieve good performance and that presents challenges in low-resource settings.
To alleviate this barrier, we explore few-shot data augmentationâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2210.14169">

<!--Generated on Thu Mar 14 04:25:11 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maximillian Chen<sup id="id11.11.id1" class="ltx_sup">1</sup>Â ,
Alexandros Papangelis<sup id="id12.12.id2" class="ltx_sup"><span id="id12.12.id2.1" class="ltx_text ltx_font_italic">2</span></sup>,
Chenyang Tao<sup id="id13.13.id3" class="ltx_sup"><span id="id13.13.id3.1" class="ltx_text ltx_font_italic">2</span></sup>,
Andy Rosenbaum<sup id="id14.14.id4" class="ltx_sup">2</sup>, 
<br class="ltx_break"><span id="id5.5.1" class="ltx_text ltx_font_bold">Seokhwan Kim<sup id="id5.5.1.1" class="ltx_sup"><span id="id5.5.1.1.1" class="ltx_text ltx_font_medium">2</span></sup></span>,
<span id="id6.6.2" class="ltx_text ltx_font_bold">Yang Liu<sup id="id6.6.2.1" class="ltx_sup"><span id="id6.6.2.1.1" class="ltx_text ltx_font_medium">2</span></sup></span>,
<span id="id7.7.3" class="ltx_text ltx_font_bold">Zhou Yu<sup id="id7.7.3.1" class="ltx_sup"><span id="id7.7.3.1.1" class="ltx_text ltx_font_medium">1</span></sup></span>
<span id="id8.8.4" class="ltx_text ltx_font_bold">Dilek Hakkani-Tur<sup id="id8.8.4.1" class="ltx_sup"><span id="id8.8.4.1.1" class="ltx_text ltx_font_medium">2</span></sup></span>,
<br class="ltx_break"><sup id="id15.15.id5" class="ltx_sup">1</sup>Columbia University, <sup id="id16.16.id6" class="ltx_sup">2</sup>Amazon Alexa AI 
<br class="ltx_break"><span id="id17.17.id7" class="ltx_text ltx_font_typewriter">maxchen@cs.columbia.edu</span>, <span id="id18.18.id8" class="ltx_text ltx_font_typewriter">zy2461@columbia.edu</span> 
<br class="ltx_break"><span id="id19.19.id9" class="ltx_text ltx_font_typewriter">{papangea, chenyt, andros, seokhwk, yangliud, hakkanit}@amazon.com</span> 
<br class="ltx_break">
</span><span class="ltx_author_notes">Work done during internship at Amazon Alexa AI</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id20.id1" class="ltx_p">Dialogue understanding tasks often necessitate abundant annotated data to achieve good performance and that presents challenges in low-resource settings.
To alleviate this barrier, we explore few-shot data augmentation for dialogue understanding by prompting large pre-trained language models and present a novel approach that iterates on augmentation quality by applying weakly-supervised filters.
We evaluate our methods on the emotion and act classification tasks in <span id="id20.id1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>and the intent classification task in <span id="id20.id1.2" class="ltx_text ltx_font_smallcaps">Facebook Multilingual Task-Oriented Dialogue</span>. Models fine-tuned on our augmented data mixed with few-shot ground truth data are able to approach or surpass existing full-shot state-of-the-art performance on both datasets. For <span id="id20.id1.3" class="ltx_text ltx_font_smallcaps">DailyDialog </span>specifically, using 10% of the ground truth data we outperform the current state-of-the-art model which uses 100% of the data.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction &amp; Related Work</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Most common ways of automatic data augmentation in natural language tasks include simple perturbationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei and Zou, <a href="#bib.bib47" title="" class="ltx_ref">2019</a>; Karimi etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>; Xie etÂ al., <a href="#bib.bib51" title="" class="ltx_ref">2020</a>)</cite> and generative approachesÂ <cite class="ltx_cite ltx_citemacro_citep">(Kim etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>; Sahu etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>; Edunov etÂ al., <a href="#bib.bib11" title="" class="ltx_ref">2018</a>)</cite>. However, these methods do not utilize intersentential context, which is essential to encode
for both dialogue understanding and generation.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">On the other hand, modern pre-trained language models (PLMs) can be prompted to complete dialogues using prefix promptsÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite>, which naturally encode conversational context. PLMs also have shown impressive zero- and few-shot capabilitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Bommasani etÂ al., <a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite> in dialogue tasks and have been successfully used in generative augmentation frameworks for tasks such as intent classificationÂ <cite class="ltx_cite ltx_citemacro_citep">(Sahu etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>; Li etÂ al., <a href="#bib.bib25" title="" class="ltx_ref">2021</a>)</cite>, commonsense reasoningÂ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al., <a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite>, and response generationÂ <cite class="ltx_cite ltx_citemacro_citep">(KulhÃ¡nek etÂ al., <a href="#bib.bib20" title="" class="ltx_ref">2021</a>; Gao etÂ al., <a href="#bib.bib15" title="" class="ltx_ref">2020b</a>)</cite>. Several studies examine in-context learning, which involves including training examples as part of a promptÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei etÂ al., <a href="#bib.bib48" title="" class="ltx_ref">2022</a>; Min etÂ al., <a href="#bib.bib31" title="" class="ltx_ref">2022</a>; Chen etÂ al., <a href="#bib.bib7" title="" class="ltx_ref">2022</a>; Lu etÂ al., <a href="#bib.bib30" title="" class="ltx_ref">2022</a>)</cite>. In this work, we take the first step towards applying few-shot prompting to augmenting dialogue datasets. We focus on low-resource<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>both in terms of data and cost of computational resources.</span></span></span> settings, contributing an empirical account of augmenting turn-level dialogue understanding tasks using discrete prompting which encodes dialogue history as in-context examples.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2210.14169/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="150" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Example augmented conversation from <span id="S1.F1.3.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>with a generated turn following the desired emotion â€œhappy.â€ <span id="S1.F1.4.2" class="ltx_text ltx_font_smallcaps">WeakDAP </span>filters out generated turns which do not follow the label (red).</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One challenge with zero- and few-shot prompting with PLMs is that the outputs may
exhibit more diversity than one would expect for a specific task, which confounds model training
Â <cite class="ltx_cite ltx_citemacro_citep">(Perez etÂ al., <a href="#bib.bib36" title="" class="ltx_ref">2021</a>; Zhao etÂ al., <a href="#bib.bib55" title="" class="ltx_ref">2021</a>)</cite>.
Specifically, PLMs often synthesize data points which lie outside of the data manifold<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><cite class="ltx_cite ltx_citemacro_citet">Kim etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite> hypothesizes that synthetic data must lie along the same natural language manifold as the ground truth data, proposing linear interpolation among existing data.</span></span></span> of a given task, instead following the distribution of the generic pretraining corpora. Due to their distance from the target taskâ€™s distribution, these augmented samples may be considered low quality.
We thus propose <span id="S1.p3.1.1" class="ltx_text ltx_font_smallcaps">WeakDAP </span>(Weakly supervised Data Augmentation through Prompting), a framework that iteratively improves the quality of augmented data in dialogue classification tasks by introducing a weakly supervised labeler to filter prospective data points. FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction &amp; Related Work â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates <span id="S1.p3.1.2" class="ltx_text ltx_font_smallcaps">WeakDAP </span>filtering out a low-quality synthetic utterance. We demonstrate the effectiveness of <span id="S1.p3.1.3" class="ltx_text ltx_font_smallcaps">WeakDAP </span>on emotion and dialogue act classification in <span id="S1.p3.1.4" class="ltx_text ltx_font_smallcaps">DailyDialog </span>Â <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a href="#bib.bib24" title="" class="ltx_ref">2017</a>)</cite>, showing on-par or better performance compared to state-of-the-art full-shot results by augmenting only 10% of the original data. We additionally examine the robustness of <span id="S1.p3.1.5" class="ltx_text ltx_font_smallcaps">WeakDAP </span>using a separate task: cross-lingual augmentation for Spanish intent detection in <span id="S1.p3.1.6" class="ltx_text ltx_font_smallcaps">FBTOD </span>Â <cite class="ltx_cite ltx_citemacro_citep">(Schuster etÂ al., <a href="#bib.bib40" title="" class="ltx_ref">2019</a>)</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Data Augmentation Methods</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our approach consists of two parts: prompting PLMs using dialogue context, and applying weak supervision to refine prompt-augmented datasets.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Constructing Dialogue Prompts</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">Dialogue contexts can be used to form prefix prompts which serve as the input to a PLM<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>While augmentation by prompting PLMs can help expand linguistic diversity, it can also introduce biases which exist in PLMsâ€™ pre-training corpora. Additionally, it may underline biases in the existing low-resource data being augmented. We discuss this further in AppendixÂ <a href="#A1" title="Appendix A Ethical Considerations â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span>. We augment the data by replacing dialogue turns, which are selected using the dialogue context construction strategies below. We illustrate specific examples of each in Figure <a href="#S2.F2" title="Figure 2 â€£ 2.2 Augmentation with Weak Supervision â€£ 2 Data Augmentation Methods â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and Section <a href="#A5" title="Appendix E Example Prompts â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> in the Appendix. Each generated utterance can be prescribed a randomly sampled or ground truth reference label. 
<br class="ltx_break"></p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.4" class="ltx_p"><span id="S2.SS1.p2.4.1" class="ltx_text ltx_font_bold">Conversation Trajectory Augmentation (</span><span id="S2.SS1.p2.4.2" class="ltx_text ltx_font_typewriter">CTA</span><span id="S2.SS1.p2.4.3" class="ltx_text ltx_font_bold">).</span> We take each speakerâ€™s first turn as ground-truth context and iteratively replace the next turn with a generated utterance. We autoregressively use each generated utterance as context to generate the next turn. Each ground truth conversation results in one synthetic conversation with a new â€œtrajectory.â€ 
<br class="ltx_break"><span id="S2.SS1.p2.4.4" class="ltx_text ltx_font_bold">All-Turn Augmentation (</span><span id="S2.SS1.p2.4.5" class="ltx_text ltx_font_typewriter">ATA</span><span id="S2.SS1.p2.4.6" class="ltx_text ltx_font_bold">).</span> <span id="S2.SS1.p2.4.7" class="ltx_text ltx_font_typewriter">ATA</span>Â iteratively replaces each turn in the conversation with a generated utterance, but uses the ground truth context instead of the generated context. For a conversation with <math id="S2.SS1.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p2.1.m1.1a"><mi id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><ci id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">n</annotation></semantics></math> turns, this results in <math id="S2.SS1.p2.2.m2.1" class="ltx_Math" alttext="n-1" display="inline"><semantics id="S2.SS1.p2.2.m2.1a"><mrow id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml"><mi id="S2.SS1.p2.2.m2.1.1.2" xref="S2.SS1.p2.2.m2.1.1.2.cmml">n</mi><mo id="S2.SS1.p2.2.m2.1.1.1" xref="S2.SS1.p2.2.m2.1.1.1.cmml">âˆ’</mo><mn id="S2.SS1.p2.2.m2.1.1.3" xref="S2.SS1.p2.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><apply id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"><minus id="S2.SS1.p2.2.m2.1.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1.1"></minus><ci id="S2.SS1.p2.2.m2.1.1.2.cmml" xref="S2.SS1.p2.2.m2.1.1.2">ğ‘›</ci><cn type="integer" id="S2.SS1.p2.2.m2.1.1.3.cmml" xref="S2.SS1.p2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">n-1</annotation></semantics></math> â€œnewâ€ conversations of length <math id="S2.SS1.p2.3.m3.1" class="ltx_Math" alttext="2" display="inline"><semantics id="S2.SS1.p2.3.m3.1a"><mn id="S2.SS1.p2.3.m3.1.1" xref="S2.SS1.p2.3.m3.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.3.m3.1b"><cn type="integer" id="S2.SS1.p2.3.m3.1.1.cmml" xref="S2.SS1.p2.3.m3.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.3.m3.1c">2</annotation></semantics></math> through <math id="S2.SS1.p2.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p2.4.m4.1a"><mi id="S2.SS1.p2.4.m4.1.1" xref="S2.SS1.p2.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.4.m4.1b"><ci id="S2.SS1.p2.4.m4.1.1.cmml" xref="S2.SS1.p2.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.4.m4.1c">n</annotation></semantics></math>.
<br class="ltx_break"><span id="S2.SS1.p2.4.8" class="ltx_text ltx_font_bold">Last-Turn Augmentation (</span><span id="S2.SS1.p2.4.9" class="ltx_text ltx_font_typewriter">LTA</span><span id="S2.SS1.p2.4.10" class="ltx_text ltx_font_bold">).</span> This is a special case of <span id="S2.SS1.p2.4.11" class="ltx_text ltx_font_typewriter">ATA</span>Â where we simply choose the last turn of the conversation to replace with a generated utterance. This results in the largest conversational context, helping guide the conditional output closer to the ground truth language manifold. Relative to a ground-truth conversation, this yields one new conversation, with an alternate last turn. Example in FigureÂ <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction &amp; Related Work â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Augmentation with Weak Supervision</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">While prompting large PLMs provides a convenient, powerful way to bridge the gap between inadequate training data and data-hungry conversational models, there is a caveat: those PLMs are trained on generic corpora (<span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">i.e.</span>, web crawls, books, <span id="S2.SS2.p1.1.2" class="ltx_text ltx_font_italic">etc</span>.), whose distribution may considerably differ from the data needed to train task-specific models (<span id="S2.SS2.p1.1.3" class="ltx_text ltx_font_italic">e.g.</span>, see Figure <a href="#S3.F4" title="Figure 4 â€£ 3.1 DailyDialog Emotion Classification â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>).
This motivates <span id="S2.SS2.p1.1.4" class="ltx_text ltx_font_italic">post-hoc</span> adjustments to make our prompted augmentations more task-aware.
Weak supervision has been proposed for finding a â€œuseful representationâ€ for a taskÂ <cite class="ltx_cite ltx_citemacro_citep">(Robinson etÂ al., <a href="#bib.bib37" title="" class="ltx_ref">2020</a>)</cite>.
Intuitively, naive prompted augmentations are less potent because they lack task-knowledge<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>PLMs only see prompts during generation; to fully account for task knowledge one should include all available examples in-context, which is generally impractical.</span></span></span>, which can be distilled from ground-truth (â€œgoldâ€) samples by training an auxiliary model. We can then use that model to filter out inconsistent generated utterances.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2210.14169/assets/x2.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="192" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Example conversation augmentation prompt for emotion classification using GPT-J, prescribing the original ground-truth emotion (left) and a randomly sampled emotion (right). This is augmented using Last Turn Augmentation, i.e., the first five turns are taken from the ground-truth data and the model is asked to generate the sixth and final turn. Both boxes represent a new augmented conversation when taken in aggregate.</figcaption>
</figure>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.2" class="ltx_p">We propose <span id="S2.SS2.p2.2.1" class="ltx_text ltx_font_smallcaps">WeakDAP</span>, a framework generalizeable to any prompt-based augmentation task. In this work, we prompt GPT-J 6BÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki, <a href="#bib.bib44" title="" class="ltx_ref">2021</a>)</cite> and the Alexa Teacher Model (ATM) 20BÂ <cite class="ltx_cite ltx_citemacro_citep">(Soltan etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2022</a>)</cite>. As FigureÂ <a href="#S2.F3" title="Figure 3 â€£ 2.2 Augmentation with Weak Supervision â€£ 2 Data Augmentation Methods â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> illustrates, <span id="S2.SS2.p2.2.2" class="ltx_text ltx_font_smallcaps">WeakDAP </span>consists of three parts. We first augment the â€œgoldâ€ data and train a task classifier on the gold and â€œsilverâ€ data. Then, we iteratively re-augment the data and re-train the classifier. For the augmentation step on each iteration, we use the classifier trained during the previous iteration to create a weak silver label for each generated instance, and filter out instances where the silver label does not match the prescribed label with high confidence, i.e., low entropy. We reason that data points which a weak labeler thinks are labeled incorrectly with low confidence could still be useful for learning during training (further discussion in Section <a href="#A7" title="Appendix G Entropy-based Weak Filtering â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">G</span></a> in the Appendix). Moreover, this indicates that their labels may be in fact be correct. To this end, we filter out incorrect instances classified in the bottom 80th percentile of entropy, computed as in the equation below, where C is the number of classes and <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="p_{i}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><msub id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.1.1.2" xref="S2.SS2.p2.1.m1.1.1.2.cmml">p</mi><mi id="S2.SS2.p2.1.m1.1.1.3" xref="S2.SS2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.p2.1.m1.1.1.2.cmml" xref="S2.SS2.p2.1.m1.1.1.2">ğ‘</ci><ci id="S2.SS2.p2.1.m1.1.1.3.cmml" xref="S2.SS2.p2.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">p_{i}</annotation></semantics></math> is the probability of class <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">i</annotation></semantics></math>.<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>This threshold is tunable.</span></span></span></p>
<table id="S2.Ex1" class="ltx_equation ltx_centering ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_centering ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table id="S2.Ex2" class="ltx_equation ltx_centering ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_centering ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex2.m1.1" class="ltx_Math" alttext="Entropy=\sum_{i}^{C}-p_{i}*log_{2}(p_{i})" display="block"><semantics id="S2.Ex2.m1.1a"><mrow id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml"><mrow id="S2.Ex2.m1.1.1.3" xref="S2.Ex2.m1.1.1.3.cmml"><mi id="S2.Ex2.m1.1.1.3.2" xref="S2.Ex2.m1.1.1.3.2.cmml">E</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.3.1" xref="S2.Ex2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.Ex2.m1.1.1.3.3" xref="S2.Ex2.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.3.1a" xref="S2.Ex2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.Ex2.m1.1.1.3.4" xref="S2.Ex2.m1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.3.1b" xref="S2.Ex2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.Ex2.m1.1.1.3.5" xref="S2.Ex2.m1.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.3.1c" xref="S2.Ex2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.Ex2.m1.1.1.3.6" xref="S2.Ex2.m1.1.1.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.3.1d" xref="S2.Ex2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.Ex2.m1.1.1.3.7" xref="S2.Ex2.m1.1.1.3.7.cmml">p</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.3.1e" xref="S2.Ex2.m1.1.1.3.1.cmml">â€‹</mo><mi id="S2.Ex2.m1.1.1.3.8" xref="S2.Ex2.m1.1.1.3.8.cmml">y</mi></mrow><mo rspace="0.111em" id="S2.Ex2.m1.1.1.2" xref="S2.Ex2.m1.1.1.2.cmml">=</mo><mrow id="S2.Ex2.m1.1.1.1" xref="S2.Ex2.m1.1.1.1.cmml"><munderover id="S2.Ex2.m1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.3.cmml"><mo movablelimits="false" rspace="0em" id="S2.Ex2.m1.1.1.1.3.2.2" xref="S2.Ex2.m1.1.1.1.3.2.2.cmml">âˆ‘</mo><mi id="S2.Ex2.m1.1.1.1.3.2.3" xref="S2.Ex2.m1.1.1.1.3.2.3.cmml">i</mi><mi id="S2.Ex2.m1.1.1.1.3.3" xref="S2.Ex2.m1.1.1.1.3.3.cmml">C</mi></munderover><mo lspace="0em" id="S2.Ex2.m1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.2.cmml">âˆ’</mo><mrow id="S2.Ex2.m1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.cmml"><mrow id="S2.Ex2.m1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.3.cmml"><msub id="S2.Ex2.m1.1.1.1.1.3.2" xref="S2.Ex2.m1.1.1.1.1.3.2.cmml"><mi id="S2.Ex2.m1.1.1.1.1.3.2.2" xref="S2.Ex2.m1.1.1.1.1.3.2.2.cmml">p</mi><mi id="S2.Ex2.m1.1.1.1.1.3.2.3" xref="S2.Ex2.m1.1.1.1.1.3.2.3.cmml">i</mi></msub><mo lspace="0.222em" rspace="0.222em" id="S2.Ex2.m1.1.1.1.1.3.1" xref="S2.Ex2.m1.1.1.1.1.3.1.cmml">âˆ—</mo><mi id="S2.Ex2.m1.1.1.1.1.3.3" xref="S2.Ex2.m1.1.1.1.1.3.3.cmml">l</mi></mrow><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.2.cmml">â€‹</mo><mi id="S2.Ex2.m1.1.1.1.1.4" xref="S2.Ex2.m1.1.1.1.1.4.cmml">o</mi><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.1.1.2a" xref="S2.Ex2.m1.1.1.1.1.2.cmml">â€‹</mo><msub id="S2.Ex2.m1.1.1.1.1.5" xref="S2.Ex2.m1.1.1.1.1.5.cmml"><mi id="S2.Ex2.m1.1.1.1.1.5.2" xref="S2.Ex2.m1.1.1.1.1.5.2.cmml">g</mi><mn id="S2.Ex2.m1.1.1.1.1.5.3" xref="S2.Ex2.m1.1.1.1.1.5.3.cmml">2</mn></msub><mo lspace="0em" rspace="0em" id="S2.Ex2.m1.1.1.1.1.2b" xref="S2.Ex2.m1.1.1.1.1.2.cmml">â€‹</mo><mrow id="S2.Ex2.m1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex2.m1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.Ex2.m1.1.1.1.1.1.1.1" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.2" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2.cmml">p</mi><mi id="S2.Ex2.m1.1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo stretchy="false" id="S2.Ex2.m1.1.1.1.1.1.1.3" xref="S2.Ex2.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.1b"><apply id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1"><eq id="S2.Ex2.m1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.2"></eq><apply id="S2.Ex2.m1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.3"><times id="S2.Ex2.m1.1.1.3.1.cmml" xref="S2.Ex2.m1.1.1.3.1"></times><ci id="S2.Ex2.m1.1.1.3.2.cmml" xref="S2.Ex2.m1.1.1.3.2">ğ¸</ci><ci id="S2.Ex2.m1.1.1.3.3.cmml" xref="S2.Ex2.m1.1.1.3.3">ğ‘›</ci><ci id="S2.Ex2.m1.1.1.3.4.cmml" xref="S2.Ex2.m1.1.1.3.4">ğ‘¡</ci><ci id="S2.Ex2.m1.1.1.3.5.cmml" xref="S2.Ex2.m1.1.1.3.5">ğ‘Ÿ</ci><ci id="S2.Ex2.m1.1.1.3.6.cmml" xref="S2.Ex2.m1.1.1.3.6">ğ‘œ</ci><ci id="S2.Ex2.m1.1.1.3.7.cmml" xref="S2.Ex2.m1.1.1.3.7">ğ‘</ci><ci id="S2.Ex2.m1.1.1.3.8.cmml" xref="S2.Ex2.m1.1.1.3.8">ğ‘¦</ci></apply><apply id="S2.Ex2.m1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1"><minus id="S2.Ex2.m1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.2"></minus><apply id="S2.Ex2.m1.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.3.1.cmml" xref="S2.Ex2.m1.1.1.1.3">superscript</csymbol><apply id="S2.Ex2.m1.1.1.1.3.2.cmml" xref="S2.Ex2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.3.2.1.cmml" xref="S2.Ex2.m1.1.1.1.3">subscript</csymbol><sum id="S2.Ex2.m1.1.1.1.3.2.2.cmml" xref="S2.Ex2.m1.1.1.1.3.2.2"></sum><ci id="S2.Ex2.m1.1.1.1.3.2.3.cmml" xref="S2.Ex2.m1.1.1.1.3.2.3">ğ‘–</ci></apply><ci id="S2.Ex2.m1.1.1.1.3.3.cmml" xref="S2.Ex2.m1.1.1.1.3.3">ğ¶</ci></apply><apply id="S2.Ex2.m1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1"><times id="S2.Ex2.m1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.2"></times><apply id="S2.Ex2.m1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3"><times id="S2.Ex2.m1.1.1.1.1.3.1.cmml" xref="S2.Ex2.m1.1.1.1.1.3.1"></times><apply id="S2.Ex2.m1.1.1.1.1.3.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.3.2.1.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.3.2.2.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.2">ğ‘</ci><ci id="S2.Ex2.m1.1.1.1.1.3.2.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3.2.3">ğ‘–</ci></apply><ci id="S2.Ex2.m1.1.1.1.1.3.3.cmml" xref="S2.Ex2.m1.1.1.1.1.3.3">ğ‘™</ci></apply><ci id="S2.Ex2.m1.1.1.1.1.4.cmml" xref="S2.Ex2.m1.1.1.1.1.4">ğ‘œ</ci><apply id="S2.Ex2.m1.1.1.1.1.5.cmml" xref="S2.Ex2.m1.1.1.1.1.5"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.5.1.cmml" xref="S2.Ex2.m1.1.1.1.1.5">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.5.2.cmml" xref="S2.Ex2.m1.1.1.1.1.5.2">ğ‘”</ci><cn type="integer" id="S2.Ex2.m1.1.1.1.1.5.3.cmml" xref="S2.Ex2.m1.1.1.1.1.5.3">2</cn></apply><apply id="S2.Ex2.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex2.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.2">ğ‘</ci><ci id="S2.Ex2.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex2.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.1c">Entropy=\sum_{i}^{C}-p_{i}*log_{2}(p_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<table id="S2.E1" class="ltx_equation ltx_centering ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_centering ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="0" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.3" class="ltx_p">This weakly guarantees that the generated data is not of low-quality. This continues until the classifierâ€™s performance doesnâ€™t improve by at least <math id="S2.SS2.p3.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS2.p3.1.m1.1a"><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">Ïµ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">italic-Ïµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">\epsilon</annotation></semantics></math> for <math id="S2.SS2.p3.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS2.p3.2.m2.1a"><mi id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">k</annotation></semantics></math> rounds. Here, we fix <math id="S2.SS2.p3.3.m3.2" class="ltx_Math" alttext="\epsilon=0.005,k=3" display="inline"><semantics id="S2.SS2.p3.3.m3.2a"><mrow id="S2.SS2.p3.3.m3.2.2.2" xref="S2.SS2.p3.3.m3.2.2.3.cmml"><mrow id="S2.SS2.p3.3.m3.1.1.1.1" xref="S2.SS2.p3.3.m3.1.1.1.1.cmml"><mi id="S2.SS2.p3.3.m3.1.1.1.1.2" xref="S2.SS2.p3.3.m3.1.1.1.1.2.cmml">Ïµ</mi><mo id="S2.SS2.p3.3.m3.1.1.1.1.1" xref="S2.SS2.p3.3.m3.1.1.1.1.1.cmml">=</mo><mn id="S2.SS2.p3.3.m3.1.1.1.1.3" xref="S2.SS2.p3.3.m3.1.1.1.1.3.cmml">0.005</mn></mrow><mo id="S2.SS2.p3.3.m3.2.2.2.3" xref="S2.SS2.p3.3.m3.2.2.3a.cmml">,</mo><mrow id="S2.SS2.p3.3.m3.2.2.2.2" xref="S2.SS2.p3.3.m3.2.2.2.2.cmml"><mi id="S2.SS2.p3.3.m3.2.2.2.2.2" xref="S2.SS2.p3.3.m3.2.2.2.2.2.cmml">k</mi><mo id="S2.SS2.p3.3.m3.2.2.2.2.1" xref="S2.SS2.p3.3.m3.2.2.2.2.1.cmml">=</mo><mn id="S2.SS2.p3.3.m3.2.2.2.2.3" xref="S2.SS2.p3.3.m3.2.2.2.2.3.cmml">3</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.3.m3.2b"><apply id="S2.SS2.p3.3.m3.2.2.3.cmml" xref="S2.SS2.p3.3.m3.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p3.3.m3.2.2.3a.cmml" xref="S2.SS2.p3.3.m3.2.2.2.3">formulae-sequence</csymbol><apply id="S2.SS2.p3.3.m3.1.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1.1.1"><eq id="S2.SS2.p3.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.p3.3.m3.1.1.1.1.1"></eq><ci id="S2.SS2.p3.3.m3.1.1.1.1.2.cmml" xref="S2.SS2.p3.3.m3.1.1.1.1.2">italic-Ïµ</ci><cn type="float" id="S2.SS2.p3.3.m3.1.1.1.1.3.cmml" xref="S2.SS2.p3.3.m3.1.1.1.1.3">0.005</cn></apply><apply id="S2.SS2.p3.3.m3.2.2.2.2.cmml" xref="S2.SS2.p3.3.m3.2.2.2.2"><eq id="S2.SS2.p3.3.m3.2.2.2.2.1.cmml" xref="S2.SS2.p3.3.m3.2.2.2.2.1"></eq><ci id="S2.SS2.p3.3.m3.2.2.2.2.2.cmml" xref="S2.SS2.p3.3.m3.2.2.2.2.2">ğ‘˜</ci><cn type="integer" id="S2.SS2.p3.3.m3.2.2.2.2.3.cmml" xref="S2.SS2.p3.3.m3.2.2.2.2.3">3</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.3.m3.2c">\epsilon=0.005,k=3</annotation></semantics></math>.</p>
</div>
<figure id="S2.F3" class="ltx_figure">
<div id="S2.F3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:261.3pt;height:201.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-87.1pt,67.2pt) scale(0.6,0.6) ;"><img src="/html/2210.14169/assets/x3.png" id="S2.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="358" alt="Refer to caption">
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
The workflow of <span id="S2.F3.3.1" class="ltx_text ltx_font_smallcaps">WeakDAP</span>.
On each iteration, the Gold Data is augmented by replacing conversation turns generated by providing a PLM with prefix prompts. Each prospective silver training instance is weakly classified as either following its intended label or not, using a task specific classifier. The gold and silver data are used as training data for the next generationâ€™s classifier. This process repeats until the performance of the classifier does not improve past a threshold.</figcaption>
</figure>
<section id="S2.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Other Task-Aware Augmentation Approaches.</h4>

<div id="S2.SS2.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS2.SSS0.Px1.p1.1" class="ltx_p">Similar task-aware generative augmentation approaches typically distill task-knowledge into the generator.
<cite class="ltx_cite ltx_citemacro_citet">Yang etÂ al. (<a href="#bib.bib53" title="" class="ltx_ref">2020</a>)</cite> proposes augmentation for commonsense reasoning by fine-tuning two generators (for answering and distracting) and relabelling synthetic data points using a task model, while <cite class="ltx_cite ltx_citemacro_citet">Papangelis etÂ al. (<a href="#bib.bib33" title="" class="ltx_ref">2021</a>)</cite> fine-tunes a generator using reinforcement learning.
With large PLMs, these methods are costly and less practical.
While few-shot prompting is a cheaper solution, it is less effective at encoding lots of task knowledge, as in-context example capacity is limited.
<span id="S2.SS2.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">WeakDAP </span>bridges the gap between prompt-based augmentation with little task-knowledge and complex mechanisms with higher computational costs; it does not need to fine-tune the generator, as we prompt it using dialogue context as in-context utterance examples.</p>
</div>
</section>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We benchmark various augmentation methods on the classification tasks in <span id="S3.p1.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog</span>, a high-quality open-domain dialogue dataset, and the intent detection task of <span id="S3.p1.1.2" class="ltx_text ltx_font_smallcaps">FBTOD</span>, a task-oriented dialogue dataset (dataset details in FigureÂ <a href="#A3" title="Appendix C Datasets â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">C</span></a>).</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span><span id="S3.SS1.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>Emotion Classification</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We first conduct a thorough evaluation of our augmentation methods using the emotion classification task in <span id="S3.SS1.p1.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>as a case study, in the full and few-shot settings<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>We randomly sample <math id="footnote6.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="footnote6.m1.1b"><mrow id="footnote6.m1.1.1" xref="footnote6.m1.1.1.cmml"><mn id="footnote6.m1.1.1.2" xref="footnote6.m1.1.1.2.cmml">1</mn><mo id="footnote6.m1.1.1.1" xref="footnote6.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote6.m1.1c"><apply id="footnote6.m1.1.1.cmml" xref="footnote6.m1.1.1"><csymbol cd="latexml" id="footnote6.m1.1.1.1.cmml" xref="footnote6.m1.1.1.1">percent</csymbol><cn type="integer" id="footnote6.m1.1.1.2.cmml" xref="footnote6.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m1.1d">1\%</annotation></semantics></math>, <math id="footnote6.m2.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="footnote6.m2.1b"><mrow id="footnote6.m2.1.1" xref="footnote6.m2.1.1.cmml"><mn id="footnote6.m2.1.1.2" xref="footnote6.m2.1.1.2.cmml">5</mn><mo id="footnote6.m2.1.1.1" xref="footnote6.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote6.m2.1c"><apply id="footnote6.m2.1.1.cmml" xref="footnote6.m2.1.1"><csymbol cd="latexml" id="footnote6.m2.1.1.1.cmml" xref="footnote6.m2.1.1.1">percent</csymbol><cn type="integer" id="footnote6.m2.1.1.2.cmml" xref="footnote6.m2.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m2.1d">5\%</annotation></semantics></math>, and <math id="footnote6.m3.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="footnote6.m3.1b"><mrow id="footnote6.m3.1.1" xref="footnote6.m3.1.1.cmml"><mn id="footnote6.m3.1.1.2" xref="footnote6.m3.1.1.2.cmml">10</mn><mo id="footnote6.m3.1.1.1" xref="footnote6.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote6.m3.1c"><apply id="footnote6.m3.1.1.cmml" xref="footnote6.m3.1.1"><csymbol cd="latexml" id="footnote6.m3.1.1.1.cmml" xref="footnote6.m3.1.1.1">percent</csymbol><cn type="integer" id="footnote6.m3.1.1.2.cmml" xref="footnote6.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote6.m3.1d">10\%</annotation></semantics></math> of the data.</span></span></span>. For our augmentation model, we use GPT-J 6B<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>We examined OPT-30BÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a href="#bib.bib54" title="" class="ltx_ref">2022</a>)</cite>, but it was far slower without large performance improvements.</span></span></span>Â <cite class="ltx_cite ltx_citemacro_citep">(Wang and Komatsuzaki, <a href="#bib.bib44" title="" class="ltx_ref">2021</a>)</cite>, which is one of the largest causal language models publicly available, and has been able to achieve performance competitive to GPT-3 on many tasksÂ <cite class="ltx_cite ltx_citemacro_citep">(Wang, <a href="#bib.bib43" title="" class="ltx_ref">2021</a>; Black etÂ al., <a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>. For all <span id="S3.SS1.p1.1.2" class="ltx_text ltx_font_smallcaps">DailyDialog </span>experiments we use the Speaker Turn Model (STM) Â <cite class="ltx_cite ltx_citemacro_citep">(He etÂ al., <a href="#bib.bib17" title="" class="ltx_ref">2021</a>)</cite>, a RoBERTaÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>-based classification model with speaker turn awareness<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>STM achieves state-of-the-art performance on full-shot <span id="footnote8.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>act classification (<math id="footnote8.m1.1" class="ltx_Math" alttext="87.5\%" display="inline"><semantics id="footnote8.m1.1b"><mrow id="footnote8.m1.1.1" xref="footnote8.m1.1.1.cmml"><mn id="footnote8.m1.1.1.2" xref="footnote8.m1.1.1.2.cmml">87.5</mn><mo id="footnote8.m1.1.1.1" xref="footnote8.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="footnote8.m1.1c"><apply id="footnote8.m1.1.1.cmml" xref="footnote8.m1.1.1"><csymbol cd="latexml" id="footnote8.m1.1.1.1.cmml" xref="footnote8.m1.1.1.1">percent</csymbol><cn type="float" id="footnote8.m1.1.1.2.cmml" xref="footnote8.m1.1.1.2">87.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote8.m1.1d">87.5\%</annotation></semantics></math> accuracy).</span></span></span>, as the classification task model and weak labeler.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.2" class="ltx_p">There are seven emotion labels: <span id="S3.SS1.p2.2.1" class="ltx_text ltx_font_italic">neutral, anger, disgust, fear, happiness, sadness</span>, and <span id="S3.SS1.p2.2.2" class="ltx_text ltx_font_italic">surprise</span>. Each label is a rich, descriptive token on its own, so in constructing a prompt, we directly use it as an adjective (e.g., â€œAlice in a <span id="S3.SS1.p2.2.3" class="ltx_text ltx_font_italic">happy</span> mood:â€). Additionally,
we conjecture that directly using conversation history forms the best set of in-context examples to generate utterances which convey a prescribed emotion while remaining within the gold data manifold.
Example prompts provided in Figure <a href="#S2.F2" title="Figure 2 â€£ 2.2 Augmentation with Weak Supervision â€£ 2 Data Augmentation Methods â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> (<span id="S3.SS1.p2.2.4" class="ltx_text ltx_font_typewriter">LTA</span>) and Section <a href="#A5" title="Appendix E Example Prompts â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> in the Appendix (<span id="S3.SS1.p2.2.5" class="ltx_text ltx_font_typewriter">ATA</span>, <span id="S3.SS1.p2.2.6" class="ltx_text ltx_font_typewriter">CTA</span>). In FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.1 DailyDialog Emotion Classification â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we compare the data manifold of the synthetic data resulting from <span id="S3.SS1.p2.2.7" class="ltx_text ltx_font_typewriter">LTA</span>Â and <span id="S3.SS1.p2.2.8" class="ltx_text ltx_font_typewriter">CTA</span>Â with a maximum augmentation size of 2x that of the amount of original data used, and <span id="S3.SS1.p2.2.9" class="ltx_text ltx_font_typewriter">ATA</span>Â which results in a size between <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="5.7" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><mn id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml">5.7</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><cn type="float" id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">5.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">5.7</annotation></semantics></math>x and <math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="6.2" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mn id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">6.2</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><cn type="float" id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">6.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">6.2</annotation></semantics></math>x (see Appendix TableÂ <a href="#A4.T3" title="Table 3 â€£ D.2 All-Turn Augmentation â€£ Appendix D DailyDialog Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>). We can see that <span id="S3.SS1.p2.2.10" class="ltx_text ltx_font_typewriter">CTA</span>Â results in a separate cluster of data, likely due to the underlying distribution of GPT-Jâ€™s pretraining data. <span id="S3.SS1.p2.2.11" class="ltx_text ltx_font_typewriter">ATA</span>â€™s distribution has a clear distinct cluster as with <span id="S3.SS1.p2.2.12" class="ltx_text ltx_font_typewriter">CTA</span>, but also posseses some overlap with the training distribution. In contrast, we see that synthetic data from <span id="S3.SS1.p2.2.13" class="ltx_text ltx_font_typewriter">LTA</span>Â lies within the training data manifold. This is likely due to the in-distribution context provided as conditional input to GPT-J. We hypothesize that this context most closely guides synthetic data towards the original data distribution. This may be more beneficial in tasks where we do not expect distribution shift from training to inference. In tasks such as response generation, where diverse output is desirable, it may be more beneficial to have training data that falls into an expanded manifold as with <span id="S3.SS1.p2.2.14" class="ltx_text ltx_font_typewriter">CTA</span>Â or <span id="S3.SS1.p2.2.15" class="ltx_text ltx_font_typewriter">ATA</span>. Comparing the three methods, we find that <span id="S3.SS1.p2.2.16" class="ltx_text ltx_font_typewriter">CTA</span>Â and <span id="S3.SS1.p2.2.17" class="ltx_text ltx_font_typewriter">ATA</span>Â (Appendix Figure <a href="#A0.F7" title="Figure 7 â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, TableÂ <a href="#A4.T3" title="Table 3 â€£ D.2 All-Turn Augmentation â€£ Appendix D DailyDialog Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>) are beneficial, but underperform <span id="S3.SS1.p2.2.18" class="ltx_text ltx_font_typewriter">LTA</span>Â (Figure <a href="#S3.F6" title="Figure 6 â€£ 3.1 DailyDialog Emotion Classification â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>) for this task. Thus, we primarily experiment using <span id="S3.SS1.p2.2.19" class="ltx_text ltx_font_typewriter">LTA</span>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2210.14169/assets/Figs/3d_tsne.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="299" height="288" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>t-SNE projection of a random sample (for clarity) of training and augmented data.</figcaption>
</figure>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.3" class="ltx_p">In low-resource settings, it is important to quantify how much ground truth data is available. When augmenting seed data, it is essential to quantify how much data is being generated (i.e., by providing a Size Multiplier relative to the amount of seed data used; <cite class="ltx_cite ltx_citemacro_citet">Feng etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>). Figure <a href="#S3.F6" title="Figure 6 â€£ 3.1 DailyDialog Emotion Classification â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows the performance of STM in each data setting with varying amounts of synthetic data resulting from <span id="S3.SS1.p3.3.1" class="ltx_text ltx_font_typewriter">LTA</span>. Following existing work, we report micro F1 ignoring the majority label, due to a heavy imbalance towards the neutral emotionÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>; Lee and Choi, <a href="#bib.bib23" title="" class="ltx_ref">2021</a>; Wang etÂ al., <a href="#bib.bib46" title="" class="ltx_ref">2020</a>)</cite>. We see that STM with augmented 10% data reaches an F1 score of <math id="S3.SS1.p3.1.m1.1" class="ltx_Math" alttext="0.686" display="inline"><semantics id="S3.SS1.p3.1.m1.1a"><mn id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml">0.686</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><cn type="float" id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">0.686</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">0.686</annotation></semantics></math> and <math id="S3.SS1.p3.2.m2.1" class="ltx_Math" alttext="0.70" display="inline"><semantics id="S3.SS1.p3.2.m2.1a"><mn id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml">0.70</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><cn type="float" id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">0.70</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">0.70</annotation></semantics></math> with augmented full-shot data, surpassing the existing state-of-the-art of <math id="S3.SS1.p3.3.m3.1" class="ltx_Math" alttext="0.641" display="inline"><semantics id="S3.SS1.p3.3.m3.1a"><mn id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml">0.641</mn><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><cn type="float" id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">0.641</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">0.641</annotation></semantics></math> set by S+PAGEÂ <cite class="ltx_cite ltx_citemacro_citep">(Liang etÂ al., <a href="#bib.bib26" title="" class="ltx_ref">2021</a>)</cite>.
We observe that adding too much augmented data eventually hurts performance in each setting, further suggesting that prompting alone has inconsistent quality. 2x is the best performing multiplier and we use that to conduct all other experiments.
Figure <a href="#S3.F6" title="Figure 6 â€£ 3.1 DailyDialog Emotion Classification â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> also shows that <span id="S3.SS1.p3.3.2" class="ltx_text ltx_font_smallcaps">WeakDAP </span>improved STMâ€™s performance in each few-shot setting. On average, the labeler reduced the augmented data size from 2x to 1.8x, indicating that it may foster more efficient learning.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Prior work has not found large differences in performance between backtranslation and other perturbation methods despite higher computational overheadÂ <cite class="ltx_cite ltx_citemacro_citep">(Xie etÂ al., <a href="#bib.bib51" title="" class="ltx_ref">2020</a>; Kim etÂ al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>. Thus, we primarily compare against EDAÂ <cite class="ltx_cite ltx_citemacro_citep">(Wei and Zou, <a href="#bib.bib47" title="" class="ltx_ref">2019</a>)</cite><span id="footnote9" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>EDA includes synonym replacement, random insertion, random swap, and random deletion.</span></span></span> and AEDAÂ <cite class="ltx_cite ltx_citemacro_citep">(Karimi etÂ al., <a href="#bib.bib18" title="" class="ltx_ref">2021</a>)</cite><span id="footnote10" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>AEDA randomly inserts punctuation marks into text.</span></span></span> as noise injection baselines. As seen in Figure <a href="#S3.F6" title="Figure 6 â€£ 3.1 DailyDialog Emotion Classification â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, using noise perturbed data underperforms normal data, an outcome corroborated by <cite class="ltx_cite ltx_citemacro_citet">Kumar etÂ al. (<a href="#bib.bib21" title="" class="ltx_ref">2020</a>); Chen etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>)</cite>. Finally, we examined the importance of context by experimenting with a standard in-context learning prompt with 10 randomly sampled utterances of the same emotion labelÂ <cite class="ltx_cite ltx_citemacro_citep">(Sahu etÂ al., <a href="#bib.bib38" title="" class="ltx_ref">2022</a>; Brown etÂ al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>)</cite>, finding that dialogue context consistently outperforms random sampling (e.g. 0.624 versus 0.600 F1 using <math id="S3.SS1.p4.1.m1.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="S3.SS1.p4.1.m1.1a"><mrow id="S3.SS1.p4.1.m1.1.1" xref="S3.SS1.p4.1.m1.1.1.cmml"><mn id="S3.SS1.p4.1.m1.1.1.2" xref="S3.SS1.p4.1.m1.1.1.2.cmml">5</mn><mo id="S3.SS1.p4.1.m1.1.1.1" xref="S3.SS1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.m1.1b"><apply id="S3.SS1.p4.1.m1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.SS1.p4.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS1.p4.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.m1.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.m1.1c">5\%</annotation></semantics></math> data).</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.1" class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" style="width:212.5pt;"><img src="/html/2210.14169/assets/x4.png" id="S3.F6.1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Micro F1 on the <span id="S3.F6.1.3.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>emotion classification task using <span id="S3.F6.1.4.2" class="ltx_text ltx_font_typewriter">LTA</span>Â for different data sizes.
</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F6.4" class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" style="width:212.5pt;"><img src="/html/2210.14169/assets/x5.png" id="S3.F6.2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
Comparison of prompt-augmentation with <span id="S3.F6.4.6.1" class="ltx_text ltx_font_typewriter">LTA</span>Â and <span id="S3.F6.4.7.2" class="ltx_text ltx_font_smallcaps">WeakDAP</span>. <span id="S3.F6.4.8.3" class="ltx_text ltx_font_smallcaps">WeakDAP </span>with <math id="S3.F6.4.2.m1.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S3.F6.4.2.m1.1b"><mrow id="S3.F6.4.2.m1.1.1" xref="S3.F6.4.2.m1.1.1.cmml"><mn id="S3.F6.4.2.m1.1.1.2" xref="S3.F6.4.2.m1.1.1.2.cmml">10</mn><mo id="S3.F6.4.2.m1.1.1.1" xref="S3.F6.4.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.F6.4.2.m1.1c"><apply id="S3.F6.4.2.m1.1.1.cmml" xref="S3.F6.4.2.m1.1.1"><csymbol cd="latexml" id="S3.F6.4.2.m1.1.1.1.cmml" xref="S3.F6.4.2.m1.1.1.1">percent</csymbol><cn type="integer" id="S3.F6.4.2.m1.1.1.2.cmml" xref="S3.F6.4.2.m1.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.4.2.m1.1d">10\%</annotation></semantics></math> data outperforms full-shot state-of-the-art performance with S+PAGE, and baseline augmentation approaches mixed with full-shot data.
</figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span id="S3.SS2.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>Act Classification</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">There are four dialogue act labels: <em id="S3.SS2.p1.1.1" class="ltx_emph ltx_font_italic">inform, question, directive</em>, and <em id="S3.SS2.p1.1.2" class="ltx_emph ltx_font_italic">commissive</em>. While these individual terms are less descriptive than emotion labels, they form descriptive tokens if used as active verbs (e.g., â€œAlice directs Bob:â€). See Section <a href="#A5" title="Appendix E Example Prompts â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> in the Appendix for an example.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">Following our findings on Emotion Classification, we augment the few-shot <span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>Act Classification task with <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_typewriter">LTA</span>Â while providing full-shot performance for reference. Our results in TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.3 Cross-lingual Augmentation â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> indicate that <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_smallcaps">WeakDAP </span>results in the highest accuracy across all few-shot settings, although using STM on the unaugmented data achieves the highest F1 in the 5% setting. Noticeably, the performance improvements on the act classification are not as drastic as in the emotion classification task. This is likely because the baseline performance on the act classification task was already competitive.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Cross-lingual Augmentation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To assess the robustness of <span id="S3.SS3.p1.1.1" class="ltx_text ltx_font_smallcaps">WeakDAP</span>, we additionally evaluated its performance on a completely different setting: low-resource cross-lingual augmentation for intent classification on <span id="S3.SS3.p1.1.2" class="ltx_text ltx_font_smallcaps">FBTOD</span>. For augmentation, we used the Alexa Teacher Model <cite class="ltx_cite ltx_citemacro_citep">(Soltan etÂ al., <a href="#bib.bib42" title="" class="ltx_ref">2022</a>)</cite>, a 20B multi-lingual seq2seq language model pre-trained on a text de-noising objective similar to <cite class="ltx_cite ltx_citemacro_citet">Liu etÂ al. (<a href="#bib.bib29" title="" class="ltx_ref">2020</a>)</cite> using large publicly available datasets including mC4Â <cite class="ltx_cite ltx_citemacro_citep">(Xue etÂ al., <a href="#bib.bib52" title="" class="ltx_ref">2021</a>)</cite> and Wikipedia. For intent classification, we fine-tuned XLMRoBERTa (XLMR;Â <cite class="ltx_cite ltx_citemacro_citet">Conneau etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>).</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.3" class="ltx_p">Full-shot fine-tuning of XLMR yields an accuracy of <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="98.8\%" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mn id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">98.8</mn><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="latexml" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">98.8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">98.8\%</annotation></semantics></math> on the Spanish test set, however we focus on the few-shot Spanish setting, with few- and full-shot cross-lingual augmentation from English. For all few-shot datasets, we use <math id="S3.SS3.p2.2.m2.1" class="ltx_Math" alttext="1\%,5\%," display="inline"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1.1"><mrow id="S3.SS3.p2.2.m2.1.1.1.1.2" xref="S3.SS3.p2.2.m2.1.1.1.1.3.cmml"><mrow id="S3.SS3.p2.2.m2.1.1.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.1.1.cmml"><mn id="S3.SS3.p2.2.m2.1.1.1.1.1.1.2" xref="S3.SS3.p2.2.m2.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.SS3.p2.2.m2.1.1.1.1.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.1.1.1.1.cmml">%</mo></mrow><mo id="S3.SS3.p2.2.m2.1.1.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.1.1.3.cmml">,</mo><mrow id="S3.SS3.p2.2.m2.1.1.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.1.1.2.2.cmml"><mn id="S3.SS3.p2.2.m2.1.1.1.1.2.2.2" xref="S3.SS3.p2.2.m2.1.1.1.1.2.2.2.cmml">5</mn><mo id="S3.SS3.p2.2.m2.1.1.1.1.2.2.1" xref="S3.SS3.p2.2.m2.1.1.1.1.2.2.1.cmml">%</mo></mrow></mrow><mo id="S3.SS3.p2.2.m2.1.1.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><list id="S3.SS3.p2.2.m2.1.1.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2"><apply id="S3.SS3.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS3.p2.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1.1.1">percent</csymbol><cn type="integer" id="S3.SS3.p2.2.m2.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.1.1.2">1</cn></apply><apply id="S3.SS3.p2.2.m2.1.1.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2.2"><csymbol cd="latexml" id="S3.SS3.p2.2.m2.1.1.1.1.2.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2.2.1">percent</csymbol><cn type="integer" id="S3.SS3.p2.2.m2.1.1.1.1.2.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.1.1.2.2.2">5</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">1\%,5\%,</annotation></semantics></math> and <math id="S3.SS3.p2.3.m3.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S3.SS3.p2.3.m3.1a"><mrow id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml"><mn id="S3.SS3.p2.3.m3.1.1.2" xref="S3.SS3.p2.3.m3.1.1.2.cmml">10</mn><mo id="S3.SS3.p2.3.m3.1.1.1" xref="S3.SS3.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.1b"><apply id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1"><csymbol cd="latexml" id="S3.SS3.p2.3.m3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1.1">percent</csymbol><cn type="integer" id="S3.SS3.p2.3.m3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.1c">10\%</annotation></semantics></math> of the original training data for that language (see Section <a href="#A6" title="Appendix F Cross-lingual Augmentation Experimental Setup â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">F</span></a>). <span id="S3.SS3.p2.3.1" class="ltx_text ltx_font_smallcaps">FBTOD </span>is a single-turn dataset, so we adapt the in-context learning prompt in
<cite class="ltx_cite ltx_citemacro_citet">Sahu etÂ al. (<a href="#bib.bib38" title="" class="ltx_ref">2022</a>)</cite>, mixing the reference Spanish instance with randomly sampled English instances of the same label for in-context learning (example in Section <a href="#A5" title="Appendix E Example Prompts â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">E</span></a> in the Appendix). We do not examine Thai because the model has not been pre-trained on Thai data.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">We augment the few-shot Spanish setting using both low-resource and high-resource English data.
With low-resource English, we use the same percentage of data as the ground-truth Spanish data. TableÂ <a href="#S3.T2" title="Table 2 â€£ 3.3 Cross-lingual Augmentation â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> indicates that fine-tuning XLMR on <span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_smallcaps">WeakDAP </span>outperforms prompt-based augmentation in all settings both in terms of accuracy and F1 score.</p>
</div>
<figure id="S3.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T2.fig1" class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" style="width:195.1pt;">
<table id="S3.T2.fig1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.fig1.1.1.1" class="ltx_tr">
<td id="S3.T2.fig1.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt"><span id="S3.T2.fig1.1.1.1.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog</span></td>
<td id="S3.T2.fig1.1.1.1.2" class="ltx_td ltx_align_left ltx_border_tt">Accuracy</td>
<td id="S3.T2.fig1.1.1.1.3" class="ltx_td ltx_align_left ltx_border_tt">F1</td>
</tr>
<tr id="S3.T2.fig1.1.2.2" class="ltx_tr">
<td id="S3.T2.fig1.1.2.2.1" class="ltx_td ltx_align_left ltx_border_t">1% No Aug.</td>
<td id="S3.T2.fig1.1.2.2.2" class="ltx_td ltx_align_left ltx_border_t">0.789</td>
<td id="S3.T2.fig1.1.2.2.3" class="ltx_td ltx_align_left ltx_border_t">0.648</td>
</tr>
<tr id="S3.T2.fig1.1.3.3" class="ltx_tr">
<td id="S3.T2.fig1.1.3.3.1" class="ltx_td ltx_align_left">1% Prompt</td>
<td id="S3.T2.fig1.1.3.3.2" class="ltx_td ltx_align_left">0.792</td>
<td id="S3.T2.fig1.1.3.3.3" class="ltx_td ltx_align_left">0.714</td>
</tr>
<tr id="S3.T2.fig1.1.4.4" class="ltx_tr">
<td id="S3.T2.fig1.1.4.4.1" class="ltx_td ltx_align_left">Â Â Â Â Â + WeakDAP</td>
<td id="S3.T2.fig1.1.4.4.2" class="ltx_td ltx_align_left"><span id="S3.T2.fig1.1.4.4.2.1" class="ltx_text ltx_font_bold">0.796</span></td>
<td id="S3.T2.fig1.1.4.4.3" class="ltx_td ltx_align_left"><span id="S3.T2.fig1.1.4.4.3.1" class="ltx_text ltx_font_bold">0.716</span></td>
</tr>
<tr id="S3.T2.fig1.1.5.5" class="ltx_tr">
<td id="S3.T2.fig1.1.5.5.1" class="ltx_td ltx_align_left ltx_border_t">5% No Aug.</td>
<td id="S3.T2.fig1.1.5.5.2" class="ltx_td ltx_align_left ltx_border_t">0.817</td>
<td id="S3.T2.fig1.1.5.5.3" class="ltx_td ltx_align_left ltx_border_t"><span id="S3.T2.fig1.1.5.5.3.1" class="ltx_text ltx_font_bold">0.809</span></td>
</tr>
<tr id="S3.T2.fig1.1.6.6" class="ltx_tr">
<td id="S3.T2.fig1.1.6.6.1" class="ltx_td ltx_align_left">5% Prompt</td>
<td id="S3.T2.fig1.1.6.6.2" class="ltx_td ltx_align_left">0.828</td>
<td id="S3.T2.fig1.1.6.6.3" class="ltx_td ltx_align_left">0.762</td>
</tr>
<tr id="S3.T2.fig1.1.7.7" class="ltx_tr">
<td id="S3.T2.fig1.1.7.7.1" class="ltx_td ltx_align_left">Â Â Â Â Â + WeakDAP</td>
<td id="S3.T2.fig1.1.7.7.2" class="ltx_td ltx_align_left"><span id="S3.T2.fig1.1.7.7.2.1" class="ltx_text ltx_font_bold">0.832</span></td>
<td id="S3.T2.fig1.1.7.7.3" class="ltx_td ltx_align_left">0.765</td>
</tr>
<tr id="S3.T2.fig1.1.8.8" class="ltx_tr">
<td id="S3.T2.fig1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_t">10% No Aug.</td>
<td id="S3.T2.fig1.1.8.8.2" class="ltx_td ltx_align_left ltx_border_t">0.839</td>
<td id="S3.T2.fig1.1.8.8.3" class="ltx_td ltx_align_left ltx_border_t">0.802</td>
</tr>
<tr id="S3.T2.fig1.1.9.9" class="ltx_tr">
<td id="S3.T2.fig1.1.9.9.1" class="ltx_td ltx_align_left">10% Prompt</td>
<td id="S3.T2.fig1.1.9.9.2" class="ltx_td ltx_align_left">0.839</td>
<td id="S3.T2.fig1.1.9.9.3" class="ltx_td ltx_align_left">0.815</td>
</tr>
<tr id="S3.T2.fig1.1.10.10" class="ltx_tr">
<td id="S3.T2.fig1.1.10.10.1" class="ltx_td ltx_align_left">Â Â Â Â Â + WeakDAP</td>
<td id="S3.T2.fig1.1.10.10.2" class="ltx_td ltx_align_left"><span id="S3.T2.fig1.1.10.10.2.1" class="ltx_text ltx_font_bold">0.842</span></td>
<td id="S3.T2.fig1.1.10.10.3" class="ltx_td ltx_align_left"><span id="S3.T2.fig1.1.10.10.3.1" class="ltx_text ltx_font_bold">0.820</span></td>
</tr>
<tr id="S3.T2.fig1.1.11.11" class="ltx_tr">
<td id="S3.T2.fig1.1.11.11.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">100% SotA</td>
<td id="S3.T2.fig1.1.11.11.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">0.875</td>
<td id="S3.T2.fig1.1.11.11.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">â€”</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 1: </span>Accuracy and Micro F1 (ignoring majority label) on DailyDialog Act classification using STM with few-shot data. We augment using <span id="S3.T2.fig1.3.1" class="ltx_text ltx_font_typewriter">LTA</span>Â resulting in data sizes of at most two times the original data size.</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.T2.6" class="ltx_figure ltx_figure_panel ltx_parbox ltx_align_middle" style="width:195.1pt;">
<table id="S3.T2.4.4" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T2.4.4.4" class="ltx_tr">
<td id="S3.T2.4.4.4.5" class="ltx_td ltx_align_left ltx_border_tt">
<span id="S3.T2.4.4.4.5.1" class="ltx_text ltx_font_smallcaps">FBTOD </span>ES</td>
<td id="S3.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Acc<sub id="S3.T2.1.1.1.1.1" class="ltx_sub"><span id="S3.T2.1.1.1.1.1.1" class="ltx_text ltx_font_italic">LR</span></sub>
</td>
<td id="S3.T2.2.2.2.2" class="ltx_td ltx_align_left ltx_border_tt">F1<sub id="S3.T2.2.2.2.2.1" class="ltx_sub"><span id="S3.T2.2.2.2.2.1.1" class="ltx_text ltx_font_italic">LR</span></sub>
</td>
<td id="S3.T2.3.3.3.3" class="ltx_td ltx_align_left ltx_border_tt">Acc<sub id="S3.T2.3.3.3.3.1" class="ltx_sub"><span id="S3.T2.3.3.3.3.1.1" class="ltx_text ltx_font_italic">HR</span></sub>
</td>
<td id="S3.T2.4.4.4.4" class="ltx_td ltx_align_left ltx_border_tt">F1<sub id="S3.T2.4.4.4.4.1" class="ltx_sub"><span id="S3.T2.4.4.4.4.1.1" class="ltx_text ltx_font_italic">HR</span></sub>
</td>
</tr>
<tr id="S3.T2.4.4.5.1" class="ltx_tr">
<td id="S3.T2.4.4.5.1.1" class="ltx_td ltx_align_left ltx_border_t">1% No Aug.</td>
<td id="S3.T2.4.4.5.1.2" class="ltx_td ltx_align_left ltx_border_t">0.572</td>
<td id="S3.T2.4.4.5.1.3" class="ltx_td ltx_align_left ltx_border_t">0.164</td>
<td id="S3.T2.4.4.5.1.4" class="ltx_td ltx_align_left ltx_border_t">0.572</td>
<td id="S3.T2.4.4.5.1.5" class="ltx_td ltx_align_left ltx_border_t">0.164</td>
</tr>
<tr id="S3.T2.4.4.6.2" class="ltx_tr">
<td id="S3.T2.4.4.6.2.1" class="ltx_td ltx_align_left">1% Prompt</td>
<td id="S3.T2.4.4.6.2.2" class="ltx_td ltx_align_left">0.737</td>
<td id="S3.T2.4.4.6.2.3" class="ltx_td ltx_align_left">0.316</td>
<td id="S3.T2.4.4.6.2.4" class="ltx_td ltx_align_left">0.776</td>
<td id="S3.T2.4.4.6.2.5" class="ltx_td ltx_align_left">0.359</td>
</tr>
<tr id="S3.T2.4.4.7.3" class="ltx_tr">
<td id="S3.T2.4.4.7.3.1" class="ltx_td ltx_align_left">Â Â Â Â Â + WeakDAP</td>
<td id="S3.T2.4.4.7.3.2" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.7.3.2.1" class="ltx_text ltx_font_bold">0.834</span></td>
<td id="S3.T2.4.4.7.3.3" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.7.3.3.1" class="ltx_text ltx_font_bold">0.495</span></td>
<td id="S3.T2.4.4.7.3.4" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.7.3.4.1" class="ltx_text ltx_font_bold">0.831</span></td>
<td id="S3.T2.4.4.7.3.5" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.7.3.5.1" class="ltx_text ltx_font_bold">0.528</span></td>
</tr>
<tr id="S3.T2.4.4.8.4" class="ltx_tr">
<td id="S3.T2.4.4.8.4.1" class="ltx_td ltx_align_left ltx_border_t">5% No Aug.</td>
<td id="S3.T2.4.4.8.4.2" class="ltx_td ltx_align_left ltx_border_t">0.845</td>
<td id="S3.T2.4.4.8.4.3" class="ltx_td ltx_align_left ltx_border_t">0.417</td>
<td id="S3.T2.4.4.8.4.4" class="ltx_td ltx_align_left ltx_border_t">0.845</td>
<td id="S3.T2.4.4.8.4.5" class="ltx_td ltx_align_left ltx_border_t">0.417</td>
</tr>
<tr id="S3.T2.4.4.9.5" class="ltx_tr">
<td id="S3.T2.4.4.9.5.1" class="ltx_td ltx_align_left">5% Prompt</td>
<td id="S3.T2.4.4.9.5.2" class="ltx_td ltx_align_left">0.953</td>
<td id="S3.T2.4.4.9.5.3" class="ltx_td ltx_align_left">0.641</td>
<td id="S3.T2.4.4.9.5.4" class="ltx_td ltx_align_left">0.954</td>
<td id="S3.T2.4.4.9.5.5" class="ltx_td ltx_align_left">0.682</td>
</tr>
<tr id="S3.T2.4.4.10.6" class="ltx_tr">
<td id="S3.T2.4.4.10.6.1" class="ltx_td ltx_align_left">Â Â Â Â Â + WeakDAP</td>
<td id="S3.T2.4.4.10.6.2" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.10.6.2.1" class="ltx_text ltx_font_bold">0.957</span></td>
<td id="S3.T2.4.4.10.6.3" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.10.6.3.1" class="ltx_text ltx_font_bold">0.715</span></td>
<td id="S3.T2.4.4.10.6.4" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.10.6.4.1" class="ltx_text ltx_font_bold">0.962</span></td>
<td id="S3.T2.4.4.10.6.5" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.10.6.5.1" class="ltx_text ltx_font_bold">0.732</span></td>
</tr>
<tr id="S3.T2.4.4.11.7" class="ltx_tr">
<td id="S3.T2.4.4.11.7.1" class="ltx_td ltx_align_left ltx_border_t">10% No Aug.</td>
<td id="S3.T2.4.4.11.7.2" class="ltx_td ltx_align_left ltx_border_t">0.942</td>
<td id="S3.T2.4.4.11.7.3" class="ltx_td ltx_align_left ltx_border_t">0.588</td>
<td id="S3.T2.4.4.11.7.4" class="ltx_td ltx_align_left ltx_border_t">0.942</td>
<td id="S3.T2.4.4.11.7.5" class="ltx_td ltx_align_left ltx_border_t">0.588</td>
</tr>
<tr id="S3.T2.4.4.12.8" class="ltx_tr">
<td id="S3.T2.4.4.12.8.1" class="ltx_td ltx_align_left">10% Prompt</td>
<td id="S3.T2.4.4.12.8.2" class="ltx_td ltx_align_left">0.973</td>
<td id="S3.T2.4.4.12.8.3" class="ltx_td ltx_align_left">0.772</td>
<td id="S3.T2.4.4.12.8.4" class="ltx_td ltx_align_left">0.973</td>
<td id="S3.T2.4.4.12.8.5" class="ltx_td ltx_align_left">0.791</td>
</tr>
<tr id="S3.T2.4.4.13.9" class="ltx_tr">
<td id="S3.T2.4.4.13.9.1" class="ltx_td ltx_align_left">Â Â Â Â Â + WeakDAP</td>
<td id="S3.T2.4.4.13.9.2" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.13.9.2.1" class="ltx_text ltx_font_bold">0.979</span></td>
<td id="S3.T2.4.4.13.9.3" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.13.9.3.1" class="ltx_text ltx_font_bold">0.905</span></td>
<td id="S3.T2.4.4.13.9.4" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.13.9.4.1" class="ltx_text ltx_font_bold">0.976</span></td>
<td id="S3.T2.4.4.13.9.5" class="ltx_td ltx_align_left"><span id="S3.T2.4.4.13.9.5.1" class="ltx_text ltx_font_bold">0.846</span></td>
</tr>
<tr id="S3.T2.4.4.14.10" class="ltx_tr">
<td id="S3.T2.4.4.14.10.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">100% No Aug.</td>
<td id="S3.T2.4.4.14.10.2" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">0.988</td>
<td id="S3.T2.4.4.14.10.3" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">0.889</td>
<td id="S3.T2.4.4.14.10.4" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">0.988</td>
<td id="S3.T2.4.4.14.10.5" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">0.889</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Table 2: </span>Accuracy and Macro F1 on the FBTOD Spanish dataset.
<math id="S3.T2.6.6.m1.1" class="ltx_Math" alttext="HR/LR" display="inline"><semantics id="S3.T2.6.6.m1.1b"><mrow id="S3.T2.6.6.m1.1.1" xref="S3.T2.6.6.m1.1.1.cmml"><mrow id="S3.T2.6.6.m1.1.1.2" xref="S3.T2.6.6.m1.1.1.2.cmml"><mrow id="S3.T2.6.6.m1.1.1.2.2" xref="S3.T2.6.6.m1.1.1.2.2.cmml"><mi id="S3.T2.6.6.m1.1.1.2.2.2" xref="S3.T2.6.6.m1.1.1.2.2.2.cmml">H</mi><mo lspace="0em" rspace="0em" id="S3.T2.6.6.m1.1.1.2.2.1" xref="S3.T2.6.6.m1.1.1.2.2.1.cmml">â€‹</mo><mi id="S3.T2.6.6.m1.1.1.2.2.3" xref="S3.T2.6.6.m1.1.1.2.2.3.cmml">R</mi></mrow><mo id="S3.T2.6.6.m1.1.1.2.1" xref="S3.T2.6.6.m1.1.1.2.1.cmml">/</mo><mi id="S3.T2.6.6.m1.1.1.2.3" xref="S3.T2.6.6.m1.1.1.2.3.cmml">L</mi></mrow><mo lspace="0em" rspace="0em" id="S3.T2.6.6.m1.1.1.1" xref="S3.T2.6.6.m1.1.1.1.cmml">â€‹</mo><mi id="S3.T2.6.6.m1.1.1.3" xref="S3.T2.6.6.m1.1.1.3.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.m1.1c"><apply id="S3.T2.6.6.m1.1.1.cmml" xref="S3.T2.6.6.m1.1.1"><times id="S3.T2.6.6.m1.1.1.1.cmml" xref="S3.T2.6.6.m1.1.1.1"></times><apply id="S3.T2.6.6.m1.1.1.2.cmml" xref="S3.T2.6.6.m1.1.1.2"><divide id="S3.T2.6.6.m1.1.1.2.1.cmml" xref="S3.T2.6.6.m1.1.1.2.1"></divide><apply id="S3.T2.6.6.m1.1.1.2.2.cmml" xref="S3.T2.6.6.m1.1.1.2.2"><times id="S3.T2.6.6.m1.1.1.2.2.1.cmml" xref="S3.T2.6.6.m1.1.1.2.2.1"></times><ci id="S3.T2.6.6.m1.1.1.2.2.2.cmml" xref="S3.T2.6.6.m1.1.1.2.2.2">ğ»</ci><ci id="S3.T2.6.6.m1.1.1.2.2.3.cmml" xref="S3.T2.6.6.m1.1.1.2.2.3">ğ‘…</ci></apply><ci id="S3.T2.6.6.m1.1.1.2.3.cmml" xref="S3.T2.6.6.m1.1.1.2.3">ğ¿</ci></apply><ci id="S3.T2.6.6.m1.1.1.3.cmml" xref="S3.T2.6.6.m1.1.1.3">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.m1.1d">HR/LR</annotation></semantics></math>: High-resource (full-shot) and Low-resource (few-shot matching the Spanish percentage) English Training data.
</figcaption>
</figure>
</div>
</div>
</figure>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">We contribute significant progress towards few-shot prompt-based augmentation for dialogue tasks. We introduce <span id="S4.p1.1.1" class="ltx_text ltx_font_smallcaps">WeakDAP </span>and demonstrate its augmentation quality filtering capabilities by surpassing full-shot state-of-the-art performance with few-shot examples on <span id="S4.p1.1.2" class="ltx_text ltx_font_smallcaps">DailyDialog </span>and achieving strong few-shot performance on <span id="S4.p1.1.3" class="ltx_text ltx_font_smallcaps">FBTOD</span>. In the future, we will examine ways to integrate soft prompting into <span id="S4.p1.1.4" class="ltx_text ltx_font_smallcaps">WeakDAP </span>as well as identify an appropriate feedback mechanism for response generation.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barikeri etÂ al. [2021]</span>
<span class="ltx_bibblock">
Soumya Barikeri, Anne Lauscher, Ivan VuliÄ‡, and Goran GlavaÅ¡.

</span>
<span class="ltx_bibblock">Redditbias: A real-world resource for bias evaluation and debiasing
of conversational language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.03521</em>, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Black etÂ al. [2022]</span>
<span class="ltx_bibblock">
Sidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao,
Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, etÂ al.

</span>
<span class="ltx_bibblock">Gpt-neox-20b: An open-source autoregressive language model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of BigScience Episode # 5â€“Workshop on
Challenges &amp; Perspectives in Creating Large Language Models</em>, pages 95â€“136,
2022.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bommasani etÂ al. [2021]</span>
<span class="ltx_bibblock">
Rishi Bommasani, DrewÂ A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
von Arx, MichaelÂ S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, etÂ al.

</span>
<span class="ltx_bibblock">On the opportunities and risks of foundation models.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2108.07258</em>, 2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. [2020]</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
etÂ al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877â€“1901, 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2021]</span>
<span class="ltx_bibblock">
Jiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal, and Diyi Yang.

</span>
<span class="ltx_bibblock">An empirical survey of data augmentation for limited data learning in
nlp.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.07499</em>, 2021.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2020]</span>
<span class="ltx_bibblock">
Wenshi Chen, Bowen Zhang, and Mingyu Lu.

</span>
<span class="ltx_bibblock">Uncertainty quantification for multilabel text classification.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery</em>, 10(6):e1384, 2020.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. [2022]</span>
<span class="ltx_bibblock">
Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and HeÂ He.

</span>
<span class="ltx_bibblock">Meta-learning via language model in-context tuning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 719â€“730, 2022.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christian [2020]</span>
<span class="ltx_bibblock">
Brian Christian.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">The alignment problem: Machine learning and human values</em>.

</span>
<span class="ltx_bibblock">WW Norton &amp; Company, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau etÂ al. [2020]</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco GuzmÃ¡n, Ã‰douard Grave, Myle Ott, Luke Zettlemoyer,
and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 8440â€“8451, 2020.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CsÃ¡ky etÂ al. [2019]</span>
<span class="ltx_bibblock">
RichÃ¡rd CsÃ¡ky, Patrik Purgai, and GÃ¡bor Recski.

</span>
<span class="ltx_bibblock">Improving neural conversational models with entropy-based data
filtering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics</em>, pages 5650â€“5669, 2019.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edunov etÂ al. [2018]</span>
<span class="ltx_bibblock">
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier.

</span>
<span class="ltx_bibblock">Understanding back-translation at scale.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing</em>, pages 489â€“500, 2018.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Faal etÂ al. [2022]</span>
<span class="ltx_bibblock">
Farshid Faal, Ketra Schmitt, and JiaÂ Yuan Yu.

</span>
<span class="ltx_bibblock">Reward modeling for mitigating toxicity in transformer-based language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Applied Intelligence</em>, pages 1â€“15, 2022.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng etÂ al. [2020]</span>
<span class="ltx_bibblock">
StevenÂ Y. Feng, Varun Gangal, Dongyeop Kang, Teruko Mitamura, and Eduard Hovy.

</span>
<span class="ltx_bibblock">GenAug: Data augmentation for finetuning text generators.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">Proceedings of Deep Learning Inside Out (DeeLIO): The First
Workshop on Knowledge Extraction and Integration for Deep Learning
Architectures</em>, pages 29â€“42, Online, November 2020. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: 10.18653/v1/2020.deelio-1.4.

</span>
<span class="ltx_bibblock">URL <span id="bib.bib13.2.1" class="ltx_text ltx_font_typewriter">https://aclanthology.org/2020.deelio-1.4</span>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. [2020a]</span>
<span class="ltx_bibblock">
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, etÂ al.

</span>
<span class="ltx_bibblock">The pile: An 800gb dataset of diverse text for language modeling.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2101.00027</em>, 2020a.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. [2020b]</span>
<span class="ltx_bibblock">
Silin Gao, Yichi Zhang, Zhijian Ou, and Zhou Yu.

</span>
<span class="ltx_bibblock">Paraphrase augmented task-oriented dialog generation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics</em>, pages 639â€“649, 2020b.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo and Viktor [2004]</span>
<span class="ltx_bibblock">
Hongyu Guo and HernaÂ L Viktor.

</span>
<span class="ltx_bibblock">Boosting with data generation: improving the classification of hard
to learn examples.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">International Conference on Industrial, Engineering and
Other Applications of Applied Intelligent Systems</em>, pages 1082â€“1091.
Springer, 2004.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He etÂ al. [2021]</span>
<span class="ltx_bibblock">
Zihao He, Leili Tavabi, Kristina Lerman, and Mohammad Soleymani.

</span>
<span class="ltx_bibblock">Speaker turn modeling for dialogue act classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pages 2150â€“2157, 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karimi etÂ al. [2021]</span>
<span class="ltx_bibblock">
Akbar Karimi, Leonardo Rossi, and Andrea Prati.

</span>
<span class="ltx_bibblock">AEDA: An easier data augmentation technique for text
classification.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2021</em>, pages 2748â€“2754, Punta Cana, Dominican Republic, November 2021.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: 10.18653/v1/2021.findings-emnlp.234.

</span>
<span class="ltx_bibblock">URL <span id="bib.bib18.2.1" class="ltx_text ltx_font_typewriter">https://aclanthology.org/2021.findings-emnlp.234</span>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim etÂ al. [2021]</span>
<span class="ltx_bibblock">
Yekyung Kim, Seohyeong Jeong, and Kyunghyun Cho.

</span>
<span class="ltx_bibblock">Linda: Unsupervised learning to interpolate in natural language
processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.13969</em>, 2021.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">KulhÃ¡nek etÂ al. [2021]</span>
<span class="ltx_bibblock">
JonÃ¡Å¡ KulhÃ¡nek, VojtÄ›ch HudeÄek, TomÃ¡Å¡
Nekvinda, and OndÅ™ej DuÅ¡ek.

</span>
<span class="ltx_bibblock">Augpt: Auxiliary tasks and data augmentation for end-to-end dialogue
with pre-trained language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 3rd Workshop on Natural Language
Processing for Conversational AI</em>, pages 198â€“210, 2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar etÂ al. [2020]</span>
<span class="ltx_bibblock">
Varun Kumar, Ashutosh Choudhary, and Eunah Cho.

</span>
<span class="ltx_bibblock">Data augmentation using pre-trained transformer models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2nd Workshop on Life-long Learning for
Spoken Language Systems</em>, pages 18â€“26, 2020.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lauscher etÂ al. [2021]</span>
<span class="ltx_bibblock">
Anne Lauscher, Tobias LÃ¼ken, and Goran GlavaÅ¡.

</span>
<span class="ltx_bibblock">Sustainable modular debiasing of language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2109.03646</em>, 2021.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee and Choi [2021]</span>
<span class="ltx_bibblock">
Bongseok Lee and YongÂ Suk Choi.

</span>
<span class="ltx_bibblock">Graph based network with contextualized representations of turns in
dialogue.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pages 443â€“455, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2017]</span>
<span class="ltx_bibblock">
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu.

</span>
<span class="ltx_bibblock">Dailydialog: A manually labelled multi-turn dialogue dataset.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Eighth International Joint Conference on
Natural Language Processing (Volume 1: Long Papers)</em>, pages 986â€“995, 2017.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. [2021]</span>
<span class="ltx_bibblock">
Zekun Li, Hong Wang, Alon Albalak, Yingrui Yang, Jing Qian, Shiyang Li, and
Xifeng Yan.

</span>
<span class="ltx_bibblock">Making something out of nothing: Building robust task-oriented
dialogue systems from scratch.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of Alexa Prize TaskBot</em>, 2021.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang etÂ al. [2021]</span>
<span class="ltx_bibblock">
Chen Liang, Chong Yang, Jing Xu, Juyang Huang, Yongliang Wang, and Yang Dong.

</span>
<span class="ltx_bibblock">S+ page: A speaker and position-aware graph neural network model for
emotion recognition in conversation.

</span>
<span class="ltx_bibblock"><em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2112.12389</em>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2021]</span>
<span class="ltx_bibblock">
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
Graham Neubig.

</span>
<span class="ltx_bibblock">Pre-train, prompt, and predict: A systematic survey of prompting
methods in natural language processing.

</span>
<span class="ltx_bibblock"><em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2107.13586</em>, 2021.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2019]</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. [2020]</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Multilingual denoising pre-training for neural machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
8:726â€“742, 2020.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu etÂ al. [2022]</span>
<span class="ltx_bibblock">
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.

</span>
<span class="ltx_bibblock">Fantastically ordered prompts and where to find them: Overcoming
few-shot prompt order sensitivity.

</span>
<span class="ltx_bibblock">In <em id="bib.bib30.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 8086â€“8098,
2022.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min etÂ al. [2022]</span>
<span class="ltx_bibblock">
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
Hajishirzi, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Rethinking the role of demonstrations: What makes in-context learning
work?

</span>
<span class="ltx_bibblock"><em id="bib.bib31.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2202.12837</em>, 2022.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ott etÂ al. [2018]</span>
<span class="ltx_bibblock">
Myle Ott, Michael Auli, David Grangier, and Marcâ€™Aurelio Ranzato.

</span>
<span class="ltx_bibblock">Analyzing uncertainty in neural machine translation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib32.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
3956â€“3965. PMLR, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papangelis etÂ al. [2021]</span>
<span class="ltx_bibblock">
Alexandros Papangelis, Karthik Gopalakrishnan, Aishwarya Padmakumar, Seokhwan
Kim, Gokhan Tur, and Dilek Hakkani-Tur.

</span>
<span class="ltx_bibblock">Generative conversational networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib33.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 22nd Annual Meeting of the Special
Interest Group on Discourse and Dialogue</em>, pages 111â€“120, Singapore and
Online, July 2021. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paszke etÂ al. [2019]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, etÂ al.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock"><em id="bib.bib34.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>, 32, 2019.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pavlopoulos etÂ al. [2020]</span>
<span class="ltx_bibblock">
John Pavlopoulos, Jeffrey Sorensen, Lucas Dixon, Nithum Thain, and Ion
Androutsopoulos.

</span>
<span class="ltx_bibblock">Toxicity detection: Does context really matter?

</span>
<span class="ltx_bibblock"><em id="bib.bib35.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2006.00998</em>, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez etÂ al. [2021]</span>
<span class="ltx_bibblock">
Ethan Perez, Douwe Kiela, and Kyunghyun Cho.

</span>
<span class="ltx_bibblock">True few-shot learning with language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib36.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, 34, 2021.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robinson etÂ al. [2020]</span>
<span class="ltx_bibblock">
Joshua Robinson, Stefanie Jegelka, and Suvrit Sra.

</span>
<span class="ltx_bibblock">Strength from weakness: fast learning using weak supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib37.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 37th International Conference on Machine
Learning</em>, pages 8127â€“8136, 2020.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu etÂ al. [2022]</span>
<span class="ltx_bibblock">
Gaurav Sahu, Pau Rodriguez, Issam Laradji, Parmida Atighehchian, David Vazquez,
and Dzmitry Bahdanau.

</span>
<span class="ltx_bibblock">Data augmentation for intent classification with off-the-shelf large
language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib38.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 4th Workshop on NLP for Conversational
AI</em>, pages 47â€“57, 2022.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick etÂ al. [2021]</span>
<span class="ltx_bibblock">
Timo Schick, Sahana Udupa, and Hinrich SchÃ¼tze.

</span>
<span class="ltx_bibblock">Self-diagnosis and self-debiasing: A proposal for reducing
corpus-based bias in nlp.

</span>
<span class="ltx_bibblock"><em id="bib.bib39.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9:1408â€“1424, 2021.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schuster etÂ al. [2019]</span>
<span class="ltx_bibblock">
Sebastian Schuster, Sonal Gupta, Rushin Shah, and Mike Lewis.

</span>
<span class="ltx_bibblock">Cross-lingual transfer learning for multilingual task oriented
dialog.

</span>
<span class="ltx_bibblock">In <em id="bib.bib40.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, pages 3795â€“3805, 2019.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Seidenfeld [1986]</span>
<span class="ltx_bibblock">
Teddy Seidenfeld.

</span>
<span class="ltx_bibblock">Entropy and uncertainty.

</span>
<span class="ltx_bibblock"><em id="bib.bib41.1.1" class="ltx_emph ltx_font_italic">Philosophy of Science</em>, 53(4):467â€“491,
1986.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soltan etÂ al. [2022]</span>
<span class="ltx_bibblock">
Saleh Soltan, Shankar Ananthakrishnan, Jack FitzGerald, Rahul Gupta, Wael
Hamza, Haidar Khan, Charith Peris, Stephen Rawls, Andy Rosenbaum, Anna
Rumshisky, etÂ al.

</span>
<span class="ltx_bibblock">Alexatm 20b: Few-shot learning using a large-scale multilingual
seq2seq model.

</span>
<span class="ltx_bibblock"><em id="bib.bib42.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2208.01448</em>, 2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang [2021]</span>
<span class="ltx_bibblock">
Ben Wang.

</span>
<span class="ltx_bibblock">Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer
Language Model with JAX.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_typewriter">https://github.com/kingoflolz/mesh-transformer-jax</span>, May 2021.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Komatsuzaki [2021]</span>
<span class="ltx_bibblock">
Ben Wang and Aran Komatsuzaki.

</span>
<span class="ltx_bibblock">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.

</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text ltx_font_typewriter">https://github.com/kingoflolz/mesh-transformer-jax</span>, May 2021.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang [2008]</span>
<span class="ltx_bibblock">
QiupingÂ A Wang.

</span>
<span class="ltx_bibblock">Probability distribution and entropy as a measure of uncertainty.

</span>
<span class="ltx_bibblock"><em id="bib.bib45.1.1" class="ltx_emph ltx_font_italic">Journal of Physics A: Mathematical and Theoretical</em>,
41(6):065004, 2008.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. [2020]</span>
<span class="ltx_bibblock">
Yan Wang, Jiayu Zhang, Jun Ma, Shaojun Wang, and Jing Xiao.

</span>
<span class="ltx_bibblock">Contextualized emotion recognition in conversation as sequence
tagging.

</span>
<span class="ltx_bibblock">In <em id="bib.bib46.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 21th annual meeting of the special
interest group on discourse and dialogue</em>, pages 186â€“195, 2020.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei and Zou [2019]</span>
<span class="ltx_bibblock">
Jason Wei and Kai Zou.

</span>
<span class="ltx_bibblock">EDA: Easy data augmentation techniques for boosting performance on
text classification tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib47.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pages 6382â€“6388, Hong Kong,
China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: 10.18653/v1/D19-1670.

</span>
<span class="ltx_bibblock">URL <span id="bib.bib47.2.1" class="ltx_text ltx_font_typewriter">https://aclanthology.org/D19-1670</span>.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. [2022]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, EdÂ Chi, Quoc Le, and
Denny Zhou.

</span>
<span class="ltx_bibblock">Chain of thought prompting elicits reasoning in large language
models.

</span>
<span class="ltx_bibblock"><em id="bib.bib48.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2201.11903</em>, 2022.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf etÂ al. [2020]</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz,
etÂ al.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em id="bib.bib49.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2020 conference on empirical methods in
natural language processing: system demonstrations</em>, pages 38â€“45, 2020.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao and Wang [2019]</span>
<span class="ltx_bibblock">
Yijun Xiao and WilliamÂ Yang Wang.

</span>
<span class="ltx_bibblock">Quantifying uncertainties in natural language processing tasks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib50.1.1" class="ltx_emph ltx_font_italic">Proceedings of the AAAI Conference on Artificial
Intelligence</em>, volumeÂ 33, pages 7322â€“7329, 2019.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie etÂ al. [2020]</span>
<span class="ltx_bibblock">
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le.

</span>
<span class="ltx_bibblock">Unsupervised data augmentation for consistency training.

</span>
<span class="ltx_bibblock"><em id="bib.bib51.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
33:6256â€“6268, 2020.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue etÂ al. [2021]</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, and Colin Raffel.

</span>
<span class="ltx_bibblock">mt5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock">In <em id="bib.bib52.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pages 483â€“498, 2021.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al. [2020]</span>
<span class="ltx_bibblock">
Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan
LeÂ Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey.

</span>
<span class="ltx_bibblock">Generative data augmentation for commonsense reasoning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib53.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics:
EMNLP 2020</em>, pages 1008â€“1025. Association for Computational Linguistics,
2020.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. [2022]</span>
<span class="ltx_bibblock">
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
Chen, Christopher Dewan, Mona Diab, Xian Li, XiÂ Victoria Lin, Todor Mihaylov,
Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, PunitÂ Singh Koura, Anjali
Sridhar, Tianlu Wang, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Opt: Open pre-trained transformer language models, 2022.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al. [2021]</span>
<span class="ltx_bibblock">
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.

</span>
<span class="ltx_bibblock">Calibrate before use: Improving few-shot performance of language
models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib55.1.1" class="ltx_emph ltx_font_italic">International Conference on Machine Learning</em>, pages
12697â€“12706. PMLR, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A0.F7" class="ltx_figure"><img src="/html/2210.14169/assets/x6.png" id="A0.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="346" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Classification results on the <span id="A0.F7.4.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>emotion classification task using Conversation Trajectory Augmentation. Multiplier size represents the dataset size in multiples of the amount of gold data. <math id="A0.F7.2.m1.1" class="ltx_Math" alttext="n\%" display="inline"><semantics id="A0.F7.2.m1.1b"><mrow id="A0.F7.2.m1.1.1" xref="A0.F7.2.m1.1.1.cmml"><mi id="A0.F7.2.m1.1.1.2" xref="A0.F7.2.m1.1.1.2.cmml">n</mi><mo id="A0.F7.2.m1.1.1.1" xref="A0.F7.2.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A0.F7.2.m1.1c"><apply id="A0.F7.2.m1.1.1.cmml" xref="A0.F7.2.m1.1.1"><csymbol cd="latexml" id="A0.F7.2.m1.1.1.1.cmml" xref="A0.F7.2.m1.1.1.1">percent</csymbol><ci id="A0.F7.2.m1.1.1.2.cmml" xref="A0.F7.2.m1.1.1.2">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A0.F7.2.m1.1d">n\%</annotation></semantics></math> represents the percentage of the gold data used.</figcaption>
</figure>
<figure id="A0.F8" class="ltx_figure"><img src="/html/2210.14169/assets/x7.png" id="A0.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Example prompt using Conversation Trajectory Augmentation for emotion classification using GPT-J. The original conversation (left) has four turns. After allowing each speaker to speak once, GPT-J autoregressively generates the rest of the conversation. Turn 3 (top right) is generated using the first two ground truth turns, and Turn 4 (bottom right) is generated using the first two ground truth turns and the generated third turn. The final candidate conversation is represented by all of the turns in the bottom right box.</figcaption>
</figure>
<figure id="A0.F9" class="ltx_figure"><img src="/html/2210.14169/assets/x8.png" id="A0.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="324" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Example prompt using All Turn Augmentation for emotion classification using GPT-J. The original conversation (left) has four turns. After allowing each speaker to speak once, GPT-J generates turn three using the first two ground truth turns (top right), turn four using the first three ground truth turns (middle right), turn five using the first four ground truth turns (bottom left), and turn six using the first five ground truth turns (bottom right). Each of the blue boxes represents a new conversation with the corresponding generated turn as the new endpoint.</figcaption>
</figure>
<figure id="A0.F10" class="ltx_figure"><img src="/html/2210.14169/assets/x9.png" id="A0.F10.g1" class="ltx_graphics ltx_centering ltx_img_square" width="176" height="147" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Example prompt using Last Turn Augmentation for act augmentation using GPT-J. The original conversation had nine turns, and the augmented conversation is the same except with the last turn replaced with a generated utterance. GPT-J learns to use some of the speakerâ€™s tendencies, such as using the term â€œErm.â€</figcaption>
</figure>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Ethical Considerations</h2>

<section id="A1.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Pre-trained Language Model Biases</h4>

<div id="A1.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px1.p1.1" class="ltx_p">In this work, we directly use only two datasets: <span id="A1.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>and <span id="A1.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_smallcaps">FBTOD</span>. However, large pre-trained language models like GPT-J have already been pre-trained on massive corpora such as The PileÂ <cite class="ltx_cite ltx_citemacro_citep">[Gao etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020a</a>]</cite>, which is a webcrawl of much of the internet. While this promotes diverse language, this also provides no guarantee over the types of content that the model is capable of producing. It is possible that the model could generate negative, hurtful, or offensive contentÂ <cite class="ltx_cite ltx_citemacro_citep">[Gao etÂ al., <a href="#bib.bib14" title="" class="ltx_ref">2020a</a>, Christian, <a href="#bib.bib8" title="" class="ltx_ref">2020</a>]</cite>. Prompting methods as the ones proposed in this work fundamentally rely on large pre-trained language models. As such, it is possible that this hurtful content could leak into augmented datasets, if left unchecked. While in this work we only discuss dialogue understanding tasks, this may present a bigger issue in dialogue generation where hurtful utterances may actually be relayed to users. There are several works that attempt to mitigate these issues, for example <cite class="ltx_cite ltx_citemacro_cite">Lauscher etÂ al. [<a href="#bib.bib22" title="" class="ltx_ref">2021</a>], Schick etÂ al. [<a href="#bib.bib39" title="" class="ltx_ref">2021</a>], Barikeri etÂ al. [<a href="#bib.bib1" title="" class="ltx_ref">2021</a>], Faal etÂ al. [<a href="#bib.bib12" title="" class="ltx_ref">2022</a>], Pavlopoulos etÂ al. [<a href="#bib.bib35" title="" class="ltx_ref">2020</a>]</cite>.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Data Biases</h4>

<div id="A1.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px2.p1.1" class="ltx_p">In a similar vein, every dataset, including <span id="A1.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>and <span id="A1.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_smallcaps">FBTOD </span>has its own biases. While introducing language model output from another language manifold (i.e., a pre-trained language modelâ€™s training corpora) will help to lessen some of the biases present in unaugmented datasets, generation conditioned solely on existing dialogue contexts may continue to reinforce some of these existing biases.</p>
</div>
</section>
<section id="A1.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reproducibility</h4>

<div id="A1.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="A1.SS0.SSS0.Px3.p1.1" class="ltx_p">Upon acceptance, we plan to release all of the augmentation code, as well as the seed data (i.e. each few-shot setting) used in all experiments.</p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Limitations</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p">In this work, we consider two different dialogue contexts â€” social chit-chat in <span id="A2.p1.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>and multi-lingual task-oriented dialogue in <span id="A2.p1.1.2" class="ltx_text ltx_font_smallcaps">Facebook Multilingual Task-Oriented Dialogue</span>. Due to computational constraints, it is difficult to consider several dialogue contexts with all of our experimental settings. However, we believe that we present a set of experiments representative of the scope of our methodâ€™s generalizability to different dialogue understanding tasks and integratability with different pre-trained language models.</p>
</div>
<section id="A2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Prompt Selection</h4>

<div id="A2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="A2.SS0.SSS0.Px1.p1.1" class="ltx_p">Much recent research focuses on how to design prompts to optimize performance on a variety of different tasksÂ <cite class="ltx_cite ltx_citemacro_cite">Liu etÂ al. [<a href="#bib.bib27" title="" class="ltx_ref">2021</a>]</cite>. In this work, we did not apply such prompt engineering methods. Instead, we focused on the conversational nature of prompting for augmentation in dialogue tasks. Our prompt primarily consists of dialogue context. Design decisions included the use of the names â€œAliceâ€ and â€œBob,â€ as well as the choice to encode instance labels in natural language form. We did not formally evaluate these decisions, but we subjectively saw that the use of other names can yield similar performance. We also noticed that using names generally provided better results than generic speaker tags such as â€œSpeaker 1â€ and â€œSpeaker 2.â€</p>
</div>
</section>
</section>
<section id="A3" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Datasets</h2>

<div id="A3.p1" class="ltx_para">
<p id="A3.p1.1" class="ltx_p"><span id="A3.p1.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog <cite class="ltx_cite ltx_citemacro_citep"><span id="A3.p1.1.1.1.1" class="ltx_text ltx_font_upright">[</span>Li etÂ al.<span id="A3.p1.1.1.2.2.1.1" class="ltx_text ltx_font_upright">, </span><a href="#bib.bib24" title="" class="ltx_ref">2017</a><span id="A3.p1.1.1.3.3" class="ltx_text ltx_font_upright">]</span></cite></span> is a high-quality open-domain conversation dataset. The official <span id="A3.p1.1.2" class="ltx_text ltx_font_smallcaps">DailyDialog </span>training set contains 11,118 dialogues, while the validation and test sets each have 1,000 dialogues. Each conversation is annotated with a topic label and has on average eight turns. Additionally, each turn is annotated with a dialogue act and an emotion label.</p>
</div>
<div id="A3.p2" class="ltx_para">
<p id="A3.p2.1" class="ltx_p">Facebook Multilingual Task-Oriented Dialogue (<span id="A3.p2.1.1" class="ltx_text ltx_font_smallcaps">FBTOD</span>; <cite class="ltx_cite ltx_citemacro_citet">Schuster etÂ al. [<a href="#bib.bib40" title="" class="ltx_ref">2019</a>]</cite>) is a dataset which contains single-turn task-oriented utterances in English, Spanish, and Thai. Each utterance is annotated with one of twelve intent labels.</p>
</div>
</section>
<section id="A4" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span><span id="A4.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>Experiments</h2>

<div id="A4.p1" class="ltx_para">
<p id="A4.p1.5" class="ltx_p">In order to measure and isolate the effect of prompting methods, we hold many of the experimental settings fixed. For data generation with GPT-J, we use top-<math id="A4.p1.1.m1.1" class="ltx_Math" alttext="p" display="inline"><semantics id="A4.p1.1.m1.1a"><mi id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml">p</mi><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><ci id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">p</annotation></semantics></math> sampling with <math id="A4.p1.2.m2.1" class="ltx_Math" alttext="p=0.92" display="inline"><semantics id="A4.p1.2.m2.1a"><mrow id="A4.p1.2.m2.1.1" xref="A4.p1.2.m2.1.1.cmml"><mi id="A4.p1.2.m2.1.1.2" xref="A4.p1.2.m2.1.1.2.cmml">p</mi><mo id="A4.p1.2.m2.1.1.1" xref="A4.p1.2.m2.1.1.1.cmml">=</mo><mn id="A4.p1.2.m2.1.1.3" xref="A4.p1.2.m2.1.1.3.cmml">0.92</mn></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.2.m2.1b"><apply id="A4.p1.2.m2.1.1.cmml" xref="A4.p1.2.m2.1.1"><eq id="A4.p1.2.m2.1.1.1.cmml" xref="A4.p1.2.m2.1.1.1"></eq><ci id="A4.p1.2.m2.1.1.2.cmml" xref="A4.p1.2.m2.1.1.2">ğ‘</ci><cn type="float" id="A4.p1.2.m2.1.1.3.cmml" xref="A4.p1.2.m2.1.1.3">0.92</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.2.m2.1c">p=0.92</annotation></semantics></math>. The resulting generated data is parsed from the decoded language model outputs. All of our downstream classification experiments are performed using STM. In order to isolate the effects of augmentation and due to computational limitations, we fix the tunable STM parameters with an initial learning rate of <math id="A4.p1.3.m3.1" class="ltx_Math" alttext=".0001" display="inline"><semantics id="A4.p1.3.m3.1a"><mn id="A4.p1.3.m3.1.1" xref="A4.p1.3.m3.1.1.cmml">.0001</mn><annotation-xml encoding="MathML-Content" id="A4.p1.3.m3.1b"><cn type="float" id="A4.p1.3.m3.1.1.cmml" xref="A4.p1.3.m3.1.1">.0001</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.3.m3.1c">.0001</annotation></semantics></math>, <math id="A4.p1.4.m4.1" class="ltx_Math" alttext="2" display="inline"><semantics id="A4.p1.4.m4.1a"><mn id="A4.p1.4.m4.1.1" xref="A4.p1.4.m4.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="A4.p1.4.m4.1b"><cn type="integer" id="A4.p1.4.m4.1.1.cmml" xref="A4.p1.4.m4.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.4.m4.1c">2</annotation></semantics></math> recurrent layers and a <math id="A4.p1.5.m5.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="A4.p1.5.m5.1a"><mrow id="A4.p1.5.m5.1.1" xref="A4.p1.5.m5.1.1.cmml"><mn id="A4.p1.5.m5.1.1.2" xref="A4.p1.5.m5.1.1.2.cmml">50</mn><mo id="A4.p1.5.m5.1.1.1" xref="A4.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.p1.5.m5.1b"><apply id="A4.p1.5.m5.1.1.cmml" xref="A4.p1.5.m5.1.1"><csymbol cd="latexml" id="A4.p1.5.m5.1.1.1.cmml" xref="A4.p1.5.m5.1.1.1">percent</csymbol><cn type="integer" id="A4.p1.5.m5.1.1.2.cmml" xref="A4.p1.5.m5.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.5.m5.1c">50\%</annotation></semantics></math> dropout layer. We let STM fine-tune for up to 100 epochs; we use early stopping with a patience of 10 epochs.</p>
</div>
<div id="A4.p2" class="ltx_para">
<p id="A4.p2.1" class="ltx_p">All experiments are implemented with <span id="A4.p2.1.1" class="ltx_text ltx_font_typewriter">PyTorch</span>Â <cite class="ltx_cite ltx_citemacro_citep">[Paszke etÂ al., <a href="#bib.bib34" title="" class="ltx_ref">2019</a>]</cite> and HuggingFaceâ€™s <span id="A4.p2.1.2" class="ltx_text ltx_font_typewriter">Transformers</span>Â <cite class="ltx_cite ltx_citemacro_citep">[Wolf etÂ al., <a href="#bib.bib49" title="" class="ltx_ref">2020</a>]</cite>, and run on AWS <span id="A4.p2.1.3" class="ltx_text ltx_font_italic">p3.16xlarge</span> EC2 instances.</p>
</div>
<section id="A4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Conversation Trajectory Augmentation</h3>

<div id="A4.SS1.p1" class="ltx_para">
<p id="A4.SS1.p1.1" class="ltx_p">In FigureÂ <a href="#A0.F7" title="Figure 7 â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we see that we are able to reach state-of-the-art performance on the emotion classification task using conversation trajectory augmentation as well. Augmented full-shot STM reaches an F1 score of 0.695. In all except the <math id="A4.SS1.p1.1.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="A4.SS1.p1.1.m1.1a"><mrow id="A4.SS1.p1.1.m1.1.1" xref="A4.SS1.p1.1.m1.1.1.cmml"><mn id="A4.SS1.p1.1.m1.1.1.2" xref="A4.SS1.p1.1.m1.1.1.2.cmml">1</mn><mo id="A4.SS1.p1.1.m1.1.1.1" xref="A4.SS1.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A4.SS1.p1.1.m1.1b"><apply id="A4.SS1.p1.1.m1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1"><csymbol cd="latexml" id="A4.SS1.p1.1.m1.1.1.1.cmml" xref="A4.SS1.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="A4.SS1.p1.1.m1.1.1.2.cmml" xref="A4.SS1.p1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS1.p1.1.m1.1c">1\%</annotation></semantics></math> ground-truth setting, Conversation Trajectory Augmentation underperforms Last Turn Augmentation. This is likely due to the fact that Conversation Trajectory Augmentation departs from the ground-truth data distribution, in contrast to Last Turn Augmentation as displayed in FigureÂ <a href="#S3.F4" title="Figure 4 â€£ 3.1 DailyDialog Emotion Classification â€£ 3 Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. This is not necessarily a downside of generating new trajectories - it just indicates that it does not perform as strongly on this particular classification task. By definition, training a model on data that closely matches the testing distribution will be advantageous during evaluation. However, beyond classification tasks, it is possible that this diverse data distribution will be more favorable, e.g., in response and dialogue generation, where one would want to see more diverse responses.</p>
</div>
</section>
<section id="A4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>All-Turn Augmentation</h3>

<figure id="A4.T3" class="ltx_table">
<table id="A4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A4.T3.1.1.1" class="ltx_tr">
<th id="A4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column">Size</th>
<th id="A4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column">Augmented Mult.</th>
<th id="A4.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column">F1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A4.T3.1.2.1" class="ltx_tr">
<td id="A4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_border_t">1%</td>
<td id="A4.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_t">5.7x</td>
<td id="A4.T3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_t">0.459</td>
</tr>
<tr id="A4.T3.1.3.2" class="ltx_tr">
<td id="A4.T3.1.3.2.1" class="ltx_td ltx_align_left">5%</td>
<td id="A4.T3.1.3.2.2" class="ltx_td ltx_align_left">6.2x</td>
<td id="A4.T3.1.3.2.3" class="ltx_td ltx_align_left">0.463</td>
</tr>
<tr id="A4.T3.1.4.3" class="ltx_tr">
<td id="A4.T3.1.4.3.1" class="ltx_td ltx_align_left">10%</td>
<td id="A4.T3.1.4.3.2" class="ltx_td ltx_align_left">6.1x</td>
<td id="A4.T3.1.4.3.3" class="ltx_td ltx_align_left">0.610</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Resulting augmentation sizes and classification F1 Micro using All-Turn Augmentation on the <span id="A4.T3.3.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>emotion classification task.</figcaption>
</figure>
<div id="A4.SS2.p1" class="ltx_para">
<p id="A4.SS2.p1.1" class="ltx_p">As displayed in TableÂ <a href="#A4.T3" title="Table 3 â€£ D.2 All-Turn Augmentation â€£ Appendix D DailyDialog Experiments â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, All-Turn Augmentation generally does not outperform either Conversation Trajectory Augmentation nor Last-Turn Augmentation. Moreover, we see that the resulting augmented sizes are roughly six times that of the original data. The closest comparable augmented size used with Last-Turn and Conversation Trajectory Augmentation is a multiplier of five, but at that multiple, either Last Turn or Conversation Trajectory Augmentation yields the strongest performance for each data setting.</p>
</div>
</section>
</section>
<section id="A5" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Example Prompts</h2>

<div id="A5.p1" class="ltx_para">
<p id="A5.p1.1" class="ltx_p">We present several examples of prompts corresponding to the different methods of conversation augmentation.</p>
</div>
<div id="A5.p2" class="ltx_para">
<p id="A5.p2.1" class="ltx_p">FigureÂ <a href="#S2.F2" title="Figure 2 â€£ 2.2 Augmentation with Weak Supervision â€£ 2 Data Augmentation Methods â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows an example of Last Turn Augmentation and demonstrates that by omitting the last turn of the original conversation, we can form a prefix prompt that allows GPT-J to generate an utterance according to a prescribed emotion, whether it be the ground truth emotion from the training data or a user-defined emotion. The ground truth turns given as context combined with the generated utterance constitute a new augmented conversation.</p>
</div>
<div id="A5.p3" class="ltx_para">
<p id="A5.p3.1" class="ltx_p">FigureÂ <a href="#A0.F8" title="Figure 8 â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows an example of Conversation Trajectory Augmentation. In order to set the context for each speaker, we always include the first ground truth turn for each speaker in the context. For each subsequent turn, we autoregressively let the generation model generate the utterance by feeding in the previously generated utterance. This process continues until the number of turns in the new augmented conversation reaches the length of the original ground truth conversation.</p>
</div>
<div id="A5.p4" class="ltx_para">
<p id="A5.p4.7" class="ltx_p">We show an example of All Turn Augmentation in FigureÂ <a href="#A0.F9" title="Figure 9 â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a>. We again include the first ground truth turn for each speaker in the conversation. However, instead of autoregressively feeding in previously generated utterances, the context of the generated turn <math id="A5.p4.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A5.p4.1.m1.1a"><mi id="A5.p4.1.m1.1.1" xref="A5.p4.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A5.p4.1.m1.1b"><ci id="A5.p4.1.m1.1.1.cmml" xref="A5.p4.1.m1.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.1.m1.1c">i</annotation></semantics></math> is always the ground truth turns <math id="A5.p4.2.m2.1" class="ltx_Math" alttext="1" display="inline"><semantics id="A5.p4.2.m2.1a"><mn id="A5.p4.2.m2.1.1" xref="A5.p4.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A5.p4.2.m2.1b"><cn type="integer" id="A5.p4.2.m2.1.1.cmml" xref="A5.p4.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.2.m2.1c">1</annotation></semantics></math> through <math id="A5.p4.3.m3.1" class="ltx_Math" alttext="i-1" display="inline"><semantics id="A5.p4.3.m3.1a"><mrow id="A5.p4.3.m3.1.1" xref="A5.p4.3.m3.1.1.cmml"><mi id="A5.p4.3.m3.1.1.2" xref="A5.p4.3.m3.1.1.2.cmml">i</mi><mo id="A5.p4.3.m3.1.1.1" xref="A5.p4.3.m3.1.1.1.cmml">âˆ’</mo><mn id="A5.p4.3.m3.1.1.3" xref="A5.p4.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p4.3.m3.1b"><apply id="A5.p4.3.m3.1.1.cmml" xref="A5.p4.3.m3.1.1"><minus id="A5.p4.3.m3.1.1.1.cmml" xref="A5.p4.3.m3.1.1.1"></minus><ci id="A5.p4.3.m3.1.1.2.cmml" xref="A5.p4.3.m3.1.1.2">ğ‘–</ci><cn type="integer" id="A5.p4.3.m3.1.1.3.cmml" xref="A5.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.3.m3.1c">i-1</annotation></semantics></math>. Since the generated turn <math id="A5.p4.4.m4.1" class="ltx_Math" alttext="i" display="inline"><semantics id="A5.p4.4.m4.1a"><mi id="A5.p4.4.m4.1.1" xref="A5.p4.4.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A5.p4.4.m4.1b"><ci id="A5.p4.4.m4.1.1.cmml" xref="A5.p4.4.m4.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.4.m4.1c">i</annotation></semantics></math> is not guaranteed to form a coherent conversation when with ground truth turns <math id="A5.p4.5.m5.1" class="ltx_Math" alttext="i+1" display="inline"><semantics id="A5.p4.5.m5.1a"><mrow id="A5.p4.5.m5.1.1" xref="A5.p4.5.m5.1.1.cmml"><mi id="A5.p4.5.m5.1.1.2" xref="A5.p4.5.m5.1.1.2.cmml">i</mi><mo id="A5.p4.5.m5.1.1.1" xref="A5.p4.5.m5.1.1.1.cmml">+</mo><mn id="A5.p4.5.m5.1.1.3" xref="A5.p4.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A5.p4.5.m5.1b"><apply id="A5.p4.5.m5.1.1.cmml" xref="A5.p4.5.m5.1.1"><plus id="A5.p4.5.m5.1.1.1.cmml" xref="A5.p4.5.m5.1.1.1"></plus><ci id="A5.p4.5.m5.1.1.2.cmml" xref="A5.p4.5.m5.1.1.2">ğ‘–</ci><cn type="integer" id="A5.p4.5.m5.1.1.3.cmml" xref="A5.p4.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.5.m5.1c">i+1</annotation></semantics></math> through <math id="A5.p4.6.m6.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A5.p4.6.m6.1a"><mi id="A5.p4.6.m6.1.1" xref="A5.p4.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A5.p4.6.m6.1b"><ci id="A5.p4.6.m6.1.1.cmml" xref="A5.p4.6.m6.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.6.m6.1c">n</annotation></semantics></math> (where <math id="A5.p4.7.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A5.p4.7.m7.1a"><mi id="A5.p4.7.m7.1.1" xref="A5.p4.7.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A5.p4.7.m7.1b"><ci id="A5.p4.7.m7.1.1.cmml" xref="A5.p4.7.m7.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p4.7.m7.1c">n</annotation></semantics></math> is the length of the ground truth conversation), we consider each generated turn to be an endpoint for a new conversation.</p>
</div>
<div id="A5.p5" class="ltx_para">
<p id="A5.p5.1" class="ltx_p">Finally, we show an example using Last Turn Augmentation while prescribing a <span id="A5.p5.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>act label in Figure <a href="#A0.F10" title="Figure 10 â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>. We see that GPT-J correctly generates an utterance that follows the act â€œinform,â€ and is even able to pick up on some of the speakerâ€™s tendencies (e.g., using the onomatopoeia â€œErmâ€).</p>
</div>
<figure id="A5.F11" class="ltx_figure"><img src="/html/2210.14169/assets/x10.png" id="A5.F11.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="154" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Cross-lingual prompt between Spanish and English for <span id="A5.F11.2.1" class="ltx_text ltx_font_smallcaps">FBTOD </span>intent detection. [CLM] is a token reserved for the model that is required for in-context prompts.</figcaption>
</figure>
</section>
<section id="A6" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Cross-lingual Augmentation Experimental Setup</h2>

<div id="A6.p1" class="ltx_para">
<p id="A6.p1.4" class="ltx_p">The original <span id="A6.p1.4.1" class="ltx_text ltx_font_smallcaps">FBTOD </span>dataset contains an English dataset with 30,521 training instances, 4,181 evaluation instances, and 8,621 testing instances. It also contains a Spanish dataset with 3,617 training instances, 1,983 evaluation instances, and 3,043 testing instances. For both languages, we created versions of the training set that had <math id="A6.p1.1.m1.1" class="ltx_Math" alttext="1\%" display="inline"><semantics id="A6.p1.1.m1.1a"><mrow id="A6.p1.1.m1.1.1" xref="A6.p1.1.m1.1.1.cmml"><mn id="A6.p1.1.m1.1.1.2" xref="A6.p1.1.m1.1.1.2.cmml">1</mn><mo id="A6.p1.1.m1.1.1.1" xref="A6.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><apply id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1"><csymbol cd="latexml" id="A6.p1.1.m1.1.1.1.cmml" xref="A6.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="A6.p1.1.m1.1.1.2.cmml" xref="A6.p1.1.m1.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">1\%</annotation></semantics></math>, <math id="A6.p1.2.m2.1" class="ltx_Math" alttext="5\%" display="inline"><semantics id="A6.p1.2.m2.1a"><mrow id="A6.p1.2.m2.1.1" xref="A6.p1.2.m2.1.1.cmml"><mn id="A6.p1.2.m2.1.1.2" xref="A6.p1.2.m2.1.1.2.cmml">5</mn><mo id="A6.p1.2.m2.1.1.1" xref="A6.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A6.p1.2.m2.1b"><apply id="A6.p1.2.m2.1.1.cmml" xref="A6.p1.2.m2.1.1"><csymbol cd="latexml" id="A6.p1.2.m2.1.1.1.cmml" xref="A6.p1.2.m2.1.1.1">percent</csymbol><cn type="integer" id="A6.p1.2.m2.1.1.2.cmml" xref="A6.p1.2.m2.1.1.2">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.2.m2.1c">5\%</annotation></semantics></math>, and <math id="A6.p1.3.m3.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="A6.p1.3.m3.1a"><mrow id="A6.p1.3.m3.1.1" xref="A6.p1.3.m3.1.1.cmml"><mn id="A6.p1.3.m3.1.1.2" xref="A6.p1.3.m3.1.1.2.cmml">10</mn><mo id="A6.p1.3.m3.1.1.1" xref="A6.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="A6.p1.3.m3.1b"><apply id="A6.p1.3.m3.1.1.cmml" xref="A6.p1.3.m3.1.1"><csymbol cd="latexml" id="A6.p1.3.m3.1.1.1.cmml" xref="A6.p1.3.m3.1.1.1">percent</csymbol><cn type="integer" id="A6.p1.3.m3.1.1.2.cmml" xref="A6.p1.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.3.m3.1c">10\%</annotation></semantics></math> of the original number of training instances. For each percentage, we randomly sampled that proportion of training examples for each intent label. In cases where that proportion would result in a number smaller than <math id="A6.p1.4.m4.1" class="ltx_Math" alttext="1.0" display="inline"><semantics id="A6.p1.4.m4.1a"><mn id="A6.p1.4.m4.1.1" xref="A6.p1.4.m4.1.1.cmml">1.0</mn><annotation-xml encoding="MathML-Content" id="A6.p1.4.m4.1b"><cn type="float" id="A6.p1.4.m4.1.1.cmml" xref="A6.p1.4.m4.1.1">1.0</cn></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.4.m4.1c">1.0</annotation></semantics></math>, we ensured that there would be at least one training instance.</p>
</div>
<div id="A6.p2" class="ltx_para">
<p id="A6.p2.1" class="ltx_p">For augmentation, we use the prompt given in FigureÂ <a href="#A5.F11" title="Figure 11 â€£ Appendix E Example Prompts â€£ Weakly Supervised Data Augmentation Through Prompting for Dialogue Understanding" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a>. We perform beam search, taking up to three sequences as augmented data and rejecting any duplicate examples. For classification, we fine-tune XLMRoBERTa with a maximum sequence length of 128, 80 training epochs, and an initial learning rate of 5e-5.</p>
</div>
</section>
<section id="A7" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Entropy-based Weak Filtering</h2>

<div id="A7.p1" class="ltx_para">
<p id="A7.p1.1" class="ltx_p">We hypothesized that two categories of synthetic data could improve to eventual task performance. The first is simply <span id="A7.p1.1.1" class="ltx_text ltx_font_italic">correct</span> data â€” synthetic training instances which match the intended instance label. However, we determine this correctness using an imperfect classifier. Thus, the second category is <span id="A7.p1.1.2" class="ltx_text ltx_font_italic">â€œhard to learnâ€</span> data. This follows from early workÂ <cite class="ltx_cite ltx_citemacro_citep">[Guo and Viktor, <a href="#bib.bib16" title="" class="ltx_ref">2004</a>]</cite> finding that classification performance can be improved by focusing on augmenting datasets with â€œhard to learnâ€ examples. However <cite class="ltx_cite ltx_citemacro_citet">Guo and Viktor [<a href="#bib.bib16" title="" class="ltx_ref">2004</a>]</cite> identify such difficult examples post hoc through classifier performance. Solely evaluating classifier performance for each example is prohibitively expensive for large models on large datasets. Thus, in our case, we make our decisions based on uncertainty. Out of those instances that the classifier states are incorrect, we do not filter out the ones for which the classification is made with high uncertainty, hypothesizing that uncertainty is a strong proxy for data pointsâ€™ learning difficulty, which may be more useful in training the next iteration of the classifier.</p>
</div>
<div id="A7.p2" class="ltx_para">
<p id="A7.p2.1" class="ltx_p">There are several ways to consider how to quantify uncertainty.
<cite class="ltx_cite ltx_citemacro_citet">Ott etÂ al. [<a href="#bib.bib32" title="" class="ltx_ref">2018</a>]</cite> quantified uncertainty for neural machine translation post hoc by comparing generation methodsâ€™ sequence-level probability mass coverage of ground-truth translations. While these methods may be appropriate for translation or even response generation-related tasks, there is no reference point for what the ideal augmented data is.</p>
</div>
<div id="A7.p3" class="ltx_para">
<p id="A7.p3.1" class="ltx_p">Other approaches have looked specifically at quantifying dataset uncertainty using Bayesian Neural NetworksÂ <cite class="ltx_cite ltx_citemacro_citep">[Xiao and Wang, <a href="#bib.bib50" title="" class="ltx_ref">2019</a>, Chen etÂ al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>]</cite>. However, dataset level approaches are not necessarily appropriate for augmentation, where one needs to make inferences about data quality at the instance level. Moreover, approaches requiring Bayesian Neural Networks do not achieve state-of-the-art performance on the <span id="A7.p3.1.1" class="ltx_text ltx_font_smallcaps">DailyDialog </span>classification tasks.</p>
</div>
<div id="A7.p4" class="ltx_para">
<p id="A7.p4.1" class="ltx_p">On the other hand, entropy maximization has long been used as an information theoretic technique for maximizing uncertaintyÂ <cite class="ltx_cite ltx_citemacro_citep">[Seidenfeld, <a href="#bib.bib41" title="" class="ltx_ref">1986</a>, Wang, <a href="#bib.bib45" title="" class="ltx_ref">2008</a>]</cite>. Even in natural language processing, <cite class="ltx_cite ltx_citemacro_citet">CsÃ¡ky etÂ al. [<a href="#bib.bib10" title="" class="ltx_ref">2019</a>]</cite> performs data filtering using entropy to improve diversity by specifically removing generic data points. They achieve promising results in dialogue generation, and moreover, entropy computation is directly applicable at the instance level. Therefore, we adopt entropy into our framework to identify uncertain and difficult data points.</p>
</div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2210.14168" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2210.14169" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2210.14169">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2210.14169" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2210.14170" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Mar 14 04:25:11 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
