<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space</title>
<!--Generated on Thu Sep 19 13:04:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.12745v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S1" title="In Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S2" title="In Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Synthetic spoken commands generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S2.SS1" title="In 2 Synthetic spoken commands generation ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>ASR-based filtering in the TTS loop</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S2.SS2" title="In 2 Synthetic spoken commands generation ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Google Speech Commands (GSC)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S2.SS3" title="In 2 Synthetic spoken commands generation ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Two synthetic GSC versions: <span class="ltx_text ltx_font_italic">Synth</span> and <span class="ltx_text ltx_font_italic">Synth (F)</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S3" title="In Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>First SCC experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S3.SS1" title="In 3 First SCC experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Baseline SCC model: MatchboxNet</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S3.SS2" title="In 3 First SCC experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Fully Synthetic SCC</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4" title="In Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>SSL-based SCC Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.SS1" title="In 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Results with WavLM-Base-Plus representations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.SS2" title="In 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>PCA Visualizations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.SS3" title="In 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>CycleGAN Domain Adaptation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S5" title="In Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S6" title="In Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Enhancing Synthetic Training Data for Speech Commands: 
<br class="ltx_break"/>From ASR-Based Filtering to Domain Adaptation in SSL Latent Space</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1"><span class="ltx_text" id="id1.id1.1" style="font-size:90%;">The use of synthetic speech as data augmentation is gaining increasing popularity in fields such as automatic speech recognition and speech classification tasks. Despite novel text-to-speech systems with voice cloning capabilities, that allow the usage of a larger amount of voices based on short audio segments, it is known that these systems tend to hallucinate and oftentimes produce bad data that will most likely have a negative impact on the downstream task. In the present work, we conduct a set of experiments around zero-shot learning with synthetic speech data for the specific task of speech commands classification. Our results on the Google Speech Commands dataset show that a simple ASR-based filtering method can have a big impact in the quality of the generated data, translating to a better performance. Furthermore, despite the good quality of the generated speech data, we also show that synthetic and real speech can still be easily distinguishable when using self-supervised (WavLM) features, an aspect further explored with a CycleGAN to bridge the gap between the two types of speech material.</span></p>
</div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="p1.1.1" style="font-size:90%;">Index Terms<span class="ltx_text ltx_font_upright" id="p1.1.1.1">— <span class="ltx_text ltx_font_medium" id="p1.1.1.1.1">
Speech commands, keyword-spotting, speech synthesis, cyclegan, self-supervised features</span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text" id="S1.p1.1.1" style="font-size:90%;">Recent works show that text-to-speech (TTS) systems, can effectively be used as a successful data augmentation scheme for automatic speech recognition (ASR) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib5" title="">5</a><span class="ltx_text" id="S1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.4" style="font-size:90%;">, specially in low resource scenarios </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib7" title="">7</a><span class="ltx_text" id="S1.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.7" style="font-size:90%;">. Moreover, it is worth nothing that there may be limits to the benefits of using synthetic data, as it has been shown that bispectral analysis can still differentiate with a good degree of confidence between state-of-the-art TTS systems and real human speech </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib8" title="">8</a><span class="ltx_text" id="S1.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.10" style="font-size:90%;">, showing a mismatch between the two types of audio. A similar case can be found when using self-supervised (SSL) features in anti-spoofing detection </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib9" title="">9</a><span class="ltx_text" id="S1.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.13" style="font-size:90%;">, showing a good discriminating ability between the two types of speech. As a consequence, there is still a noticeable difference between systems trained on synthetic speech and those trained on real data </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib10" title="">10</a><span class="ltx_text" id="S1.p1.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.16" style="font-size:90%;">, leaving an interesting research gap to explore.</span></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text" id="S1.p2.1.1" style="font-size:90%;">Current TTS systems allow the possibility of multi-speaker synthesis by simply using short reference audio files </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib11" title="">11</a><span class="ltx_text" id="S1.p2.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.4" style="font-size:90%;">. On one hand, the usage of these voice cloning technologies </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib12" title="">12</a><span class="ltx_text" id="S1.p2.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.7" style="font-size:90%;">, allows the employment of significantly larger corpora and speech resources as generative material. On the other hand, it is known that autoregressive speech synthesis can hallucinate, an aspect that can oftentimes make the generated data low-quality or even unusable </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib13" title="">13</a><span class="ltx_text" id="S1.p2.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.10" style="font-size:90%;">. The lack of control on these hallucinations can be detrimental to the quality of the generated data, as well as to the downstream task that makes use of it.</span></p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text" id="S1.p3.1.1" style="font-size:90%;">Despite the extensive literature on TTS as a data augmentation technique useful for ASR, there is only a couple of articles on the topic of data augmentation, and more specifically synthetic audio for speech commands classification (SCC) and keyword spotting (KWS). The work of </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib14" title="">14</a><span class="ltx_text" id="S1.p3.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.4" style="font-size:90%;"> fine-tunes a RNN-T KWS system on new keywords obtained from a multi-speaker TTS. The results show considerable improvements on those keywords, however the performance improvements are dependent on speaker diversity and amount of data. In </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib15" title="">15</a><span class="ltx_text" id="S1.p3.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.7" style="font-size:90%;">, a self-supervised learning (SSL) approach for KWS was used. Here, the system was trained on a large general speech corpora of untranscribed audio, containing noise and reverb as data augmentation. The results show that with this pre-training, the quantity of real data used can be greatly reduced, while still achieving a good performance. Similarly, the work of </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib16" title="">16</a><span class="ltx_text" id="S1.p3.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.10" style="font-size:90%;"> also proposes a SSL learning structure for downstream KWS fine-tuning. Here, a large amount of keywords are obtained from well-known public corpora to train a keyword embedding extractor. The results suggest a good adaptability and performance gains when applied to new classes as well as an easy extrapolation to different languages other than English. Similarly, the work of </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p3.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib17" title="">17</a><span class="ltx_text" id="S1.p3.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p3.1.13" style="font-size:90%;"> also tested a SSL hypothesis for KWS, however this time by pairing synthetic data with normal data as an augmentation. Here, a pre-trained embedding model was fine tuned on the downstream task of KWS, using different mixes of real and synthetic data, showcasing high accuracy.</span></p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1"><span class="ltx_text" id="S1.p4.1.1" style="font-size:90%;">In the present work, we intend to explore different topics around SCC systems fully trained on synthetic data, such as: </span><span class="ltx_text ltx_font_bold" id="S1.p4.1.2" style="font-size:90%;">(i)</span><span class="ltx_text" id="S1.p4.1.3" style="font-size:90%;"> Propose an ASR filtering method that is able to generate hallucination-free synthetic data. </span><span class="ltx_text ltx_font_bold" id="S1.p4.1.4" style="font-size:90%;">(ii)</span><span class="ltx_text" id="S1.p4.1.5" style="font-size:90%;"> Show that despite the clean synthetic data, there is still a performance gap, and that synthetic speech is still easy to differentiate from real speech using SSL features. </span><span class="ltx_text ltx_font_bold" id="S1.p4.1.6" style="font-size:90%;">(iii)</span><span class="ltx_text" id="S1.p4.1.7" style="font-size:90%;"> Propose a method to improve the quality of synthetic speech representations in higher dimensional spaces, using the same SSL features, that can further improve SCC performance.</span></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Synthetic spoken commands generation</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text" id="S2.p1.1.1" style="font-size:90%;">We propose a framework to generate synthetic speech data using the open-source implementation of the XTTS v2 speech synthesizer and its zero-shot voice cloning capabilities </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib11" title="">11</a><span class="ltx_text" id="S2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.p1.1.4" style="font-size:90%;">. In order to ensure a large variety of cloned speakers, our method randomly samples, without reposition, spoken utterances gathered from the Common Voice (CV) open source and multilingual initiative </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib18" title="">18</a><span class="ltx_text" id="S2.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.p1.1.7" style="font-size:90%;">. CV features a collection of read sentences in a variety of languages. We gathered together a total of 4.36 million utterances spoken by 174 thousand distinct speakers, from the CV datasets available in English, Spanish, French, Italian, German, Mandarin and Japanese. It allowed to use a voice only once for each synthesized audio snippet.
Preliminary experiments showed us that using multilingual speech data is on-par with using speech data from the target language only.
</span></p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>ASR-based filtering in the TTS loop</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text" id="S2.SS1.p1.1.1" style="font-size:90%;">Given the end-to-end nature of XTTS v2, that can oftentimes produce hallucinations and artefacts, we introduce an ASR filtering scheme, illustrated in Fig. </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S2.F1" style="font-size:90%;" title="Figure 1 ‣ 2.1 ASR-based filtering in the TTS loop ‣ 2 Synthetic spoken commands generation ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S2.SS1.p1.1.2" style="font-size:90%;">. We keep audio snippets for which the two transcriptions obtained with two distinct large vocabulary ASR systems exactly match the word of interest. The two ASR systems are a fast-conformer transducer-based and a Jasper CTC-based models, both trained for English and available off-the-shelf within the NeMO toolkit </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib19" title="">19</a><span class="ltx_text" id="S2.SS1.p1.1.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.5" style="font-size:90%;">.
Using two ASR systems improved the filtering, compared to using a single one.</span></p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="97" id="S2.F1.g1" src="extracted/5863711/filtering_short1.png" width="329"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S2.F1.4.1.1">Fig. 1</span>: </span>Proposed TTS generation loop with ASR filtering.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Google Speech Commands (GSC)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text" id="S2.SS2.p1.1.1" style="font-size:90%;">We perform our experiments based on the Google Speech Commands (v2) Dataset </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib20" title="">20</a><span class="ltx_text" id="S2.SS2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.4" style="font-size:90%;">, widely used to benchmark KWS systems. The corpus contains 105,000 utterances, one-second long each, belonging to one of 35 classes. Among these classes lie the ten speech commands (</span><span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.5" style="font-size:90%;">Yes, No, Up, Down, Left, Right, On, Off, Stop, Go</span><span class="ltx_text" id="S2.SS2.p1.1.6" style="font-size:90%;">), followed by numbers from zero to nine and other arbitrary words that covered different phonemes (</span><span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.7" style="font-size:90%;">Backward, Bed, Bird, Cat, Dog, Forward, Follow, Happy, House, Learn, Marvin, Sheila, Tree, Visual, Wow</span><span class="ltx_text" id="S2.SS2.p1.1.8" style="font-size:90%;">). In the present work, we focus on the subset of labels containing the original ten speech commands, categorizing the remaining words under an </span><span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.9" style="font-size:90%;">unknown</span><span class="ltx_text" id="S2.SS2.p1.1.10" style="font-size:90%;"> class. While GSC also contains silence audio files, the proposed way to generate these silences is random extraction from a small set of larger files, therefore, it is likely that the same silence slice will end up in both train and validation or test sets, biasing the performance and not promoting a static train, validation and test sets. We believe that this aspect is oftentimes ignored when benchmarking with this dataset. Therefore, we opted to not use the silence class in our experiments.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Two synthetic GSC versions: <span class="ltx_text ltx_font_italic" id="S2.SS3.1.1">Synth</span> and <span class="ltx_text ltx_font_italic" id="S2.SS3.2.2">Synth (F)</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1"><span class="ltx_text" id="S2.SS3.p1.1.1" style="font-size:90%;">We used XTTS v2 with voice cloning on CV, as explained above, to generate a first synthetic version of the training subset of GSC, equal in number of audio files. We will refer, here-after, to the data as </span><span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2" style="font-size:90%;">Synth</span><span class="ltx_text" id="S2.SS3.p1.1.3" style="font-size:90%;"> data in the result tables. It contains speech that were not properly synthesized (hallucinations and artefacts). In parallel, we generated another synthetic version, referred to as </span><span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.4" style="font-size:90%;">Synth (F)</span><span class="ltx_text" id="S2.SS3.p1.1.5" style="font-size:90%;">, F standing for using the ASR-based filtering during the generation process, as explained also above. </span><span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.6" style="font-size:90%;">Synth (F)</span><span class="ltx_text" id="S2.SS3.p1.1.7" style="font-size:90%;"> data is expected to contain less poor speech material, if not none, and, thus, is expected to lead to better results.</span></p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold" id="S2.T1.12.1.1">Table 1</span>: </span>Results obtained on GSC test set from training a set of MatchboxNet models with real and synthetic data. Each system was trained 5 times using different random seeds, the mean and standard deviation values are presented. Literature results are also reported for reference, but they are not directly comparable since they use an additional silence class.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.8">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.8.9.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.9.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.1.1" style="font-size:90%;">System</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.9.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.2.1" style="font-size:90%;">Params.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.9.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.3.1" style="font-size:90%;">Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.8.9.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.8.9.1.4.1" style="font-size:90%;">Acc. (%)</span></td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr class="ltx_tr" id="S2.T1.8.10.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.8.10.1.1">
<span class="ltx_text" id="S2.T1.8.10.1.1.1"></span><span class="ltx_text" id="S2.T1.8.10.1.1.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.10.1.1.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.10.1.1.3.1">
<span class="ltx_tr" id="S2.T1.8.10.1.1.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.10.1.1.3.1.1.1"><span class="ltx_text" id="S2.T1.8.10.1.1.3.1.1.1.1" style="font-size:89%;">BC-ResNet-8 </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.8.10.1.1.3.1.1.1.2.1" style="font-size:89%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib21" title="">21</a><span class="ltx_text" id="S2.T1.8.10.1.1.3.1.1.1.3.2" style="font-size:89%;">]</span></cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.10.1.1.4"></span><span class="ltx_text" id="S2.T1.8.10.1.1.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.8.10.1.2">
<span class="ltx_text" id="S2.T1.8.10.1.2.1"></span><span class="ltx_text" id="S2.T1.8.10.1.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.10.1.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.10.1.2.3.1">
<span class="ltx_tr" id="S2.T1.8.10.1.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.10.1.2.3.1.1.1"><span class="ltx_text" id="S2.T1.8.10.1.2.3.1.1.1.1" style="font-size:89%;">321k</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.10.1.2.4"></span><span class="ltx_text" id="S2.T1.8.10.1.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.8.10.1.3">
<span class="ltx_text" id="S2.T1.8.10.1.3.1"></span><span class="ltx_text" id="S2.T1.8.10.1.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.10.1.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.10.1.3.3.1">
<span class="ltx_tr" id="S2.T1.8.10.1.3.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.10.1.3.3.1.1.1"><span class="ltx_text" id="S2.T1.8.10.1.3.3.1.1.1.1" style="font-size:89%;">Real</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.10.1.3.4"></span><span class="ltx_text" id="S2.T1.8.10.1.3.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.8.10.1.4">
<span class="ltx_text" id="S2.T1.8.10.1.4.1"></span><span class="ltx_text" id="S2.T1.8.10.1.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.10.1.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.10.1.4.3.1">
<span class="ltx_tr" id="S2.T1.8.10.1.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.10.1.4.3.1.1.1"><span class="ltx_text" id="S2.T1.8.10.1.4.3.1.1.1.1" style="font-size:89%;">98.6</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.10.1.4.4"></span><span class="ltx_text" id="S2.T1.8.10.1.4.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.8.11.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.8.11.2.1">
<span class="ltx_text" id="S2.T1.8.11.2.1.1"></span><span class="ltx_text" id="S2.T1.8.11.2.1.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.11.2.1.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.11.2.1.3.1">
<span class="ltx_tr" id="S2.T1.8.11.2.1.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.11.2.1.3.1.1.1"><span class="ltx_text" id="S2.T1.8.11.2.1.3.1.1.1.1" style="font-size:89%;">Emb. + head </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.8.11.2.1.3.1.1.1.2.1" style="font-size:89%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib17" title="">17</a><span class="ltx_text" id="S2.T1.8.11.2.1.3.1.1.1.3.2" style="font-size:89%;">]</span></cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.11.2.1.4"></span><span class="ltx_text" id="S2.T1.8.11.2.1.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.8.11.2.2" rowspan="2"><span class="ltx_text" id="S2.T1.8.11.2.2.1" style="font-size:90%;"><span class="ltx_text" id="S2.T1.8.11.2.2.1.1"></span> <span class="ltx_text" id="S2.T1.8.11.2.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.11.2.2.1.2.1">
<span class="ltx_tr" id="S2.T1.8.11.2.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.11.2.2.1.2.1.1.1"><span class="ltx_text" id="S2.T1.8.11.2.2.1.2.1.1.1.1" style="font-size:89%;">385k</span></span></span>
</span></span> <span class="ltx_text" id="S2.T1.8.11.2.2.1.3"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.8.11.2.3">
<span class="ltx_text" id="S2.T1.8.11.2.3.1"></span><span class="ltx_text" id="S2.T1.8.11.2.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.11.2.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.11.2.3.3.1">
<span class="ltx_tr" id="S2.T1.8.11.2.3.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.11.2.3.3.1.1.1"><span class="ltx_text" id="S2.T1.8.11.2.3.3.1.1.1.1" style="font-size:89%;">Real</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.11.2.3.4"></span><span class="ltx_text" id="S2.T1.8.11.2.3.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.8.11.2.4">
<span class="ltx_text" id="S2.T1.8.11.2.4.1"></span><span class="ltx_text" id="S2.T1.8.11.2.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.11.2.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.11.2.4.3.1">
<span class="ltx_tr" id="S2.T1.8.11.2.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.11.2.4.3.1.1.1"><span class="ltx_text" id="S2.T1.8.11.2.4.3.1.1.1.1" style="font-size:89%;">97.7</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.11.2.4.4"></span><span class="ltx_text" id="S2.T1.8.11.2.4.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.8.12.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.8.12.3.1">
<span class="ltx_text" id="S2.T1.8.12.3.1.1"></span><span class="ltx_text" id="S2.T1.8.12.3.1.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.12.3.1.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.12.3.1.3.1">
<span class="ltx_tr" id="S2.T1.8.12.3.1.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.12.3.1.3.1.1.1"><span class="ltx_text" id="S2.T1.8.12.3.1.3.1.1.1.1" style="font-size:89%;">Emb. + head </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.8.12.3.1.3.1.1.1.2.1" style="font-size:89%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib17" title="">17</a><span class="ltx_text" id="S2.T1.8.12.3.1.3.1.1.1.3.2" style="font-size:89%;">]</span></cite></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.12.3.1.4"></span><span class="ltx_text" id="S2.T1.8.12.3.1.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.8.12.3.2">
<span class="ltx_text" id="S2.T1.8.12.3.2.1"></span><span class="ltx_text" id="S2.T1.8.12.3.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.12.3.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.12.3.2.3.1">
<span class="ltx_tr" id="S2.T1.8.12.3.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.12.3.2.3.1.1.1"><span class="ltx_text" id="S2.T1.8.12.3.2.3.1.1.1.1" style="font-size:89%;">Synth.</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.12.3.2.4"></span><span class="ltx_text" id="S2.T1.8.12.3.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.8.12.3.3">
<span class="ltx_text" id="S2.T1.8.12.3.3.1"></span><span class="ltx_text" id="S2.T1.8.12.3.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.12.3.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.12.3.3.3.1">
<span class="ltx_tr" id="S2.T1.8.12.3.3.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.12.3.3.3.1.1.1"><span class="ltx_text" id="S2.T1.8.12.3.3.3.1.1.1.1" style="font-size:89%;">92.6</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.12.3.3.4"></span><span class="ltx_text" id="S2.T1.8.12.3.3.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.1.1.1" rowspan="3"><span class="ltx_text" id="S2.T1.1.1.1.1" style="font-size:90%;"><span class="ltx_text" id="S2.T1.1.1.1.1.2"></span> <span class="ltx_text" id="S2.T1.1.1.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.1.1.1.1.1.1.1">
<span class="ltx_tr" id="S2.T1.1.1.1.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.1.1.1.1.1.2.1"><span class="ltx_text" id="S2.T1.1.1.1.1.1.1.1.2.1.1" style="font-size:89%;">MatchboxNet</span></span></span>
<span class="ltx_tr" id="S2.T1.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.1.1.1.1.1.1.1.1.1"><math alttext="3\times 1\times 64" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">3</mn><mo id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.1" lspace="0.222em" mathsize="89%" rspace="0.222em" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">1</mn><mo id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.1a" lspace="0.222em" mathsize="89%" rspace="0.222em" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.4" mathsize="89%" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.4.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1"><times id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.1"></times><cn id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.2">3</cn><cn id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.3">1</cn><cn id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.4.cmml" type="integer" xref="S2.T1.1.1.1.1.1.1.1.1.1.m1.1.1.4">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1c">3\times 1\times 64</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.1.1.1.1.m1.1d">3 × 1 × 64</annotation></semantics></math></span></span>
</span></span> <span class="ltx_text" id="S2.T1.1.1.1.1.3"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.2.2.3" rowspan="3"><span class="ltx_text" id="S2.T1.2.2.3.1" style="font-size:90%;"><span class="ltx_text" id="S2.T1.2.2.3.1.1"></span> <span class="ltx_text" id="S2.T1.2.2.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.3.1.2.1">
<span class="ltx_tr" id="S2.T1.2.2.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.3.1.2.1.1.1"><span class="ltx_text" id="S2.T1.2.2.3.1.2.1.1.1.1" style="font-size:89%;">73k</span></span></span>
</span></span> <span class="ltx_text" id="S2.T1.2.2.3.1.3"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.2.2.4">
<span class="ltx_text" id="S2.T1.2.2.4.1"></span><span class="ltx_text" id="S2.T1.2.2.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.4.3.1">
<span class="ltx_tr" id="S2.T1.2.2.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.4.3.1.1.1"><span class="ltx_text" id="S2.T1.2.2.4.3.1.1.1.1" style="font-size:89%;">Real</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.4.4"></span><span class="ltx_text" id="S2.T1.2.2.4.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.2.2.2">
<span class="ltx_text" id="S2.T1.2.2.2.2"></span><span class="ltx_text" id="S2.T1.2.2.2.3" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.2.2.2.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.2.2.2.1.1.1">
<span class="ltx_tr" id="S2.T1.2.2.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.2.2.2.1.1.1.1.1"><math alttext="98.37\pm 0.08" class="ltx_Math" display="inline" id="S2.T1.2.2.2.1.1.1.1.1.m1.1"><semantics id="S2.T1.2.2.2.1.1.1.1.1.m1.1a"><mrow id="S2.T1.2.2.2.1.1.1.1.1.m1.1.1" xref="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.2.cmml">98.37</mn><mo id="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.3.cmml">0.08</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.1.1.1.1.m1.1b"><apply id="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.2">98.37</cn><cn id="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S2.T1.2.2.2.1.1.1.1.1.m1.1.1.3">0.08</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.1.1.1.1.m1.1c">98.37\pm 0.08</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.1.1.1.1.1.m1.1d">98.37 ± 0.08</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S2.T1.2.2.2.4"></span><span class="ltx_text" id="S2.T1.2.2.2.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.3.3.2">
<span class="ltx_text" id="S2.T1.3.3.2.1"></span><span class="ltx_text" id="S2.T1.3.3.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.3.3.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.3.3.2.3.1">
<span class="ltx_tr" id="S2.T1.3.3.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.3.3.2.3.1.1.1"><span class="ltx_text" id="S2.T1.3.3.2.3.1.1.1.1" style="font-size:89%;">Synth.</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.3.3.2.4"></span><span class="ltx_text" id="S2.T1.3.3.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.3.3.1">
<span class="ltx_text" id="S2.T1.3.3.1.2"></span><span class="ltx_text" id="S2.T1.3.3.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.3.3.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.3.3.1.1.1.1">
<span class="ltx_tr" id="S2.T1.3.3.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.3.3.1.1.1.1.1.1"><math alttext="89.29\pm 0.49" class="ltx_Math" display="inline" id="S2.T1.3.3.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.3.3.1.1.1.1.1.1.m1.1a"><mrow id="S2.T1.3.3.1.1.1.1.1.1.m1.1.1" xref="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.2.cmml">89.29</mn><mo id="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.3.cmml">0.49</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.3.3.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.2">89.29</cn><cn id="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S2.T1.3.3.1.1.1.1.1.1.m1.1.1.3">0.49</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.1.1.1.1.1.1.m1.1c">89.29\pm 0.49</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.1.1.1.1.1.1.m1.1d">89.29 ± 0.49</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S2.T1.3.3.1.4"></span><span class="ltx_text" id="S2.T1.3.3.1.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.4.4.2">
<span class="ltx_text" id="S2.T1.4.4.2.1"></span><span class="ltx_text" id="S2.T1.4.4.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.4.4.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.4.4.2.3.1">
<span class="ltx_tr" id="S2.T1.4.4.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.4.4.2.3.1.1.1"><span class="ltx_text" id="S2.T1.4.4.2.3.1.1.1.1" style="font-size:89%;">Synth. (F)</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.4.4.2.4"></span><span class="ltx_text" id="S2.T1.4.4.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.4.4.1">
<span class="ltx_text" id="S2.T1.4.4.1.2"></span><span class="ltx_text" id="S2.T1.4.4.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.4.4.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.4.4.1.1.1.1">
<span class="ltx_tr" id="S2.T1.4.4.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.4.4.1.1.1.1.1.1"><math alttext="92.06\pm 0.35" class="ltx_Math" display="inline" id="S2.T1.4.4.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.4.4.1.1.1.1.1.1.m1.1a"><mrow id="S2.T1.4.4.1.1.1.1.1.1.m1.1.1" xref="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.2.cmml">92.06</mn><mo id="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.3.cmml">0.35</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.4.4.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.2">92.06</cn><cn id="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S2.T1.4.4.1.1.1.1.1.1.m1.1.1.3">0.35</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.1.1.1.1.1.1.m1.1c">92.06\pm 0.35</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.1.1.1.1.1.1.m1.1d">92.06 ± 0.35</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S2.T1.4.4.1.4"></span><span class="ltx_text" id="S2.T1.4.4.1.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_b ltx_border_t" id="S2.T1.5.5.1" rowspan="3"><span class="ltx_text" id="S2.T1.5.5.1.1" style="font-size:90%;"><span class="ltx_text" id="S2.T1.5.5.1.1.2"></span> <span class="ltx_text" id="S2.T1.5.5.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.5.1.1.1.1.1">
<span class="ltx_tr" id="S2.T1.5.5.1.1.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.5.1.1.1.1.1.2.1"><span class="ltx_text" id="S2.T1.5.5.1.1.1.1.1.2.1.1" style="font-size:89%;">MatchboxNet</span></span></span>
<span class="ltx_tr" id="S2.T1.5.5.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.5.1.1.1.1.1.1.1"><math alttext="6\times 2\times 64" class="ltx_Math" display="inline" id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1a"><mrow id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.2.cmml">6</mn><mo id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.1" lspace="0.222em" mathsize="89%" rspace="0.222em" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.3.cmml">2</mn><mo id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.1a" lspace="0.222em" mathsize="89%" rspace="0.222em" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.1.cmml">×</mo><mn id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.4" mathsize="89%" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.4.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1"><times id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.1"></times><cn id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="integer" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.2">6</cn><cn id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.3.cmml" type="integer" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.3">2</cn><cn id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.4.cmml" type="integer" xref="S2.T1.5.5.1.1.1.1.1.1.1.m1.1.1.4">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1c">6\times 2\times 64</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.1.1.1.1.1.1.1.m1.1d">6 × 2 × 64</annotation></semantics></math></span></span>
</span></span> <span class="ltx_text" id="S2.T1.5.5.1.1.3"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_b ltx_border_t" id="S2.T1.6.6.3" rowspan="4"><span class="ltx_text" id="S2.T1.6.6.3.1" style="font-size:90%;"><span class="ltx_text" id="S2.T1.6.6.3.1.1"></span> <span class="ltx_text" id="S2.T1.6.6.3.1.2">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.6.6.3.1.2.1">
<span class="ltx_tr" id="S2.T1.6.6.3.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.6.6.3.1.2.1.1.1"><span class="ltx_text" id="S2.T1.6.6.3.1.2.1.1.1.1" style="font-size:89%;">134k</span></span></span>
</span></span> <span class="ltx_text" id="S2.T1.6.6.3.1.3"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.6.6.4">
<span class="ltx_text" id="S2.T1.6.6.4.1"></span><span class="ltx_text" id="S2.T1.6.6.4.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.6.6.4.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.6.6.4.3.1">
<span class="ltx_tr" id="S2.T1.6.6.4.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.6.6.4.3.1.1.1"><span class="ltx_text" id="S2.T1.6.6.4.3.1.1.1.1" style="font-size:89%;">Real</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.6.6.4.4"></span><span class="ltx_text" id="S2.T1.6.6.4.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S2.T1.6.6.2">
<span class="ltx_text" id="S2.T1.6.6.2.2"></span><span class="ltx_text" id="S2.T1.6.6.2.3" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.6.6.2.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.6.6.2.1.1.1">
<span class="ltx_tr" id="S2.T1.6.6.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.6.6.2.1.1.1.1.1"><math alttext="98.49\pm 0.11" class="ltx_Math" display="inline" id="S2.T1.6.6.2.1.1.1.1.1.m1.1"><semantics id="S2.T1.6.6.2.1.1.1.1.1.m1.1a"><mrow id="S2.T1.6.6.2.1.1.1.1.1.m1.1.1" xref="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.2.cmml">98.49</mn><mo id="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.3.cmml">0.11</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.2.1.1.1.1.1.m1.1b"><apply id="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.6.6.2.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.2">98.49</cn><cn id="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S2.T1.6.6.2.1.1.1.1.1.m1.1.1.3">0.11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.2.1.1.1.1.1.m1.1c">98.49\pm 0.11</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.6.2.1.1.1.1.1.m1.1d">98.49 ± 0.11</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S2.T1.6.6.2.4"></span><span class="ltx_text" id="S2.T1.6.6.2.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.7.7.2">
<span class="ltx_text" id="S2.T1.7.7.2.1"></span><span class="ltx_text" id="S2.T1.7.7.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.7.7.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.7.7.2.3.1">
<span class="ltx_tr" id="S2.T1.7.7.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.7.7.2.3.1.1.1"><span class="ltx_text" id="S2.T1.7.7.2.3.1.1.1.1" style="font-size:89%;">Synth.</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.7.7.2.4"></span><span class="ltx_text" id="S2.T1.7.7.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S2.T1.7.7.1">
<span class="ltx_text" id="S2.T1.7.7.1.2"></span><span class="ltx_text" id="S2.T1.7.7.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.7.7.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.7.7.1.1.1.1">
<span class="ltx_tr" id="S2.T1.7.7.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.7.7.1.1.1.1.1.1"><math alttext="90.02\pm 0.43" class="ltx_Math" display="inline" id="S2.T1.7.7.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.7.7.1.1.1.1.1.1.m1.1a"><mrow id="S2.T1.7.7.1.1.1.1.1.1.m1.1.1" xref="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.2.cmml">90.02</mn><mo id="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.3.cmml">0.43</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.7.7.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.2">90.02</cn><cn id="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S2.T1.7.7.1.1.1.1.1.1.m1.1.1.3">0.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.1.1.1.1.1.1.m1.1c">90.02\pm 0.43</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.7.1.1.1.1.1.1.m1.1d">90.02 ± 0.43</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S2.T1.7.7.1.4"></span><span class="ltx_text" id="S2.T1.7.7.1.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S2.T1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_b" id="S2.T1.8.8.2">
<span class="ltx_text" id="S2.T1.8.8.2.1"></span><span class="ltx_text" id="S2.T1.8.8.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S2.T1.8.8.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.8.2.3.1">
<span class="ltx_tr" id="S2.T1.8.8.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.8.2.3.1.1.1"><span class="ltx_text" id="S2.T1.8.8.2.3.1.1.1.1" style="font-size:89%;">Synth. (F)</span></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.8.2.4"></span><span class="ltx_text" id="S2.T1.8.8.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_b" id="S2.T1.8.8.1">
<span class="ltx_text" id="S2.T1.8.8.1.2"></span><span class="ltx_text ltx_font_bold" id="S2.T1.8.8.1.1" style="font-size:90%;"> <span class="ltx_text" id="S2.T1.8.8.1.1.1">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.8.8.1.1.1.1.1">
<span class="ltx_tr" id="S2.T1.8.8.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.8.8.1.1.1.1.1.1.1"><math alttext="92.57\pm 0.58" class="ltx_Math" display="inline" id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1a"><mrow id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.2.cmml">92.57</mn><mo id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.3.cmml">0.58</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1b"><apply id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.2">92.57</cn><cn id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S2.T1.8.8.1.1.1.1.1.1.1.m1.1.1.3">0.58</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1c">92.57\pm 0.58</annotation><annotation encoding="application/x-llamapun" id="S2.T1.8.8.1.1.1.1.1.1.1.m1.1d">92.57 ± 0.58</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S2.T1.8.8.1.1.2"></span></span>
</th>
</tr>
</tfoot>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>First SCC experiments</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Baseline SCC model: MatchboxNet</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3"><span class="ltx_text" id="S3.SS1.p1.3.1" style="font-size:90%;">A MatchboxNet model </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS1.p1.3.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib22" title="">22</a><span class="ltx_text" id="S3.SS1.p1.3.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS1.p1.3.4" style="font-size:90%;"> is a convolutional lightweight architecture specifically targeted for keyword spotting, command classification and wake-word detection. The architecture can easily be scaled in order to accommodate larger amounts of parameters under the form </span><math alttext="B\times R\times C" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" mathsize="90%" xref="S3.SS1.p1.1.m1.1.1.2.cmml">B</mi><mo id="S3.SS1.p1.1.m1.1.1.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.3" mathsize="90%" xref="S3.SS1.p1.1.m1.1.1.3.cmml">R</mi><mo id="S3.SS1.p1.1.m1.1.1.1a" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.1.m1.1.1.1.cmml">×</mo><mi id="S3.SS1.p1.1.m1.1.1.4" mathsize="90%" xref="S3.SS1.p1.1.m1.1.1.4.cmml">C</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝐵</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">𝑅</ci><ci id="S3.SS1.p1.1.m1.1.1.4.cmml" xref="S3.SS1.p1.1.m1.1.1.4">𝐶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">B\times R\times C</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_B × italic_R × italic_C</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.3.5" style="font-size:90%;"> , where B stands for main quantity of convolutional blocks, R stands for sub-blocks (time-channel separable convolutional sub-blocks) and C for the amount of channels inside each block. In the present study, two different configurations of Matchboxnet models were used: </span><math alttext="3\times 1\times 64" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mrow id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mn id="S3.SS1.p1.2.m2.1.1.2" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.2.cmml">3</mn><mo id="S3.SS1.p1.2.m2.1.1.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.3" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.3.cmml">1</mn><mo id="S3.SS1.p1.2.m2.1.1.1a" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.2.m2.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.2.m2.1.1.4" mathsize="90%" xref="S3.SS1.p1.2.m2.1.1.4.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><times id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1"></times><cn id="S3.SS1.p1.2.m2.1.1.2.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1.2">3</cn><cn id="S3.SS1.p1.2.m2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1.3">1</cn><cn id="S3.SS1.p1.2.m2.1.1.4.cmml" type="integer" xref="S3.SS1.p1.2.m2.1.1.4">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">3\times 1\times 64</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">3 × 1 × 64</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.3.6" style="font-size:90%;"> and </span><math alttext="6\times 2\times 64" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mn id="S3.SS1.p1.3.m3.1.1.2" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.2.cmml">6</mn><mo id="S3.SS1.p1.3.m3.1.1.1" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.3.m3.1.1.3" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.3.cmml">2</mn><mo id="S3.SS1.p1.3.m3.1.1.1a" lspace="0.222em" mathsize="90%" rspace="0.222em" xref="S3.SS1.p1.3.m3.1.1.1.cmml">×</mo><mn id="S3.SS1.p1.3.m3.1.1.4" mathsize="90%" xref="S3.SS1.p1.3.m3.1.1.4.cmml">64</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><times id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></times><cn id="S3.SS1.p1.3.m3.1.1.2.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.2">6</cn><cn id="S3.SS1.p1.3.m3.1.1.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.3">2</cn><cn id="S3.SS1.p1.3.m3.1.1.4.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.4">64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">6\times 2\times 64</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">6 × 2 × 64</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.3.7" style="font-size:90%;">. These models can easily be adapted to a variety of domains and present state-of-the-art results on the GSC test set.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Fully Synthetic SCC</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text" id="S3.SS2.p1.1.1" style="font-size:90%;">Matchboxnet models were trained on three different subsets: either the original speech commands corpus (</span><span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2" style="font-size:90%;">Real</span><span class="ltx_text" id="S3.SS2.p1.1.3" style="font-size:90%;">), or the synthetic version (</span><span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.4" style="font-size:90%;">Synth</span><span class="ltx_text" id="S3.SS2.p1.1.5" style="font-size:90%;">), or the synthetic version generated with ASR-based filtering (</span><span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.6" style="font-size:90%;">Synth. (F)</span><span class="ltx_text" id="S3.SS2.p1.1.7" style="font-size:90%;">). A total of 50 training epochs with early stopping, a batch size of 128, a dropout of 0.25 and a cosine annealing learning scheduler, starting at 5e-3 and finishing at 5e-12, were used as main training hyperparameters. At each epoch, our models were validated using the real GSC validation set. Furthermore, all our models were similarly tested on the original test set. Accuracy values are reported in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S2.T1" style="font-size:90%;" title="Table 1 ‣ 2.3 Two synthetic GSC versions: Synth and Synth (F) ‣ 2 Synthetic spoken commands generation ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S3.SS2.p1.1.8" style="font-size:90%;">. As expected, the best results were obtained with the original GSC training data (</span><span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.9" style="font-size:90%;">Real</span><span class="ltx_text" id="S3.SS2.p1.1.10" style="font-size:90%;">), with accuracy values above 98%. Results obtained in the </span><span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.11" style="font-size:90%;">Synth.</span><span class="ltx_text" id="S3.SS2.p1.1.12" style="font-size:90%;"> condition, using synthetic data for training, achieved accuracy values of 89-90%. The ASR-based filtering technique led to an accuracy gain of over two percentage points on MatchboxNet models, demonstrating that uncontrolled hallucinations in synthetic training data can contaminate the dataset and degrade model performance.
A significant gap still remains between training high-performing models with real, original, speech data and purely synthetic data, generated with XTTS v2 and voice cloning.
We may compare these results to other approaches in the literature using synthetic data, namely </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS2.p1.1.13.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib17" title="">17</a><span class="ltx_text" id="S3.SS2.p1.1.14.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS2.p1.1.15" style="font-size:90%;">, with the limitation that they used the additional silence class, while we did not. Nevertheless, while the accuracy results remain quite similar ( 92.6%), our largest Matchboxnet model (134 k) is roughly one third smaller than their model sizes, in number of parameters.</span></p>
</div>
<figure class="ltx_figure" id="S3.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="319" id="S3.F2.1.g1" src="extracted/5863711/PCA_MFCC_SYN_BEST.png" width="598"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S3.F2.2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="319" id="S3.F2.2.g1" src="extracted/5863711/PCA_WAVLM_SYN_BEST_changed.png" width="598"/>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S3.F2.6.1.1">Fig. 2</span>: </span>PCA analysis of filtered synthetic data and real speech data using Mel Frequency Cesptral Coefficients (MFCC) features (left) and WavLM self-supervised feature (right).</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>SSL-based SCC Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text" id="S4.p1.1.1" style="font-size:90%;">Despite the relatively high base accuracy of 92.57% witnessed with our approach, there is still a gap of roughly 6% between systems trained on real </span><span class="ltx_text ltx_font_italic" id="S4.p1.1.2" style="font-size:90%;">vs.</span><span class="ltx_text" id="S4.p1.1.3" style="font-size:90%;"> synthetic data. As an attempt to reduce this gap, we explored models based on SSL speech representations, trained with our synthetic data. Indeed, SSL benchmarking results already displayed strong results, in particular on GSC (v1) </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.p1.1.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib23" title="">23</a><span class="ltx_text" id="S4.p1.1.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.p1.1.6" style="font-size:90%;">.</span></p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Results with WavLM-Base-Plus representations</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1"><span class="ltx_text" id="S4.SS1.p1.1.1" style="font-size:90%;">We used WavLM-Base-Plus to extract 768-d speech feature representations, from its twelfth transformer layer </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib24" title="">24</a><span class="ltx_text" id="S4.SS1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.p1.1.4" style="font-size:90%;">. Each audio excerpt is, in fact, represented by a sequence of such features, but we reduced the sequences into single 768-d vectors using statistic pooling, a method that computes the mean and standard deviation by feature dimension </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib25" title="">25</a><span class="ltx_text" id="S4.SS1.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.p1.1.7" style="font-size:90%;">. A single linear layer is used as classification head for our task.</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text" id="S4.SS1.p2.1.1" style="font-size:90%;">We trained the classification layer (WavLM-Base-Plus was frozen) over 30 epochs with a fixed learning rate of 5e-3 and a batch size of 128. As we did with the MatchboxNet models, we compare the three scenarios, using either the original GSC speech training data, or synthetic speech, without and with ASR-based filtering. The results, reported in table </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.T2" style="font-size:90%;" title="Table 2 ‣ 4.3 CycleGAN Domain Adaptation ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S4.SS1.p2.1.2" style="font-size:90%;">, show that the performance gap between training a model either on real data or on the filtered synthetic speech has been reduced, compared to the previous experiments using MatchboxNet and MFCCs: 96.11% (</span><span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.3" style="font-size:90%;">Synth.(F)</span><span class="ltx_text" id="S4.SS1.p2.1.4" style="font-size:90%;">) </span><span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.5" style="font-size:90%;">vs.</span><span class="ltx_text" id="S4.SS1.p2.1.6" style="font-size:90%;"> 98.03% (</span><span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.7" style="font-size:90%;">Real</span><span class="ltx_text" id="S4.SS1.p2.1.8" style="font-size:90%;">). The </span><span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.9" style="font-size:90%;">Synth.(F)</span><span class="ltx_text" id="S4.SS1.p2.1.10" style="font-size:90%;"> SSL-based linear classifier is also much better that its MatchboxNet counterpart, with an absolute gain of 3.5% in accuracy. Interestingly, the results obtained when using the synthetic data without filtering (</span><span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.11" style="font-size:90%;">Synth.</span><span class="ltx_text" id="S4.SS1.p2.1.12" style="font-size:90%;">) displayed a subpar accuracy of around 83%, showing that hallucinated speech has a negative impact stronger when using SSL features compared to MFCCs.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>PCA Visualizations</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="font-size:90%;">In order to get some insights between GSC real speech and our synthesized speech material, we visualize, in Fig. </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S3.F2" style="font-size:90%;" title="Figure 2 ‣ 3.2 Fully Synthetic SCC ‣ 3 First SCC experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S4.SS2.p1.1.2" style="font-size:90%;">), a 2-d dimension reduction through PCA over two sets of features: i) the 64 Mel-Frequency Cepstral Coefficients (MFCCs), as used by the MatchboxNet models, ii) the 768-d WavLM-Base-Plus feature vector representations, </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib24" title="">24</a><span class="ltx_text" id="S4.SS2.p1.1.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.5" style="font-size:90%;">. In both cases, statistic pooling over the time dimension was used to reduce sequences into single vectors. The MFCC plot (left plot) shows two superposed clusters, hinting that real speech is less clustered and more scattered than real speech. Surprisingly, with the SSL features, the two data points are much less superposed (right plot). They seem to even be linearly separable, showing that SSL features, even reduced to two dimensions, could be reliably used to differentiate between real and synthetic speech material. Given the impressive results witnessed when using SSL features on different tasks such as speech classification and KWS </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.6.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib23" title="">23</a><span class="ltx_text" id="S4.SS2.p1.1.7.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.8" style="font-size:90%;">, this clear separation in the two dimensional PCA space comes as unexpected. We draw the hypothesis that domain adaptation between the two types of speech material, in the SSL latent space, could help reduce the remaining performance gap in our experiments.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>CycleGAN Domain Adaptation</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text" id="S4.SS3.p1.1.1" style="font-size:90%;">CycleGAN models </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib26" title="">26</a><span class="ltx_text" id="S4.SS3.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS3.p1.1.4" style="font-size:90%;"> have been widely used to perform domain adaptation, in particular due to the increased training stability, compared to simpler GANs. They are a generative approach to convert inputs from one domain to another, and then convert them back to the original domain in a cyclic fashion. Our objective is to make closer in distribution the two domains that are the synthetic and real speech representations, in the 768-d SSL space.</span></p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.4"><span class="ltx_text" id="S4.SS3.p2.4.1" style="font-size:90%;">Our proposed CycleGAN, illustrated in Fig. </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.F3" style="font-size:90%;" title="Figure 3 ‣ 4.3 CycleGAN Domain Adaptation ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S4.SS3.p2.4.2" style="font-size:90%;">, contains two generators (</span><math alttext="G_{A},G_{B}" class="ltx_Math" display="inline" id="S4.SS3.p2.1.m1.2"><semantics id="S4.SS3.p2.1.m1.2a"><mrow id="S4.SS3.p2.1.m1.2.2.2" xref="S4.SS3.p2.1.m1.2.2.3.cmml"><msub id="S4.SS3.p2.1.m1.1.1.1.1" xref="S4.SS3.p2.1.m1.1.1.1.1.cmml"><mi id="S4.SS3.p2.1.m1.1.1.1.1.2" mathsize="90%" xref="S4.SS3.p2.1.m1.1.1.1.1.2.cmml">G</mi><mi id="S4.SS3.p2.1.m1.1.1.1.1.3" mathsize="90%" xref="S4.SS3.p2.1.m1.1.1.1.1.3.cmml">A</mi></msub><mo id="S4.SS3.p2.1.m1.2.2.2.3" mathsize="90%" xref="S4.SS3.p2.1.m1.2.2.3.cmml">,</mo><msub id="S4.SS3.p2.1.m1.2.2.2.2" xref="S4.SS3.p2.1.m1.2.2.2.2.cmml"><mi id="S4.SS3.p2.1.m1.2.2.2.2.2" mathsize="90%" xref="S4.SS3.p2.1.m1.2.2.2.2.2.cmml">G</mi><mi id="S4.SS3.p2.1.m1.2.2.2.2.3" mathsize="90%" xref="S4.SS3.p2.1.m1.2.2.2.2.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.1.m1.2b"><list id="S4.SS3.p2.1.m1.2.2.3.cmml" xref="S4.SS3.p2.1.m1.2.2.2"><apply id="S4.SS3.p2.1.m1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p2.1.m1.1.1.1.1.2.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.2">𝐺</ci><ci id="S4.SS3.p2.1.m1.1.1.1.1.3.cmml" xref="S4.SS3.p2.1.m1.1.1.1.1.3">𝐴</ci></apply><apply id="S4.SS3.p2.1.m1.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.1.m1.2.2.2.2.1.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.SS3.p2.1.m1.2.2.2.2.2.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.2">𝐺</ci><ci id="S4.SS3.p2.1.m1.2.2.2.2.3.cmml" xref="S4.SS3.p2.1.m1.2.2.2.2.3">𝐵</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.1.m1.2c">G_{A},G_{B}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.1.m1.2d">italic_G start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , italic_G start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS3.p2.4.3" style="font-size:90%;">) and two discriminators (</span><math alttext="D_{A},D_{B}" class="ltx_Math" display="inline" id="S4.SS3.p2.2.m2.2"><semantics id="S4.SS3.p2.2.m2.2a"><mrow id="S4.SS3.p2.2.m2.2.2.2" xref="S4.SS3.p2.2.m2.2.2.3.cmml"><msub id="S4.SS3.p2.2.m2.1.1.1.1" xref="S4.SS3.p2.2.m2.1.1.1.1.cmml"><mi id="S4.SS3.p2.2.m2.1.1.1.1.2" mathsize="90%" xref="S4.SS3.p2.2.m2.1.1.1.1.2.cmml">D</mi><mi id="S4.SS3.p2.2.m2.1.1.1.1.3" mathsize="90%" xref="S4.SS3.p2.2.m2.1.1.1.1.3.cmml">A</mi></msub><mo id="S4.SS3.p2.2.m2.2.2.2.3" mathsize="90%" xref="S4.SS3.p2.2.m2.2.2.3.cmml">,</mo><msub id="S4.SS3.p2.2.m2.2.2.2.2" xref="S4.SS3.p2.2.m2.2.2.2.2.cmml"><mi id="S4.SS3.p2.2.m2.2.2.2.2.2" mathsize="90%" xref="S4.SS3.p2.2.m2.2.2.2.2.2.cmml">D</mi><mi id="S4.SS3.p2.2.m2.2.2.2.2.3" mathsize="90%" xref="S4.SS3.p2.2.m2.2.2.2.2.3.cmml">B</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.2.m2.2b"><list id="S4.SS3.p2.2.m2.2.2.3.cmml" xref="S4.SS3.p2.2.m2.2.2.2"><apply id="S4.SS3.p2.2.m2.1.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.1.1.1.1.1.cmml" xref="S4.SS3.p2.2.m2.1.1.1.1">subscript</csymbol><ci id="S4.SS3.p2.2.m2.1.1.1.1.2.cmml" xref="S4.SS3.p2.2.m2.1.1.1.1.2">𝐷</ci><ci id="S4.SS3.p2.2.m2.1.1.1.1.3.cmml" xref="S4.SS3.p2.2.m2.1.1.1.1.3">𝐴</ci></apply><apply id="S4.SS3.p2.2.m2.2.2.2.2.cmml" xref="S4.SS3.p2.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS3.p2.2.m2.2.2.2.2.1.cmml" xref="S4.SS3.p2.2.m2.2.2.2.2">subscript</csymbol><ci id="S4.SS3.p2.2.m2.2.2.2.2.2.cmml" xref="S4.SS3.p2.2.m2.2.2.2.2.2">𝐷</ci><ci id="S4.SS3.p2.2.m2.2.2.2.2.3.cmml" xref="S4.SS3.p2.2.m2.2.2.2.2.3">𝐵</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.2.m2.2c">D_{A},D_{B}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.2.m2.2d">italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS3.p2.4.4" style="font-size:90%;">). The first generator aims to convert inputs from a synthetic domain to a real domain, and the second one the opposite. The two discriminators aim to differentiate between either real and generated real (</span><math alttext="D_{A}" class="ltx_Math" display="inline" id="S4.SS3.p2.3.m3.1"><semantics id="S4.SS3.p2.3.m3.1a"><msub id="S4.SS3.p2.3.m3.1.1" xref="S4.SS3.p2.3.m3.1.1.cmml"><mi id="S4.SS3.p2.3.m3.1.1.2" mathsize="90%" xref="S4.SS3.p2.3.m3.1.1.2.cmml">D</mi><mi id="S4.SS3.p2.3.m3.1.1.3" mathsize="90%" xref="S4.SS3.p2.3.m3.1.1.3.cmml">A</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.3.m3.1b"><apply id="S4.SS3.p2.3.m3.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.3.m3.1.1.1.cmml" xref="S4.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.p2.3.m3.1.1.2.cmml" xref="S4.SS3.p2.3.m3.1.1.2">𝐷</ci><ci id="S4.SS3.p2.3.m3.1.1.3.cmml" xref="S4.SS3.p2.3.m3.1.1.3">𝐴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.3.m3.1c">D_{A}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS3.p2.4.5" style="font-size:90%;">) or synthetic and reconstructed synthetic (</span><math alttext="D_{B}" class="ltx_Math" display="inline" id="S4.SS3.p2.4.m4.1"><semantics id="S4.SS3.p2.4.m4.1a"><msub id="S4.SS3.p2.4.m4.1.1" xref="S4.SS3.p2.4.m4.1.1.cmml"><mi id="S4.SS3.p2.4.m4.1.1.2" mathsize="90%" xref="S4.SS3.p2.4.m4.1.1.2.cmml">D</mi><mi id="S4.SS3.p2.4.m4.1.1.3" mathsize="90%" xref="S4.SS3.p2.4.m4.1.1.3.cmml">B</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p2.4.m4.1b"><apply id="S4.SS3.p2.4.m4.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS3.p2.4.m4.1.1.1.cmml" xref="S4.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S4.SS3.p2.4.m4.1.1.2.cmml" xref="S4.SS3.p2.4.m4.1.1.2">𝐷</ci><ci id="S4.SS3.p2.4.m4.1.1.3.cmml" xref="S4.SS3.p2.4.m4.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p2.4.m4.1c">D_{B}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p2.4.m4.1d">italic_D start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS3.p2.4.6" style="font-size:90%;">). Each generator contains three linear layers of dimensions [1536x512], [512x512] and [512x1536] respectively, with an intermediary ReLU non-linearity and a final hyperbolic tangent as activation function. The discriminators share a similar architecture, with the only difference being in the final linear layer, adjusted to have dimensions of [512x1], and a sigmoid activation function.</span></p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="185" id="S4.F3.g1" src="extracted/5863711/CycleGAN_paper5.png" width="329"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S4.F3.4.1.1">Fig. 3</span>: </span>Proposed CycleGAN architecture and domain adaptation for a WavLM based SCC system.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text" id="S4.SS3.p3.1.1" style="font-size:90%;">The CycleGAN is optimized using a cycle-consistency loss function </span><math alttext="\mathcal{L}_{CC}" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><msub id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS3.p3.1.m1.1.1.2" mathsize="90%" xref="S4.SS3.p3.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml"><mi id="S4.SS3.p3.1.m1.1.1.3.2" mathsize="90%" xref="S4.SS3.p3.1.m1.1.1.3.2.cmml">C</mi><mo id="S4.SS3.p3.1.m1.1.1.3.1" xref="S4.SS3.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.p3.1.m1.1.1.3.3" mathsize="90%" xref="S4.SS3.p3.1.m1.1.1.3.3.cmml">C</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">ℒ</ci><apply id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3"><times id="S4.SS3.p3.1.m1.1.1.3.1.cmml" xref="S4.SS3.p3.1.m1.1.1.3.1"></times><ci id="S4.SS3.p3.1.m1.1.1.3.2.cmml" xref="S4.SS3.p3.1.m1.1.1.3.2">𝐶</ci><ci id="S4.SS3.p3.1.m1.1.1.3.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\mathcal{L}_{CC}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_C italic_C end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS3.p3.1.2" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS3.p3.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib26" title="">26</a><span class="ltx_text" id="S4.SS3.p3.1.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS3.p3.1.5" style="font-size:90%;"> (see Eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.E1" style="font-size:90%;" title="In 4.3 CycleGAN Domain Adaptation ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S4.SS3.p3.1.6" style="font-size:90%;">) that combines the discriminator and generator loss functions (MSE loss function, see equation </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.E2" style="font-size:90%;" title="In 4.3 CycleGAN Domain Adaptation ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S4.SS3.p3.1.7" style="font-size:90%;">) and a cyclic and identity loss with their respective weights. The cycle loss calculates the full reconstruction loss of the same input, enforcing consistency (eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.E3" style="font-size:90%;" title="In 4.3 CycleGAN Domain Adaptation ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S4.SS3.p3.1.8" style="font-size:90%;">). Contrarily, the identity loss encourages the generator to produce outputs that are close to the inputs when the inputs are already in the target domain (Eq. </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.E4" style="font-size:90%;" title="In 4.3 CycleGAN Domain Adaptation ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">4</span></a><span class="ltx_text" id="S4.SS3.p3.1.9" style="font-size:90%;">).</span></p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<table class="ltx_equation ltx_eqn_table" id="S4.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{\mathcal{L}_{CC}}=\mathcal{L}_{gan}^{A}+\mathcal{L}_{gan}^{B}+\lambda_{c}(%
\mathcal{L}_{c}^{A}+\mathcal{L}_{c}^{B})+\lambda_{id}(\mathcal{L}_{id}^{A}+%
\mathcal{L}_{id}^{B})" class="ltx_Math" display="block" id="S4.E1.m1.2"><semantics id="S4.E1.m1.2a"><mrow id="S4.E1.m1.2.2" xref="S4.E1.m1.2.2.cmml"><msub id="S4.E1.m1.2.2.4" xref="S4.E1.m1.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.4.2" mathsize="90%" xref="S4.E1.m1.2.2.4.2.cmml">𝓛</mi><mrow id="S4.E1.m1.2.2.4.3" xref="S4.E1.m1.2.2.4.3.cmml"><mi id="S4.E1.m1.2.2.4.3.2" mathsize="90%" xref="S4.E1.m1.2.2.4.3.2.cmml">𝑪</mi><mo id="S4.E1.m1.2.2.4.3.1" xref="S4.E1.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.4.3.3" mathsize="90%" xref="S4.E1.m1.2.2.4.3.3.cmml">𝑪</mi></mrow></msub><mo id="S4.E1.m1.2.2.3" mathsize="90%" xref="S4.E1.m1.2.2.3.cmml">=</mo><mrow id="S4.E1.m1.2.2.2" xref="S4.E1.m1.2.2.2.cmml"><msubsup id="S4.E1.m1.2.2.2.4" xref="S4.E1.m1.2.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.2.4.2.2" mathsize="90%" xref="S4.E1.m1.2.2.2.4.2.2.cmml">ℒ</mi><mrow id="S4.E1.m1.2.2.2.4.2.3" xref="S4.E1.m1.2.2.2.4.2.3.cmml"><mi id="S4.E1.m1.2.2.2.4.2.3.2" mathsize="90%" xref="S4.E1.m1.2.2.2.4.2.3.2.cmml">g</mi><mo id="S4.E1.m1.2.2.2.4.2.3.1" xref="S4.E1.m1.2.2.2.4.2.3.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.2.4.2.3.3" mathsize="90%" xref="S4.E1.m1.2.2.2.4.2.3.3.cmml">a</mi><mo id="S4.E1.m1.2.2.2.4.2.3.1a" xref="S4.E1.m1.2.2.2.4.2.3.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.2.4.2.3.4" mathsize="90%" xref="S4.E1.m1.2.2.2.4.2.3.4.cmml">n</mi></mrow><mi id="S4.E1.m1.2.2.2.4.3" mathsize="90%" xref="S4.E1.m1.2.2.2.4.3.cmml">A</mi></msubsup><mo id="S4.E1.m1.2.2.2.3" mathsize="90%" xref="S4.E1.m1.2.2.2.3.cmml">+</mo><msubsup id="S4.E1.m1.2.2.2.5" xref="S4.E1.m1.2.2.2.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.2.5.2.2" mathsize="90%" xref="S4.E1.m1.2.2.2.5.2.2.cmml">ℒ</mi><mrow id="S4.E1.m1.2.2.2.5.2.3" xref="S4.E1.m1.2.2.2.5.2.3.cmml"><mi id="S4.E1.m1.2.2.2.5.2.3.2" mathsize="90%" xref="S4.E1.m1.2.2.2.5.2.3.2.cmml">g</mi><mo id="S4.E1.m1.2.2.2.5.2.3.1" xref="S4.E1.m1.2.2.2.5.2.3.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.2.5.2.3.3" mathsize="90%" xref="S4.E1.m1.2.2.2.5.2.3.3.cmml">a</mi><mo id="S4.E1.m1.2.2.2.5.2.3.1a" xref="S4.E1.m1.2.2.2.5.2.3.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.2.5.2.3.4" mathsize="90%" xref="S4.E1.m1.2.2.2.5.2.3.4.cmml">n</mi></mrow><mi id="S4.E1.m1.2.2.2.5.3" mathsize="90%" xref="S4.E1.m1.2.2.2.5.3.cmml">B</mi></msubsup><mo id="S4.E1.m1.2.2.2.3a" mathsize="90%" xref="S4.E1.m1.2.2.2.3.cmml">+</mo><mrow id="S4.E1.m1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.cmml"><msub id="S4.E1.m1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.3.cmml"><mi id="S4.E1.m1.1.1.1.1.3.2" mathsize="90%" xref="S4.E1.m1.1.1.1.1.3.2.cmml">λ</mi><mi id="S4.E1.m1.1.1.1.1.3.3" mathsize="90%" xref="S4.E1.m1.1.1.1.1.3.3.cmml">c</mi></msub><mo id="S4.E1.m1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E1.m1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><mo id="S4.E1.m1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.1.1.1.1.1.1.1" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml"><msubsup id="S4.E1.m1.1.1.1.1.1.1.1.2" xref="S4.E1.m1.1.1.1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.1.1.1.2.2.2" mathsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml">ℒ</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.2.2.3" mathsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.2.2.3.cmml">c</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.2.3" mathsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.2.3.cmml">A</mi></msubsup><mo id="S4.E1.m1.1.1.1.1.1.1.1.1" mathsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.1.cmml">+</mo><msubsup id="S4.E1.m1.1.1.1.1.1.1.1.3" xref="S4.E1.m1.1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.1.1.1.1.1.1.1.3.2.2" mathsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.3.2.2.cmml">ℒ</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.3.2.3" mathsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.3.2.3.cmml">c</mi><mi id="S4.E1.m1.1.1.1.1.1.1.1.3.3" mathsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3.cmml">B</mi></msubsup></mrow><mo id="S4.E1.m1.1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S4.E1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.E1.m1.2.2.2.3b" mathsize="90%" xref="S4.E1.m1.2.2.2.3.cmml">+</mo><mrow id="S4.E1.m1.2.2.2.2" xref="S4.E1.m1.2.2.2.2.cmml"><msub id="S4.E1.m1.2.2.2.2.3" xref="S4.E1.m1.2.2.2.2.3.cmml"><mi id="S4.E1.m1.2.2.2.2.3.2" mathsize="90%" xref="S4.E1.m1.2.2.2.2.3.2.cmml">λ</mi><mrow id="S4.E1.m1.2.2.2.2.3.3" xref="S4.E1.m1.2.2.2.2.3.3.cmml"><mi id="S4.E1.m1.2.2.2.2.3.3.2" mathsize="90%" xref="S4.E1.m1.2.2.2.2.3.3.2.cmml">i</mi><mo id="S4.E1.m1.2.2.2.2.3.3.1" xref="S4.E1.m1.2.2.2.2.3.3.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.2.2.3.3.3" mathsize="90%" xref="S4.E1.m1.2.2.2.2.3.3.3.cmml">d</mi></mrow></msub><mo id="S4.E1.m1.2.2.2.2.2" xref="S4.E1.m1.2.2.2.2.2.cmml">⁢</mo><mrow id="S4.E1.m1.2.2.2.2.1.1" xref="S4.E1.m1.2.2.2.2.1.1.1.cmml"><mo id="S4.E1.m1.2.2.2.2.1.1.2" maxsize="90%" minsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S4.E1.m1.2.2.2.2.1.1.1" xref="S4.E1.m1.2.2.2.2.1.1.1.cmml"><msubsup id="S4.E1.m1.2.2.2.2.1.1.1.2" xref="S4.E1.m1.2.2.2.2.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.2.2.1.1.1.2.2.2" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.2.cmml">ℒ</mi><mrow id="S4.E1.m1.2.2.2.2.1.1.1.2.2.3" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.cmml"><mi id="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.2" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.2.cmml">i</mi><mo id="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.1" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.3" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.3.cmml">d</mi></mrow><mi id="S4.E1.m1.2.2.2.2.1.1.1.2.3" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.2.3.cmml">A</mi></msubsup><mo id="S4.E1.m1.2.2.2.2.1.1.1.1" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.1.cmml">+</mo><msubsup id="S4.E1.m1.2.2.2.2.1.1.1.3" xref="S4.E1.m1.2.2.2.2.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.E1.m1.2.2.2.2.1.1.1.3.2.2" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.2.cmml">ℒ</mi><mrow id="S4.E1.m1.2.2.2.2.1.1.1.3.2.3" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.cmml"><mi id="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.2" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.2.cmml">i</mi><mo id="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.1" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.3" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.3.cmml">d</mi></mrow><mi id="S4.E1.m1.2.2.2.2.1.1.1.3.3" mathsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.3.3.cmml">B</mi></msubsup></mrow><mo id="S4.E1.m1.2.2.2.2.1.1.3" maxsize="90%" minsize="90%" xref="S4.E1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E1.m1.2b"><apply id="S4.E1.m1.2.2.cmml" xref="S4.E1.m1.2.2"><eq id="S4.E1.m1.2.2.3.cmml" xref="S4.E1.m1.2.2.3"></eq><apply id="S4.E1.m1.2.2.4.cmml" xref="S4.E1.m1.2.2.4"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.4.1.cmml" xref="S4.E1.m1.2.2.4">subscript</csymbol><ci id="S4.E1.m1.2.2.4.2.cmml" xref="S4.E1.m1.2.2.4.2">𝓛</ci><apply id="S4.E1.m1.2.2.4.3.cmml" xref="S4.E1.m1.2.2.4.3"><times id="S4.E1.m1.2.2.4.3.1.cmml" xref="S4.E1.m1.2.2.4.3.1"></times><ci id="S4.E1.m1.2.2.4.3.2.cmml" xref="S4.E1.m1.2.2.4.3.2">𝑪</ci><ci id="S4.E1.m1.2.2.4.3.3.cmml" xref="S4.E1.m1.2.2.4.3.3">𝑪</ci></apply></apply><apply id="S4.E1.m1.2.2.2.cmml" xref="S4.E1.m1.2.2.2"><plus id="S4.E1.m1.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.3"></plus><apply id="S4.E1.m1.2.2.2.4.cmml" xref="S4.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.4.1.cmml" xref="S4.E1.m1.2.2.2.4">superscript</csymbol><apply id="S4.E1.m1.2.2.2.4.2.cmml" xref="S4.E1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.4.2.1.cmml" xref="S4.E1.m1.2.2.2.4">subscript</csymbol><ci id="S4.E1.m1.2.2.2.4.2.2.cmml" xref="S4.E1.m1.2.2.2.4.2.2">ℒ</ci><apply id="S4.E1.m1.2.2.2.4.2.3.cmml" xref="S4.E1.m1.2.2.2.4.2.3"><times id="S4.E1.m1.2.2.2.4.2.3.1.cmml" xref="S4.E1.m1.2.2.2.4.2.3.1"></times><ci id="S4.E1.m1.2.2.2.4.2.3.2.cmml" xref="S4.E1.m1.2.2.2.4.2.3.2">𝑔</ci><ci id="S4.E1.m1.2.2.2.4.2.3.3.cmml" xref="S4.E1.m1.2.2.2.4.2.3.3">𝑎</ci><ci id="S4.E1.m1.2.2.2.4.2.3.4.cmml" xref="S4.E1.m1.2.2.2.4.2.3.4">𝑛</ci></apply></apply><ci id="S4.E1.m1.2.2.2.4.3.cmml" xref="S4.E1.m1.2.2.2.4.3">𝐴</ci></apply><apply id="S4.E1.m1.2.2.2.5.cmml" xref="S4.E1.m1.2.2.2.5"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.5.1.cmml" xref="S4.E1.m1.2.2.2.5">superscript</csymbol><apply id="S4.E1.m1.2.2.2.5.2.cmml" xref="S4.E1.m1.2.2.2.5"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.5.2.1.cmml" xref="S4.E1.m1.2.2.2.5">subscript</csymbol><ci id="S4.E1.m1.2.2.2.5.2.2.cmml" xref="S4.E1.m1.2.2.2.5.2.2">ℒ</ci><apply id="S4.E1.m1.2.2.2.5.2.3.cmml" xref="S4.E1.m1.2.2.2.5.2.3"><times id="S4.E1.m1.2.2.2.5.2.3.1.cmml" xref="S4.E1.m1.2.2.2.5.2.3.1"></times><ci id="S4.E1.m1.2.2.2.5.2.3.2.cmml" xref="S4.E1.m1.2.2.2.5.2.3.2">𝑔</ci><ci id="S4.E1.m1.2.2.2.5.2.3.3.cmml" xref="S4.E1.m1.2.2.2.5.2.3.3">𝑎</ci><ci id="S4.E1.m1.2.2.2.5.2.3.4.cmml" xref="S4.E1.m1.2.2.2.5.2.3.4">𝑛</ci></apply></apply><ci id="S4.E1.m1.2.2.2.5.3.cmml" xref="S4.E1.m1.2.2.2.5.3">𝐵</ci></apply><apply id="S4.E1.m1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1"><times id="S4.E1.m1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.2"></times><apply id="S4.E1.m1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.3.2">𝜆</ci><ci id="S4.E1.m1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.3.3">𝑐</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1"><plus id="S4.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.1"></plus><apply id="S4.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2.2.2">ℒ</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2.2.3">𝑐</ci></apply><ci id="S4.E1.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.2.3">𝐴</ci></apply><apply id="S4.E1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S4.E1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.2.2">ℒ</ci><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.2.3">𝑐</ci></apply><ci id="S4.E1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E1.m1.1.1.1.1.1.1.1.3.3">𝐵</ci></apply></apply></apply><apply id="S4.E1.m1.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2"><times id="S4.E1.m1.2.2.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.2"></times><apply id="S4.E1.m1.2.2.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.3">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.3.2.cmml" xref="S4.E1.m1.2.2.2.2.3.2">𝜆</ci><apply id="S4.E1.m1.2.2.2.2.3.3.cmml" xref="S4.E1.m1.2.2.2.2.3.3"><times id="S4.E1.m1.2.2.2.2.3.3.1.cmml" xref="S4.E1.m1.2.2.2.2.3.3.1"></times><ci id="S4.E1.m1.2.2.2.2.3.3.2.cmml" xref="S4.E1.m1.2.2.2.2.3.3.2">𝑖</ci><ci id="S4.E1.m1.2.2.2.2.3.3.3.cmml" xref="S4.E1.m1.2.2.2.2.3.3.3">𝑑</ci></apply></apply><apply id="S4.E1.m1.2.2.2.2.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1"><plus id="S4.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.1"></plus><apply id="S4.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.1.1.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2">superscript</csymbol><apply id="S4.E1.m1.2.2.2.2.1.1.1.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.1.1.2.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.1.1.1.2.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.2">ℒ</ci><apply id="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.3"><times id="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.1"></times><ci id="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.2">𝑖</ci><ci id="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2.2.3.3">𝑑</ci></apply></apply><ci id="S4.E1.m1.2.2.2.2.1.1.1.2.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.2.3">𝐴</ci></apply><apply id="S4.E1.m1.2.2.2.2.1.1.1.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.1.1.3.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3">superscript</csymbol><apply id="S4.E1.m1.2.2.2.2.1.1.1.3.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S4.E1.m1.2.2.2.2.1.1.1.3.2.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3">subscript</csymbol><ci id="S4.E1.m1.2.2.2.2.1.1.1.3.2.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.2">ℒ</ci><apply id="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.3"><times id="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.1.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.1"></times><ci id="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.2.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.2">𝑖</ci><ci id="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3.2.3.3">𝑑</ci></apply></apply><ci id="S4.E1.m1.2.2.2.2.1.1.1.3.3.cmml" xref="S4.E1.m1.2.2.2.2.1.1.1.3.3">𝐵</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E1.m1.2c">\bm{\mathcal{L}_{CC}}=\mathcal{L}_{gan}^{A}+\mathcal{L}_{gan}^{B}+\lambda_{c}(%
\mathcal{L}_{c}^{A}+\mathcal{L}_{c}^{B})+\lambda_{id}(\mathcal{L}_{id}^{A}+%
\mathcal{L}_{id}^{B})</annotation><annotation encoding="application/x-llamapun" id="S4.E1.m1.2d">bold_caligraphic_L start_POSTSUBSCRIPT bold_italic_C bold_italic_C end_POSTSUBSCRIPT = caligraphic_L start_POSTSUBSCRIPT italic_g italic_a italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_g italic_a italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT + italic_λ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT ) + italic_λ start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p5">
<table class="ltx_equation ltx_eqn_table" id="S4.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{gan:}A\xrightarrow{D_{A}(G_{A})}B\;\;\;\;\;\;\;\;\;{\color[rgb]{.5,.5,.5}%
\text{[MSE Loss]}}" class="ltx_Math" display="block" id="S4.E2.m1.3"><semantics id="S4.E2.m1.3a"><mrow id="S4.E2.m1.3.4" xref="S4.E2.m1.3.4.cmml"><mrow id="S4.E2.m1.3.4.2" xref="S4.E2.m1.3.4.2.cmml"><mi id="S4.E2.m1.3.4.2.2" mathsize="90%" xref="S4.E2.m1.3.4.2.2.cmml">𝒈</mi><mo id="S4.E2.m1.3.4.2.1" xref="S4.E2.m1.3.4.2.1.cmml">⁢</mo><mi id="S4.E2.m1.3.4.2.3" mathsize="90%" xref="S4.E2.m1.3.4.2.3.cmml">𝒂</mi><mo id="S4.E2.m1.3.4.2.1a" xref="S4.E2.m1.3.4.2.1.cmml">⁢</mo><mi id="S4.E2.m1.3.4.2.4" mathsize="90%" xref="S4.E2.m1.3.4.2.4.cmml">𝒏</mi></mrow><mo class="ltx_mathvariant_bold" id="S4.E2.m1.3.4.1" lspace="0.278em" mathsize="90%" mathvariant="bold" rspace="0.278em" xref="S4.E2.m1.3.4.1.cmml">:</mo><mrow id="S4.E2.m1.3.4.3" xref="S4.E2.m1.3.4.3.cmml"><mi id="S4.E2.m1.3.4.3.1" mathsize="90%" xref="S4.E2.m1.3.4.3.1.cmml">A</mi><mover accent="true" id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mo id="S4.E2.m1.1.1.2" mathsize="90%" stretchy="false" xref="S4.E2.m1.1.1.2.cmml">→</mo><mrow id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml"><msub id="S4.E2.m1.1.1.1.3" xref="S4.E2.m1.1.1.1.3.cmml"><mi id="S4.E2.m1.1.1.1.3.2" mathsize="90%" xref="S4.E2.m1.1.1.1.3.2.cmml">D</mi><mi id="S4.E2.m1.1.1.1.3.3" mathsize="90%" xref="S4.E2.m1.1.1.1.3.3.cmml">A</mi></msub><mo id="S4.E2.m1.1.1.1.2" xref="S4.E2.m1.1.1.1.2.cmml">⁢</mo><mrow id="S4.E2.m1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mo id="S4.E2.m1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S4.E2.m1.1.1.1.1.1.1.cmml">(</mo><msub id="S4.E2.m1.1.1.1.1.1.1" xref="S4.E2.m1.1.1.1.1.1.1.cmml"><mi id="S4.E2.m1.1.1.1.1.1.1.2" mathsize="90%" xref="S4.E2.m1.1.1.1.1.1.1.2.cmml">G</mi><mi id="S4.E2.m1.1.1.1.1.1.1.3" mathsize="90%" xref="S4.E2.m1.1.1.1.1.1.1.3.cmml">A</mi></msub><mo id="S4.E2.m1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S4.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mover><mrow id="S4.E2.m1.3.4.3.2.2" xref="S4.E2.m1.3.4.3.2.1.cmml"><mi id="S4.E2.m1.2.2" mathsize="90%" xref="S4.E2.m1.2.2.cmml">B</mi><mspace id="S4.E2.m1.3.4.3.2.2.1" width="2.50000000000001em" xref="S4.E2.m1.3.4.3.2.1.cmml"></mspace><mtext id="S4.E2.m1.3.3" mathcolor="#808080" mathsize="90%" xref="S4.E2.m1.3.3a.cmml">[MSE Loss]</mtext></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.3b"><apply id="S4.E2.m1.3.4.cmml" xref="S4.E2.m1.3.4"><ci id="S4.E2.m1.3.4.1.cmml" xref="S4.E2.m1.3.4.1">bold-:</ci><apply id="S4.E2.m1.3.4.2.cmml" xref="S4.E2.m1.3.4.2"><times id="S4.E2.m1.3.4.2.1.cmml" xref="S4.E2.m1.3.4.2.1"></times><ci id="S4.E2.m1.3.4.2.2.cmml" xref="S4.E2.m1.3.4.2.2">𝒈</ci><ci id="S4.E2.m1.3.4.2.3.cmml" xref="S4.E2.m1.3.4.2.3">𝒂</ci><ci id="S4.E2.m1.3.4.2.4.cmml" xref="S4.E2.m1.3.4.2.4">𝒏</ci></apply><apply id="S4.E2.m1.3.4.3.cmml" xref="S4.E2.m1.3.4.3"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><apply id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"><times id="S4.E2.m1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.2"></times><apply id="S4.E2.m1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.1.3">subscript</csymbol><ci id="S4.E2.m1.1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.1.3.2">𝐷</ci><ci id="S4.E2.m1.1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.1.3.3">𝐴</ci></apply><apply id="S4.E2.m1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.1.1.1.1.1.cmml" xref="S4.E2.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.E2.m1.1.1.1.1.1.1.2.cmml" xref="S4.E2.m1.1.1.1.1.1.1.2">𝐺</ci><ci id="S4.E2.m1.1.1.1.1.1.1.3.cmml" xref="S4.E2.m1.1.1.1.1.1.1.3">𝐴</ci></apply></apply><ci id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2">→</ci></apply><ci id="S4.E2.m1.3.4.3.1.cmml" xref="S4.E2.m1.3.4.3.1">𝐴</ci><list id="S4.E2.m1.3.4.3.2.1.cmml" xref="S4.E2.m1.3.4.3.2.2"><ci id="S4.E2.m1.2.2.cmml" xref="S4.E2.m1.2.2">𝐵</ci><ci id="S4.E2.m1.3.3a.cmml" xref="S4.E2.m1.3.3"><mtext id="S4.E2.m1.3.3.cmml" mathcolor="#808080" mathsize="90%" xref="S4.E2.m1.3.3">[MSE Loss]</mtext></ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.3c">\bm{gan:}A\xrightarrow{D_{A}(G_{A})}B\;\;\;\;\;\;\;\;\;{\color[rgb]{.5,.5,.5}%
\text{[MSE Loss]}}</annotation><annotation encoding="application/x-llamapun" id="S4.E2.m1.3d">bold_italic_g bold_italic_a bold_italic_n bold_: italic_A start_ARROW start_OVERACCENT italic_D start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT ) end_OVERACCENT → end_ARROW italic_B [MSE Loss]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p6">
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{cyc:}A\xrightarrow{G_{A}}B\xrightarrow{G_{B}}A,B\&gt;\;\;\;{\color[rgb]{%
.5,.5,.5}\text{[L1 Loss]}}" class="ltx_math_unparsed" display="block" id="S4.E3.m1.1"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1b"><mi id="S4.E3.m1.1.1" mathsize="90%">𝒄</mi><mi id="S4.E3.m1.1.2" mathsize="90%">𝒚</mi><mi id="S4.E3.m1.1.3" mathsize="90%">𝒄</mi><mo class="ltx_mathvariant_bold" id="S4.E3.m1.1.4" lspace="0.278em" mathsize="90%" mathvariant="bold" rspace="0.278em">:</mo><mi id="S4.E3.m1.1.5" mathsize="90%">A</mi><mover accent="true" id="S4.E3.m1.1.6"><mo id="S4.E3.m1.1.6.2" mathsize="90%" stretchy="false">→</mo><msub id="S4.E3.m1.1.6.1"><mi id="S4.E3.m1.1.6.1.2" mathsize="90%">G</mi><mi id="S4.E3.m1.1.6.1.3" mathsize="90%">A</mi></msub></mover><mi id="S4.E3.m1.1.7" mathsize="90%">B</mi><mover accent="true" id="S4.E3.m1.1.8"><mo id="S4.E3.m1.1.8.2" mathsize="90%" stretchy="false">→</mo><msub id="S4.E3.m1.1.8.1"><mi id="S4.E3.m1.1.8.1.2" mathsize="90%">G</mi><mi id="S4.E3.m1.1.8.1.3" mathsize="90%">B</mi></msub></mover><mi id="S4.E3.m1.1.9" mathsize="90%">A</mi><mo id="S4.E3.m1.1.10" mathsize="90%">,</mo><mi id="S4.E3.m1.1.11" mathsize="90%">B</mi><mspace id="S4.E3.m1.1.12" width="1.05555555555556em"></mspace><mtext id="S4.E3.m1.1.13" mathcolor="#808080" mathsize="90%">[L1 Loss]</mtext></mrow><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\bm{cyc:}A\xrightarrow{G_{A}}B\xrightarrow{G_{B}}A,B\&gt;\;\;\;{\color[rgb]{%
.5,.5,.5}\text{[L1 Loss]}}</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.1d">bold_italic_c bold_italic_y bold_italic_c bold_: italic_A start_ARROW start_OVERACCENT italic_G start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW italic_B start_ARROW start_OVERACCENT italic_G start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW italic_A , italic_B [L1 Loss]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p7">
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\bm{id:}A\xrightarrow{G_{B}}A\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;%
\;{\color[rgb]{.5,.5,.5}\text{[L1 Loss]}}" class="ltx_Math" display="block" id="S4.E4.m1.2"><semantics id="S4.E4.m1.2a"><mrow id="S4.E4.m1.2.3" xref="S4.E4.m1.2.3.cmml"><mrow id="S4.E4.m1.2.3.2" xref="S4.E4.m1.2.3.2.cmml"><mi id="S4.E4.m1.2.3.2.2" mathsize="90%" xref="S4.E4.m1.2.3.2.2.cmml">𝒊</mi><mo id="S4.E4.m1.2.3.2.1" xref="S4.E4.m1.2.3.2.1.cmml">⁢</mo><mi id="S4.E4.m1.2.3.2.3" mathsize="90%" xref="S4.E4.m1.2.3.2.3.cmml">𝒅</mi></mrow><mo class="ltx_mathvariant_bold" id="S4.E4.m1.2.3.1" lspace="0.278em" mathsize="90%" mathvariant="bold" rspace="0.278em" xref="S4.E4.m1.2.3.1.cmml">:</mo><mrow id="S4.E4.m1.2.3.3" xref="S4.E4.m1.2.3.3.cmml"><mi id="S4.E4.m1.2.3.3.2" mathsize="90%" xref="S4.E4.m1.2.3.3.2.cmml">A</mi><mover accent="true" id="S4.E4.m1.2.3.3.1" xref="S4.E4.m1.2.3.3.1.cmml"><mo id="S4.E4.m1.2.3.3.1.2" mathsize="90%" stretchy="false" xref="S4.E4.m1.2.3.3.1.2.cmml">→</mo><msub id="S4.E4.m1.2.3.3.1.1" xref="S4.E4.m1.2.3.3.1.1.cmml"><mi id="S4.E4.m1.2.3.3.1.1.2" mathsize="90%" xref="S4.E4.m1.2.3.3.1.1.2.cmml">G</mi><mi id="S4.E4.m1.2.3.3.1.1.3" mathsize="90%" xref="S4.E4.m1.2.3.3.1.1.3.cmml">B</mi></msub></mover><mrow id="S4.E4.m1.2.3.3.3.2" xref="S4.E4.m1.2.3.3.3.1.cmml"><mi id="S4.E4.m1.1.1" mathsize="90%" xref="S4.E4.m1.1.1.cmml">A</mi><mspace id="S4.E4.m1.2.3.3.3.2.1" width="6.94444444444449em" xref="S4.E4.m1.2.3.3.3.1.cmml"></mspace><mtext id="S4.E4.m1.2.2" mathcolor="#808080" mathsize="90%" xref="S4.E4.m1.2.2a.cmml">[L1 Loss]</mtext></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.2b"><apply id="S4.E4.m1.2.3.cmml" xref="S4.E4.m1.2.3"><ci id="S4.E4.m1.2.3.1.cmml" xref="S4.E4.m1.2.3.1">bold-:</ci><apply id="S4.E4.m1.2.3.2.cmml" xref="S4.E4.m1.2.3.2"><times id="S4.E4.m1.2.3.2.1.cmml" xref="S4.E4.m1.2.3.2.1"></times><ci id="S4.E4.m1.2.3.2.2.cmml" xref="S4.E4.m1.2.3.2.2">𝒊</ci><ci id="S4.E4.m1.2.3.2.3.cmml" xref="S4.E4.m1.2.3.2.3">𝒅</ci></apply><apply id="S4.E4.m1.2.3.3.cmml" xref="S4.E4.m1.2.3.3"><apply id="S4.E4.m1.2.3.3.1.cmml" xref="S4.E4.m1.2.3.3.1"><apply id="S4.E4.m1.2.3.3.1.1.cmml" xref="S4.E4.m1.2.3.3.1.1"><csymbol cd="ambiguous" id="S4.E4.m1.2.3.3.1.1.1.cmml" xref="S4.E4.m1.2.3.3.1.1">subscript</csymbol><ci id="S4.E4.m1.2.3.3.1.1.2.cmml" xref="S4.E4.m1.2.3.3.1.1.2">𝐺</ci><ci id="S4.E4.m1.2.3.3.1.1.3.cmml" xref="S4.E4.m1.2.3.3.1.1.3">𝐵</ci></apply><ci id="S4.E4.m1.2.3.3.1.2.cmml" xref="S4.E4.m1.2.3.3.1.2">→</ci></apply><ci id="S4.E4.m1.2.3.3.2.cmml" xref="S4.E4.m1.2.3.3.2">𝐴</ci><list id="S4.E4.m1.2.3.3.3.1.cmml" xref="S4.E4.m1.2.3.3.3.2"><ci id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1">𝐴</ci><ci id="S4.E4.m1.2.2a.cmml" xref="S4.E4.m1.2.2"><mtext id="S4.E4.m1.2.2.cmml" mathcolor="#808080" mathsize="90%" xref="S4.E4.m1.2.2">[L1 Loss]</mtext></ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.2c">\bm{id:}A\xrightarrow{G_{B}}A\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;%
\;{\color[rgb]{.5,.5,.5}\text{[L1 Loss]}}</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.2d">bold_italic_i bold_italic_d bold_: italic_A start_ARROW start_OVERACCENT italic_G start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW italic_A [L1 Loss]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S4.SS3.p8">
<p class="ltx_p" id="S4.SS3.p8.2"><span class="ltx_text" id="S4.SS3.p8.2.1" style="font-size:90%;">Our CycleGAN is trained over 200 epochs using two data batches, one comprised of synthetic data and one of real data, using the 25 words of GSC classified as </span><span class="ltx_text ltx_font_italic" id="S4.SS3.p8.2.2" style="font-size:90%;">unknown</span><span class="ltx_text" id="S4.SS3.p8.2.3" style="font-size:90%;"> (see </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S2.SS2" style="font-size:90%;" title="2.2 Google Speech Commands (GSC) ‣ 2 Synthetic spoken commands generation ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">2.2</span></a><span class="ltx_text" id="S4.SS3.p8.2.4" style="font-size:90%;">). The real and synthetic mini-batches are drawn randomly and independently. The system is optimized with a learning rate of 1e-5, a batch size of 128, a lambda cycle (</span><math alttext="\lambda_{cyc}" class="ltx_Math" display="inline" id="S4.SS3.p8.1.m1.1"><semantics id="S4.SS3.p8.1.m1.1a"><msub id="S4.SS3.p8.1.m1.1.1" xref="S4.SS3.p8.1.m1.1.1.cmml"><mi id="S4.SS3.p8.1.m1.1.1.2" mathsize="90%" xref="S4.SS3.p8.1.m1.1.1.2.cmml">λ</mi><mrow id="S4.SS3.p8.1.m1.1.1.3" xref="S4.SS3.p8.1.m1.1.1.3.cmml"><mi id="S4.SS3.p8.1.m1.1.1.3.2" mathsize="90%" xref="S4.SS3.p8.1.m1.1.1.3.2.cmml">c</mi><mo id="S4.SS3.p8.1.m1.1.1.3.1" xref="S4.SS3.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.p8.1.m1.1.1.3.3" mathsize="90%" xref="S4.SS3.p8.1.m1.1.1.3.3.cmml">y</mi><mo id="S4.SS3.p8.1.m1.1.1.3.1a" xref="S4.SS3.p8.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.p8.1.m1.1.1.3.4" mathsize="90%" xref="S4.SS3.p8.1.m1.1.1.3.4.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p8.1.m1.1b"><apply id="S4.SS3.p8.1.m1.1.1.cmml" xref="S4.SS3.p8.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p8.1.m1.1.1.1.cmml" xref="S4.SS3.p8.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p8.1.m1.1.1.2.cmml" xref="S4.SS3.p8.1.m1.1.1.2">𝜆</ci><apply id="S4.SS3.p8.1.m1.1.1.3.cmml" xref="S4.SS3.p8.1.m1.1.1.3"><times id="S4.SS3.p8.1.m1.1.1.3.1.cmml" xref="S4.SS3.p8.1.m1.1.1.3.1"></times><ci id="S4.SS3.p8.1.m1.1.1.3.2.cmml" xref="S4.SS3.p8.1.m1.1.1.3.2">𝑐</ci><ci id="S4.SS3.p8.1.m1.1.1.3.3.cmml" xref="S4.SS3.p8.1.m1.1.1.3.3">𝑦</ci><ci id="S4.SS3.p8.1.m1.1.1.3.4.cmml" xref="S4.SS3.p8.1.m1.1.1.3.4">𝑐</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p8.1.m1.1c">\lambda_{cyc}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p8.1.m1.1d">italic_λ start_POSTSUBSCRIPT italic_c italic_y italic_c end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS3.p8.2.5" style="font-size:90%;">) of 10.0 and a lambda identity (</span><math alttext="\lambda_{id}" class="ltx_Math" display="inline" id="S4.SS3.p8.2.m2.1"><semantics id="S4.SS3.p8.2.m2.1a"><msub id="S4.SS3.p8.2.m2.1.1" xref="S4.SS3.p8.2.m2.1.1.cmml"><mi id="S4.SS3.p8.2.m2.1.1.2" mathsize="90%" xref="S4.SS3.p8.2.m2.1.1.2.cmml">λ</mi><mrow id="S4.SS3.p8.2.m2.1.1.3" xref="S4.SS3.p8.2.m2.1.1.3.cmml"><mi id="S4.SS3.p8.2.m2.1.1.3.2" mathsize="90%" xref="S4.SS3.p8.2.m2.1.1.3.2.cmml">i</mi><mo id="S4.SS3.p8.2.m2.1.1.3.1" xref="S4.SS3.p8.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S4.SS3.p8.2.m2.1.1.3.3" mathsize="90%" xref="S4.SS3.p8.2.m2.1.1.3.3.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p8.2.m2.1b"><apply id="S4.SS3.p8.2.m2.1.1.cmml" xref="S4.SS3.p8.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p8.2.m2.1.1.1.cmml" xref="S4.SS3.p8.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p8.2.m2.1.1.2.cmml" xref="S4.SS3.p8.2.m2.1.1.2">𝜆</ci><apply id="S4.SS3.p8.2.m2.1.1.3.cmml" xref="S4.SS3.p8.2.m2.1.1.3"><times id="S4.SS3.p8.2.m2.1.1.3.1.cmml" xref="S4.SS3.p8.2.m2.1.1.3.1"></times><ci id="S4.SS3.p8.2.m2.1.1.3.2.cmml" xref="S4.SS3.p8.2.m2.1.1.3.2">𝑖</ci><ci id="S4.SS3.p8.2.m2.1.1.3.3.cmml" xref="S4.SS3.p8.2.m2.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p8.2.m2.1c">\lambda_{id}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p8.2.m2.1d">italic_λ start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS3.p8.2.6" style="font-size:90%;">) value of 0.5. Since our main goal is to make synthetic speech representations closer to real ones, only the generator A is used to perform the domain transformation, as illustrated in Fig. </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.F3" style="font-size:90%;" title="Figure 3 ‣ 4.3 CycleGAN Domain Adaptation ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S4.SS3.p8.2.7" style="font-size:90%;">.</span></p>
</div>
<div class="ltx_para" id="S4.SS3.p9">
<p class="ltx_p" id="S4.SS3.p9.1"><span class="ltx_text" id="S4.SS3.p9.1.1" style="font-size:90%;">Similarly to section </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.SS1" style="font-size:90%;" title="4.1 Results with WavLM-Base-Plus representations ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">4.1</span></a><span class="ltx_text" id="S4.SS3.p9.1.2" style="font-size:90%;">, linear classifiers are trained using the same hyperparameters as before, this time using the pre-trained domain adaptation provided by the CycleGAN, with a frozen Generator A. The results, reported in Table </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S4.T2" style="font-size:90%;" title="Table 2 ‣ 4.3 CycleGAN Domain Adaptation ‣ 4 SSL-based SCC Experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S4.SS3.p9.1.3" style="font-size:90%;">, show a small but noticeable improvement in accuracy when using our CycleGAN versus the system trained without this domain adaptation.</span></p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold" id="S4.T2.8.1.1">Table 2</span>: </span>Results obtained on the GSC test set from training a set of WavLM-based linear classifiers using real data, synthetic data (without and with filtering) and synthetic data transformed with our CycleGAN. Each system was trained 5 times using different random seeds, the mean and standard deviation values are presented</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.4.5.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.5.1.1"><span class="ltx_text ltx_font_bold" id="S4.T2.4.5.1.1.1" style="font-size:90%;">System</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.5.1.2"><span class="ltx_text ltx_font_bold" id="S4.T2.4.5.1.2.1" style="font-size:90%;">Data</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.4.5.1.3"><span class="ltx_text ltx_font_bold" id="S4.T2.4.5.1.3.1" style="font-size:90%;">Acc. (%)</span></td>
</tr>
</tbody>
<tfoot class="ltx_tfoot">
<tr class="ltx_tr" id="S4.T2.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_b ltx_border_t" id="S4.T2.1.1.2" rowspan="4"><span class="ltx_text" id="S4.T2.1.1.2.1" style="font-size:90%;"><span class="ltx_text" id="S4.T2.1.1.2.1.1"></span> <span class="ltx_text" id="S4.T2.1.1.2.1.2">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.2.1.2.1">
<span class="ltx_tr" id="S4.T2.1.1.2.1.2.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.2.1.2.1.1.1"><span class="ltx_text" id="S4.T2.1.1.2.1.2.1.1.1.1" style="font-size:89%;">WavLM Base Plus</span></span></span>
<span class="ltx_tr" id="S4.T2.1.1.2.1.2.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.2.1.2.1.2.1"><span class="ltx_text" id="S4.T2.1.1.2.1.2.1.2.1.1" style="font-size:89%;">w/ stats pooling</span></span></span>
</span></span> <span class="ltx_text" id="S4.T2.1.1.2.1.3"></span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.1.1.3">
<span class="ltx_text" id="S4.T2.1.1.3.1"></span><span class="ltx_text" id="S4.T2.1.1.3.2" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.1.1.3.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.3.3.1">
<span class="ltx_tr" id="S4.T2.1.1.3.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.3.3.1.1.1"><span class="ltx_text" id="S4.T2.1.1.3.3.1.1.1.1" style="font-size:89%;">Real</span></span></span>
</span></span><span class="ltx_text" id="S4.T2.1.1.3.4"></span><span class="ltx_text" id="S4.T2.1.1.3.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S4.T2.1.1.1">
<span class="ltx_text" id="S4.T2.1.1.1.2"></span><span class="ltx_text" id="S4.T2.1.1.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.1.1.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.1.1.1.1.1.1">
<span class="ltx_tr" id="S4.T2.1.1.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.1.1.1.1.1.1.1.1"><math alttext="98.03\pm 0.11" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.1.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.2.cmml">98.03</mn><mo id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.3.cmml">0.11</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.1.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.2">98.03</cn><cn id="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.1.1.1.1.1.1.1.1.m1.1.1.3">0.11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.1.1.1.1.m1.1c">98.03\pm 0.11</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.1.1.1.1.m1.1d">98.03 ± 0.11</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S4.T2.1.1.1.4"></span><span class="ltx_text" id="S4.T2.1.1.1.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T2.2.2.2">
<span class="ltx_text" id="S4.T2.2.2.2.1"></span><span class="ltx_text" id="S4.T2.2.2.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.2.2.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.2.2.2.3.1">
<span class="ltx_tr" id="S4.T2.2.2.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.2.3.1.1.1"><span class="ltx_text" id="S4.T2.2.2.2.3.1.1.1.1" style="font-size:89%;">Synth.</span></span></span>
</span></span><span class="ltx_text" id="S4.T2.2.2.2.4"></span><span class="ltx_text" id="S4.T2.2.2.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T2.2.2.1">
<span class="ltx_text" id="S4.T2.2.2.1.2"></span><span class="ltx_text" id="S4.T2.2.2.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.2.2.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.2.2.1.1.1.1">
<span class="ltx_tr" id="S4.T2.2.2.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.2.2.1.1.1.1.1.1"><math alttext="83.05\pm 0.82" class="ltx_Math" display="inline" id="S4.T2.2.2.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.2.2.1.1.1.1.1.1.m1.1a"><mrow id="S4.T2.2.2.1.1.1.1.1.1.m1.1.1" xref="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.2.cmml">83.05</mn><mo id="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.3.cmml">0.82</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.1.1.1.1.1.1.m1.1b"><apply id="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.2.2.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.2">83.05</cn><cn id="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.2.2.1.1.1.1.1.1.m1.1.1.3">0.82</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.1.1.1.1.1.1.m1.1c">83.05\pm 0.82</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.1.1.1.1.1.1.m1.1d">83.05 ± 0.82</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S4.T2.2.2.1.4"></span><span class="ltx_text" id="S4.T2.2.2.1.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T2.3.3.2">
<span class="ltx_text" id="S4.T2.3.3.2.1"></span><span class="ltx_text" id="S4.T2.3.3.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.3.3.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.2.3.1">
<span class="ltx_tr" id="S4.T2.3.3.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.2.3.1.1.1"><span class="ltx_text" id="S4.T2.3.3.2.3.1.1.1.1" style="font-size:89%;">Synth. (F)</span></span></span>
</span></span><span class="ltx_text" id="S4.T2.3.3.2.4"></span><span class="ltx_text" id="S4.T2.3.3.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row" id="S4.T2.3.3.1">
<span class="ltx_text" id="S4.T2.3.3.1.2"></span><span class="ltx_text" id="S4.T2.3.3.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.3.3.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.3.3.1.1.1.1">
<span class="ltx_tr" id="S4.T2.3.3.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.3.3.1.1.1.1.1.1"><math alttext="96.11\pm 0.08" class="ltx_Math" display="inline" id="S4.T2.3.3.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.3.3.1.1.1.1.1.1.m1.1a"><mrow id="S4.T2.3.3.1.1.1.1.1.1.m1.1.1" xref="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.2.cmml">96.11</mn><mo id="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.3.cmml">0.08</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.1.1.1.1.1.1.m1.1b"><apply id="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.3.3.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.2">96.11</cn><cn id="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.3.3.1.1.1.1.1.1.m1.1.1.3">0.08</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.1.1.1.1.1.1.m1.1c">96.11\pm 0.08</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.1.1.1.1.1.1.m1.1d">96.11 ± 0.08</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S4.T2.3.3.1.4"></span><span class="ltx_text" id="S4.T2.3.3.1.5" style="font-size:90%;"></span>
</th>
</tr>
<tr class="ltx_tr" id="S4.T2.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_b" id="S4.T2.4.4.2">
<span class="ltx_text" id="S4.T2.4.4.2.1"></span><span class="ltx_text" id="S4.T2.4.4.2.2" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.4.4.2.3" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.4.4.2.3.1">
<span class="ltx_tr" id="S4.T2.4.4.2.3.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.4.2.3.1.1.1"><span class="ltx_text" id="S4.T2.4.4.2.3.1.1.1.1" style="font-size:89%;">Synth. (F) + GAN</span></span></span>
</span></span><span class="ltx_text" id="S4.T2.4.4.2.4"></span><span class="ltx_text" id="S4.T2.4.4.2.5" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_b" id="S4.T2.4.4.1">
<span class="ltx_text" id="S4.T2.4.4.1.2"></span><span class="ltx_text" id="S4.T2.4.4.1.3" style="font-size:90%;"> </span><span class="ltx_text" id="S4.T2.4.4.1.1" style="font-size:90%;">
<span class="ltx_tabular ltx_align_middle" id="S4.T2.4.4.1.1.1.1">
<span class="ltx_tr" id="S4.T2.4.4.1.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T2.4.4.1.1.1.1.1.1"><math alttext="96.51\pm 0.11" class="ltx_Math" display="inline" id="S4.T2.4.4.1.1.1.1.1.1.m1.1"><semantics id="S4.T2.4.4.1.1.1.1.1.1.m1.1a"><mrow id="S4.T2.4.4.1.1.1.1.1.1.m1.1.1" xref="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.cmml"><mn id="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.2" mathsize="89%" xref="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.2.cmml">96.51</mn><mo id="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.1" mathsize="89%" xref="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.3" mathsize="89%" xref="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.3.cmml">0.11</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.1.1.1.1.1.m1.1b"><apply id="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.cmml" xref="S4.T2.4.4.1.1.1.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.2.cmml" type="float" xref="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.2">96.51</cn><cn id="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.4.4.1.1.1.1.1.1.m1.1.1.3">0.11</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.1.1.1.1.1.m1.1c">96.51\pm 0.11</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.1.1.1.1.1.1.m1.1d">96.51 ± 0.11</annotation></semantics></math></span></span>
</span></span><span class="ltx_text" id="S4.T2.4.4.1.4"></span><span class="ltx_text" id="S4.T2.4.4.1.5" style="font-size:90%;"></span>
</th>
</tr>
</tfoot>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text" id="S5.p1.1.1" style="font-size:90%;">Despite advancements in the quality of modern TTS systems, PCA analysis on MFCCs — and even more so on SSL features (Fig. </span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#S3.F2" style="font-size:90%;" title="Figure 2 ‣ 3.2 Fully Synthetic SCC ‣ 3 First SCC experiments ‣ Enhancing Synthetic Training Data for Speech Commands: From ASR-Based Filtering to Domain Adaptation in SSL Latent Space"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S5.p1.1.2" style="font-size:90%;">) — revealed noticeable differences in cluster size and position between synthetic and real speech data. This indicates that synthetic speech still lacks the variety and diversity inherent in real speech, but also a mismatch between the feature representation spaces, at least with SSL features. While the literature and our experiments show good performance when using SSL speech representations for our task, it was surprising to observe such a distinct separation between synthetic and real speech data. Despite the CycleGAN methodology promoting a small performance gain, a number of questions appear, left for future work. We aim to conduct an in-depth analysis of the SSL features to identify which specific features are most relevant in distinguishing between synthetic and real speech data. We could potentially exclude them in our downstream application, thereby aligning it more closely with real data characteristics.
A deeper analysis of the domain adaptation actually achieved by the CycleGAN is essential to evaluate how closely the transformed data distribution aligns with the real data. The modest performance gain obtained with CycleGAN adaptation suggests that the distributions were brought closer together. However, we would like to further quantify this alignment more rigorously. Additionally, exploring other recent domain adaptation methods, such as Flow Matching </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S5.p1.1.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.12745v1#bib.bib27" title="">27</a><span class="ltx_text" id="S5.p1.1.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S5.p1.1.5" style="font-size:90%;">, could provide further insights and potentially enhance the adaptation process.</span></p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text" id="S5.p2.1.1" style="font-size:90%;">We are also interested in exploring the impact of potential distribution differences between synthetic and real speech on other downstream tasks, such as emotion recognition and intention detection. In these tasks, fixed-length representations could be adapted from one domain to another (</span><span class="ltx_text ltx_font_italic" id="S5.p2.1.2" style="font-size:90%;">e.g.</span><span class="ltx_text" id="S5.p2.1.3" style="font-size:90%;">, converting neutral speech to various emotional states), especially for systems based on self-supervised learning features. This approach could lead to improved performance and the development of new data augmentation strategies.</span></p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text" id="S5.p3.1.1" style="font-size:90%;">Additionally, our current study is limited to a single TTS system that, although widely used and prominent in the speech community. Testing other recent TTS systems, such as WhisperSpeech</span><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/collabora/WhisperSpeech</span></span></span><span class="ltx_text" id="S5.p3.1.2" style="font-size:90%;">, would be a valuable future extension to this work.</span></p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text" id="S6.p1.1.1" style="font-size:90%;">We used XTTS v2, a recent end-to-end TTS system with voice cloning capabilities, to generate synthetic speech commands data that mimic the training subset of the standard Google Speech Commands dataset. We sourced speech excerpts from Common Voice multilingual datasets, ensuring a large enough sample to assign a unique speaker to each generated audio clip through voice cloning.</span></p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1"><span class="ltx_text" id="S6.p2.1.1" style="font-size:90%;">The lightweight MatchboxNet model (134k parameters), trained on this synthetic data, achieved 90.0% accuracy on the test set, which improved to 92.6% when an ASR-based filtering technique was employed to remove poorly synthesized audio samples. These results are comparable to other methods in the literature but use significantly fewer parameters. Nevertheless, this result remains below the 98.5% accuracy achieved using the real training set. Using the synthetic data (with filtering), we achieved a higher accuracy of 96.1% by leveraging self-supervised representations from WavLM-Base-Plus, which were vectorized with a statistical pooling layer and classified using a fully connected layer.</span></p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text" id="S6.p3.1.1" style="font-size:90%;">A PCA plot of the WavLM SSL features showed that the distributions of the synthetic and real speech feature representations do not fully align. We proposed using a CycleGAN for domain adaptation to transform synthetic speech representations into more realistic ones. This approach further improved the accuracy to 96.5%, narrowing the performance gap between models trained on synthetic and real speech data.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Aleksandr Laptev, Roman Korostik, Aleksey Svischev, Andrei Andrusenko, Ivan Medennikov, and Sergey Rybin,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">“You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib1.4.2" style="font-size:90%;">2020 13th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)</span><span class="ltx_text" id="bib.bib1.5.3" style="font-size:90%;">, 2020, pp. 439–444.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Zhehuai Chen, Andrew Rosenberg, Yu Zhang, Heiga Zen, Mohammadreza Ghodsi, Yinghui Huang, Jesse Emond, Gary Wang, Bhuvana Ramabhadran, and Pedro Moreno,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">“Semi-supervision in asr: Sequential mixmatch and factorized tts-based augmentation,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">Proc. INTERSPEECH</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Sei Ueno, Masato Mimura, Shinsuke Sakai, and Tatsuya Kawahara,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">“Multi-speaker sequence-to-sequence speech synthesis for data augmentation in acoustic-to-word speech recognition,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib3.4.2" style="font-size:90%;">Proc. ICASSP</span><span class="ltx_text" id="bib.bib3.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Nick Rossenbach, Albert Zeyer, Ralf Schlüter, and Hermann Ney,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">“Generating synthetic audio data for attention-based speech recognition systems,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib4.4.2" style="font-size:90%;">Proc. ICASSP</span><span class="ltx_text" id="bib.bib4.5.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Nick Rossenbach, Mohammad Zeineldeen, Benedikt Hilmes, Ralf Schlüter, and Hermann Ney,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">“Comparing the benefit of synthetic training data for various automatic speech recognition architectures,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Enno Hermann and Mathew Magimai.-Doss,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">“Few-shot dysarthric speech recognition with text-to-speech data augmentation,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">Proc. INTERSPEECH</span><span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Virender ”Kadyan, Hemant Kathania, Prajjval Govil, and Mikko” Kurimo,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">“”synthesis speech based data augmentation for low resource children asr”,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="font-size:90%;">”Speech and Computer”</span><span class="ltx_text" id="bib.bib7.5.3" style="font-size:90%;">, 2021, pp. 317–326.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Arun Kumar Singh and Priyanka Singh,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">“Detection of ai-synthesized speech using cepstral &amp; bispectral statistics,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">2021 IEEE 4th International Conference on Multimedia Information Processing and Retrieval (MIPR)</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, 2021, pp. 412–417.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Yinlin Guo, Haofan Huang, Xi Chen, He Zhao, and Yuehai Wang,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">“Audio deepfake detection with self-supervised wavlm and multi-fusion attentive classifier,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib9.4.2" style="font-size:90%;">Proc. ICASSP</span><span class="ltx_text" id="bib.bib9.5.3" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Andrew Rosenberg, Yu Zhang, Bhuvana Ramabhadran, Ye Jia, Pedro Moreno, Yonghui Wu, and Zelin Wu,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">“Speech recognition with augmented synthesized speech,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, 2019, pp. 996–1002.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
“Coqui tts - a library for advanced text-to-speech generation,” </span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/coqui-ai/TTS" style="font-size:90%;" title="">https://github.com/coqui-ai/TTS</a><span class="ltx_text" id="bib.bib11.2.2" style="font-size:90%;">, 2023,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.3.1" style="font-size:90%;">Accessed: 14/05/2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Andrew Werchniak, Roberto Barra Chicote, Yuriy Mishchenko, Jasha Droppo, Jeff Condal, Peng Liu, and Anish Shah,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">“Exploring the application of synthetic audio in training keyword spotters,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib12.4.2" style="font-size:90%;">Proc. ICASSP</span><span class="ltx_text" id="bib.bib12.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Chol-Jin Han, Un-Chol Ri, Song-Il Mun, Kang-Song Jang, and Song-Yun Han,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">“An end-to-end tts model with pronunciation predictor,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">International Journal of Speech Technology</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, vol. 25, pp. 1013–1024, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Eva Sharma, Guoli Ye, Wenning Wei, Rui Zhao, Yao Tian, Jian Wu, Lei He, Ed Lin, and Yifan Gong,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">“Adaptation of rnn transducer with text-to-speech technology for keyword spotting,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">Proc. ICASSP</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Yixin Gao, Yuriy Mishchenko, Anish Shah, Spyros Matsoukas, and Shiv Vitaladevuni,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">“Towards data efficient modeling for wake word spotting,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">Proc. ICASSP</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, 2020, pp. 7479–7483.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Abhijeet Awasthi, Kevin Kilgour, and Hassan Rom,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">“Teaching keyword spotters to spot new keywords with limited examples,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">Proc. INTERSPEECH</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
James Lin, Kevin Kilgour, Dominik Roblek, and Matthew Sharifi,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">“Training keyword spotters with limited and synthesized speech data,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">Proc. ICASSP</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, 2020, pp. 7474–7478.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">“Common voice: A massively-multilingual speech corpus,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib18.4.2" style="font-size:90%;">arXiv:1912.06670</span><span class="ltx_text" id="bib.bib18.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, Patrice Castonguay, Mariya Popova, Jocelyn Huang, and Jonathan M. Cohen,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">“Nemo: a toolkit for building ai applications using neural modules,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">arXiv:1909.09577</span><span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Pete Warden,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">“Speech commands: A dataset for limited-vocabulary speech,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">arXiv:1804.03209</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Byeonggeun Kim, Simyung Chang, Jinkyu Lee, and Dooyong Sung,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">“Broadcasted residual learning for efficient keyword spotting,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib21.4.2" style="font-size:90%;">arXiv:2106.04140</span><span class="ltx_text" id="bib.bib21.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Somshubra Majumdar and Boris Ginsburg,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">“Matchboxnet: 1d time-channel separable convolutional neural network architecture for speech commands recognition,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">Proc. INTERSPEECH</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, 2022, pp. 3356–3360.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung yi Lee, and Shinji Watanabe,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">“Ml-superb: Multilingual speech universal performance benchmark,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">arXiv:2305.10615</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu, Michael Zeng, Xiangzhan Yu, and Furu Wei,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">“Wavlm: Large-scale self-supervised pre-training for full stack speech processing,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">IEEE Journal of Selected Topics in Signal Processing</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, vol. 16, no. 6, pp. 1505–1518, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
David Snyder, Daniel Garcia-Romero, Daniel Povey, and Sanjeev Khudanpur,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">“Deep neural network embeddings for text-independent speaker verification,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib25.4.2" style="font-size:90%;">Proc. INTERSPEECH</span><span class="ltx_text" id="bib.bib25.5.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">“Unpaired image-to-image translation using cycle-consistent adversarial networks,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.3.1" style="font-size:90%;">in </span><span class="ltx_text ltx_font_italic" id="bib.bib26.4.2" style="font-size:90%;">2017 IEEE International Conference on Computer Vision (ICCV)</span><span class="ltx_text" id="bib.bib26.5.3" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le,
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">“Flow matching for generative modeling,”
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">arXiv preprint arXiv:2210.02747</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 19 13:04:41 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
