<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?</title>
<!--Generated on Thu Jun 20 12:44:10 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.14267v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S1" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Current Evaluation Practices in Multilingual NLP</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS1" title="In 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Multilingual evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2" title="In 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Limitations</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS1" title="In 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Tasks and languages</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS1.Px1" title="In 2.2.1 Tasks and languages ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Focus on task complexity rather than language coverage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS1.Px2" title="In 2.2.1 Tasks and languages ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Language coverage is highly skewed to few tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS1.Px3" title="In 2.2.1 Tasks and languages ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Language diversity covered by each task varies</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS2" title="In 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Dataset construction</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS2.Px1" title="In 2.2.2 Dataset construction ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Domain and cross-lingual transfer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS2.Px2" title="In 2.2.2 Dataset construction ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Naturally occurring or parallel data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS2.Px3" title="In 2.2.2 Dataset construction ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Professional or machine translated data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS3" title="In 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Interpreting and reporting results</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS3.Px1" title="In 2.2.3 Interpreting and reporting results ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">High- vs. low-resource language categorization differs per model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2.SSS3.Px2" title="In 2.2.3 Interpreting and reporting results ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">No strong baseline for comparison.</span></a></li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS1" title="In 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Tasks and Datasets</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS1.SSS0.Px1" title="In 3.1 Tasks and Datasets ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">XNLI</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS1.SSS0.Px2" title="In 3.1 Tasks and Datasets ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">PAWS-X</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS1.SSS0.Px3" title="In 3.1 Tasks and Datasets ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">XCOPA</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS1.SSS0.Px4" title="In 3.1 Tasks and Datasets ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">XStorycloze</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS2" title="In 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Language Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS2.SSS0.Px1" title="In 3.2 Language Models ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS3" title="In 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Task Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS3.SSS0.Px1" title="In 3.3 Task Evaluation ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Zero-shot testing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS3.SSS0.Px2" title="In 3.3 Task Evaluation ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Zero-shot prompting</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS4" title="In 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Machine Translation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS4.SSS0.Px1" title="In 3.4 Machine Translation ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Evaluation metric</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.SS5" title="In 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Selection of Test Languages</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S4" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results on Machine Translation Quality</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S4.SS0.SSS0.Px1" title="In 4 Results on Machine Translation Quality ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">ChrF++ scores</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S4.SS0.SSS0.Px2" title="In 4 Results on Machine Translation Quality ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Comparison to professional translations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S4.SS0.SSS0.Px3" title="In 4 Results on Machine Translation Quality ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Correlation between translation quality and model performance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S5" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Large-scale Evaluation Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S5.SS1" title="In 5 Large-scale Evaluation Results ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Representativeness of Selected Subsets of Languages</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S5.SS2" title="In 5 Large-scale Evaluation Results ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Baseline Performance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S6" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions and Recommendations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Experimental setups</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1.SS0.SSS0.Px1" title="In Appendix A Experimental setups ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">XLM-R fine-tuning details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1.SS0.SSS0.Px2" title="In Appendix A Experimental setups ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">BLOOMz and AYA zero-shot prompts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1.SS0.SSS0.Px3" title="In Appendix A Experimental setups ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title">Baseline fine-tuning details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A2" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Correlation between Chrf++ scores and translations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A3" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Lessons learned for MT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A4" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Baselines for PAWS-X</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A5" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Full results XLM-R Large</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A6" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Full results BLOOMz</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A7" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>ChrF++ scores per language</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A8" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Language coverage</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A9" title="In On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>Full Results for XLM-R base</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">On the Evaluation Practices in Multilingual NLP:
<br class="ltx_break"/>Can Machine Translation Offer an Alternative to Human Translations?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rochelle Choenni<math alttext="~{}~{}^{\spadesuit}" class="ltx_Math" display="inline" id="id2.2.m1.1"><semantics id="id2.2.m1.1a"><msup id="id2.2.m1.1.1" xref="id2.2.m1.1.1.cmml"><mi id="id2.2.m1.1.1a" xref="id2.2.m1.1.1.cmml"></mi><mi id="id2.2.m1.1.1.1" mathvariant="normal" xref="id2.2.m1.1.1.1.cmml">♠</mi></msup><annotation-xml encoding="MathML-Content" id="id2.2.m1.1b"><apply id="id2.2.m1.1.1.cmml" xref="id2.2.m1.1.1"><ci id="id2.2.m1.1.1.1.cmml" xref="id2.2.m1.1.1.1">♠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m1.1c">~{}~{}^{\spadesuit}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m1.1d">start_FLOATSUPERSCRIPT ♠ end_FLOATSUPERSCRIPT</annotation></semantics></math>     Sara Rajaee<span class="ltx_note ltx_role_footnotemark" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotemark: </span></span></span></span><math alttext="~{}~{}^{\diamondsuit}" class="ltx_Math" display="inline" id="id3.3.m2.1"><semantics id="id3.3.m2.1a"><msup id="id3.3.m2.1.1" xref="id3.3.m2.1.1.cmml"><mi id="id3.3.m2.1.1a" xref="id3.3.m2.1.1.cmml"></mi><mi id="id3.3.m2.1.1.1" mathvariant="normal" xref="id3.3.m2.1.1.1.cmml">♢</mi></msup><annotation-xml encoding="MathML-Content" id="id3.3.m2.1b"><apply id="id3.3.m2.1.1.cmml" xref="id3.3.m2.1.1"><ci id="id3.3.m2.1.1.1.cmml" xref="id3.3.m2.1.1.1">♢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id3.3.m2.1c">~{}~{}^{\diamondsuit}</annotation><annotation encoding="application/x-llamapun" id="id3.3.m2.1d">start_FLOATSUPERSCRIPT ♢ end_FLOATSUPERSCRIPT</annotation></semantics></math>     Christof Monz<sup class="ltx_sup" id="id8.8.id1"><span class="ltx_text ltx_font_italic" id="id8.8.id1.1">♢</span></sup>    
Ekaterina Shutova<sup class="ltx_sup" id="id9.9.id2">♠</sup>
<br class="ltx_break"/><math alttext="~{}^{\spadesuit}" class="ltx_Math" display="inline" id="id6.6.m5.1"><semantics id="id6.6.m5.1a"><msup id="id6.6.m5.1.1" xref="id6.6.m5.1.1.cmml"><mi id="id6.6.m5.1.1a" xref="id6.6.m5.1.1.cmml"></mi><mi id="id6.6.m5.1.1.1" mathvariant="normal" xref="id6.6.m5.1.1.1.cmml">♠</mi></msup><annotation-xml encoding="MathML-Content" id="id6.6.m5.1b"><apply id="id6.6.m5.1.1.cmml" xref="id6.6.m5.1.1"><ci id="id6.6.m5.1.1.1.cmml" xref="id6.6.m5.1.1.1">♠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id6.6.m5.1c">~{}^{\spadesuit}</annotation><annotation encoding="application/x-llamapun" id="id6.6.m5.1d">start_FLOATSUPERSCRIPT ♠ end_FLOATSUPERSCRIPT</annotation></semantics></math>ILLC, University of Amsterdam, the Netherlands 
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.id3">♢</sup>Language Technology Lab, University of Amsterdam, the Netherlands 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id11.11.id4">{r.m.v.k.choenni, s.rajaee, c.monz, e.shutova}@uva.nl
<br class="ltx_break"/></span>
</span><span class="ltx_author_notes">  The authors ordered alphabetically with an equal contribution.</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">While multilingual language models (MLMs) have been trained on 100+ languages, they are typically only evaluated across a handful of them due to a lack of available test data in most languages. This is particularly problematic when assessing MLM’s potential for low-resource and unseen languages. In this paper, we present an analysis of existing evaluation frameworks in multilingual NLP, discuss their limitations, and propose several directions for more robust and reliable evaluation practices. Furthermore, we empirically study to what extent machine translation offers a reliable alternative to human translation for large-scale evaluation of MLMs across a wide set of languages. We use a SOTA translation model to translate test data from 4 tasks to 198 languages and use them to evaluate three MLMs. We show that while the selected subsets of high-resource test languages are generally sufficiently representative of a wider range of high-resource languages, we tend to overestimate MLMs’ ability on low-resource languages. Finally, we show that simpler baselines can achieve relatively strong performance without having benefited from large-scale multilingual pretraining. <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The code and translated data are available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/Sara-Rajaee/mt4multilinguality" title="">https://github.com/Sara-Rajaee/mt4multilinguality</a></span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The field of multilingual NLP has seen rapid advances in recent years, both in terms of performance and the coverage of languages. Abundant research on multilingual word embeddings, such as MUSE <cite class="ltx_cite ltx_citemacro_citep">(Lample et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib24" title="">2018</a>)</cite> and FastText <cite class="ltx_cite ltx_citemacro_citep">(Bojanowski et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib6" title="">2017</a>)</cite>, quickly gave way to contextualized language models (LMs), where popular models such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Kenton and Toutanova, <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib19" title="">2019</a>)</cite> and GPT <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib8" title="">2020</a>)</cite>, were extended to the multilingual setting <cite class="ltx_cite ltx_citemacro_citep">(Shliazhko et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib52" title="">2022</a>)</cite>. Due to the LMs reliance on subword tokenization, their vocabularies could naturally be expanded to cover many languages and writing scripts without exploding the model size <cite class="ltx_cite ltx_citemacro_cite">Sennrich et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib51" title="">2016</a>); Kudo and Richardson (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib22" title="">2018</a>)</cite>. Nowadays, we have multilingual language models (MLMs) that have seen 100+ languages during pretraining <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib10" title="">2020</a>)</cite>.
In this regard, much effort has been put into enhancing MLMs, i.e., building stronger and larger models with a higher language coverage during pretraining. However, to what extent are our standard practices for the evaluation of MLMs accurate and comprehensive? In this work, we study the current evaluation practices, including multilingual benchmarks, evaluation setups, and performance interpretation, and discuss their limitations.
</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In particular, our analysis of popular multilingual evaluation tasks shows that previous works have mostly followed the trends in monolingual NLP in their efforts to scale evaluation to more complex tasks <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib26" title="">2020</a>; Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib18" title="">2020</a>; Ruder et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib48" title="">2021b</a>)</cite>. This has left a big gap in the literature on scaling MLM evaluation to cover more test languages <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib39" title="">2020</a>; Srinivasan et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib53" title="">2021</a>; Ahuja et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib1" title="">2022</a>; Patankar et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib36" title="">2022</a>; Hada et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib15" title="">2023</a>; Dac Lai et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib12" title="">2023</a>; Ploeger et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib38" title="">2024</a>)</cite>.
While recent MLMs have been shown to perform well on a variety of tasks <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib64" title="">2021</a>); He et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib16" title="">2023</a>)</cite>, current benchmarks still have a limited language coverage, which means that most languages seen during pretraining have never been evaluated on in any task. Moreover, the set of languages that are covered by each task varies considerably. This makes it difficult to systematically compare the abilities of MLMs across languages and tasks. In addition, it has been questioned whether these language subsets are representative for generalization to a wide range of typologically diverse <cite class="ltx_cite ltx_citemacro_citep">(Ploeger et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib38" title="">2024</a>)</cite> and unseen <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib39" title="">2020</a>)</cite> languages. Thus, the applicability of MLMs in creating language technology for the majority of languages is still unexplored.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we address this problem by massively scaling the number of test languages to 198 languages, using NLLB ("No Language Left Behind") <cite class="ltx_cite ltx_citemacro_cite">NLLB Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib34" title="">2022</a>)</cite>, a SOTA machine translation (MT) model. Given that considerable progress has been made in MT in recent years, we believe that it is worthwhile to reassess the reliability of translated data for evaluation <cite class="ltx_cite ltx_citemacro_citep">(Ranathunga et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib45" title="">2023</a>; Popel et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib42" title="">2020</a>)</cite>. We compare performance of three popular MLMs, i.e., XLM-R (both base and large versions) <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib10" title="">2020</a>)</cite>, BLOOMz <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib31" title="">2022</a>)</cite> and AYA-101 <cite class="ltx_cite ltx_citemacro_citep">(Üstün et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib59" title="">2024</a>)</cite>, across 4 tasks and two evaluation frameworks (fine-tuning and zero-shot prompting). Our analysis shows that differences in performance on the machine and human-translated data are negligible, hence, we believe that MT can offer a reliable alternative to human translation to estimate the generalization capabilities of MLMs across a wide range of languages.
</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We use the high language coverage in our translated datasets to study to what extent the selected language subsets in the original datasets are representative.
We find that language coverage tends to be representative of high-resource languages, but for low-resource languages,
current selections overestimate MLM ability by up to <math alttext="7\%" class="ltx_Math" display="inline" id="S1.p4.1.m1.1"><semantics id="S1.p4.1.m1.1a"><mrow id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mn id="S1.p4.1.m1.1.1.2" xref="S1.p4.1.m1.1.1.2.cmml">7</mn><mo id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><csymbol cd="latexml" id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1">percent</csymbol><cn id="S1.p4.1.m1.1.1.2.cmml" type="integer" xref="S1.p4.1.m1.1.1.2">7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">7\%</annotation><annotation encoding="application/x-llamapun" id="S1.p4.1.m1.1d">7 %</annotation></semantics></math>.
Moreover, we find that performance across unseen languages can be surprisingly high compared to a random baseline.
To put these scores into perspective, we compare MLMs to less powerful baselines and find that simpler models achieve similar results.
This sheds doubt on whether unseen languages benefit from large-scale multilingual pretraining.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our contributions can be summarized as follows: (1) We provide a comprehensive overview of the current evaluation practices in multilingual NLP and their limitations and offer a set of recommendations.
(2) We release our translated test sets and propose a general framework for machine-translating datasets.
(3) Based on our translated test sets, we show that the performance on low-resource languages covered by popular datasets is not sufficiently
representative for a wide range of languages. To the best of our knowledge, we are the first to conduct a large-scale evaluation of MLMs using MT. We hope that our contributions can aid a more comprehensive evaluation of MLMs. </p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Current Evaluation Practices in Multilingual NLP</h2>
<figure class="ltx_figure" id="S2.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.F1.1" style="width:413.7pt;height:118.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.9pt,3.1pt) scale(0.95,0.95) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="173" id="S2.F1.1.g1" src="extracted/5680801/figures/language_coverage_pertask.png" width="598"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We report the datasets included in each benchmark along with the number of languages that they cover. The datasets are color-coded by type of task: <span class="ltx_text" id="S2.F1.6.1" style="color:#82204A;">classification</span>, <span class="ltx_text" id="S2.F1.7.2" style="color:#5C85FF;">retrieval</span>, <span class="ltx_text" id="S2.F1.8.3" style="color:#EF7B45;">question answering</span>, or <span class="ltx_text" id="S2.F1.9.4" style="color:#558C8C;">structured prediction.</span></figcaption>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Multilingual evaluation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Early methods in NLP relied on supervised learning, which is dependent on the availability of manually annotated datasets, which are lacking for most languages <cite class="ltx_cite ltx_citemacro_citep">(O’Horan et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib35" title="">2016</a>)</cite>. This made it difficult to extend NLP technology to more languages, and as a result, two learning approaches that limit the need for annotated data emerged: language transfer <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib41" title="">2018</a>)</cite> and joint multilingual learning <cite class="ltx_cite ltx_citemacro_citep">(Navigli and Ponzetto, <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib32" title="">2012</a>; Ammar et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib2" title="">2016</a>)</cite>. While the former enables the transfer of models from high to low-resource languages, the latter jointly learns from annotated examples in multiple languages to leverage language commonalities.
Recently, the rise of Transformers <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib60" title="">2017</a>)</cite> and subword segmentation coupled with multilingual joint learning on the self-supervised masked language modeling task (which does not require annotated data), led to the first large-scale MLMs covering 100+ languages.
</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Yet, while MLMs circumvent the need for annotated data during pretraining, we still require such data for fine-tuning and evaluation on downstream tasks. Thus, two evaluation approaches have been standardized that do not require large datasets, namely zero-shot and few-shot testing. In the former, the model is fine-tuned on an available dataset for a high-resource language only (typically English) and tested in a target language without having seen any annotated example from it. In the few-shot setup, we instead feed the model with data in the test language but only show <em class="ltx_emph ltx_font_italic" id="S2.SS1.p2.1.1">few</em> examples during fine-tuning. Even more recent LLMs like BLOOM <cite class="ltx_cite ltx_citemacro_citep">(Scao et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib50" title="">2022</a>)</cite> have been evaluated using zero-shot prompting, where no examples were seen from any language for the task.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">While zero-shot and few-shot settings further mitigate the need for annotated data, we still require annotations for testing. As such, researchers started translating English datasets that evaluate the models’ ability on a wide variety of semantic and syntactic abilities to multiple languages. However, manual translation is time-consuming, and this has forced the community to focus on a selection of tasks and languages only. Moreover, it has become standard practice to machine translate the training set but use human translation for test sets. To help standardize the evaluation of MLMs, a range of benchmarks have been proposed, e.g., XGLUE <cite class="ltx_cite ltx_citemacro_citep">(Liang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib26" title="">2020</a>)</cite>, XTREME <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib18" title="">2020</a>)</cite>, XTREME-R <cite class="ltx_cite ltx_citemacro_cite">Ruder et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib48" title="">2021b</a>)</cite> and XTREME-UP <cite class="ltx_cite ltx_citemacro_citep">(Ruder et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib46" title="">2023</a>)</cite>. These benchmarks encompass a careful selection of challenging multilingual datasets that should provide a comprehensive view of the models’ linguistic capabilities. Reporting performance on individual tasks within a benchmark has now been widely accepted as the gold standard for multilingual evaluation.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Limitations</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Tasks and languages</h4>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Focus on task complexity rather than language coverage</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p1.1">As more powerful MLMs have been developed, tasks for which a handful of languages achieved high performance have quickly been replaced with tasks that are perceived as more difficult <cite class="ltx_cite ltx_citemacro_citep">(Ruder et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib47" title="">2021a</a>)</cite>.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.F1" title="Figure 1 ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">1</span></a>, we depict the evolution of some of the most popular multilingual benchmarks towards more challenging and diverse tasks.
In particular, we see that the field has mostly moved away from classification tasks and replaced them with question-answering (e.g., XQuAD <cite class="ltx_cite ltx_citemacro_citep">(Artetxe et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib3" title="">2020</a>)</cite> and MLQA <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib25" title="">2019</a>)</cite>) and retrieval (e.g., Tatoeba <cite class="ltx_cite ltx_citemacro_citep">(Tiedemann, <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib57" title="">2020</a>)</cite>, BUCC <cite class="ltx_cite ltx_citemacro_cite">Zweigenbaum et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib67" title="">2017</a>)</cite>) tasks.
For instance, PAWS-X <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib65" title="">2019</a>)</cite>, a paraphrasing dataset, has been discarded from the XTREME successor, XTREME-R, as it has been claimed that MLMs already beat human performance, see e.g. <cite class="ltx_cite ltx_citemacro_citet">Tedeschi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib56" title="">2023</a>)</cite> for reservations on this.
However, as the performance on such tasks has in fact never been tested for most languages, we do not believe that we can label them as solved just yet. We argue that in order to discard a task, we need to have reported high performance on at least the languages seen during pretraining.
</p>
</div>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.F2.1" style="width:370.2pt;height:132.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.7pt,11.7pt) scale(0.85,0.85) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="300" id="S2.F2.1.g1" src="x1.png" width="830"/>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Number of test languages for each task and the average typological diversity score between them computed as the average cosine similarity between URIEL features of each language pair.</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Language coverage is highly skewed to few tasks</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p1.1">The XGLUE, XTREME, and XTREME-R benchmarks span 19, 40, and 50 languages, respectively. However, while the benchmarks span <math alttext="\pm" class="ltx_Math" display="inline" id="S2.SS2.SSS1.Px2.p1.1.m1.1"><semantics id="S2.SS2.SSS1.Px2.p1.1.m1.1a"><mo id="S2.SS2.SSS1.Px2.p1.1.m1.1.1" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.Px2.p1.1.m1.1b"><csymbol cd="latexml" id="S2.SS2.SSS1.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.Px2.p1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.Px2.p1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.Px2.p1.1.m1.1d">±</annotation></semantics></math>20+ languages, it is important to consider that in each benchmark not all tasks cover the same amount of languages. In fact, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.F1" title="Figure 1 ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">1</span></a>, we show that the distribution of covered languages is highly skewed towards only a few tasks. For instance, while XTREME-R spans 50 languages, out of the ten proposed tasks, only three, i.e., UD-POS (POS-tagging), WikiANN-NER <cite class="ltx_cite ltx_citemacro_citep">(Rahimi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib44" title="">2019</a>)</cite> (named entity recognition), and Tatoeba (retrieval), cover more than 15 languages. In addition, the subsets of languages covered by each task differ considerably, yet the average performance across tasks is often directly compared.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Language diversity covered by each task varies</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p1.1">Apart from taking the number of available test languages into account, we should also pay attention to the typological diversity covered by each task. This is particularly needed when we estimate the performance on low-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib39" title="">2020</a>)</cite>.
Typological diversity can be achieved along many different axes, e.g., syntactic features and geographical distance, etc.  <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib40" title="">2019</a>)</cite>. However, most works underspecify the type of diversity that they consider to obtain a typologically diverse language selection <cite class="ltx_cite ltx_citemacro_citep">(Ploeger et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib38" title="">2024</a>)</cite>.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.F2" title="Figure 2 ‣ Focus on task complexity rather than language coverage ‣ 2.2.1 Tasks and languages ‣ 2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">2</span></a>, we show the differences between the number of test languages of popular tasks and their average typological diversity scores as computed by the cosine similarity between the syntactic URIEL feature vectors from the LANG2VEC library for all test language pairs <cite class="ltx_cite ltx_citemacro_citep">(Littell et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib28" title="">2017</a>)</cite>.
While we find that tasks covering more languages also tend to score slightly higher in terms of typological diversity, there are large differences between datasets.
For instance, while XNLI covers relatively many languages, TyDIQAGolP <cite class="ltx_cite ltx_citemacro_citep">(Clark et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib9" title="">2020</a>)</cite> scores much higher on diversity.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Dataset construction</h4>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Domain and cross-lingual transfer</h5>
<div class="ltx_para" id="S2.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px1.p1.1">As explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS1" title="2.1 Multilingual evaluation ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">2.1</span></a>, multilingual evaluation typically relies on zero-shot or few-shot testing.
However, in some cases, this means that we are testing the model’s ability to perform domain transfer and cross-lingual transfer at the same time <cite class="ltx_cite ltx_citemacro_citep">(Lai et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib23" title="">2019</a>)</cite>.
For instance, the Universal Dependency (UD) project <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib33" title="">Nivre et al., </a>)</cite> offers consistent treebank annotations across 300+ languages and has become a valuable resource for researchers working on dependency parsing and part-of-speech tagging.
Yet, these treebanks come from vastly different data sources.
While the commonly used English treebank named ‘Gum’ contains data from 9 different domains (including Wikipedia, spoken language, fiction, etc.), Yoruba and Wolof contain bible texts only.
Thus, when evaluating performance across languages, this should be taken into consideration, especially as low-resource languages tend to come from different domains more frequently, i.e., spoken language, grammar examples, and bible texts.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://universaldependencies.org/" title="">https://universaldependencies.org/</a>.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Naturally occurring or parallel data</h5>
<div class="ltx_para" id="S2.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px2.p1.1">While the use of parallel data allows for a controlled evaluation setup that alleviates the aforementioned problem of domain transfer, the use of naturally occurring in-language data also has its benefits.
In particular, naturally occurring data allows us to test whether the model can handle the nuances and biases that come with different languages <cite class="ltx_cite ltx_citemacro_citep">(Talat et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib54" title="">2022</a>)</cite>.
For instance, <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib29" title="">2023</a>)</cite> show that proverbs and sayings that occur in different languages are not understood by MLMs, bringing into question whether they can reason in different languages, e.g., ‘The apple doesn’t fall far from the tree’ is understood, yet the model fails on ‘Bamboo shoots are not far from the clump’ a popular saying in Indonesian.
By limiting MLM evaluation to parallel data—with a mostly Western-centric bias—we can not account for such language characteristics and, therefore, risk overestimating our models’ applicability in practice.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px3">
<h5 class="ltx_title ltx_title_paragraph">Professional or machine translated data</h5>
<div class="ltx_para" id="S2.SS2.SSS2.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px3.p1.1">While translation always has the disadvantage of adding some possible noise, machine translation also brings the additional problem of the ‘translationese’ effect <cite class="ltx_cite ltx_citemacro_citep">(Koppel and Ordan, <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib21" title="">2011</a>)</cite>.
This means that in translation, the sentences in the target language more closely resemble the linguistic patterns from the source language, resulting in phrasings that would not typically occur in the target language.
As such, it can simplify cross-lingual generalization and cause us to overestimate a model’s capabilities in the test language <cite class="ltx_cite ltx_citemacro_citep">(Zhang and Toral, <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib66" title="">2019</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3 </span>Interpreting and reporting results</h4>
<section class="ltx_paragraph" id="S2.SS2.SSS3.Px1">
<h5 class="ltx_title ltx_title_paragraph">High- vs. low-resource language categorization differs per model</h5>
<div class="ltx_para" id="S2.SS2.SSS3.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS3.Px1.p1.1">The amount of data that each MLM has seen during pretraining varies.
For instance, for XLM-R, which is pre-trained on CommonCrawl, Indonesian is amongst the top 3 most seen languages <cite class="ltx_cite ltx_citemacro_cite">Conneau et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib10" title="">2020</a>)</cite>, while for mT5 it has the 13th largest dataset <cite class="ltx_cite ltx_citemacro_cite">Xue et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib64" title="">2021</a>)</cite>, and it has only the 22nd largest dataset in Wikipedia used for training mBERT <cite class="ltx_cite ltx_citemacro_cite">Pires et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib37" title="">2019</a>)</cite>.
In addition, each MLM uses upsampling strategies for their data before pretraining.
This means that the categorization of high- and low-resource languages is different for each MLM, and we can not compare performance across MLMs using the same resource-based categorization.
Moreover, the data distribution of pretraining languages is often unclear because (1) developers report this in different formats, e.g., Wikipedia coverage <cite class="ltx_cite ltx_citemacro_citep">(Wu and Dredze, <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib63" title="">2020</a>)</cite>, data size in GB <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib10" title="">2020</a>)</cite>, or percentage towards the full pretraining data <cite class="ltx_cite ltx_citemacro_citep">(Muennighoff et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib31" title="">2022</a>)</cite>, (2) the problem of language contamination <cite class="ltx_cite ltx_citemacro_cite">Blevins and Zettlemoyer (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib5" title="">2022</a>)</cite> which means that texts can contain multiple languages, e.g. through quotations or code-switching, making it impossible to know for certain how much data was seen for each language.
Lastly, language categorization by data-scarcity is further complicated because many MLMs are instruction-tuned using large datasets. While this can further affect the categorization, these numbers are typically not taken into account <cite class="ltx_cite ltx_citemacro_citep">(Üstün et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib59" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS3.Px2">
<h5 class="ltx_title ltx_title_paragraph">No strong baseline for comparison.</h5>
<div class="ltx_para" id="S2.SS2.SSS3.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS3.Px2.p1.1">The performance of MLMs across tasks and languages has usually been compared to previous SOTA models or random baselines.
However, as various works have shown that data artifacts can help the model in obtaining misleadingly high performance <cite class="ltx_cite ltx_citemacro_citep">(McCoy et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib30" title="">2020</a>)</cite>, we question whether random classification is a sufficiently strong baseline <cite class="ltx_cite ltx_citemacro_citep">(Glavaš et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib13" title="">2019</a>)</cite>.
Especially on low-resource languages, we tend to achieve relatively small improvements over random, thus we believe that it is important to keep evaluating our models against less powerful models to assess to what extent these languages have benefited from large-scale multilingual pretraining.</p>
</div>
</section>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In the next part, we address some of the limitations discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2" title="2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">2.2</span></a> by machine translating 4 classification datasets into 198 languages.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tasks and Datasets</h3>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">XNLI</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">The Cross-Lingual Natural Language Inference (XNLI) dataset <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib11" title="">2018</a>)</cite> contains premise-hypothesis pairs labeled with
their relationship: ‘entailment’, ‘neutral’, or ‘contradiction’. The dataset has parallel data in 15 languages.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">PAWS-X</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">The Cross-Lingual Paraphrase Adversaries from Word
Scrambling (PAWS-X) dataset <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib65" title="">2019</a>)</cite> requires the model to determine whether two sentences are paraphrased.The parallel test data has been provided in 7 languages.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">XCOPA</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">The Cross-lingual Choice of Plausible Alternatives (XCOPA) <cite class="ltx_cite ltx_citemacro_citep">(Ponti et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib39" title="">2020</a>)</cite> evaluates common-sense reasoning abilities in 11 different languages. The samples contain a premise and question paired with two answer choices from which the model can select.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">XStorycloze</h5>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">The Cross-lingual Storycloze <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib27" title="">2021</a>)</cite> proposes a common-sense reasoning task in 11 languages, in which the model predicts which one of two story endings is the most likely to follow after a given short story.

<br class="ltx_break"/></p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Language Models</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We evaluate the base and large version of XLM-R pre-trained on 100 languages as it is one of the most popular MLMs.
In addition, we report scores from BLOOMz and AYA-101 (AYA).
BLOOMz is trained on 46 languages and AYA on 101, both models are further instruction-tuned on a mixture of prompts in different languages.
Given that the PAWS-X dataset was included during instruction-tuning, we evaluate BLOOMz and AYA on the held out datasets only. Note that BLOOMz and AYA were selected over other MLMs, such as Llama <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib58" title="">2023</a>)</cite>, as they are the largest publicly available and explicitly MLMs.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Baselines</h5>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">As a baseline, we use a vanilla unidirectional LSTM <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib17" title="">1997</a>)</cite> with a simple MLP classifier on top. While the model is initialized with the FastText word embeddings, the LSTM is only fine-tuned on the English training data for each task.
Importantly, the FastText embeddings were pretrained using monolingual data only and later automatically aligned into a shared multilingual vector space <cite class="ltx_cite ltx_citemacro_cite">Lample et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib24" title="">2018</a>)</cite>, see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1" title="Appendix A Experimental setups ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">A</span></a> for more details.
Thus, using this baseline, we can get a better idea of how much each language has truly benefited from large-scale multilingual pretraining of XLM-R and BLOOMz.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Task Evaluation</h3>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Zero-shot testing</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">We fine-tune XLM-R on the entire training set for each task in English.
We then use our fine-tuned model for zero-shot testing in the test languages.
We fine-tune our model using the HugginFace Library,
see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1" title="Appendix A Experimental setups ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">A</span></a> for details.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Zero-shot prompting</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">BLOOMz and AYA are instruction-tuned on multiple classification tasks, thus we test these models out-of-the-box in a zero-shot prompting set up.
This has the benefit that dataset artifacts, which are commonly known to be leveraged during fine-tuning, can not be learned <cite class="ltx_cite ltx_citemacro_cite">McCoy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib30" title="">2020</a>)</cite>.
As BLOOMz and AYA fail to predict a third option for XNLI (neutral), we report results on a binarized version of the task by aggregating sentences with the ‘neutral’ and ‘contradiction’ labels into one class and making the model predict entailment or not.
Moreover, the instructions are given in English,
see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1" title="Appendix A Experimental setups ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">A</span></a> for the prompts per task. Finally, note that our goal is not to compare performance to XLM-R but rather to test the reliability of machine-translated test sets in two popular evaluation paradigms.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Machine Translation</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">We employ the NLLB model covering 202 languages for translation <cite class="ltx_cite ltx_citemacro_cite">NLLB Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib34" title="">2022</a>)</cite>.
To gain a better understanding of the role of the MT system on the translated data quality and, consequently, the performance on downstream tasks, we experiment with two NLLB versions. We choose the distill NLLB with 600M parameters and the 3.3B NLLB model using greedy sampling. For each example, we translate each sentence (e.g., <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.p1.1.1">sentence1</span> and <span class="ltx_text ltx_font_smallcaps" id="S3.SS4.p1.1.2">sentence2</span> in PAWS-X) separately. In Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A3" title="Appendix C Lessons learned for MT ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">C</span></a>, we provide the lessons learned from our MT experiments.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.6" style="width:211.1pt;height:58.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-56.8pt,15.8pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.6.6">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.6.6.7.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S3.T1.6.6.7.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.6.6.7.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">Unseen</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.6.6.7.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">Low</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.6.6.7.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">Mid</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.6.6.7.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">High</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.6.6.7.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">Total</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.6.6.6">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;">
<math alttext="\%" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">%</annotation></semantics></math> of data</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.6.6.6.7" style="padding-left:6.5pt;padding-right:6.5pt;">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.3.3.3" style="padding-left:6.5pt;padding-right:6.5pt;">
<math alttext="\textgreater" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.2.m1.1a"><mo id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><gt id="S3.T1.2.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.2.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">\textgreater</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.m1.1d">&gt;</annotation></semantics></math> 0 and <math alttext="\textless" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.m2.1"><semantics id="S3.T1.3.3.3.3.m2.1a"><mo id="S3.T1.3.3.3.3.m2.1.1" xref="S3.T1.3.3.3.3.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m2.1b"><lt id="S3.T1.3.3.3.3.m2.1.1.cmml" xref="S3.T1.3.3.3.3.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m2.1c">\textless</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.m2.1d">&lt;</annotation></semantics></math> 0.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.5.5.5.5" style="padding-left:6.5pt;padding-right:6.5pt;">
<math alttext="\geq" class="ltx_Math" display="inline" id="S3.T1.4.4.4.4.m1.1"><semantics id="S3.T1.4.4.4.4.m1.1a"><mo id="S3.T1.4.4.4.4.m1.1.1" xref="S3.T1.4.4.4.4.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.m1.1b"><geq id="S3.T1.4.4.4.4.m1.1.1.cmml" xref="S3.T1.4.4.4.4.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.4.m1.1d">≥</annotation></semantics></math> 0.1 and <math alttext="\textless" class="ltx_Math" display="inline" id="S3.T1.5.5.5.5.m2.1"><semantics id="S3.T1.5.5.5.5.m2.1a"><mo id="S3.T1.5.5.5.5.m2.1.1" xref="S3.T1.5.5.5.5.m2.1.1.cmml">&lt;</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.m2.1b"><lt id="S3.T1.5.5.5.5.m2.1.1.cmml" xref="S3.T1.5.5.5.5.m2.1.1"></lt></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.m2.1c">\textless</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.5.m2.1d">&lt;</annotation></semantics></math>1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.6.6.6.6" style="padding-left:6.5pt;padding-right:6.5pt;">
<math alttext="\geq" class="ltx_Math" display="inline" id="S3.T1.6.6.6.6.m1.1"><semantics id="S3.T1.6.6.6.6.m1.1a"><mo id="S3.T1.6.6.6.6.m1.1.1" xref="S3.T1.6.6.6.6.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.m1.1b"><geq id="S3.T1.6.6.6.6.m1.1.1.cmml" xref="S3.T1.6.6.6.6.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.6.m1.1d">≥</annotation></semantics></math>1</td>
<td class="ltx_td ltx_border_t" id="S3.T1.6.6.6.8" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.8.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S3.T1.6.6.8.1.1" style="padding-left:6.5pt;padding-right:6.5pt;">XLM-R</th>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.8.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">106</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.8.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">30</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.8.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">34</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.8.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">26</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.8.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">196</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.9.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S3.T1.6.6.9.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">BLOOMz</th>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.9.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">131</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.9.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">21</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.9.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">7</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.6.6.9.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">9</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.6.9.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">168</td>
</tr>
<tr class="ltx_tr" id="S3.T1.6.6.10.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S3.T1.6.6.10.3.1" style="padding-left:6.5pt;padding-right:6.5pt;">AYA</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.10.3.2" style="padding-left:6.5pt;padding-right:6.5pt;">103</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.10.3.3" style="padding-left:6.5pt;padding-right:6.5pt;">57</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.10.3.4" style="padding-left:6.5pt;padding-right:6.5pt;">25</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.6.6.10.3.5" style="padding-left:6.5pt;padding-right:6.5pt;">13</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.10.3.6" style="padding-left:6.5pt;padding-right:6.5pt;">198</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Number of languages categorized as high, mid, low, and unseen languages when looking at the percentage of seen pretraining data of the respective LMs.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.3" style="width:457.3pt;height:148.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-158.9pt,51.7pt) scale(0.59,0.59) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T2.3.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">ar</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">bg</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">de</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">el</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">es</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">et</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">eu</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">fr</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.10" style="padding-left:2.0pt;padding-right:2.0pt;">hi</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.11" style="padding-left:2.0pt;padding-right:2.0pt;">ht</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.12" style="padding-left:2.0pt;padding-right:2.0pt;">id</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.13" style="padding-left:2.0pt;padding-right:2.0pt;">it</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.14" style="padding-left:2.0pt;padding-right:2.0pt;">ja</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.15" style="padding-left:2.0pt;padding-right:2.0pt;">ko</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.16" style="padding-left:2.0pt;padding-right:2.0pt;">my</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.17" style="padding-left:2.0pt;padding-right:2.0pt;">qu</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.18" style="padding-left:2.0pt;padding-right:2.0pt;">ru</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.19" style="padding-left:2.0pt;padding-right:2.0pt;">sw</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.20" style="padding-left:2.0pt;padding-right:2.0pt;">ta</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.21" style="padding-left:2.0pt;padding-right:2.0pt;">te</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.22" style="padding-left:2.0pt;padding-right:2.0pt;">th</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.23" style="padding-left:2.0pt;padding-right:2.0pt;">tr</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.24" style="padding-left:2.0pt;padding-right:2.0pt;">ur</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.25" style="padding-left:2.0pt;padding-right:2.0pt;">vi</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.1.1.1.26" style="padding-left:2.0pt;padding-right:2.0pt;">zh</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="25" id="S3.T2.3.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.2.2.2.1">XLM-R</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.1.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">XCOPA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">69/73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.11" style="padding-left:2.0pt;padding-right:2.0pt;">50/57</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.12" style="padding-left:2.0pt;padding-right:2.0pt;">76/69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.13" style="padding-left:2.0pt;padding-right:2.0pt;">75/73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.17" style="padding-left:2.0pt;padding-right:2.0pt;">49/59</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.19" style="padding-left:2.0pt;padding-right:2.0pt;">66/58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.20" style="padding-left:2.0pt;padding-right:2.0pt;">64/68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.22" style="padding-left:2.0pt;padding-right:2.0pt;">69/66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.23" style="padding-left:2.0pt;padding-right:2.0pt;">71/66</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.25" style="padding-left:2.0pt;padding-right:2.0pt;">68/70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.3.3.26" style="padding-left:2.0pt;padding-right:2.0pt;">72/73</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">XStoryCloze</th>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">80/80</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">84/84</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.8" style="padding-left:2.0pt;padding-right:2.0pt;">79/79</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.10" style="padding-left:2.0pt;padding-right:2.0pt;">78/79</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.12" style="padding-left:2.0pt;padding-right:2.0pt;">88/87</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.16" style="padding-left:2.0pt;padding-right:2.0pt;">71/70</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.18" style="padding-left:2.0pt;padding-right:2.0pt;">84/85</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.19" style="padding-left:2.0pt;padding-right:2.0pt;">75/76</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.21" style="padding-left:2.0pt;padding-right:2.0pt;">76/75</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.22" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.23" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.25" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.4.4.26" style="padding-left:2.0pt;padding-right:2.0pt;">87/86</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.5.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">XNLI</th>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">78/78</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">82/82</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">82/82</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">81/80</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">83/78</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.9" style="padding-left:2.0pt;padding-right:2.0pt;">82/83</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.10" style="padding-left:2.0pt;padding-right:2.0pt;">75/80</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.18" style="padding-left:2.0pt;padding-right:2.0pt;">79/82</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.19" style="padding-left:2.0pt;padding-right:2.0pt;">70/74</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.22" style="padding-left:2.0pt;padding-right:2.0pt;">76/76</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.23" style="padding-left:2.0pt;padding-right:2.0pt;">77/79</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.25" style="padding-left:2.0pt;padding-right:2.0pt;">78/81</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.5.5.26" style="padding-left:2.0pt;padding-right:2.0pt;">79/74</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.6.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">PAWS-X</th>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">90/91</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">91/91</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.9" style="padding-left:2.0pt;padding-right:2.0pt;">91/90</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.14" style="padding-left:2.0pt;padding-right:2.0pt;">81/84</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.15" style="padding-left:2.0pt;padding-right:2.0pt;">81/83</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.22" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.23" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.25" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.6.6.26" style="padding-left:2.0pt;padding-right:2.0pt;">83/82</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.7.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.1.7.7.1" style="padding-left:2.0pt;padding-right:2.0pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="25" id="S3.T2.3.1.7.7.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.7.7.2.1">BLOOMz</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.1.8.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">XCOPA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.7" style="padding-left:2.0pt;padding-right:2.0pt;">52/52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.11" style="padding-left:2.0pt;padding-right:2.0pt;">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.12" style="padding-left:2.0pt;padding-right:2.0pt;">78/78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.13" style="padding-left:2.0pt;padding-right:2.0pt;">62/65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.17" style="padding-left:2.0pt;padding-right:2.0pt;">51/52</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.19" style="padding-left:2.0pt;padding-right:2.0pt;">60/63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.20" style="padding-left:2.0pt;padding-right:2.0pt;">75/72</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.22" style="padding-left:2.0pt;padding-right:2.0pt;">N/A</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.23" style="padding-left:2.0pt;padding-right:2.0pt;">50/50</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.25" style="padding-left:2.0pt;padding-right:2.0pt;">80/77</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.8.8.26" style="padding-left:2.0pt;padding-right:2.0pt;">71/67</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.9.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">XStoryCloze</th>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">88/88</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.6" style="padding-left:2.0pt;padding-right:2.0pt;">91/91</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.8" style="padding-left:2.0pt;padding-right:2.0pt;">84/78</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.10" style="padding-left:2.0pt;padding-right:2.0pt;">85/84</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.12" style="padding-left:2.0pt;padding-right:2.0pt;">91/90</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.16" style="padding-left:2.0pt;padding-right:2.0pt;">54/52</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.18" style="padding-left:2.0pt;padding-right:2.0pt;">73/73</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.19" style="padding-left:2.0pt;padding-right:2.0pt;">79/79</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.21" style="padding-left:2.0pt;padding-right:2.0pt;">74/73</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.22" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.23" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.25" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.9.9.26" style="padding-left:2.0pt;padding-right:2.0pt;">70/70</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.10.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">B-NLI</th>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.2" style="padding-left:2.0pt;padding-right:2.0pt;">71/72</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.3" style="padding-left:2.0pt;padding-right:2.0pt;">66/68</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.4" style="padding-left:2.0pt;padding-right:2.0pt;">69/68</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.5" style="padding-left:2.0pt;padding-right:2.0pt;">65/66</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.6" style="padding-left:2.0pt;padding-right:2.0pt;">73/74</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.9" style="padding-left:2.0pt;padding-right:2.0pt;">72/73</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.10" style="padding-left:2.0pt;padding-right:2.0pt;">70/72</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.18" style="padding-left:2.0pt;padding-right:2.0pt;">69/70</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.19" style="padding-left:2.0pt;padding-right:2.0pt;">70/71</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.22" style="padding-left:2.0pt;padding-right:2.0pt;">N/A</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.23" style="padding-left:2.0pt;padding-right:2.0pt;">68/70</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.25" style="padding-left:2.0pt;padding-right:2.0pt;">72/73</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.10.10.26" style="padding-left:2.0pt;padding-right:2.0pt;">74/72</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.11.11">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.1.11.11.1" style="padding-left:2.0pt;padding-right:2.0pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="25" id="S3.T2.3.1.11.11.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.3.1.11.11.2.1">AYA</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.1.12.12.1" style="padding-left:2.0pt;padding-right:2.0pt;">XCOPA</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.6" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.7" style="padding-left:2.0pt;padding-right:2.0pt;">87/84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.11" style="padding-left:2.0pt;padding-right:2.0pt;">82/83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.12" style="padding-left:2.0pt;padding-right:2.0pt;">87/87</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.13" style="padding-left:2.0pt;padding-right:2.0pt;">88/88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.17" style="padding-left:2.0pt;padding-right:2.0pt;">56/56</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.19" style="padding-left:2.0pt;padding-right:2.0pt;">79/83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.20" style="padding-left:2.0pt;padding-right:2.0pt;">86/83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.22" style="padding-left:2.0pt;padding-right:2.0pt;">84/82</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.23" style="padding-left:2.0pt;padding-right:2.0pt;">86/85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.25" style="padding-left:2.0pt;padding-right:2.0pt;">85/84</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.1.12.12.26" style="padding-left:2.0pt;padding-right:2.0pt;">86/84</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.1.13.13.1" style="padding-left:2.0pt;padding-right:2.0pt;">XStoryCloze</th>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.2" style="padding-left:2.0pt;padding-right:2.0pt;">95/92</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.6" style="padding-left:2.0pt;padding-right:2.0pt;">94/94</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.8" style="padding-left:2.0pt;padding-right:2.0pt;">83/75</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.10" style="padding-left:2.0pt;padding-right:2.0pt;">93/91</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.12" style="padding-left:2.0pt;padding-right:2.0pt;">91/87</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.16" style="padding-left:2.0pt;padding-right:2.0pt;">94/86</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.18" style="padding-left:2.0pt;padding-right:2.0pt;">90/82</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.19" style="padding-left:2.0pt;padding-right:2.0pt;">93/89</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.21" style="padding-left:2.0pt;padding-right:2.0pt;">93/88</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.22" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.23" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.25" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.1.13.13.26" style="padding-left:2.0pt;padding-right:2.0pt;">95/ 90</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.1.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.3.1.14.14.1" style="padding-left:2.0pt;padding-right:2.0pt;">B-NLI</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.2" style="padding-left:2.0pt;padding-right:2.0pt;">78/79</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.3" style="padding-left:2.0pt;padding-right:2.0pt;">79/79</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.4" style="padding-left:2.0pt;padding-right:2.0pt;">78/78</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.5" style="padding-left:2.0pt;padding-right:2.0pt;">78/78</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.6" style="padding-left:2.0pt;padding-right:2.0pt;">79/80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.9" style="padding-left:2.0pt;padding-right:2.0pt;">79/80</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.10" style="padding-left:2.0pt;padding-right:2.0pt;">75/75</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.18" style="padding-left:2.0pt;padding-right:2.0pt;">79/79</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.19" style="padding-left:2.0pt;padding-right:2.0pt;">74/75</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.22" style="padding-left:2.0pt;padding-right:2.0pt;">79/79</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.23" style="padding-left:2.0pt;padding-right:2.0pt;">79/79</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.25" style="padding-left:2.0pt;padding-right:2.0pt;">76/77</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.1.14.14.26" style="padding-left:2.0pt;padding-right:2.0pt;">77/77</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The (<math alttext="\%" class="ltx_Math" display="inline" id="S3.T2.2.m1.1"><semantics id="S3.T2.2.m1.1b"><mo id="S3.T2.2.m1.1.1" xref="S3.T2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.m1.1c"><csymbol cd="latexml" id="S3.T2.2.m1.1.1.cmml" xref="S3.T2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.m1.1d">\%</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.m1.1e">%</annotation></semantics></math>) accuracy of the models on the human translated (original)/our machine translated datasets.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:437.9pt;height:50.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-93.8pt,10.8pt) scale(0.7,0.7) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T3.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S3.T3.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.2.1">XLM-R</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S3.T3.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.3.1">BLOOMz</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S3.T3.1.1.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.4.1">AYA</span></th>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.2.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.2.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.2.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">PAWS-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.2.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.2.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.2.2.8" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.2.2.9" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.2.2.10" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.1.1.2.2.11" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.3.1">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S3.T3.1.1.3.1.1" style="padding-left:6.5pt;padding-right:6.5pt;">Pearson corr.</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">95.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">69.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">93.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">98.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">85.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">97.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.1.8" style="padding-left:6.5pt;padding-right:6.5pt;">98.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1.9" style="padding-left:6.5pt;padding-right:6.5pt;">98.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1.10" style="padding-left:6.5pt;padding-right:6.5pt;">95.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.1.3.1.11" style="padding-left:6.5pt;padding-right:6.5pt;">90.7</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.2">
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S3.T3.1.1.4.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">Spearman rank corr.</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.4.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">77.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.4.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">82.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.4.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">82.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T3.1.1.4.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">98.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.4.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">89.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.4.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">95.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T3.1.1.4.2.8" style="padding-left:6.5pt;padding-right:6.5pt;">97.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.4.2.9" style="padding-left:6.5pt;padding-right:6.5pt;">81.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.4.2.10" style="padding-left:6.5pt;padding-right:6.5pt;">92.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.1.4.2.11" style="padding-left:6.5pt;padding-right:6.5pt;">77.0</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>
Pearson and Spearman rank correlation between human- (original data) and machine-translated data (ours). </figcaption>
</figure>
<section class="ltx_paragraph" id="S3.SS4.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Evaluation metric</h5>
<div class="ltx_para" id="S3.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS4.SSS0.Px1.p1.1">As an evaluation metric for testing our machine translation quality, we use the chrF++ metric <cite class="ltx_cite ltx_citemacro_cite">Popović (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib43" title="">2017</a>)</cite>. chrF++ calculates the character and word n-gram overlap between the machine and human reference translation.
It is a tokenization-independent metric aligning better with human judgments for morphologically-rich languages compared to BLEU <cite class="ltx_cite ltx_citemacro_cite">Tan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib55" title="">2015</a>); Kocmi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib20" title="">2021</a>); Briakou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib7" title="">2023</a>)</cite>. For the evaluation, we use the FLORES-200 dataset <cite class="ltx_cite ltx_citemacro_cite">NLLB Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib34" title="">2022</a>)</cite>, which includes human-translated data for 200 languages, and select 100 sentences from each language.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Selection of Test Languages</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">For our selection of test languages, there are two constraining factors: (1) the language has to be covered by the NLLB-200 translation model and FLORES-200 dataset, and (2) the script of the language needs to have been seen during the pretraining of the model.
We then separately filter out the test languages by unseen scripts for each model.
This leaves us with 196, 168, and 198 test languages for XLM-R, BLOOMz, and AYA, respectively, see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A8" title="Appendix H Language coverage ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">H</span></a> for the complete lists.
Note that the number of compatible languages is lower for BLOOMz as it has seen fewer writing scripts during pretraining.
Moreover, we categorize the test languages for each model separately based on the percentage of total data that they contributed during pretraining.
In Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.T1" title="Table 1 ‣ 3.4 Machine Translation ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">1</span></a>, we report the percentage thresholds used for our categorization and the resulting number of test languages for each category and model.
As the pretraining data coverage is reported in numbers of GB for XLM-R, we convert these scores to percentages of the full pretraining data.
For BLOOMz, we use the reported language distribution numbers <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/bigscience/bloom" title="">https://huggingface.co/bigscience/bloom</a></span></span></span>, and for AYA, we consider mT5 pretraining data distribution as it is utilized as the base model for AYA.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results on Machine Translation Quality</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">MT systems are known to have problems <cite class="ltx_cite ltx_citemacro_citep">(Artetxe et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib3" title="">2020</a>)</cite> such as hallucination <cite class="ltx_cite ltx_citemacro_citep">(Wang and Sennrich, <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib61" title="">2020</a>)</cite>, or translationese output, but recent models have shown considerable advances, especially on low-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Ranathunga et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib45" title="">2023</a>)</cite>. To ensure that this is indeed the case, we now validate the high quality of our translations using three different analyses.</p>
</div>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">ChrF++ scores</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1.T6" title="Table 6 ‣ Baseline fine-tuning details ‣ Appendix A Experimental setups ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">6</span></a>, we report the average chrF++ scores for translations obtained with the NLLB-Distil and NLLB-3.3B models on the dev set of FLORES-200 dataset including human translation data in 200 languages <cite class="ltx_cite ltx_citemacro_cite">NLLB Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib34" title="">2022</a>)</cite>.
We categorize languages by XLM-R resource ranking (high, mid, low, and unseen) and observe that while the quality for high-resource languages is comparable, for low-resource languages, NLLB-3.3B tends to perform substantially better.
Thus, in all further experiments, we use NLLB-3.3B for translation. Moreover, we confirm that our scores for all languages are among the best performances of the SOTA multilingual MT systems <cite class="ltx_cite ltx_citemacro_cite">Bapna et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib4" title="">2022</a>); NLLB Team et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib34" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Comparison to professional translations</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">To test to what extent machine-translated data causes differences in performance compared to the professionally translated data, we run an evaluation on both subsets.
In Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A9.T12" title="Table 12 ‣ Appendix I Full Results for XLM-R base ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">12</span></a>, we report the performance obtained using machine and professionally translated data, respectively.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Results of XLM-R base are reported in the appendix. </span></span></span>
We find that on average, the difference is only 2.6% in accuracy scores.
Moreover, we see that the same trend holds for all models.
Thus, in both evaluation setups, the effect of MT quality is relatively small, making MT an acceptable alternative to human translation.
Finally, in some cases, we find that MT slightly increases performance. We suspect that this is due to the ‘translationese’ effect explained in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S2.SS2" title="2.2 Limitations ‣ 2 Current Evaluation Practices in Multilingual NLP ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">2.2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Correlation between translation quality and model performance</h5>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">Finally, to test how reliable machine translation is in the scope of our study, we report the average Pearson and Spearman rank correlation between the models’ performance using human-translated and machine-translated data, see Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A2.T7" title="Table 7 ‣ Appendix B Correlation between Chrf++ scores and translations ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">7</span></a>. The high correlation shows that
the rank order of the models’ performance across different languages using human and machine-translated data is almost the same.
Moreover, we compute the correlation between the MT quality measured by chrF++ and the model performance on human and machine-translated data; see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A2" title="Appendix B Correlation between Chrf++ scores and translations ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">B</span></a> for the numerical results. The results show that there is no meaningful correlation between the performance on the human-translated data and MT quality, and this pattern holds for our machine-translated data.

<br class="ltx_break"/>Based on the results from all three analyses, we conclude that the MT quality is sufficient and results in model performance close to when using human translation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Large-scale Evaluation Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.2">Having confirmed that our translated test sets have a reliable quality, we now move on to analyze how MLMs perform on them.
In Figure <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S5.F3" title="Figure 3 ‣ 5 Large-scale Evaluation Results ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">3</span></a>, we summarize performance from XLM-R, BLOOMz, and AYA across 196, 168, and 198 languages, categorized by their data-scarcity.
We find that the average performance for all models is similar for high- and mid-resource languages.
Yet, while still above the random baseline, there is a notable drop in performance for low-resource and unseen languages.
Moreover, we find that their standard deviations are larger than high- and mid-resource ones.
This shows performance across low-resource languages varies a lot, making the average score less reliable.
Still, performance on unseen languages is relatively high; for XNLI and PAWS-X, on average, we obtain 18<math alttext="\%" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><csymbol cd="latexml" id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">%</annotation></semantics></math> and 29<math alttext="\%" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mo id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><csymbol cd="latexml" id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">%</annotation></semantics></math> above random performance.</p>
</div>
<figure class="ltx_figure" id="S5.F3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.F3.3" style="width:451.7pt;height:106.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-96.8pt,22.8pt) scale(0.7,0.7) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="291" id="S5.F3.1.g1" src="x2.png" width="407"/><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="291" id="S5.F3.2.g2" src="x3.png" width="407"/><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="291" id="S5.F3.3.g3" src="x4.png" width="407"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average performance across test languages in a zero-shot fine-tuning setup for XLM-R and in a zero-shot prompting using BLOOMz. Results are categorized per task and data coverage during pretraining as reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.T1" title="Table 1 ‣ 3.4 Machine Translation ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">1</span></a>. Results across models are not directly comparable as their language categorizations differ. </figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Representativeness of Selected Subsets of Languages</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2">As each dataset contains a distinct selection of languages for testing, we study to what extent each of them provides a reliable estimate for how MLM performance will generalize to more languages. While we do not cover all the world’s languages, we compare the averages between the languages covered by the original datasets and those covered by our much larger translated datasets. To this end, we split the languages from the original datasets based on our resource categorization reported in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S3.T1" title="Table 1 ‣ 3.4 Machine Translation ‣ 3 Methods ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">1</span></a>, and report the average performance for each category in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S5.T4" title="Table 4 ‣ 5.1 Representativeness of Selected Subsets of Languages ‣ 5 Large-scale Evaluation Results ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">4</span></a>.
Importantly, all performance scores are computed on the translated data.
From the results, we observe that for high and, to some extent, mid-resource languages, average performance on both language selections is similar, making the language coverage from the original datasets sufficiently representative.
Yet, for low-resource languages, we find a notable difference, which suggests that the datasets’ language coverage is not representative of a wider range of low-resource languages.
Specifically, across all tasks, we tend to overestimate performance, which can go up to 4.7<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mo id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><csymbol cd="latexml" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">%</annotation></semantics></math> and 8.7<math alttext="\%" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mo id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><csymbol cd="latexml" id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">%</annotation></semantics></math> accuracy points (for XCOPA).</p>
</div>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.1" style="width:205.8pt;height:163.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-55.4pt,44.1pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S5.T4.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">High</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">Mid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T4.1.1.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">Low</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.1.1.1.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">Ave.</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S5.T4.1.1.2.2.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.2.2.1.1">XLM-R</span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.3.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.1.1.3.3.1" style="padding-left:6.5pt;padding-right:6.5pt;">PAWS-X</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.3.3.2" style="padding-left:6.5pt;padding-right:6.5pt;">89.5 / 89.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.3.3.3" style="padding-left:6.5pt;padding-right:6.5pt;">   -   / 87.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.3.3.4" style="padding-left:6.5pt;padding-right:6.5pt;">   -   / 85.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.3.3.5" style="padding-left:6.5pt;padding-right:6.5pt;">89.5 / 85.3</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.4.4">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.4.4.1" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.4.2" style="padding-left:6.5pt;padding-right:6.5pt;">82.2 / 81.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.4.3" style="padding-left:6.5pt;padding-right:6.5pt;">79.5 / 78.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.4.4.4" style="padding-left:6.5pt;padding-right:6.5pt;">74.4 / 71.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.4.4.5" style="padding-left:6.5pt;padding-right:6.5pt;">80.5 / 72.4</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.5.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.5.5.1" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.5.2" style="padding-left:6.5pt;padding-right:6.5pt;">71.6 / 71.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.5.3" style="padding-left:6.5pt;padding-right:6.5pt;">69.6 / 68.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.5.5.4" style="padding-left:6.5pt;padding-right:6.5pt;">66.4 / 61.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.5.5.5" style="padding-left:6.5pt;padding-right:6.5pt;">70.3 / 69.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.6.6">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.6.6.1" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.6.2" style="padding-left:6.5pt;padding-right:6.5pt;">85.5 / 83.1</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.6.3" style="padding-left:6.5pt;padding-right:6.5pt;">77.2 / 78.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.6.6.4" style="padding-left:6.5pt;padding-right:6.5pt;">74.1 / 69.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.6.6.5" style="padding-left:6.5pt;padding-right:6.5pt;">78.9 / 77.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S5.T4.1.1.7.7.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.7.7.1.1">BLOOMz</span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.8.8">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.1.1.8.8.1" style="padding-left:6.5pt;padding-right:6.5pt;">B-NLI</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.8.8.2" style="padding-left:6.5pt;padding-right:6.5pt;">72.8 / 73.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.8.8.3" style="padding-left:6.5pt;padding-right:6.5pt;">70.8 / 70.2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.8.8.4" style="padding-left:6.5pt;padding-right:6.5pt;">70.9 / 68.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.8.8.5" style="padding-left:6.5pt;padding-right:6.5pt;">72.2 / 69.8</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.9.9">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.9.9.1" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.9.2" style="padding-left:6.5pt;padding-right:6.5pt;">76.9 / 79.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.9.3" style="padding-left:6.5pt;padding-right:6.5pt;">71.6 / 67.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.9.9.4" style="padding-left:6.5pt;padding-right:6.5pt;">63.0 / 54.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.9.9.5" style="padding-left:6.5pt;padding-right:6.5pt;">73.7 / 62.3</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.10.10">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.10.10.1" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.10.2" style="padding-left:6.5pt;padding-right:6.5pt;">86.2 / 87.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.10.3" style="padding-left:6.5pt;padding-right:6.5pt;">81.1 / 72.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.10.10.4" style="padding-left:6.5pt;padding-right:6.5pt;">79.4 / 64.5</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.10.10.5" style="padding-left:6.5pt;padding-right:6.5pt;">82.8 / 71.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.11.11">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" colspan="5" id="S5.T4.1.1.11.11.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.11.11.1.1">AYA</span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.12.12">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T4.1.1.12.12.1" style="padding-left:6.5pt;padding-right:6.5pt;">B-NLI</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.12.12.2" style="padding-left:6.5pt;padding-right:6.5pt;">78.4/77.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.12.12.3" style="padding-left:6.5pt;padding-right:6.5pt;">77.7/77.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.1.1.12.12.4" style="padding-left:6.5pt;padding-right:6.5pt;">73.5/75.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.1.1.12.12.5" style="padding-left:6.5pt;padding-right:6.5pt;">77.4/76.4</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.13.13">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S5.T4.1.1.13.13.1" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.13.13.2" style="padding-left:6.5pt;padding-right:6.5pt;">83.65/86.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.13.13.3" style="padding-left:6.5pt;padding-right:6.5pt;">84.8/84.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.1.1.13.13.4" style="padding-left:6.5pt;padding-right:6.5pt;">82.8/79.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.1.1.13.13.5" style="padding-left:6.5pt;padding-right:6.5pt;">83.7/82.0</td>
</tr>
<tr class="ltx_tr" id="S5.T4.1.1.14.14">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T4.1.1.14.14.1" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.14.14.2" style="padding-left:6.5pt;padding-right:6.5pt;">85.8/89.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.14.14.3" style="padding-left:6.5pt;padding-right:6.5pt;">91.0/89.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.1.1.14.14.4" style="padding-left:6.5pt;padding-right:6.5pt;">85.5/86.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.1.1.14.14.5" style="padding-left:6.5pt;padding-right:6.5pt;">87.0/87.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>
The average performance of high, mid, and low-resource languages covered by the original dataset/the languages covered by our machine-translated datasets. All results are computed on the machine-translated data.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.1" style="width:191.2pt;height:70.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.5pt,18.9pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S5.T5.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">ar</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">bg</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">de</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">el</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">es</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.8" style="padding-left:6.5pt;padding-right:6.5pt;">fr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.1.9" style="padding-left:6.5pt;padding-right:6.5pt;">hu</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.1.2.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.1.2.1.1" style="padding-left:6.5pt;padding-right:6.5pt;">MLP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">54.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">53.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">56.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">55.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">54.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">55.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.8" style="padding-left:6.5pt;padding-right:6.5pt;">56.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.2.1.9" style="padding-left:6.5pt;padding-right:6.5pt;">53.8</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.3.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S5.T5.1.1.3.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">LSTM</th>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">74.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">68.1</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">72.9</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">73.4</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">68.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">77.7</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.8" style="padding-left:6.5pt;padding-right:6.5pt;">78.9</td>
<td class="ltx_td ltx_align_center" id="S5.T5.1.1.3.2.9" style="padding-left:6.5pt;padding-right:6.5pt;">72.5</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.4.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.1.4.3.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4.3.2" style="padding-left:6.5pt;padding-right:6.5pt;">id</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4.3.3" style="padding-left:6.5pt;padding-right:6.5pt;">mk</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4.3.4" style="padding-left:6.5pt;padding-right:6.5pt;">no</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4.3.5" style="padding-left:6.5pt;padding-right:6.5pt;">ro</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4.3.6" style="padding-left:6.5pt;padding-right:6.5pt;">ru</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4.3.7" style="padding-left:6.5pt;padding-right:6.5pt;">sk</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4.3.8" style="padding-left:6.5pt;padding-right:6.5pt;">tr</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.4.3.9" style="padding-left:6.5pt;padding-right:6.5pt;">vi</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.5.4">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S5.T5.1.1.5.4.1" style="padding-left:6.5pt;padding-right:6.5pt;">MLP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.2" style="padding-left:6.5pt;padding-right:6.5pt;">55.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.3" style="padding-left:6.5pt;padding-right:6.5pt;">56.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.4" style="padding-left:6.5pt;padding-right:6.5pt;">55.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.5" style="padding-left:6.5pt;padding-right:6.5pt;">56.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.6" style="padding-left:6.5pt;padding-right:6.5pt;">56.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.7" style="padding-left:6.5pt;padding-right:6.5pt;">55.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.8" style="padding-left:6.5pt;padding-right:6.5pt;">54.9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.1.5.4.9" style="padding-left:6.5pt;padding-right:6.5pt;">54.7</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.6.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S5.T5.1.1.6.5.1" style="padding-left:6.5pt;padding-right:6.5pt;">LSTM</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.6.5.2" style="padding-left:6.5pt;padding-right:6.5pt;">73.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.6.5.3" style="padding-left:6.5pt;padding-right:6.5pt;">72.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.6.5.4" style="padding-left:6.5pt;padding-right:6.5pt;">74.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.6.5.5" style="padding-left:6.5pt;padding-right:6.5pt;">75.0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.6.5.6" style="padding-left:6.5pt;padding-right:6.5pt;">74.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.6.5.7" style="padding-left:6.5pt;padding-right:6.5pt;">71.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.6.5.8" style="padding-left:6.5pt;padding-right:6.5pt;">68.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.1.1.6.5.9" style="padding-left:6.5pt;padding-right:6.5pt;">85.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>The performance of the baselines on XNLI. </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Baseline Performance</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">While more powerful MLMs have been proposed in recent years, random baselines are still commonly used to interpret if performance improvements are meaningful.
We argue that having a stronger yet simpler baseline can better put performance improvements into perspective.
Thus,
we use an LSTM and the FastText embeddings with an MLP on top for XNLI and PAWS-X, as we see unexpectedly high performance for unseen languages for these two tasks.
Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#S5.T5" title="Table 5 ‣ 5.1 Representativeness of Selected Subsets of Languages ‣ 5 Large-scale Evaluation Results ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes our results across 16 languages, see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A5" title="Appendix E Full results XLM-R Large ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">E</span></a> for PAWS-X.
As can be seen, these simple models can impressively achieve high performance across different languages.
Especially among low-resource languages like Turkish, there is no notable difference between these models and our advanced MLMs. We conclude that even though we have used much more computation and data for pretraining MLMs, their performance has not improved proportionally to this on mid and low-resource languages.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions and Recommendations</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We demonstrate that machine translation provides a valid alternative to human translation of test sets. Employing our large-scale translated test sets, we show that the subsets of languages selected for each dataset tend to give a misleading estimate of MLM performance on low-resource languages. In addition, we show that random performance is a relatively weak baseline to determine meaningful performance improvement. Taking into consideration some of the limitations discussed in this work, we provide a set of practical recommendations for future work:
(1) <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">Reconsidering MT for evaluation</em>: While MT has its shortcomings, we show that MT quality is sufficient to accurately estimate performance across a wide range of languages. (2) <em class="ltx_emph ltx_font_italic" id="S6.p1.1.2">Reconsidering simpler baselines</em>: We suggest reconsidering simpler neural networks to test the extent to which an MLM’s large-scale pretraining has benefited performance. (3) <em class="ltx_emph ltx_font_italic" id="S6.p1.1.3">Paying attention to the categorization of high- and low-resource languages</em>: To compare MLMs’ performance, it is important to pay attention to the different categorizations of high- and low-resource languages for each model separately.
(4) <em class="ltx_emph ltx_font_italic" id="S6.p1.1.4">Keeping domain sources consistent between languages</em>: For a fair comparison across languages, it is crucial to make sure that test data come from similar domain sources.
We believe that these practices improve evaluation and interpretation of performance in multilingual NLP.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our machine translation alternative might only be applicable to the evaluation of classification tasks. For more complicated tasks, translation noise is more likely to cause a bigger performance gap compared to human-translated test sets.
In Appendix <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A3" title="Appendix C Lessons learned for MT ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">C</span></a>, we provide a set of lessons learned from our MT experiments and found that for automatically translating questions-answering datasets, for instance, we face more challenges than when simply translating classification tasks.
Besides, when using machine translation, different cultural information is not transferred from one language to another. This could make it more difficult to generalize across languages. As such, we are probably still overestimating the models’ ability to perform cross-lingual transfer when testing on parallel data.
In addition, due to computational limitations, we only use the NLLB 3.3B model. However, as we saw in Table <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#A1.T6" title="Table 6 ‣ Baseline fine-tuning details ‣ Appendix A Experimental setups ‣ On the Evaluation Practices in Multilingual NLP: Can Machine Translation Offer an Alternative to Human Translations?"><span class="ltx_text ltx_ref_tag">6</span></a>, a bigger version of the MT model can enhance the translation quality, especially for low-resource languages. This suggests that using even larger models could have made the performance even more reliable.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Moreover, in this paper, we used the percentage of data that each language contributes to the total amount of pretraining data as a measure of data coverage. Based on this measure, we set thresholds to distinguish high, mid, and low resource languages. However, this value is relative to the total amount of seen pretraining data, which can differ per LM. One could argue that these threshold values should instead be based on the absolute amount of data seen during pretraining. Future work should focus on studying whether the categorization of data coverage should be made based on absolute or relative numbers.
</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahuja et al. (2022)</span>
<span class="ltx_bibblock">
Kabir Ahuja, Shanu Kumar, Sandipan Dandapat, and Monojit Choudhury. 2022.

</span>
<span class="ltx_bibblock">Multi task learning for zero shot performance prediction of multilingual models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 5454–5467.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ammar et al. (2016)</span>
<span class="ltx_bibblock">
Waleed Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah A Smith. 2016.

</span>
<span class="ltx_bibblock">Many languages, one parser.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Transactions of the Association for Computational Linguistics</em>, 4:431–444.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe et al. (2020)</span>
<span class="ltx_bibblock">
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama. 2020.

</span>
<span class="ltx_bibblock">On the cross-lingual transferability of monolingual representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4623–4637.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bapna et al. (2022)</span>
<span class="ltx_bibblock">
Ankur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2205.03983" title="">Building machine translation systems for the next thousand languages</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blevins and Zettlemoyer (2022)</span>
<span class="ltx_bibblock">
Terra Blevins and Luke Zettlemoyer. 2022.

</span>
<span class="ltx_bibblock">Language contamination explains the cross-lingual capabilities of english pretrained models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of Empirical Methods in Natural Language Processing</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bojanowski et al. (2017)</span>
<span class="ltx_bibblock">
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017.

</span>
<span class="ltx_bibblock">Enriching word vectors with subword information.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Transactions of the association for computational linguistics</em>, 5:135–146.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Briakou et al. (2023)</span>
<span class="ltx_bibblock">
Eleftheria Briakou, Colin Cherry, and George Foster. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.acl-long.524" title="">Searching for needles in a haystack: On the role of incidental bilingualism in PaLM’s translation capability</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 9432–9452, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Advances in neural information processing systems</em>, 33:1877–1901.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark et al. (2020)</span>
<span class="ltx_bibblock">
Jonathan H. Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. 2020.

</span>
<span class="ltx_bibblock">Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Transactions of the Association for Computational Linguistics</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Édouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8440–8451.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2018)</span>
<span class="ltx_bibblock">
Alexis Conneau, Ruty Rinott, Guillaume Lample, Adina Williams, Samuel Bowman, Holger Schwenk, and Veselin Stoyanov. 2018.

</span>
<span class="ltx_bibblock">XNLI: Evaluating Cross-lingual Sentence Representations.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, pages 2475–2485.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dac Lai et al. (2023)</span>
<span class="ltx_bibblock">
Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023.

</span>
<span class="ltx_bibblock">Chatgpt beyond english: Towards a comprehensive evaluation of large language models in multilingual learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv e-prints</em>, pages arXiv–2304.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glavaš et al. (2019)</span>
<span class="ltx_bibblock">
Goran Glavaš, Robert Litschko, Sebastian Ruder, and Ivan Vulić. 2019.

</span>
<span class="ltx_bibblock">How to (properly) evaluate cross-lingual word embeddings: On strong baselines, comparative analyses, and some misconceptions.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 710–721.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gordon et al. (2012)</span>
<span class="ltx_bibblock">
Andrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/S12-1052" title="">SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">*SEM 2012: The First Joint Conference on Lexical and Computational Semantics – Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)</em>, pages 394–398, Montréal, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hada et al. (2023)</span>
<span class="ltx_bibblock">
Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023.

</span>
<span class="ltx_bibblock">Are large language model-based evaluators the solution to scaling up multilingual evaluation?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2309.07462</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2023)</span>
<span class="ltx_bibblock">
Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=sE7-XhLxHA" title="">DeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter et al. (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter, J urgen Schmidhuber, and Corso Elvezia. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Neural Computation</em>, 9(8):1735–1780.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2020)</span>
<span class="ltx_bibblock">
Junjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020.

</span>
<span class="ltx_bibblock">Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International Conference on Machine Learning</em>, pages 4411–4421. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenton and Toutanova (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Proceedings of NAACL-HLT</em>, pages 4171–4186.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2021)</span>
<span class="ltx_bibblock">
Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.57" title="">To ship or not to ship: An extensive evaluation of automatic metrics for machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 478–494, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koppel and Ordan (2011)</span>
<span class="ltx_bibblock">
Moshe Koppel and Noam Ordan. 2011.

</span>
<span class="ltx_bibblock">Translationese and its dialects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies</em>, pages 1318–1326.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo and Richardson (2018)</span>
<span class="ltx_bibblock">
Taku Kudo and John Richardson. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D18-2012" title="">SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pages 66–71, Brussels, Belgium. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. (2019)</span>
<span class="ltx_bibblock">
Guokun Lai, Barlas Oguz, Yiming Yang, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Bridging the domain gap in cross-lingual document classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:1909.07009</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lample et al. (2018)</span>
<span class="ltx_bibblock">
Guillaume Lample, Alexis Conneau, Marc’Aurelio Ranzato, Ludovic Denoyer, and Hervé Jégou. 2018.

</span>
<span class="ltx_bibblock">Word translation without parallel data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2019)</span>
<span class="ltx_bibblock">
Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian Riedel, and Holger Schwenk. 2019.

</span>
<span class="ltx_bibblock">Mlqa: Evaluating cross-lingual extractive question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:1910.07475</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2020)</span>
<span class="ltx_bibblock">
Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fenfei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Daxin Jiang, Guihong Cao, et al. 2020.

</span>
<span class="ltx_bibblock">Xglue: A new benchmark datasetfor cross-lingual pre-training, understanding and generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 6008–6018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2021)</span>
<span class="ltx_bibblock">
Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.

</span>
<span class="ltx_bibblock">Few-shot learning with multilingual language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2112.10668</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Littell et al. (2017)</span>
<span class="ltx_bibblock">
Patrick Littell, David R Mortensen, Ke Lin, Katherine Kairis, Carlisle Turner, and Lori Levin. 2017.

</span>
<span class="ltx_bibblock">Uriel and lang2vec: Representing languages as typological, geographical, and phylogenetic vectors.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers</em>, volume 2, pages 8–14.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Chen Cecilia Liu, Fajri Koto, Timothy Baldwin, and Iryna Gurevych. 2023.

</span>
<span class="ltx_bibblock">Are multilingual llms culturally-diverse reasoners? an investigation into multicultural proverbs and sayings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2309.08591</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McCoy et al. (2020)</span>
<span class="ltx_bibblock">
R Thomas McCoy, Ellie Pavlick, and Tal Linzen. 2020.

</span>
<span class="ltx_bibblock">Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">57th Annual Meeting of the Association for Computational Linguistics, ACL 2019</em>, pages 3428–3448. Association for Computational Linguistics (ACL).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muennighoff et al. (2022)</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2211.01786</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Navigli and Ponzetto (2012)</span>
<span class="ltx_bibblock">
Roberto Navigli and Simone Paolo Ponzetto. 2012.

</span>
<span class="ltx_bibblock">Joining forces pays off: Multilingual joint word sense disambiguation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational natural language learning</em>, pages 1399–1410. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(33)</span>
<span class="ltx_bibblock">
Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajic, Christopher D Manning, Sampo Pyysalo, Sebastian Schuster, and Francis Tyers Daniel Zeman.

</span>
<span class="ltx_bibblock">Universal dependencies v2: An evergrowing multilingual treebank collection.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NLLB Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia-Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Horan et al. (2016)</span>
<span class="ltx_bibblock">
Helen O’Horan, Yevgeni Berzak, Ivan Vulić, Roi Reichart, and Anna Korhonen. 2016.

</span>
<span class="ltx_bibblock">Survey on the use of typological information in natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</em>, pages 1297–1308.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patankar et al. (2022)</span>
<span class="ltx_bibblock">
Shantanu Patankar, Omkar Gokhale, Onkar Litake, Aditya Mandke, and Dipali Kadam. 2022.

</span>
<span class="ltx_bibblock">To train or not to train: Predicting the performance of massively multilingual models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the First Workshop on Scaling Up Multilingual Evaluation</em>, pages 8–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pires et al. (2019)</span>
<span class="ltx_bibblock">
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1493" title="">How multilingual is multilingual BERT?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 4996–5001, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ploeger et al. (2024)</span>
<span class="ltx_bibblock">
Esther Ploeger, Wessel Poelman, Miryam de Lhoneux, and Johannes Bjerva. 2024.

</span>
<span class="ltx_bibblock">What is’ typological diversity’in nlp?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2402.04222</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et al. (2020)</span>
<span class="ltx_bibblock">
Edoardo M. Ponti, Goran Glavaš, Olga Majewska, Qianchu Liu, Ivan Vulić, and Anna Korhonen. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://ducdauge.github.io/files/xcopa.pdf" title="">XCOPA: A multilingual dataset for causal commonsense reasoning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et al. (2019)</span>
<span class="ltx_bibblock">
Edoardo Maria Ponti, Helen O’horan, Yevgeni Berzak, Ivan Vulić, Roi Reichart, Thierry Poibeau, Ekaterina Shutova, and Anna Korhonen. 2019.

</span>
<span class="ltx_bibblock">Modeling language variation and universals: A survey on typological linguistics for natural language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Computational Linguistics</em>, 45(3):559–601.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ponti et al. (2018)</span>
<span class="ltx_bibblock">
Edoardo Maria Ponti, Roi Reichart, Anna Korhonen, and Ivan Vulić. 2018.

</span>
<span class="ltx_bibblock">Isomorphic transfer of syntactic structures in cross-lingual nlp.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1531–1542.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popel et al. (2020)</span>
<span class="ltx_bibblock">
Martin Popel, Marketa Tomkova, Jakub Tomek, Łukasz Kaiser, Jakob Uszkoreit, Ondřej Bojar, and Zdeněk Žabokrtskỳ. 2020.

</span>
<span class="ltx_bibblock">Transforming machine translation: a deep learning system reaches news translation quality comparable to human professionals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Nature communications</em>, 11(1):1–15.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popović (2017)</span>
<span class="ltx_bibblock">
Maja Popović. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/W17-4770" title="">chrF++: words helping character n-grams</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the Second Conference on Machine Translation</em>, pages 612–618, Copenhagen, Denmark. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahimi et al. (2019)</span>
<span class="ltx_bibblock">
Afshin Rahimi, Yuan Li, and Trevor Cohn. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P19-1015" title="">Massively multilingual transfer for NER</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pages 151–164, Florence, Italy. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranathunga et al. (2023)</span>
<span class="ltx_bibblock">
Surangika Ranathunga, En-Shiun Annie Lee, Marjana Prifti Skenduli, Ravi Shekhar, Mehreen Alam, and Rishemjit Kaur. 2023.

</span>
<span class="ltx_bibblock">Neural machine translation for low-resource languages: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">ACM Computing Surveys</em>, 55(11):1–37.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruder et al. (2023)</span>
<span class="ltx_bibblock">
Sebastian Ruder, Jonathan H Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia, Shruti Rijhwani, Parker Riley, Jean-Michel A Sarr, Xinyi Wang, et al. 2023.

</span>
<span class="ltx_bibblock">Xtreme-up: A user-centric scarce-data benchmark for under-represented languages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2305.11938</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruder et al. (2021a)</span>
<span class="ltx_bibblock">
Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, and Melvin Johnson. 2021a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.emnlp-main.802" title="">XTREME-R: Towards more challenging and nuanced multilingual evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 10215–10245, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruder et al. (2021b)</span>
<span class="ltx_bibblock">
Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Dan Garrette, Graham Neubig, et al. 2021b.

</span>
<span class="ltx_bibblock">Xtreme-r: Towards more challenging and nuanced multilingual evaluation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 10215–10245.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al. (2019)</span>
<span class="ltx_bibblock">
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/D19-1454" title="">Social IQa: Commonsense reasoning about social interactions</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 4463–4473, Hong Kong, China. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scao et al. (2022)</span>
<span class="ltx_bibblock">
Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">arXiv preprint arXiv:2211.05100</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. (2016)</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/P16-1162" title="">Neural machine translation of rare words with subword units</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 1715–1725, Berlin, Germany. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shliazhko et al. (2022)</span>
<span class="ltx_bibblock">
Oleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia Kozlova, and Tatiana Shavrina. 2022.

</span>
<span class="ltx_bibblock">mgpt: Few-shot learners go multilingual.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv e-prints</em>, pages arXiv–2204.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srinivasan et al. (2021)</span>
<span class="ltx_bibblock">
Anirudh Srinivasan, Sunayana Sitaram, Tanuja Ganu, Sandipan Dandapat, Kalika Bali, and Monojit Choudhury. 2021.

</span>
<span class="ltx_bibblock">Predicting the performance of multilingual nlp models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2110.08875</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talat et al. (2022)</span>
<span class="ltx_bibblock">
Zeerak Talat, Aurélie Névéol, Stella Biderman, Miruna Clinciu, Manan Dey, Shayne Longpre, Alexandra Sasha Luccioni10, Maraim Masoud11, Margaret Mitchell10, Dragomir Radev12, et al. 2022.

</span>
<span class="ltx_bibblock">You reap what you sow: On the challenges of bias evaluation under multilingual settings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Challenges &amp; Perspectives in Creating Large Language Models</em>, page 26.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2015)</span>
<span class="ltx_bibblock">
Liling Tan, Jon Dehdari, and Josef van Genabith. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W15-5009" title="">An awkward disparity between BLEU / RIBES scores and human judgements in machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 2nd Workshop on Asian Translation (WAT2015)</em>, pages 74–81, Kyoto, Japan. Workshop on Asian Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tedeschi et al. (2023)</span>
<span class="ltx_bibblock">
Simone Tedeschi, Johan Bos, Thierry Declerck, Jan Hajic, Daniel Hershcovich, Eduard Hovy, Alexander Koller, Simon Krek, Steven Schockaert, Rico Sennrich, et al. 2023.

</span>
<span class="ltx_bibblock">What’s the meaning of superhuman performance in today’s nlu?

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiedemann (2020)</span>
<span class="ltx_bibblock">
Jörg Tiedemann. 2020.

</span>
<span class="ltx_bibblock">The tatoeba translation challenge–realistic data sets for low resource and multilingual mt.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 1174–1182.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2307.09288</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Üstün et al. (2024)</span>
<span class="ltx_bibblock">
Ahmet Üstün, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D’souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, et al. 2024.

</span>
<span class="ltx_bibblock">Aya model: An instruction finetuned open-access multilingual language model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv e-prints</em>, pages arXiv–2402.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang and Sennrich (2020)</span>
<span class="ltx_bibblock">
Chaojun Wang and Rico Sennrich. 2020.

</span>
<span class="ltx_bibblock">On exposure bias, hallucination and domain shift in neural machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 3544–3552.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2019)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al. 2019.

</span>
<span class="ltx_bibblock">Huggingface’s transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:1910.03771</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Dredze (2020)</span>
<span class="ltx_bibblock">
Shijie Wu and Mark Dredze. 2020.

</span>
<span class="ltx_bibblock">Are all languages created equal in multilingual bert?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Proceedings of the 5th Workshop on Representation Learning for NLP</em>, pages 120–130.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.naacl-main.41" title="">mT5: A massively multilingual pre-trained text-to-text transformer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 483–498, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2019)</span>
<span class="ltx_bibblock">
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. 2019.

</span>
<span class="ltx_bibblock">PAWS-X: A Cross-lingual Adversarial Dataset for Paraphrase Identification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pages 3687–3692.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Toral (2019)</span>
<span class="ltx_bibblock">
Mike Zhang and Antonio Toral. 2019.

</span>
<span class="ltx_bibblock">The effect of translationese in machine translation test sets.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research Papers)</em>, pages 73–81.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zweigenbaum et al. (2017)</span>
<span class="ltx_bibblock">
Pierre Zweigenbaum, Serge Sharoff, and Reinhard Rapp. 2017.

</span>
<span class="ltx_bibblock">Overview of the second bucc shared task: Spotting parallel sentences in comparable corpora.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the 10th Workshop on Building and Using Comparable Corpora</em>, pages 60–67.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Experimental setups</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">For the implementation of all models, we rely on the HuggingFace Library <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib62" title="">2019</a>)</cite>. XLM-R large, BLOOMz, and AYA-101 (AYA) have  330M, 7.1B, and 13B parameters, respectively. Moreover, we have run all the BLOOMz and AYA experiments on an NVIDIA A100-SXM4 GPU with 40GB memory, and a single NVIDIA A6000 has been used for the MT and XLM-R experiments with 48GB memory.</p>
</div>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">XLM-R fine-tuning details</h5>
<div class="ltx_para" id="A1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.9">For the NLI task, we have fine-tuned XLM-R with a learning rate of 2<math alttext="e" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="A1.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="A1.SS0.SSS0.Px1.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="A1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.1.m1.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.1.m1.1c">e</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.1.m1.1d">italic_e</annotation></semantics></math>-5, AdamW optimizer, and a batch size of 32 for <math alttext="3" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="A1.SS0.SSS0.Px1.p1.2.m2.1a"><mn id="A1.SS0.SSS0.Px1.p1.2.m2.1.1" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.2.m2.1b"><cn id="A1.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px1.p1.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.2.m2.1c">3</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.2.m2.1d">3</annotation></semantics></math> epochs. For the PAWS-X task, we have considered a learning rate of 2<math alttext="e" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="A1.SS0.SSS0.Px1.p1.3.m3.1a"><mi id="A1.SS0.SSS0.Px1.p1.3.m3.1.1" xref="A1.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.3.m3.1b"><ci id="A1.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.3.m3.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.3.m3.1c">e</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.3.m3.1d">italic_e</annotation></semantics></math>-6, batch size 16 with a warm-up ratio of <math alttext="0.01" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.4.m4.1"><semantics id="A1.SS0.SSS0.Px1.p1.4.m4.1a"><mn id="A1.SS0.SSS0.Px1.p1.4.m4.1.1" xref="A1.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.4.m4.1b"><cn id="A1.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" type="float" xref="A1.SS0.SSS0.Px1.p1.4.m4.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.4.m4.1c">0.01</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.4.m4.1d">0.01</annotation></semantics></math> for 3 epochs. For XCOPA and XStoryCloze tasks, first, we train the model on the training set of Social IQa <cite class="ltx_cite ltx_citemacro_cite">Sap et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib49" title="">2019</a>)</cite> and then fine-tune it on the training set of XCOPA dataset <cite class="ltx_cite ltx_citemacro_cite">Gordon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.14267v1#bib.bib14" title="">2012</a>)</cite>. We have selected a learning rate of 3<math alttext="e" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.5.m5.1"><semantics id="A1.SS0.SSS0.Px1.p1.5.m5.1a"><mi id="A1.SS0.SSS0.Px1.p1.5.m5.1.1" xref="A1.SS0.SSS0.Px1.p1.5.m5.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.5.m5.1b"><ci id="A1.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="A1.SS0.SSS0.Px1.p1.5.m5.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.5.m5.1c">e</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.5.m5.1d">italic_e</annotation></semantics></math>-6, batch size of <math alttext="16" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.6.m6.1"><semantics id="A1.SS0.SSS0.Px1.p1.6.m6.1a"><mn id="A1.SS0.SSS0.Px1.p1.6.m6.1.1" xref="A1.SS0.SSS0.Px1.p1.6.m6.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.6.m6.1b"><cn id="A1.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px1.p1.6.m6.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.6.m6.1c">16</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.6.m6.1d">16</annotation></semantics></math> for SIQa and <math alttext="8" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.7.m7.1"><semantics id="A1.SS0.SSS0.Px1.p1.7.m7.1a"><mn id="A1.SS0.SSS0.Px1.p1.7.m7.1.1" xref="A1.SS0.SSS0.Px1.p1.7.m7.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.7.m7.1b"><cn id="A1.SS0.SSS0.Px1.p1.7.m7.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px1.p1.7.m7.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.7.m7.1c">8</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.7.m7.1d">8</annotation></semantics></math> for XCOPA, a warm-up ratio of <math alttext="0.1" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.8.m8.1"><semantics id="A1.SS0.SSS0.Px1.p1.8.m8.1a"><mn id="A1.SS0.SSS0.Px1.p1.8.m8.1.1" xref="A1.SS0.SSS0.Px1.p1.8.m8.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.8.m8.1b"><cn id="A1.SS0.SSS0.Px1.p1.8.m8.1.1.cmml" type="float" xref="A1.SS0.SSS0.Px1.p1.8.m8.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.8.m8.1c">0.1</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.8.m8.1d">0.1</annotation></semantics></math>, and fine-tune the model for <math alttext="3" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px1.p1.9.m9.1"><semantics id="A1.SS0.SSS0.Px1.p1.9.m9.1a"><mn id="A1.SS0.SSS0.Px1.p1.9.m9.1.1" xref="A1.SS0.SSS0.Px1.p1.9.m9.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px1.p1.9.m9.1b"><cn id="A1.SS0.SSS0.Px1.p1.9.m9.1.1.cmml" type="integer" xref="A1.SS0.SSS0.Px1.p1.9.m9.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px1.p1.9.m9.1c">3</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px1.p1.9.m9.1d">3</annotation></semantics></math> epochs on each dataset.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">BLOOMz and AYA zero-shot prompts</h5>
<div class="ltx_para" id="A1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1">For zero-shot prompting, we constructed the following prompts for XNLI, XCOPA and XStorycloze respectively:</p>
</div>
<div class="ltx_para" id="A1.SS0.SSS0.Px2.p2">
<blockquote class="ltx_quote" id="A1.SS0.SSS0.Px2.p2.1">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.1.1">Premise: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.1.1">&lt;premise&gt;
<br class="ltx_break"/></span>Hypothesis: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.1.1.2">&lt;hypothesis&gt;
<br class="ltx_break"/></span>Does the premise entail the hypothesis?
Pick between yes or no.</p>
</blockquote>
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.2">—————————————————–</p>
<blockquote class="ltx_quote" id="A1.SS0.SSS0.Px2.p2.3">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.3.1">Premise: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.3.1.1">&lt;premise&gt;
<br class="ltx_break"/></span>Option A: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.3.1.2">&lt;choice1&gt;
<br class="ltx_break"/></span>Option B: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.3.1.3">&lt;choice2&gt;
<br class="ltx_break"/></span>Based on the premise, which <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.3.1.4">&lt;cause/effect&gt;</span> is more likely? Pick between options A and B.
<br class="ltx_break"/>Answer:</p>
</blockquote>
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.4">—————————————————–</p>
<blockquote class="ltx_quote" id="A1.SS0.SSS0.Px2.p2.5">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.5.1">Consider the following story:
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.5.1.1">&lt;story&gt;
<br class="ltx_break"/></span>Which ending to the story is most likely?
<br class="ltx_break"/>Pick between options A and B:
<br class="ltx_break"/>A: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.5.1.2">&lt;story_ending1&gt;
<br class="ltx_break"/></span>B: <span class="ltx_text ltx_font_typewriter" id="A1.SS0.SSS0.Px2.p2.5.1.3">&lt;story_ending2&gt;
<br class="ltx_break"/></span>Answer:</p>
</blockquote>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Baseline fine-tuning details</h5>
<div class="ltx_para" id="A1.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px3.p1.2">We have trained the LSTM with a hidden size of 300, a learning rate of 4<math alttext="e" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px3.p1.1.m1.1"><semantics id="A1.SS0.SSS0.Px3.p1.1.m1.1a"><mi id="A1.SS0.SSS0.Px3.p1.1.m1.1.1" xref="A1.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px3.p1.1.m1.1b"><ci id="A1.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.1.m1.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px3.p1.1.m1.1c">e</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px3.p1.1.m1.1d">italic_e</annotation></semantics></math>-4, and a batch size of 32 for 64 epochs with an early stopping strategy. The MLP classifier has a 2-layer architecture (same as the XLM-R fine-tuning setup) with the <math alttext="tanh" class="ltx_Math" display="inline" id="A1.SS0.SSS0.Px3.p1.2.m2.1"><semantics id="A1.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="A1.SS0.SSS0.Px3.p1.2.m2.1.1" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">t</mi><mo id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">a</mi><mo id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1a" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.4" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.4.cmml">n</mi><mo id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1b" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.5" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.5.cmml">h</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1"><times id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.1"></times><ci id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.2">𝑡</ci><ci id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.3">𝑎</ci><ci id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.4.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.4">𝑛</ci><ci id="A1.SS0.SSS0.Px3.p1.2.m2.1.1.5.cmml" xref="A1.SS0.SSS0.Px3.p1.2.m2.1.1.5">ℎ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS0.SSS0.Px3.p1.2.m2.1c">tanh</annotation><annotation encoding="application/x-llamapun" id="A1.SS0.SSS0.Px3.p1.2.m2.1d">italic_t italic_a italic_n italic_h</annotation></semantics></math> function as the non-linearity.
We have used the MLP model with the same hyperparameters for the MLP experiments.
For all the evaluations, we have utilized our translated datasets.</p>
</div>
<figure class="ltx_table" id="A1.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T6.1" style="width:179.0pt;height:58.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-48.2pt,15.8pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T6.1.1.1.1">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="A1.T6.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.1.1.1.1">metric</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">High</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">Mid</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">Low</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T6.1.1.1.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">Unseen</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">Ave.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T6.1.1.1.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">Med.</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7" id="A1.T6.1.1.2.2.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.2.2.1.1">NLLB-Distil</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.3.3">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A1.T6.1.1.3.3.1" style="padding-left:6.5pt;padding-right:6.5pt;">chrF++</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.3.2" style="padding-left:6.5pt;padding-right:6.5pt;">49.34</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.3.3" style="padding-left:6.5pt;padding-right:6.5pt;">46.92</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.3.4" style="padding-left:6.5pt;padding-right:6.5pt;">43.22</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T6.1.1.3.3.5" style="padding-left:6.5pt;padding-right:6.5pt;">37.12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.3.6" style="padding-left:6.5pt;padding-right:6.5pt;">44.15</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T6.1.1.3.3.7" style="padding-left:6.5pt;padding-right:6.5pt;">45.07</td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="7" id="A1.T6.1.1.4.4.1" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A1.T6.1.1.4.4.1.1">NLLB-3.3B</span></td>
</tr>
<tr class="ltx_tr" id="A1.T6.1.1.5.5">
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.1.1.5.5.1" style="padding-left:6.5pt;padding-right:6.5pt;">chrF++</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.1.5.5.2" style="padding-left:6.5pt;padding-right:6.5pt;">52.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.1.5.5.3" style="padding-left:6.5pt;padding-right:6.5pt;">50.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.1.5.5.4" style="padding-left:6.5pt;padding-right:6.5pt;">46.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="A1.T6.1.1.5.5.5" style="padding-left:6.5pt;padding-right:6.5pt;">40.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.1.5.5.6" style="padding-left:6.5pt;padding-right:6.5pt;">44.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="A1.T6.1.1.5.5.7" style="padding-left:6.5pt;padding-right:6.5pt;">45.5</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>The quality of the MT system across high, mid, and low-resource languages using chrF++ based on the XLM-R model’s categorization. </figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Correlation between Chrf++ scores and translations</h2>
<figure class="ltx_table" id="A2.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T7.1" style="width:438.0pt;height:46.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-117.9pt,12.6pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T7.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T7.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="A2.T7.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="A2.T7.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.1.2.1">XLM-R</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A2.T7.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.1.3.1">BLOOMz</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A2.T7.1.1.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T7.1.1.1.1.4.1">AYA</span></th>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A2.T7.1.1.2.2.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.2.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.2.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.2.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">PAWS-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A2.T7.1.1.2.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.2.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.2.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A2.T7.1.1.2.2.8" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.2.2.9" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.2.2.10" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T7.1.1.2.2.11" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T7.1.1.3.1">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A2.T7.1.1.3.1.1" style="padding-left:6.5pt;padding-right:6.5pt;">chrF++ vs. Human translated</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.3.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">29.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.3.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">24.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.3.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">65.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T7.1.1.3.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">16.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.3.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">5.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.3.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">5.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T7.1.1.3.1.8" style="padding-left:6.5pt;padding-right:6.5pt;">68.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.3.1.9" style="padding-left:6.5pt;padding-right:6.5pt;">22.7</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.3.1.10" style="padding-left:6.5pt;padding-right:6.5pt;">8.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T7.1.1.3.1.11" style="padding-left:6.5pt;padding-right:6.5pt;">-34.6</td>
</tr>
<tr class="ltx_tr" id="A2.T7.1.1.4.2">
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="A2.T7.1.1.4.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">chrF++ vs. Machine translated</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.1.4.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">22.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.1.4.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">48.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.1.4.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">82.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T7.1.1.4.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">22.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.1.4.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">18.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.1.4.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">14.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T7.1.1.4.2.8" style="padding-left:6.5pt;padding-right:6.5pt;">75.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.1.4.2.9" style="padding-left:6.5pt;padding-right:6.5pt;">55.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.1.4.2.10" style="padding-left:6.5pt;padding-right:6.5pt;">26.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T7.1.1.4.2.11" style="padding-left:6.5pt;padding-right:6.5pt;">22.4</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>
Average Spearman rank correlation between chrf++ scores and human- (original data) and machine-translated data (ours). </figcaption>
</figure>
<figure class="ltx_table" id="A2.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T8.1" style="width:438.0pt;height:46.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-117.9pt,12.6pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T8.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T8.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="A2.T8.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="A2.T8.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.1.1.2.1">XLM-R</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A2.T8.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.1.1.3.1">BLOOMz</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="A2.T8.1.1.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text ltx_font_bold" id="A2.T8.1.1.1.1.4.1">AYA</span></th>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.2.2">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A2.T8.1.1.2.2.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.2.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.2.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.2.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">PAWS-X</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A2.T8.1.1.2.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.2.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.2.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A2.T8.1.1.2.2.8" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.2.2.9" style="padding-left:6.5pt;padding-right:6.5pt;">XCOPA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.2.2.10" style="padding-left:6.5pt;padding-right:6.5pt;">XNLI</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A2.T8.1.1.2.2.11" style="padding-left:6.5pt;padding-right:6.5pt;">XStoryCloze</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T8.1.1.3.1">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A2.T8.1.1.3.1.1" style="padding-left:6.5pt;padding-right:6.5pt;">chrF++ vs. Human translated</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">27.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">10.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">89.3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T8.1.1.3.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">3.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">-10.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">-10.4</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A2.T8.1.1.3.1.8" style="padding-left:6.5pt;padding-right:6.5pt;">64.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1.9" style="padding-left:6.5pt;padding-right:6.5pt;">41.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1.10" style="padding-left:6.5pt;padding-right:6.5pt;">8.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A2.T8.1.1.3.1.11" style="padding-left:6.5pt;padding-right:6.5pt;">-18.5</td>
</tr>
<tr class="ltx_tr" id="A2.T8.1.1.4.2">
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="A2.T8.1.1.4.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">chrF++ vs. Machine translated</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.4.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">30.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.4.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">66.2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.4.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">96.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T8.1.1.4.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">8.8</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.4.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">14.5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.4.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">2.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A2.T8.1.1.4.2.8" style="padding-left:6.5pt;padding-right:6.5pt;">66.7</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.4.2.9" style="padding-left:6.5pt;padding-right:6.5pt;">52.1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.4.2.10" style="padding-left:6.5pt;padding-right:6.5pt;">16.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A2.T8.1.1.4.2.11" style="padding-left:6.5pt;padding-right:6.5pt;">11.3</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>
Average Pearson correlation between chrf++ scores and human- (original data) and machine-translated data (ours). </figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Lessons learned for MT</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We now share a few lessons learned from MT using NLLB to facilitate the translation of new datasets in future work:</p>
<ul class="ltx_itemize" id="A3.I1">
<li class="ltx_item" id="A3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i1.p1">
<p class="ltx_p" id="A3.I1.i1.p1.1">NLLB tends to skip sentences when translating paragraphs. Thus, it is important to translate the sentences one by one.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i2.p1">
<p class="ltx_p" id="A3.I1.i2.p1.1">NLLB has difficulty translating short phrases/names such as names, dates, locations, etc., because it tends to hallucinate additional content. This makes it challenging to translate the answers from QA datasets such as XQuAD.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i3.p1">
<p class="ltx_p" id="A3.I1.i3.p1.1">NLLB inconsistently chooses to code-switch to the target language. For instance, when translating the sentence ‘Sara is asleep’, it can choose to translate it either to ‘Sara ? farsi’ or ‘fully farsi’. This can be particularly challenging for retrieval datasets where the answer does not tend to fully match the context.</p>
</div>
</li>
<li class="ltx_item" id="A3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A3.I1.i4.p1">
<p class="ltx_p" id="A3.I1.i4.p1.1">While translation quality tends to be similar for different NLLB model sizes, at least the 3.3B version should be used when translating to languages that were low-resource, considering NLLB’s pretraining data.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Baselines for PAWS-X</h2>
<figure class="ltx_table" id="A4.T9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A4.T9.1" style="width:191.2pt;height:70.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-51.5pt,18.9pt) scale(0.65,0.65) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A4.T9.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T9.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A4.T9.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T9.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">en</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T9.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">ar</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T9.1.1.1.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">bg</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T9.1.1.1.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">de</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T9.1.1.1.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">el</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T9.1.1.1.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">es</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T9.1.1.1.1.8" style="padding-left:6.5pt;padding-right:6.5pt;">fr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T9.1.1.1.1.9" style="padding-left:6.5pt;padding-right:6.5pt;">hu</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T9.1.1.2.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T9.1.1.2.1.1" style="padding-left:6.5pt;padding-right:6.5pt;">MLP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.2.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">57.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.2.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">57.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.2.1.4" style="padding-left:6.5pt;padding-right:6.5pt;">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.2.1.5" style="padding-left:6.5pt;padding-right:6.5pt;">58.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.2.1.6" style="padding-left:6.5pt;padding-right:6.5pt;">57.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.2.1.7" style="padding-left:6.5pt;padding-right:6.5pt;">58.3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.2.1.8" style="padding-left:6.5pt;padding-right:6.5pt;">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.2.1.9" style="padding-left:6.5pt;padding-right:6.5pt;">58.9</td>
</tr>
<tr class="ltx_tr" id="A4.T9.1.1.3.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="A4.T9.1.1.3.2.1" style="padding-left:6.5pt;padding-right:6.5pt;">LSTM</th>
<td class="ltx_td ltx_align_center" id="A4.T9.1.1.3.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">58.5</td>
<td class="ltx_td ltx_align_center" id="A4.T9.1.1.3.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">57.3</td>
<td class="ltx_td ltx_align_center" id="A4.T9.1.1.3.2.4" style="padding-left:6.5pt;padding-right:6.5pt;">57.9</td>
<td class="ltx_td ltx_align_center" id="A4.T9.1.1.3.2.5" style="padding-left:6.5pt;padding-right:6.5pt;">60.3</td>
<td class="ltx_td ltx_align_center" id="A4.T9.1.1.3.2.6" style="padding-left:6.5pt;padding-right:6.5pt;">58.8</td>
<td class="ltx_td ltx_align_center" id="A4.T9.1.1.3.2.7" style="padding-left:6.5pt;padding-right:6.5pt;">58.8</td>
<td class="ltx_td ltx_align_center" id="A4.T9.1.1.3.2.8" style="padding-left:6.5pt;padding-right:6.5pt;">57.8</td>
<td class="ltx_td ltx_align_center" id="A4.T9.1.1.3.2.9" style="padding-left:6.5pt;padding-right:6.5pt;">59.8</td>
</tr>
<tr class="ltx_tr" id="A4.T9.1.1.4.3">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T9.1.1.4.3.1" style="padding-left:6.5pt;padding-right:6.5pt;"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.4.3.2" style="padding-left:6.5pt;padding-right:6.5pt;">id</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.4.3.3" style="padding-left:6.5pt;padding-right:6.5pt;">mk</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.4.3.4" style="padding-left:6.5pt;padding-right:6.5pt;">no</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.4.3.5" style="padding-left:6.5pt;padding-right:6.5pt;">ro</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.4.3.6" style="padding-left:6.5pt;padding-right:6.5pt;">ru</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.4.3.7" style="padding-left:6.5pt;padding-right:6.5pt;">sk</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.4.3.8" style="padding-left:6.5pt;padding-right:6.5pt;">tr</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.4.3.9" style="padding-left:6.5pt;padding-right:6.5pt;">vi</td>
</tr>
<tr class="ltx_tr" id="A4.T9.1.1.5.4">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T9.1.1.5.4.1" style="padding-left:6.5pt;padding-right:6.5pt;">MLP</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.5.4.2" style="padding-left:6.5pt;padding-right:6.5pt;">59.0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.5.4.3" style="padding-left:6.5pt;padding-right:6.5pt;">57.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.5.4.4" style="padding-left:6.5pt;padding-right:6.5pt;">58.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.5.4.5" style="padding-left:6.5pt;padding-right:6.5pt;">59.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.5.4.6" style="padding-left:6.5pt;padding-right:6.5pt;">57.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.5.4.7" style="padding-left:6.5pt;padding-right:6.5pt;">58.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.5.4.8" style="padding-left:6.5pt;padding-right:6.5pt;">58.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T9.1.1.5.4.9" style="padding-left:6.5pt;padding-right:6.5pt;">56.1</td>
</tr>
<tr class="ltx_tr" id="A4.T9.1.1.6.5">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A4.T9.1.1.6.5.1" style="padding-left:6.5pt;padding-right:6.5pt;">LSTM</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T9.1.1.6.5.2" style="padding-left:6.5pt;padding-right:6.5pt;">60.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T9.1.1.6.5.3" style="padding-left:6.5pt;padding-right:6.5pt;">58.9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T9.1.1.6.5.4" style="padding-left:6.5pt;padding-right:6.5pt;">60.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T9.1.1.6.5.5" style="padding-left:6.5pt;padding-right:6.5pt;">60.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T9.1.1.6.5.6" style="padding-left:6.5pt;padding-right:6.5pt;">57.6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T9.1.1.6.5.7" style="padding-left:6.5pt;padding-right:6.5pt;">59.3</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T9.1.1.6.5.8" style="padding-left:6.5pt;padding-right:6.5pt;">60.4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T9.1.1.6.5.9" style="padding-left:6.5pt;padding-right:6.5pt;">57.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>The performance of the baselines on PAWS-X. </figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Full results XLM-R Large</h2>
<figure class="ltx_figure" id="A5.F4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.F4.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A5.F4.1.g1" src="x5.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>The accuracy score of XLM-R model on PAWS-X task across 196 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.F5.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A5.F5.1.g1" src="x6.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The accuracy score of XLM-R model on XNLI task across 196 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.F6.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A5.F6.1.g1" src="x7.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>The accuracy score of XLM-R model on XStoryCloze task across 196 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A5.F7.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A5.F7.1.g1" src="x8.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>The accuracy score of XLM-R model on XCOPA task across 196 languages.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Full results BLOOMz</h2>
<figure class="ltx_figure" id="A6.F8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F8.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F8.1.g1" src="x9.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>The accuracy score of BLOOMz model on B-NLI task across 168 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F9">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F9.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F9.1.g1" src="x10.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>The accuracy score of BLOOMz model on XStoryCloze task across 168 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F10.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F10.1.g1" src="x11.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>The accuracy score of BLOOMz model on XCOPA task across 168 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F11">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F11.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F11.1.g1" src="x12.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>The accuracy score of AYA on XStoryCloze task across 198 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F12">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F12.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F12.1.g1" src="x13.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>The accuracy score of AYA on XCOPA task across 198 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F13">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F13.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F13.1.g1" src="x14.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>The accuracy score of AYA on B-NLI task across 198 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F14">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F14.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F14.1.g1" src="x15.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>The accuracy score of XLM-R base on XNLI task across 196 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F15">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F15.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F15.1.g1" src="x16.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>The accuracy score of XLM-R base on XStoryCloze task across 196 languages.</figcaption>
</figure>
<figure class="ltx_figure" id="A6.F16">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A6.F16.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A6.F16.1.g1" src="x17.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>The accuracy score of XLM-R base on PAWS-X task across 196 languages.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>ChrF++ scores per language</h2>
<figure class="ltx_figure" id="A7.F17">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A7.F17.1" style="width:435.5pt;height:527.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1.0,1.0) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="1013" id="A7.F17.1.g1" src="x18.png" width="830"/>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Chrf++ scores for the selected languages.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Language coverage</h2>
<figure class="ltx_table" id="A8.T10">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A8.T10.1" style="width:374.4pt;height:807.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.8pt,100.8pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_align_middle" id="A8.T10.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A8.T10.1.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_tt" id="A8.T10.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="A8.T10.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">Category</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A8.T10.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">Languages</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A8.T10.1.1.2.2.1" rowspan="30" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="A8.T10.1.1.2.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A8.T10.1.1.2.2.1.1.1" style="width:6.8pt;height:33.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:33.6pt;transform:translate(-13.39pt,-13.39pt) rotate(-90deg) ;">
<span class="ltx_p" id="A8.T10.1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A8.T10.1.1.2.2.1.1.1.1.1">XLM-R</span></span>
</span></span></span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T10.1.1.2.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">High</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T10.1.1.2.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">arb_Arab ,
bul_Cyrl ,
dan_Latn ,
deu_Latn ,
ell_Grek ,
eng_Latn ,
fin_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.3.3">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.3.3.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.3.3.2" style="padding-left:6.5pt;padding-right:6.5pt;">fra_Latn ,
heb_Hebr ,
hun_Latn ,
ind_Latn ,
ita_Latn ,
jpn_Jpan ,
kor_Hang ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.4.4">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.4.4.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.4.4.2" style="padding-left:6.5pt;padding-right:6.5pt;">nld_Latn ,
nob_Latn ,
pes_Arab ,
pol_Latn ,
por_Latn ,
ron_Latn ,
rus_Cyrl ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.5.5">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.5.5.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.5.5.2" style="padding-left:6.5pt;padding-right:6.5pt;">spa_Latn ,
tha_Thai ,
ukr_Cyrl ,
vie_Latn ,
zho_Hans</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.6.6">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T10.1.1.6.6.1" style="padding-left:6.5pt;padding-right:6.5pt;">Mid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T10.1.1.6.6.2" style="padding-left:6.5pt;padding-right:6.5pt;">als_Latn ,
azj_Latn ,
bel_Cyrl ,
ben_Beng ,
cat_Latn ,
ces_Latn ,
est_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.7.7">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.7.7.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.7.7.2" style="padding-left:6.5pt;padding-right:6.5pt;">glg_Latn ,
hin_Deva ,
hrv_Latn ,
hye_Armn ,
isl_Latn ,
kan_Knda ,
kat_Geor ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.8.8">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.8.8.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.8.8.2" style="padding-left:6.5pt;padding-right:6.5pt;">kaz_Cyrl ,
khk_Cyrl ,
lit_Latn ,
lvs_Latn ,
mal_Mlym ,
mar_Deva ,
mkd_Cyrl ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.9.9">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.9.9.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.9.9.2" style="padding-left:6.5pt;padding-right:6.5pt;">npi_Deva ,
sin_Sinh ,
slk_Latn ,
slv_Latn ,
srp_Cyrl ,
swe_Latn ,
tam_Taml ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.10.10">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.10.10.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.10.10.2" style="padding-left:6.5pt;padding-right:6.5pt;">tel_Telu ,
tgl_Latn ,
tur_Latn ,
urd_Arab ,
zho_Hant ,
zsm_Latn</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.11.11">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T10.1.1.11.11.1" style="padding-left:6.5pt;padding-right:6.5pt;">Low</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T10.1.1.11.11.2" style="padding-left:6.5pt;padding-right:6.5pt;">afr_Latn ,
amh_Ethi ,
asm_Beng ,
bos_Latn ,
ckb_Arab ,
cym_Latn ,
epo_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.12.12">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.12.12.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.12.12.2" style="padding-left:6.5pt;padding-right:6.5pt;">eus_Latn ,
gaz_Latn ,
gla_Latn ,
gle_Latn ,
guj_Gujr ,
hau_Latn ,
jav_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.13.13">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.13.13.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.13.13.2" style="padding-left:6.5pt;padding-right:6.5pt;">khm_Khmr ,
kir_Cyrl ,
lao_Laoo ,
mya_Mymr ,
pan_Guru ,
pbt_Arab ,
plt_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.14.14">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.14.14.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.14.14.2" style="padding-left:6.5pt;padding-right:6.5pt;">san_Deva ,
snd_Arab ,
som_Latn ,
sun_Latn ,
swh_Latn ,
uig_Arab ,
uzn_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.15.15">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.15.15.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.15.15.2" style="padding-left:6.5pt;padding-right:6.5pt;">xho_Latn ,
ydd_Hebr</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.16.16">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T10.1.1.16.16.1" style="padding-left:6.5pt;padding-right:6.5pt;">Unseen</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T10.1.1.16.16.2" style="padding-left:6.5pt;padding-right:6.5pt;">ace_Arab ,
ace_Latn ,
acm_Arab ,
acq_Arab ,
aeb_Arab ,
ajp_Arab ,
aka_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.17.17">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.17.17.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.17.17.2" style="padding-left:6.5pt;padding-right:6.5pt;">apc_Arab ,
ars_Arab ,
ary_Arab ,
arz_Arab ,
ast_Latn ,
awa_Deva ,
ayr_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.18.18">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.18.18.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.18.18.2" style="padding-left:6.5pt;padding-right:6.5pt;">azb_Arab ,
bak_Cyrl ,
bam_Latn ,
ban_Latn ,
bem_Latn ,
bho_Deva ,
bjn_Arab ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.19.19">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.19.19.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.19.19.2" style="padding-left:6.5pt;padding-right:6.5pt;">bjn_Latn ,
bug_Latn ,
ceb_Latn ,
cjk_Latn ,
crh_Latn ,
dik_Latn ,
dyu_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.20.20">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.20.20.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.20.20.2" style="padding-left:6.5pt;padding-right:6.5pt;">ewe_Latn ,
fao_Latn ,
fij_Latn ,
fon_Latn ,
fur_Latn ,
fuv_Latn ,
grn_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.21.21">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.21.21.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.21.21.2" style="padding-left:6.5pt;padding-right:6.5pt;">hat_Latn ,
hne_Deva ,
ibo_Latn ,
ilo_Latn ,
kab_Latn ,
kac_Latn ,
kam_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.22.22">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.22.22.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.22.22.2" style="padding-left:6.5pt;padding-right:6.5pt;">kas_Arab ,
kas_Deva ,
kbp_Latn ,
kea_Latn ,
kik_Latn ,
kin_Latn ,
kmb_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.23.23">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.23.23.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.23.23.2" style="padding-left:6.5pt;padding-right:6.5pt;">kmr_Latn ,
knc_Arab ,
knc_Latn ,
kon_Latn ,
lij_Latn ,
lim_Latn ,
lin_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.24.24">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.24.24.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.24.24.2" style="padding-left:6.5pt;padding-right:6.5pt;">lmo_Latn ,
ltg_Latn ,
ltz_Latn ,
lua_Latn ,
lug_Latn ,
luo_Latn ,
lus_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.25.25">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.25.25.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.25.25.2" style="padding-left:6.5pt;padding-right:6.5pt;">mag_Deva ,
mai_Deva ,
min_Latn ,
mlt_Latn ,
mni_Beng ,
mos_Latn ,
mri_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.26.26">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.26.26.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.26.26.2" style="padding-left:6.5pt;padding-right:6.5pt;">nno_Latn ,
nso_Latn ,
nus_Latn ,
nya_Latn ,
oci_Latn ,
pag_Latn ,
pap_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.27.27">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.27.27.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.27.27.2" style="padding-left:6.5pt;padding-right:6.5pt;">prs_Arab ,
quy_Latn ,
run_Latn ,
sag_Latn ,
scn_Latn ,
shn_Mymr ,
smo_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.28.28">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.28.28.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.28.28.2" style="padding-left:6.5pt;padding-right:6.5pt;">sna_Latn ,
sot_Latn ,
srd_Latn ,
ssw_Latn ,
szl_Latn ,
taq_Latn ,
tat_Cyrl ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.29.29">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.29.29.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.29.29.2" style="padding-left:6.5pt;padding-right:6.5pt;">tgk_Cyrl ,
tir_Ethi ,
tpi_Latn ,
tsn_Latn ,
tso_Latn ,
tuk_Latn ,
tum_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.30.30">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.30.30.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.30.30.2" style="padding-left:6.5pt;padding-right:6.5pt;">twi_Latn ,
umb_Latn ,
vec_Latn ,
war_Latn ,
wol_Latn ,
yor_Latn ,
yue_Hant ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.31.31">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.31.31.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.31.31.2" style="padding-left:6.5pt;padding-right:6.5pt;">zul_Latn</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.32.32">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_l ltx_border_r ltx_border_tt" id="A8.T10.1.1.32.32.1" rowspan="28" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="A8.T10.1.1.32.32.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A8.T10.1.1.32.32.1.1.1" style="width:6.8pt;height:42.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.5pt;transform:translate(-17.83pt,-17.83pt) rotate(-90deg) ;">
<span class="ltx_p" id="A8.T10.1.1.32.32.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A8.T10.1.1.32.32.1.1.1.1.1">BLOOMz</span></span>
</span></span></span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="A8.T10.1.1.32.32.2" style="padding-left:6.5pt;padding-right:6.5pt;">High</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A8.T10.1.1.32.32.3" style="padding-left:6.5pt;padding-right:6.5pt;">arb_Arab ,
cat_Latn ,
eng_Latn ,
fra_Latn ,
ind_Latn ,
por_Latn ,
spa_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.33.33">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.33.33.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.33.33.2" style="padding-left:6.5pt;padding-right:6.5pt;">vie_Latn ,
zho_Hans</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.34.34">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T10.1.1.34.34.1" style="padding-left:6.5pt;padding-right:6.5pt;">Mid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T10.1.1.34.34.2" style="padding-left:6.5pt;padding-right:6.5pt;">ben_Beng ,
eus_Latn ,
hin_Deva ,
mal_Mlym ,
tam_Taml ,
urd_Arab ,
zho_Hant’</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.35.35">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T10.1.1.35.35.1" style="padding-left:6.5pt;padding-right:6.5pt;">Low</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T10.1.1.35.35.2" style="padding-left:6.5pt;padding-right:6.5pt;">aka_Latn ,
asm_Beng ,
bam_Latn ,
bho_Deva ,
fon_Latn ,
guj_Gujr ,
ibo_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.36.36">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.36.36.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.36.36.2" style="padding-left:6.5pt;padding-right:6.5pt;">kan_Knda ,
kik_Latn ,
kin_Latn ,
lin_Latn ,
mar_Deva ,
npi_Deva ,
nso_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.37.37">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.37.37.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.37.37.2" style="padding-left:6.5pt;padding-right:6.5pt;">sot_Latn ,
swh_Latn ,
tel_Telu ,
wol_Latn ,
xho_Latn ,
yor_Latn ,
zul_Latn</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.38.38">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T10.1.1.38.38.1" style="padding-left:6.5pt;padding-right:6.5pt;">Unseen</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T10.1.1.38.38.2" style="padding-left:6.5pt;padding-right:6.5pt;">ace_Arab ,
ace_Latn ,
acm_Arab ,
acq_Arab ,
aeb_Arab ,
afr_Latn ,
ajp_Arab ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.39.39">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.39.39.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.39.39.2" style="padding-left:6.5pt;padding-right:6.5pt;">apc_Arab ,
ars_Arab ,
ary_Arab ,
arz_Arab ,
ast_Latn ,
awa_Deva ,
ayr_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.40.40">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.40.40.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.40.40.2" style="padding-left:6.5pt;padding-right:6.5pt;">azb_Arab ,
azj_Latn ,
ban_Latn ,
bem_Latn ,
bjn_Arab ,
bjn_Latn ,
bos_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.41.41">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.41.41.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.41.41.2" style="padding-left:6.5pt;padding-right:6.5pt;">bug_Latn ,
ceb_Latn ,
ces_Latn ,
cjk_Latn ,
ckb_Arab ,
crh_Latn ,
cym_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.42.42">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.42.42.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.42.42.2" style="padding-left:6.5pt;padding-right:6.5pt;">dan_Latn ,
deu_Latn ,
dik_Latn ,
dyu_Latn ,
epo_Latn ,
est_Latn ,
ewe_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.43.43">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.43.43.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.43.43.2" style="padding-left:6.5pt;padding-right:6.5pt;">fao_Latn ,
fij_Latn ,
fin_Latn ,
fur_Latn ,
fuv_Latn ,
gla_Latn ,
gle_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.44.44">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.44.44.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.44.44.2" style="padding-left:6.5pt;padding-right:6.5pt;">glg_Latn ,
grn_Latn ,
hat_Latn ,
hau_Latn ,
hne_Deva ,
hrv_Latn ,
hun_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.45.45">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.45.45.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.45.45.2" style="padding-left:6.5pt;padding-right:6.5pt;">ilo_Latn ,
isl_Latn ,
ita_Latn ,
jav_Latn ,
kab_Latn ,
kac_Latn ,
kam_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.46.46">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.46.46.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.46.46.2" style="padding-left:6.5pt;padding-right:6.5pt;">kas_Arab ,
kas_Deva ,
knc_Arab ,
knc_Latn ,
kbp_Latn ,
kea_Latn ,
kmb_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.47.47">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.47.47.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.47.47.2" style="padding-left:6.5pt;padding-right:6.5pt;">kmr_Latn ,
kon_Latn ,
lij_Latn ,
lim_Latn ,
lit_Latn ,
lmo_Latn ,
ltg_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.48.48">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.48.48.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.48.48.2" style="padding-left:6.5pt;padding-right:6.5pt;">ltz_Latn ,
lua_Latn ,
lug_Latn ,
luo_Latn ,
lus_Latn ,
lvs_Latn ,
mag_Deva ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.49.49">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.49.49.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.49.49.2" style="padding-left:6.5pt;padding-right:6.5pt;">mai_Deva ,
min_Latn ,
plt_Latn ,
mlt_Latn ,
mni_Beng ,
mos_Latn ,
mri_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.50.50">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.50.50.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.50.50.2" style="padding-left:6.5pt;padding-right:6.5pt;">nld_Latn ,
nno_Latn ,
nob_Latn ,
nus_Latn ,
nya_Latn ,
oci_Latn ,
gaz_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.51.51">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.51.51.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.51.51.2" style="padding-left:6.5pt;padding-right:6.5pt;">pag_Latn ,
pap_Latn ,
pes_Arab ,
pol_Latn ,
prs_Arab ,
pbt_Arab ,
quy_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.52.52">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.52.52.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.52.52.2" style="padding-left:6.5pt;padding-right:6.5pt;">ron_Latn ,
run_Latn ,
sag_Latn ,
san_Deva ,
scn_Latn ,
slk_Latn ,
slv_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.53.53">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.53.53.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.53.53.2" style="padding-left:6.5pt;padding-right:6.5pt;">smo_Latn ,
sna_Latn ,
snd_Arab ,
som_Latn ,
als_Latn ,
srd_Latn ,
ssw_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.54.54">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.54.54.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.54.54.2" style="padding-left:6.5pt;padding-right:6.5pt;">sun_Latn ,
swe_Latn ,
szl_Latn ,
tgl_Latn ,
taq_Latn ,
tpi_Latn ,
tsn_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.55.55">
<td class="ltx_td ltx_border_r" id="A8.T10.1.1.55.55.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T10.1.1.55.55.2" style="padding-left:6.5pt;padding-right:6.5pt;">tso_Latn ,
tuk_Latn ,
tum_Latn ,
tur_Latn ,
twi_Latn ,
uig_Arab ,
umb_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T10.1.1.56.56">
<td class="ltx_td ltx_border_bb ltx_border_r" id="A8.T10.1.1.56.56.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A8.T10.1.1.56.56.2" style="padding-left:6.5pt;padding-right:6.5pt;">uzn_Latn ,
vec_Latn ,
war_Latn ,
yue_Hant ,
zsm_Latn</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>The languages covered during pretraining of each of the MLMs categorized by the amount of data that was seen for them during pretraining.</figcaption>
</figure>
<figure class="ltx_table" id="A8.T11">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A8.T11.1" style="width:375.1pt;height:447.2pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-46.9pt,55.8pt) scale(0.8,0.8) ;">
<table class="ltx_tabular ltx_align_middle" id="A8.T11.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A8.T11.1.1.1.1">
<td class="ltx_td ltx_border_l ltx_border_r ltx_border_tt" id="A8.T11.1.1.1.1.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="A8.T11.1.1.1.1.2" style="padding-left:6.5pt;padding-right:6.5pt;">Category</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A8.T11.1.1.1.1.3" style="padding-left:6.5pt;padding-right:6.5pt;">Languages</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_l ltx_border_r ltx_border_t" id="A8.T11.1.1.2.2.1" rowspan="30" style="padding-left:6.5pt;padding-right:6.5pt;"><span class="ltx_text" id="A8.T11.1.1.2.2.1.1">
<span class="ltx_inline-block ltx_transformed_outer" id="A8.T11.1.1.2.2.1.1.1" style="width:6.8pt;height:20.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:20.8pt;transform:translate(-7pt,-7pt) rotate(-90deg) ;">
<span class="ltx_p" id="A8.T11.1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="A8.T11.1.1.2.2.1.1.1.1.1">AYA</span></span>
</span></span></span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T11.1.1.2.2.2" style="padding-left:6.5pt;padding-right:6.5pt;">High</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T11.1.1.2.2.3" style="padding-left:6.5pt;padding-right:6.5pt;">hye_Armn ,
kan_Knda ,
tur_Latn ,
ita_Latn ,
nld_Latn ,
pol_Latn ,
por_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.3.3">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.3.3.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.3.3.2" style="padding-left:6.5pt;padding-right:6.5pt;">isl_Latn ,
fra_Latn ,
deu_Latn ,
spa_Latn ,
rus_Cyrl ,
eng_Latn</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.4.4">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T11.1.1.4.4.1" style="padding-left:6.5pt;padding-right:6.5pt;">Mid</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T11.1.1.4.4.2" style="padding-left:6.5pt;padding-right:6.5pt;">est_Latn ,
ben_Beng ,
mar_Deva ,
slv_Latn ,
lit_Latn ,
heb_Hebr ,
zsm_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.5.5">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.5.5.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.5.5.2" style="padding-left:6.5pt;padding-right:6.5pt;">cat_Latn ,
tha_Thai ,
kor_Hang ,
slk_Latn ,
hin_Deva ,
bul_Cyrl ,
nob_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.6.6">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.6.6.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.6.6.2" style="padding-left:6.5pt;padding-right:6.5pt;">fin_Latn ,
dan_Latn ,
hun_Latn ,
ukr_Cyrl ,
ell_Grek ,
ron_Latn ,
swe_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.7.7">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.7.7.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.7.7.2" style="padding-left:6.5pt;padding-right:6.5pt;">arb_Arab ,
pes_Arab ,
zho_Hans ,
ces_Latn</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.8.8">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T11.1.1.8.8.1" style="padding-left:6.5pt;padding-right:6.5pt;">Low</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T11.1.1.8.8.2" style="padding-left:6.5pt;padding-right:6.5pt;">hat_Latn ,
kor_Hang ,
xho_Latn ,
ibo_Latn ,
lao_Laoo ,
mri_Latn ,
smo_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.9.9">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.9.9.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.9.9.2" style="padding-left:6.5pt;padding-right:6.5pt;">ckb_Arab ,
amh_Ethi ,
nya_Latn ,
hau_Latn ,
plt_Latn ,
pbt_Arab ,
gla_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.10.10">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.10.10.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.10.10.2" style="padding-left:6.5pt;padding-right:6.5pt;">sun_Latn ,
jpn_Jpan ,
sot_Latn ,
ceb_Latn ,
pan_Guru ,
gle_Latn ,
kir_Cyrl ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.11.11">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.11.11.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.11.11.2" style="padding-left:6.5pt;padding-right:6.5pt;">epo_Latn ,
sin_Sinh ,
guj_Gujr ,
yor_Latn ,
tgk_Cyrl ,
snd_Arab ,
mya_Mymr ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.12.12">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.12.12.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.12.12.2" style="padding-left:6.5pt;padding-right:6.5pt;">kaz_Cyrl ,
khm_Khmr ,
som_Latn ,
swh_Latn ,
ydd_Hebr ,
uzn_Latn ,
hun_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.13.13">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.13.13.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.13.13.2" style="padding-left:6.5pt;padding-right:6.5pt;">mlt_Latn ,
eus_Latn ,
bel_Cyrl ,
kat_Geor ,
mkd_Cyrl ,
mal_Mlym ,
khk_Cyrl ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.14.14">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.14.14.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.14.14.2" style="padding-left:6.5pt;padding-right:6.5pt;">tha_Thai ,
afr_Latn ,
ukr_Cyrl ,
ltz_Latn ,
tel_Telu ,
urd_Arab ,
lit_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.15.15">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.15.15.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.15.15.2" style="padding-left:6.5pt;padding-right:6.5pt;">npi_Deva ,
srp_Cyrl ,
tam_Taml ,
cym_Latn ,
als_Latn ,
glg_Latn ,
azj_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.16.16">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.16.16.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.16.16.2" style="padding-left:6.5pt;padding-right:6.5pt;">lvs_Latn</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.17.17">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="A8.T11.1.1.17.17.1" style="padding-left:6.5pt;padding-right:6.5pt;">Unseen</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A8.T11.1.1.17.17.2" style="padding-left:6.5pt;padding-right:6.5pt;">ace_Arab ,
ace_Latn ,
acm_Arab ,
acq_Arab ,
aeb_Arab ,
ajp_Arab ,
aka_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.18.18">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.18.18.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.18.18.2" style="padding-left:6.5pt;padding-right:6.5pt;">apc_Arab ,
ars_Arab ,
ary_Arab ,
arz_Arab ,
asm_Beng ,
ast_Latn ,
awa_Deva ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.19.19">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.19.19.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.19.19.2" style="padding-left:6.5pt;padding-right:6.5pt;">ayr_Latn ,
azb_Arab ,
bak_Cyrl ,
bam_Latn ,
ban_Latn ,
bem_Latn ,
bho_Deva ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.20.20">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.20.20.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.20.20.2" style="padding-left:6.5pt;padding-right:6.5pt;">bjn_Arab ,
bjn_Latn ,
bos_Latn ,
bug_Latn ,
cjk_Latn ,
crh_Latn ,
dik_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.21.21">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.21.21.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.21.21.2" style="padding-left:6.5pt;padding-right:6.5pt;">dyu_Latn ,
ewe_Latn ,
fao_Latn ,
fij_Latn ,
fon_Latn ,
fur_Latn ,
fuv_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.22.22">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.22.22.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.22.22.2" style="padding-left:6.5pt;padding-right:6.5pt;">grn_Latn ,
hne_Deva ,
hrv_Latn ,
ilo_Latn ,
kab_Latn ,
kac_Latn ,
kam_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.23.23">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.23.23.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.23.23.2" style="padding-left:6.5pt;padding-right:6.5pt;">kas_Arab ,
kas_Deva ,
knc_Arab ,
knc_Latn ,
kbp_Latn ,
kea_Latn ,
kik_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.24.24">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.24.24.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.24.24.2" style="padding-left:6.5pt;padding-right:6.5pt;">kin_Latn ,
kmb_Latn ,
kmr_Latn ,
kon_Latn ,
lij_Latn ,
lim_Latn ,
lin_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.25.25">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.25.25.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.25.25.2" style="padding-left:6.5pt;padding-right:6.5pt;">lmo_Latn ,
ltg_Latn ,
lua_Latn ,
lug_Latn ,
luo_Latn ,
lus_Latn ,
mag_Deva ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.26.26">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.26.26.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.26.26.2" style="padding-left:6.5pt;padding-right:6.5pt;">mai_Deva ,
min_Latn ,
mni_Beng ,
mos_Latn ,
nno_Latn ,
nso_Latn ,
nus_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.27.27">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.27.27.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.27.27.2" style="padding-left:6.5pt;padding-right:6.5pt;">oci_Latn ,
gaz_Latn ,
pag_Latn ,
pap_Latn ,
prs_Arab ,
quy_Latn ,
run_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.28.28">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.28.28.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.28.28.2" style="padding-left:6.5pt;padding-right:6.5pt;">sag_Latn ,
san_Deva ,
scn_Latn ,
shn_Mymr ,
srd_Latn ,
ssw_Latn ,
szl_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.29.29">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.29.29.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.29.29.2" style="padding-left:6.5pt;padding-right:6.5pt;">tat_Cyrl ,
tgl_Latn ,
tir_Ethi ,
taq_Latn ,
tpi_Latn ,
tsn_Latn ,
tso_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.30.30">
<td class="ltx_td ltx_border_r" id="A8.T11.1.1.30.30.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A8.T11.1.1.30.30.2" style="padding-left:6.5pt;padding-right:6.5pt;">tuk_Latn ,
tum_Latn ,
twi_Latn ,
tzm_Tfng ,
uig_Arab ,
umb_Latn ,
vec_Latn ,</td>
</tr>
<tr class="ltx_tr" id="A8.T11.1.1.31.31">
<td class="ltx_td ltx_border_bb ltx_border_r" id="A8.T11.1.1.31.31.1" style="padding-left:6.5pt;padding-right:6.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A8.T11.1.1.31.31.2" style="padding-left:6.5pt;padding-right:6.5pt;">war_Latn ,
wol_Latn ,
yue_Hant ,
zho_Hant ,
dzo_Tibt</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>The languages covered during AYA’s pretraining categorized by the amount of data that was seen during pretraining.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A9">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Full Results for XLM-R base</h2>
<figure class="ltx_table" id="A9.T12">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A9.T12.3" style="width:416.7pt;height:53.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-144.8pt,18.4pt) scale(0.59,0.59) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A9.T12.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A9.T12.3.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="A9.T12.3.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">ar</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">bg</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">de</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">el</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">es</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">et</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">eu</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">fr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.10" style="padding-left:2.0pt;padding-right:2.0pt;">hi</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.11" style="padding-left:2.0pt;padding-right:2.0pt;">ht</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.12" style="padding-left:2.0pt;padding-right:2.0pt;">id</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.13" style="padding-left:2.0pt;padding-right:2.0pt;">it</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.14" style="padding-left:2.0pt;padding-right:2.0pt;">ja</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.15" style="padding-left:2.0pt;padding-right:2.0pt;">ko</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.16" style="padding-left:2.0pt;padding-right:2.0pt;">my</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.17" style="padding-left:2.0pt;padding-right:2.0pt;">qu</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.18" style="padding-left:2.0pt;padding-right:2.0pt;">ru</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.19" style="padding-left:2.0pt;padding-right:2.0pt;">sw</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.20" style="padding-left:2.0pt;padding-right:2.0pt;">ta</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.21" style="padding-left:2.0pt;padding-right:2.0pt;">te</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.22" style="padding-left:2.0pt;padding-right:2.0pt;">th</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.23" style="padding-left:2.0pt;padding-right:2.0pt;">tr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.24" style="padding-left:2.0pt;padding-right:2.0pt;">ur</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.25" style="padding-left:2.0pt;padding-right:2.0pt;">vi</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A9.T12.3.1.1.1.26" style="padding-left:2.0pt;padding-right:2.0pt;">zh</th>
</tr>
<tr class="ltx_tr" id="A9.T12.3.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="A9.T12.3.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="25" id="A9.T12.3.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="A9.T12.3.1.2.2.2.1">XLM-R base</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A9.T12.3.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A9.T12.3.1.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">XStoryCloze</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">68/70</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">74/74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.8" style="padding-left:2.0pt;padding-right:2.0pt;">67/65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.9" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.10" style="padding-left:2.0pt;padding-right:2.0pt;">69/69</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.12" style="padding-left:2.0pt;padding-right:2.0pt;">76/76</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.16" style="padding-left:2.0pt;padding-right:2.0pt;">63/65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.18" style="padding-left:2.0pt;padding-right:2.0pt;">75/74</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.19" style="padding-left:2.0pt;padding-right:2.0pt;">64/63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.21" style="padding-left:2.0pt;padding-right:2.0pt;">68/65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.22" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.23" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.25" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A9.T12.3.1.3.1.26" style="padding-left:2.0pt;padding-right:2.0pt;">77/77</td>
</tr>
<tr class="ltx_tr" id="A9.T12.3.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A9.T12.3.1.4.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">XNLI</th>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">71/73</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">78/79</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">75/77</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">75/77</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">79/75</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.9" style="padding-left:2.0pt;padding-right:2.0pt;">78/78</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.10" style="padding-left:2.0pt;padding-right:2.0pt;">65/67</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.14" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.15" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.18" style="padding-left:2.0pt;padding-right:2.0pt;">75/78</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.19" style="padding-left:2.0pt;padding-right:2.0pt;">70/74</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.22" style="padding-left:2.0pt;padding-right:2.0pt;">71/73</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.23" style="padding-left:2.0pt;padding-right:2.0pt;">73/75</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.24" style="padding-left:2.0pt;padding-right:2.0pt;">65/73</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.25" style="padding-left:2.0pt;padding-right:2.0pt;">74/77</td>
<td class="ltx_td ltx_align_center" id="A9.T12.3.1.4.2.26" style="padding-left:2.0pt;padding-right:2.0pt;">73/69</td>
</tr>
<tr class="ltx_tr" id="A9.T12.3.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="A9.T12.3.1.5.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">PAWS-X</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">87/89</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">88/89</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.8" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.9" style="padding-left:2.0pt;padding-right:2.0pt;">89/89</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.10" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.11" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.12" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.13" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.14" style="padding-left:2.0pt;padding-right:2.0pt;">77/81</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.15" style="padding-left:2.0pt;padding-right:2.0pt;">76/82</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.16" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.17" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.18" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.19" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.20" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.21" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.22" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.23" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.24" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.25" style="padding-left:2.0pt;padding-right:2.0pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A9.T12.3.1.5.3.26" style="padding-left:2.0pt;padding-right:2.0pt;">82/79</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>The performance in (<math alttext="\%" class="ltx_Math" display="inline" id="A9.T12.2.m1.1"><semantics id="A9.T12.2.m1.1b"><mo id="A9.T12.2.m1.1.1" xref="A9.T12.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A9.T12.2.m1.1c"><csymbol cd="latexml" id="A9.T12.2.m1.1.1.cmml" xref="A9.T12.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A9.T12.2.m1.1d">\%</annotation><annotation encoding="application/x-llamapun" id="A9.T12.2.m1.1e">%</annotation></semantics></math>) accuracy when evaluating the XLM-R base on the human translated (original) datasets/our machine translated datasets.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Jun 20 12:44:10 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
