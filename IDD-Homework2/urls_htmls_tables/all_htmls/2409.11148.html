<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Improving the Efficiency of Visually Augmented Language Models</title>
<!--Generated on Tue Sep 17 12:57:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.11148v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S1" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S2" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S3" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">Blind-VaLM</span> architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S4" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S5" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S6" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S7" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A1" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Training hyper-parameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A2" title="In Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Evaluation details</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">Improving the Efficiency of Visually Augmented Language Models</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Paula Ontalvilla 
<br class="ltx_break"/>HiTZ Center - Ixa, 
<br class="ltx_break"/>University of the 
<br class="ltx_break"/>Basque Country UPV/EHU
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1.id1">paula.ontalvilla@ehu.eus</span>
<br class="ltx_break"/>&amp;Aitor Ormazabal 
<br class="ltx_break"/>HiTZ Center - Ixa, 
<br class="ltx_break"/>University of the 
<br class="ltx_break"/>Basque Country UPV/EHU
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.2.id2">aitor.ormazabal@ehu.eus</span>
<br class="ltx_break"/>
<br class="ltx_break"/>&amp;Gorka Azkune 
<br class="ltx_break"/>HiTZ Center - Ixa, 
<br class="ltx_break"/>University of the 
<br class="ltx_break"/>Basque Country UPV/EHU
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.3.id3">gorka.azcune@ehu.eus</span>
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id4.id1">Despite the impressive performance of autoregressive Language Models (LM) it has been shown that due to reporting bias, LMs lack visual knowledge, i.e. they do not know much about the visual world and its properties. To augment LMs with visual knowledge, existing solutions often rely on explicit images, requiring time-consuming retrieval or image generation systems.
This paper shows that
explicit images are not necessary to visually augment an LM. Instead, we use visually-grounded text representations obtained from the well-known CLIP multimodal system. For a fair comparison, we modify <span class="ltx_text ltx_font_smallcaps" id="id4.id1.1">VaLM</span>, a visually-augmented LM which uses image retrieval and representation, to work directly with visually-grounded text representations. We name this new model <span class="ltx_text ltx_font_smallcaps" id="id4.id1.2">Blind-VaLM</span>. We show that <span class="ltx_text ltx_font_smallcaps" id="id4.id1.3">Blind-VaLM</span> performs on par with <span class="ltx_text ltx_font_smallcaps" id="id4.id1.4">VaLM</span> for Visual Language Understanding (VLU), Natural Language Understanding (NLU) and Language Modeling tasks, despite being significantly more efficient and simpler.
We also show that scaling up our model within the compute budget of <span class="ltx_text ltx_font_smallcaps" id="id4.id1.5">VaLM</span>, either increasing the model or pre-training corpus size, we outperform <span class="ltx_text ltx_font_smallcaps" id="id4.id1.6">VaLM</span> for all the evaluation tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">Improving the Efficiency of Visually Augmented Language Models</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Paula Ontalvilla</span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1">HiTZ Center - Ixa,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1">University of the</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1">Basque Country UPV/EHU</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.1.1.5.5.1.1">paula.ontalvilla@ehu.eus</span></span></span>
</span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.1.2.2" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.2.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.2.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.2.1.1.1.1.1">Aitor Ormazabal</span></span></span>
<span class="ltx_tr" id="p1.1.2.2.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.2.2.1">HiTZ Center - Ixa,</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.3.3.1">University of the</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.4.4.1">Basque Country UPV/EHU</span></span>
<span class="ltx_tr" id="p1.1.2.2.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.2.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.2.1.5.5.1.1">aitor.ormazabal@ehu.eus</span></span></span>
</span>
</span></span>                      <span class="ltx_text ltx_inline-block" id="p1.1.2.3" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.3.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.3.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.3.1.1.1.1.1">Gorka Azkune</span></span></span>
<span class="ltx_tr" id="p1.1.2.3.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.2.2.1">HiTZ Center - Ixa,</span></span>
<span class="ltx_tr" id="p1.1.2.3.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.3.3.1">University of the</span></span>
<span class="ltx_tr" id="p1.1.2.3.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.4.4.1">Basque Country UPV/EHU</span></span>
<span class="ltx_tr" id="p1.1.2.3.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.3.1.5.5.1"><span class="ltx_text ltx_font_typewriter" id="p1.1.2.3.1.5.5.1.1">gorka.azcune@ehu.eus</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Autoregressive Language Models, such as GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib1" title="">2023</a>)</cite> and Llama <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib8" title="">2024</a>)</cite>, are the reference systems for Natural Language Understanding and Generation. However, due to reporting bias in textual corpora <cite class="ltx_cite ltx_citemacro_citep">(Shwartz and Choi, <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib20" title="">2020</a>)</cite>, LMs lack visual knowledge, which means that they do not know the visual properties of our world, struggling to predict the typical colors, sizes and shapes of real objects, for instance <cite class="ltx_cite ltx_citemacro_citep">(Alper et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib2" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib28" title="">2022</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib12" title="">2022</a>)</cite>. Several researchers tried to overcome those problems augmenting LMs with visual knowledge <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib22" title="">2020</a>; Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib24" title="">2021</a>; Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib27" title="">2022</a>; Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib14" title="">2022</a>)</cite>, but focusing specially on Masked Language Models (MLM). MLMs are limited for text generation and are not as versatile as autoregressive LMs. A recent example of visually augmenting autoregressive LMs is <span class="ltx_text ltx_font_smallcaps" id="S1.p1.1.1">VaLM</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib25" title="">2022</a>)</cite>, which leverages image retrieval and representation using a pretrained CLIP multimodal model <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib18" title="">2021</a>)</cite> to improve next token prediction. To effectively use visual information, they add a Fusion Layer to a base LM, allowing textual tokens to attend visual representations before next token prediction. They show that <span class="ltx_text ltx_font_smallcaps" id="S1.p1.1.2">VaLM</span> improves significantly the performance for Visual Language Understanding (VLU), without degrading the NLU and text generation capabilities of the base LM.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">But image retrieval and representation are very resource intensive, significantly impacting training and inference times. For a improved efficiency, we propose to directly use visually-grounded textual representations, obtained from the CLIP model. Based on the <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.1">VaLM</span> architecture, we input visually-grounded textual representations to the Fusion Layer, avoiding image retrieval and representation. We name this new model <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.2">Blind-VaLM</span>. As the result of our experiments we show that: i) <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.3">Blind-VaLM</span> is orders of magnitude faster than <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.4">VaLM</span> for both training and inference; ii) <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.5">Blind-VaLM</span> performs on par with <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.6">VaLM</span> for VLU, NLU and LM tasks; iii) maintaining within the compute budget of <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.7">VaLM</span>, but increasing the size of the pretraining corpus or the base LLM, <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.8">Blind-VaLM</span> improves the results of <span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.9">VaLM</span> for all the evaluation tasks. All the code is publicly available<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/paulaonta/Blind-VaLM" title="">https://github.com/paulaonta/Blind-VaLM</a></span></span></span>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="233" id="S1.F1.g1" src="extracted/5860392/img/VaLM_coling.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Architecture comparison of the original <span class="ltx_text ltx_font_smallcaps" id="S1.F1.3.1">VaLM</span> (left) and our proposed <span class="ltx_text ltx_font_smallcaps" id="S1.F1.4.2">Blind-VaLM</span> (right).</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">There are several approaches in the literature to augment Language Models with visual knowledge. Most of them focus on Masked Language Models (MLM) such as BERT <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib7" title="">2018</a>)</cite> or RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib13" title="">2019</a>)</cite>, proposing different approaches: Vokenization <cite class="ltx_cite ltx_citemacro_citep">(Tan and Bansal, <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib22" title="">2020</a>)</cite>, VidLanKD <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib24" title="">2021</a>)</cite>, Z-LaVI <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib27" title="">2022</a>)</cite> and iACE <cite class="ltx_cite ltx_citemacro_citep">(Lu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib14" title="">2022</a>)</cite> are good examples. Given the limitations of MLMs for text generation, <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib23" title="">2023</a>)</cite> explore encoder-decoder architectures showing promising results for various generation tasks. But to the best of our knowledge, <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.1">VaLM</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib25" title="">2022</a>)</cite> is the first work for augmenting decoder-only LMs with visual knowledge. More concretely, they augment a decoder-only LM with the so-called <span class="ltx_text ltx_font_italic" id="S2.p1.1.2">Visual Knowledge Fusion Layer</span>, where contextual text representations generated by the LM are combined with the visual representations computed for retrieved images. <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.3">VaLM</span>, similarly to previous approaches, uses images for training and inference, adding a significant overhead to the model. We use <span class="ltx_text ltx_font_smallcaps" id="S2.p1.1.4">VaLM</span> as our base system, since it provides a solid framework to validate our hypothesis, i.e. LMs can be augmented without time-consuming image retrieval and representation. A similar idea is explored by concurrent work <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib9" title="">2023</a>)</cite>, but they do not cover decoder-only LMs.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Blind-VaLM</span> architecture</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">VaLM</span> architecture is composed of three main modules (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> left): 1) a backbone autoregressive LM (GPT2 <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib19" title="">2019</a>)</cite>), 2) a text-to-image retrieval module based on CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib18" title="">2021</a>)</cite>, and 3) the Visual Knowledge Fusion Layer (Fusion Layer for short), to fuse the contextual text representations of the LM with the image representations retrieved for the input text. The intuition is that the retrieved visual representations should help to better predict the next token. For further details on the <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">VaLM</span> architecture and Fusion Layer, see <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib25" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To show that image retrieval and representation are not necessary to augment the backbone LM with visual knowledge, we make one modification to the <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.1">VaLM</span> architecture: Instead of using the CLIP image encoder representations of the retrieved images, <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.2">Blind-VaLM</span> directly uses CLIP text encoder representations of the text itself (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> right).</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.7">More formally, given an input text sequence <math alttext="\{x_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><msubsup id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml"><mrow id="S3.p3.1.m1.1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.2.cmml"><mo id="S3.p3.1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.p3.1.m1.1.1.1.1.2.cmml">{</mo><msub id="S3.p3.1.m1.1.1.1.1.1.1" xref="S3.p3.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.p3.1.m1.1.1.1.1.1.1.2" xref="S3.p3.1.m1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.p3.1.m1.1.1.1.1.1.1.3" xref="S3.p3.1.m1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p3.1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.p3.1.m1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p3.1.m1.1.1.1.3" xref="S3.p3.1.m1.1.1.1.3.cmml"><mi id="S3.p3.1.m1.1.1.1.3.2" xref="S3.p3.1.m1.1.1.1.3.2.cmml">i</mi><mo id="S3.p3.1.m1.1.1.1.3.1" xref="S3.p3.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.p3.1.m1.1.1.1.3.3" xref="S3.p3.1.m1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p3.1.m1.1.1.3" xref="S3.p3.1.m1.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><apply id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.2.cmml" xref="S3.p3.1.m1.1.1">superscript</csymbol><apply id="S3.p3.1.m1.1.1.1.cmml" xref="S3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.2.cmml" xref="S3.p3.1.m1.1.1">subscript</csymbol><set id="S3.p3.1.m1.1.1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.1.1.1"><apply id="S3.p3.1.m1.1.1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p3.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.p3.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.p3.1.m1.1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.1.3"><eq id="S3.p3.1.m1.1.1.1.3.1.cmml" xref="S3.p3.1.m1.1.1.1.3.1"></eq><ci id="S3.p3.1.m1.1.1.1.3.2.cmml" xref="S3.p3.1.m1.1.1.1.3.2">𝑖</ci><cn id="S3.p3.1.m1.1.1.1.3.3.cmml" type="integer" xref="S3.p3.1.m1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p3.1.m1.1.1.3.cmml" xref="S3.p3.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">\{x_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">{ italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>, let <math alttext="H^{0}" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><msup id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml"><mi id="S3.p3.2.m2.1.1.2" xref="S3.p3.2.m2.1.1.2.cmml">H</mi><mn id="S3.p3.2.m2.1.1.3" xref="S3.p3.2.m2.1.1.3.cmml">0</mn></msup><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><apply id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p3.2.m2.1.1.1.cmml" xref="S3.p3.2.m2.1.1">superscript</csymbol><ci id="S3.p3.2.m2.1.1.2.cmml" xref="S3.p3.2.m2.1.1.2">𝐻</ci><cn id="S3.p3.2.m2.1.1.3.cmml" type="integer" xref="S3.p3.2.m2.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">H^{0}</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_H start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT</annotation></semantics></math> denote its corresponding embedding sequence and let <math alttext="H^{l}=LM_{l}(H^{l-1}),l\in[1,L-2]" class="ltx_Math" display="inline" id="S3.p3.3.m3.3"><semantics id="S3.p3.3.m3.3a"><mrow id="S3.p3.3.m3.3.3.2" xref="S3.p3.3.m3.3.3.3.cmml"><mrow id="S3.p3.3.m3.2.2.1.1" xref="S3.p3.3.m3.2.2.1.1.cmml"><msup id="S3.p3.3.m3.2.2.1.1.3" xref="S3.p3.3.m3.2.2.1.1.3.cmml"><mi id="S3.p3.3.m3.2.2.1.1.3.2" xref="S3.p3.3.m3.2.2.1.1.3.2.cmml">H</mi><mi id="S3.p3.3.m3.2.2.1.1.3.3" xref="S3.p3.3.m3.2.2.1.1.3.3.cmml">l</mi></msup><mo id="S3.p3.3.m3.2.2.1.1.2" xref="S3.p3.3.m3.2.2.1.1.2.cmml">=</mo><mrow id="S3.p3.3.m3.2.2.1.1.1" xref="S3.p3.3.m3.2.2.1.1.1.cmml"><mi id="S3.p3.3.m3.2.2.1.1.1.3" xref="S3.p3.3.m3.2.2.1.1.1.3.cmml">L</mi><mo id="S3.p3.3.m3.2.2.1.1.1.2" xref="S3.p3.3.m3.2.2.1.1.1.2.cmml">⁢</mo><msub id="S3.p3.3.m3.2.2.1.1.1.4" xref="S3.p3.3.m3.2.2.1.1.1.4.cmml"><mi id="S3.p3.3.m3.2.2.1.1.1.4.2" xref="S3.p3.3.m3.2.2.1.1.1.4.2.cmml">M</mi><mi id="S3.p3.3.m3.2.2.1.1.1.4.3" xref="S3.p3.3.m3.2.2.1.1.1.4.3.cmml">l</mi></msub><mo id="S3.p3.3.m3.2.2.1.1.1.2a" xref="S3.p3.3.m3.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.p3.3.m3.2.2.1.1.1.1.1" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.cmml"><mo id="S3.p3.3.m3.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.cmml">(</mo><msup id="S3.p3.3.m3.2.2.1.1.1.1.1.1" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.cmml"><mi id="S3.p3.3.m3.2.2.1.1.1.1.1.1.2" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.2.cmml">H</mi><mrow id="S3.p3.3.m3.2.2.1.1.1.1.1.1.3" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.cmml"><mi id="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.2" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.2.cmml">l</mi><mo id="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.1" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.3" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msup><mo id="S3.p3.3.m3.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.p3.3.m3.3.3.2.3" xref="S3.p3.3.m3.3.3.3a.cmml">,</mo><mrow id="S3.p3.3.m3.3.3.2.2" xref="S3.p3.3.m3.3.3.2.2.cmml"><mi id="S3.p3.3.m3.3.3.2.2.3" xref="S3.p3.3.m3.3.3.2.2.3.cmml">l</mi><mo id="S3.p3.3.m3.3.3.2.2.2" xref="S3.p3.3.m3.3.3.2.2.2.cmml">∈</mo><mrow id="S3.p3.3.m3.3.3.2.2.1.1" xref="S3.p3.3.m3.3.3.2.2.1.2.cmml"><mo id="S3.p3.3.m3.3.3.2.2.1.1.2" stretchy="false" xref="S3.p3.3.m3.3.3.2.2.1.2.cmml">[</mo><mn id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">1</mn><mo id="S3.p3.3.m3.3.3.2.2.1.1.3" xref="S3.p3.3.m3.3.3.2.2.1.2.cmml">,</mo><mrow id="S3.p3.3.m3.3.3.2.2.1.1.1" xref="S3.p3.3.m3.3.3.2.2.1.1.1.cmml"><mi id="S3.p3.3.m3.3.3.2.2.1.1.1.2" xref="S3.p3.3.m3.3.3.2.2.1.1.1.2.cmml">L</mi><mo id="S3.p3.3.m3.3.3.2.2.1.1.1.1" xref="S3.p3.3.m3.3.3.2.2.1.1.1.1.cmml">−</mo><mn id="S3.p3.3.m3.3.3.2.2.1.1.1.3" xref="S3.p3.3.m3.3.3.2.2.1.1.1.3.cmml">2</mn></mrow><mo id="S3.p3.3.m3.3.3.2.2.1.1.4" stretchy="false" xref="S3.p3.3.m3.3.3.2.2.1.2.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.3b"><apply id="S3.p3.3.m3.3.3.3.cmml" xref="S3.p3.3.m3.3.3.2"><csymbol cd="ambiguous" id="S3.p3.3.m3.3.3.3a.cmml" xref="S3.p3.3.m3.3.3.2.3">formulae-sequence</csymbol><apply id="S3.p3.3.m3.2.2.1.1.cmml" xref="S3.p3.3.m3.2.2.1.1"><eq id="S3.p3.3.m3.2.2.1.1.2.cmml" xref="S3.p3.3.m3.2.2.1.1.2"></eq><apply id="S3.p3.3.m3.2.2.1.1.3.cmml" xref="S3.p3.3.m3.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.p3.3.m3.2.2.1.1.3.1.cmml" xref="S3.p3.3.m3.2.2.1.1.3">superscript</csymbol><ci id="S3.p3.3.m3.2.2.1.1.3.2.cmml" xref="S3.p3.3.m3.2.2.1.1.3.2">𝐻</ci><ci id="S3.p3.3.m3.2.2.1.1.3.3.cmml" xref="S3.p3.3.m3.2.2.1.1.3.3">𝑙</ci></apply><apply id="S3.p3.3.m3.2.2.1.1.1.cmml" xref="S3.p3.3.m3.2.2.1.1.1"><times id="S3.p3.3.m3.2.2.1.1.1.2.cmml" xref="S3.p3.3.m3.2.2.1.1.1.2"></times><ci id="S3.p3.3.m3.2.2.1.1.1.3.cmml" xref="S3.p3.3.m3.2.2.1.1.1.3">𝐿</ci><apply id="S3.p3.3.m3.2.2.1.1.1.4.cmml" xref="S3.p3.3.m3.2.2.1.1.1.4"><csymbol cd="ambiguous" id="S3.p3.3.m3.2.2.1.1.1.4.1.cmml" xref="S3.p3.3.m3.2.2.1.1.1.4">subscript</csymbol><ci id="S3.p3.3.m3.2.2.1.1.1.4.2.cmml" xref="S3.p3.3.m3.2.2.1.1.1.4.2">𝑀</ci><ci id="S3.p3.3.m3.2.2.1.1.1.4.3.cmml" xref="S3.p3.3.m3.2.2.1.1.1.4.3">𝑙</ci></apply><apply id="S3.p3.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S3.p3.3.m3.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.3.m3.2.2.1.1.1.1.1.1.1.cmml" xref="S3.p3.3.m3.2.2.1.1.1.1.1">superscript</csymbol><ci id="S3.p3.3.m3.2.2.1.1.1.1.1.1.2.cmml" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.2">𝐻</ci><apply id="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.cmml" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.3"><minus id="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.1.cmml" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.1"></minus><ci id="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.2.cmml" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.2">𝑙</ci><cn id="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.p3.3.m3.2.2.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply><apply id="S3.p3.3.m3.3.3.2.2.cmml" xref="S3.p3.3.m3.3.3.2.2"><in id="S3.p3.3.m3.3.3.2.2.2.cmml" xref="S3.p3.3.m3.3.3.2.2.2"></in><ci id="S3.p3.3.m3.3.3.2.2.3.cmml" xref="S3.p3.3.m3.3.3.2.2.3">𝑙</ci><interval closure="closed" id="S3.p3.3.m3.3.3.2.2.1.2.cmml" xref="S3.p3.3.m3.3.3.2.2.1.1"><cn id="S3.p3.3.m3.1.1.cmml" type="integer" xref="S3.p3.3.m3.1.1">1</cn><apply id="S3.p3.3.m3.3.3.2.2.1.1.1.cmml" xref="S3.p3.3.m3.3.3.2.2.1.1.1"><minus id="S3.p3.3.m3.3.3.2.2.1.1.1.1.cmml" xref="S3.p3.3.m3.3.3.2.2.1.1.1.1"></minus><ci id="S3.p3.3.m3.3.3.2.2.1.1.1.2.cmml" xref="S3.p3.3.m3.3.3.2.2.1.1.1.2">𝐿</ci><cn id="S3.p3.3.m3.3.3.2.2.1.1.1.3.cmml" type="integer" xref="S3.p3.3.m3.3.3.2.2.1.1.1.3">2</cn></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.3c">H^{l}=LM_{l}(H^{l-1}),l\in[1,L-2]</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.3d">italic_H start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT = italic_L italic_M start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_H start_POSTSUPERSCRIPT italic_l - 1 end_POSTSUPERSCRIPT ) , italic_l ∈ [ 1 , italic_L - 2 ]</annotation></semantics></math> denote the contextual representations of the backbone LM for the first <math alttext="L-2" class="ltx_Math" display="inline" id="S3.p3.4.m4.1"><semantics id="S3.p3.4.m4.1a"><mrow id="S3.p3.4.m4.1.1" xref="S3.p3.4.m4.1.1.cmml"><mi id="S3.p3.4.m4.1.1.2" xref="S3.p3.4.m4.1.1.2.cmml">L</mi><mo id="S3.p3.4.m4.1.1.1" xref="S3.p3.4.m4.1.1.1.cmml">−</mo><mn id="S3.p3.4.m4.1.1.3" xref="S3.p3.4.m4.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.4.m4.1b"><apply id="S3.p3.4.m4.1.1.cmml" xref="S3.p3.4.m4.1.1"><minus id="S3.p3.4.m4.1.1.1.cmml" xref="S3.p3.4.m4.1.1.1"></minus><ci id="S3.p3.4.m4.1.1.2.cmml" xref="S3.p3.4.m4.1.1.2">𝐿</ci><cn id="S3.p3.4.m4.1.1.3.cmml" type="integer" xref="S3.p3.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.m4.1c">L-2</annotation><annotation encoding="application/x-llamapun" id="S3.p3.4.m4.1d">italic_L - 2</annotation></semantics></math> layers. Then, let <math alttext="H_{g}=\textsc{CLIP}_{text}(\{x_{i}\}_{i=1}^{N})" class="ltx_Math" display="inline" id="S3.p3.5.m5.1"><semantics id="S3.p3.5.m5.1a"><mrow id="S3.p3.5.m5.1.1" xref="S3.p3.5.m5.1.1.cmml"><msub id="S3.p3.5.m5.1.1.3" xref="S3.p3.5.m5.1.1.3.cmml"><mi id="S3.p3.5.m5.1.1.3.2" xref="S3.p3.5.m5.1.1.3.2.cmml">H</mi><mi id="S3.p3.5.m5.1.1.3.3" xref="S3.p3.5.m5.1.1.3.3.cmml">g</mi></msub><mo id="S3.p3.5.m5.1.1.2" xref="S3.p3.5.m5.1.1.2.cmml">=</mo><mrow id="S3.p3.5.m5.1.1.1" xref="S3.p3.5.m5.1.1.1.cmml"><msub id="S3.p3.5.m5.1.1.1.3" xref="S3.p3.5.m5.1.1.1.3.cmml"><mtext class="ltx_font_smallcaps" id="S3.p3.5.m5.1.1.1.3.2" xref="S3.p3.5.m5.1.1.1.3.2a.cmml">CLIP</mtext><mrow id="S3.p3.5.m5.1.1.1.3.3" xref="S3.p3.5.m5.1.1.1.3.3.cmml"><mi id="S3.p3.5.m5.1.1.1.3.3.2" xref="S3.p3.5.m5.1.1.1.3.3.2.cmml">t</mi><mo id="S3.p3.5.m5.1.1.1.3.3.1" xref="S3.p3.5.m5.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p3.5.m5.1.1.1.3.3.3" xref="S3.p3.5.m5.1.1.1.3.3.3.cmml">e</mi><mo id="S3.p3.5.m5.1.1.1.3.3.1a" xref="S3.p3.5.m5.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p3.5.m5.1.1.1.3.3.4" xref="S3.p3.5.m5.1.1.1.3.3.4.cmml">x</mi><mo id="S3.p3.5.m5.1.1.1.3.3.1b" xref="S3.p3.5.m5.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p3.5.m5.1.1.1.3.3.5" xref="S3.p3.5.m5.1.1.1.3.3.5.cmml">t</mi></mrow></msub><mo id="S3.p3.5.m5.1.1.1.2" xref="S3.p3.5.m5.1.1.1.2.cmml">⁢</mo><mrow id="S3.p3.5.m5.1.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.1.1.cmml"><mo id="S3.p3.5.m5.1.1.1.1.1.2" stretchy="false" xref="S3.p3.5.m5.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.p3.5.m5.1.1.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.1.1.cmml"><mrow id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.2.cmml">{</mo><msub id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.2" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.3" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.p3.5.m5.1.1.1.1.1.1.1.3" xref="S3.p3.5.m5.1.1.1.1.1.1.1.3.cmml"><mi id="S3.p3.5.m5.1.1.1.1.1.1.1.3.2" xref="S3.p3.5.m5.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.p3.5.m5.1.1.1.1.1.1.1.3.1" xref="S3.p3.5.m5.1.1.1.1.1.1.1.3.1.cmml">=</mo><mn id="S3.p3.5.m5.1.1.1.1.1.1.1.3.3" xref="S3.p3.5.m5.1.1.1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.p3.5.m5.1.1.1.1.1.1.3" xref="S3.p3.5.m5.1.1.1.1.1.1.3.cmml">N</mi></msubsup><mo id="S3.p3.5.m5.1.1.1.1.1.3" stretchy="false" xref="S3.p3.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.5.m5.1b"><apply id="S3.p3.5.m5.1.1.cmml" xref="S3.p3.5.m5.1.1"><eq id="S3.p3.5.m5.1.1.2.cmml" xref="S3.p3.5.m5.1.1.2"></eq><apply id="S3.p3.5.m5.1.1.3.cmml" xref="S3.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.p3.5.m5.1.1.3.1.cmml" xref="S3.p3.5.m5.1.1.3">subscript</csymbol><ci id="S3.p3.5.m5.1.1.3.2.cmml" xref="S3.p3.5.m5.1.1.3.2">𝐻</ci><ci id="S3.p3.5.m5.1.1.3.3.cmml" xref="S3.p3.5.m5.1.1.3.3">𝑔</ci></apply><apply id="S3.p3.5.m5.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1"><times id="S3.p3.5.m5.1.1.1.2.cmml" xref="S3.p3.5.m5.1.1.1.2"></times><apply id="S3.p3.5.m5.1.1.1.3.cmml" xref="S3.p3.5.m5.1.1.1.3"><csymbol cd="ambiguous" id="S3.p3.5.m5.1.1.1.3.1.cmml" xref="S3.p3.5.m5.1.1.1.3">subscript</csymbol><ci id="S3.p3.5.m5.1.1.1.3.2a.cmml" xref="S3.p3.5.m5.1.1.1.3.2"><mtext class="ltx_font_smallcaps" id="S3.p3.5.m5.1.1.1.3.2.cmml" xref="S3.p3.5.m5.1.1.1.3.2">CLIP</mtext></ci><apply id="S3.p3.5.m5.1.1.1.3.3.cmml" xref="S3.p3.5.m5.1.1.1.3.3"><times id="S3.p3.5.m5.1.1.1.3.3.1.cmml" xref="S3.p3.5.m5.1.1.1.3.3.1"></times><ci id="S3.p3.5.m5.1.1.1.3.3.2.cmml" xref="S3.p3.5.m5.1.1.1.3.3.2">𝑡</ci><ci id="S3.p3.5.m5.1.1.1.3.3.3.cmml" xref="S3.p3.5.m5.1.1.1.3.3.3">𝑒</ci><ci id="S3.p3.5.m5.1.1.1.3.3.4.cmml" xref="S3.p3.5.m5.1.1.1.3.3.4">𝑥</ci><ci id="S3.p3.5.m5.1.1.1.3.3.5.cmml" xref="S3.p3.5.m5.1.1.1.3.3.5">𝑡</ci></apply></apply><apply id="S3.p3.5.m5.1.1.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.p3.5.m5.1.1.1.1.1">superscript</csymbol><apply id="S3.p3.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.5.m5.1.1.1.1.1.1.1.2.cmml" xref="S3.p3.5.m5.1.1.1.1.1">subscript</csymbol><set id="S3.p3.5.m5.1.1.1.1.1.1.1.1.2.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.1"><apply id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.p3.5.m5.1.1.1.1.1.1.1.3.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.1.3"><eq id="S3.p3.5.m5.1.1.1.1.1.1.1.3.1.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.1.3.1"></eq><ci id="S3.p3.5.m5.1.1.1.1.1.1.1.3.2.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.1.3.2">𝑖</ci><cn id="S3.p3.5.m5.1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.p3.5.m5.1.1.1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.p3.5.m5.1.1.1.1.1.1.3.cmml" xref="S3.p3.5.m5.1.1.1.1.1.1.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.5.m5.1c">H_{g}=\textsc{CLIP}_{text}(\{x_{i}\}_{i=1}^{N})</annotation><annotation encoding="application/x-llamapun" id="S3.p3.5.m5.1d">italic_H start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT = CLIP start_POSTSUBSCRIPT italic_t italic_e italic_x italic_t end_POSTSUBSCRIPT ( { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT )</annotation></semantics></math> denote grounded representations over the same input text sequence obtained from CLIP. As in <span class="ltx_text ltx_font_smallcaps" id="S3.p3.7.1">VaLM</span>, combine both representations of the input using the Fusion Layer, such that <math alttext="H^{L-1}=FusionLayer(H^{L-2},H_{g})" class="ltx_Math" display="inline" id="S3.p3.6.m6.2"><semantics id="S3.p3.6.m6.2a"><mrow id="S3.p3.6.m6.2.2" xref="S3.p3.6.m6.2.2.cmml"><msup id="S3.p3.6.m6.2.2.4" xref="S3.p3.6.m6.2.2.4.cmml"><mi id="S3.p3.6.m6.2.2.4.2" xref="S3.p3.6.m6.2.2.4.2.cmml">H</mi><mrow id="S3.p3.6.m6.2.2.4.3" xref="S3.p3.6.m6.2.2.4.3.cmml"><mi id="S3.p3.6.m6.2.2.4.3.2" xref="S3.p3.6.m6.2.2.4.3.2.cmml">L</mi><mo id="S3.p3.6.m6.2.2.4.3.1" xref="S3.p3.6.m6.2.2.4.3.1.cmml">−</mo><mn id="S3.p3.6.m6.2.2.4.3.3" xref="S3.p3.6.m6.2.2.4.3.3.cmml">1</mn></mrow></msup><mo id="S3.p3.6.m6.2.2.3" xref="S3.p3.6.m6.2.2.3.cmml">=</mo><mrow id="S3.p3.6.m6.2.2.2" xref="S3.p3.6.m6.2.2.2.cmml"><mi id="S3.p3.6.m6.2.2.2.4" xref="S3.p3.6.m6.2.2.2.4.cmml">F</mi><mo id="S3.p3.6.m6.2.2.2.3" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.5" xref="S3.p3.6.m6.2.2.2.5.cmml">u</mi><mo id="S3.p3.6.m6.2.2.2.3a" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.6" xref="S3.p3.6.m6.2.2.2.6.cmml">s</mi><mo id="S3.p3.6.m6.2.2.2.3b" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.7" xref="S3.p3.6.m6.2.2.2.7.cmml">i</mi><mo id="S3.p3.6.m6.2.2.2.3c" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.8" xref="S3.p3.6.m6.2.2.2.8.cmml">o</mi><mo id="S3.p3.6.m6.2.2.2.3d" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.9" xref="S3.p3.6.m6.2.2.2.9.cmml">n</mi><mo id="S3.p3.6.m6.2.2.2.3e" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.10" xref="S3.p3.6.m6.2.2.2.10.cmml">L</mi><mo id="S3.p3.6.m6.2.2.2.3f" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.11" xref="S3.p3.6.m6.2.2.2.11.cmml">a</mi><mo id="S3.p3.6.m6.2.2.2.3g" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.12" xref="S3.p3.6.m6.2.2.2.12.cmml">y</mi><mo id="S3.p3.6.m6.2.2.2.3h" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.13" xref="S3.p3.6.m6.2.2.2.13.cmml">e</mi><mo id="S3.p3.6.m6.2.2.2.3i" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mi id="S3.p3.6.m6.2.2.2.14" xref="S3.p3.6.m6.2.2.2.14.cmml">r</mi><mo id="S3.p3.6.m6.2.2.2.3j" xref="S3.p3.6.m6.2.2.2.3.cmml">⁢</mo><mrow id="S3.p3.6.m6.2.2.2.2.2" xref="S3.p3.6.m6.2.2.2.2.3.cmml"><mo id="S3.p3.6.m6.2.2.2.2.2.3" stretchy="false" xref="S3.p3.6.m6.2.2.2.2.3.cmml">(</mo><msup id="S3.p3.6.m6.1.1.1.1.1.1" xref="S3.p3.6.m6.1.1.1.1.1.1.cmml"><mi id="S3.p3.6.m6.1.1.1.1.1.1.2" xref="S3.p3.6.m6.1.1.1.1.1.1.2.cmml">H</mi><mrow id="S3.p3.6.m6.1.1.1.1.1.1.3" xref="S3.p3.6.m6.1.1.1.1.1.1.3.cmml"><mi id="S3.p3.6.m6.1.1.1.1.1.1.3.2" xref="S3.p3.6.m6.1.1.1.1.1.1.3.2.cmml">L</mi><mo id="S3.p3.6.m6.1.1.1.1.1.1.3.1" xref="S3.p3.6.m6.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.p3.6.m6.1.1.1.1.1.1.3.3" xref="S3.p3.6.m6.1.1.1.1.1.1.3.3.cmml">2</mn></mrow></msup><mo id="S3.p3.6.m6.2.2.2.2.2.4" xref="S3.p3.6.m6.2.2.2.2.3.cmml">,</mo><msub id="S3.p3.6.m6.2.2.2.2.2.2" xref="S3.p3.6.m6.2.2.2.2.2.2.cmml"><mi id="S3.p3.6.m6.2.2.2.2.2.2.2" xref="S3.p3.6.m6.2.2.2.2.2.2.2.cmml">H</mi><mi id="S3.p3.6.m6.2.2.2.2.2.2.3" xref="S3.p3.6.m6.2.2.2.2.2.2.3.cmml">g</mi></msub><mo id="S3.p3.6.m6.2.2.2.2.2.5" stretchy="false" xref="S3.p3.6.m6.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.6.m6.2b"><apply id="S3.p3.6.m6.2.2.cmml" xref="S3.p3.6.m6.2.2"><eq id="S3.p3.6.m6.2.2.3.cmml" xref="S3.p3.6.m6.2.2.3"></eq><apply id="S3.p3.6.m6.2.2.4.cmml" xref="S3.p3.6.m6.2.2.4"><csymbol cd="ambiguous" id="S3.p3.6.m6.2.2.4.1.cmml" xref="S3.p3.6.m6.2.2.4">superscript</csymbol><ci id="S3.p3.6.m6.2.2.4.2.cmml" xref="S3.p3.6.m6.2.2.4.2">𝐻</ci><apply id="S3.p3.6.m6.2.2.4.3.cmml" xref="S3.p3.6.m6.2.2.4.3"><minus id="S3.p3.6.m6.2.2.4.3.1.cmml" xref="S3.p3.6.m6.2.2.4.3.1"></minus><ci id="S3.p3.6.m6.2.2.4.3.2.cmml" xref="S3.p3.6.m6.2.2.4.3.2">𝐿</ci><cn id="S3.p3.6.m6.2.2.4.3.3.cmml" type="integer" xref="S3.p3.6.m6.2.2.4.3.3">1</cn></apply></apply><apply id="S3.p3.6.m6.2.2.2.cmml" xref="S3.p3.6.m6.2.2.2"><times id="S3.p3.6.m6.2.2.2.3.cmml" xref="S3.p3.6.m6.2.2.2.3"></times><ci id="S3.p3.6.m6.2.2.2.4.cmml" xref="S3.p3.6.m6.2.2.2.4">𝐹</ci><ci id="S3.p3.6.m6.2.2.2.5.cmml" xref="S3.p3.6.m6.2.2.2.5">𝑢</ci><ci id="S3.p3.6.m6.2.2.2.6.cmml" xref="S3.p3.6.m6.2.2.2.6">𝑠</ci><ci id="S3.p3.6.m6.2.2.2.7.cmml" xref="S3.p3.6.m6.2.2.2.7">𝑖</ci><ci id="S3.p3.6.m6.2.2.2.8.cmml" xref="S3.p3.6.m6.2.2.2.8">𝑜</ci><ci id="S3.p3.6.m6.2.2.2.9.cmml" xref="S3.p3.6.m6.2.2.2.9">𝑛</ci><ci id="S3.p3.6.m6.2.2.2.10.cmml" xref="S3.p3.6.m6.2.2.2.10">𝐿</ci><ci id="S3.p3.6.m6.2.2.2.11.cmml" xref="S3.p3.6.m6.2.2.2.11">𝑎</ci><ci id="S3.p3.6.m6.2.2.2.12.cmml" xref="S3.p3.6.m6.2.2.2.12">𝑦</ci><ci id="S3.p3.6.m6.2.2.2.13.cmml" xref="S3.p3.6.m6.2.2.2.13">𝑒</ci><ci id="S3.p3.6.m6.2.2.2.14.cmml" xref="S3.p3.6.m6.2.2.2.14">𝑟</ci><interval closure="open" id="S3.p3.6.m6.2.2.2.2.3.cmml" xref="S3.p3.6.m6.2.2.2.2.2"><apply id="S3.p3.6.m6.1.1.1.1.1.1.cmml" xref="S3.p3.6.m6.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.6.m6.1.1.1.1.1.1.1.cmml" xref="S3.p3.6.m6.1.1.1.1.1.1">superscript</csymbol><ci id="S3.p3.6.m6.1.1.1.1.1.1.2.cmml" xref="S3.p3.6.m6.1.1.1.1.1.1.2">𝐻</ci><apply id="S3.p3.6.m6.1.1.1.1.1.1.3.cmml" xref="S3.p3.6.m6.1.1.1.1.1.1.3"><minus id="S3.p3.6.m6.1.1.1.1.1.1.3.1.cmml" xref="S3.p3.6.m6.1.1.1.1.1.1.3.1"></minus><ci id="S3.p3.6.m6.1.1.1.1.1.1.3.2.cmml" xref="S3.p3.6.m6.1.1.1.1.1.1.3.2">𝐿</ci><cn id="S3.p3.6.m6.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.p3.6.m6.1.1.1.1.1.1.3.3">2</cn></apply></apply><apply id="S3.p3.6.m6.2.2.2.2.2.2.cmml" xref="S3.p3.6.m6.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p3.6.m6.2.2.2.2.2.2.1.cmml" xref="S3.p3.6.m6.2.2.2.2.2.2">subscript</csymbol><ci id="S3.p3.6.m6.2.2.2.2.2.2.2.cmml" xref="S3.p3.6.m6.2.2.2.2.2.2.2">𝐻</ci><ci id="S3.p3.6.m6.2.2.2.2.2.2.3.cmml" xref="S3.p3.6.m6.2.2.2.2.2.2.3">𝑔</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.6.m6.2c">H^{L-1}=FusionLayer(H^{L-2},H_{g})</annotation><annotation encoding="application/x-llamapun" id="S3.p3.6.m6.2d">italic_H start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT = italic_F italic_u italic_s italic_i italic_o italic_n italic_L italic_a italic_y italic_e italic_r ( italic_H start_POSTSUPERSCRIPT italic_L - 2 end_POSTSUPERSCRIPT , italic_H start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT )</annotation></semantics></math>, and apply a final transformer layer to obtain the final representation: <math alttext="H^{L}=LM_{L}(H^{L-1})" class="ltx_Math" display="inline" id="S3.p3.7.m7.1"><semantics id="S3.p3.7.m7.1a"><mrow id="S3.p3.7.m7.1.1" xref="S3.p3.7.m7.1.1.cmml"><msup id="S3.p3.7.m7.1.1.3" xref="S3.p3.7.m7.1.1.3.cmml"><mi id="S3.p3.7.m7.1.1.3.2" xref="S3.p3.7.m7.1.1.3.2.cmml">H</mi><mi id="S3.p3.7.m7.1.1.3.3" xref="S3.p3.7.m7.1.1.3.3.cmml">L</mi></msup><mo id="S3.p3.7.m7.1.1.2" xref="S3.p3.7.m7.1.1.2.cmml">=</mo><mrow id="S3.p3.7.m7.1.1.1" xref="S3.p3.7.m7.1.1.1.cmml"><mi id="S3.p3.7.m7.1.1.1.3" xref="S3.p3.7.m7.1.1.1.3.cmml">L</mi><mo id="S3.p3.7.m7.1.1.1.2" xref="S3.p3.7.m7.1.1.1.2.cmml">⁢</mo><msub id="S3.p3.7.m7.1.1.1.4" xref="S3.p3.7.m7.1.1.1.4.cmml"><mi id="S3.p3.7.m7.1.1.1.4.2" xref="S3.p3.7.m7.1.1.1.4.2.cmml">M</mi><mi id="S3.p3.7.m7.1.1.1.4.3" xref="S3.p3.7.m7.1.1.1.4.3.cmml">L</mi></msub><mo id="S3.p3.7.m7.1.1.1.2a" xref="S3.p3.7.m7.1.1.1.2.cmml">⁢</mo><mrow id="S3.p3.7.m7.1.1.1.1.1" xref="S3.p3.7.m7.1.1.1.1.1.1.cmml"><mo id="S3.p3.7.m7.1.1.1.1.1.2" stretchy="false" xref="S3.p3.7.m7.1.1.1.1.1.1.cmml">(</mo><msup id="S3.p3.7.m7.1.1.1.1.1.1" xref="S3.p3.7.m7.1.1.1.1.1.1.cmml"><mi id="S3.p3.7.m7.1.1.1.1.1.1.2" xref="S3.p3.7.m7.1.1.1.1.1.1.2.cmml">H</mi><mrow id="S3.p3.7.m7.1.1.1.1.1.1.3" xref="S3.p3.7.m7.1.1.1.1.1.1.3.cmml"><mi id="S3.p3.7.m7.1.1.1.1.1.1.3.2" xref="S3.p3.7.m7.1.1.1.1.1.1.3.2.cmml">L</mi><mo id="S3.p3.7.m7.1.1.1.1.1.1.3.1" xref="S3.p3.7.m7.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.p3.7.m7.1.1.1.1.1.1.3.3" xref="S3.p3.7.m7.1.1.1.1.1.1.3.3.cmml">1</mn></mrow></msup><mo id="S3.p3.7.m7.1.1.1.1.1.3" stretchy="false" xref="S3.p3.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.7.m7.1b"><apply id="S3.p3.7.m7.1.1.cmml" xref="S3.p3.7.m7.1.1"><eq id="S3.p3.7.m7.1.1.2.cmml" xref="S3.p3.7.m7.1.1.2"></eq><apply id="S3.p3.7.m7.1.1.3.cmml" xref="S3.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.p3.7.m7.1.1.3.1.cmml" xref="S3.p3.7.m7.1.1.3">superscript</csymbol><ci id="S3.p3.7.m7.1.1.3.2.cmml" xref="S3.p3.7.m7.1.1.3.2">𝐻</ci><ci id="S3.p3.7.m7.1.1.3.3.cmml" xref="S3.p3.7.m7.1.1.3.3">𝐿</ci></apply><apply id="S3.p3.7.m7.1.1.1.cmml" xref="S3.p3.7.m7.1.1.1"><times id="S3.p3.7.m7.1.1.1.2.cmml" xref="S3.p3.7.m7.1.1.1.2"></times><ci id="S3.p3.7.m7.1.1.1.3.cmml" xref="S3.p3.7.m7.1.1.1.3">𝐿</ci><apply id="S3.p3.7.m7.1.1.1.4.cmml" xref="S3.p3.7.m7.1.1.1.4"><csymbol cd="ambiguous" id="S3.p3.7.m7.1.1.1.4.1.cmml" xref="S3.p3.7.m7.1.1.1.4">subscript</csymbol><ci id="S3.p3.7.m7.1.1.1.4.2.cmml" xref="S3.p3.7.m7.1.1.1.4.2">𝑀</ci><ci id="S3.p3.7.m7.1.1.1.4.3.cmml" xref="S3.p3.7.m7.1.1.1.4.3">𝐿</ci></apply><apply id="S3.p3.7.m7.1.1.1.1.1.1.cmml" xref="S3.p3.7.m7.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.7.m7.1.1.1.1.1.1.1.cmml" xref="S3.p3.7.m7.1.1.1.1.1">superscript</csymbol><ci id="S3.p3.7.m7.1.1.1.1.1.1.2.cmml" xref="S3.p3.7.m7.1.1.1.1.1.1.2">𝐻</ci><apply id="S3.p3.7.m7.1.1.1.1.1.1.3.cmml" xref="S3.p3.7.m7.1.1.1.1.1.1.3"><minus id="S3.p3.7.m7.1.1.1.1.1.1.3.1.cmml" xref="S3.p3.7.m7.1.1.1.1.1.1.3.1"></minus><ci id="S3.p3.7.m7.1.1.1.1.1.1.3.2.cmml" xref="S3.p3.7.m7.1.1.1.1.1.1.3.2">𝐿</ci><cn id="S3.p3.7.m7.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.p3.7.m7.1.1.1.1.1.1.3.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.7.m7.1c">H^{L}=LM_{L}(H^{L-1})</annotation><annotation encoding="application/x-llamapun" id="S3.p3.7.m7.1d">italic_H start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT = italic_L italic_M start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT ( italic_H start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.2">In other words, we remove the image retrieval aspect from <span class="ltx_text ltx_font_smallcaps" id="S3.p4.2.1">VaLM</span>, where <math alttext="H^{L-1}=FusionLayer(H^{L-2},\{\textsc{CLIP}_{img}(I_{i})\}_{i=1}^{K})" class="ltx_Math" display="inline" id="S3.p4.1.m1.2"><semantics id="S3.p4.1.m1.2a"><mrow id="S3.p4.1.m1.2.2" xref="S3.p4.1.m1.2.2.cmml"><msup id="S3.p4.1.m1.2.2.4" xref="S3.p4.1.m1.2.2.4.cmml"><mi id="S3.p4.1.m1.2.2.4.2" xref="S3.p4.1.m1.2.2.4.2.cmml">H</mi><mrow id="S3.p4.1.m1.2.2.4.3" xref="S3.p4.1.m1.2.2.4.3.cmml"><mi id="S3.p4.1.m1.2.2.4.3.2" xref="S3.p4.1.m1.2.2.4.3.2.cmml">L</mi><mo id="S3.p4.1.m1.2.2.4.3.1" xref="S3.p4.1.m1.2.2.4.3.1.cmml">−</mo><mn id="S3.p4.1.m1.2.2.4.3.3" xref="S3.p4.1.m1.2.2.4.3.3.cmml">1</mn></mrow></msup><mo id="S3.p4.1.m1.2.2.3" xref="S3.p4.1.m1.2.2.3.cmml">=</mo><mrow id="S3.p4.1.m1.2.2.2" xref="S3.p4.1.m1.2.2.2.cmml"><mi id="S3.p4.1.m1.2.2.2.4" xref="S3.p4.1.m1.2.2.2.4.cmml">F</mi><mo id="S3.p4.1.m1.2.2.2.3" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.5" xref="S3.p4.1.m1.2.2.2.5.cmml">u</mi><mo id="S3.p4.1.m1.2.2.2.3a" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.6" xref="S3.p4.1.m1.2.2.2.6.cmml">s</mi><mo id="S3.p4.1.m1.2.2.2.3b" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.7" xref="S3.p4.1.m1.2.2.2.7.cmml">i</mi><mo id="S3.p4.1.m1.2.2.2.3c" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.8" xref="S3.p4.1.m1.2.2.2.8.cmml">o</mi><mo id="S3.p4.1.m1.2.2.2.3d" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.9" xref="S3.p4.1.m1.2.2.2.9.cmml">n</mi><mo id="S3.p4.1.m1.2.2.2.3e" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.10" xref="S3.p4.1.m1.2.2.2.10.cmml">L</mi><mo id="S3.p4.1.m1.2.2.2.3f" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.11" xref="S3.p4.1.m1.2.2.2.11.cmml">a</mi><mo id="S3.p4.1.m1.2.2.2.3g" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.12" xref="S3.p4.1.m1.2.2.2.12.cmml">y</mi><mo id="S3.p4.1.m1.2.2.2.3h" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.13" xref="S3.p4.1.m1.2.2.2.13.cmml">e</mi><mo id="S3.p4.1.m1.2.2.2.3i" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.14" xref="S3.p4.1.m1.2.2.2.14.cmml">r</mi><mo id="S3.p4.1.m1.2.2.2.3j" xref="S3.p4.1.m1.2.2.2.3.cmml">⁢</mo><mrow id="S3.p4.1.m1.2.2.2.2.2" xref="S3.p4.1.m1.2.2.2.2.3.cmml"><mo id="S3.p4.1.m1.2.2.2.2.2.3" stretchy="false" xref="S3.p4.1.m1.2.2.2.2.3.cmml">(</mo><msup id="S3.p4.1.m1.1.1.1.1.1.1" xref="S3.p4.1.m1.1.1.1.1.1.1.cmml"><mi id="S3.p4.1.m1.1.1.1.1.1.1.2" xref="S3.p4.1.m1.1.1.1.1.1.1.2.cmml">H</mi><mrow id="S3.p4.1.m1.1.1.1.1.1.1.3" xref="S3.p4.1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.p4.1.m1.1.1.1.1.1.1.3.2" xref="S3.p4.1.m1.1.1.1.1.1.1.3.2.cmml">L</mi><mo id="S3.p4.1.m1.1.1.1.1.1.1.3.1" xref="S3.p4.1.m1.1.1.1.1.1.1.3.1.cmml">−</mo><mn id="S3.p4.1.m1.1.1.1.1.1.1.3.3" xref="S3.p4.1.m1.1.1.1.1.1.1.3.3.cmml">2</mn></mrow></msup><mo id="S3.p4.1.m1.2.2.2.2.2.4" xref="S3.p4.1.m1.2.2.2.2.3.cmml">,</mo><msubsup id="S3.p4.1.m1.2.2.2.2.2.2" xref="S3.p4.1.m1.2.2.2.2.2.2.cmml"><mrow id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.2.cmml"><mo id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.2" stretchy="false" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.2.cmml">{</mo><mrow id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.cmml"><msub id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.cmml"><mtext class="ltx_font_smallcaps" id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.2" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.2a.cmml">CLIP</mtext><mrow id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.cmml"><mi id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.2" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.2.cmml">i</mi><mo id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.1" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.3" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.3.cmml">m</mi><mo id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.1a" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.4" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.4.cmml">g</mi></mrow></msub><mo id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.2" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.cmml"><mo id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.2" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.2.cmml">I</mi><mi id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.3" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.3" stretchy="false" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.2.cmml">}</mo></mrow><mrow id="S3.p4.1.m1.2.2.2.2.2.2.1.3" xref="S3.p4.1.m1.2.2.2.2.2.2.1.3.cmml"><mi id="S3.p4.1.m1.2.2.2.2.2.2.1.3.2" xref="S3.p4.1.m1.2.2.2.2.2.2.1.3.2.cmml">i</mi><mo id="S3.p4.1.m1.2.2.2.2.2.2.1.3.1" xref="S3.p4.1.m1.2.2.2.2.2.2.1.3.1.cmml">=</mo><mn id="S3.p4.1.m1.2.2.2.2.2.2.1.3.3" xref="S3.p4.1.m1.2.2.2.2.2.2.1.3.3.cmml">1</mn></mrow><mi id="S3.p4.1.m1.2.2.2.2.2.2.3" xref="S3.p4.1.m1.2.2.2.2.2.2.3.cmml">K</mi></msubsup><mo id="S3.p4.1.m1.2.2.2.2.2.5" stretchy="false" xref="S3.p4.1.m1.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.2b"><apply id="S3.p4.1.m1.2.2.cmml" xref="S3.p4.1.m1.2.2"><eq id="S3.p4.1.m1.2.2.3.cmml" xref="S3.p4.1.m1.2.2.3"></eq><apply id="S3.p4.1.m1.2.2.4.cmml" xref="S3.p4.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.p4.1.m1.2.2.4.1.cmml" xref="S3.p4.1.m1.2.2.4">superscript</csymbol><ci id="S3.p4.1.m1.2.2.4.2.cmml" xref="S3.p4.1.m1.2.2.4.2">𝐻</ci><apply id="S3.p4.1.m1.2.2.4.3.cmml" xref="S3.p4.1.m1.2.2.4.3"><minus id="S3.p4.1.m1.2.2.4.3.1.cmml" xref="S3.p4.1.m1.2.2.4.3.1"></minus><ci id="S3.p4.1.m1.2.2.4.3.2.cmml" xref="S3.p4.1.m1.2.2.4.3.2">𝐿</ci><cn id="S3.p4.1.m1.2.2.4.3.3.cmml" type="integer" xref="S3.p4.1.m1.2.2.4.3.3">1</cn></apply></apply><apply id="S3.p4.1.m1.2.2.2.cmml" xref="S3.p4.1.m1.2.2.2"><times id="S3.p4.1.m1.2.2.2.3.cmml" xref="S3.p4.1.m1.2.2.2.3"></times><ci id="S3.p4.1.m1.2.2.2.4.cmml" xref="S3.p4.1.m1.2.2.2.4">𝐹</ci><ci id="S3.p4.1.m1.2.2.2.5.cmml" xref="S3.p4.1.m1.2.2.2.5">𝑢</ci><ci id="S3.p4.1.m1.2.2.2.6.cmml" xref="S3.p4.1.m1.2.2.2.6">𝑠</ci><ci id="S3.p4.1.m1.2.2.2.7.cmml" xref="S3.p4.1.m1.2.2.2.7">𝑖</ci><ci id="S3.p4.1.m1.2.2.2.8.cmml" xref="S3.p4.1.m1.2.2.2.8">𝑜</ci><ci id="S3.p4.1.m1.2.2.2.9.cmml" xref="S3.p4.1.m1.2.2.2.9">𝑛</ci><ci id="S3.p4.1.m1.2.2.2.10.cmml" xref="S3.p4.1.m1.2.2.2.10">𝐿</ci><ci id="S3.p4.1.m1.2.2.2.11.cmml" xref="S3.p4.1.m1.2.2.2.11">𝑎</ci><ci id="S3.p4.1.m1.2.2.2.12.cmml" xref="S3.p4.1.m1.2.2.2.12">𝑦</ci><ci id="S3.p4.1.m1.2.2.2.13.cmml" xref="S3.p4.1.m1.2.2.2.13">𝑒</ci><ci id="S3.p4.1.m1.2.2.2.14.cmml" xref="S3.p4.1.m1.2.2.2.14">𝑟</ci><interval closure="open" id="S3.p4.1.m1.2.2.2.2.3.cmml" xref="S3.p4.1.m1.2.2.2.2.2"><apply id="S3.p4.1.m1.1.1.1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p4.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1.1.1.1">superscript</csymbol><ci id="S3.p4.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.p4.1.m1.1.1.1.1.1.1.2">𝐻</ci><apply id="S3.p4.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.p4.1.m1.1.1.1.1.1.1.3"><minus id="S3.p4.1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.p4.1.m1.1.1.1.1.1.1.3.1"></minus><ci id="S3.p4.1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.p4.1.m1.1.1.1.1.1.1.3.2">𝐿</ci><cn id="S3.p4.1.m1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S3.p4.1.m1.1.1.1.1.1.1.3.3">2</cn></apply></apply><apply id="S3.p4.1.m1.2.2.2.2.2.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p4.1.m1.2.2.2.2.2.2.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2">superscript</csymbol><apply id="S3.p4.1.m1.2.2.2.2.2.2.1.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.p4.1.m1.2.2.2.2.2.2.1.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2">subscript</csymbol><set id="S3.p4.1.m1.2.2.2.2.2.2.1.1.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1"><apply id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1"><times id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.2"></times><apply id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.1.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3">subscript</csymbol><ci id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.2a.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.2"><mtext class="ltx_font_smallcaps" id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.2">CLIP</mtext></ci><apply id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3"><times id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.1.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.1"></times><ci id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.2">𝑖</ci><ci id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.3.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.3">𝑚</ci><ci id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.4.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.3.3.4">𝑔</ci></apply></apply><apply id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.2">𝐼</ci><ci id="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply></set><apply id="S3.p4.1.m1.2.2.2.2.2.2.1.3.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.3"><eq id="S3.p4.1.m1.2.2.2.2.2.2.1.3.1.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.3.1"></eq><ci id="S3.p4.1.m1.2.2.2.2.2.2.1.3.2.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.1.3.2">𝑖</ci><cn id="S3.p4.1.m1.2.2.2.2.2.2.1.3.3.cmml" type="integer" xref="S3.p4.1.m1.2.2.2.2.2.2.1.3.3">1</cn></apply></apply><ci id="S3.p4.1.m1.2.2.2.2.2.2.3.cmml" xref="S3.p4.1.m1.2.2.2.2.2.2.3">𝐾</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.2c">H^{L-1}=FusionLayer(H^{L-2},\{\textsc{CLIP}_{img}(I_{i})\}_{i=1}^{K})</annotation><annotation encoding="application/x-llamapun" id="S3.p4.1.m1.2d">italic_H start_POSTSUPERSCRIPT italic_L - 1 end_POSTSUPERSCRIPT = italic_F italic_u italic_s italic_i italic_o italic_n italic_L italic_a italic_y italic_e italic_r ( italic_H start_POSTSUPERSCRIPT italic_L - 2 end_POSTSUPERSCRIPT , { CLIP start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT )</annotation></semantics></math>, by replacing the retrieved image representations <math alttext="\textsc{CLIP}_{img}(I_{i})" class="ltx_Math" display="inline" id="S3.p4.2.m2.1"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><msub id="S3.p4.2.m2.1.1.3" xref="S3.p4.2.m2.1.1.3.cmml"><mtext class="ltx_font_smallcaps" id="S3.p4.2.m2.1.1.3.2" xref="S3.p4.2.m2.1.1.3.2a.cmml">CLIP</mtext><mrow id="S3.p4.2.m2.1.1.3.3" xref="S3.p4.2.m2.1.1.3.3.cmml"><mi id="S3.p4.2.m2.1.1.3.3.2" xref="S3.p4.2.m2.1.1.3.3.2.cmml">i</mi><mo id="S3.p4.2.m2.1.1.3.3.1" xref="S3.p4.2.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p4.2.m2.1.1.3.3.3" xref="S3.p4.2.m2.1.1.3.3.3.cmml">m</mi><mo id="S3.p4.2.m2.1.1.3.3.1a" xref="S3.p4.2.m2.1.1.3.3.1.cmml">⁢</mo><mi id="S3.p4.2.m2.1.1.3.3.4" xref="S3.p4.2.m2.1.1.3.3.4.cmml">g</mi></mrow></msub><mo id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">⁢</mo><mrow id="S3.p4.2.m2.1.1.1.1" xref="S3.p4.2.m2.1.1.1.1.1.cmml"><mo id="S3.p4.2.m2.1.1.1.1.2" stretchy="false" xref="S3.p4.2.m2.1.1.1.1.1.cmml">(</mo><msub id="S3.p4.2.m2.1.1.1.1.1" xref="S3.p4.2.m2.1.1.1.1.1.cmml"><mi id="S3.p4.2.m2.1.1.1.1.1.2" xref="S3.p4.2.m2.1.1.1.1.1.2.cmml">I</mi><mi id="S3.p4.2.m2.1.1.1.1.1.3" xref="S3.p4.2.m2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.p4.2.m2.1.1.1.1.3" stretchy="false" xref="S3.p4.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><times id="S3.p4.2.m2.1.1.2.cmml" xref="S3.p4.2.m2.1.1.2"></times><apply id="S3.p4.2.m2.1.1.3.cmml" xref="S3.p4.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.3.1.cmml" xref="S3.p4.2.m2.1.1.3">subscript</csymbol><ci id="S3.p4.2.m2.1.1.3.2a.cmml" xref="S3.p4.2.m2.1.1.3.2"><mtext class="ltx_font_smallcaps" id="S3.p4.2.m2.1.1.3.2.cmml" xref="S3.p4.2.m2.1.1.3.2">CLIP</mtext></ci><apply id="S3.p4.2.m2.1.1.3.3.cmml" xref="S3.p4.2.m2.1.1.3.3"><times id="S3.p4.2.m2.1.1.3.3.1.cmml" xref="S3.p4.2.m2.1.1.3.3.1"></times><ci id="S3.p4.2.m2.1.1.3.3.2.cmml" xref="S3.p4.2.m2.1.1.3.3.2">𝑖</ci><ci id="S3.p4.2.m2.1.1.3.3.3.cmml" xref="S3.p4.2.m2.1.1.3.3.3">𝑚</ci><ci id="S3.p4.2.m2.1.1.3.3.4.cmml" xref="S3.p4.2.m2.1.1.3.3.4">𝑔</ci></apply></apply><apply id="S3.p4.2.m2.1.1.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.p4.2.m2.1.1.1.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.p4.2.m2.1.1.1.1.1.2.cmml" xref="S3.p4.2.m2.1.1.1.1.1.2">𝐼</ci><ci id="S3.p4.2.m2.1.1.1.1.1.3.cmml" xref="S3.p4.2.m2.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">\textsc{CLIP}_{img}(I_{i})</annotation><annotation encoding="application/x-llamapun" id="S3.p4.2.m2.1d">CLIP start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT ( italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> with a single textual <span class="ltx_text ltx_font_smallcaps" id="S3.p4.2.2">CLIP</span> representation, following the intuition that the latter already encodes relevant visual information as the product of the contrastive training process with images.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span><span class="ltx_text ltx_font_smallcaps" id="S3.T1.13.1">Blind-VaLM</span> matches <span class="ltx_text ltx_font_smallcaps" id="S3.T1.14.2">VaLM</span> on VLU, NLU and LM tasks when trained on the same setup, while being significantly more efficient to train.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S3.T1.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.3.3.4"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.4.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="3" id="S3.T1.1.1.1">
<span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1">Color</span> (ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.2.2">
<span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.1">Shape</span> (ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.m1.1a"><mo id="S3.T1.2.2.2.m1.1.1" stretchy="false" xref="S3.T1.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.m1.1b"><ci id="S3.T1.2.2.2.m1.1.1.cmml" xref="S3.T1.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S3.T1.3.3.3">
<span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.1">Size</span> (ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.3.3.3.m1.1"><semantics id="S3.T1.3.3.3.m1.1a"><mo id="S3.T1.3.3.3.m1.1.1" stretchy="false" xref="S3.T1.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.m1.1b"><ci id="S3.T1.3.3.3.m1.1.1.cmml" xref="S3.T1.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.m1.1d">↑</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.3.3.5"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.5.1">AVG</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.3.4.1">
<td class="ltx_td" id="S3.T1.3.4.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.3.4.1.2">MemoryC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.3.4.1.3">ColorTerms</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.3.4.1.4">VCT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.3.4.1.5">ShapeITC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.3.4.1.6">RelativeS</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.3.4.1.7">TNWT</th>
<td class="ltx_td" id="S3.T1.3.4.1.8"></td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.5.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.3.5.2.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.3.5.2.1.1">VaLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.5.2.2">47.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.5.2.3">41.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.5.2.4">20.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.5.2.5"><span class="ltx_text ltx_font_bold" id="S3.T1.3.5.2.5.1">40.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.5.2.6"><span class="ltx_text ltx_font_bold" id="S3.T1.3.5.2.6.1">26.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.5.2.7">23.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.3.5.2.8">33.30</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.6.3">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.3.6.3.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.3.6.3.1.1">Blind-VaLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.6.3.2"><span class="ltx_text ltx_font_bold" id="S3.T1.3.6.3.2.1">47.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.6.3.3"><span class="ltx_text ltx_font_bold" id="S3.T1.3.6.3.3.1">46.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.6.3.4"><span class="ltx_text ltx_font_bold" id="S3.T1.3.6.3.4.1">22.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.6.3.5">40.07</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.6.3.6">25.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.6.3.7"><span class="ltx_text ltx_font_bold" id="S3.T1.3.6.3.7.1">25.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.6.3.8"><span class="ltx_text ltx_font_bold" id="S3.T1.3.6.3.8.1">34.48</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S3.T1.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.10.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.10.8.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.10.8.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.10.8.1.2"><span class="ltx_text ltx_font_bold" id="S3.T1.10.8.1.2.1">Wikitext-103</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S3.T1.10.8.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.10.8.1.3.1">Lambada</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.10.8.1.4"><span class="ltx_text ltx_font_bold" id="S3.T1.10.8.1.4.1">SST-2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.10.8.1.5"><span class="ltx_text ltx_font_bold" id="S3.T1.10.8.1.5.1">DBPedia</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.10.8.1.6"><span class="ltx_text ltx_font_bold" id="S3.T1.10.8.1.6.1">AGNews</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.10.8.1.7"><span class="ltx_text ltx_font_bold" id="S3.T1.10.8.1.7.1">MPQA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.10.7">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T1.10.7.8"></th>
<td class="ltx_td ltx_align_center" id="S3.T1.4.1.1">PPL <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.4.1.1.m1.1"><semantics id="S3.T1.4.1.1.m1.1a"><mo id="S3.T1.4.1.1.m1.1.1" stretchy="false" xref="S3.T1.4.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.1.1.m1.1b"><ci id="S3.T1.4.1.1.m1.1.1.cmml" xref="S3.T1.4.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.5.2.2">PPL <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.5.2.2.m1.1"><semantics id="S3.T1.5.2.2.m1.1a"><mo id="S3.T1.5.2.2.m1.1.1" stretchy="false" xref="S3.T1.5.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.2.2.m1.1b"><ci id="S3.T1.5.2.2.m1.1.1.cmml" xref="S3.T1.5.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.2.2.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.6.3.3">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.6.3.3.m1.1"><semantics id="S3.T1.6.3.3.m1.1a"><mo id="S3.T1.6.3.3.m1.1.1" stretchy="false" xref="S3.T1.6.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.3.3.m1.1b"><ci id="S3.T1.6.3.3.m1.1.1.cmml" xref="S3.T1.6.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.3.3.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.4.4">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.7.4.4.m1.1"><semantics id="S3.T1.7.4.4.m1.1a"><mo id="S3.T1.7.4.4.m1.1.1" stretchy="false" xref="S3.T1.7.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.7.4.4.m1.1b"><ci id="S3.T1.7.4.4.m1.1.1.cmml" xref="S3.T1.7.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.4.4.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.8.5.5">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.8.5.5.m1.1"><semantics id="S3.T1.8.5.5.m1.1a"><mo id="S3.T1.8.5.5.m1.1.1" stretchy="false" xref="S3.T1.8.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.8.5.5.m1.1b"><ci id="S3.T1.8.5.5.m1.1.1.cmml" xref="S3.T1.8.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.5.5.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.9.6.6">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.9.6.6.m1.1"><semantics id="S3.T1.9.6.6.m1.1a"><mo id="S3.T1.9.6.6.m1.1.1" stretchy="false" xref="S3.T1.9.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.9.6.6.m1.1b"><ci id="S3.T1.9.6.6.m1.1.1.cmml" xref="S3.T1.9.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.9.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.9.6.6.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S3.T1.10.7.7">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.10.7.7.m1.1"><semantics id="S3.T1.10.7.7.m1.1a"><mo id="S3.T1.10.7.7.m1.1.1" stretchy="false" xref="S3.T1.10.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.10.7.7.m1.1b"><ci id="S3.T1.10.7.7.m1.1.1.cmml" xref="S3.T1.10.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.7.7.m1.1d">↑</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.10.9.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T1.10.9.1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.10.9.1.1.1">VaLM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.10.9.1.2">43.68</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.10.9.1.3"><span class="ltx_text ltx_font_bold" id="S3.T1.10.9.1.3.1">45.69</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.10.9.1.4">35.61</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.10.9.1.5">44.66</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.10.9.1.6">66.77</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.10.9.1.7">41.63</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T1.10.9.1.8">67.95</th>
</tr>
<tr class="ltx_tr" id="S3.T1.10.10.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T1.10.10.2.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T1.10.10.2.1.1">Blind-VaLM</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.10.10.2.2"><span class="ltx_text ltx_font_bold" id="S3.T1.10.10.2.2.1">43.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.10.10.2.3">45.78</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.10.10.2.4"><span class="ltx_text ltx_font_bold" id="S3.T1.10.10.2.4.1">39.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.10.10.2.5"><span class="ltx_text ltx_font_bold" id="S3.T1.10.10.2.5.1">44.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.10.10.2.6"><span class="ltx_text ltx_font_bold" id="S3.T1.10.10.2.6.1">73.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.10.10.2.7"><span class="ltx_text ltx_font_bold" id="S3.T1.10.10.2.7.1">44.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.10.10.2.8"><span class="ltx_text ltx_font_bold" id="S3.T1.10.10.2.8.1">71.65</span></td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>When trained within the same compute budget by increasing either model size (<span class="ltx_text ltx_font_smallcaps" id="S3.T2.15.1">Blind-VaLM-Medium</span>, referred to as <span class="ltx_text ltx_font_smallcaps" id="S3.T2.16.2">Blind-VaLM-M</span> in the table) or pre-training compute (<span class="ltx_text ltx_font_smallcaps" id="S3.T2.17.3">Blind-VaLM+</span>), our approach outperforms <span class="ltx_text ltx_font_smallcaps" id="S3.T2.18.4">VaLM</span> on both VLU and NLU tasks.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S3.T2.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T2.3.3.4"><span class="ltx_text ltx_font_bold" id="S3.T2.3.3.4.1">Model</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S3.T2.1.1.1">
<span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1">Color</span> (ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.m1.1.1" stretchy="false" xref="S3.T2.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.m1.1d">↑</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.2.2.2">
<span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.1">Shape</span> (ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.2.2.2.m1.1"><semantics id="S3.T2.2.2.2.m1.1a"><mo id="S3.T2.2.2.2.m1.1.1" stretchy="false" xref="S3.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m1.1b"><ci id="S3.T2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.m1.1d">↑</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T2.3.3.3">
<span class="ltx_text ltx_font_bold" id="S3.T2.3.3.3.1">Size</span> (ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.3.3.3.m1.1"><semantics id="S3.T2.3.3.3.m1.1a"><mo id="S3.T2.3.3.3.m1.1.1" stretchy="false" xref="S3.T2.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.m1.1b"><ci id="S3.T2.3.3.3.m1.1.1.cmml" xref="S3.T2.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.3.m1.1d">↑</annotation></semantics></math>)</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.3.3.5"><span class="ltx_text ltx_font_bold" id="S3.T2.3.3.5.1">AVG</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.4.1">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T2.3.4.1.1"></th>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.1.2">MemoryC</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.1.3">CTerms</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.1.4">VCT</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.1.5">ShapeITC</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.1.6">RelativeS</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.4.1.7">TNWT</td>
<td class="ltx_td" id="S3.T2.3.4.1.8"></td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.5.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.5.2.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T2.3.5.2.1.1">VaLM</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.2.2">47.09</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.2.3">41.88</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.2.4">20.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.2.5">40.45</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.2.6"><span class="ltx_text ltx_font_bold" id="S3.T2.3.5.2.6.1">26.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.2.7">23.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.5.2.8">33.30</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.3.6.3.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T2.3.6.3.1.1">Blind-VaLM</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.3.2">47.20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.3.3">46.37</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.3.4">22.60</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.3.5">40.07</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.3.6">25.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.3.7">25.18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.3.6.3.8">34.47</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.3.7.4.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T2.3.7.4.1.1">Blind-VaLM+</span></th>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.4.2">45.97</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.4.3">48.71</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.4.4">20.51</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.4.5"><span class="ltx_text ltx_font_bold" id="S3.T2.3.7.4.5.1">43.64</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.4.6">25.33</td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.4.7"><span class="ltx_text ltx_font_bold" id="S3.T2.3.7.4.7.1">25.40</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.3.7.4.8">34.93</td>
</tr>
<tr class="ltx_tr" id="S3.T2.3.8.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.3.8.5.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T2.3.8.5.1.1">Blind-VaLM-M</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.8.5.2"><span class="ltx_text ltx_font_bold" id="S3.T2.3.8.5.2.1">47.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.8.5.3"><span class="ltx_text ltx_font_bold" id="S3.T2.3.8.5.3.1">48.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.8.5.4"><span class="ltx_text ltx_font_bold" id="S3.T2.3.8.5.4.1">24.09</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.8.5.5">43.33</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.8.5.6">24.36</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.8.5.7">24.93</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.3.8.5.8"><span class="ltx_text ltx_font_bold" id="S3.T2.3.8.5.8.1">35.54</span></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S3.T2.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.10.8.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T2.10.8.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.10.8.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.10.8.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.10.8.1.2.1">Wikitext-103</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S3.T2.10.8.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.10.8.1.3.1">Lambada</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.10.8.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.10.8.1.4.1">SST-2</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.10.8.1.5"><span class="ltx_text ltx_font_bold" id="S3.T2.10.8.1.5.1">DBPedia</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.10.8.1.6"><span class="ltx_text ltx_font_bold" id="S3.T2.10.8.1.6.1">AGNews</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T2.10.8.1.7"><span class="ltx_text ltx_font_bold" id="S3.T2.10.8.1.7.1">MPQA</span></th>
</tr>
<tr class="ltx_tr" id="S3.T2.10.7">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T2.10.7.8"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.4.1.1">PPL <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.4.1.1.m1.1"><semantics id="S3.T2.4.1.1.m1.1a"><mo id="S3.T2.4.1.1.m1.1.1" stretchy="false" xref="S3.T2.4.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.1.1.m1.1b"><ci id="S3.T2.4.1.1.m1.1.1.cmml" xref="S3.T2.4.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.5.2.2">PPL <math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.5.2.2.m1.1"><semantics id="S3.T2.5.2.2.m1.1a"><mo id="S3.T2.5.2.2.m1.1.1" stretchy="false" xref="S3.T2.5.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.2.2.m1.1b"><ci id="S3.T2.5.2.2.m1.1.1.cmml" xref="S3.T2.5.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.2.2.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.6.3.3">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.6.3.3.m1.1"><semantics id="S3.T2.6.3.3.m1.1a"><mo id="S3.T2.6.3.3.m1.1.1" stretchy="false" xref="S3.T2.6.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.3.3.m1.1b"><ci id="S3.T2.6.3.3.m1.1.1.cmml" xref="S3.T2.6.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.6.3.3.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.7.4.4">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.7.4.4.m1.1"><semantics id="S3.T2.7.4.4.m1.1a"><mo id="S3.T2.7.4.4.m1.1.1" stretchy="false" xref="S3.T2.7.4.4.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.4.4.m1.1b"><ci id="S3.T2.7.4.4.m1.1.1.cmml" xref="S3.T2.7.4.4.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.4.4.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.7.4.4.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.8.5.5">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.8.5.5.m1.1"><semantics id="S3.T2.8.5.5.m1.1a"><mo id="S3.T2.8.5.5.m1.1.1" stretchy="false" xref="S3.T2.8.5.5.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.5.5.m1.1b"><ci id="S3.T2.8.5.5.m1.1.1.cmml" xref="S3.T2.8.5.5.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.5.5.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.8.5.5.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.9.6.6">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.9.6.6.m1.1"><semantics id="S3.T2.9.6.6.m1.1a"><mo id="S3.T2.9.6.6.m1.1.1" stretchy="false" xref="S3.T2.9.6.6.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.9.6.6.m1.1b"><ci id="S3.T2.9.6.6.m1.1.1.cmml" xref="S3.T2.9.6.6.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.6.6.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.9.6.6.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T2.10.7.7">ACC <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.10.7.7.m1.1"><semantics id="S3.T2.10.7.7.m1.1a"><mo id="S3.T2.10.7.7.m1.1.1" stretchy="false" xref="S3.T2.10.7.7.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.10.7.7.m1.1b"><ci id="S3.T2.10.7.7.m1.1.1.cmml" xref="S3.T2.10.7.7.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.7.7.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.10.7.7.m1.1d">↑</annotation></semantics></math>
</th>
</tr>
<tr class="ltx_tr" id="S3.T2.10.9.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S3.T2.10.9.2.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T2.10.9.2.1.1">VaLM</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.10.9.2.2">43.68</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.10.9.2.3">45.69</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.10.9.2.4">35.61</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.10.9.2.5">44.66</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.10.9.2.6">66.77</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.10.9.2.7">41.63</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T2.10.9.2.8">67.95</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.10.10.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.10.10.1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T2.10.10.1.1.1">Blind-VaLM</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.1.2">43.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.1.3">45.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.1.4">39.06</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.1.5">44.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.1.6">73.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.1.7">44.97</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.10.10.1.8">71.65</td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.11.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.10.11.2.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T2.10.11.2.1.1">Blind-VaLM+</span></th>
<td class="ltx_td ltx_align_center" id="S3.T2.10.11.2.2">40.95</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.11.2.3">43.83</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.11.2.4">39.05</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.11.2.5">44.66</td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.11.2.6"><span class="ltx_text ltx_font_bold" id="S3.T2.10.11.2.6.1">74.42</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.11.2.7"><span class="ltx_text ltx_font_bold" id="S3.T2.10.11.2.7.1">50.44</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.10.11.2.8"><span class="ltx_text ltx_font_bold" id="S3.T2.10.11.2.8.1">78.10</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.10.12.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.10.12.3.1"><span class="ltx_text ltx_font_smallcaps" id="S3.T2.10.12.3.1.1">Blind-VaLM-M</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.10.12.3.2"><span class="ltx_text ltx_font_bold" id="S3.T2.10.12.3.2.1">33.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.10.12.3.3"><span class="ltx_text ltx_font_bold" id="S3.T2.10.12.3.3.1">37.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.10.12.3.4"><span class="ltx_text ltx_font_bold" id="S3.T2.10.12.3.4.1">45.08</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.10.12.3.5"><span class="ltx_text ltx_font_bold" id="S3.T2.10.12.3.5.1">59.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.10.12.3.6">70.57</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.10.12.3.7">48.28</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.10.12.3.8">55.65</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental setup</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We want to show that actual retrieval and representation of images is not necessary to augment a LM with visual knowledge, and that directly using textual representations from a visually grounded text encoder instead is sufficient. To that end, we initially pre-train <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">Blind-VaLM</span> and <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.2">VaLM</span> models in a comparable setting, which we now describe.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Text Corpus.</span> Following the original <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.2">VaLM</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib25" title="">2022</a>)</cite>, we use the English corpus of CC-100 <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib6" title="">2020</a>)</cite> as the pre-training text corpus for all models.
Due to limited access to compute, we only consume 10.5B tokens for pre-training, which accounts for around 19% of the English portion of the corpus.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Image data and retrieval module.</span> <span class="ltx_text ltx_font_smallcaps" id="S4.p3.1.2">VaLM</span> requires an image database and a vector retrieval module. We use a <em class="ltx_emph ltx_font_italic" id="S4.p3.1.3">FAISS</em> <cite class="ltx_cite ltx_citemacro_citep">(Johnson et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib10" title="">2019</a>)</cite> index, trained on the exact same setup as the original <span class="ltx_text ltx_font_smallcaps" id="S4.p3.1.4">VaLM</span>, based on the <span class="ltx_text ltx_font_smallcaps" id="S4.p3.1.5">GPT2-Small</span> architecture (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A1" title="Appendix A Training hyper-parameters ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Pre-training hyperparameters.</span> We train both models on the exact same setup, following a configuration similar to the original <span class="ltx_text ltx_font_smallcaps" id="S4.p4.1.2">VaLM</span>. Details can be found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A1" title="Appendix A Training hyper-parameters ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">A</span></a>.</p>
</div>
<div class="ltx_para" id="S4.p5">
<p class="ltx_p" id="S4.p5.1">Due to the increased efficiency of our architecture, our model employs significantly less compute than the original: <span class="ltx_text ltx_font_smallcaps" id="S4.p5.1.1">Blind-VaLM</span> was trained on 530 GPU-hours, while the <span class="ltx_text ltx_font_smallcaps" id="S4.p5.1.2">VaLM</span> baseline required 1.2K GPU-hours, making our approach <span class="ltx_text ltx_font_bold" id="S4.p5.1.3">2.2x faster to train</span>. We train all our models on a cluster of 8 A100 GPUs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1"><span class="ltx_text ltx_font_bold" id="S4.p6.1.1">Evaluation.</span> We evaluate our models on VLU, NLU and LM tasks (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A2" title="Appendix B Evaluation details ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">B</span></a> for details).</p>
</div>
<div class="ltx_para" id="S4.p7">
<p class="ltx_p" id="S4.p7.1">For <span class="ltx_text ltx_font_bold" id="S4.p7.1.1">VLU</span>, we focus on three basic visual properties of objects: color, shape and size. We evaluate color knowlege on the following datasets: Memory Color <cite class="ltx_cite ltx_citemacro_citep">(Norlund et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib16" title="">2021</a>)</cite>, Color Terms <cite class="ltx_cite ltx_citemacro_citep">(Bruni et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib5" title="">2012</a>)</cite> and ViComTe (color subset) <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib28" title="">2022</a>)</cite>. We evaluate knowledge about shape on the ShapeITC <cite class="ltx_cite ltx_citemacro_citep">(Alper et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib2" title="">2023</a>)</cite> dataset, and knowledge of size on the RelativeSize <cite class="ltx_cite ltx_citemacro_citep">(Bagherinezhad et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib4" title="">2016</a>)</cite> and Things Not Written in Text <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib12" title="">2022</a>)</cite> datasets.</p>
</div>
<div class="ltx_para" id="S4.p8">
<p class="ltx_p" id="S4.p8.1">We evaluate <span class="ltx_text ltx_font_bold" id="S4.p8.1.1">NLU</span> capabilities on four downstream tasks: two sentiment analysis tasks on the SST-2 and MPQA datasets <cite class="ltx_cite ltx_citemacro_citep">(Socher et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib21" title="">2013</a>; Wiebe et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib26" title="">2005</a>)</cite>, and two topic classification tasks on the AGNews and DBPedia datasets <cite class="ltx_cite ltx_citemacro_citep">(Auer et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib3" title="">2007</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib29" title="">2015</a>)</cite>. Additionally, we evaluate pure language modeling ability by measuring perplexity on the Wikitext-103 and Lambada datasets <cite class="ltx_cite ltx_citemacro_citep">(Merity et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib15" title="">2016</a>; Paperno et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib17" title="">2016</a>)</cite>. For Lambada, we also report accuracy at predicting the last word of each sentence, following the original <span class="ltx_text ltx_font_smallcaps" id="S4.p8.1.2">VaLM</span> work.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p9">
<p class="ltx_p" id="S4.p9.1"><span class="ltx_text ltx_font_bold" id="S4.p9.1.1">Scaling <span class="ltx_text ltx_font_smallcaps" id="S4.p9.1.1.1">Blind-VaLM</span></span>. It is important to note that, since <span class="ltx_text ltx_font_smallcaps" id="S4.p9.1.2">Blind-VaLM</span> does not require an actual image retrieval step, it is significantly more efficient at both training and inference time.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>The efficiency gains are two-fold: 1) since we only use a single CLIP text representation in the Fusion Layer, this layer requires less floating-point operations (FLOPs), and 2) since we do away with the dense vector database retrieval, training and inference latency is significantly reduced, leading to improved wall-clock efficiency.</span></span></span> Taking advantage of this increased efficiency, we explore two ways of scaling up <span class="ltx_text ltx_font_smallcaps" id="S4.p9.1.3">Blind-VaLM</span>, within the compute budget of our <span class="ltx_text ltx_font_smallcaps" id="S4.p9.1.4">VaLM</span> baseline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p10">
<p class="ltx_p" id="S4.p10.1"><span class="ltx_text ltx_font_bold" id="S4.p10.1.1">Scaling model size.</span> We train <span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.p10.1.2">Blind-VaLM-Medium</span>, by switching the LM backbone architecture to <span class="ltx_text ltx_font_smallcaps" id="S4.p10.1.3">GPT2-Medium</span> instead of <span class="ltx_text ltx_font_smallcaps" id="S4.p10.1.4">GPT2-Small</span>. See Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A1" title="Appendix A Training hyper-parameters ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">A</span></a> for details. This larger model was trained on 595 GPU-hours, still within the compute budget of the <span class="ltx_text ltx_font_smallcaps" id="S4.p10.1.5">VaLM</span> baseline.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p11">
<p class="ltx_p" id="S4.p11.1"><span class="ltx_text ltx_font_bold" id="S4.p11.1.1">Scaling pre-training compute</span>. We train <span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S4.p11.1.2">Blind-VaLM+</span>, by simply continuing pre-training the baseline <span class="ltx_text ltx_font_smallcaps" id="S4.p11.1.3">Blind-VaLM</span> for longer, until we reach a total of 88255 steps, which corresponds to 23.1B tokens (42% of CC-100). Again, thanks to the increased efficiency of our approach, this model is compute-matched<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>Note that we use <span class="ltx_text ltx_font_italic" id="footnote3.1">compute-matched</span> in the wall-clock time sense here, not the FLOPs sense, since the time consuming dense vector retrieval step we remove is not reflected by FLOPs.</span></span></span> to the original <span class="ltx_text ltx_font_smallcaps" id="S4.p11.1.4">VaLM</span> baseline, taking a total of 1.17K GPU-hours to train.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We next describe our main results.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.p2.1.1">Blind-VaLM<span class="ltx_text ltx_font_upright" id="S5.p2.1.1.1"> matches </span>VaLM<span class="ltx_text ltx_font_upright" id="S5.p2.1.1.2"> on VLU, NLU and LM tasks</span></span>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S3.T1" title="Table 1 ‣ 3 Blind-VaLM architecture ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">1</span></a> shows results for <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.2">Blind-VaLM</span> and <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.3">VaLM</span> trained on the same setup, as described in §<a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S4" title="4 Experimental setup ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>. We observe that our approach matches the original <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.4">VaLM</span> on VLU tasks, as well as NLU and LM tasks: it achieves an average score <math alttext="1.18" class="ltx_Math" display="inline" id="S5.p2.1.m1.1"><semantics id="S5.p2.1.m1.1a"><mn id="S5.p2.1.m1.1.1" xref="S5.p2.1.m1.1.1.cmml">1.18</mn><annotation-xml encoding="MathML-Content" id="S5.p2.1.m1.1b"><cn id="S5.p2.1.m1.1.1.cmml" type="float" xref="S5.p2.1.m1.1.1">1.18</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p2.1.m1.1c">1.18</annotation><annotation encoding="application/x-llamapun" id="S5.p2.1.m1.1d">1.18</annotation></semantics></math> points higher in VLU tasks, and outperforms <span class="ltx_text ltx_font_smallcaps" id="S5.p2.1.5">VaLM</span> on 6/7 NLU &amp; LM tasks. This supports our hypothesis that <span class="ltx_text ltx_font_bold" id="S5.p2.1.6">actually retrieving and encoding images is not required for visual augmentation</span>, since simply utilizing textual representations from an already visually grounded text encoder works equally well. Additionally, as described in section §<a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S4" title="4 Experimental setup ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> <span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.p2.1.7">Blind-VaLM<span class="ltx_text ltx_font_upright" id="S5.p2.1.7.1"> is 2.2x faster</span></span> to train, since it skips the time consuming vector retrieval step. Speedups are even more significant at inference time, since generation is not as compute-bound and retrieval latency plays a bigger role.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold ltx_font_smallcaps" id="S5.p3.1.1">Blind-VaLM<span class="ltx_text ltx_font_upright" id="S5.p3.1.1.1"> outperforms </span>VaLM<span class="ltx_text ltx_font_upright" id="S5.p3.1.1.2">, when trained within the same compute budget</span></span>. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S3.T2" title="Table 2 ‣ 3 Blind-VaLM architecture ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">2</span></a> shows results for two scaled-up <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.2">Blind-VaLM</span> variants, obtained through scaling either pre-training compute or model size, as described in §<a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#S4" title="4 Experimental setup ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">4</span></a>. We observe that both variants outperform <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.3">VaLM</span>, while being trained within the same compute budget as it. For example, in the case of <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.4">Blind-VaLM-Medium</span>, we outperform <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.5">VaLM</span> by <math alttext="2.2" class="ltx_Math" display="inline" id="S5.p3.1.m1.1"><semantics id="S5.p3.1.m1.1a"><mn id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">2.2</mn><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><cn id="S5.p3.1.m1.1.1.cmml" type="float" xref="S5.p3.1.m1.1.1">2.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">2.2</annotation><annotation encoding="application/x-llamapun" id="S5.p3.1.m1.1d">2.2</annotation></semantics></math> points on average for VLU tasks, and outperform <span class="ltx_text ltx_font_smallcaps" id="S5.p3.1.6">VaLM</span> on 6/7 NLU &amp; LM tasks.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusions</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work we test the hypothesis that explicit image retrieval is not necessary to augment an LM with visual information. To that end, we train a modified variant of <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">VaLM</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib25" title="">2022</a>)</cite>, which we call <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.2">Blind-VaLM</span>, by replacing the retrieved image encoding vectors with textual embeddings obtained from the visually grounded CLIP encoder <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib18" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Our results show that <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.1">Blind-VaLM</span> matches <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.2">VaLM</span> when trained on the same data, while being significantly more efficient to train. Additionally, scaling up our model within the compute budget of <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.3">VaLM</span>,
our approach outperforms <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.4">VaLM</span>. Overall, these results show that simply leveraging the textual encoding from an already visually grounded CLIP encoder is enough to obtain the same gains on visual tasks as <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.5">VaLM</span>, supporting our hypothesis that actual image retrieval is not essential.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">These results open up new avenues in the line of work of visually augmenting language models, beyond the paradigm of image retrieval.
Our findings allow for more efficient visual augmentations, which will result on broader exploration capacity for future works.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Limitations</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Our work is focused on English only, due to the size and accessibility of the resources, i.e. text corpora, image-text datasets, different pre-trained models and evaluation benchmarks. However, it would be very interesting to extend the work to other languages.</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">The VLU evaluation is limited to visual object properties, such as color, shape and size. But, visual language is broader and extending evaluation benchmarks would allow to better understand how VLU evolves in pure and visually-augmented LMs.</p>
</div>
<div class="ltx_para" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">Finally, we use the original CLIP model <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib18" title="">2021</a>)</cite> to visually-augment LMs, as done in <span class="ltx_text ltx_font_smallcaps" id="S7.p3.1.1">VaLM</span>. Nowadays there are more powerful multimodal models and it would be very interesting to explore how those better models impact in the knowledge acquisition of visually-augmented LMs.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alper et al. (2023)</span>
<span class="ltx_bibblock">
Morris Alper, Michael Fiman, and Hadar Averbuch-Elor. 2023.

</span>
<span class="ltx_bibblock">Is bert blind? exploring the effect of vision-and-language pretraining on visual language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer et al. (2007)</span>
<span class="ltx_bibblock">
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007.

</span>
<span class="ltx_bibblock">Dbpedia: A nucleus for a web of open data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">international semantic web conference</em>, pages 722–735. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bagherinezhad et al. (2016)</span>
<span class="ltx_bibblock">
Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, and Ali Farhadi. 2016.

</span>
<span class="ltx_bibblock">Are elephants bigger than butterflies? reasoning about sizes of objects.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bruni et al. (2012)</span>
<span class="ltx_bibblock">
Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran. 2012.

</span>
<span class="ltx_bibblock">Distributional semantics in technicolor.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 136–145.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2018)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:1810.04805</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2407.21783</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Hangyu Guo, Kun Zhou, Wayne Xin Zhao, Qinyu Zhang, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock">Visually-augmented pretrained language models for nlp tasks without images.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 14912–14929.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Johnson et al. (2019)</span>
<span class="ltx_bibblock">
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.

</span>
<span class="ltx_bibblock">Billion-scale similarity search with gpus.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Big Data</em>, 7(3):535–547.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba (2015)</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba. 2015.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/1412.6980" title="">Adam: A method for stochastic optimization</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2022)</span>
<span class="ltx_bibblock">
Xiao Liu, Da Yin, Yansong Feng, and Dongyan Zhao. 2022.

</span>
<span class="ltx_bibblock">Things not written in text: Exploring spatial commonsense from visual signals.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 2365–2376.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized bert pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:1907.11692</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2022)</span>
<span class="ltx_bibblock">
Yujie Lu, Wanrong Zhu, Xin Wang, Miguel Eckstein, and William Yang Wang. 2022.

</span>
<span class="ltx_bibblock">Imagination-augmented natural language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 4392–4402.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merity et al. (2016)</span>
<span class="ltx_bibblock">
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016.

</span>
<span class="ltx_bibblock">Pointer sentinel mixture models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Norlund et al. (2021)</span>
<span class="ltx_bibblock">
Tobias Norlund, Lovisa Hagström, and Richard Johansson. 2021.

</span>
<span class="ltx_bibblock">Transferring knowledge from vision to language: How to achieve it and how to measure it?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</em>, pages 149–162.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paperno et al. (2016)</span>
<span class="ltx_bibblock">
Denis Paperno, German David Kruszewski Martel, Angeliki Lazaridou, Ngoc Pham Quan, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda Torrent, Fernández Raquel, et al. 2016.

</span>
<span class="ltx_bibblock">The lambada dataset: Word prediction requiring a broad discourse context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">The 54th Annual Meeting of the Association for Computational Linguistics Proceedings of the Conference: Vol. 1 Long Papers</em>, volume 3, pages 1525–1534. ACL.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">International conference on machine learning</em>, pages 8748–8763. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shwartz and Choi (2020)</span>
<span class="ltx_bibblock">
Vered Shwartz and Yejin Choi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.coling-main.605" title="">Do neural language models overcome reporting bias?</a>
</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 6863–6870, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Socher et al. (2013)</span>
<span class="ltx_bibblock">
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013.

</span>
<span class="ltx_bibblock">Recursive deep models for semantic compositionality over a sentiment treebank.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the 2013 conference on empirical methods in natural language processing</em>, pages 1631–1642.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan and Bansal (2020)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal. 2020.

</span>
<span class="ltx_bibblock">Vokenization: Improving language understanding with contextualized, visual-grounded supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 2066–2080.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock">Learning to imagine: Visually-augmented natural language generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 9468–9481.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2021)</span>
<span class="ltx_bibblock">
Zineng Tang, Jaemin Cho, Hao Tan, and Mohit Bansal. 2021.

</span>
<span class="ltx_bibblock">Vidlankd: Improving language understanding via video-distilled knowledge transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Advances in Neural Information Processing Systems</em>, 34:24468–24481.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Weizhi Wang, Li Dong, Hao Cheng, Haoyu Song, Xiaodong Liu, Xifeng Yan, Jianfeng Gao, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">Visually-augmented language modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">The Eleventh International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wiebe et al. (2005)</span>
<span class="ltx_bibblock">
Janyce Wiebe, Theresa Wilson, and Claire Cardie. 2005.

</span>
<span class="ltx_bibblock">Annotating expressions of opinions and emotions in language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Language resources and evaluation</em>, 39:165–210.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2022)</span>
<span class="ltx_bibblock">
Yue Yang, Wenlin Yao, Hongming Zhang, Xiaoyang Wang, Dong Yu, and Jianshu Chen. 2022.

</span>
<span class="ltx_bibblock">Z-lavi: Zero-shot language solver fueled by visual imagination.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>, pages 1186–1203.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2022)</span>
<span class="ltx_bibblock">
Chenyu Zhang, Benjamin Van Durme, Zhuowan Li, and Elias Stengel-Eskin. 2022.

</span>
<span class="ltx_bibblock">Visual commonsense in pretrained unimodal and multimodal models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 5321–5335.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2015)</span>
<span class="ltx_bibblock">
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015.

</span>
<span class="ltx_bibblock">Character-level convolutional networks for text classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Advances in neural information processing systems</em>, 28.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Training hyper-parameters</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.8">We train all models using an inverse-square-root learning rate schedule, the <span class="ltx_text ltx_font_smallcaps" id="A1.p1.8.1">Adam</span> optimizer <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib11" title="">2015</a>)</cite> with <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mrow id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><msub id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml"><mi id="A1.p1.1.m1.1.1.2.2" xref="A1.p1.1.m1.1.1.2.2.cmml">β</mi><mn id="A1.p1.1.m1.1.1.2.3" xref="A1.p1.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="A1.p1.1.m1.1.1.1" xref="A1.p1.1.m1.1.1.1.cmml">=</mo><mn id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><eq id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1.1"></eq><apply id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.2.1.cmml" xref="A1.p1.1.m1.1.1.2">subscript</csymbol><ci id="A1.p1.1.m1.1.1.2.2.cmml" xref="A1.p1.1.m1.1.1.2.2">𝛽</ci><cn id="A1.p1.1.m1.1.1.2.3.cmml" type="integer" xref="A1.p1.1.m1.1.1.2.3">1</cn></apply><cn id="A1.p1.1.m1.1.1.3.cmml" type="float" xref="A1.p1.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math>, <math alttext="\beta_{2}=0.98" class="ltx_Math" display="inline" id="A1.p1.2.m2.1"><semantics id="A1.p1.2.m2.1a"><mrow id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><msub id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml"><mi id="A1.p1.2.m2.1.1.2.2" xref="A1.p1.2.m2.1.1.2.2.cmml">β</mi><mn id="A1.p1.2.m2.1.1.2.3" xref="A1.p1.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="A1.p1.2.m2.1.1.1" xref="A1.p1.2.m2.1.1.1.cmml">=</mo><mn id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">0.98</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><eq id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1.1"></eq><apply id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="A1.p1.2.m2.1.1.2.1.cmml" xref="A1.p1.2.m2.1.1.2">subscript</csymbol><ci id="A1.p1.2.m2.1.1.2.2.cmml" xref="A1.p1.2.m2.1.1.2.2">𝛽</ci><cn id="A1.p1.2.m2.1.1.2.3.cmml" type="integer" xref="A1.p1.2.m2.1.1.2.3">2</cn></apply><cn id="A1.p1.2.m2.1.1.3.cmml" type="float" xref="A1.p1.2.m2.1.1.3">0.98</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">\beta_{2}=0.98</annotation><annotation encoding="application/x-llamapun" id="A1.p1.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.98</annotation></semantics></math>, a peak LR of <math alttext="2e-3" class="ltx_Math" display="inline" id="A1.p1.3.m3.1"><semantics id="A1.p1.3.m3.1a"><mrow id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml"><mrow id="A1.p1.3.m3.1.1.2" xref="A1.p1.3.m3.1.1.2.cmml"><mn id="A1.p1.3.m3.1.1.2.2" xref="A1.p1.3.m3.1.1.2.2.cmml">2</mn><mo id="A1.p1.3.m3.1.1.2.1" xref="A1.p1.3.m3.1.1.2.1.cmml">⁢</mo><mi id="A1.p1.3.m3.1.1.2.3" xref="A1.p1.3.m3.1.1.2.3.cmml">e</mi></mrow><mo id="A1.p1.3.m3.1.1.1" xref="A1.p1.3.m3.1.1.1.cmml">−</mo><mn id="A1.p1.3.m3.1.1.3" xref="A1.p1.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><apply id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1"><minus id="A1.p1.3.m3.1.1.1.cmml" xref="A1.p1.3.m3.1.1.1"></minus><apply id="A1.p1.3.m3.1.1.2.cmml" xref="A1.p1.3.m3.1.1.2"><times id="A1.p1.3.m3.1.1.2.1.cmml" xref="A1.p1.3.m3.1.1.2.1"></times><cn id="A1.p1.3.m3.1.1.2.2.cmml" type="integer" xref="A1.p1.3.m3.1.1.2.2">2</cn><ci id="A1.p1.3.m3.1.1.2.3.cmml" xref="A1.p1.3.m3.1.1.2.3">𝑒</ci></apply><cn id="A1.p1.3.m3.1.1.3.cmml" type="integer" xref="A1.p1.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">2e-3</annotation><annotation encoding="application/x-llamapun" id="A1.p1.3.m3.1d">2 italic_e - 3</annotation></semantics></math>, a weight-decay of <math alttext="0.01" class="ltx_Math" display="inline" id="A1.p1.4.m4.1"><semantics id="A1.p1.4.m4.1a"><mn id="A1.p1.4.m4.1.1" xref="A1.p1.4.m4.1.1.cmml">0.01</mn><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><cn id="A1.p1.4.m4.1.1.cmml" type="float" xref="A1.p1.4.m4.1.1">0.01</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">0.01</annotation><annotation encoding="application/x-llamapun" id="A1.p1.4.m4.1d">0.01</annotation></semantics></math>, dropout of <math alttext="0.1" class="ltx_Math" display="inline" id="A1.p1.5.m5.1"><semantics id="A1.p1.5.m5.1a"><mn id="A1.p1.5.m5.1.1" xref="A1.p1.5.m5.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="A1.p1.5.m5.1b"><cn id="A1.p1.5.m5.1.1.cmml" type="float" xref="A1.p1.5.m5.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.5.m5.1c">0.1</annotation><annotation encoding="application/x-llamapun" id="A1.p1.5.m5.1d">0.1</annotation></semantics></math>, <math alttext="4000" class="ltx_Math" display="inline" id="A1.p1.6.m6.1"><semantics id="A1.p1.6.m6.1a"><mn id="A1.p1.6.m6.1.1" xref="A1.p1.6.m6.1.1.cmml">4000</mn><annotation-xml encoding="MathML-Content" id="A1.p1.6.m6.1b"><cn id="A1.p1.6.m6.1.1.cmml" type="integer" xref="A1.p1.6.m6.1.1">4000</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.6.m6.1c">4000</annotation><annotation encoding="application/x-llamapun" id="A1.p1.6.m6.1d">4000</annotation></semantics></math> warmup steps, a global batch size of <math alttext="512" class="ltx_Math" display="inline" id="A1.p1.7.m7.1"><semantics id="A1.p1.7.m7.1a"><mn id="A1.p1.7.m7.1.1" xref="A1.p1.7.m7.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A1.p1.7.m7.1b"><cn id="A1.p1.7.m7.1.1.cmml" type="integer" xref="A1.p1.7.m7.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.7.m7.1c">512</annotation><annotation encoding="application/x-llamapun" id="A1.p1.7.m7.1d">512</annotation></semantics></math>, and a sequence length of <math alttext="512" class="ltx_Math" display="inline" id="A1.p1.8.m8.1"><semantics id="A1.p1.8.m8.1a"><mn id="A1.p1.8.m8.1.1" xref="A1.p1.8.m8.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="A1.p1.8.m8.1b"><cn id="A1.p1.8.m8.1.1.cmml" type="integer" xref="A1.p1.8.m8.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.8.m8.1c">512</annotation><annotation encoding="application/x-llamapun" id="A1.p1.8.m8.1d">512</annotation></semantics></math> tokens. We train both models for 40600 steps.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Since both <span class="ltx_text ltx_font_smallcaps" id="A1.p2.1.1">Blind-VaLM</span> and the original <span class="ltx_text ltx_font_smallcaps" id="A1.p2.1.2">VaLM</span> share the limitation of needing tokenization to be compatible with the CLIP used for retrieval, use use the same BPE tokenizer as <span class="ltx_text ltx_font_smallcaps" id="A1.p2.1.3">CLIP</span> and the original <span class="ltx_text ltx_font_smallcaps" id="A1.p2.1.4">GPT2</span>.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1"><span class="ltx_text ltx_font_smallcaps" id="A1.p3.1.1">VaLM</span>, <span class="ltx_text ltx_font_smallcaps" id="A1.p3.1.2">Blind-VaLM</span> and <span class="ltx_text ltx_font_smallcaps" id="A1.p3.1.3">Blind-VaLM</span>+ use the <span class="ltx_text ltx_font_smallcaps" id="A1.p3.1.4">GPT2-Small</span> architecture, comprising 124M parameters, for the LM backbone, and the <span class="ltx_text ltx_font_smallcaps" id="A1.p3.1.5">CLIP-RN50x16 </span> model for the visually grounded text-image encoder, with a total of 85M parameters on the text encoder.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1">For <span class="ltx_text ltx_font_smallcaps" id="A1.p4.1.1">Blind-VaLM-Medium</span>, we switch the LM backbone architecture to <span class="ltx_text ltx_font_smallcaps" id="A1.p4.1.2">GPT2-Medium</span>, with a total of 345M parameters. We switch the CLIP encoder to the <span class="ltx_text ltx_font_smallcaps" id="A1.p4.1.3">CLIP-RN50x64 </span> version with 151M parameters, which employs the hidden-size corresponding to <span class="ltx_text ltx_font_smallcaps" id="A1.p4.1.4">GPT2-Medium</span>, required by the Fusion Layer.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Evaluation details</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">To evaluate VLU and NLU capabilities we follow the prompting approach designed by <span class="ltx_text ltx_font_smallcaps" id="A2.p1.1.1">VaLM</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#bib.bib25" title="">2022</a>)</cite>, with some slight modifications. Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A2.T3" title="Table 3 ‣ Appendix B Evaluation details ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A2.T4" title="Table 4 ‣ Appendix B Evaluation details ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A2.T5" title="Table 5 ‣ Appendix B Evaluation details ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">5</span></a> show the prompts we use for color, shape and size evaluation. Similarly, Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11148v1#A2.T6" title="Table 6 ‣ Appendix B Evaluation details ‣ Improving the Efficiency of Visually Augmented Language Models"><span class="ltx_text ltx_ref_tag">6</span></a> defines the prompts for all the NLU tasks we consider. The reported results in the paper are always the average of the results obtained for all the prompts for a given task, with the objective of avoiding any bias towards different prompting choices.</p>
</div>
<figure class="ltx_table" id="A2.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span> The prompts and prediction labels used in object color reasoning.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="A2.T3.1" style="width:684.2pt;height:315.4pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-38.0pt,17.5pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T3.1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_t" id="A2.T3.1.1.1.1.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.1.1.1.1">
<span class="ltx_p" id="A2.T3.1.1.1.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.1.1.1.1.1">Task</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T3.1.1.1.1.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.1.1.2.1">
<span class="ltx_p" id="A2.T3.1.1.1.1.2.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.1.2.1.1.1">Dataset</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T3.1.1.1.1.3" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.1.1.3.1">
<span class="ltx_p" id="A2.T3.1.1.1.1.3.1.1" style="width:256.1pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.1.3.1.1.1">Prompt</span></span>
</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T3.1.1.1.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.1.1.4.1">Labels</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T3.1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" id="A2.T3.1.1.2.1.1" rowspan="16" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.2.1.1.1">
<span class="ltx_p" id="A2.T3.1.1.2.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A2.T3.1.1.2.1.1.1.1.1">Object Color Reasoning</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.1.2.1.2" rowspan="9" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.2.1.2.1">
<span class="ltx_p" id="A2.T3.1.1.2.1.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A2.T3.1.1.2.1.2.1.1.1">Memory Colors and Color Terms</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T3.1.1.2.1.3" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.2.1.3.1">
<span class="ltx_p" id="A2.T3.1.1.2.1.3.1.1" style="width:256.1pt;">Q: What is the color of [DESCRIPTOR] [ITEM]? A: It is [Label]</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T3.1.1.2.1.4" rowspan="9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="A2.T3.1.1.2.1.4.1">{red, white, orange, green, blue, yellow, purple, black, pink, grey, brown}</span></td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.3.2.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.3.2.1.1">
<span class="ltx_p" id="A2.T3.1.1.3.2.1.1.1" style="width:256.1pt;">Q: What is the colour of [DESCRIPTOR] [ITEM] ? A: It is [Label]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.4.3.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.4.3.1.1">
<span class="ltx_p" id="A2.T3.1.1.4.3.1.1.1" style="width:256.1pt;">What is the color of [DESCRIPTOR] [ITEM]? It is [Label]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.5.4.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.5.4.1.1">
<span class="ltx_p" id="A2.T3.1.1.5.4.1.1.1" style="width:256.1pt;">What is the colour of [DESCRIPTOR] [ITEM]? [Label]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.6.5.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.6.5.1.1">
<span class="ltx_p" id="A2.T3.1.1.6.5.1.1.1" style="width:256.1pt;">The color of [DESCRIPTOR] [ITEM] is [Label]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.7.6.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.7.6.1.1">
<span class="ltx_p" id="A2.T3.1.1.7.6.1.1.1" style="width:256.1pt;">The usual color of [DESCRIPTOR] [ITEM] is [Label]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.8.7.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.8.7.1.1">
<span class="ltx_p" id="A2.T3.1.1.8.7.1.1.1" style="width:256.1pt;">[DESCRIPTOR] [ITEM] usually has the color of [Label]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.9.8.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.9.8.1.1">
<span class="ltx_p" id="A2.T3.1.1.9.8.1.1.1" style="width:256.1pt;">What is the usual color of [DESCRIPTOR] [ITEM]? [Label]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.10.9.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.10.9.1.1">
<span class="ltx_p" id="A2.T3.1.1.10.9.1.1.1" style="width:256.1pt;">What is the typical color of [DESCRIPTOR] [ITEM]? [Label]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" id="A2.T3.1.1.11.10.1" rowspan="7" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.11.10.1.1">
<span class="ltx_p" id="A2.T3.1.1.11.10.1.1.1" style="width:71.1pt;"><span class="ltx_text" id="A2.T3.1.1.11.10.1.1.1.1">ViComTe</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.11.10.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.11.10.2.1">
<span class="ltx_p" id="A2.T3.1.1.11.10.2.1.1" style="width:256.1pt;">[ITEM] can be of color</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_b" id="A2.T3.1.1.11.10.3" rowspan="7" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="A2.T3.1.1.11.10.3.1">{red, white, orange, green, blue, yellow, purple, black, pink, grey, brown, silver}</span></td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.12.11.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.12.11.1.1">
<span class="ltx_p" id="A2.T3.1.1.12.11.1.1.1" style="width:256.1pt;">[ITEM] has color</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.13.12.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.13.12.1.1">
<span class="ltx_p" id="A2.T3.1.1.13.12.1.1.1" style="width:256.1pt;">The color of [ITEM] can be</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.14.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.14.13.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.14.13.1.1">
<span class="ltx_p" id="A2.T3.1.1.14.13.1.1.1" style="width:256.1pt;">The color of the [ITEM] is</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.15.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.15.14.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.15.14.1.1">
<span class="ltx_p" id="A2.T3.1.1.15.14.1.1.1" style="width:256.1pt;">[ITEM] is</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.16.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T3.1.1.16.15.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.16.15.1.1">
<span class="ltx_p" id="A2.T3.1.1.16.15.1.1.1" style="width:256.1pt;">This [ITEM] is</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T3.1.1.17.16">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" id="A2.T3.1.1.17.16.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T3.1.1.17.16.1.1">
<span class="ltx_p" id="A2.T3.1.1.17.16.1.1.1" style="width:256.1pt;">[ITEM] is of color</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="A2.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span> The prompts and prediction labels used in object shape reasoning.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T4.1" style="width:306.7pt;height:162pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-17.0pt,9.0pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T4.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T4.1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_t" id="A2.T4.1.1.1.1.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.1.1.1.1">
<span class="ltx_p" id="A2.T4.1.1.1.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.1.1.1.1.1">Task</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T4.1.1.1.1.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.1.1.2.1">
<span class="ltx_p" id="A2.T4.1.1.1.1.2.1.1" style="width:142.3pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.1.2.1.1.1">Prompt</span></span>
</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T4.1.1.1.1.3" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.1.1.3.1">Labels</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T4.1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" id="A2.T4.1.1.2.1.1" rowspan="9" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.2.1.1.1">
<span class="ltx_p" id="A2.T4.1.1.2.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A2.T4.1.1.2.1.1.1.1.1">Object Shape Reasoning</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T4.1.1.2.1.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.2.1.2.1">
<span class="ltx_p" id="A2.T4.1.1.2.1.2.1.1" style="width:142.3pt;">[ITEM] can be shape of</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_b ltx_border_t" id="A2.T4.1.1.2.1.3" rowspan="9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="A2.T4.1.1.2.1.3.1">{circle, rectangle, triangle}</span></td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.1.3.2.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.3.2.1.1">
<span class="ltx_p" id="A2.T4.1.1.3.2.1.1.1" style="width:142.3pt;">[ITEM] has shape of</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.1.4.3.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.4.3.1.1">
<span class="ltx_p" id="A2.T4.1.1.4.3.1.1.1" style="width:142.3pt;">[ITEM] is of shape</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.1.5.4.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.5.4.1.1">
<span class="ltx_p" id="A2.T4.1.1.5.4.1.1.1" style="width:142.3pt;">The shape of [ITEM] can be</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.1.6.5.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.6.5.1.1">
<span class="ltx_p" id="A2.T4.1.1.6.5.1.1.1" style="width:142.3pt;">The shape of the [ITEM] is</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.1.7.6.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.7.6.1.1">
<span class="ltx_p" id="A2.T4.1.1.7.6.1.1.1" style="width:142.3pt;">[ITEM] is</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.1.8.7.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.8.7.1.1">
<span class="ltx_p" id="A2.T4.1.1.8.7.1.1.1" style="width:142.3pt;">This [ITEM] is</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T4.1.1.9.8.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.9.8.1.1">
<span class="ltx_p" id="A2.T4.1.1.9.8.1.1.1" style="width:142.3pt;">[ITEM] can be shape</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T4.1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" id="A2.T4.1.1.10.9.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T4.1.1.10.9.1.1">
<span class="ltx_p" id="A2.T4.1.1.10.9.1.1.1" style="width:142.3pt;">[ITEM] has shape</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="A2.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span> The prompts and prediction labels used in object size reasoning.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="A2.T5.1" style="width:432.3pt;height:277.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-24.0pt,15.4pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T5.1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_t" id="A2.T5.1.1.1.1.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.1.1.1.1">
<span class="ltx_p" id="A2.T5.1.1.1.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.1.1.1.1.1">Task</span></span>
</span>
</th>
<th class="ltx_td ltx_align_top ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T5.1.1.1.1.2" style="padding-left:2.8pt;padding-right:2.8pt;"></th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T5.1.1.1.1.3" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.1.1.3.1">
<span class="ltx_p" id="A2.T5.1.1.1.1.3.1.1" style="width:241.8pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.1.3.1.1.1">Prompt</span></span>
</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T5.1.1.1.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.1.1.4.1">Labels</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T5.1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" id="A2.T5.1.1.2.1.1" rowspan="15" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.2.1.1.1">
<span class="ltx_p" id="A2.T5.1.1.2.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A2.T5.1.1.2.1.1.1.1.1">Object Size Reasoning</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T5.1.1.2.1.2" rowspan="9" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.2.1.2.1">
<span class="ltx_p" id="A2.T5.1.1.2.1.2.1.1" style="width:71.1pt;"><span class="ltx_text" id="A2.T5.1.1.2.1.2.1.1.1">Object prediction</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T5.1.1.2.1.3" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.2.1.3.1">
<span class="ltx_p" id="A2.T5.1.1.2.1.3.1.1" style="width:241.8pt;">Which is bigger? [ITEMA] or [ITEMB]</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T5.1.1.2.1.4" rowspan="9" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="A2.T5.1.1.2.1.4.1">{ITEMA, ITEMB}</span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.3.2.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.3.2.1.1">
<span class="ltx_p" id="A2.T5.1.1.3.2.1.1.1" style="width:241.8pt;">Which is smaller? [ITEMA] or [ITEMB]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.4.3.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.4.3.1.1">
<span class="ltx_p" id="A2.T5.1.1.4.3.1.1.1" style="width:241.8pt;">Which is larger? [ITEMA] or [ITEMB]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.5.4.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.5.4.1.1">
<span class="ltx_p" id="A2.T5.1.1.5.4.1.1.1" style="width:241.8pt;">Which is tinier? [ITEMA] or [ITEMB]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.6.5.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.6.5.1.1">
<span class="ltx_p" id="A2.T5.1.1.6.5.1.1.1" style="width:241.8pt;">Which has a bigger size? [ITEMA] or [ITEMB]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.7.6.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.7.6.1.1">
<span class="ltx_p" id="A2.T5.1.1.7.6.1.1.1" style="width:241.8pt;">Which has a bigger size? [ITEMA] or [ITEMB]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.8.7.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.8.7.1.1">
<span class="ltx_p" id="A2.T5.1.1.8.7.1.1.1" style="width:241.8pt;">Which has a smaller size? [ITEMA] or [ITEMB]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.9.8.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.9.8.1.1">
<span class="ltx_p" id="A2.T5.1.1.9.8.1.1.1" style="width:241.8pt;">Which one is larger in size? [ITEMA] or [ITEMB]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.10.9.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.10.9.1.1">
<span class="ltx_p" id="A2.T5.1.1.10.9.1.1.1" style="width:241.8pt;">Which one is smaller in size? [ITEMA] or [ITEMB]</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" id="A2.T5.1.1.11.10.1" rowspan="6" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.11.10.1.1">
<span class="ltx_p" id="A2.T5.1.1.11.10.1.1.1" style="width:71.1pt;"><span class="ltx_text" id="A2.T5.1.1.11.10.1.1.1.1">Size prediction</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.11.10.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.11.10.2.1">
<span class="ltx_p" id="A2.T5.1.1.11.10.2.1.1" style="width:241.8pt;">[ITEMA] is larger or smaller than [ITEMB]?</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_b" id="A2.T5.1.1.11.10.3" rowspan="6" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text" id="A2.T5.1.1.11.10.3.1">{larger, smaller}</span></td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.12.11.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.12.11.1.1">
<span class="ltx_p" id="A2.T5.1.1.12.11.1.1.1" style="width:241.8pt;">[ITEMB] is larger or smaller than [ITEMA]?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.13.12.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.13.12.1.1">
<span class="ltx_p" id="A2.T5.1.1.13.12.1.1.1" style="width:241.8pt;">The size of [ITEMA] is larger or smaller than [ITEMB]?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.14.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.14.13.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.14.13.1.1">
<span class="ltx_p" id="A2.T5.1.1.14.13.1.1.1" style="width:241.8pt;">The size of [ITEMB] is larger or smaller than [ITEMA]?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.15.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T5.1.1.15.14.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.15.14.1.1">
<span class="ltx_p" id="A2.T5.1.1.15.14.1.1.1" style="width:241.8pt;">[ITEMA] has a larger or smaller size than [ITEMB]?</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A2.T5.1.1.16.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" id="A2.T5.1.1.16.15.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T5.1.1.16.15.1.1">
<span class="ltx_p" id="A2.T5.1.1.16.15.1.1.1" style="width:241.8pt;">[ITEMB] has a larger or a smaller size than [ITEMA]?</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="A2.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span> The prompts and prediction labels used in 4 natural language understanding datasets.
The labels for AGNews are {world, sports, business, technology} and the labels for DBPedia are {company, school, artist, athlete, politics, transportation, building, nature, village, animal, plant, album, film, book}.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T6.1" style="width:399.2pt;height:81.9pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.2pt,4.5pt) scale(0.9,0.9) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T6.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A2.T6.1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_t" id="A2.T6.1.1.1.1.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.1.1.1.1">
<span class="ltx_p" id="A2.T6.1.1.1.1.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.1.1.1.1.1">Task</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T6.1.1.1.1.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.1.1.2.1">
<span class="ltx_p" id="A2.T6.1.1.1.1.2.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.1.2.1.1.1">Dataset</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T6.1.1.1.1.3" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.1.1.3.1">
<span class="ltx_p" id="A2.T6.1.1.1.1.3.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.1.3.1.1.1">Prompt</span></span>
</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt ltx_border_t" id="A2.T6.1.1.1.1.4" style="padding-left:2.8pt;padding-right:2.8pt;"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.1.1.4.1">Labels</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A2.T6.1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b ltx_border_r ltx_border_t" id="A2.T6.1.1.2.1.1" rowspan="4" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.2.1.1.1">
<span class="ltx_p" id="A2.T6.1.1.2.1.1.1.1" style="width:71.1pt;"><span class="ltx_text ltx_font_bold" id="A2.T6.1.1.2.1.1.1.1.1">Natural Language Understanding</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T6.1.1.2.1.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.2.1.2.1">
<span class="ltx_p" id="A2.T6.1.1.2.1.2.1.1" style="width:56.9pt;">SST-2</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A2.T6.1.1.2.1.3" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.2.1.3.1">
<span class="ltx_p" id="A2.T6.1.1.2.1.3.1.1" style="width:170.7pt;">Review: [Sentence] Sentiment: [Label]</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="A2.T6.1.1.2.1.4" style="padding-left:2.8pt;padding-right:2.8pt;">{Positive, Negative}</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T6.1.1.3.2.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.3.2.1.1">
<span class="ltx_p" id="A2.T6.1.1.3.2.1.1.1" style="width:56.9pt;">MPQA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T6.1.1.3.2.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.3.2.2.1">
<span class="ltx_p" id="A2.T6.1.1.3.2.2.1.1" style="width:170.7pt;">Review: [Sentence] Sentiment: [Label]</span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="A2.T6.1.1.3.2.3" style="padding-left:2.8pt;padding-right:2.8pt;">{Positive, Negative}</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T6.1.1.4.3.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.4.3.1.1">
<span class="ltx_p" id="A2.T6.1.1.4.3.1.1.1" style="width:56.9pt;">AGNews</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A2.T6.1.1.4.3.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.4.3.2.1">
<span class="ltx_p" id="A2.T6.1.1.4.3.2.1.1" style="width:170.7pt;">input: [Sentence] type: [Label]</span>
</span>
</td>
<td class="ltx_td ltx_align_left" id="A2.T6.1.1.4.3.3" style="padding-left:2.8pt;padding-right:2.8pt;">{world,…, technology}</td>
</tr>
<tr class="ltx_tr" id="A2.T6.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" id="A2.T6.1.1.5.4.1" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.5.4.1.1">
<span class="ltx_p" id="A2.T6.1.1.5.4.1.1.1" style="width:56.9pt;">DBPedia</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb ltx_border_b" id="A2.T6.1.1.5.4.2" style="padding-left:2.8pt;padding-right:2.8pt;">
<span class="ltx_inline-block ltx_align_top" id="A2.T6.1.1.5.4.2.1">
<span class="ltx_p" id="A2.T6.1.1.5.4.2.1.1" style="width:170.7pt;">input: [Sentence] type: [Label]</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_b" id="A2.T6.1.1.5.4.3" style="padding-left:2.8pt;padding-right:2.8pt;">{company, school,…, book}</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 17 12:57:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
