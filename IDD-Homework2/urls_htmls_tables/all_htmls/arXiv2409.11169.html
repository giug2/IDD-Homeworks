<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>MAISI: Medical AI for Synthetic Imaging</title>
<!--Generated on Fri Sep 13 18:09:01 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.11169v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S1" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S2" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S3" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S3.SS1" title="In 3 Methodology ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Volume Compression Network</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S3.SS2" title="In 3 Methodology ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Diffusion Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S3.SS3" title="In 3 Methodology ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Additional Conditioning Mechanisms</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.SS1" title="In 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets and Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.SS2" title="In 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Evaluation of MAISI VAE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.SS3" title="In 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Evaluation of MAISI Diffusion Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.SS4" title="In 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Data Augmentation in Downstream Tasks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S5" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion and Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S6" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Dataset Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.SS1" title="In Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>MAISI VAE</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.SS2" title="In Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>MAISI Diffusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.SS3" title="In Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>MAISI ControlNet</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A2" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Additional Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A3" title="In MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Supplementary Experiment Results</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">MAISI: Medical AI for Synthetic Imaging</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Pengfei Guo 
<br class="ltx_break"/>NVIDIA
</span><span class="ltx_author_notes">Equal contribution. The code is available at <a class="ltx_ref ltx_href" href="https://github.com/Project-MONAI/tutorials/tree/main/generation/maisi" title="">MONAI Tutorial</a>. The online demo is available at <a class="ltx_ref ltx_href" href="https://build.nvidia.com/nvidia/maisi" title="">NVIDIA NIM</a>.</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Can Zhao<sup class="ltx_sup" id="id3.1.id1">*</sup>
<br class="ltx_break"/>NVIDIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dong Yang<sup class="ltx_sup" id="id4.1.id1">*</sup>
<br class="ltx_break"/>NVIDIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ziyue Xu
<br class="ltx_break"/>NVIDIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vishwesh Nath
<br class="ltx_break"/>NVIDIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yucheng Tang 
<br class="ltx_break"/>NVIDIA
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Benjamin Simon 
<br class="ltx_break"/>National Institutes of Health 
<br class="ltx_break"/>University of Oxford
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mason Belue 
<br class="ltx_break"/>University of Arkansas for Medical Sciences
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Stephanie Harmon 
<br class="ltx_break"/>National Institutes of Health
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Baris Turkbey 
<br class="ltx_break"/>National Institutes of Health
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Daguang Xu 
<br class="ltx_break"/>NVIDIA
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.2">Medical imaging analysis faces challenges such as data scarcity, high annotation costs, and privacy concerns. This paper introduces the Medical AI for Synthetic Imaging (MAISI), an innovative approach using the diffusion model to generate synthetic 3D computed tomography (CT) images to address those challenges. MAISI leverages the foundation volume compression network and the latent diffusion model to produce high-resolution CT images (up to a landmark volume dimension of 512 <math alttext="\times" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1a"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1b"><times id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1d">×</annotation></semantics></math> 512 <math alttext="\times" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1a"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1b"><times id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1d">×</annotation></semantics></math> 768 ) with flexible volume dimensions and voxel spacing. By incorporating ControlNet, MAISI can process organ segmentation, including 127 anatomical structures, as additional conditions and enables the generation of accurately annotated synthetic images that can be used for various downstream tasks. Our experiment results show that MAISI’s capabilities in generating realistic, anatomically accurate images for diverse regions and conditions reveal its promising potential to mitigate challenges using synthetic data.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="461" id="S1.F1.g1" src="x1.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.12.6.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.10.5" style="font-size:90%;">(a) A generated high-resolution CT volume (with volume dimensions of 512 <math alttext="\times" class="ltx_Math" display="inline" id="S1.F1.6.1.m1.1"><semantics id="S1.F1.6.1.m1.1b"><mo id="S1.F1.6.1.m1.1.1" xref="S1.F1.6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.F1.6.1.m1.1c"><times id="S1.F1.6.1.m1.1.1.cmml" xref="S1.F1.6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.6.1.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S1.F1.6.1.m1.1e">×</annotation></semantics></math> 512 <math alttext="\times" class="ltx_Math" display="inline" id="S1.F1.7.2.m2.1"><semantics id="S1.F1.7.2.m2.1b"><mo id="S1.F1.7.2.m2.1.1" xref="S1.F1.7.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.F1.7.2.m2.1c"><times id="S1.F1.7.2.m2.1.1.cmml" xref="S1.F1.7.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.7.2.m2.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S1.F1.7.2.m2.1e">×</annotation></semantics></math> 768 and voxel spacing of 0.86 <math alttext="\times" class="ltx_Math" display="inline" id="S1.F1.8.3.m3.1"><semantics id="S1.F1.8.3.m3.1b"><mo id="S1.F1.8.3.m3.1.1" xref="S1.F1.8.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.F1.8.3.m3.1c"><times id="S1.F1.8.3.m3.1.1.cmml" xref="S1.F1.8.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.8.3.m3.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S1.F1.8.3.m3.1e">×</annotation></semantics></math> 0.86 <math alttext="\times" class="ltx_Math" display="inline" id="S1.F1.9.4.m4.1"><semantics id="S1.F1.9.4.m4.1b"><mo id="S1.F1.9.4.m4.1.1" xref="S1.F1.9.4.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S1.F1.9.4.m4.1c"><times id="S1.F1.9.4.m4.1.1.cmml" xref="S1.F1.9.4.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.9.4.m4.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S1.F1.9.4.m4.1e">×</annotation></semantics></math> 0.92 <math alttext="\text{mm}^{3}" class="ltx_Math" display="inline" id="S1.F1.10.5.m5.1"><semantics id="S1.F1.10.5.m5.1b"><msup id="S1.F1.10.5.m5.1.1" xref="S1.F1.10.5.m5.1.1.cmml"><mtext id="S1.F1.10.5.m5.1.1.2" xref="S1.F1.10.5.m5.1.1.2a.cmml">mm</mtext><mn id="S1.F1.10.5.m5.1.1.3" xref="S1.F1.10.5.m5.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S1.F1.10.5.m5.1c"><apply id="S1.F1.10.5.m5.1.1.cmml" xref="S1.F1.10.5.m5.1.1"><csymbol cd="ambiguous" id="S1.F1.10.5.m5.1.1.1.cmml" xref="S1.F1.10.5.m5.1.1">superscript</csymbol><ci id="S1.F1.10.5.m5.1.1.2a.cmml" xref="S1.F1.10.5.m5.1.1.2"><mtext id="S1.F1.10.5.m5.1.1.2.cmml" xref="S1.F1.10.5.m5.1.1.2">mm</mtext></ci><cn id="S1.F1.10.5.m5.1.1.3.cmml" type="integer" xref="S1.F1.10.5.m5.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.10.5.m5.1d">\text{mm}^{3}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.10.5.m5.1e">mm start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math>) by the proposed method and its corresponding segmentation condition overlaid on generated volume. We show the axial, sagittal, and coronal views from top to bottom, respectively. (b) 3D volume rendering of generated CT by MAISI. The rendering setting is tuned to highlight bone structures and demonstrate the realism of the generated CT volume.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Medical imaging analysis has been integral to modern healthcare, providing critical insights into patient diagnosis, treatment planning, and monitoring. The rapid advancement of machine learning (ML) approaches has revolutionized diagnostic and therapeutic practices in modern healthcare. However, the development of effective ML models in this domain continues to face the following significant challenges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib40" title="">40</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib77" title="">77</a>]</cite>: (1) <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">data scarcity</span>: the rarity of certain medical conditions (<em class="ltx_emph ltx_font_italic" id="S1.p1.1.2">e.g</em>.<span class="ltx_text" id="S1.p1.1.3"></span>, certain types of cancer, and rare diseases) complicates the data acquisition process, which leads to the limited acquired data that might not adequately represent the diversity of real-world cases. (2) <span class="ltx_text ltx_font_bold" id="S1.p1.1.4">high human-annotation costs</span>: annotating medical images, such as MRI and CT scans, is inherently more expertise-demanding than annotating objects in general images. Medical images often contain subtle features that are critical for accurate diagnosis and treatment. Expert knowledge is usually required to accurately identify and annotate these conditions.
(3) <span class="ltx_text ltx_font_bold" id="S1.p1.1.5">privacy concerns</span>: conventional data acquisition and processing of medical images often require access to large volumes of patient data, which raises ethical concerns and poses significant logistical challenges due to the sensitive nature of patient information.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To address these limitations, generating synthetic data has emerged as a promising direction. By creating artificial yet realistic medical images, synthetic data can augment existing datasets, reduce the dependency on real patient data, and provide a cost-effective alternative to manual data annotation. With the recent advancement of the generative model, many novel approaches, such as generative adversarial networks (GAN) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib21" title="">21</a>]</cite> and Diffusion Models (DM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib29" title="">29</a>]</cite>, have been extensively studied for their capacity to generate photo-realistic images in various tasks in general computer vision society. In the context of medical image generation, several generative models have been successfully
applied for medical image synthesis, such as multi-contrast MR/CT image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib61" title="">61</a>]</cite>, cross-modality image translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib65" title="">65</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib15" title="">15</a>]</cite>, and image reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib64" title="">64</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib76" title="">76</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib14" title="">14</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, several key challenges are not fully explored in previous studies. <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">First</span>, realistic high-resolution (larger than the volume dimension of 512<sup class="ltx_sup" id="S1.p3.1.2">3</sup>) 3D volume generation is still a challenging task due to the huge memory consumption imposed by unified 3D frameworks, which must handle the vast amount of data involved in such high-dimensional representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib58" title="">58</a>]</cite>. Overcoming this memory bottleneck is essential for advancing the realism and applicability of 3D volume generation in clinical contexts. <span class="ltx_text ltx_font_bold" id="S1.p3.1.3">Second</span>, the constraint of fixed output volume dimensions and voxel spacing poses substantial limitations in real-world applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib11" title="">11</a>]</cite>. These parameter presets are often incompatible with the diverse requirements of different tasks, such as the analysis of varying anatomical structures. The ability to dynamically adjust both the volume dimensions and the voxel spacing is crucial for enhancing the flexibility and utility of 3D generative models.
<span class="ltx_text ltx_font_bold" id="S1.p3.1.4">Third</span>, another common limitation of current generative models for medical image generation is their specialization to dedicated datasets or particular types of organs. These models, once trained, are typically not generalizable beyond the specific data and target organ they are developed on, which restricts their broader application in diverse settings. Developing more versatile models that can adapt to multiple datasets and organ types and mitigate the need for extensive retraining is a key objective for advancing the field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib10" title="">10</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we propose a method, namely Medical AI for Synthetic Imaging (MAISI), a new framework for high-resolution 3D CT volume generation, which consists of three 3D networks including two foundation models (<em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">i.e</em>.<span class="ltx_text" id="S1.p4.1.2"></span>, a volume compression network, a latent diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a>]</cite>) and a ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite> for versatile generation tasks. Volume Compression Network is trained on a large amount of data (<em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">i.e</em>.<span class="ltx_text" id="S1.p4.1.4"></span>, 39,206 3D CT volumes) and is responsible for compressing the 3D medical images into latent space and mapping the generated latent features back to image space by a visual encoder and a visual decoder, respectively. To reduce the memory footprint, we introduce the tensor splitting parallelism (TSP) inspiring from the tensor parallelism technique <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib57" title="">57</a>]</cite>, originally proposed for linear layers, to the 3D convolutional layers allowing for the encoding and decoding of high-resolution CT volumes in a unified 3D network.
The latent diffusion model in MAISI facilitates the creation of realistic latent features of 3D medical images. Benefiting from a compressed latent space with flexible dimensions and taking body region and voxel spacing as conditions, it enables the generation of complex anatomical structures with a high degree of fidelity while maintaining relatively low memory consumption. The latent diffusion model is trained on 10,277 CT volumes from diverse datasets, encompassing various body regions and disease conditions to enhance its generalizability and robustness, which enables the model to capture the knowledge represented in a wide range of clinical scenarios. Further, the integration of ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite> into the MAISI framework introduces a mechanism for dynamic control over the generated outputs. This component enhances MAISI’s versatility and applicability across a wider range of tasks (<em class="ltx_emph ltx_font_italic" id="S1.p4.1.5">e.g</em>.<span class="ltx_text" id="S1.p4.1.6"></span>, conditional generation based on segmentation masks, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">1</span></a>, image inpainting, <em class="ltx_emph ltx_font_italic" id="S1.p4.1.7">etc</em>.). Additionally, this capability minimizes the need for extensive retraining of the two underlying foundation models when transitioning between different tasks or clinical objectives, thereby conserving both time and computational resources.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To summarize, this paper makes the following contributions:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A novel framework, MAISI, for high-resolution 3D CT volume generation is proposed, which enables the versatile generation of synthetic CT images.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Tensor splitting parallelism (TSP) is introduced to 3D convolutional networks. To the best of our knowledge, MAISI is the first attempt to generate realistic 3D CT images larger than 512<sup class="ltx_sup" id="S1.I1.i2.p1.1.1">3</sup> voxels.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">MAISI provides dynamic control over outputs, enabling annotated synthetic images to improve downstream task performance.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="531" id="S1.F2.g1" src="x2.png" width="951"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;">The overview of three development stages of MAISI.</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Medical image synthesis has become an increasingly prominent research area, particularly in response to the challenges discussed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S1" title="1 Introduction ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">1</span></a>. Early approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib7" title="">7</a>]</cite> to medical image synthesis were predominantly based on traditional image processing techniques, such as the example-based approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib52" title="">52</a>]</cite> and geometry-regularized dictionary learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib31" title="">31</a>]</cite>, which, while effective to some extent, are limited in their ability to generate realistic and diverse medical images. The advent of machine learning, particularly deep learning, has significantly advanced the field, enabling more sophisticated and accurate models for image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib59" title="">59</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.2"><span class="ltx_text ltx_font_bold" id="S2.p2.2.1">GAN in medical image synthesis.</span> GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib21" title="">21</a>]</cite>, one of generative models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib45" title="">45</a>]</cite>, has been widely adopted for various tasks, such as MRI/CT image synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib61" title="">61</a>]</cite>, cross-modality image translation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib56" title="">56</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib65" title="">65</a>]</cite>, image reconstruction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib64" title="">64</a>]</cite> and super-resolution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib67" title="">67</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib1" title="">1</a>]</cite>, in medical imaging synthesis due to its promising ability to generate high-quality images. One of the most critical applications of GAN in medical imaging is data augmentation by generating annotated images. Several studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib74" title="">74</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib32" title="">32</a>]</cite>, have employed GAN to generate lesion images to augment training data for improving downstream tasks to overcome data scarcity issues.
However, those methods focus on 2D medical imaging or small volumetric patch synthesis, which is fundamentally limited due to neglecting the inherent complexity and the 3D nature of medical data. In this work, we focus on generating full CT volume in realistic dimensions (up to 512 <math alttext="\times" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mo id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><times id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">×</annotation></semantics></math> 512 <math alttext="\times" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mo id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><times id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">×</annotation></semantics></math> 768) to model complex volumetric features in a unified framework.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p3">
<p class="ltx_p" id="S2.p3.1"><span class="ltx_text ltx_font_bold" id="S2.p3.1.1">DM in medical image synthesis.</span> Diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a>]</cite> have recently emerged as a powerful generative model that has shown great potential in medical imaging synthesis due to its capabilities in high-quality image synthesis, stable training process, and flexibility in conditioning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib66" title="">66</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib5" title="">5</a>]</cite>.  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib55" title="">55</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib35" title="">35</a>]</cite> demonstrate the effectiveness of DM-based methods in generating high-quality 2D medical images that capture intricate details with minimal artifacts, making them suitable for clinical use. GenerateCT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib23" title="">23</a>]</cite> is designed to synthesize 3D CT volumes from free-form medical text prompts and accomplishes arbitrary-size CT volume generation by decomposing the process into a sequential generation of individual slices using DM. However, due to the nature of 2D approaches, the issue of 3D structural inconsistencies across slices is noticeable and problematic in the generated images. Application-wise, many recent studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib73" title="">73</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib72" title="">72</a>]</cite> are focusing on tumor synthesizing and improving models’ performance in downstream tasks. DiffTumor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib10" title="">10</a>]</cite> seeks to enhance the robustness and generalizability of tumor segmentation models across various organs, such as the liver, pancreas, and kidney, by leveraging high-quality synthetic tumors generated through specialized diffusion models. In this work, we focus on achieving conditional generation tailored to versatile tasks by leveraging robust foundation models, which significantly minimizes the need for extensive retraining across different applications, thereby conserving both time and computational resources while maintaining adaptability and efficiency in diverse clinical scenarios.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">2</span></a>, the development of MAISI
involves three stages. In the first stage, the volume compression network (<em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">i.e</em>.<span class="ltx_text" id="S3.p1.1.2"></span>, VAE-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a>]</cite>) is trained on a substantial dataset comprising 39,206 3D CT volumes and 18,827 3D MRI volumes. This network effectively compresses high-resolution 3D medical images into a latent space that is perceptually equivalent to the image space, reducing memory usage and computational complexity for later stages. In the second stage, a latent diffusion model is trained on 10,277 CT volumes sourced from diverse datasets. This model operates within the compressed latent space, conditioned on specific body regions and voxel spacing, to generate features of realistic and complex 3D anatomical structures in flexible dimensions. Training on a broad range of data enhances the model’s generalizability and adaptability in different tasks. The final stage involves the integration of ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite> into the MAISI framework. This component allows for dynamic control over the generated outputs by injecting additional conditions into the trained latent DM in the second stage, potentially supporting a wide range of tasks. The integration reduces the need for extensive retraining when the model is adapted to different tasks. In what follows, we provide detailed descriptions of each key component of the MAISI framework.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Volume Compression Network</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.4">The volume compression model builds upon previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib19" title="">19</a>]</cite> and employs a Variational Autoencoder (VAE) trained on combined objectives, which integrates perceptual loss <math alttext="\mathcal{L}_{\text{lpips}}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">ℒ</mi><mtext id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3a.cmml">lpips</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ℒ</ci><ci id="S3.SS1.p1.1.m1.1.1.3a.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><mtext id="S3.SS1.p1.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.1.m1.1.1.3">lpips</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathcal{L}_{\text{lpips}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT lpips end_POSTSUBSCRIPT</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib71" title="">71</a>]</cite>, adversarial loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib68" title="">68</a>]</cite> <math alttext="\mathcal{L}_{\text{adv}}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">ℒ</mi><mtext id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3a.cmml">adv</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">ℒ</ci><ci id="S3.SS1.p1.2.m2.1.1.3a.cmml" xref="S3.SS1.p1.2.m2.1.1.3"><mtext id="S3.SS1.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.2.m2.1.1.3">adv</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathcal{L}_{\text{adv}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT</annotation></semantics></math>, and L1 reconstruction loss <math alttext="\mathcal{L}_{\text{recon}}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msub id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">ℒ</mi><mtext id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3a.cmml">recon</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">ℒ</ci><ci id="S3.SS1.p1.3.m3.1.1.3a.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><mtext id="S3.SS1.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.3.m3.1.1.3">recon</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\mathcal{L}_{\text{recon}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT recon end_POSTSUBSCRIPT</annotation></semantics></math> on voxel-space. These combined objectives ensure that the volume reconstructions adhere closely to the image manifold and enforce local realism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a>]</cite>. In addition, we follow <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib48" title="">48</a>]</cite> adding Kullback-Leibler (KL) regularization <math alttext="\mathcal{L}_{\text{reg}}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><msub id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml">ℒ</mi><mtext id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3a.cmml">reg</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2">ℒ</ci><ci id="S3.SS1.p1.4.m4.1.1.3a.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><mtext id="S3.SS1.p1.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p1.4.m4.1.1.3">reg</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathcal{L}_{\text{reg}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT</annotation></semantics></math> toward a standard normal on the learned latent features for avoiding high-variance latent spaces.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.13">Given a CT volume <math alttext="x\in\mathbb{R}^{H\times W\times D}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><mrow id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">x</mi><mo id="S3.SS1.p2.1.m1.1.1.1" xref="S3.SS1.p2.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.2" xref="S3.SS1.p2.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.1.m1.1.1.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3.3.2" xref="S3.SS1.p2.1.m1.1.1.3.3.2.cmml">H</mi><mo id="S3.SS1.p2.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.1.m1.1.1.3.3.3" xref="S3.SS1.p2.1.m1.1.1.3.3.3.cmml">W</mi><mo id="S3.SS1.p2.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS1.p2.1.m1.1.1.3.3.4" xref="S3.SS1.p2.1.m1.1.1.3.3.4.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><in id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1.1"></in><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑥</ci><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS1.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3"><times id="S3.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.2">𝐻</ci><ci id="S3.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.3">𝑊</ci><ci id="S3.SS1.p2.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p2.1.m1.1.1.3.3.4">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">x\in\mathbb{R}^{H\times W\times D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_x ∈ blackboard_R start_POSTSUPERSCRIPT italic_H × italic_W × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> in grayscale voxel
space, where <math alttext="H" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">H</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝐻</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">H</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_H</annotation></semantics></math> denotes the height, <math alttext="W" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">W</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">W</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">italic_W</annotation></semantics></math> the width, and <math alttext="D" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><ci id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_D</annotation></semantics></math> the
depth, the encoder <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><ci id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">caligraphic_E</annotation></semantics></math> of AE downsamples <math alttext="x" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><mi id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><ci id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">italic_x</annotation></semantics></math> and generates the
latent representation <math alttext="z=\mathcal{E}(x)\in\mathbb{R}^{h\times w\times d}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.1"><semantics id="S3.SS1.p2.7.m7.1a"><mrow id="S3.SS1.p2.7.m7.1.2" xref="S3.SS1.p2.7.m7.1.2.cmml"><mi id="S3.SS1.p2.7.m7.1.2.2" xref="S3.SS1.p2.7.m7.1.2.2.cmml">z</mi><mo id="S3.SS1.p2.7.m7.1.2.3" xref="S3.SS1.p2.7.m7.1.2.3.cmml">=</mo><mrow id="S3.SS1.p2.7.m7.1.2.4" xref="S3.SS1.p2.7.m7.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.7.m7.1.2.4.2" xref="S3.SS1.p2.7.m7.1.2.4.2.cmml">ℰ</mi><mo id="S3.SS1.p2.7.m7.1.2.4.1" xref="S3.SS1.p2.7.m7.1.2.4.1.cmml">⁢</mo><mrow id="S3.SS1.p2.7.m7.1.2.4.3.2" xref="S3.SS1.p2.7.m7.1.2.4.cmml"><mo id="S3.SS1.p2.7.m7.1.2.4.3.2.1" stretchy="false" xref="S3.SS1.p2.7.m7.1.2.4.cmml">(</mo><mi id="S3.SS1.p2.7.m7.1.1" xref="S3.SS1.p2.7.m7.1.1.cmml">x</mi><mo id="S3.SS1.p2.7.m7.1.2.4.3.2.2" stretchy="false" xref="S3.SS1.p2.7.m7.1.2.4.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.7.m7.1.2.5" xref="S3.SS1.p2.7.m7.1.2.5.cmml">∈</mo><msup id="S3.SS1.p2.7.m7.1.2.6" xref="S3.SS1.p2.7.m7.1.2.6.cmml"><mi id="S3.SS1.p2.7.m7.1.2.6.2" xref="S3.SS1.p2.7.m7.1.2.6.2.cmml">ℝ</mi><mrow id="S3.SS1.p2.7.m7.1.2.6.3" xref="S3.SS1.p2.7.m7.1.2.6.3.cmml"><mi id="S3.SS1.p2.7.m7.1.2.6.3.2" xref="S3.SS1.p2.7.m7.1.2.6.3.2.cmml">h</mi><mo id="S3.SS1.p2.7.m7.1.2.6.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.7.m7.1.2.6.3.1.cmml">×</mo><mi id="S3.SS1.p2.7.m7.1.2.6.3.3" xref="S3.SS1.p2.7.m7.1.2.6.3.3.cmml">w</mi><mo id="S3.SS1.p2.7.m7.1.2.6.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS1.p2.7.m7.1.2.6.3.1.cmml">×</mo><mi id="S3.SS1.p2.7.m7.1.2.6.3.4" xref="S3.SS1.p2.7.m7.1.2.6.3.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.1b"><apply id="S3.SS1.p2.7.m7.1.2.cmml" xref="S3.SS1.p2.7.m7.1.2"><and id="S3.SS1.p2.7.m7.1.2a.cmml" xref="S3.SS1.p2.7.m7.1.2"></and><apply id="S3.SS1.p2.7.m7.1.2b.cmml" xref="S3.SS1.p2.7.m7.1.2"><eq id="S3.SS1.p2.7.m7.1.2.3.cmml" xref="S3.SS1.p2.7.m7.1.2.3"></eq><ci id="S3.SS1.p2.7.m7.1.2.2.cmml" xref="S3.SS1.p2.7.m7.1.2.2">𝑧</ci><apply id="S3.SS1.p2.7.m7.1.2.4.cmml" xref="S3.SS1.p2.7.m7.1.2.4"><times id="S3.SS1.p2.7.m7.1.2.4.1.cmml" xref="S3.SS1.p2.7.m7.1.2.4.1"></times><ci id="S3.SS1.p2.7.m7.1.2.4.2.cmml" xref="S3.SS1.p2.7.m7.1.2.4.2">ℰ</ci><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">𝑥</ci></apply></apply><apply id="S3.SS1.p2.7.m7.1.2c.cmml" xref="S3.SS1.p2.7.m7.1.2"><in id="S3.SS1.p2.7.m7.1.2.5.cmml" xref="S3.SS1.p2.7.m7.1.2.5"></in><share href="https://arxiv.org/html/2409.11169v1#S3.SS1.p2.7.m7.1.2.4.cmml" id="S3.SS1.p2.7.m7.1.2d.cmml" xref="S3.SS1.p2.7.m7.1.2"></share><apply id="S3.SS1.p2.7.m7.1.2.6.cmml" xref="S3.SS1.p2.7.m7.1.2.6"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.1.2.6.1.cmml" xref="S3.SS1.p2.7.m7.1.2.6">superscript</csymbol><ci id="S3.SS1.p2.7.m7.1.2.6.2.cmml" xref="S3.SS1.p2.7.m7.1.2.6.2">ℝ</ci><apply id="S3.SS1.p2.7.m7.1.2.6.3.cmml" xref="S3.SS1.p2.7.m7.1.2.6.3"><times id="S3.SS1.p2.7.m7.1.2.6.3.1.cmml" xref="S3.SS1.p2.7.m7.1.2.6.3.1"></times><ci id="S3.SS1.p2.7.m7.1.2.6.3.2.cmml" xref="S3.SS1.p2.7.m7.1.2.6.3.2">ℎ</ci><ci id="S3.SS1.p2.7.m7.1.2.6.3.3.cmml" xref="S3.SS1.p2.7.m7.1.2.6.3.3">𝑤</ci><ci id="S3.SS1.p2.7.m7.1.2.6.3.4.cmml" xref="S3.SS1.p2.7.m7.1.2.6.3.4">𝑑</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.1c">z=\mathcal{E}(x)\in\mathbb{R}^{h\times w\times d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.1d">italic_z = caligraphic_E ( italic_x ) ∈ blackboard_R start_POSTSUPERSCRIPT italic_h × italic_w × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> with much smaller spatial dimensions. The decoder <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.1"><semantics id="S3.SS1.p2.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><ci id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.1d">caligraphic_D</annotation></semantics></math> of AE approximates the reconstructed volume <math alttext="\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m9.3"><semantics id="S3.SS1.p2.9.m9.3a"><mrow id="S3.SS1.p2.9.m9.3.3" xref="S3.SS1.p2.9.m9.3.3.cmml"><mover accent="true" id="S3.SS1.p2.9.m9.3.3.3" xref="S3.SS1.p2.9.m9.3.3.3.cmml"><mi id="S3.SS1.p2.9.m9.3.3.3.2" xref="S3.SS1.p2.9.m9.3.3.3.2.cmml">x</mi><mo id="S3.SS1.p2.9.m9.3.3.3.1" xref="S3.SS1.p2.9.m9.3.3.3.1.cmml">~</mo></mover><mo id="S3.SS1.p2.9.m9.3.3.4" xref="S3.SS1.p2.9.m9.3.3.4.cmml">=</mo><mrow id="S3.SS1.p2.9.m9.3.3.5" xref="S3.SS1.p2.9.m9.3.3.5.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.9.m9.3.3.5.2" xref="S3.SS1.p2.9.m9.3.3.5.2.cmml">𝒟</mi><mo id="S3.SS1.p2.9.m9.3.3.5.1" xref="S3.SS1.p2.9.m9.3.3.5.1.cmml">⁢</mo><mrow id="S3.SS1.p2.9.m9.3.3.5.3.2" xref="S3.SS1.p2.9.m9.3.3.5.cmml"><mo id="S3.SS1.p2.9.m9.3.3.5.3.2.1" stretchy="false" xref="S3.SS1.p2.9.m9.3.3.5.cmml">(</mo><mi id="S3.SS1.p2.9.m9.1.1" xref="S3.SS1.p2.9.m9.1.1.cmml">z</mi><mo id="S3.SS1.p2.9.m9.3.3.5.3.2.2" stretchy="false" xref="S3.SS1.p2.9.m9.3.3.5.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.9.m9.3.3.6" xref="S3.SS1.p2.9.m9.3.3.6.cmml">=</mo><mrow id="S3.SS1.p2.9.m9.3.3.1" xref="S3.SS1.p2.9.m9.3.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.9.m9.3.3.1.3" xref="S3.SS1.p2.9.m9.3.3.1.3.cmml">𝒟</mi><mo id="S3.SS1.p2.9.m9.3.3.1.2" xref="S3.SS1.p2.9.m9.3.3.1.2.cmml">⁢</mo><mrow id="S3.SS1.p2.9.m9.3.3.1.1.1" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.cmml"><mo id="S3.SS1.p2.9.m9.3.3.1.1.1.2" stretchy="false" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p2.9.m9.3.3.1.1.1.1" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.9.m9.3.3.1.1.1.1.2" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.2.cmml">ℰ</mi><mo id="S3.SS1.p2.9.m9.3.3.1.1.1.1.1" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS1.p2.9.m9.3.3.1.1.1.1.3.2" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.cmml"><mo id="S3.SS1.p2.9.m9.3.3.1.1.1.1.3.2.1" stretchy="false" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.cmml">(</mo><mi id="S3.SS1.p2.9.m9.2.2" xref="S3.SS1.p2.9.m9.2.2.cmml">x</mi><mo id="S3.SS1.p2.9.m9.3.3.1.1.1.1.3.2.2" stretchy="false" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.9.m9.3.3.1.1.1.3" stretchy="false" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.3b"><apply id="S3.SS1.p2.9.m9.3.3.cmml" xref="S3.SS1.p2.9.m9.3.3"><and id="S3.SS1.p2.9.m9.3.3a.cmml" xref="S3.SS1.p2.9.m9.3.3"></and><apply id="S3.SS1.p2.9.m9.3.3b.cmml" xref="S3.SS1.p2.9.m9.3.3"><eq id="S3.SS1.p2.9.m9.3.3.4.cmml" xref="S3.SS1.p2.9.m9.3.3.4"></eq><apply id="S3.SS1.p2.9.m9.3.3.3.cmml" xref="S3.SS1.p2.9.m9.3.3.3"><ci id="S3.SS1.p2.9.m9.3.3.3.1.cmml" xref="S3.SS1.p2.9.m9.3.3.3.1">~</ci><ci id="S3.SS1.p2.9.m9.3.3.3.2.cmml" xref="S3.SS1.p2.9.m9.3.3.3.2">𝑥</ci></apply><apply id="S3.SS1.p2.9.m9.3.3.5.cmml" xref="S3.SS1.p2.9.m9.3.3.5"><times id="S3.SS1.p2.9.m9.3.3.5.1.cmml" xref="S3.SS1.p2.9.m9.3.3.5.1"></times><ci id="S3.SS1.p2.9.m9.3.3.5.2.cmml" xref="S3.SS1.p2.9.m9.3.3.5.2">𝒟</ci><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">𝑧</ci></apply></apply><apply id="S3.SS1.p2.9.m9.3.3c.cmml" xref="S3.SS1.p2.9.m9.3.3"><eq id="S3.SS1.p2.9.m9.3.3.6.cmml" xref="S3.SS1.p2.9.m9.3.3.6"></eq><share href="https://arxiv.org/html/2409.11169v1#S3.SS1.p2.9.m9.3.3.5.cmml" id="S3.SS1.p2.9.m9.3.3d.cmml" xref="S3.SS1.p2.9.m9.3.3"></share><apply id="S3.SS1.p2.9.m9.3.3.1.cmml" xref="S3.SS1.p2.9.m9.3.3.1"><times id="S3.SS1.p2.9.m9.3.3.1.2.cmml" xref="S3.SS1.p2.9.m9.3.3.1.2"></times><ci id="S3.SS1.p2.9.m9.3.3.1.3.cmml" xref="S3.SS1.p2.9.m9.3.3.1.3">𝒟</ci><apply id="S3.SS1.p2.9.m9.3.3.1.1.1.1.cmml" xref="S3.SS1.p2.9.m9.3.3.1.1.1"><times id="S3.SS1.p2.9.m9.3.3.1.1.1.1.1.cmml" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.1"></times><ci id="S3.SS1.p2.9.m9.3.3.1.1.1.1.2.cmml" xref="S3.SS1.p2.9.m9.3.3.1.1.1.1.2">ℰ</ci><ci id="S3.SS1.p2.9.m9.2.2.cmml" xref="S3.SS1.p2.9.m9.2.2">𝑥</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.3c">\tilde{x}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(x))</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m9.3d">over~ start_ARG italic_x end_ARG = caligraphic_D ( italic_z ) = caligraphic_D ( caligraphic_E ( italic_x ) )</annotation></semantics></math> from the latent features. A 3D discriminator, denoted as
<math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m10.1"><semantics id="S3.SS1.p2.10.m10.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><ci id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m10.1d">caligraphic_C</annotation></semantics></math>, is utilized to identify and penalize any unrealistic artifacts in the reconstructed volume <math alttext="\tilde{x}" class="ltx_Math" display="inline" id="S3.SS1.p2.11.m11.1"><semantics id="S3.SS1.p2.11.m11.1a"><mover accent="true" id="S3.SS1.p2.11.m11.1.1" xref="S3.SS1.p2.11.m11.1.1.cmml"><mi id="S3.SS1.p2.11.m11.1.1.2" xref="S3.SS1.p2.11.m11.1.1.2.cmml">x</mi><mo id="S3.SS1.p2.11.m11.1.1.1" xref="S3.SS1.p2.11.m11.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.11.m11.1b"><apply id="S3.SS1.p2.11.m11.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1"><ci id="S3.SS1.p2.11.m11.1.1.1.cmml" xref="S3.SS1.p2.11.m11.1.1.1">~</ci><ci id="S3.SS1.p2.11.m11.1.1.2.cmml" xref="S3.SS1.p2.11.m11.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.11.m11.1c">\tilde{x}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.11.m11.1d">over~ start_ARG italic_x end_ARG</annotation></semantics></math>. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> step 1, the overall objective <math alttext="\mathcal{L}_{\text{AE}}" class="ltx_Math" display="inline" id="S3.SS1.p2.12.m12.1"><semantics id="S3.SS1.p2.12.m12.1a"><msub id="S3.SS1.p2.12.m12.1.1" xref="S3.SS1.p2.12.m12.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.12.m12.1.1.2" xref="S3.SS1.p2.12.m12.1.1.2.cmml">ℒ</mi><mtext id="S3.SS1.p2.12.m12.1.1.3" xref="S3.SS1.p2.12.m12.1.1.3a.cmml">AE</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.12.m12.1b"><apply id="S3.SS1.p2.12.m12.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.12.m12.1.1.1.cmml" xref="S3.SS1.p2.12.m12.1.1">subscript</csymbol><ci id="S3.SS1.p2.12.m12.1.1.2.cmml" xref="S3.SS1.p2.12.m12.1.1.2">ℒ</ci><ci id="S3.SS1.p2.12.m12.1.1.3a.cmml" xref="S3.SS1.p2.12.m12.1.1.3"><mtext id="S3.SS1.p2.12.m12.1.1.3.cmml" mathsize="70%" xref="S3.SS1.p2.12.m12.1.1.3">AE</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.12.m12.1c">\mathcal{L}_{\text{AE}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.12.m12.1d">caligraphic_L start_POSTSUBSCRIPT AE end_POSTSUBSCRIPT</annotation></semantics></math> to train the volume compression network (<math alttext="\mathcal{E},\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.p2.13.m13.2"><semantics id="S3.SS1.p2.13.m13.2a"><mrow id="S3.SS1.p2.13.m13.2.3.2" xref="S3.SS1.p2.13.m13.2.3.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.13.m13.1.1" xref="S3.SS1.p2.13.m13.1.1.cmml">ℰ</mi><mo id="S3.SS1.p2.13.m13.2.3.2.1" xref="S3.SS1.p2.13.m13.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p2.13.m13.2.2" xref="S3.SS1.p2.13.m13.2.2.cmml">𝒟</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.13.m13.2b"><list id="S3.SS1.p2.13.m13.2.3.1.cmml" xref="S3.SS1.p2.13.m13.2.3.2"><ci id="S3.SS1.p2.13.m13.1.1.cmml" xref="S3.SS1.p2.13.m13.1.1">ℰ</ci><ci id="S3.SS1.p2.13.m13.2.2.cmml" xref="S3.SS1.p2.13.m13.2.2">𝒟</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.13.m13.2c">\mathcal{E},\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.13.m13.2d">caligraphic_E , caligraphic_D</annotation></semantics></math>) in MAISI can be defined as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\min\limits_{\mathcal{E},\mathcal{D}}\max\limits_{\mathcal{C}}%
\Bigl{(}\mathcal{L}_{\text{recon}}(x,\mathcal{D}(\mathcal{E}(x)))" class="ltx_math_unparsed" display="inline" id="S3.E1X.2.1.1.m1.4"><semantics id="S3.E1X.2.1.1.m1.4a"><mrow id="S3.E1X.2.1.1.m1.4b"><munder id="S3.E1X.2.1.1.m1.4.5"><mi id="S3.E1X.2.1.1.m1.4.5.2">min</mi><mrow id="S3.E1X.2.1.1.m1.2.2.2.4"><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.1.1.1.1">ℰ</mi><mo id="S3.E1X.2.1.1.m1.2.2.2.4.1">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.2.2.2.2">𝒟</mi></mrow></munder><munder id="S3.E1X.2.1.1.m1.4.6"><mi id="S3.E1X.2.1.1.m1.4.6.2">max</mi><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.4.6.3">𝒞</mi></munder><mrow id="S3.E1X.2.1.1.m1.4.7"><mo id="S3.E1X.2.1.1.m1.4.7.1" maxsize="160%" minsize="160%">(</mo><msub id="S3.E1X.2.1.1.m1.4.7.2"><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.4.7.2.2">ℒ</mi><mtext id="S3.E1X.2.1.1.m1.4.7.2.3">recon</mtext></msub><mrow id="S3.E1X.2.1.1.m1.4.7.3"><mo id="S3.E1X.2.1.1.m1.4.7.3.1" stretchy="false">(</mo><mi id="S3.E1X.2.1.1.m1.4.4">x</mi><mo id="S3.E1X.2.1.1.m1.4.7.3.2">,</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.4.7.3.3">𝒟</mi><mrow id="S3.E1X.2.1.1.m1.4.7.3.4"><mo id="S3.E1X.2.1.1.m1.4.7.3.4.1" stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1X.2.1.1.m1.4.7.3.4.2">ℰ</mi><mrow id="S3.E1X.2.1.1.m1.4.7.3.4.3"><mo id="S3.E1X.2.1.1.m1.4.7.3.4.3.1" stretchy="false">(</mo><mi id="S3.E1X.2.1.1.m1.3.3">x</mi><mo id="S3.E1X.2.1.1.m1.4.7.3.4.3.2" stretchy="false">)</mo></mrow><mo id="S3.E1X.2.1.1.m1.4.7.3.4.4" stretchy="false">)</mo></mrow><mo id="S3.E1X.2.1.1.m1.4.7.3.5" stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.4c">\displaystyle\min\limits_{\mathcal{E},\mathcal{D}}\max\limits_{\mathcal{C}}%
\Bigl{(}\mathcal{L}_{\text{recon}}(x,\mathcal{D}(\mathcal{E}(x)))</annotation><annotation encoding="application/x-llamapun" id="S3.E1X.2.1.1.m1.4d">roman_min start_POSTSUBSCRIPT caligraphic_E , caligraphic_D end_POSTSUBSCRIPT roman_max start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ( caligraphic_L start_POSTSUBSCRIPT recon end_POSTSUBSCRIPT ( italic_x , caligraphic_D ( caligraphic_E ( italic_x ) ) )</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\mathcal{L}_{\text{lpips}}(x,\mathcal{D}(\mathcal{E}(x)))" class="ltx_Math" display="inline" id="S3.E1X.3.2.2.m1.3"><semantics id="S3.E1X.3.2.2.m1.3a"><mrow id="S3.E1X.3.2.2.m1.3.3" xref="S3.E1X.3.2.2.m1.3.3.cmml"><mo id="S3.E1X.3.2.2.m1.3.3a" xref="S3.E1X.3.2.2.m1.3.3.cmml">+</mo><mrow id="S3.E1X.3.2.2.m1.3.3.1" xref="S3.E1X.3.2.2.m1.3.3.1.cmml"><msub id="S3.E1X.3.2.2.m1.3.3.1.3" xref="S3.E1X.3.2.2.m1.3.3.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1X.3.2.2.m1.3.3.1.3.2" xref="S3.E1X.3.2.2.m1.3.3.1.3.2.cmml">ℒ</mi><mtext id="S3.E1X.3.2.2.m1.3.3.1.3.3" xref="S3.E1X.3.2.2.m1.3.3.1.3.3a.cmml">lpips</mtext></msub><mo id="S3.E1X.3.2.2.m1.3.3.1.2" xref="S3.E1X.3.2.2.m1.3.3.1.2.cmml">⁢</mo><mrow id="S3.E1X.3.2.2.m1.3.3.1.1.1" xref="S3.E1X.3.2.2.m1.3.3.1.1.2.cmml"><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.2" stretchy="false" xref="S3.E1X.3.2.2.m1.3.3.1.1.2.cmml">(</mo><mi id="S3.E1X.3.2.2.m1.2.2" xref="S3.E1X.3.2.2.m1.2.2.cmml">x</mi><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.3" xref="S3.E1X.3.2.2.m1.3.3.1.1.2.cmml">,</mo><mrow id="S3.E1X.3.2.2.m1.3.3.1.1.1.1" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.3" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.3.cmml">𝒟</mi><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.cmml">ℰ</mi><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.1" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.3.2" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E1X.3.2.2.m1.1.1" xref="S3.E1X.3.2.2.m1.1.1.cmml">x</mi><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1X.3.2.2.m1.3.3.1.1.1.4" stretchy="false" xref="S3.E1X.3.2.2.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.3.2.2.m1.3b"><apply id="S3.E1X.3.2.2.m1.3.3.cmml" xref="S3.E1X.3.2.2.m1.3.3"><plus id="S3.E1X.3.2.2.m1.3.3.2.cmml" xref="S3.E1X.3.2.2.m1.3.3"></plus><apply id="S3.E1X.3.2.2.m1.3.3.1.cmml" xref="S3.E1X.3.2.2.m1.3.3.1"><times id="S3.E1X.3.2.2.m1.3.3.1.2.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.2"></times><apply id="S3.E1X.3.2.2.m1.3.3.1.3.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.3"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.3.3.1.3.1.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.3">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.3.3.1.3.2.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.3.2">ℒ</ci><ci id="S3.E1X.3.2.2.m1.3.3.1.3.3a.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.3.3"><mtext id="S3.E1X.3.2.2.m1.3.3.1.3.3.cmml" mathsize="70%" xref="S3.E1X.3.2.2.m1.3.3.1.3.3">lpips</mtext></ci></apply><interval closure="open" id="S3.E1X.3.2.2.m1.3.3.1.1.2.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.1.1"><ci id="S3.E1X.3.2.2.m1.2.2.cmml" xref="S3.E1X.3.2.2.m1.2.2">𝑥</ci><apply id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1"><times id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.2"></times><ci id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.3">𝒟</ci><apply id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1"><times id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.1"></times><ci id="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.E1X.3.2.2.m1.3.3.1.1.1.1.1.1.1.2">ℰ</ci><ci id="S3.E1X.3.2.2.m1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1">𝑥</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.3.2.2.m1.3c">\displaystyle+\mathcal{L}_{\text{lpips}}(x,\mathcal{D}(\mathcal{E}(x)))</annotation><annotation encoding="application/x-llamapun" id="S3.E1X.3.2.2.m1.3d">+ caligraphic_L start_POSTSUBSCRIPT lpips end_POSTSUBSCRIPT ( italic_x , caligraphic_D ( caligraphic_E ( italic_x ) ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle+\mathcal{L}_{\text{reg}}(\mathcal{E}(x))+\mathcal{L}_{\text{adv}%
}\Bigr{)}," class="ltx_math_unparsed" display="inline" id="S3.E1Xa.2.1.1.m1.1"><semantics id="S3.E1Xa.2.1.1.m1.1a"><mrow id="S3.E1Xa.2.1.1.m1.1b"><mo id="S3.E1Xa.2.1.1.m1.1.2">+</mo><msub id="S3.E1Xa.2.1.1.m1.1.3"><mi class="ltx_font_mathcaligraphic" id="S3.E1Xa.2.1.1.m1.1.3.2">ℒ</mi><mtext id="S3.E1Xa.2.1.1.m1.1.3.3">reg</mtext></msub><mrow id="S3.E1Xa.2.1.1.m1.1.4"><mo id="S3.E1Xa.2.1.1.m1.1.4.1" stretchy="false">(</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1Xa.2.1.1.m1.1.4.2">ℰ</mi><mrow id="S3.E1Xa.2.1.1.m1.1.4.3"><mo id="S3.E1Xa.2.1.1.m1.1.4.3.1" stretchy="false">(</mo><mi id="S3.E1Xa.2.1.1.m1.1.1">x</mi><mo id="S3.E1Xa.2.1.1.m1.1.4.3.2" stretchy="false">)</mo></mrow><mo id="S3.E1Xa.2.1.1.m1.1.4.4" stretchy="false">)</mo></mrow><mo id="S3.E1Xa.2.1.1.m1.1.5">+</mo><msub id="S3.E1Xa.2.1.1.m1.1.6"><mi class="ltx_font_mathcaligraphic" id="S3.E1Xa.2.1.1.m1.1.6.2">ℒ</mi><mtext id="S3.E1Xa.2.1.1.m1.1.6.3">adv</mtext></msub><mo id="S3.E1Xa.2.1.1.m1.1.7" maxsize="160%" minsize="160%">)</mo><mo id="S3.E1Xa.2.1.1.m1.1.8">,</mo></mrow><annotation encoding="application/x-tex" id="S3.E1Xa.2.1.1.m1.1c">\displaystyle+\mathcal{L}_{\text{reg}}(\mathcal{E}(x))+\mathcal{L}_{\text{adv}%
}\Bigr{)},</annotation><annotation encoding="application/x-llamapun" id="S3.E1Xa.2.1.1.m1.1d">+ caligraphic_L start_POSTSUBSCRIPT reg end_POSTSUBSCRIPT ( caligraphic_E ( italic_x ) ) + caligraphic_L start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS1.p2.14">where</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{adv}}=\log\mathcal{C}(x)+\log(1-\mathcal{C}(%
\mathcal{D}(\mathcal{E}(x))))." class="ltx_Math" display="inline" id="S3.E2X.2.1.1.m1.4"><semantics id="S3.E2X.2.1.1.m1.4a"><mrow id="S3.E2X.2.1.1.m1.4.4.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.cmml"><mrow id="S3.E2X.2.1.1.m1.4.4.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.cmml"><msub id="S3.E2X.2.1.1.m1.4.4.1.1.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.4.4.1.1.3.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.3.2.cmml">ℒ</mi><mtext id="S3.E2X.2.1.1.m1.4.4.1.1.3.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.3.3a.cmml">adv</mtext></msub><mo id="S3.E2X.2.1.1.m1.4.4.1.1.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.2.cmml">=</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.cmml"><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.cmml"><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.cmml"><mi id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.1.cmml">log</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2a" lspace="0.167em" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.cmml">⁡</mo><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.2.cmml">𝒞</mi></mrow><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.1.cmml">⁢</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.3.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.cmml"><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.3.2.1" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.cmml">(</mo><mi id="S3.E2X.2.1.1.m1.1.1" xref="S3.E2X.2.1.1.m1.1.1.cmml">x</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.3.2.2" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.cmml">)</mo></mrow></mrow><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.2.cmml">+</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.2.cmml"><mi id="S3.E2X.2.1.1.m1.3.3" xref="S3.E2X.2.1.1.m1.3.3.cmml">log</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1a" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.2.cmml">⁡</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.2.cmml"><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.2.cmml">(</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.cmml"><mn id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.3.cmml">1</mn><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.2.cmml">−</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml">𝒞</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml">𝒟</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">ℰ</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mi id="S3.E2X.2.1.1.m1.2.2" xref="S3.E2X.2.1.1.m1.2.2.cmml">x</mi><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.2.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E2X.2.1.1.m1.4.4.1.2" lspace="0em" xref="S3.E2X.2.1.1.m1.4.4.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.4b"><apply id="S3.E2X.2.1.1.m1.4.4.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1"><eq id="S3.E2X.2.1.1.m1.4.4.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.2"></eq><apply id="S3.E2X.2.1.1.m1.4.4.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.4.4.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.3">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.4.4.1.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.3.2">ℒ</ci><ci id="S3.E2X.2.1.1.m1.4.4.1.1.3.3a.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.3.3"><mtext id="S3.E2X.2.1.1.m1.4.4.1.1.3.3.cmml" mathsize="70%" xref="S3.E2X.2.1.1.m1.4.4.1.1.3.3">adv</mtext></ci></apply><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1"><plus id="S3.E2X.2.1.1.m1.4.4.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.2"></plus><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3"><times id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.1"></times><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2"><log id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.1"></log><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.3.2.2">𝒞</ci></apply><ci id="S3.E2X.2.1.1.m1.1.1.cmml" xref="S3.E2X.2.1.1.m1.1.1">𝑥</ci></apply><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1"><log id="S3.E2X.2.1.1.m1.3.3.cmml" xref="S3.E2X.2.1.1.m1.3.3"></log><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1"><minus id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.2"></minus><cn id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.3">1</cn><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1"><times id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.3">𝒞</ci><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1"><times id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.3">𝒟</ci><apply id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"></times><ci id="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2X.2.1.1.m1.4.4.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">ℰ</ci><ci id="S3.E2X.2.1.1.m1.2.2.cmml" xref="S3.E2X.2.1.1.m1.2.2">𝑥</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.4c">\displaystyle\mathcal{L}_{\text{adv}}=\log\mathcal{C}(x)+\log(1-\mathcal{C}(%
\mathcal{D}(\mathcal{E}(x)))).</annotation><annotation encoding="application/x-llamapun" id="S3.E2X.2.1.1.m1.4d">caligraphic_L start_POSTSUBSCRIPT adv end_POSTSUBSCRIPT = roman_log caligraphic_C ( italic_x ) + roman_log ( 1 - caligraphic_C ( caligraphic_D ( caligraphic_E ( italic_x ) ) ) ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="188" id="S3.F3.g1" src="x3.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">The schematics of tensor splitting parallelism in MAISI. Feature maps are first partitioned into smaller segments with overlaps and allocated to designated devices. Then, these segments are stitched together to compose the output of the layer.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Generating high-resolution 3D volumes, particularly those exceeding dimensions of <math alttext="512^{3}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msup id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mn id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">512</mn><mn id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml">3</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">superscript</csymbol><cn id="S3.SS1.p3.1.m1.1.1.2.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1.2">512</cn><cn id="S3.SS1.p3.1.m1.1.1.3.cmml" type="integer" xref="S3.SS1.p3.1.m1.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">512^{3}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">512 start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT</annotation></semantics></math> voxels, poses a significant challenge due to the substantial memory demands imposed by the 3D convolution networks. In order to address memory bottleneck, previous methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib53" title="">53</a>]</cite> achieve 2D high-resolution image synthesis via an additional super-resolution model. However, in the context of 3D whole-volume generation, the memory consumption can still quickly reach the hardware limitation of modern GPUs (<em class="ltx_emph ltx_font_italic" id="S3.SS1.p3.1.1">e.g</em>.<span class="ltx_text" id="S3.SS1.p3.1.2"></span>, NVIDIA A100 80G). To overcome GPU memory constraints, sliding window inference <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib6" title="">6</a>]</cite> is a common technique. It divides the large network input into smaller 3D patches in a sliding-window fashion and then stitches the network’s output of each patch together to form the final results. When used in the 3D medical image segmentation model inference, it can often lead to artifacts/discontinuities along the window boundaries. While overlapping windows can help in segmentation tasks by smoothing over the boundary artifacts of probability maps, we empirically found this issue in transition areas between windows is more pronounced for the synthesis task due to the direct generation of image intensities, and thus the direct adaptation of sliding window inference is not self-sufficient. To minimize the use of the sliding-window approach for image synthesis, we propose a simple yet effective solution by introducing tensor splitting parallelism into convolutional networks. The tensor parallelism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib57" title="">57</a>]</cite> is initially developed to distribute the inputs or model weights of matrix multiplication operations in fully connected layers across multiple GPUs. Unlike language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib18" title="">18</a>]</cite> built upon linear layers, the memory bottleneck usually attributes to the large feature maps. As demonstrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S3.F3" title="Figure 3 ‣ 3.1 Volume Compression Network ‣ 3 Methodology ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">3</span></a>, the proposed TSP is utilized to divide feature maps into smaller segments while preserving necessary overlaps across both convolution and normalization layers of AE. Each segment is assigned to a designated device, and these segments are subsequently merged to generate the layer’s output. This flexible implementation enables segments to be distributed across multiple devices to accelerate the inference and also allows each segment to be processed sequentially within a single device in a loop to reduce peak memory usage.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Diffusion Model</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.8">The Diffusion Model in MAISI operates on a compressed latent space with flexible dimensions and incorporates body region and voxel spacing as conditional inputs, facilitating the high-fidelity generation of anatomical structures. Diffusion models are probabilistic models that aim to learn a data distribution <math alttext="p(x)" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mrow id="S3.SS2.p1.1.m1.1.2" xref="S3.SS2.p1.1.m1.1.2.cmml"><mi id="S3.SS2.p1.1.m1.1.2.2" xref="S3.SS2.p1.1.m1.1.2.2.cmml">p</mi><mo id="S3.SS2.p1.1.m1.1.2.1" xref="S3.SS2.p1.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS2.p1.1.m1.1.2.3.2" xref="S3.SS2.p1.1.m1.1.2.cmml"><mo id="S3.SS2.p1.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS2.p1.1.m1.1.2.cmml">(</mo><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">x</mi><mo id="S3.SS2.p1.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS2.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.2.cmml" xref="S3.SS2.p1.1.m1.1.2"><times id="S3.SS2.p1.1.m1.1.2.1.cmml" xref="S3.SS2.p1.1.m1.1.2.1"></times><ci id="S3.SS2.p1.1.m1.1.2.2.cmml" xref="S3.SS2.p1.1.m1.1.2.2">𝑝</ci><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">p(x)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_p ( italic_x )</annotation></semantics></math> by gradually denoising a normally distributed variable. This process is equivalent to learning the reverse dynamics of a fixed Markov Chain over a sequence of <math alttext="T" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_T</annotation></semantics></math> steps. The denoising score-matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib60" title="">60</a>]</cite> is widely adopted in image synthesis tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib54" title="">54</a>]</cite>. In the context of latent diffusion model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a>]</cite>, the learning model <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi id="S3.SS2.p1.3.m3.1.1.2" xref="S3.SS2.p1.3.m3.1.1.2.cmml">ϵ</mi><mi id="S3.SS2.p1.3.m3.1.1.3" xref="S3.SS2.p1.3.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">italic-ϵ</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> functions as a uniformly weighted sequence of denoising autoencoders <math alttext="\epsilon_{\theta}(z_{t},t);t=1\dots T" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.3"><semantics id="S3.SS2.p1.4.m4.3a"><mrow id="S3.SS2.p1.4.m4.3.3" xref="S3.SS2.p1.4.m4.3.3.cmml"><mrow id="S3.SS2.p1.4.m4.3.3.1.1" xref="S3.SS2.p1.4.m4.3.3.1.2.cmml"><mrow id="S3.SS2.p1.4.m4.3.3.1.1.1" xref="S3.SS2.p1.4.m4.3.3.1.1.1.cmml"><msub id="S3.SS2.p1.4.m4.3.3.1.1.1.3" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3.cmml"><mi id="S3.SS2.p1.4.m4.3.3.1.1.1.3.2" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3.2.cmml">ϵ</mi><mi id="S3.SS2.p1.4.m4.3.3.1.1.1.3.3" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3.3.cmml">θ</mi></msub><mo id="S3.SS2.p1.4.m4.3.3.1.1.1.2" xref="S3.SS2.p1.4.m4.3.3.1.1.1.2.cmml">⁢</mo><mrow id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.2.cmml"><mo id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.2.cmml">(</mo><msub id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.2" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.3" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.3" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.2.cmml">,</mo><mi id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml">t</mi><mo id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.4" stretchy="false" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.4.m4.3.3.1.1.2" xref="S3.SS2.p1.4.m4.3.3.1.2.cmml">;</mo><mi id="S3.SS2.p1.4.m4.2.2" xref="S3.SS2.p1.4.m4.2.2.cmml">t</mi></mrow><mo id="S3.SS2.p1.4.m4.3.3.2" xref="S3.SS2.p1.4.m4.3.3.2.cmml">=</mo><mrow id="S3.SS2.p1.4.m4.3.3.3" xref="S3.SS2.p1.4.m4.3.3.3.cmml"><mn id="S3.SS2.p1.4.m4.3.3.3.2" xref="S3.SS2.p1.4.m4.3.3.3.2.cmml">1</mn><mo id="S3.SS2.p1.4.m4.3.3.3.1" xref="S3.SS2.p1.4.m4.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.4.m4.3.3.3.3" mathvariant="normal" xref="S3.SS2.p1.4.m4.3.3.3.3.cmml">…</mi><mo id="S3.SS2.p1.4.m4.3.3.3.1a" xref="S3.SS2.p1.4.m4.3.3.3.1.cmml">⁢</mo><mi id="S3.SS2.p1.4.m4.3.3.3.4" xref="S3.SS2.p1.4.m4.3.3.3.4.cmml">T</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.3b"><apply id="S3.SS2.p1.4.m4.3.3.cmml" xref="S3.SS2.p1.4.m4.3.3"><eq id="S3.SS2.p1.4.m4.3.3.2.cmml" xref="S3.SS2.p1.4.m4.3.3.2"></eq><list id="S3.SS2.p1.4.m4.3.3.1.2.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1"><apply id="S3.SS2.p1.4.m4.3.3.1.1.1.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1"><times id="S3.SS2.p1.4.m4.3.3.1.1.1.2.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.2"></times><apply id="S3.SS2.p1.4.m4.3.3.1.1.1.3.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.3.3.1.1.1.3.1.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p1.4.m4.3.3.1.1.1.3.2.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3.2">italic-ϵ</ci><ci id="S3.SS2.p1.4.m4.3.3.1.1.1.3.3.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.3.3">𝜃</ci></apply><interval closure="open" id="S3.SS2.p1.4.m4.3.3.1.1.1.1.2.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.1"><apply id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.4.m4.3.3.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1">𝑡</ci></interval></apply><ci id="S3.SS2.p1.4.m4.2.2.cmml" xref="S3.SS2.p1.4.m4.2.2">𝑡</ci></list><apply id="S3.SS2.p1.4.m4.3.3.3.cmml" xref="S3.SS2.p1.4.m4.3.3.3"><times id="S3.SS2.p1.4.m4.3.3.3.1.cmml" xref="S3.SS2.p1.4.m4.3.3.3.1"></times><cn id="S3.SS2.p1.4.m4.3.3.3.2.cmml" type="integer" xref="S3.SS2.p1.4.m4.3.3.3.2">1</cn><ci id="S3.SS2.p1.4.m4.3.3.3.3.cmml" xref="S3.SS2.p1.4.m4.3.3.3.3">…</ci><ci id="S3.SS2.p1.4.m4.3.3.3.4.cmml" xref="S3.SS2.p1.4.m4.3.3.3.4">𝑇</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.3c">\epsilon_{\theta}(z_{t},t);t=1\dots T</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.3d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t ) ; italic_t = 1 … italic_T</annotation></semantics></math>, which are designed to predict a denoised version of the input latent features <math alttext="z_{t}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><msub id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">z</mi><mi id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">𝑧</ci><ci id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">z_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="z_{t}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><msub id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">z</mi><mi id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">𝑧</ci><ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">z_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> represents a noisy variant of the original input at time step <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><mi id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><ci id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.1d">italic_t</annotation></semantics></math>. The neural backbone <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1"><semantics id="S3.SS2.p1.8.m8.1a"><msub id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml"><mi id="S3.SS2.p1.8.m8.1.1.2" xref="S3.SS2.p1.8.m8.1.1.2.cmml">ϵ</mi><mi id="S3.SS2.p1.8.m8.1.1.3" xref="S3.SS2.p1.8.m8.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><apply id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.8.m8.1.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p1.8.m8.1.1.2.cmml" xref="S3.SS2.p1.8.m8.1.1.2">italic-ϵ</ci><ci id="S3.SS2.p1.8.m8.1.1.3.cmml" xref="S3.SS2.p1.8.m8.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m8.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is defined as a time-conditional U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib50" title="">50</a>]</cite></p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.6">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> step 2, the diffusion model in MAISI additionally conditions on both the body region and voxel spacing. The body region is defined by a top-region index <math alttext="\bm{i}_{\text{top}}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">𝒊</mi><mtext id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3a.cmml">top</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝒊</ci><ci id="S3.SS2.p2.1.m1.1.1.3a.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><mtext id="S3.SS2.p2.1.m1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p2.1.m1.1.1.3">top</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\bm{i}_{\text{top}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">bold_italic_i start_POSTSUBSCRIPT top end_POSTSUBSCRIPT</annotation></semantics></math> and a bottom-region index <math alttext="\bm{i}_{\text{bottom}}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">𝒊</mi><mtext id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3a.cmml">bottom</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝒊</ci><ci id="S3.SS2.p2.2.m2.1.1.3a.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><mtext id="S3.SS2.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p2.2.m2.1.1.3">bottom</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\bm{i}_{\text{bottom}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">bold_italic_i start_POSTSUBSCRIPT bottom end_POSTSUBSCRIPT</annotation></semantics></math>, indicating the extent of the CT scan coverage. <math alttext="\bm{i}_{\text{top}}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><msub id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">𝒊</mi><mtext id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3a.cmml">top</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">𝒊</ci><ci id="S3.SS2.p2.3.m3.1.1.3a.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><mtext id="S3.SS2.p2.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p2.3.m3.1.1.3">top</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\bm{i}_{\text{top}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">bold_italic_i start_POSTSUBSCRIPT top end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\bm{i}_{\text{bottom}}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><msub id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">𝒊</mi><mtext id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3a.cmml">bottom</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">𝒊</ci><ci id="S3.SS2.p2.4.m4.1.1.3a.cmml" xref="S3.SS2.p2.4.m4.1.1.3"><mtext id="S3.SS2.p2.4.m4.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p2.4.m4.1.1.3">bottom</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\bm{i}_{\text{bottom}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">bold_italic_i start_POSTSUBSCRIPT bottom end_POSTSUBSCRIPT</annotation></semantics></math> are defined by 4-dimensional one-hot vectors for head-neck, chest, abdomen, and lower-body regions). We ascertain the body region either through segmentation ground truth or predated segmentation masks from whole-body CT segmentation models, such as TotalSegmentator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib62" title="">62</a>]</cite> or VISTA3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib26" title="">26</a>]</cite>. The condition of voxel spacing <math alttext="\bm{s}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">𝒔</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">𝒔</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\bm{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">bold_italic_s</annotation></semantics></math> is defined by a vector containing three float numbers representing the physical size of each voxel along each of the three dimensions in millimeters. We denote the primary conditions as <math alttext="\bm{c}_{p}:=\{\bm{i}_{\text{top}},\bm{i}_{\text{bottom}},\bm{s}\}" class="ltx_Math" display="inline" id="S3.SS2.p2.6.m6.3"><semantics id="S3.SS2.p2.6.m6.3a"><mrow id="S3.SS2.p2.6.m6.3.3" xref="S3.SS2.p2.6.m6.3.3.cmml"><msub id="S3.SS2.p2.6.m6.3.3.4" xref="S3.SS2.p2.6.m6.3.3.4.cmml"><mi id="S3.SS2.p2.6.m6.3.3.4.2" xref="S3.SS2.p2.6.m6.3.3.4.2.cmml">𝒄</mi><mi id="S3.SS2.p2.6.m6.3.3.4.3" xref="S3.SS2.p2.6.m6.3.3.4.3.cmml">p</mi></msub><mo id="S3.SS2.p2.6.m6.3.3.3" lspace="0.278em" rspace="0.278em" xref="S3.SS2.p2.6.m6.3.3.3.cmml">:=</mo><mrow id="S3.SS2.p2.6.m6.3.3.2.2" xref="S3.SS2.p2.6.m6.3.3.2.3.cmml"><mo id="S3.SS2.p2.6.m6.3.3.2.2.3" stretchy="false" xref="S3.SS2.p2.6.m6.3.3.2.3.cmml">{</mo><msub id="S3.SS2.p2.6.m6.2.2.1.1.1" xref="S3.SS2.p2.6.m6.2.2.1.1.1.cmml"><mi id="S3.SS2.p2.6.m6.2.2.1.1.1.2" xref="S3.SS2.p2.6.m6.2.2.1.1.1.2.cmml">𝒊</mi><mtext id="S3.SS2.p2.6.m6.2.2.1.1.1.3" xref="S3.SS2.p2.6.m6.2.2.1.1.1.3a.cmml">top</mtext></msub><mo id="S3.SS2.p2.6.m6.3.3.2.2.4" xref="S3.SS2.p2.6.m6.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p2.6.m6.3.3.2.2.2" xref="S3.SS2.p2.6.m6.3.3.2.2.2.cmml"><mi id="S3.SS2.p2.6.m6.3.3.2.2.2.2" xref="S3.SS2.p2.6.m6.3.3.2.2.2.2.cmml">𝒊</mi><mtext id="S3.SS2.p2.6.m6.3.3.2.2.2.3" xref="S3.SS2.p2.6.m6.3.3.2.2.2.3a.cmml">bottom</mtext></msub><mo id="S3.SS2.p2.6.m6.3.3.2.2.5" xref="S3.SS2.p2.6.m6.3.3.2.3.cmml">,</mo><mi id="S3.SS2.p2.6.m6.1.1" xref="S3.SS2.p2.6.m6.1.1.cmml">𝒔</mi><mo id="S3.SS2.p2.6.m6.3.3.2.2.6" stretchy="false" xref="S3.SS2.p2.6.m6.3.3.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.6.m6.3b"><apply id="S3.SS2.p2.6.m6.3.3.cmml" xref="S3.SS2.p2.6.m6.3.3"><csymbol cd="latexml" id="S3.SS2.p2.6.m6.3.3.3.cmml" xref="S3.SS2.p2.6.m6.3.3.3">assign</csymbol><apply id="S3.SS2.p2.6.m6.3.3.4.cmml" xref="S3.SS2.p2.6.m6.3.3.4"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.3.3.4.1.cmml" xref="S3.SS2.p2.6.m6.3.3.4">subscript</csymbol><ci id="S3.SS2.p2.6.m6.3.3.4.2.cmml" xref="S3.SS2.p2.6.m6.3.3.4.2">𝒄</ci><ci id="S3.SS2.p2.6.m6.3.3.4.3.cmml" xref="S3.SS2.p2.6.m6.3.3.4.3">𝑝</ci></apply><set id="S3.SS2.p2.6.m6.3.3.2.3.cmml" xref="S3.SS2.p2.6.m6.3.3.2.2"><apply id="S3.SS2.p2.6.m6.2.2.1.1.1.cmml" xref="S3.SS2.p2.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.2.2.1.1.1.1.cmml" xref="S3.SS2.p2.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.6.m6.2.2.1.1.1.2.cmml" xref="S3.SS2.p2.6.m6.2.2.1.1.1.2">𝒊</ci><ci id="S3.SS2.p2.6.m6.2.2.1.1.1.3a.cmml" xref="S3.SS2.p2.6.m6.2.2.1.1.1.3"><mtext id="S3.SS2.p2.6.m6.2.2.1.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p2.6.m6.2.2.1.1.1.3">top</mtext></ci></apply><apply id="S3.SS2.p2.6.m6.3.3.2.2.2.cmml" xref="S3.SS2.p2.6.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.6.m6.3.3.2.2.2.1.cmml" xref="S3.SS2.p2.6.m6.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.6.m6.3.3.2.2.2.2.cmml" xref="S3.SS2.p2.6.m6.3.3.2.2.2.2">𝒊</ci><ci id="S3.SS2.p2.6.m6.3.3.2.2.2.3a.cmml" xref="S3.SS2.p2.6.m6.3.3.2.2.2.3"><mtext id="S3.SS2.p2.6.m6.3.3.2.2.2.3.cmml" mathsize="70%" xref="S3.SS2.p2.6.m6.3.3.2.2.2.3">bottom</mtext></ci></apply><ci id="S3.SS2.p2.6.m6.1.1.cmml" xref="S3.SS2.p2.6.m6.1.1">𝒔</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.6.m6.3c">\bm{c}_{p}:=\{\bm{i}_{\text{top}},\bm{i}_{\text{bottom}},\bm{s}\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.6.m6.3d">bold_italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT := { bold_italic_i start_POSTSUBSCRIPT top end_POSTSUBSCRIPT , bold_italic_i start_POSTSUBSCRIPT bottom end_POSTSUBSCRIPT , bold_italic_s }</annotation></semantics></math>. Formally, the training objective of MAISI diffusion model is as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{\mathcal{E}(x),\epsilon\sim\mathcal{N}(0,1),t,\bm{c}_%
{p}}\Bigl{[}\lVert\epsilon-\epsilon_{\theta}(z_{t},t,\bm{c}_{p})\rVert_{1}%
\Bigr{]}," class="ltx_Math" display="inline" id="S3.E3X.2.1.1.m1.9"><semantics id="S3.E3X.2.1.1.m1.9a"><mrow id="S3.E3X.2.1.1.m1.9.9.1" xref="S3.E3X.2.1.1.m1.9.9.1.1.cmml"><mrow id="S3.E3X.2.1.1.m1.9.9.1.1" xref="S3.E3X.2.1.1.m1.9.9.1.1.cmml"><msub id="S3.E3X.2.1.1.m1.9.9.1.1.3" xref="S3.E3X.2.1.1.m1.9.9.1.1.3.cmml"><mi id="S3.E3X.2.1.1.m1.9.9.1.1.3.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E3X.2.1.1.m1.7.7.7.7" xref="S3.E3X.2.1.1.m1.7.7.7.8.cmml"><mrow id="S3.E3X.2.1.1.m1.6.6.6.6.1" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.cmml"><mrow id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.2.cmml"><mrow id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.2" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.2.cmml">ℰ</mi><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.1" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.3.2" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml"><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.3.2.1" stretchy="false" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml">(</mo><mi id="S3.E3X.2.1.1.m1.1.1.1.1" xref="S3.E3X.2.1.1.m1.1.1.1.1.cmml">x</mi><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.3.2.2" stretchy="false" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.2" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.2.cmml">,</mo><mi id="S3.E3X.2.1.1.m1.4.4.4.4" xref="S3.E3X.2.1.1.m1.4.4.4.4.cmml">ϵ</mi></mrow><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.2" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.2.cmml">∼</mo><mrow id="S3.E3X.2.1.1.m1.6.6.6.6.1.3" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.2" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.2.cmml">𝒩</mi><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.1" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.1.cmml">⁢</mo><mrow id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.2" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml"><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.2.1" stretchy="false" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">(</mo><mn id="S3.E3X.2.1.1.m1.2.2.2.2" xref="S3.E3X.2.1.1.m1.2.2.2.2.cmml">0</mn><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.2.2" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">,</mo><mn id="S3.E3X.2.1.1.m1.3.3.3.3" xref="S3.E3X.2.1.1.m1.3.3.3.3.cmml">1</mn><mo id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.2.3" stretchy="false" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3X.2.1.1.m1.7.7.7.7.3" xref="S3.E3X.2.1.1.m1.7.7.7.8a.cmml">,</mo><mrow id="S3.E3X.2.1.1.m1.7.7.7.7.2.1" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.2.cmml"><mi id="S3.E3X.2.1.1.m1.5.5.5.5" xref="S3.E3X.2.1.1.m1.5.5.5.5.cmml">t</mi><mo id="S3.E3X.2.1.1.m1.7.7.7.7.2.1.2" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.2.cmml">,</mo><msub id="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.cmml"><mi id="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.2" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.2.cmml">𝒄</mi><mi id="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.3" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.3.cmml">p</mi></msub></mrow></mrow></msub><mo id="S3.E3X.2.1.1.m1.9.9.1.1.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.2.cmml">⁢</mo><mrow id="S3.E3X.2.1.1.m1.9.9.1.1.1.1" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.2.cmml"><mo id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.2" maxsize="160%" minsize="160%" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.2.1.cmml">[</mo><msub id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.cmml"><mrow id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.2.cmml"><mo fence="true" id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.2.1.cmml">∥</mo><mrow id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.4" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.4.cmml">ϵ</mi><mo id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.cmml"><msub id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.cmml"><mi id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.2.cmml">ϵ</mi><mi id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.3" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.3.cmml">θ</mi></msub><mo id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.3" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.3.cmml"><mo id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.4" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><mi id="S3.E3X.2.1.1.m1.8.8" xref="S3.E3X.2.1.1.m1.8.8.cmml">t</mi><mo id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.5" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">𝒄</mi><mi id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">p</mi></msub><mo id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.6" stretchy="false" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo fence="true" id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.3" lspace="0em" rspace="0em" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.3" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.3" maxsize="160%" minsize="160%" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E3X.2.1.1.m1.9.9.1.2" xref="S3.E3X.2.1.1.m1.9.9.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3X.2.1.1.m1.9b"><apply id="S3.E3X.2.1.1.m1.9.9.1.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1"><times id="S3.E3X.2.1.1.m1.9.9.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.2"></times><apply id="S3.E3X.2.1.1.m1.9.9.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.3"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.9.9.1.1.3.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.3">subscript</csymbol><ci id="S3.E3X.2.1.1.m1.9.9.1.1.3.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.3.2">𝔼</ci><apply id="S3.E3X.2.1.1.m1.7.7.7.8.cmml" xref="S3.E3X.2.1.1.m1.7.7.7.7"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.7.7.7.8a.cmml" xref="S3.E3X.2.1.1.m1.7.7.7.7.3">formulae-sequence</csymbol><apply id="S3.E3X.2.1.1.m1.6.6.6.6.1.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1"><csymbol cd="latexml" id="S3.E3X.2.1.1.m1.6.6.6.6.1.2.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.2">similar-to</csymbol><list id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1"><apply id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1"><times id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.1"></times><ci id="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.1.1.1.2">ℰ</ci><ci id="S3.E3X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.1.1.1.1">𝑥</ci></apply><ci id="S3.E3X.2.1.1.m1.4.4.4.4.cmml" xref="S3.E3X.2.1.1.m1.4.4.4.4">italic-ϵ</ci></list><apply id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3"><times id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.1.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.1"></times><ci id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.2.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.2">𝒩</ci><interval closure="open" id="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml" xref="S3.E3X.2.1.1.m1.6.6.6.6.1.3.3.2"><cn id="S3.E3X.2.1.1.m1.2.2.2.2.cmml" type="integer" xref="S3.E3X.2.1.1.m1.2.2.2.2">0</cn><cn id="S3.E3X.2.1.1.m1.3.3.3.3.cmml" type="integer" xref="S3.E3X.2.1.1.m1.3.3.3.3">1</cn></interval></apply></apply><list id="S3.E3X.2.1.1.m1.7.7.7.7.2.2.cmml" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.1"><ci id="S3.E3X.2.1.1.m1.5.5.5.5.cmml" xref="S3.E3X.2.1.1.m1.5.5.5.5">𝑡</ci><apply id="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.cmml" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1">subscript</csymbol><ci id="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.2">𝒄</ci><ci id="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.7.7.7.7.2.1.1.3">𝑝</ci></apply></list></apply></apply><apply id="S3.E3X.2.1.1.m1.9.9.1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1"><csymbol cd="latexml" id="S3.E3X.2.1.1.m1.9.9.1.1.1.2.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1">subscript</csymbol><apply id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.2.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1"><minus id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3"></minus><ci id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.4">italic-ϵ</ci><apply id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2"><times id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.3"></times><apply id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.2">italic-ϵ</ci><ci id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.3.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.4.3">𝜃</ci></apply><vector id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2"><apply id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.E3X.2.1.1.m1.8.8.cmml" xref="S3.E3X.2.1.1.m1.8.8">𝑡</ci><apply id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.2">𝒄</ci><ci id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.3">𝑝</ci></apply></vector></apply></apply></apply><cn id="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.3.cmml" type="integer" xref="S3.E3X.2.1.1.m1.9.9.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3X.2.1.1.m1.9c">\displaystyle\mathbb{E}_{\mathcal{E}(x),\epsilon\sim\mathcal{N}(0,1),t,\bm{c}_%
{p}}\Bigl{[}\lVert\epsilon-\epsilon_{\theta}(z_{t},t,\bm{c}_{p})\rVert_{1}%
\Bigr{]},</annotation><annotation encoding="application/x-llamapun" id="S3.E3X.2.1.1.m1.9d">blackboard_E start_POSTSUBSCRIPT caligraphic_E ( italic_x ) , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , italic_t , bold_italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , bold_italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS2.p2.11">where the neural backbone <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m1.1"><semantics id="S3.SS2.p2.7.m1.1a"><msub id="S3.SS2.p2.7.m1.1.1" xref="S3.SS2.p2.7.m1.1.1.cmml"><mi id="S3.SS2.p2.7.m1.1.1.2" xref="S3.SS2.p2.7.m1.1.1.2.cmml">ϵ</mi><mi id="S3.SS2.p2.7.m1.1.1.3" xref="S3.SS2.p2.7.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m1.1b"><apply id="S3.SS2.p2.7.m1.1.1.cmml" xref="S3.SS2.p2.7.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m1.1.1.1.cmml" xref="S3.SS2.p2.7.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.7.m1.1.1.2.cmml" xref="S3.SS2.p2.7.m1.1.1.2">italic-ϵ</ci><ci id="S3.SS2.p2.7.m1.1.1.3.cmml" xref="S3.SS2.p2.7.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m1.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m1.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is configured to condition on time step <math alttext="t" class="ltx_Math" display="inline" id="S3.SS2.p2.8.m2.1"><semantics id="S3.SS2.p2.8.m2.1a"><mi id="S3.SS2.p2.8.m2.1.1" xref="S3.SS2.p2.8.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.8.m2.1b"><ci id="S3.SS2.p2.8.m2.1.1.cmml" xref="S3.SS2.p2.8.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.8.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.8.m2.1d">italic_t</annotation></semantics></math> and the primary conditions as <math alttext="\bm{c}_{p}" class="ltx_Math" display="inline" id="S3.SS2.p2.9.m3.1"><semantics id="S3.SS2.p2.9.m3.1a"><msub id="S3.SS2.p2.9.m3.1.1" xref="S3.SS2.p2.9.m3.1.1.cmml"><mi id="S3.SS2.p2.9.m3.1.1.2" xref="S3.SS2.p2.9.m3.1.1.2.cmml">𝒄</mi><mi id="S3.SS2.p2.9.m3.1.1.3" xref="S3.SS2.p2.9.m3.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.9.m3.1b"><apply id="S3.SS2.p2.9.m3.1.1.cmml" xref="S3.SS2.p2.9.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.9.m3.1.1.1.cmml" xref="S3.SS2.p2.9.m3.1.1">subscript</csymbol><ci id="S3.SS2.p2.9.m3.1.1.2.cmml" xref="S3.SS2.p2.9.m3.1.1.2">𝒄</ci><ci id="S3.SS2.p2.9.m3.1.1.3.cmml" xref="S3.SS2.p2.9.m3.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.9.m3.1c">\bm{c}_{p}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.9.m3.1d">bold_italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math>. Moreover, <math alttext="\epsilon_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.p2.10.m4.1"><semantics id="S3.SS2.p2.10.m4.1a"><msub id="S3.SS2.p2.10.m4.1.1" xref="S3.SS2.p2.10.m4.1.1.cmml"><mi id="S3.SS2.p2.10.m4.1.1.2" xref="S3.SS2.p2.10.m4.1.1.2.cmml">ϵ</mi><mi id="S3.SS2.p2.10.m4.1.1.3" xref="S3.SS2.p2.10.m4.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.10.m4.1b"><apply id="S3.SS2.p2.10.m4.1.1.cmml" xref="S3.SS2.p2.10.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.10.m4.1.1.1.cmml" xref="S3.SS2.p2.10.m4.1.1">subscript</csymbol><ci id="S3.SS2.p2.10.m4.1.1.2.cmml" xref="S3.SS2.p2.10.m4.1.1.2">italic-ϵ</ci><ci id="S3.SS2.p2.10.m4.1.1.3.cmml" xref="S3.SS2.p2.10.m4.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.10.m4.1c">\epsilon_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.10.m4.1d">italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> undergoes training on the latent variable <math alttext="z_{t}" class="ltx_Math" display="inline" id="S3.SS2.p2.11.m5.1"><semantics id="S3.SS2.p2.11.m5.1a"><msub id="S3.SS2.p2.11.m5.1.1" xref="S3.SS2.p2.11.m5.1.1.cmml"><mi id="S3.SS2.p2.11.m5.1.1.2" xref="S3.SS2.p2.11.m5.1.1.2.cmml">z</mi><mi id="S3.SS2.p2.11.m5.1.1.3" xref="S3.SS2.p2.11.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.11.m5.1b"><apply id="S3.SS2.p2.11.m5.1.1.cmml" xref="S3.SS2.p2.11.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.11.m5.1.1.1.cmml" xref="S3.SS2.p2.11.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.11.m5.1.1.2.cmml" xref="S3.SS2.p2.11.m5.1.1.2">𝑧</ci><ci id="S3.SS2.p2.11.m5.1.1.3.cmml" xref="S3.SS2.p2.11.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.11.m5.1c">z_{t}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.11.m5.1d">italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, which varies in dimensions throughout the training process. This training regimen is designed to facilitate the generation of outputs with flexible volumetric dimensions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Additional Conditioning Mechanisms</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In addition to the primary conditioning on body region and voxel spacing described in the Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S3.SS2" title="3.2 Diffusion Model ‣ 3 Methodology ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">3.2</span></a>, MAISI incorporates an additional mechanism for enhancing the control and flexibility of the generated outputs through the integration of ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite>. It is seamlessly embedded into the MAISI architecture with the latent diffusion model, to provide additional conditioning paths that allow for task-specific adaptations. ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite> is designed to inject auxiliary conditions into the diffusion process, enabling more precise control over the generated anatomical structures. It operates by creating two copies of the neural network blocks: a locked copy that preserves the original model’s knowledge, and a trainable copy that learns to respond to specific conditions. These copies are connected using zero convolution layers, which gradually evolve from zero weights to optimal settings during training. These additional conditions can include a variety of inputs such as segmentation masks for conditional generation based on masks, or masked images and tumor masks for the tumor inpainting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib10" title="">10</a>]</cite>. Similar to <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib38" title="">38</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib44" title="">44</a>]</cite>, we employ a compact encoder network to transform the additional condition from its original resolution into latent features, which are denoted by the task-specific condition <math alttext="\bm{c}_{f}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">𝒄</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝒄</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\bm{c}_{f}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">bold_italic_c start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math>. This transformation process effectively aligns the additional condition with the spatial dimensions of the latent space.
The integration of ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite> occurs during the third stage (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S1.F2" title="Figure 2 ‣ 1 Introduction ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> step 3) of MAISI’s development, where it is trained with the frozen latent diffusion model. The overall learning objective of the entire diffusion algorithm, which incorporates the ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite>, is formulated as follows:</p>
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E4">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E4X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathbb{E}_{\mathcal{E}(x),\epsilon\sim\mathcal{N}(0,1),t,\bm{c}_%
{p},\bm{c}_{f}}\Bigl{[}\lVert\epsilon-\epsilon_{\theta}(z_{t},t,\bm{c}_{p},\bm%
{c}_{f})\rVert_{1}\Bigr{]}." class="ltx_Math" display="inline" id="S3.E4X.2.1.1.m1.9"><semantics id="S3.E4X.2.1.1.m1.9a"><mrow id="S3.E4X.2.1.1.m1.9.9.1" xref="S3.E4X.2.1.1.m1.9.9.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.9.9.1.1" xref="S3.E4X.2.1.1.m1.9.9.1.1.cmml"><msub id="S3.E4X.2.1.1.m1.9.9.1.1.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.3.cmml"><mi id="S3.E4X.2.1.1.m1.9.9.1.1.3.2" xref="S3.E4X.2.1.1.m1.9.9.1.1.3.2.cmml">𝔼</mi><mrow id="S3.E4X.2.1.1.m1.7.7.7.7" xref="S3.E4X.2.1.1.m1.7.7.7.8.cmml"><mrow id="S3.E4X.2.1.1.m1.6.6.6.6.1" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.cmml"><mrow id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.2.cmml"><mrow id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.2.cmml">ℰ</mi><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.3.2" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml"><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.3.2.1" stretchy="false" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml">(</mo><mi id="S3.E4X.2.1.1.m1.1.1.1.1" xref="S3.E4X.2.1.1.m1.1.1.1.1.cmml">x</mi><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.3.2.2" stretchy="false" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.2" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.2.cmml">,</mo><mi id="S3.E4X.2.1.1.m1.4.4.4.4" xref="S3.E4X.2.1.1.m1.4.4.4.4.cmml">ϵ</mi></mrow><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.2" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.2.cmml">∼</mo><mrow id="S3.E4X.2.1.1.m1.6.6.6.6.1.3" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.2" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.2.cmml">𝒩</mi><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.1" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.1.cmml">⁢</mo><mrow id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.2" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml"><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.2.1" stretchy="false" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">(</mo><mn id="S3.E4X.2.1.1.m1.2.2.2.2" xref="S3.E4X.2.1.1.m1.2.2.2.2.cmml">0</mn><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.2.2" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">,</mo><mn id="S3.E4X.2.1.1.m1.3.3.3.3" xref="S3.E4X.2.1.1.m1.3.3.3.3.cmml">1</mn><mo id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.2.3" stretchy="false" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4X.2.1.1.m1.7.7.7.7.3" xref="S3.E4X.2.1.1.m1.7.7.7.8a.cmml">,</mo><mrow id="S3.E4X.2.1.1.m1.7.7.7.7.2.2" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.3.cmml"><mi id="S3.E4X.2.1.1.m1.5.5.5.5" xref="S3.E4X.2.1.1.m1.5.5.5.5.cmml">t</mi><mo id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.3" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.3.cmml">,</mo><msub id="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.2" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.2.cmml">𝒄</mi><mi id="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.3" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.3.cmml">p</mi></msub><mo id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.4" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.3.cmml">,</mo><msub id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.cmml"><mi id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.2" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.2.cmml">𝒄</mi><mi id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.3" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.3.cmml">f</mi></msub></mrow></mrow></msub><mo id="S3.E4X.2.1.1.m1.9.9.1.1.2" xref="S3.E4X.2.1.1.m1.9.9.1.1.2.cmml">⁢</mo><mrow id="S3.E4X.2.1.1.m1.9.9.1.1.1.1" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.2.cmml"><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.2" maxsize="160%" minsize="160%" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.2.1.cmml">[</mo><msub id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.cmml"><mrow id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.2.cmml"><mo fence="true" id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.2" lspace="0em" rspace="0em" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.2.1.cmml">∥</mo><mrow id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.5" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.5.cmml">ϵ</mi><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.4" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.4.cmml">−</mo><mrow id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.cmml"><msub id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.cmml"><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.2" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.2.cmml">ϵ</mi><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.3.cmml">θ</mi></msub><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.4" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.4.cmml">⁢</mo><mrow id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.4.cmml"><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.4" stretchy="false" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.4.cmml">(</mo><msub id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">z</mi><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.5" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><mi id="S3.E4X.2.1.1.m1.8.8" xref="S3.E4X.2.1.1.m1.8.8.cmml">t</mi><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.6" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.2" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml">𝒄</mi><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml">p</mi></msub><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.7" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.4.cmml">,</mo><msub id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.2" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml">𝒄</mi><mi id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml">f</mi></msub><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.8" stretchy="false" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.4.cmml">)</mo></mrow></mrow></mrow><mo fence="true" id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.3" lspace="0em" rspace="0em" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.2.1.cmml">∥</mo></mrow><mn id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.3" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.3" maxsize="160%" minsize="160%" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E4X.2.1.1.m1.9.9.1.2" lspace="0em" xref="S3.E4X.2.1.1.m1.9.9.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4X.2.1.1.m1.9b"><apply id="S3.E4X.2.1.1.m1.9.9.1.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1"><times id="S3.E4X.2.1.1.m1.9.9.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.2"></times><apply id="S3.E4X.2.1.1.m1.9.9.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.3"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.9.9.1.1.3.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.3">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.9.9.1.1.3.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.3.2">𝔼</ci><apply id="S3.E4X.2.1.1.m1.7.7.7.8.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.7.7.7.8a.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.3">formulae-sequence</csymbol><apply id="S3.E4X.2.1.1.m1.6.6.6.6.1.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1"><csymbol cd="latexml" id="S3.E4X.2.1.1.m1.6.6.6.6.1.2.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.2">similar-to</csymbol><list id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1"><apply id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1"><times id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.1"></times><ci id="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.1.1.1.2">ℰ</ci><ci id="S3.E4X.2.1.1.m1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.1.1.1.1">𝑥</ci></apply><ci id="S3.E4X.2.1.1.m1.4.4.4.4.cmml" xref="S3.E4X.2.1.1.m1.4.4.4.4">italic-ϵ</ci></list><apply id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3"><times id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.1.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.1"></times><ci id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.2.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.2">𝒩</ci><interval closure="open" id="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.1.cmml" xref="S3.E4X.2.1.1.m1.6.6.6.6.1.3.3.2"><cn id="S3.E4X.2.1.1.m1.2.2.2.2.cmml" type="integer" xref="S3.E4X.2.1.1.m1.2.2.2.2">0</cn><cn id="S3.E4X.2.1.1.m1.3.3.3.3.cmml" type="integer" xref="S3.E4X.2.1.1.m1.3.3.3.3">1</cn></interval></apply></apply><list id="S3.E4X.2.1.1.m1.7.7.7.7.2.3.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.2"><ci id="S3.E4X.2.1.1.m1.5.5.5.5.cmml" xref="S3.E4X.2.1.1.m1.5.5.5.5">𝑡</ci><apply id="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.2">𝒄</ci><ci id="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.1.1.3">𝑝</ci></apply><apply id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.1.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.2.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.2">𝒄</ci><ci id="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.3.cmml" xref="S3.E4X.2.1.1.m1.7.7.7.7.2.2.2.3">𝑓</ci></apply></list></apply></apply><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1"><csymbol cd="latexml" id="S3.E4X.2.1.1.m1.9.9.1.1.1.2.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1">subscript</csymbol><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.2.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.2">delimited-∥∥</csymbol><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1"><minus id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.4.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.4"></minus><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.5.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.5">italic-ϵ</ci><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3"><times id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.4"></times><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.2">italic-ϵ</ci><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.3.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.5.3">𝜃</ci></apply><vector id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.4.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3"><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑧</ci><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑡</ci></apply><ci id="S3.E4X.2.1.1.m1.8.8.cmml" xref="S3.E4X.2.1.1.m1.8.8">𝑡</ci><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.2">𝒄</ci><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.2.2.2.2.3">𝑝</ci></apply><apply id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3"><csymbol cd="ambiguous" id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3">subscript</csymbol><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.2">𝒄</ci><ci id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.1.1.1.3.3.3.3.3">𝑓</ci></apply></vector></apply></apply></apply><cn id="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.3.cmml" type="integer" xref="S3.E4X.2.1.1.m1.9.9.1.1.1.1.1.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4X.2.1.1.m1.9c">\displaystyle\mathbb{E}_{\mathcal{E}(x),\epsilon\sim\mathcal{N}(0,1),t,\bm{c}_%
{p},\bm{c}_{f}}\Bigl{[}\lVert\epsilon-\epsilon_{\theta}(z_{t},t,\bm{c}_{p},\bm%
{c}_{f})\rVert_{1}\Bigr{]}.</annotation><annotation encoding="application/x-llamapun" id="S3.E4X.2.1.1.m1.9d">blackboard_E start_POSTSUBSCRIPT caligraphic_E ( italic_x ) , italic_ϵ ∼ caligraphic_N ( 0 , 1 ) , italic_t , bold_italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , bold_italic_c start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_POSTSUBSCRIPT [ ∥ italic_ϵ - italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_t , bold_italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT , bold_italic_c start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ) ∥ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(4)</span></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S3.SS3.p1.2">This integration adds a flexible mechanism to MAISI for controlling the generation of 3D anatomical structures. By injecting task-specific conditions, MAISI can be fine-tuned to meet the specific needs of various medical imaging tasks without retraining the two foundation models, making it a versatile tool for various medical image synthesis tasks.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets and Implementation Details</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To develop and evaluate the proposed MAISI framework, we curate a large-scale medical imaging dataset from publicly available datasets to capture a diverse range of anatomical structures, imaging conditions, and disease states. These datasets are integral to training the three networks within the MAISI framework.
The <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Volume Compression Network</span> (MAISI VAE) is trained on a dataset comprising 37,243 CT volumes for training and 1,963 CT volumes for validation, covering the chest, abdomen, and head and neck regions. Additionally, we include 17,887 MRI volumes for training and 940 MRI volumes for validation, spanning the brain, skull-stripped brain, chest, and below-abdomen regions to potentially support MRI modality in future work.
The <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">Latent DM</span> (MAISI Diffusion Model) was trained using 10,277 CT volumes sourced from multiple public datasets. These datasets are chosen to represent various clinical scenarios, including different body regions and pathological conditions. Including diverse voxel spacings and anatomical regions as conditional inputs during training is essential to ensure the model’s ability to generate high-fidelity anatomical structures with flexible dimensions. For compatibility with the shape requirement of U-Net <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib50" title="">50</a>]</cite>, we resample the dimensions of volumes to the multiples of 128 in this stage. Supplementary Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.F1" title="Figure S1 ‣ A.2 MAISI Diffusion ‣ Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">S1</span></a> visualizes the characteristics and spatial complexity of the data involved in training the diffusion model.
The <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.3">ControlNet</span> part was further trained using subsets of the datasets used for the diffusion model based on different downstream tasks, with additional annotations such as segmentation masks and tumor labels. For example, segmentation masks with 127 anatomical structures are derived from annotated ground truth or pre-trained models, such as TotalSegmentator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib62" title="">62</a>]</cite> and VISTA3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib26" title="">26</a>]</cite>. These additional annotations allow ControlNet to provide fine-grained control over the generation process, enabling tasks such as conditional generation from segmentation masks and tumor inpainting. More details about dataset creation for three development stages can be found in Supplementary Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1" title="Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">A</span></a>. We implement all networks using PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib2" title="">2</a>]</cite> and MONAI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib6" title="">6</a>]</cite>. The models are trained using the NVIDIA V100 and A100 GPUs. We utilize a quality check function to evaluate the generated images used in downstream tasks, which is designed to verify that the median Hounsfield Units (HU) intensity values for major organs in the CT images are within the established normal range from training data. More details about model training are provided in Supplementary Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A2" title="Appendix B Additional Implementation Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation of MAISI VAE</h3>
<figure class="ltx_table" id="S4.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.4" style="width:214.6pt;height:79.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-61.9pt,23.0pt) scale(0.634334042440456,0.634334042440456) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T1.4.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.4.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.4.5">Dataset</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.4.6">Model</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1">LPIPS <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.1.1.1.1.m1.1"><semantics id="S4.T1.1.1.1.1.m1.1a"><mo id="S4.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S4.T1.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.2.2.2.2">SSIM <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.2.2.2.2.m1.1"><semantics id="S4.T1.2.2.2.2.m1.1a"><mo id="S4.T1.2.2.2.2.m1.1.1" stretchy="false" xref="S4.T1.2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.2.2.2.2.m1.1b"><ci id="S4.T1.2.2.2.2.m1.1.1.cmml" xref="S4.T1.2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.2.2.2.2.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.3.3.3.3">PSNR <math alttext="\uparrow" class="ltx_Math" display="inline" id="S4.T1.3.3.3.3.m1.1"><semantics id="S4.T1.3.3.3.3.m1.1a"><mo id="S4.T1.3.3.3.3.m1.1.1" stretchy="false" xref="S4.T1.3.3.3.3.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S4.T1.3.3.3.3.m1.1b"><ci id="S4.T1.3.3.3.3.m1.1.1.cmml" xref="S4.T1.3.3.3.3.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.3.3.3.3.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.3.3.3.3.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.4.4">GPU <math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T1.4.4.4.4.m1.1"><semantics id="S4.T1.4.4.4.4.m1.1a"><mo id="S4.T1.4.4.4.4.m1.1.1" stretchy="false" xref="S4.T1.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T1.4.4.4.4.m1.1b"><ci id="S4.T1.4.4.4.4.m1.1.1.cmml" xref="S4.T1.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T1.4.4.4.4.m1.1d">↓</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.5.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.5.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.4.4.5.1.1.1">MSD Task07</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.5.1.2">MAISI VAE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.5.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.5.1.3.1">0.038</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.5.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.5.1.4.1">0.978</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.5.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.5.1.5.1">37.266</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.4.5.1.6"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.5.1.6.1">0h</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.6.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.2.1">Dedicated VAE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.2.2">0.047</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.2.3">0.971</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.6.2.4">34.750</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.6.2.5">619h</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.7.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.7.3.1" rowspan="2"><span class="ltx_text" id="S4.T1.4.4.7.3.1.1">MSD Task08</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.7.3.2">MAISI VAE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.7.3.3">0.046</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.7.3.4">0.970</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.7.3.5">36.559</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.4.7.3.6"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.7.3.6.1">0h</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.8.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.8.4.1">Dedicated VAE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.8.4.2"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.4.2.1">0.041</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.8.4.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.4.3.1">0.973</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.4.8.4.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.8.4.4.1">37.110</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.4.8.4.5">669h</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.9.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_tt" id="S4.T1.4.4.9.5.1" rowspan="2"><span class="ltx_text" id="S4.T1.4.4.9.5.1.1">Brats18</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.9.5.2">MAISI VAE</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.9.5.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.9.5.3.1">0.026</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.9.5.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.9.5.4.1">0.0977</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T1.4.4.9.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.9.5.5.1">39.003</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T1.4.4.9.5.6"><span class="ltx_text ltx_font_bold" id="S4.T1.4.4.9.5.6.1">0h</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.10.6">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.4.10.6.1">Dedicated VAE</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.4.10.6.2">0.030</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.4.10.6.3">0.0975</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.4.4.10.6.4">38.971</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T1.4.4.10.6.5">672h</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.6.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.7.2" style="font-size:90%;">Performance comparison of the MAISI VAE model on out-of-distribution datasets versus dedicated VAE models. The “GPU” column shows additional GPU hours for training with one 32G V100 GPU.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">To demonstrate the robustness and generalizability of the MAISI VAE model as a foundational model, we test its performance on several out-of-distribution datasets (<em class="ltx_emph ltx_font_italic" id="S4.SS2.p1.1.1">i.e</em>.<span class="ltx_text" id="S4.SS2.p1.1.2"></span>, unseen during training), including MSD Pancreas Tumor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (MSD Task07), MSD Hepatic Vessels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (MSD Task08), and BraTS18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib4" title="">4</a>]</cite> (post-contrast T1-weighted MRI). Notably, this application required no additional training, resulting in eliminating any associated training costs of GPU hours. For comparison, we also train the dedicated VAE models separately on each dataset using 80% of the data, with the same data augmentation techniques and hyper-parameters as those employed for the MAISI VAE training, to establish a benchmark for dedicated VAE models.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">The results from testing on the remaining 20% of the data, shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.T1" title="Table 1 ‣ 4.2 Evaluation of MAISI VAE ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">1</span></a>, revealed that the MAISI VAE model achieved comparable results without additional GPU resource expenditure. This underscores the model’s cost-effectiveness and practicality, suggesting its potential to assist the research community in optimizing resource utilization while maintaining the model’s performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Evaluation of MAISI Diffusion Model</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Synthesis quality.</span> We assess the synthesis quality of the standalone MAISI DM by conducting comparisons with several established baseline methods, including DDPM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib29" title="">29</a>]</cite>, LDM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a>]</cite>, and HA-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib61" title="">61</a>]</cite>. The first evaluation focuses on comparing the fidelity of images generated by our model against those produced by the HA-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib61" title="">61</a>]</cite>, utilizing its publicly available trained weights<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/batmanlab/HA-GAN</span></span></span>. Given that HA-GAN  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib61" title="">61</a>]</cite> specifically targets CT images of the chest region, we curate a collection of chest CT datasets for this analysis, including MSD Lung Tumor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (MSD Task06), LIDC-IDRI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib24" title="">24</a>]</cite>, and TCIA COVID-19 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib25" title="">25</a>]</cite>. These datasets provide a diverse range of imaging conditions and pathology, enriching the comparative study. We use the Fréchet Inception Distance (FID) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib27" title="">27</a>]</cite> as the metric for evaluating the similarity between the distributions of generated images and real counterparts from varied sources. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.T2" title="Table 2 ‣ 4.3 Evaluation of MAISI Diffusion Model ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">2</span></a> presents the average FID for both real and synthesized images across the three datasets. Notably, the MAISI DM significantly outperforms HA-GAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib61" title="">61</a>]</cite> in all datasets, demonstrating its capability to generate images with a much closer appearance to real data.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T2.1" style="width:214.6pt;height:57.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-95.5pt,25.4pt) scale(0.52925101730572,0.52925101730572) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S4.T2.1.1.1.1">
<span class="ltx_text" id="S4.T2.1.1.1.1.1" style="font-size:90%;">FID </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T2.1.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.1.m1.1a"><mo id="S4.T2.1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T2.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.1b"><ci id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.1.m1.1d">↓</annotation></semantics></math><span class="ltx_text" id="S4.T2.1.1.1.1.2" style="font-size:90%;"> (Avg.)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.2"><span class="ltx_text" id="S4.T2.1.1.1.2.1" style="font-size:90%;">MSD Task 06</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.1.1.1.3"><span class="ltx_text" id="S4.T2.1.1.1.3.1" style="font-size:90%;">LIDC-IDRI</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.4"><span class="ltx_text" id="S4.T2.1.1.1.4.1" style="font-size:90%;">TCIA
COVID-19</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.1.2.1.1" rowspan="3"><span class="ltx_text" id="S4.T2.1.1.2.1.1.1" style="font-size:90%;">Real</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.1.2.1.2"><span class="ltx_text" id="S4.T2.1.1.2.1.2.1" style="font-size:90%;">MSD Task06</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.1.2.1.3"><span class="ltx_text" id="S4.T2.1.1.2.1.3.1" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.1.2.1.4"><span class="ltx_text" id="S4.T2.1.1.2.1.4.1" style="font-size:90%;">3.987</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.2.1.5"><span class="ltx_text" id="S4.T2.1.1.2.1.5.1" style="font-size:90%;">1.858</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3.2.1"><span class="ltx_text" id="S4.T2.1.1.3.2.1.1" style="font-size:90%;">LIDC-IDRI</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3.2.2"><span class="ltx_text" id="S4.T2.1.1.3.2.2.1" style="font-size:90%;">3.987</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.3.2.3"><span class="ltx_text" id="S4.T2.1.1.3.2.3.1" style="font-size:90%;">–</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.3.2.4"><span class="ltx_text" id="S4.T2.1.1.3.2.4.1" style="font-size:90%;">4.744</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4.3.1"><span class="ltx_text" id="S4.T2.1.1.4.3.1.1" style="font-size:90%;">TCIA COVID-19</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4.3.2"><span class="ltx_text" id="S4.T2.1.1.4.3.2.1" style="font-size:90%;">1.858</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4.3.3"><span class="ltx_text" id="S4.T2.1.1.4.3.3.1" style="font-size:90%;">4.744</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.3.4"><span class="ltx_text" id="S4.T2.1.1.4.3.4.1" style="font-size:90%;">–</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.1.5.4.1" rowspan="2"><span class="ltx_text" id="S4.T2.1.1.5.4.1.1" style="font-size:90%;">Synthesis</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5.4.2">
<span class="ltx_text" id="S4.T2.1.1.5.4.2.1" style="font-size:90%;">HA-GAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T2.1.1.5.4.2.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib61" title="">61</a><span class="ltx_text" id="S4.T2.1.1.5.4.2.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5.4.3"><span class="ltx_text" id="S4.T2.1.1.5.4.3.1" style="font-size:90%;">98.208</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5.4.4"><span class="ltx_text" id="S4.T2.1.1.5.4.4.1" style="font-size:90%;">116.260</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.4.5"><span class="ltx_text" id="S4.T2.1.1.5.4.5.1" style="font-size:90%;">98.064</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.1.6.5.1"><span class="ltx_text" id="S4.T2.1.1.6.5.1.1" style="font-size:90%;">MAISI DM</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.1.6.5.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.5.2.1" style="font-size:90%;">4.349</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.1.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.5.3.1" style="font-size:90%;">6.200</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T2.1.1.6.5.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.5.4.1" style="font-size:90%;">8.346</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Fréchet Inception Distance of the
MAISI model and the baseline method using its
released checkpoint with multiple public datasets
as the references.</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T3.4" style="width:214.6pt;height:44.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-111.1pt,22.9pt) scale(0.491247051377268,0.491247051377268) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.4.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.4.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.4.4.4.5"><span class="ltx_text" id="S4.T3.4.4.4.5.1" style="font-size:90%;">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.1.1">
<span class="ltx_text" id="S4.T3.1.1.1.1.1" style="font-size:90%;">FID </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.1.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.1.m1.1a"><mo id="S4.T3.1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.1.m1.1b"><ci id="S4.T3.1.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.1.m1.1d">↓</annotation></semantics></math><span class="ltx_text" id="S4.T3.1.1.1.1.2" style="font-size:90%;"> (Axial)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.2.2.2.2">
<span class="ltx_text" id="S4.T3.2.2.2.2.1" style="font-size:90%;">FID </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.2.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.2.m1.1a"><mo id="S4.T3.2.2.2.2.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.2.2.2.2.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.2.m1.1b"><ci id="S4.T3.2.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.2.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.2.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.2.m1.1d">↓</annotation></semantics></math><span class="ltx_text" id="S4.T3.2.2.2.2.2" style="font-size:90%;"> (Sagittal)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.3.3.3.3">
<span class="ltx_text" id="S4.T3.3.3.3.3.1" style="font-size:90%;">FID </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.3.3.3.3.m1.1"><semantics id="S4.T3.3.3.3.3.m1.1a"><mo id="S4.T3.3.3.3.3.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.3.3.3.3.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.3.3.m1.1b"><ci id="S4.T3.3.3.3.3.m1.1.1.cmml" xref="S4.T3.3.3.3.3.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.3.3.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.3.3.m1.1d">↓</annotation></semantics></math><span class="ltx_text" id="S4.T3.3.3.3.3.2" style="font-size:90%;"> (Coronal)</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.4.4.4.4">
<span class="ltx_text" id="S4.T3.4.4.4.4.1" style="font-size:90%;">FID </span><math alttext="\downarrow" class="ltx_Math" display="inline" id="S4.T3.4.4.4.4.m1.1"><semantics id="S4.T3.4.4.4.4.m1.1a"><mo id="S4.T3.4.4.4.4.m1.1.1" mathsize="90%" stretchy="false" xref="S4.T3.4.4.4.4.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.4.4.m1.1b"><ci id="S4.T3.4.4.4.4.m1.1.1.cmml" xref="S4.T3.4.4.4.4.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.4.4.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.4.4.m1.1d">↓</annotation></semantics></math><span class="ltx_text" id="S4.T3.4.4.4.4.2" style="font-size:90%;"> (Avg.)</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.4.4.5.1">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.4.4.5.1.1">
<span class="ltx_text" id="S4.T3.4.4.5.1.1.1" style="font-size:90%;">DDPM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.4.4.5.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib29" title="">29</a><span class="ltx_text" id="S4.T3.4.4.5.1.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.4.4.5.1.2"><span class="ltx_text" id="S4.T3.4.4.5.1.2.1" style="font-size:90%;">18.524</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.4.4.5.1.3"><span class="ltx_text" id="S4.T3.4.4.5.1.3.1" style="font-size:90%;">23.696</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T3.4.4.5.1.4"><span class="ltx_text" id="S4.T3.4.4.5.1.4.1" style="font-size:90%;">25.604</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T3.4.4.5.1.5"><span class="ltx_text" id="S4.T3.4.4.5.1.5.1" style="font-size:90%;">22.608</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.6.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.6.2.1">
<span class="ltx_text" id="S4.T3.4.4.6.2.1.1" style="font-size:90%;">LDM </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.4.4.6.2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib49" title="">49</a><span class="ltx_text" id="S4.T3.4.4.6.2.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.6.2.2"><span class="ltx_text" id="S4.T3.4.4.6.2.2.1" style="font-size:90%;">16.853</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.6.2.3"><span class="ltx_text" id="S4.T3.4.4.6.2.3.1" style="font-size:90%;">10.191</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.6.2.4"><span class="ltx_text" id="S4.T3.4.4.6.2.4.1" style="font-size:90%;">10.093</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.6.2.5"><span class="ltx_text" id="S4.T3.4.4.6.2.5.1" style="font-size:90%;">12.379</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.7.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.7.3.1">
<span class="ltx_text" id="S4.T3.4.4.7.3.1.1" style="font-size:90%;">HA-GAN </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.4.4.7.3.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib61" title="">61</a><span class="ltx_text" id="S4.T3.4.4.7.3.1.3.2" style="font-size:90%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.7.3.2"><span class="ltx_text" id="S4.T3.4.4.7.3.2.1" style="font-size:90%;">17.432</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.7.3.3"><span class="ltx_text" id="S4.T3.4.4.7.3.3.1" style="font-size:90%;">10.266</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.7.3.4"><span class="ltx_text" id="S4.T3.4.4.7.3.4.1" style="font-size:90%;">13.572</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.4.7.3.5"><span class="ltx_text" id="S4.T3.4.4.7.3.5.1" style="font-size:90%;">13.757</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.8.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.4.4.8.4.1"><span class="ltx_text" id="S4.T3.4.4.8.4.1.1" style="font-size:90%;">MAISI DM</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.4.4.8.4.2"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.8.4.2.1" style="font-size:90%;">3.301</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.4.4.8.4.3"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.8.4.3.1" style="font-size:90%;">5.838</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.4.4.8.4.4"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.8.4.4.1" style="font-size:90%;">9.109</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S4.T3.4.4.8.4.5"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.8.4.5.1" style="font-size:90%;">6.083</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 3: </span>Fréchet Inception Distance across three views between MAISI DM and retrained baseline methods using the unseen dataset autoPET 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib20" title="">20</a>]</cite> as the reference.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In addition, we retrain all baseline methods using our large-scale datasets, described in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.SS1" title="4.1 Datasets and Implementation Details ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">4.1</span></a>. For a more comprehensive evaluation of synthesis quality, we utilize an unseen dataset autoPET 2023 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib20" title="">20</a>]</cite> as the reference to conduct synthesis quality evaluation. This dataset encompasses whole-body CT scans from patients with various types of cancer and negative controls.
Results in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.T3" title="Table 3 ‣ 4.3 Evaluation of MAISI Diffusion Model ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">3</span></a> demonstrate that our MAISI DM surpasses the retrained baseline models in generating high-quality images in the external evaluation. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.F4" title="Figure 4 ‣ 4.3 Evaluation of MAISI Diffusion Model ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">4</span></a>
presents a visual comparison illustrating that the high-resolution images synthesized by the MAISI DM show improved detail and a more precise representation of global anatomical structures compared to baseline methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">Response to primary conditions.</span> Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.F5" title="Figure 5 ‣ 4.3 Evaluation of MAISI Diffusion Model ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the model’s adaptability to different body regions and voxel spacing conditions. The MAISI model effectively generates anatomically consistent and high-quality images across different primary conditions <math alttext="\bm{c}_{p}" class="ltx_Math" display="inline" id="S4.SS3.p3.1.m1.1"><semantics id="S4.SS3.p3.1.m1.1a"><msub id="S4.SS3.p3.1.m1.1.1" xref="S4.SS3.p3.1.m1.1.1.cmml"><mi id="S4.SS3.p3.1.m1.1.1.2" xref="S4.SS3.p3.1.m1.1.1.2.cmml">𝒄</mi><mi id="S4.SS3.p3.1.m1.1.1.3" xref="S4.SS3.p3.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p3.1.m1.1b"><apply id="S4.SS3.p3.1.m1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS3.p3.1.m1.1.1.1.cmml" xref="S4.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S4.SS3.p3.1.m1.1.1.2.cmml" xref="S4.SS3.p3.1.m1.1.1.2">𝒄</ci><ci id="S4.SS3.p3.1.m1.1.1.3.cmml" xref="S4.SS3.p3.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p3.1.m1.1c">\bm{c}_{p}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p3.1.m1.1d">bold_italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math>, demonstrating its flexibility and control over synthesized images.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="366" id="S4.F4.g1" src="x4.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;">Qualitative comparison of generated images between retrained baseline methods using our large-scale datasets and MAISI DM.
</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="174" id="S4.F5.g1" src="x5.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.4.2.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.2.1" style="font-size:90%;">The sagittal view of generated CT images from MAISI DM under different primary conditions <math alttext="\bm{c}_{p}" class="ltx_Math" display="inline" id="S4.F5.2.1.m1.1"><semantics id="S4.F5.2.1.m1.1b"><msub id="S4.F5.2.1.m1.1.1" xref="S4.F5.2.1.m1.1.1.cmml"><mi id="S4.F5.2.1.m1.1.1.2" xref="S4.F5.2.1.m1.1.1.2.cmml">𝒄</mi><mi id="S4.F5.2.1.m1.1.1.3" xref="S4.F5.2.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F5.2.1.m1.1c"><apply id="S4.F5.2.1.m1.1.1.cmml" xref="S4.F5.2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.F5.2.1.m1.1.1.1.cmml" xref="S4.F5.2.1.m1.1.1">subscript</csymbol><ci id="S4.F5.2.1.m1.1.1.2.cmml" xref="S4.F5.2.1.m1.1.1.2">𝒄</ci><ci id="S4.F5.2.1.m1.1.1.3.cmml" xref="S4.F5.2.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F5.2.1.m1.1d">\bm{c}_{p}</annotation><annotation encoding="application/x-llamapun" id="S4.F5.2.1.m1.1e">bold_italic_c start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math>. From left to right, the voxel spacing is first increased by 50%, followed by a doubling of the output dimensions. The coverage of the generated CT images gradually expands, starting from a local region of the abdomen and extending to the entire chest-abdomen region.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Data Augmentation in Downstream Tasks</h3>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="520" id="S4.F6.g1" src="x6.png" width="951"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.4.2" style="font-size:90%;">The 5-fold averaged DSC of data augmentation experiments using synthetic data across 5 tumor types. The percentage of relative improvement compared to <span class="ltx_text ltx_font_bold" id="S4.F6.4.2.1">Real Only</span> experiments is shown in green above each bar plot. All reported improvements are significant under the Wilcoxon signed rank test.
</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">One of the critical applications of generative models in medical imaging is data augmentation for training deep learning models. To assess the effectiveness of synthetic images in improving model performance, especially for rare medical conditions, we integrate synthetic data generated by MAISI into a standard training pipeline and evaluate it across five tumor types. Specifically, we employ the Auto3DSeg<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://monai.io/apps/auto3dseg</span></span></span> pipeline—an auto-configuration solution for training medical image segmentation models—to train models on the MSD Task03 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (liver tumor), Task06 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (lung tumor), Task07 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (pancreas tumor), Task10 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (colon tumor), and an in-house bone lesion dataset. We conduct experiments by training segmentation models either using only real data (referred to <span class="ltx_text ltx_font_bold" id="S4.SS4.p1.1.1">Real Only</span> in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.F6" title="Figure 6 ‣ 4.4 Data Augmentation in Downstream Tasks ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>) or by incorporating synthetic data from different models, thereby demonstrating the impact of synthetic data on data augmentation. To ensure robustness, we performed 5-fold cross-validation and reported the average Dice Similarity Coefficient (DSC) on the testing set across the five folds.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">As discussed in Sec.<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S3.SS3" title="3.3 Additional Conditioning Mechanisms ‣ 3 Methodology ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">3.3</span></a>, the integration of ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite> introduces a flexible mechanism in MAISI, enabling the incorporation of task-specific conditions. To illustrate its versatility, we trained ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite> for two distinct tasks aimed at generating synthetic data for augmentation purposes. The first task (denoted as <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">MAISI CT Generation</span> in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.F6" title="Figure 6 ‣ 4.4 Data Augmentation in Downstream Tasks ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>) is conditional generation from segmentation masks of 127 anatomical structures, including the five tumor types mentioned earlier. This approach allowed us to generate synthetic data by augmenting real patient tumor masks corresponding to each tumor type. The second task (denoted as <span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.2">MAISI Inpainting</span> in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.F6" title="Figure 6 ‣ 4.4 Data Augmentation in Downstream Tasks ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>) involves training a tumor inpainting model designed to simultaneously support liver, pancreas, and lung tumors, following the setting in<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib10" title="">10</a>]</cite>. The tumor inpainting model requires a function to simulate tumor masks for adding synthetic tumors into healthy patient data. However, simulating tumors with irregular shapes, such as bone lesions and colon tumors, poses significant challenges. For a comparative analysis, we benchmark against the state-of-the-art tumor synthesis method,
DiffTumor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib10" title="">10</a>]</cite>, using their released model<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://github.com/MrGiovanni/DiffTumor</span></span></span> which supports liver and pancreas tumors among five tumor types in our experiments.</p>
</div>
<div class="ltx_para" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.1">Results shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.F6" title="Figure 6 ‣ 4.4 Data Augmentation in Downstream Tasks ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>(a)<math alttext="\sim" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><mo id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><csymbol cd="latexml" id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">∼</annotation></semantics></math>(e) indicate prominent improvements in DSC scores across all tumor types when incorporating synthetic data from our two augmentation tasks. Specifically, the MAISI CT Generation results in an average DSC improvement of 4% across the five tumor types. The MAISI Inpainting demonstrated a more substantial average improvement of 6.5% in DSC for liver, lung, and pancreas tumors, performing comparably or better than the DiffTumor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib10" title="">10</a>]</cite>, which trains dedicated synthesis models for each tumor type. Additionally, we conduct an out-of-distribution evaluation by testing tumor segmentation models trained on MSD Task03 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> on 303 liver tumor samples from MSD Task08 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite>. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.F6" title="Figure 6 ‣ 4.4 Data Augmentation in Downstream Tasks ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>(f), models incorporating synthetic data consistently show greater relative performance improvements compared to those evaluated within their original training dataset in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.F6" title="Figure 6 ‣ 4.4 Data Augmentation in Downstream Tasks ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">6</span></a>(b). These findings underscore the effectiveness of synthetic data as a powerful augmentation strategy to bolster the generalizability of segmentation models. More ablation studies and visualization of synthetic data can be found in Supplementary Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A3" title="Appendix C Supplementary Experiment Results ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion and Limitations</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">While the proposed MAISI demonstrates great potentials in generating high-quality CT images, it is essential to recognize its limitations and potential societal impacts. While MAISI shows robust performance across various datasets, its ability to accurately represent demographic variations (such as age, ethnicity, and gender differences) in generated anatomy has not been extensively validated. Future studies can focus on ensuring that synthetic data adequately captures this diversity to avoid bias in downstream applications. The capabilities of generating high-resolution images of MAISI, while innovative, still demand substantial computation resources. This could limit accessibility for researchers and institutions with less computational power, potentially widening the gap between high-resource and low-resource entities. Future efforts can focus on improving the accessibility of MAISI, particularly in resource-constrained environments.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we propose MAISI, a novel framework for generating high-resolution 3D CT volumes using a combination of foundation models and ControlNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib70" title="">70</a>]</cite>. MAISI aims to provide an adaptable and versatile solution for generating anatomically accurate images. Our experiments demonstrate that MAISI can produce realistic CT images with flexible volume dimensions and voxel spacing, offering promising potential to augment medical datasets and improve the performance of downstream tasks.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Waqar Ahmad, Hazrat Ali, Zubair Shah, and Shoaib Azmat.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">A new generative adversarial network for medical images super resolution.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">Scientific Reports</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 12(1):9533, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, Geeta Chauhan, Anjali Chourdia, Will Constable, Alban Desmaison, Zachary DeVito, Elias Ellison, Will Feng, Jiong Gong, Michael Gschwind, Brian Hirsh, Sherlock Huang, Kshiteej Kalambarkar, Laurent Kirsch, Michael Lazos, Mario Lezcano, Yanbo Liang, Jason Liang, Yinghai Lu, CK Luk, Bert Maher, Yunjie Pan, Christian Puhrsch, Matthias Reso, Mark Saroufim, Marcos Yukio Siraichi, Helen Suk, Michael Suo, Phil Tillet, Eikan Wang, Xiaodong Wang, William Wen, Shunting Zhang, Xu Zhao, Keren Zhou, Richard Zou, Ajit Mathews, Gregory Chanan, Peng Wu, and Soumith Chintala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’24)</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">. ACM, Apr. 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett A Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald M Summers, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">The medical segmentation decathlon.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">Nature communications</span><span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">, 13(1):4128, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Spyridon Bakas, Mauricio Reyes, Andras Jakab, Stefan Bauer, Markus Rempfler, Alessandro Crimi, Russell Takeshi Shinohara, Christoph Berger, Sung Min Ha, Martin Rozycki, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">arXiv preprint arXiv:1811.02629</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng, and Stan Z Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">A survey on generative diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">IEEE Transactions on Knowledge and Data Engineering</span><span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
M Jorge Cardoso, Wenqi Li, Richard Brown, Nic Ma, Eric Kerfoot, Yiheng Wang, Benjamin Murrey, Andriy Myronenko, Can Zhao, Dong Yang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Monai: An open-source framework for deep learning in healthcare.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">arXiv preprint arXiv:2211.02701</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
M Jorge Cardoso, Carole H Sudre, Marc Modat, and Sebastien Ourselin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Template-based multimodal joint generative model of brain data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib7.4.2" style="font-size:90%;">Information Processing in Medical Imaging: 24th International Conference, IPMI 2015, Sabhal Mor Ostaig, Isle of Skye, UK, June 28-July 3, 2015, Proceedings 24</span><span class="ltx_text" id="bib.bib7.5.3" style="font-size:90%;">, pages 17–29. Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Agisilaos Chartsias, Thomas Joyce, Rohan Dharmakumar, and Sotirios A Tsaftaris.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Adversarial image synthesis for unpaired multi-modal cardiac data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">Simulation and Synthesis in Medical Imaging: Second International Workshop, SASHIMI 2017, Held in Conjunction with MICCAI 2017, Québec City, QC, Canada, September 10, 2017, Proceedings 2</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, pages 3–13. Springer, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Agisilaos Chartsias, Thomas Joyce, Mario Valerio Giuffrida, and Sotirios A Tsaftaris.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Multimodal mr synthesis via modality-invariant latent representation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 37(3):803–814, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Qi Chen, Xiaoxi Chen, Haorui Song, Zhiwei Xiong, Alan Yuille, Chen Wei, and Zongwei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Towards generalizable tumor synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, pages 11147–11158, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Xiang Chen, Andres Diaz-Pinto, Nishant Ravikumar, and Alejandro F Frangi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Deep learning in medical image registration.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">Progress in Biomedical Engineering</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 3(1):012003, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Generative adversarial networks: An overview.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">IEEE signal processing magazine</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 35(1):53–65, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Diffusion models in vision: A survey.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.3.1" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</span><span class="ltx_text" id="bib.bib13.4.2" style="font-size:90%;">, 45(9):10850–10869, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Mohammad Zalbagi Darestani, Vishwesh Nath, Wenqi Li, Yufan He, Holger R Roth, Ziyue Xu, Daguang Xu, Reinhard Heckel, and Can Zhao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Ir-frestormer: Iterative refinement with fourier-based restormer for accelerated mri reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib14.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</span><span class="ltx_text" id="bib.bib14.5.3" style="font-size:90%;">, pages 7655–7664, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Blake E Dewey, Can Zhao, Jacob C Reinhold, Aaron Carass, Kathryn C Fitzgerald, Elias S Sotirchos, Shiv Saidha, Jiwon Oh, Dzung L Pham, Peter A Calabresi, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Deepharmony: A deep learning approach to contrast harmonization across scanner changes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.3.1" style="font-size:90%;">Magnetic resonance imaging</span><span class="ltx_text" id="bib.bib15.4.2" style="font-size:90%;">, 64:160–170, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Prafulla Dhariwal and Alexander Nichol.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Diffusion models beat gans on image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib16.4.2" style="font-size:90%;">, 34:8780–8794, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Yilun Du and Igor Mordatch.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Implicit generation and modeling with energy based models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib17.4.2" style="font-size:90%;">, 32, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">The llama 3 herd of models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">arXiv preprint arXiv:2407.21783</span><span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Patrick Esser, Robin Rombach, and Bjorn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Taming transformers for high-resolution image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib19.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib19.5.3" style="font-size:90%;">, pages 12873–12883, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Sergios Gatidis, Tobias Hepp, Marcel Früh, Christian La Fougère, Konstantin Nikolaou, Christina Pfannenberg, Bernhard Schölkopf, Thomas Küstner, Clemens Cyran, and Daniel Rubin.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">A whole-body fdg-pet/ct dataset with manually annotated tumor lesions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.3.1" style="font-size:90%;">Scientific Data</span><span class="ltx_text" id="bib.bib20.4.2" style="font-size:90%;">, 9(1):601, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Pengfei Guo, Puyang Wang, Rajeev Yasarla, Jinyuan Zhou, Vishal M Patel, and Shanshan Jiang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Anatomic and molecular mr image synthesis using confidence guided cnns.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib22.4.2" style="font-size:90%;">, 40(10):2832–2844, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Ibrahim Ethem Hamamci, Sezgin Er, Anjany Sekuboyina, Enis Simsar, Alperen Tezcan, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Furkan Almas, Irem Dogan, Muhammed Furkan Dasdelen, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Generatect: Text-conditional generation of 3d chest ct volumes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1" style="font-size:90%;">arXiv preprint arXiv:2305.16037</span><span class="ltx_text" id="bib.bib23.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Matthew C Hancock and Jerry F Magnan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Lung nodule malignancy classification using only radiologist-quantified image features as inputs to statistical learning algorithms: probing the lung image database consortium dataset with two statistical learning methods.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">Journal of Medical Imaging</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 3(4):044504–044504, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Stephanie A Harmon, Thomas H Sanford, Sheng Xu, Evrim B Turkbey, Holger Roth, Ziyue Xu, Dong Yang, Andriy Myronenko, Victoria Anderson, Amel Amalou, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Artificial intelligence for the detection of covid-19 pneumonia on chest ct using multinational datasets.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">Nature communications</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 11(1):4080, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Yufan He, Pengfei Guo, Yucheng Tang, Andriy Myronenko, Vishwesh Nath, Ziyue Xu, Dong Yang, Can Zhao, Benjamin Simon, Mason Belue, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Vista3d: Versatile imaging segmentation and annotation model for 3d computed tomography.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">arXiv preprint arXiv:2406.05285</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Gans trained by a two time-scale update rule converge to a local nash equilibrium.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 30, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Irina Higgins, Loic Matthey, Arka Pal, Christopher P Burgess, Xavier Glorot, Matthew M Botvinick, Shakir Mohamed, and Alexander Lerchner.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">beta-vae: Learning basic visual concepts with a constrained variational framework.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">ICLR (Poster)</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, 3, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Denoising diffusion probabilistic models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 33:6840–6851, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Qixin Hu, Junfei Xiao, Yixiong Chen, Shuwen Sun, Jie-Neng Chen, Alan Yuille, and Zongwei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Synthetic tumors make ai segment tumors better.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">arXiv preprint arXiv:2210.14845</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Yawen Huang, Leandro Beltrachini, Ling Shao, and Alejandro F Frangi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Geometry regularized joint dictionary learning for cross-modality image synthesis in magnetic resonance imaging.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib31.4.2" style="font-size:90%;">Simulation and Synthesis in Medical Imaging: First International Workshop, SASHIMI 2016, Held in Conjunction with MICCAI 2016, Athens, Greece, October 21, 2016, Proceedings 1</span><span class="ltx_text" id="bib.bib31.5.3" style="font-size:90%;">, pages 118–126. Springer, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Yuankai Huo, Zhoubing Xu, Hyeonsoo Moon, Shunxing Bao, Albert Assad, Tamara K Moyo, Michael R Savona, Richard G Abramson, and Bennett A Landman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Synseg-net: Synthetic segmentation without target modality ground truth.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 38(4):1016–1025, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Thomas Joyce, Agisilaos Chartsias, and Sotirios A Tsaftaris.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Robust multi-modal mr image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib33.4.2" style="font-size:90%;">Medical Image Computing and Computer Assisted Intervention- MICCAI 2017: 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part III 20</span><span class="ltx_text" id="bib.bib33.5.3" style="font-size:90%;">, pages 347–355. Springer, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Mintong Kang, Bowen Li, Zengle Zhu, Yongyi Lu, Elliot K Fishman, Alan Yuille, and Zongwei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Label-assemble: Leveraging multiple datasets with partial labels.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">2023 IEEE 20th International Symposium on Biomedical Imaging (ISBI)</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, pages 1–5. IEEE, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Bardia Khosravi, Frank Li, Theo Dapamede, Pouria Rouzrokh, Cooper U Gamble, Hari M Trivedi, Cody C Wyles, Andrew B Sellergren, Saptarshi Purkayastha, Bradley J Erickson, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Synthetically enhanced: unveiling synthetic data’s potential in medical imaging research.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">EBioMedicine</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, 104, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Diederik P Kingma.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Auto-encoding variational bayes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.3.1" style="font-size:90%;">arXiv preprint arXiv:1312.6114</span><span class="ltx_text" id="bib.bib36.4.2" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Diederik P Kingma, Max Welling, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">An introduction to variational autoencoders.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">Foundations and Trends® in Machine Learning</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 12(4):307–392, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Nicholas Konz, Yuwen Chen, Haoyu Dong, and Maciej A Mazurowski.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Anatomically-controllable medical image generation with segmentation-guided diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.3.1" style="font-size:90%;">arXiv preprint arXiv:2402.05210</span><span class="ltx_text" id="bib.bib38.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Yuxiang Lai, Xiaoxi Chen, Angtian Wang, Alan Yuille, and Zongwei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">From pixel to cancer: Cellular automata in computed tomography.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.3.1" style="font-size:90%;">arXiv preprint arXiv:2403.06459</span><span class="ltx_text" id="bib.bib39.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Jie Liu, Yixiao Zhang, Jie-Neng Chen, Junfei Xiao, Yongyi Lu, Bennett A Landman, Yixuan Yuan, Alan Yuille, Yucheng Tang, and Zongwei Zhou.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Clip-driven universal model for organ segmentation and tumor detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib40.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib40.5.3" style="font-size:90%;">, pages 21152–21164, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Xiangde Luo, Wenjun Liao, Jianghong Xiao, Jieneng Chen, Tao Song, Xiaofan Zhang, Kang Li, Dimitris N Metaxas, Guotai Wang, and Shaoting Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">Word: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from ct image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.3.1" style="font-size:90%;">Medical Image Analysis</span><span class="ltx_text" id="bib.bib41.4.2" style="font-size:90%;">, 82:102642, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Fei Lyu, Mang Ye, Andy J Ma, Terry Cheuk-Fung Yip, Grace Lai-Hung Wong, and Pong C Yuen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Learning from synthetic ct images via test-time training for liver tumor segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib42.4.2" style="font-size:90%;">, 41(9):2510–2520, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
Michael I Miller, Gary E Christensen, Yali Amit, and Ulf Grenander.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Mathematical textbook of deformable neuroanatomies.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.3.1" style="font-size:90%;">Proceedings of the National Academy of Sciences</span><span class="ltx_text" id="bib.bib43.4.2" style="font-size:90%;">, 90(24):11944–11948, 1993.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.3.1" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</span><span class="ltx_text" id="bib.bib44.4.2" style="font-size:90%;">, 38(5):4296–4304, Mar. 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">Normalizing flows for probabilistic modeling and inference.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.3.1" style="font-size:90%;">Journal of Machine Learning Research</span><span class="ltx_text" id="bib.bib45.4.2" style="font-size:90%;">, 22(57):1–64, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
Cheng Peng, Pengfei Guo, S Kevin Zhou, Vishal M Patel, and Rama Chellappa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">Towards performant and reliable undersampled mr reconstruction via diffusion model sampling.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib46.4.2" style="font-size:90%;">International Conference on Medical Image Computing and Computer-Assisted Intervention</span><span class="ltx_text" id="bib.bib46.5.3" style="font-size:90%;">, pages 623–633. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
Chi-Hieu Pham, Carlos Tor-Díez, Hélène Meunier, Nathalie Bednarek, Ronan Fablet, Nicolas Passat, and François Rousseau.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">Multiscale brain mri super-resolution using deep 3d convolutional networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.3.1" style="font-size:90%;">Computerized Medical Imaging and Graphics</span><span class="ltx_text" id="bib.bib47.4.2" style="font-size:90%;">, 77:101647, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">Stochastic backpropagation and approximate inference in deep generative models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib48.4.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib48.5.3" style="font-size:90%;">, pages 1278–1286. PMLR, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">High-resolution image synthesis with latent diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib49.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib49.5.3" style="font-size:90%;">, pages 10684–10695, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">U-net: Convolutional networks for biomedical image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib50.4.2" style="font-size:90%;">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</span><span class="ltx_text" id="bib.bib50.5.3" style="font-size:90%;">, pages 234–241. Springer, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.1.1" style="font-size:90%;">
Snehashis Roy, Aaron Carass, and Jerry Prince.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.2.1" style="font-size:90%;">A compressed sensing approach for mr tissue contrast synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib51.4.2" style="font-size:90%;">Information Processing in Medical Imaging: 22nd International Conference, IPMI 2011, Kloster Irsee, Germany, July 3-8, 2011. Proceedings 22</span><span class="ltx_text" id="bib.bib51.5.3" style="font-size:90%;">, pages 371–383. Springer, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.1.1" style="font-size:90%;">
Snehashis Roy, Aaron Carass, and Jerry L Prince.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.2.1" style="font-size:90%;">Magnetic resonance image example-based contrast synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib52.4.2" style="font-size:90%;">, 32(12):2348–2363, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.1.1" style="font-size:90%;">
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.2.1" style="font-size:90%;">Photorealistic text-to-image diffusion models with deep language understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib53.4.2" style="font-size:90%;">, 35:36479–36494, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.1.1" style="font-size:90%;">
Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.2.1" style="font-size:90%;">Image super-resolution via iterative refinement.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.3.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span class="ltx_text" id="bib.bib54.4.2" style="font-size:90%;">, 45(4):4713–4726, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.1.1" style="font-size:90%;">
Fangxin Shang, Jie Fu, Yehui Yang, Haifeng Huang, Junwei Liu, and Lei Ma.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.2.1" style="font-size:90%;">Synfundus: A synthetic fundus images dataset with millions of samples and multi-disease annotations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib55.3.1" style="font-size:90%;">arXiv preprint arXiv:2312.00377</span><span class="ltx_text" id="bib.bib55.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.1.1" style="font-size:90%;">
Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G Schwarz, Matthew L Senjem, Jeffrey L Gunter, Katherine P Andriole, and Mark Michalski.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.2.1" style="font-size:90%;">Medical image synthesis for data augmentation and anonymization using generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib56.4.2" style="font-size:90%;">Simulation and Synthesis in Medical Imaging: Third International Workshop, SASHIMI 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3</span><span class="ltx_text" id="bib.bib56.5.3" style="font-size:90%;">, pages 1–11. Springer, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.1.1" style="font-size:90%;">
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.2.1" style="font-size:90%;">Megatron-lm: Training multi-billion parameter language models using model parallelism.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib57.3.1" style="font-size:90%;">arXiv preprint arXiv:1909.08053</span><span class="ltx_text" id="bib.bib57.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.1.1" style="font-size:90%;">
Satya P Singh, Lipo Wang, Sukrit Gupta, Haveesh Goli, Parasuraman Padmanabhan, and Balázs Gulyás.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.2.1" style="font-size:90%;">3d deep learning on medical images: a review.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.3.1" style="font-size:90%;">Sensors</span><span class="ltx_text" id="bib.bib58.4.2" style="font-size:90%;">, 20(18):5097, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.1.1" style="font-size:90%;">
Youssef Skandarani, Pierre-Marc Jodoin, and Alain Lalande.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.2.1" style="font-size:90%;">Gans for medical image synthesis: An empirical study.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib59.3.1" style="font-size:90%;">Journal of Imaging</span><span class="ltx_text" id="bib.bib59.4.2" style="font-size:90%;">, 9(3):69, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.1.1" style="font-size:90%;">
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.2.1" style="font-size:90%;">Score-based generative modeling through stochastic differential equations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib60.3.1" style="font-size:90%;">arXiv preprint arXiv:2011.13456</span><span class="ltx_text" id="bib.bib60.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.1.1" style="font-size:90%;">
Li Sun, Junxiang Chen, Yanwu Xu, Mingming Gong, Ke Yu, and Kayhan Batmanghelich.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.2.1" style="font-size:90%;">Hierarchical amortized gan for 3d high resolution medical image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.3.1" style="font-size:90%;">IEEE journal of biomedical and health informatics</span><span class="ltx_text" id="bib.bib61.4.2" style="font-size:90%;">, 26(8):3966–3975, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.1.1" style="font-size:90%;">
Jakob Wasserthal, Hanns-Christian Breit, Manfred T Meyer, Maurice Pradella, Daniel Hinck, Alexander W Sauter, Tobias Heye, Daniel T Boll, Joshy Cyriac, Shan Yang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.2.1" style="font-size:90%;">Totalsegmentator: robust segmentation of 104 anatomic structures in ct images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib62.3.1" style="font-size:90%;">Radiology: Artificial Intelligence</span><span class="ltx_text" id="bib.bib62.4.2" style="font-size:90%;">, 5(5), 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.1.1" style="font-size:90%;">
Linshan Wu, Jiaxin Zhuang, Xuefeng Ni, and Hao Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.2.1" style="font-size:90%;">Freetumor: Advance tumor segmentation via large-scale tumor synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib63.3.1" style="font-size:90%;">arXiv preprint arXiv:2406.01264</span><span class="ltx_text" id="bib.bib63.4.2" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.1.1" style="font-size:90%;">
Yutong Xie and Quanzheng Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.2.1" style="font-size:90%;">Measurement-conditioned denoising diffusion probabilistic model for under-sampled medical image reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib64.4.2" style="font-size:90%;">International Conference on Medical Image Computing and Computer-Assisted Intervention</span><span class="ltx_text" id="bib.bib64.5.3" style="font-size:90%;">, pages 655–664. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.1.1" style="font-size:90%;">
Heran Yang, Jian Sun, Aaron Carass, Can Zhao, Junghoon Lee, Jerry L Prince, and Zongben Xu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.2.1" style="font-size:90%;">Unsupervised mr-to-ct synthesis using structure-constrained cyclegan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib65.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib65.4.2" style="font-size:90%;">, 39(12):4249–4261, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.1.1" style="font-size:90%;">
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.2.1" style="font-size:90%;">Diffusion models: A comprehensive survey of methods and applications.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib66.3.1" style="font-size:90%;">ACM Computing Surveys</span><span class="ltx_text" id="bib.bib66.4.2" style="font-size:90%;">, 56(4):1–39, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.1.1" style="font-size:90%;">
Chenyu You, Guang Li, Yi Zhang, Xiaoliu Zhang, Hongming Shan, Mengzhou Li, Shenghong Ju, Zhen Zhao, Zhuiyang Zhang, Wenxiang Cong, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.2.1" style="font-size:90%;">Ct super-resolution gan constrained by the identical, residual, and cycle learning ensemble (gan-circle).
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib67.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib67.4.2" style="font-size:90%;">, 39(1):188–203, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.1.1" style="font-size:90%;">
Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.2.1" style="font-size:90%;">Vector-quantized image modeling with improved vqgan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib68.3.1" style="font-size:90%;">arXiv preprint arXiv:2110.04627</span><span class="ltx_text" id="bib.bib68.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.1.1" style="font-size:90%;">
Mahmut Yurt, Salman UH Dar, Aykut Erdem, Erkut Erdem, Kader K Oguz, and Tolga Çukur.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.2.1" style="font-size:90%;">mustgan: multi-stream generative adversarial networks for mr image synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib69.3.1" style="font-size:90%;">Medical image analysis</span><span class="ltx_text" id="bib.bib69.4.2" style="font-size:90%;">, 70:101944, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.1.1" style="font-size:90%;">
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.2.1" style="font-size:90%;">Adding conditional control to text-to-image diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib70.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib70.5.3" style="font-size:90%;">, pages 3836–3847, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.1.1" style="font-size:90%;">
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.2.1" style="font-size:90%;">The unreasonable effectiveness of deep features as a perceptual metric.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib71.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib71.5.3" style="font-size:90%;">, pages 586–595, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.1.1" style="font-size:90%;">
Xiaoman Zhang, Weidi Xie, Chaoqin Huang, Ya Zhang, Xin Chen, Qi Tian, and Yanfeng Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.2.1" style="font-size:90%;">Self-supervised tumor segmentation with sim2real adaptation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib72.3.1" style="font-size:90%;">IEEE Journal of Biomedical and Health Informatics</span><span class="ltx_text" id="bib.bib72.4.2" style="font-size:90%;">, 27(9):4373–4384, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.1.1" style="font-size:90%;">
Zhaoxiang Zhang, Hanqiu Deng, and Xingyu Li.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.2.1" style="font-size:90%;">Unsupervised liver tumor segmentation with pseudo anomaly synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib73.4.2" style="font-size:90%;">International Workshop on Simulation and Synthesis in Medical Imaging</span><span class="ltx_text" id="bib.bib73.5.3" style="font-size:90%;">, pages 86–96. Springer, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.1.1" style="font-size:90%;">
Zizhao Zhang, Lin Yang, and Yefeng Zheng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.2.1" style="font-size:90%;">Translating and segmenting multimodal medical volumes with cycle-and shape-consistency generative adversarial network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib74.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern Recognition</span><span class="ltx_text" id="bib.bib74.5.3" style="font-size:90%;">, pages 9242–9251, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.1.1" style="font-size:90%;">
Can Zhao, Aaron Carass, Junghoon Lee, Yufan He, and Jerry L Prince.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.2.1" style="font-size:90%;">Whole brain segmentation and labeling from ct using synthetic mr images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib75.4.2" style="font-size:90%;">Machine Learning in Medical Imaging: 8th International Workshop, MLMI 2017, Held in Conjunction with MICCAI 2017, Quebec City, QC, Canada, September 10, 2017, Proceedings 8</span><span class="ltx_text" id="bib.bib75.5.3" style="font-size:90%;">, pages 291–298. Springer, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.1.1" style="font-size:90%;">
Can Zhao, Blake E Dewey, Dzung L Pham, Peter A Calabresi, Daniel S Reich, and Jerry L Prince.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.2.1" style="font-size:90%;">Smore: a self-supervised anti-aliasing and super-resolution algorithm for mri using deep learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib76.3.1" style="font-size:90%;">IEEE transactions on medical imaging</span><span class="ltx_text" id="bib.bib76.4.2" style="font-size:90%;">, 40(3):805–817, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.1.1" style="font-size:90%;">
Yi Zhou, Xiaodong He, Shanshan Cui, Fan Zhu, Li Liu, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.2.1" style="font-size:90%;">High-resolution diabetic retinopathy image synthesis manipulated by grading and lesions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib77.4.2" style="font-size:90%;">International conference on medical image computing and computer-assisted intervention</span><span class="ltx_text" id="bib.bib77.5.3" style="font-size:90%;">, pages 505–513. Springer, 2019.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">This supplementary material is organized as follows: Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1" title="Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">A</span></a> provides more details about the datasets utilized in model training. More implantation details about three networks and downstream tumor segmentation tasks are provided in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A2" title="Appendix B Additional Implementation Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">B</span></a>. Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A3" title="Appendix C Supplementary Experiment Results ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">C</span></a> contains additional visualizations of synthetic data and ablation studies.</p>
</div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Dataset Details</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>MAISI VAE</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">For the foundational 3D VAE in MAISI, we include a diverse dataset comprising 37,243 CT volumes for training and 1,963 CT volumes for validation, covering the chest, abdomen, and head and neck regions. Additionally, we include 17,887 MRI volumes for training and 940 MRI volumes for validation, spanning the brain, skull-stripped brain, chest, and below-abdomen regions. The training data were sourced from various repositories, including TCIA COVID-19 Chest CT, TCIA Colon Abdomen CT, MSD03 Liver Abdomen CT, LIDC Chest CT, TCIA Stony Brook COVID Chest CT, NLST Chest CT, TCIA Upenn GBM Brain MR, AOMIC Brain MR, QTIM Brain MR, TCIA Acrin Chest MR, and TCIA Prostate MR. This extensive and varied dataset not only ensures that our model is exposed to a broad range of anatomical regions but also supports its application to both MRI and CT images.</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">The details of MAISI VAE training data are shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.T1" title="Table S1 ‣ A.1 MAISI VAE ‣ Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">S1</span></a>.</p>
</div>
<figure class="ltx_table" id="A1.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T1.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T1.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T1.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T1.2.1.1.1.1">Dataset Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T1.2.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T1.2.1.1.2.1">Number of Training Data</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T1.2.1.1.3"><span class="ltx_text ltx_font_bold" id="A1.T1.2.1.1.3.1">Number of Validation Data</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T1.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T1.2.2.1.1">Covid 19 Chest CT</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.2.2.1.2">722</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.2.2.1.3">49</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.3.2.1">TCIA Colon Abdomen CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.3.2.2">1522</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.3.2.3">77</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.4.3.1">MSD03 Liver Abdomen CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.4.3.2">104</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.4.3.3">0</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.5.4.1">LIDC chest CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.5.4.2">450</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.5.4.3">24</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.6.5.1">TCIA Stony Brook Covid Chest CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.6.5.2">2644</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.6.5.3">139</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.7.6.1">NLST Chest CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.7.6.2">31801</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.7.6.3">1674</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.8.7.1">TCIA Upenn GBM Brain MR (skull-stripped)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.8.7.2">2550</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.8.7.3">134</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.9.8.1">Aomic Brain MR</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.9.8.2">2630</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.9.8.3">138</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.10.9.1">QTIM Brain MR</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.10.9.2">1275</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.10.9.3">67</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.11.10.1">Acrin Chest MR</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.11.10.2">6599</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.11.10.3">347</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.12.11.1">TCIA Prostate MR Below-Abdomen MR</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.12.11.2">928</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.12.11.3">49</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.13.12.1">Aomic Brain MR, skull-stripped</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.13.12.2">2630</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.13.12.3">138</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T1.2.14.13.1">QTIM Brain MR, skull-stripped</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.14.13.2">1275</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T1.2.14.13.3">67</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T1.2.15.14.1"><span class="ltx_text ltx_font_bold" id="A1.T1.2.15.14.1.1">Total CT</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.2.15.14.2">37243</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T1.2.15.14.3">1963</td>
</tr>
<tr class="ltx_tr" id="A1.T1.2.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r" id="A1.T1.2.16.15.1"><span class="ltx_text ltx_font_bold" id="A1.T1.2.16.15.1.1">Total MRI</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T1.2.16.15.2">17887</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A1.T1.2.16.15.3">940</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T1.3.1.1" style="font-size:90%;">Table S1</span>: </span><span class="ltx_text" id="A1.T1.4.2" style="font-size:90%;">MAISI VAE Dataset Information</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>MAISI Diffusion</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">The datasets for developing the Diffusion model used in MAISI comprise 10,277 CT volumes from 24 distinct datasets, encompassing various body regions and disease patterns. Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.T2" title="Table S2 ‣ A.2 MAISI Diffusion ‣ Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">S2</span></a> provides a summary of the number of volumes for each dataset. For compatibility with the shape requirement of the U-shape network, we resample the dimensions of volumes to multiples of 128. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.F1" title="Figure S1 ‣ A.2 MAISI Diffusion ‣ Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">S1</span></a> visualizes the characteristics and spatial complexity of the data involved in training the diffusion model.</p>
</div>
<figure class="ltx_table" id="A1.T2">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T2.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T2.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T2.2.1.1.1.1">Dataset name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T2.2.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T2.2.1.1.2.1">Number of volumes</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T2.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.2.2.1.1">AbdomenCT-1K</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T2.2.2.1.2">789</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.3.2.1">AeroPath</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.3.2.2">15</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.4.3.1">AMOS22</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.4.3.2">240</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.5.4.1">autoPET23 (testing only)</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.5.4.2">200</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.6.5.1">Bone-Lesion</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.6.5.2">223</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.7.6.1">BTCV</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.7.6.2">48</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.8.7.1">COVID-19</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.8.7.2">524</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.9.8.1">CRLM-CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.9.8.2">158</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.10.9.1">CT-ORG</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.10.9.2">94</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.11.10.1">CTPelvic1K-CLINIC</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.11.10.2">94</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.12.11.1">LIDC</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.12.11.2">422</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.13.12.1">MSD Task03</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.13.12.2">88</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.14.13.1">MSD Task06</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.14.13.2">50</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.15.14.1">MSD Task07</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.15.14.2">224</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.16.15.1">MSD Task08</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.16.15.2">235</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.17.16.1">MSD Task09</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.17.16.2">33</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.18.17.1">MSD Task10</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.18.17.2">87</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.19.18.1">Multi-organ-Abdominal-CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.19.18.2">65</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.20.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.20.19.1">NLST</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.20.19.2">3109</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.21.20.1">Pancreas-CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.21.20.2">51</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.22.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.22.21.1">StonyBrook-CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.22.21.2">1258</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.23.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.23.22.1">TCIA_Colon</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.23.22.2">1437</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.24.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.24.23.1">TotalSegmentatorV2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.24.23.2">654</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.25.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T2.2.25.24.1">VerSe</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T2.2.25.24.2">179</td>
</tr>
<tr class="ltx_tr" id="A1.T2.2.26.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T2.2.26.25.1"><span class="ltx_text ltx_font_bold" id="A1.T2.2.26.25.1.1">Total</span></th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T2.2.26.25.2">10277</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T2.3.1.1" style="font-size:90%;">Table S2</span>: </span><span class="ltx_text" id="A1.T2.4.2" style="font-size:90%;">MAISI DM Dataset Information</span></figcaption>
</figure>
<figure class="ltx_figure" id="A1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="A1.F1.g1" src="x7.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F1.2.1.1" style="font-size:90%;">Figure S1</span>: </span><span class="ltx_text" id="A1.F1.3.2" style="font-size:90%;">The characteristics of the datasets utilized for the MAISI Diffusion Model are detailed through two subplots. Subplot (a) illustrates the volume dimensions of the datasets, providing insight into the variability and range of sizes used in the training data. Subplot (b) presents the voxel spacing in millimeters for each data point, emphasizing the spatial configuration within the CT scans. Notably, in CT imaging, the X and Y directions typically share identical dimensions and spacing, so they are represented on a single axis in both subplots. </span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>MAISI ControlNet</h3>
<div class="ltx_para" id="A1.SS3.p1">
<p class="ltx_p" id="A1.SS3.p1.1">The ControlNet training dataset for <span class="ltx_text ltx_font_bold" id="A1.SS3.p1.1.1">MAISI CT Generation</span> discussed in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.SS4" title="4.4 Data Augmentation in Downstream Tasks ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">4.4</span></a> contains 6,330 CT volumes (5,058 and 1,272 volumes are used for training and validation, respectively) across 20 datasets and covers different body regions and diseases.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.T3" title="Table S3 ‣ A.3 MAISI ControlNet ‣ Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">S3</span></a> summarizes the number of volumes for each dataset.</p>
</div>
<figure class="ltx_table" id="A1.T3">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T3.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T3.2.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.2.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T3.2.1.1.1.1">Dataset name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A1.T3.2.1.1.2"><span class="ltx_text ltx_font_bold" id="A1.T3.2.1.1.2.1">Number of volumes</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T3.2.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.2.2.1.1">AbdomenCT-1K</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T3.2.2.1.2">789</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.3.2.1">AeroPath</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.3.2.2">15</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.4.3.1">AMOS22</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.4.3.2">240</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.5.4.1">Bone-Lesion</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.5.4.2">237</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.6.5.1">BTCV</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.6.5.2">48</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.7.6.1">CT-ORG</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.7.6.2">94</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.8.7.1">CTPelvic1K-CLINIC</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.8.7.2">94</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.9.8.1">LIDC</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.9.8.2">422</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.10.9.1">MSD Task03</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.10.9.2">105</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.11.10.1">MSD Task06</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.11.10.2">50</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.12.11.1">MSD Task07</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.12.11.2">225</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.13.12.1">MSD Task08</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.13.12.2">235</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.14.13.1">MSD Task09</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.14.13.2">33</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.15.14.1">MSD Task10</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.15.14.2">101</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.16.15.1">Multi-organ-Abdominal-CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.16.15.2">64</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.17.16.1">Pancreas-CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.17.16.2">51</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.18.17.1">StonyBrook-CT</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.18.17.2">1258</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.19.18.1">TCIA_Colon</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.19.18.2">1436</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.20.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.20.19.1">TotalSegmentatorV2</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.20.19.2">654</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r" id="A1.T3.2.21.20.1">VerSe</th>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T3.2.21.20.2">179</td>
</tr>
<tr class="ltx_tr" id="A1.T3.2.22.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A1.T3.2.22.21.1">Total</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="A1.T3.2.22.21.2">6330</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A1.T3.3.1.1" style="font-size:90%;">Table S3</span>: </span><span class="ltx_text" id="A1.T3.4.2" style="font-size:90%;">MAISI ControlNet Dataset Information</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Implementation Details</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1"><span class="ltx_text ltx_font_bold" id="A2.p1.1.1">MAISI VAE.</span>
To establish the VAE as a foundational model, we employ an extensive range of data augmentation techniques. For CT images, intensities are clipped to a Hounsfield Unit (HU) range of -1000 to 1000 and normalized to a range of [0,1]. For MR images, intensities were normalized such that the 0th to 99.5th percentile values were scaled to the range [0,1]. For MR images, we applied intensity augmentations including random bias field, random Gibbs noise, random contrast adjustment, and random histogram shifts. Both CT and MR images underwent spatial augmentations, such as random flipping, random rotation, random intensity scaling, random intensity shifting, and random upsampling or downsampling.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">The MAISI VAE model is trained with 8 32G V100 GPU. It is initially trained for 100 epochs using small, randomly cropped patches of size [64,64,64]. This approach is adopted to improve the model’s ability to generalize to images with partial volume effects. After this initial phase, training is continued for an additional 200 epochs using larger patches of size [128,128,128], which allows the model to capture more contextual information and improve overall accuracy.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">The MAISI VAE is used to compress the latent features that will be employed in latent diffusion models, where having a well-structured and meaningful latent space is crucial for effective diffusion dynamics. Therefore, during MAISI VAE training, we adjust the weight of the KL loss to ensure the standard deviation remains between 0.9 to 1.1. This calibration balances the model’s focus between accurate data reconstruction and adherence to the prior distribution. As the MAISI VAE is intended to serve as a foundational model, maintaining this balance also helps to prevent over-fitting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib28" title="">28</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p4">
<p class="ltx_p" id="A2.p4.8"><span class="ltx_text ltx_font_bold" id="A2.p4.8.1">MAISI Diffusion.</span>
Data preprocessing for diffusion model training involves applying a series of precise transformations to the image data, including loading the images, ensuring the correct channel structure, adjusting the orientation according to the ”RAS” axcode, and scaling intensity values from <math alttext="-1000" class="ltx_Math" display="inline" id="A2.p4.1.m1.1"><semantics id="A2.p4.1.m1.1a"><mrow id="A2.p4.1.m1.1.1" xref="A2.p4.1.m1.1.1.cmml"><mo id="A2.p4.1.m1.1.1a" xref="A2.p4.1.m1.1.1.cmml">−</mo><mn id="A2.p4.1.m1.1.1.2" xref="A2.p4.1.m1.1.1.2.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.1.m1.1b"><apply id="A2.p4.1.m1.1.1.cmml" xref="A2.p4.1.m1.1.1"><minus id="A2.p4.1.m1.1.1.1.cmml" xref="A2.p4.1.m1.1.1"></minus><cn id="A2.p4.1.m1.1.1.2.cmml" type="integer" xref="A2.p4.1.m1.1.1.2">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.1.m1.1c">-1000</annotation><annotation encoding="application/x-llamapun" id="A2.p4.1.m1.1d">- 1000</annotation></semantics></math> to <math alttext="1000" class="ltx_Math" display="inline" id="A2.p4.2.m2.1"><semantics id="A2.p4.2.m2.1a"><mn id="A2.p4.2.m2.1.1" xref="A2.p4.2.m2.1.1.cmml">1000</mn><annotation-xml encoding="MathML-Content" id="A2.p4.2.m2.1b"><cn id="A2.p4.2.m2.1.1.cmml" type="integer" xref="A2.p4.2.m2.1.1">1000</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.2.m2.1c">1000</annotation><annotation encoding="application/x-llamapun" id="A2.p4.2.m2.1d">1000</annotation></semantics></math> to normalize the data between 0 and 1.
The process further refines the images by adjusting dimensions to the nearest multiple of <math alttext="128" class="ltx_Math" display="inline" id="A2.p4.3.m3.1"><semantics id="A2.p4.3.m3.1a"><mn id="A2.p4.3.m3.1.1" xref="A2.p4.3.m3.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="A2.p4.3.m3.1b"><cn id="A2.p4.3.m3.1.1.cmml" type="integer" xref="A2.p4.3.m3.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.3.m3.1c">128</annotation><annotation encoding="application/x-llamapun" id="A2.p4.3.m3.1d">128</annotation></semantics></math>, recording the new spatial details, using trilinear interpolation.
Then each image is passed through a pre-trained autoencoder, generating a compressed latent representation that is saved for subsequent model training.
The diffusion model requires additional input attributes, including output dimensions, output spacing, and top/bottom body region indicators. These dimensions and spacing are extracted from the header information of the training images. The top and bottom body regions can be identified either through manual inspection or by using segmentation tools such as TotalSegmentator <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib62" title="">62</a>]</cite> and VISTA3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib26" title="">26</a>]</cite>. These regions are encoded as <math alttext="4" class="ltx_Math" display="inline" id="A2.p4.4.m4.1"><semantics id="A2.p4.4.m4.1a"><mn id="A2.p4.4.m4.1.1" xref="A2.p4.4.m4.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="A2.p4.4.m4.1b"><cn id="A2.p4.4.m4.1.1.cmml" type="integer" xref="A2.p4.4.m4.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.4.m4.1c">4</annotation><annotation encoding="application/x-llamapun" id="A2.p4.4.m4.1d">4</annotation></semantics></math>-dimensional one-hot vectors: the head and neck region is represented by <math alttext="[1,0,0,0]" class="ltx_Math" display="inline" id="A2.p4.5.m5.4"><semantics id="A2.p4.5.m5.4a"><mrow id="A2.p4.5.m5.4.5.2" xref="A2.p4.5.m5.4.5.1.cmml"><mo id="A2.p4.5.m5.4.5.2.1" stretchy="false" xref="A2.p4.5.m5.4.5.1.cmml">[</mo><mn id="A2.p4.5.m5.1.1" xref="A2.p4.5.m5.1.1.cmml">1</mn><mo id="A2.p4.5.m5.4.5.2.2" xref="A2.p4.5.m5.4.5.1.cmml">,</mo><mn id="A2.p4.5.m5.2.2" xref="A2.p4.5.m5.2.2.cmml">0</mn><mo id="A2.p4.5.m5.4.5.2.3" xref="A2.p4.5.m5.4.5.1.cmml">,</mo><mn id="A2.p4.5.m5.3.3" xref="A2.p4.5.m5.3.3.cmml">0</mn><mo id="A2.p4.5.m5.4.5.2.4" xref="A2.p4.5.m5.4.5.1.cmml">,</mo><mn id="A2.p4.5.m5.4.4" xref="A2.p4.5.m5.4.4.cmml">0</mn><mo id="A2.p4.5.m5.4.5.2.5" stretchy="false" xref="A2.p4.5.m5.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.5.m5.4b"><list id="A2.p4.5.m5.4.5.1.cmml" xref="A2.p4.5.m5.4.5.2"><cn id="A2.p4.5.m5.1.1.cmml" type="integer" xref="A2.p4.5.m5.1.1">1</cn><cn id="A2.p4.5.m5.2.2.cmml" type="integer" xref="A2.p4.5.m5.2.2">0</cn><cn id="A2.p4.5.m5.3.3.cmml" type="integer" xref="A2.p4.5.m5.3.3">0</cn><cn id="A2.p4.5.m5.4.4.cmml" type="integer" xref="A2.p4.5.m5.4.4">0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.5.m5.4c">[1,0,0,0]</annotation><annotation encoding="application/x-llamapun" id="A2.p4.5.m5.4d">[ 1 , 0 , 0 , 0 ]</annotation></semantics></math>, the chest by <math alttext="[0,1,0,0]" class="ltx_Math" display="inline" id="A2.p4.6.m6.4"><semantics id="A2.p4.6.m6.4a"><mrow id="A2.p4.6.m6.4.5.2" xref="A2.p4.6.m6.4.5.1.cmml"><mo id="A2.p4.6.m6.4.5.2.1" stretchy="false" xref="A2.p4.6.m6.4.5.1.cmml">[</mo><mn id="A2.p4.6.m6.1.1" xref="A2.p4.6.m6.1.1.cmml">0</mn><mo id="A2.p4.6.m6.4.5.2.2" xref="A2.p4.6.m6.4.5.1.cmml">,</mo><mn id="A2.p4.6.m6.2.2" xref="A2.p4.6.m6.2.2.cmml">1</mn><mo id="A2.p4.6.m6.4.5.2.3" xref="A2.p4.6.m6.4.5.1.cmml">,</mo><mn id="A2.p4.6.m6.3.3" xref="A2.p4.6.m6.3.3.cmml">0</mn><mo id="A2.p4.6.m6.4.5.2.4" xref="A2.p4.6.m6.4.5.1.cmml">,</mo><mn id="A2.p4.6.m6.4.4" xref="A2.p4.6.m6.4.4.cmml">0</mn><mo id="A2.p4.6.m6.4.5.2.5" stretchy="false" xref="A2.p4.6.m6.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.6.m6.4b"><list id="A2.p4.6.m6.4.5.1.cmml" xref="A2.p4.6.m6.4.5.2"><cn id="A2.p4.6.m6.1.1.cmml" type="integer" xref="A2.p4.6.m6.1.1">0</cn><cn id="A2.p4.6.m6.2.2.cmml" type="integer" xref="A2.p4.6.m6.2.2">1</cn><cn id="A2.p4.6.m6.3.3.cmml" type="integer" xref="A2.p4.6.m6.3.3">0</cn><cn id="A2.p4.6.m6.4.4.cmml" type="integer" xref="A2.p4.6.m6.4.4">0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.6.m6.4c">[0,1,0,0]</annotation><annotation encoding="application/x-llamapun" id="A2.p4.6.m6.4d">[ 0 , 1 , 0 , 0 ]</annotation></semantics></math>, the abdomen by <math alttext="[0,0,1,0]" class="ltx_Math" display="inline" id="A2.p4.7.m7.4"><semantics id="A2.p4.7.m7.4a"><mrow id="A2.p4.7.m7.4.5.2" xref="A2.p4.7.m7.4.5.1.cmml"><mo id="A2.p4.7.m7.4.5.2.1" stretchy="false" xref="A2.p4.7.m7.4.5.1.cmml">[</mo><mn id="A2.p4.7.m7.1.1" xref="A2.p4.7.m7.1.1.cmml">0</mn><mo id="A2.p4.7.m7.4.5.2.2" xref="A2.p4.7.m7.4.5.1.cmml">,</mo><mn id="A2.p4.7.m7.2.2" xref="A2.p4.7.m7.2.2.cmml">0</mn><mo id="A2.p4.7.m7.4.5.2.3" xref="A2.p4.7.m7.4.5.1.cmml">,</mo><mn id="A2.p4.7.m7.3.3" xref="A2.p4.7.m7.3.3.cmml">1</mn><mo id="A2.p4.7.m7.4.5.2.4" xref="A2.p4.7.m7.4.5.1.cmml">,</mo><mn id="A2.p4.7.m7.4.4" xref="A2.p4.7.m7.4.4.cmml">0</mn><mo id="A2.p4.7.m7.4.5.2.5" stretchy="false" xref="A2.p4.7.m7.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.7.m7.4b"><list id="A2.p4.7.m7.4.5.1.cmml" xref="A2.p4.7.m7.4.5.2"><cn id="A2.p4.7.m7.1.1.cmml" type="integer" xref="A2.p4.7.m7.1.1">0</cn><cn id="A2.p4.7.m7.2.2.cmml" type="integer" xref="A2.p4.7.m7.2.2">0</cn><cn id="A2.p4.7.m7.3.3.cmml" type="integer" xref="A2.p4.7.m7.3.3">1</cn><cn id="A2.p4.7.m7.4.4.cmml" type="integer" xref="A2.p4.7.m7.4.4">0</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.7.m7.4c">[0,0,1,0]</annotation><annotation encoding="application/x-llamapun" id="A2.p4.7.m7.4d">[ 0 , 0 , 1 , 0 ]</annotation></semantics></math>, and the lower body (below the abdomen) by <math alttext="[0,0,0,1]" class="ltx_Math" display="inline" id="A2.p4.8.m8.4"><semantics id="A2.p4.8.m8.4a"><mrow id="A2.p4.8.m8.4.5.2" xref="A2.p4.8.m8.4.5.1.cmml"><mo id="A2.p4.8.m8.4.5.2.1" stretchy="false" xref="A2.p4.8.m8.4.5.1.cmml">[</mo><mn id="A2.p4.8.m8.1.1" xref="A2.p4.8.m8.1.1.cmml">0</mn><mo id="A2.p4.8.m8.4.5.2.2" xref="A2.p4.8.m8.4.5.1.cmml">,</mo><mn id="A2.p4.8.m8.2.2" xref="A2.p4.8.m8.2.2.cmml">0</mn><mo id="A2.p4.8.m8.4.5.2.3" xref="A2.p4.8.m8.4.5.1.cmml">,</mo><mn id="A2.p4.8.m8.3.3" xref="A2.p4.8.m8.3.3.cmml">0</mn><mo id="A2.p4.8.m8.4.5.2.4" xref="A2.p4.8.m8.4.5.1.cmml">,</mo><mn id="A2.p4.8.m8.4.4" xref="A2.p4.8.m8.4.4.cmml">1</mn><mo id="A2.p4.8.m8.4.5.2.5" stretchy="false" xref="A2.p4.8.m8.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.p4.8.m8.4b"><list id="A2.p4.8.m8.4.5.1.cmml" xref="A2.p4.8.m8.4.5.2"><cn id="A2.p4.8.m8.1.1.cmml" type="integer" xref="A2.p4.8.m8.1.1">0</cn><cn id="A2.p4.8.m8.2.2.cmml" type="integer" xref="A2.p4.8.m8.2.2">0</cn><cn id="A2.p4.8.m8.3.3.cmml" type="integer" xref="A2.p4.8.m8.3.3">0</cn><cn id="A2.p4.8.m8.4.4.cmml" type="integer" xref="A2.p4.8.m8.4.4">1</cn></list></annotation-xml><annotation encoding="application/x-tex" id="A2.p4.8.m8.4c">[0,0,0,1]</annotation><annotation encoding="application/x-llamapun" id="A2.p4.8.m8.4d">[ 0 , 0 , 0 , 1 ]</annotation></semantics></math>. These additional input attributes are stored in a separate configuration file. In this example, it is assumed that the images encompass the chest and abdomen regions.</p>
</div>
<div class="ltx_para" id="A2.p5">
<p class="ltx_p" id="A2.p5.3">Next, the diffusion model training process begins with an initial learning rate of <math alttext="1e^{-4}" class="ltx_Math" display="inline" id="A2.p5.1.m1.1"><semantics id="A2.p5.1.m1.1a"><mrow id="A2.p5.1.m1.1.1" xref="A2.p5.1.m1.1.1.cmml"><mn id="A2.p5.1.m1.1.1.2" xref="A2.p5.1.m1.1.1.2.cmml">1</mn><mo id="A2.p5.1.m1.1.1.1" xref="A2.p5.1.m1.1.1.1.cmml">⁢</mo><msup id="A2.p5.1.m1.1.1.3" xref="A2.p5.1.m1.1.1.3.cmml"><mi id="A2.p5.1.m1.1.1.3.2" xref="A2.p5.1.m1.1.1.3.2.cmml">e</mi><mrow id="A2.p5.1.m1.1.1.3.3" xref="A2.p5.1.m1.1.1.3.3.cmml"><mo id="A2.p5.1.m1.1.1.3.3a" xref="A2.p5.1.m1.1.1.3.3.cmml">−</mo><mn id="A2.p5.1.m1.1.1.3.3.2" xref="A2.p5.1.m1.1.1.3.3.2.cmml">4</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.p5.1.m1.1b"><apply id="A2.p5.1.m1.1.1.cmml" xref="A2.p5.1.m1.1.1"><times id="A2.p5.1.m1.1.1.1.cmml" xref="A2.p5.1.m1.1.1.1"></times><cn id="A2.p5.1.m1.1.1.2.cmml" type="integer" xref="A2.p5.1.m1.1.1.2">1</cn><apply id="A2.p5.1.m1.1.1.3.cmml" xref="A2.p5.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.p5.1.m1.1.1.3.1.cmml" xref="A2.p5.1.m1.1.1.3">superscript</csymbol><ci id="A2.p5.1.m1.1.1.3.2.cmml" xref="A2.p5.1.m1.1.1.3.2">𝑒</ci><apply id="A2.p5.1.m1.1.1.3.3.cmml" xref="A2.p5.1.m1.1.1.3.3"><minus id="A2.p5.1.m1.1.1.3.3.1.cmml" xref="A2.p5.1.m1.1.1.3.3"></minus><cn id="A2.p5.1.m1.1.1.3.3.2.cmml" type="integer" xref="A2.p5.1.m1.1.1.3.3.2">4</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p5.1.m1.1c">1e^{-4}</annotation><annotation encoding="application/x-llamapun" id="A2.p5.1.m1.1d">1 italic_e start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT</annotation></semantics></math>, a batch size of <math alttext="1" class="ltx_Math" display="inline" id="A2.p5.2.m2.1"><semantics id="A2.p5.2.m2.1a"><mn id="A2.p5.2.m2.1.1" xref="A2.p5.2.m2.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="A2.p5.2.m2.1b"><cn id="A2.p5.2.m2.1.1.cmml" type="integer" xref="A2.p5.2.m2.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p5.2.m2.1c">1</annotation><annotation encoding="application/x-llamapun" id="A2.p5.2.m2.1d">1</annotation></semantics></math>, and spans <math alttext="200" class="ltx_Math" display="inline" id="A2.p5.3.m3.1"><semantics id="A2.p5.3.m3.1a"><mn id="A2.p5.3.m3.1.1" xref="A2.p5.3.m3.1.1.cmml">200</mn><annotation-xml encoding="MathML-Content" id="A2.p5.3.m3.1b"><cn id="A2.p5.3.m3.1.1.cmml" type="integer" xref="A2.p5.3.m3.1.1">200</cn></annotation-xml><annotation encoding="application/x-tex" id="A2.p5.3.m3.1c">200</annotation><annotation encoding="application/x-llamapun" id="A2.p5.3.m3.1d">200</annotation></semantics></math> epochs. To ensure the data is optimally prepared for training, various transformations are applied to the image inputs.
The U-Net architecture is employed for noise prediction, with distributed computing utilized to enhance efficiency when multiple GPUs are available.
The Adam optimizer is responsible for adjusting the model’s parameters, while a polynomial learning rate scheduler controls the update rate over training steps.
Noise is systematically introduced to the input data by the noise scheduler, and the model iteratively refines its predictions using an L1 loss function to minimize this noise.
Mixed precision training and gradient scaling are implemented to optimize memory usage and computational performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p6">
<p class="ltx_p" id="A2.p6.2"><span class="ltx_text ltx_font_bold" id="A2.p6.2.1">MAISI ControlNet.</span> We train a versatile ControlNet Model (MAISI CT Generation task in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#S4.SS4" title="4.4 Data Augmentation in Downstream Tasks ‣ 4 Experiments ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">4.4</span></a>) to support all five types of tumors using the datasets summarized in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A1.T3" title="Table S3 ‣ A.3 MAISI ControlNet ‣ Appendix A Dataset Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">S3</span></a>. The data preprocessing protocol is the same in the training of the MAISI Diffusion Model. The Adam optimizer is employed for training purposes, with hyperparameters <math alttext="\beta_{1}=0.9" class="ltx_Math" display="inline" id="A2.p6.1.m1.1"><semantics id="A2.p6.1.m1.1a"><mrow id="A2.p6.1.m1.1.1" xref="A2.p6.1.m1.1.1.cmml"><msub id="A2.p6.1.m1.1.1.2" xref="A2.p6.1.m1.1.1.2.cmml"><mi id="A2.p6.1.m1.1.1.2.2" xref="A2.p6.1.m1.1.1.2.2.cmml">β</mi><mn id="A2.p6.1.m1.1.1.2.3" xref="A2.p6.1.m1.1.1.2.3.cmml">1</mn></msub><mo id="A2.p6.1.m1.1.1.1" xref="A2.p6.1.m1.1.1.1.cmml">=</mo><mn id="A2.p6.1.m1.1.1.3" xref="A2.p6.1.m1.1.1.3.cmml">0.9</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.1.m1.1b"><apply id="A2.p6.1.m1.1.1.cmml" xref="A2.p6.1.m1.1.1"><eq id="A2.p6.1.m1.1.1.1.cmml" xref="A2.p6.1.m1.1.1.1"></eq><apply id="A2.p6.1.m1.1.1.2.cmml" xref="A2.p6.1.m1.1.1.2"><csymbol cd="ambiguous" id="A2.p6.1.m1.1.1.2.1.cmml" xref="A2.p6.1.m1.1.1.2">subscript</csymbol><ci id="A2.p6.1.m1.1.1.2.2.cmml" xref="A2.p6.1.m1.1.1.2.2">𝛽</ci><cn id="A2.p6.1.m1.1.1.2.3.cmml" type="integer" xref="A2.p6.1.m1.1.1.2.3">1</cn></apply><cn id="A2.p6.1.m1.1.1.3.cmml" type="float" xref="A2.p6.1.m1.1.1.3">0.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.1.m1.1c">\beta_{1}=0.9</annotation><annotation encoding="application/x-llamapun" id="A2.p6.1.m1.1d">italic_β start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.9</annotation></semantics></math> and <math alttext="\beta_{2}=0.999" class="ltx_Math" display="inline" id="A2.p6.2.m2.1"><semantics id="A2.p6.2.m2.1a"><mrow id="A2.p6.2.m2.1.1" xref="A2.p6.2.m2.1.1.cmml"><msub id="A2.p6.2.m2.1.1.2" xref="A2.p6.2.m2.1.1.2.cmml"><mi id="A2.p6.2.m2.1.1.2.2" xref="A2.p6.2.m2.1.1.2.2.cmml">β</mi><mn id="A2.p6.2.m2.1.1.2.3" xref="A2.p6.2.m2.1.1.2.3.cmml">2</mn></msub><mo id="A2.p6.2.m2.1.1.1" xref="A2.p6.2.m2.1.1.1.cmml">=</mo><mn id="A2.p6.2.m2.1.1.3" xref="A2.p6.2.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.p6.2.m2.1b"><apply id="A2.p6.2.m2.1.1.cmml" xref="A2.p6.2.m2.1.1"><eq id="A2.p6.2.m2.1.1.1.cmml" xref="A2.p6.2.m2.1.1.1"></eq><apply id="A2.p6.2.m2.1.1.2.cmml" xref="A2.p6.2.m2.1.1.2"><csymbol cd="ambiguous" id="A2.p6.2.m2.1.1.2.1.cmml" xref="A2.p6.2.m2.1.1.2">subscript</csymbol><ci id="A2.p6.2.m2.1.1.2.2.cmml" xref="A2.p6.2.m2.1.1.2.2">𝛽</ci><cn id="A2.p6.2.m2.1.1.2.3.cmml" type="integer" xref="A2.p6.2.m2.1.1.2.3">2</cn></apply><cn id="A2.p6.2.m2.1.1.3.cmml" type="float" xref="A2.p6.2.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.p6.2.m2.1c">\beta_{2}=0.999</annotation><annotation encoding="application/x-llamapun" id="A2.p6.2.m2.1d">italic_β start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math>. The learning rate is set at 0.0001, with the polynomial learning rate decay. The batch size is set to 1 per GPU. Training is performed on a server with 8 A100 GPUs with about 10k optimization steps. For the MAISI Inpainting task, we employ the same hyperparameters for training but only use datasets with supported tumor types, including MSD Task03 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (liver tumor), Task06 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (lung tumor), Task07 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib3" title="">3</a>]</cite> (pancreas tumor).</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p7">
<p class="ltx_p" id="A2.p7.1"><span class="ltx_text ltx_font_bold" id="A2.p7.1.1">Downstream tumor segmentation.</span> The implementation of all tumor segmentation models is based on the Auto3DSeg<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://monai.io/apps/auto3dseg</span></span></span> pipeline. Auto3DSeg is an auto-configuration pipeline designed for 3D medical image segmentation, utilizing MONAI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib6" title="">6</a>]</cite>. The pipeline begins with data analysis to extract global information from the dataset, followed by algorithm generation based on data statistics and predefined templates. It then proceeds to model training to obtain optimal checkpoints. All used tumor dataset is split into 80% for training and 20% for testing. The training set is further divided into five folds for 5-fold cross-validation. We report the segmentation performance on the holdout testing set. For the MAISI CT Generation task, we generate synthetic data from augmented real masks containing tumors. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A2.F2" title="Figure S2 ‣ Appendix B Additional Implementation Details ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">S2</span></a> shows an example of mask augmentation for a case with the lung tumor. For the MAISI Inpainting task, we follow the same setting in DiffTumor <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib10" title="">10</a>]</cite> and use the provided healthy cases in the open-source repository<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/MrGiovanni/DiffTumor</span></span></span> to generate synthetic data with tumors. For both tasks, the amount of synthesized data is equivalent to the original dataset size for each tumor type. We explore the impact of using different amounts of synthetic data for data augmentation in Supplementary Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#A3" title="Appendix C Supplementary Experiment Results ‣ MAISI: Medical AI for Synthetic Imaging"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<figure class="ltx_figure" id="A2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="A2.F2.g1" src="x8.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F2.2.1.1" style="font-size:90%;">Figure S2</span>: </span><span class="ltx_text" id="A2.F2.3.2" style="font-size:90%;">The example lung tumor mask and corresponding augmented mask. The green boxes highlight the tumor regions in different views.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Supplementary Experiment Results</h2>
<figure class="ltx_figure" id="A3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="334" id="A3.F3.g1" src="x9.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F3.2.1.1" style="font-size:90%;">Figure S3</span>: </span><span class="ltx_text" id="A3.F3.3.2" style="font-size:90%;">The example of generated images from MAISI CT Generation task.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="426" id="A3.F4.g1" src="x10.png" width="270"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F4.2.1.1" style="font-size:90%;">Figure S4</span>: </span><span class="ltx_text" id="A3.F4.3.2" style="font-size:90%;">The example of generated images from MAISI Inpainting task.</span></figcaption>
</figure>
<figure class="ltx_table" id="A3.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="A3.T4.2" style="width:214.6pt;height:83.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.9pt,48.1pt) scale(0.466070084811286,0.466070084811286) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T4.2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T4.2.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T4.2.1.1.1.1"><span class="ltx_text" id="A3.T4.2.1.1.1.1.1" style="font-size:90%;">MSD Task06</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T4.2.1.1.1.2"><span class="ltx_text" id="A3.T4.2.1.1.1.2.1" style="font-size:90%;">Real v.s. Synthetic</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.1.1.3"><span class="ltx_text" id="A3.T4.2.1.1.1.3.1" style="font-size:90%;">fold 0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.1.1.4"><span class="ltx_text" id="A3.T4.2.1.1.1.4.1" style="font-size:90%;">fold 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.1.1.5"><span class="ltx_text" id="A3.T4.2.1.1.1.5.1" style="font-size:90%;">fold 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.1.1.6"><span class="ltx_text" id="A3.T4.2.1.1.1.6.1" style="font-size:90%;">fold 3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.1.1.7"><span class="ltx_text" id="A3.T4.2.1.1.1.7.1" style="font-size:90%;">fold 4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T4.2.1.1.1.8"><span class="ltx_text" id="A3.T4.2.1.1.1.8.1" style="font-size:90%;">Avg.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.1.1.9"><span class="ltx_text" id="A3.T4.2.1.1.1.9.1" style="font-size:90%;">Improvement</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A3.T4.2.1.2.2.1"><span class="ltx_text" id="A3.T4.2.1.2.2.1.1" style="font-size:90%;">Real Only</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A3.T4.2.1.2.2.2"><span class="ltx_text" id="A3.T4.2.1.2.2.2.1" style="font-size:90%;">1:0</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.2.2.3"><span class="ltx_text" id="A3.T4.2.1.2.2.3.1" style="font-size:90%;">0.494</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.2.2.4"><span class="ltx_text" id="A3.T4.2.1.2.2.4.1" style="font-size:90%;">0.601</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.2.2.5"><span class="ltx_text" id="A3.T4.2.1.2.2.5.1" style="font-size:90%;">0.535</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.2.2.6"><span class="ltx_text" id="A3.T4.2.1.2.2.6.1" style="font-size:90%;">0.674</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.2.2.7"><span class="ltx_text" id="A3.T4.2.1.2.2.7.1" style="font-size:90%;">0.599</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A3.T4.2.1.2.2.8"><span class="ltx_text" id="A3.T4.2.1.2.2.8.1" style="font-size:90%;">0.581</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.2.2.9"><span class="ltx_text" id="A3.T4.2.1.2.2.9.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.3.3.1"><span class="ltx_text" id="A3.T4.2.1.3.3.1.1" style="font-size:90%;">MAISI CT Generation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.3.3.2"><span class="ltx_text" id="A3.T4.2.1.3.3.2.1" style="font-size:90%;">1:1</span></th>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.3.3.3"><span class="ltx_text" id="A3.T4.2.1.3.3.3.1" style="font-size:90%;">0.585</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.3.3.4"><span class="ltx_text" id="A3.T4.2.1.3.3.4.1" style="font-size:90%;">0.649</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.3.3.5"><span class="ltx_text" id="A3.T4.2.1.3.3.5.1" style="font-size:90%;">0.631</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.3.3.6"><span class="ltx_text" id="A3.T4.2.1.3.3.6.1" style="font-size:90%;">0.647</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.3.3.7"><span class="ltx_text" id="A3.T4.2.1.3.3.7.1" style="font-size:90%;">0.664</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T4.2.1.3.3.8"><span class="ltx_text" id="A3.T4.2.1.3.3.8.1" style="font-size:90%;">0.635</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.3.3.9"><span class="ltx_text" id="A3.T4.2.1.3.3.9.1" style="font-size:90%;">5.5%</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.4.4.1"><span class="ltx_text" id="A3.T4.2.1.4.4.1.1" style="font-size:90%;">MAISI CT Generation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.4.4.2"><span class="ltx_text" id="A3.T4.2.1.4.4.2.1" style="font-size:90%;">1:0.5</span></th>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.4.4.3"><span class="ltx_text" id="A3.T4.2.1.4.4.3.1" style="font-size:90%;">0.640</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.4.4.4"><span class="ltx_text" id="A3.T4.2.1.4.4.4.1" style="font-size:90%;">0.593</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.4.4.5"><span class="ltx_text" id="A3.T4.2.1.4.4.5.1" style="font-size:90%;">0.606</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.4.4.6"><span class="ltx_text" id="A3.T4.2.1.4.4.6.1" style="font-size:90%;">0.639</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.4.4.7"><span class="ltx_text" id="A3.T4.2.1.4.4.7.1" style="font-size:90%;">0.644</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T4.2.1.4.4.8"><span class="ltx_text" id="A3.T4.2.1.4.4.8.1" style="font-size:90%;">0.624</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.4.4.9"><span class="ltx_text" id="A3.T4.2.1.4.4.9.1" style="font-size:90%;">4.4%</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.5.5.1"><span class="ltx_text" id="A3.T4.2.1.5.5.1.1" style="font-size:90%;">MAISI CT Generation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.5.5.2"><span class="ltx_text" id="A3.T4.2.1.5.5.2.1" style="font-size:90%;">1:1.5</span></th>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.5.5.3"><span class="ltx_text" id="A3.T4.2.1.5.5.3.1" style="font-size:90%;">0.641</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.5.5.4"><span class="ltx_text" id="A3.T4.2.1.5.5.4.1" style="font-size:90%;">0.658</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.5.5.5"><span class="ltx_text" id="A3.T4.2.1.5.5.5.1" style="font-size:90%;">0.586</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.5.5.6"><span class="ltx_text" id="A3.T4.2.1.5.5.6.1" style="font-size:90%;">0.645</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.5.5.7"><span class="ltx_text" id="A3.T4.2.1.5.5.7.1" style="font-size:90%;">0.666</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T4.2.1.5.5.8"><span class="ltx_text" id="A3.T4.2.1.5.5.8.1" style="font-size:90%;">0.639</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.5.5.9"><span class="ltx_text" id="A3.T4.2.1.5.5.9.1" style="font-size:90%;">5.8%</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T4.2.1.6.6.1"><span class="ltx_text" id="A3.T4.2.1.6.6.1.1" style="font-size:90%;">MSD Task07</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T4.2.1.6.6.2"><span class="ltx_text" id="A3.T4.2.1.6.6.2.1" style="font-size:90%;">Real v.s. Synthetic</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.6.6.3"><span class="ltx_text" id="A3.T4.2.1.6.6.3.1" style="font-size:90%;">fold 0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.6.6.4"><span class="ltx_text" id="A3.T4.2.1.6.6.4.1" style="font-size:90%;">fold 1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.6.6.5"><span class="ltx_text" id="A3.T4.2.1.6.6.5.1" style="font-size:90%;">fold 2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.6.6.6"><span class="ltx_text" id="A3.T4.2.1.6.6.6.1" style="font-size:90%;">fold 3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.6.6.7"><span class="ltx_text" id="A3.T4.2.1.6.6.7.1" style="font-size:90%;">fold 4</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A3.T4.2.1.6.6.8"><span class="ltx_text" id="A3.T4.2.1.6.6.8.1" style="font-size:90%;">Avg.</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T4.2.1.6.6.9"><span class="ltx_text" id="A3.T4.2.1.6.6.9.1" style="font-size:90%;">Improvement</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A3.T4.2.1.7.7.1"><span class="ltx_text" id="A3.T4.2.1.7.7.1.1" style="font-size:90%;">Real Only</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="A3.T4.2.1.7.7.2"><span class="ltx_text" id="A3.T4.2.1.7.7.2.1" style="font-size:90%;">1:0</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.7.7.3"><span class="ltx_text" id="A3.T4.2.1.7.7.3.1" style="font-size:90%;">0.423</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.7.7.4"><span class="ltx_text" id="A3.T4.2.1.7.7.4.1" style="font-size:90%;">0.463</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.7.7.5"><span class="ltx_text" id="A3.T4.2.1.7.7.5.1" style="font-size:90%;">0.414</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.7.7.6"><span class="ltx_text" id="A3.T4.2.1.7.7.6.1" style="font-size:90%;">0.42</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.7.7.7"><span class="ltx_text" id="A3.T4.2.1.7.7.7.1" style="font-size:90%;">0.444</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A3.T4.2.1.7.7.8"><span class="ltx_text" id="A3.T4.2.1.7.7.8.1" style="font-size:90%;">0.433</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A3.T4.2.1.7.7.9"><span class="ltx_text" id="A3.T4.2.1.7.7.9.1" style="font-size:90%;">-</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.8.8.1"><span class="ltx_text" id="A3.T4.2.1.8.8.1.1" style="font-size:90%;">MAISI CT Generation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.8.8.2"><span class="ltx_text" id="A3.T4.2.1.8.8.2.1" style="font-size:90%;">1:1</span></th>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.8.8.3"><span class="ltx_text" id="A3.T4.2.1.8.8.3.1" style="font-size:90%;">0.504</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.8.8.4"><span class="ltx_text" id="A3.T4.2.1.8.8.4.1" style="font-size:90%;">0.448</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.8.8.5"><span class="ltx_text" id="A3.T4.2.1.8.8.5.1" style="font-size:90%;">0.467</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.8.8.6"><span class="ltx_text" id="A3.T4.2.1.8.8.6.1" style="font-size:90%;">0.482</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.8.8.7"><span class="ltx_text" id="A3.T4.2.1.8.8.7.1" style="font-size:90%;">0.508</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T4.2.1.8.8.8"><span class="ltx_text" id="A3.T4.2.1.8.8.8.1" style="font-size:90%;">0.482</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.8.8.9"><span class="ltx_text" id="A3.T4.2.1.8.8.9.1" style="font-size:90%;">4.9%</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.9.9.1"><span class="ltx_text" id="A3.T4.2.1.9.9.1.1" style="font-size:90%;">MAISI CT Generation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T4.2.1.9.9.2"><span class="ltx_text" id="A3.T4.2.1.9.9.2.1" style="font-size:90%;">1:0.5</span></th>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.9.9.3"><span class="ltx_text" id="A3.T4.2.1.9.9.3.1" style="font-size:90%;">0.465</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.9.9.4"><span class="ltx_text" id="A3.T4.2.1.9.9.4.1" style="font-size:90%;">0.463</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.9.9.5"><span class="ltx_text" id="A3.T4.2.1.9.9.5.1" style="font-size:90%;">0.423</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.9.9.6"><span class="ltx_text" id="A3.T4.2.1.9.9.6.1" style="font-size:90%;">0.447</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.9.9.7"><span class="ltx_text" id="A3.T4.2.1.9.9.7.1" style="font-size:90%;">0.478</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A3.T4.2.1.9.9.8"><span class="ltx_text" id="A3.T4.2.1.9.9.8.1" style="font-size:90%;">0.455</span></td>
<td class="ltx_td ltx_align_center" id="A3.T4.2.1.9.9.9"><span class="ltx_text" id="A3.T4.2.1.9.9.9.1" style="font-size:90%;">2.2%</span></td>
</tr>
<tr class="ltx_tr" id="A3.T4.2.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="A3.T4.2.1.10.10.1"><span class="ltx_text" id="A3.T4.2.1.10.10.1.1" style="font-size:90%;">MAISI CT Generation</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="A3.T4.2.1.10.10.2"><span class="ltx_text" id="A3.T4.2.1.10.10.2.1" style="font-size:90%;">1:1.5</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T4.2.1.10.10.3"><span class="ltx_text" id="A3.T4.2.1.10.10.3.1" style="font-size:90%;">0.466</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T4.2.1.10.10.4"><span class="ltx_text" id="A3.T4.2.1.10.10.4.1" style="font-size:90%;">0.481</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T4.2.1.10.10.5"><span class="ltx_text" id="A3.T4.2.1.10.10.5.1" style="font-size:90%;">0.465</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T4.2.1.10.10.6"><span class="ltx_text" id="A3.T4.2.1.10.10.6.1" style="font-size:90%;">0.480</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T4.2.1.10.10.7"><span class="ltx_text" id="A3.T4.2.1.10.10.7.1" style="font-size:90%;">0.467</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A3.T4.2.1.10.10.8"><span class="ltx_text" id="A3.T4.2.1.10.10.8.1" style="font-size:90%;">0.471</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T4.2.1.10.10.9"><span class="ltx_text" id="A3.T4.2.1.10.10.9.1" style="font-size:90%;">3.9%</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table S4: </span>The ablation study examines the effect of varying amounts of synthetic data in data augmentation experiments. The ’Improvement’ column reports the percentage of relative improvement compared to experiments using only real data. We conduct this ablation study on the smallest dataset (MSD Task06) and the largest dataset (MSD Task07) across five tumor types. Our empirical results suggest that using a synthetic dataset equivalent in size to the original dataset is an effective choice for data augmentation.
</figcaption>
</figure>
<figure class="ltx_table" id="A3.T5">
<div class="ltx_inline-block ltx_transformed_outer" id="A3.T5.2" style="width:214.6pt;height:17.7pt;vertical-align:-0.3pt;"><span class="ltx_transformed_inner" style="transform:translate(-225.6pt,18.3pt) scale(0.322316647300805,0.322316647300805) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A3.T5.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T5.2.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" id="A3.T5.2.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.2"><span class="ltx_text" id="A3.T5.2.1.1.1.2.1" style="font-size:90%;">Liver</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.3"><span class="ltx_text" id="A3.T5.2.1.1.1.3.1" style="font-size:90%;">Spleen</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.4"><span class="ltx_text" id="A3.T5.2.1.1.1.4.1" style="font-size:90%;">Left Kidney</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.5"><span class="ltx_text" id="A3.T5.2.1.1.1.5.1" style="font-size:90%;">Right Kidney</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.6"><span class="ltx_text" id="A3.T5.2.1.1.1.6.1" style="font-size:90%;">Stomach</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.7"><span class="ltx_text" id="A3.T5.2.1.1.1.7.1" style="font-size:90%;">Gallbladder</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.8"><span class="ltx_text" id="A3.T5.2.1.1.1.8.1" style="font-size:90%;">Esophagus</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.9"><span class="ltx_text" id="A3.T5.2.1.1.1.9.1" style="font-size:90%;">Pancreas</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.10"><span class="ltx_text" id="A3.T5.2.1.1.1.10.1" style="font-size:90%;">Duodenum</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.11"><span class="ltx_text" id="A3.T5.2.1.1.1.11.1" style="font-size:90%;">Colon</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.12"><span class="ltx_text" id="A3.T5.2.1.1.1.12.1" style="font-size:90%;">Small Bowel</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A3.T5.2.1.1.1.13"><span class="ltx_text" id="A3.T5.2.1.1.1.13.1" style="font-size:90%;">Bladder</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T5.2.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T5.2.1.2.1.1"><span class="ltx_text" id="A3.T5.2.1.2.1.1.1" style="font-size:90%;">Real Data</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.2"><span class="ltx_text" id="A3.T5.2.1.2.1.2.1" style="font-size:90%;">0.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.3"><span class="ltx_text" id="A3.T5.2.1.2.1.3.1" style="font-size:90%;">0.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.4"><span class="ltx_text" id="A3.T5.2.1.2.1.4.1" style="font-size:90%;">0.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.5"><span class="ltx_text" id="A3.T5.2.1.2.1.5.1" style="font-size:90%;">0.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.6"><span class="ltx_text" id="A3.T5.2.1.2.1.6.1" style="font-size:90%;">0.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.7"><span class="ltx_text" id="A3.T5.2.1.2.1.7.1" style="font-size:90%;">0.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.8"><span class="ltx_text" id="A3.T5.2.1.2.1.8.1" style="font-size:90%;">0.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.9"><span class="ltx_text" id="A3.T5.2.1.2.1.9.1" style="font-size:90%;">0.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.10"><span class="ltx_text" id="A3.T5.2.1.2.1.10.1" style="font-size:90%;">0.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.11"><span class="ltx_text" id="A3.T5.2.1.2.1.11.1" style="font-size:90%;">0.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.12"><span class="ltx_text" id="A3.T5.2.1.2.1.12.1" style="font-size:90%;">0.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T5.2.1.2.1.13"><span class="ltx_text" id="A3.T5.2.1.2.1.13.1" style="font-size:90%;">0.91</span></td>
</tr>
<tr class="ltx_tr" id="A3.T5.2.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r" id="A3.T5.2.1.3.2.1"><span class="ltx_text" id="A3.T5.2.1.3.2.1.1" style="font-size:90%;">Synthetic Data</span></th>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.2"><span class="ltx_text" id="A3.T5.2.1.3.2.2.1" style="font-size:90%;">0.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.3"><span class="ltx_text" id="A3.T5.2.1.3.2.3.1" style="font-size:90%;">0.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.4"><span class="ltx_text" id="A3.T5.2.1.3.2.4.1" style="font-size:90%;">0.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.5"><span class="ltx_text" id="A3.T5.2.1.3.2.5.1" style="font-size:90%;">0.95</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.6"><span class="ltx_text" id="A3.T5.2.1.3.2.6.1" style="font-size:90%;">0.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.7"><span class="ltx_text" id="A3.T5.2.1.3.2.7.1" style="font-size:90%;">0.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.8"><span class="ltx_text" id="A3.T5.2.1.3.2.8.1" style="font-size:90%;">0.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.9"><span class="ltx_text" id="A3.T5.2.1.3.2.9.1" style="font-size:90%;">0.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.10"><span class="ltx_text" id="A3.T5.2.1.3.2.10.1" style="font-size:90%;">0.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.11"><span class="ltx_text" id="A3.T5.2.1.3.2.11.1" style="font-size:90%;">0.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.12"><span class="ltx_text" id="A3.T5.2.1.3.2.12.1" style="font-size:90%;">0.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A3.T5.2.1.3.2.13"><span class="ltx_text" id="A3.T5.2.1.3.2.13.1" style="font-size:90%;">0.86</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table S5: </span>Segmentation performance on synthetic data. Synthetic data is generated using the MAISI CT Generation task and evaluated with the VISTA 3D <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib26" title="">26</a>]</cite> segmentation model. DSC are presented for both synthetic and real data on the unseen WORD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11169v1#bib.bib41" title="">41</a>]</cite> dataset. The results demonstrate that the segmentation model achieves comparable performance on major organs (<em class="ltx_emph ltx_font_italic" id="A3.T5.19.1">e.g</em>.<span class="ltx_text" id="A3.T5.20.2"></span>, liver, spleen, kidney) for both synthetic and real data. However, smaller organs (<em class="ltx_emph ltx_font_italic" id="A3.T5.21.3">e.g</em>.<span class="ltx_text" id="A3.T5.22.4"></span>, gallbladder, duodenum, pancreas) show a more pronounced performance gap between synthetic and real data. Addressing this gap presents a promising direction for future research.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 13 18:09:01 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
