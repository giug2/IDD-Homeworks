<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.02534] AngOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model</title><meta property="og:description" content="In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="AngOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="AngOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.02534">

<!--Generated on Sun May  5 18:16:23 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">AngOFA: Leveraging OFA Embedding 
<br class="ltx_break">Initialization and Synthetic Data for 
<br class="ltx_break">Angolan Language Model</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Osvaldo Luamba Quinjica 
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">Beijing Institute of Technology
<br class="ltx_break">Beijing, China 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">aosiwaduo@outlook.com</span> 
<br class="ltx_break">&amp;David Ifeoluwa Adelani 
<br class="ltx_break">Department of Computer Science
<br class="ltx_break">University College London 
<br class="ltx_break">United Kingdom 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_typewriter">d.adelani@ucl.ac.uk</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">In recent years, the development of pre-trained language models (PLMs) has gained momentum, showcasing their capacity to transcend linguistic barriers and facilitate knowledge transfer across diverse languages. However, this progress has predominantly bypassed the inclusion of very-low resource languages, creating a notable void in the multilingual landscape. This paper addresses this gap by introducing four tailored PLMs specifically finetuned for Angolan languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In this paper, we survey the role of informed embedding initialization and synthetic data in enhancing the performance of MAFT models in downstream tasks. We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA (an effective embedding initialization) by 12.3 and 3.8 points respectively.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para ltx_noindent">
<p id="S1.p1.1" class="ltx_p">Significant advancements have marked the progress of language models and evaluation datasets across various global languages <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>; Conneau et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>; Workshop et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Xue et al., <a href="#bib.bib29" title="" class="ltx_ref">2021</a>)</cite>. Nevertheless, this progress has often bypassed numerous African languages, creating a significant gap. Simultaneously, the majority of African-centric language models have overlooked the inclusion of Angolan languages <cite class="ltx_cite ltx_citemacro_citep">(Dossou et al., <a href="#bib.bib9" title="" class="ltx_ref">2022</a>; Alabi et al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>; Ogueji et al., <a href="#bib.bib19" title="" class="ltx_ref">2021</a>)</cite>. Efforts within the AfricaNLP community have been commendable in broadening downstream evaluation datasets <cite class="ltx_cite ltx_citemacro_citep">(Adelani et al., <a href="#bib.bib3" title="" class="ltx_ref">2021</a>; <a href="#bib.bib2" title="" class="ltx_ref">2022</a>; Muhammad et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>; Ma et al., <a href="#bib.bib15" title="" class="ltx_ref">2023</a>)</cite>. However, despite these initiatives, Angolan languages still lack representation.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p id="S1.p2.1" class="ltx_p">In the pursuit of developing a multilingual pre-trained language model (PLM), there are two primary approaches. The first entails building a model from scratch, training it directly on multiple languages, employing a specific self-supervised learning such as masked language modeling <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a href="#bib.bib7" title="" class="ltx_ref">2019</a>)</cite>. An alternative approach is multilingual adaptive fine-tuning (MAFT) which involves adapting an existing multilingual pretrained language model to a new set of languages <cite class="ltx_cite ltx_citemacro_citep">(Alabi et al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>; Wang et al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>; ImaniGooghari et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>. MAFT gains favor for its resource efficiency, especially in scenarios where computational budgets pose constraints amid the escalating model sizes <cite class="ltx_cite ltx_citemacro_citep">(Tay et al., <a href="#bib.bib24" title="" class="ltx_ref">2022</a>; Gupta et al., <a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>. The performance of MAFT can be further enhanced by introducing new vocabulary tokens for the additional languages and employing non-Gaussian embedding initialization <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a href="#bib.bib16" title="" class="ltx_ref">2022</a>; Dobler &amp; de Melo, <a href="#bib.bib8" title="" class="ltx_ref">2023</a>; Liu et al., <a href="#bib.bib13" title="" class="ltx_ref">2023a</a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p id="S1.p3.1" class="ltx_p">In this paper, we introduce the first set of multilingual PLMs tailored for five Angolan languages using the MAFT approach. We compare PLMs developed through MAFT with and without informed embedding initialization, denoted as <span id="S1.p3.1.1" class="ltx_text ltx_font_smallcaps">AngOFA </span>and <span id="S1.p3.1.2" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>, respectively. Leveraging OFA approach to perform embedding initialization before performing MAFT, our findings reveal that <span id="S1.p3.1.3" class="ltx_text ltx_font_smallcaps">AngOFA </span>significantly outperforms <span id="S1.p3.1.4" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>and OFA, underscoring the substantial performance improvements achievable through the incorporation of informed embedding initialization and synthetic data.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Angolan Languages</h2>

<div id="S2.p1" class="ltx_para ltx_noindent">
<p id="S2.p1.1" class="ltx_p">Boasting a rich linguistic landscape comprising more than 40 languages and a population of 32 million people, Angolan languages include Portuguese, some Khoisan languages, and mostly Bantu languages from the Niger-Congo family. Despite this linguistic diversity, there is a notable scarcity of literature, radio, or television programming in native Angolan languages. All languages in Angola are written using the Latin script, and many share common digraphs. Due to data scarcity, our focus will primarily revolve around the five most spoken Angolan languages: Umbundu, Kimbundu, Kikongo, Chokwe, and Luba-Kasai. See Table <a href="#S2.T1" title="Table 1 ‣ 2 Angolan Languages ‣ AngOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> for more details.</p>
</div>
<figure id="S2.T1" class="ltx_table">
<div id="S2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:134.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.7pt,5.2pt) scale(0.928465683089303,0.928465683089303) ;">
<table id="S2.T1.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S2.T1.1.1.1.1" class="ltx_tr">
<td id="S2.T1.1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S2.T1.1.1.1.1.2" class="ltx_td ltx_align_right ltx_border_tt"><span id="S2.T1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Bantu</span></td>
<td id="S2.T1.1.1.1.1.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_tt"><span id="S2.T1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">No.</span></td>
<td id="S2.T1.1.1.1.1.4" class="ltx_td ltx_align_right ltx_border_tt"><span id="S2.T1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">NLLB</span></td>
<td id="S2.T1.1.1.1.1.5" class="ltx_td ltx_align_right ltx_border_tt"><span id="S2.T1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">Synthetic</span></td>
<td id="S2.T1.1.1.1.1.6" class="ltx_td ltx_align_right ltx_border_tt"><span id="S2.T1.1.1.1.1.6.1" class="ltx_text ltx_font_bold">Combined</span></td>
<td id="S2.T1.1.1.1.1.7" class="ltx_td ltx_align_right ltx_border_tt"><span id="S2.T1.1.1.1.1.7.1" class="ltx_text ltx_font_bold">Combined</span></td>
</tr>
<tr id="S2.T1.1.1.2.2" class="ltx_tr">
<td id="S2.T1.1.1.2.2.1" class="ltx_td ltx_align_left"><span id="S2.T1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Language</span></td>
<td id="S2.T1.1.1.2.2.2" class="ltx_td ltx_align_right"><span id="S2.T1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Zone</span></td>
<td id="S2.T1.1.1.2.2.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S2.T1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Speakers</span></td>
<td id="S2.T1.1.1.2.2.4" class="ltx_td ltx_align_right"><span id="S2.T1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Corpus (MB)</span></td>
<td id="S2.T1.1.1.2.2.5" class="ltx_td ltx_align_right"><span id="S2.T1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Corpus (MB)</span></td>
<td id="S2.T1.1.1.2.2.6" class="ltx_td ltx_align_right"><span id="S2.T1.1.1.2.2.6.1" class="ltx_text ltx_font_bold">Corpus (MB)</span></td>
<td id="S2.T1.1.1.2.2.7" class="ltx_td ltx_align_right"><span id="S2.T1.1.1.2.2.7.1" class="ltx_text ltx_font_bold">No. Sentences</span></td>
</tr>
<tr id="S2.T1.1.1.3.3" class="ltx_tr">
<td id="S2.T1.1.1.3.3.1" class="ltx_td ltx_align_left ltx_border_t">Chokwe (cjk)</td>
<td id="S2.T1.1.1.3.3.2" class="ltx_td ltx_align_right ltx_border_t">Zone K</td>
<td id="S2.T1.1.1.3.3.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">0.5M</td>
<td id="S2.T1.1.1.3.3.4" class="ltx_td ltx_align_right ltx_border_t">11.3</td>
<td id="S2.T1.1.1.3.3.5" class="ltx_td ltx_align_right ltx_border_t">108.2</td>
<td id="S2.T1.1.1.3.3.6" class="ltx_td ltx_align_right ltx_border_t">119.5</td>
<td id="S2.T1.1.1.3.3.7" class="ltx_td ltx_align_right ltx_border_t">878,824</td>
</tr>
<tr id="S2.T1.1.1.4.4" class="ltx_tr">
<td id="S2.T1.1.1.4.4.1" class="ltx_td ltx_align_left">Kimbundu (kmb)</td>
<td id="S2.T1.1.1.4.4.2" class="ltx_td ltx_align_right">Zone H</td>
<td id="S2.T1.1.1.4.4.3" class="ltx_td ltx_align_right ltx_border_r">1.7M</td>
<td id="S2.T1.1.1.4.4.4" class="ltx_td ltx_align_right">10</td>
<td id="S2.T1.1.1.4.4.5" class="ltx_td ltx_align_right">98.5</td>
<td id="S2.T1.1.1.4.4.6" class="ltx_td ltx_align_right">108.5</td>
<td id="S2.T1.1.1.4.4.7" class="ltx_td ltx_align_right">800,603</td>
</tr>
<tr id="S2.T1.1.1.5.5" class="ltx_tr">
<td id="S2.T1.1.1.5.5.1" class="ltx_td ltx_align_left">Kikongo (kon)</td>
<td id="S2.T1.1.1.5.5.2" class="ltx_td ltx_align_right">Zone H</td>
<td id="S2.T1.1.1.5.5.3" class="ltx_td ltx_align_right ltx_border_r">2M</td>
<td id="S2.T1.1.1.5.5.4" class="ltx_td ltx_align_right">112.1</td>
<td id="S2.T1.1.1.5.5.5" class="ltx_td ltx_align_right">107.9</td>
<td id="S2.T1.1.1.5.5.6" class="ltx_td ltx_align_right">220</td>
<td id="S2.T1.1.1.5.5.7" class="ltx_td ltx_align_right">2,189,413</td>
</tr>
<tr id="S2.T1.1.1.6.6" class="ltx_tr">
<td id="S2.T1.1.1.6.6.1" class="ltx_td ltx_align_left">Luba-Kasai (lua)</td>
<td id="S2.T1.1.1.6.6.2" class="ltx_td ltx_align_right">Zone L</td>
<td id="S2.T1.1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_r">0.06M</td>
<td id="S2.T1.1.1.6.6.4" class="ltx_td ltx_align_right">133.2</td>
<td id="S2.T1.1.1.6.6.5" class="ltx_td ltx_align_right">113.9</td>
<td id="S2.T1.1.1.6.6.6" class="ltx_td ltx_align_right">247.1</td>
<td id="S2.T1.1.1.6.6.7" class="ltx_td ltx_align_right">2,415,794</td>
</tr>
<tr id="S2.T1.1.1.7.7" class="ltx_tr">
<td id="S2.T1.1.1.7.7.1" class="ltx_td ltx_align_left">Umbundu (umb)</td>
<td id="S2.T1.1.1.7.7.2" class="ltx_td ltx_align_right">Zone R</td>
<td id="S2.T1.1.1.7.7.3" class="ltx_td ltx_align_right ltx_border_r">6M</td>
<td id="S2.T1.1.1.7.7.4" class="ltx_td ltx_align_right">15.1</td>
<td id="S2.T1.1.1.7.7.5" class="ltx_td ltx_align_right">98. 5</td>
<td id="S2.T1.1.1.7.7.6" class="ltx_td ltx_align_right">113.6</td>
<td id="S2.T1.1.1.7.7.7" class="ltx_td ltx_align_right">902,961</td>
</tr>
<tr id="S2.T1.1.1.8.8" class="ltx_tr">
<td id="S2.T1.1.1.8.8.1" class="ltx_td ltx_align_left ltx_border_bb ltx_border_t">Total</td>
<td id="S2.T1.1.1.8.8.2" class="ltx_td ltx_border_bb ltx_border_t"></td>
<td id="S2.T1.1.1.8.8.3" class="ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t">10.2M</td>
<td id="S2.T1.1.1.8.8.4" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">281.6</td>
<td id="S2.T1.1.1.8.8.5" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">527</td>
<td id="S2.T1.1.1.8.8.6" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">808.6</td>
<td id="S2.T1.1.1.8.8.7" class="ltx_td ltx_align_right ltx_border_bb ltx_border_t">7,187,595</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span><span id="S2.T1.3.1" class="ltx_text ltx_font_bold">Language Information and Statistics</span>: Summary of language, language family, number of speakers, number of sentences. All languages belong to the Niger-Congo/Bantu family, we state the Bantu Zones according to <cite class="ltx_cite ltx_citemacro_citep">(Smith, <a href="#bib.bib22" title="" class="ltx_ref">1949</a>)</cite>. Synthetic corpus was generated using NLLB-600M machine translation model</figcaption>
</figure>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approaches to improve MAFT </h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Vocabulary Expansion</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p id="S3.SS1.p1.1" class="ltx_p">PLMs are proned to Out-of-Vocabulary (OOV) tokens for languages or scripts uncovered during pre-training. The situation is more pronounced for unseen scripts <cite class="ltx_cite ltx_citemacro_citep">(Adelani et al., <a href="#bib.bib3" title="" class="ltx_ref">2021</a>; Pfeiffer et al., <a href="#bib.bib20" title="" class="ltx_ref">2021</a>)</cite>, one of the most effective way of dealing with this is to expand the vocabulary of the PLM to cover new tokens <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>. Glot-500 <cite class="ltx_cite ltx_citemacro_citep">(ImaniGooghari et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite> was created by first expanding the vocabulary of XLM-R from 250K to 400K before MAFT. However, the new tokens added were randomly initialized.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>OFA: Embedding Factorization</h3>

<div id="S3.SS2.p1" class="ltx_para ltx_noindent">
<p id="S3.SS2.p1.1" class="ltx_p">OFA addresses two problems of adapting PLMs to new languages (1) the random initialization of embeddings for new subwords fails to exploit the lexical knowledge encoded in the source model (2) the introduction of additional parameters poses potential obstacles to the efficient training of the finetuned model <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib13" title="" class="ltx_ref">2023a</a>)</cite>. OFA solves these problems by leveraging both external multilingual embeddings and embeddings in the source PLM to initialize the embeddings of new subwords. In its approach, OFA factorizes the embeddings matrix of the source PLM into two smaller matrices as replacements. Within a lower-dimensional space, the embeddings of non-overlapping new subwords are expressed as combinations of source PLM subword embeddings. These combinations are weighted by similarities derived from well-aligned external multilingual embeddings, i.e., ColexNet+ <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib14" title="" class="ltx_ref">2023b</a>)</cite>, covering more than one thousand languages. Overlapping subword embeddings are directly copied. This approach ensures that embeddings for subwords shared between the source PLM and the extended vocabulary are integrated, preserving continuity in representation. To complete the process, OFA duplicates all non-embedding parameters from the source PLM model, and the source tokenizer is substituted with the target tokenizer post-vocabulary extension.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Synthetic data for language modeling</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p id="S3.SS3.p1.1" class="ltx_p">For languages lacking sufficient pre-training data, synthetic data can be generated through dictionary augmentation <cite class="ltx_cite ltx_citemacro_citep">(Reid et al., <a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite> or machine translation (MT) model—an approach very popular in MT research known as back-translation is an effective way to improve MT model for low-resource languages <cite class="ltx_cite ltx_citemacro_citep">(Sugiyama &amp; Yoshinaga, <a href="#bib.bib23" title="" class="ltx_ref">2019</a>; Xia et al., <a href="#bib.bib28" title="" class="ltx_ref">2019</a>)</cite>. In this paper, we utilize synthetic data obtained through machine translation as described in  <cite class="ltx_cite ltx_citemacro_citep">(Adelani et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>. The authors generated machine-translated data for 34 African languages(including Angolan languages) with less than 10MB of data, using the English news commentary dataset  <cite class="ltx_cite ltx_citemacro_citep">(Kocmi et al., <a href="#bib.bib12" title="" class="ltx_ref">2022</a>)</cite>, which contains over 600K sentences.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Data</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Training data</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p id="S4.SS1.p1.1" class="ltx_p">We leveraged the NLLB dataset <cite class="ltx_cite ltx_citemacro_citep">(NLLB-Team et al., <a href="#bib.bib18" title="" class="ltx_ref">2022</a>)</cite>, excluding English translations, and focused solely on Kimbundu, Umbundu, Kikongo, Chokwe, and Luba-Kasai. These languages were concatenated into a single file as our pre-training corpus. Additionally, we added synthetic data generated through NLLB. <a href="#S2.T1" title="Table 1 ‣ 2 Angolan Languages ‣ AngOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model" class="ltx_ref ltx_refmacro_autoref"><span class="ltx_text ltx_ref_tag">Table 1</span></a> shows the details of the monolingual data.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Evaluation data</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p id="S4.SS2.p1.1" class="ltx_p">In our work, we evaluated on SIB-200 <cite class="ltx_cite ltx_citemacro_citep">(Adelani et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>, a text classification dataset that provides train/dev/test sets with 7 classes in more than 200 African languages and dialects. The distribution of the classes are: science/technology (252), travel (198), politics (146), sports (122), health (110), entertainment (93), geography (83). SIB-200 is the only benchmark dataset that covers Angolan languages. We evaluated only on the subset of Angolan languages covered in this work.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>

<div id="S5.p1" class="ltx_para ltx_noindent">
<p id="S5.p1.1" class="ltx_p">We utilized the cross-lingual capabilities of XLM-R <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite> for training, resulting in the creation of a novel set of PLMs<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Models available at <a target="_blank" href="https://github.com/zuela-ai/ANGOFA" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/zuela-ai/ANGOFA</a></span></span></span>: <span id="S5.p1.1.1" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>and <span id="S5.p1.1.2" class="ltx_text ltx_font_smallcaps">AngOFA</span>. These models, underwent distinct fine-tuning processes. Specifically, <span id="S5.p1.1.3" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>underwent fine-tuning using the MAFT approach outlined in  <cite class="ltx_cite ltx_citemacro_cite">Alabi et al. (<a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>, with two variants—one trained solely on monolingual data (281.6 MB), and the other incorporating both monolingual and synthetic data (808.7 MB).</p>
</div>
<div id="S5.p2" class="ltx_para ltx_noindent">
<p id="S5.p2.1" class="ltx_p">Similarly, <span id="S5.p2.1.1" class="ltx_text ltx_font_smallcaps">AngOFA </span>also underwent two variations of fine-tuning, utilizing the datasets in the same manner as <span id="S5.p2.1.2" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>. However, <span id="S5.p2.1.3" class="ltx_text ltx_font_smallcaps">AngOFA </span>followed the configurations outlined for <span id="S5.p2.1.4" class="ltx_text ltx_font_typewriter">ofa-multi-768</span>, as described in  <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib13" title="" class="ltx_ref">2023a</a>)</cite>.
We opted to maintain 768 as the only latent dimension in our experiments based on insights from <cite class="ltx_cite ltx_citemacro_citep">(ImaniGooghari et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>; Liu et al., <a href="#bib.bib13" title="" class="ltx_ref">2023a</a>)</cite> and further supported by preliminary results from our own experiments. These findings revealed evidence of information loss in lower dimensions, particularly noticeable in tasks such as text classification. This dataset partitioning approach aimed to investigate the effects of the MAFT and OFA approaches, both with and without synthetic data, on model performance.</p>
</div>
<div id="S5.p3" class="ltx_para ltx_noindent">
<p id="S5.p3.1" class="ltx_p">We compared our new models to the following baseline models:</p>
</div>
<div id="S5.p4" class="ltx_para ltx_noindent">
<ol id="S5.I1" class="ltx_enumerate">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">XLM-R <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a href="#bib.bib6" title="" class="ltx_ref">2020</a>)</cite>: an encoder-only model that underwent pre-training on 100 languages through a masked language model objective. XLM-R does not cover any language evaluated in this work.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Serengeti <cite class="ltx_cite ltx_citemacro_citep">(Adebara et al., <a href="#bib.bib1" title="" class="ltx_ref">2023</a>)</cite>: trained on 500 African languages, including 10 high-resource ones. It includes Kimbundu, Umbundu, and Chokwe.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Glot-500 <cite class="ltx_cite ltx_citemacro_citep">(ImaniGooghari et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>: derived from XLM-R, was extended to cover 500 languages by expanding its vocabulary from 250K to 400K, thus accommodating new tokens representing 400 languages previously absent in XLM-R. Glot-500 covers all Angolan languages used in our evaluation.</p>
</div>
</li>
<li id="S5.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S5.I1.i4.p1" class="ltx_para">
<p id="S5.I1.i4.p1.1" class="ltx_p">AfroXLMR-base <cite class="ltx_cite ltx_citemacro_citep">(Alabi et al., <a href="#bib.bib5" title="" class="ltx_ref">2022</a>)</cite>: developed using the MAFT approach, it covers 20 languages with a monolingual corpus of at least 50MB. Angolan languages are not included.</p>
</div>
</li>
<li id="S5.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span> 
<div id="S5.I1.i5.p1" class="ltx_para">
<p id="S5.I1.i5.p1.1" class="ltx_p">AfroXLMR-base-76L <cite class="ltx_cite ltx_citemacro_citep">(Adelani et al., <a href="#bib.bib4" title="" class="ltx_ref">2023</a>)</cite>: developed using the MAFT approach, it covers languages with at least 10MB of data on the web. It expands coverage to include more languages, notably those listed in the NLLB-200 MT model. Synthetic data was also generated for approximately 30 languages with limited data, including all five Angolan languages. In total, it covers 76 languages.</p>
</div>
</li>
<li id="S5.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">6.</span> 
<div id="S5.I1.i6.p1" class="ltx_para ltx_noindent">
<p id="S5.I1.i6.p1.1" class="ltx_p">OFA <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a href="#bib.bib13" title="" class="ltx_ref">2023a</a>)</cite>: integrates OFA embedding initialization alongside MAFT using Glot500-c <cite class="ltx_cite ltx_citemacro_citep">(ImaniGooghari et al., <a href="#bib.bib11" title="" class="ltx_ref">2023</a>)</cite>, thus including all languages addressed in this work.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results and Discussion</h2>

<figure id="S6.T2" class="ltx_table">
<div id="S6.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:147.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-22.6pt,7.6pt) scale(0.905600986005175,0.905600986005175) ;">
<table id="S6.T2.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S6.T2.1.1.1.1" class="ltx_tr">
<td id="S6.T2.1.1.1.1.1" class="ltx_td ltx_border_tt"></td>
<td id="S6.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T2.1.1.1.1.2.1" class="ltx_text ltx_font_italic">Pre-trained (scratch)</span></td>
<td id="S6.T2.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt" colspan="3"><span id="S6.T2.1.1.1.1.3.1" class="ltx_text ltx_font_italic">MAFT</span></td>
<td id="S6.T2.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_tt" colspan="2">
<span id="S6.T2.1.1.1.1.4.1" class="ltx_text ltx_font_italic">MAFT</span> + syn. data</td>
<td id="S6.T2.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_tt" colspan="2"><span id="S6.T2.1.1.1.1.5.1" class="ltx_text ltx_font_italic">OFA</span></td>
<td id="S6.T2.1.1.1.1.6" class="ltx_td ltx_align_right ltx_border_tt"><span id="S6.T2.1.1.1.1.6.1" class="ltx_text ltx_font_italic">OFA + syn</span></td>
</tr>
<tr id="S6.T2.1.1.2.2" class="ltx_tr">
<td id="S6.T2.1.1.2.2.1" class="ltx_td"></td>
<td id="S6.T2.1.1.2.2.2" class="ltx_td"></td>
<td id="S6.T2.1.1.2.2.3" class="ltx_td ltx_border_r"></td>
<td id="S6.T2.1.1.2.2.4" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.2.2.4.1" class="ltx_text ltx_font_bold">Glot</span></td>
<td id="S6.T2.1.1.2.2.5" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.2.2.5.1" class="ltx_text ltx_font_bold">Afro</span></td>
<td id="S6.T2.1.1.2.2.6" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T2.1.1.2.2.6.1" class="ltx_text ltx_font_smallcaps">Ang</span></td>
<td id="S6.T2.1.1.2.2.7" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.2.2.7.1" class="ltx_text ltx_font_bold">Afro</span></td>
<td id="S6.T2.1.1.2.2.8" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T2.1.1.2.2.8.1" class="ltx_text ltx_font_smallcaps">Ang</span></td>
<td id="S6.T2.1.1.2.2.9" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.2.2.9.1" class="ltx_text ltx_font_smallcaps">Ang</span></td>
<td id="S6.T2.1.1.2.2.10" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T2.1.1.2.2.10.1" class="ltx_text ltx_font_bold">OFA</span></td>
<td id="S6.T2.1.1.2.2.11" class="ltx_td"></td>
</tr>
<tr id="S6.T2.1.1.3.3" class="ltx_tr">
<td id="S6.T2.1.1.3.3.1" class="ltx_td ltx_align_left"><span id="S6.T2.1.1.3.3.1.1" class="ltx_text ltx_font_bold">Lang.</span></td>
<td id="S6.T2.1.1.3.3.2" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.3.3.2.1" class="ltx_text ltx_font_bold">XLM-R</span></td>
<td id="S6.T2.1.1.3.3.3" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T2.1.1.3.3.3.1" class="ltx_text ltx_font_bold">Serengeti</span></td>
<td id="S6.T2.1.1.3.3.4" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.3.3.4.1" class="ltx_text ltx_font_bold">500</span></td>
<td id="S6.T2.1.1.3.3.5" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.3.3.5.1" class="ltx_text ltx_font_bold">XLMR</span></td>
<td id="S6.T2.1.1.3.3.6" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T2.1.1.3.3.6.1" class="ltx_text ltx_font_smallcaps">XLM-R</span></td>
<td id="S6.T2.1.1.3.3.7" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.3.3.7.1" class="ltx_text ltx_font_bold">XLMR76</span></td>
<td id="S6.T2.1.1.3.3.8" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T2.1.1.3.3.8.1" class="ltx_text ltx_font_smallcaps">XLM-R</span></td>
<td id="S6.T2.1.1.3.3.9" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.3.3.9.1" class="ltx_text ltx_font_smallcaps">OFA</span></td>
<td id="S6.T2.1.1.3.3.10" class="ltx_td ltx_align_right ltx_border_r"><span id="S6.T2.1.1.3.3.10.1" class="ltx_text ltx_font_bold">500</span></td>
<td id="S6.T2.1.1.3.3.11" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.3.3.11.1" class="ltx_text ltx_font_smallcaps">AngOFA</span></td>
</tr>
<tr id="S6.T2.1.1.4.4" class="ltx_tr">
<td id="S6.T2.1.1.4.4.1" class="ltx_td ltx_align_left ltx_border_t">cjk</td>
<td id="S6.T2.1.1.4.4.2" class="ltx_td ltx_align_right ltx_border_t">41.3</td>
<td id="S6.T2.1.1.4.4.3" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">43.2</td>
<td id="S6.T2.1.1.4.4.4" class="ltx_td ltx_align_right ltx_border_t">42.9</td>
<td id="S6.T2.1.1.4.4.5" class="ltx_td ltx_align_right ltx_border_t">51.3</td>
<td id="S6.T2.1.1.4.4.6" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">43.6</td>
<td id="S6.T2.1.1.4.4.7" class="ltx_td ltx_align_right ltx_border_t">55.6</td>
<td id="S6.T2.1.1.4.4.8" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">51.7</td>
<td id="S6.T2.1.1.4.4.9" class="ltx_td ltx_align_right ltx_border_t">46.3</td>
<td id="S6.T2.1.1.4.4.10" class="ltx_td ltx_align_right ltx_border_r ltx_border_t">52.8</td>
<td id="S6.T2.1.1.4.4.11" class="ltx_td ltx_align_right ltx_border_t"><span id="S6.T2.1.1.4.4.11.1" class="ltx_text ltx_font_bold">58.4</span></td>
</tr>
<tr id="S6.T2.1.1.5.5" class="ltx_tr">
<td id="S6.T2.1.1.5.5.1" class="ltx_td ltx_align_left">kmb</td>
<td id="S6.T2.1.1.5.5.2" class="ltx_td ltx_align_right">44.8</td>
<td id="S6.T2.1.1.5.5.3" class="ltx_td ltx_align_right ltx_border_r">46.9</td>
<td id="S6.T2.1.1.5.5.4" class="ltx_td ltx_align_right">43.5</td>
<td id="S6.T2.1.1.5.5.5" class="ltx_td ltx_align_right">50.6</td>
<td id="S6.T2.1.1.5.5.6" class="ltx_td ltx_align_right ltx_border_r">50.2</td>
<td id="S6.T2.1.1.5.5.7" class="ltx_td ltx_align_right">58.5</td>
<td id="S6.T2.1.1.5.5.8" class="ltx_td ltx_align_right ltx_border_r">56.6</td>
<td id="S6.T2.1.1.5.5.9" class="ltx_td ltx_align_right">58.5</td>
<td id="S6.T2.1.1.5.5.10" class="ltx_td ltx_align_right ltx_border_r">63.2</td>
<td id="S6.T2.1.1.5.5.11" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.5.5.11.1" class="ltx_text ltx_font_bold">64.7</span></td>
</tr>
<tr id="S6.T2.1.1.6.6" class="ltx_tr">
<td id="S6.T2.1.1.6.6.1" class="ltx_td ltx_align_left">kon</td>
<td id="S6.T2.1.1.6.6.2" class="ltx_td ltx_align_right">67.8</td>
<td id="S6.T2.1.1.6.6.3" class="ltx_td ltx_align_right ltx_border_r">69.1</td>
<td id="S6.T2.1.1.6.6.4" class="ltx_td ltx_align_right">72.6</td>
<td id="S6.T2.1.1.6.6.5" class="ltx_td ltx_align_right">65.7</td>
<td id="S6.T2.1.1.6.6.6" class="ltx_td ltx_align_right ltx_border_r">72.5</td>
<td id="S6.T2.1.1.6.6.7" class="ltx_td ltx_align_right">77.2</td>
<td id="S6.T2.1.1.6.6.8" class="ltx_td ltx_align_right ltx_border_r">76.1</td>
<td id="S6.T2.1.1.6.6.9" class="ltx_td ltx_align_right">78.8</td>
<td id="S6.T2.1.1.6.6.10" class="ltx_td ltx_align_right ltx_border_r">76.9</td>
<td id="S6.T2.1.1.6.6.11" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.6.6.11.1" class="ltx_text ltx_font_bold">82.4</span></td>
</tr>
<tr id="S6.T2.1.1.7.7" class="ltx_tr">
<td id="S6.T2.1.1.7.7.1" class="ltx_td ltx_align_left">lua</td>
<td id="S6.T2.1.1.7.7.2" class="ltx_td ltx_align_right">54.5</td>
<td id="S6.T2.1.1.7.7.3" class="ltx_td ltx_align_right ltx_border_r">57.9</td>
<td id="S6.T2.1.1.7.7.4" class="ltx_td ltx_align_right">54.7</td>
<td id="S6.T2.1.1.7.7.5" class="ltx_td ltx_align_right">62.5</td>
<td id="S6.T2.1.1.7.7.6" class="ltx_td ltx_align_right ltx_border_r">65.4</td>
<td id="S6.T2.1.1.7.7.7" class="ltx_td ltx_align_right">64.4</td>
<td id="S6.T2.1.1.7.7.8" class="ltx_td ltx_align_right ltx_border_r">73.2</td>
<td id="S6.T2.1.1.7.7.9" class="ltx_td ltx_align_right">69.1</td>
<td id="S6.T2.1.1.7.7.10" class="ltx_td ltx_align_right ltx_border_r">68.6</td>
<td id="S6.T2.1.1.7.7.11" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.7.7.11.1" class="ltx_text ltx_font_bold">73.5</span></td>
</tr>
<tr id="S6.T2.1.1.8.8" class="ltx_tr">
<td id="S6.T2.1.1.8.8.1" class="ltx_td ltx_align_left">umb</td>
<td id="S6.T2.1.1.8.8.2" class="ltx_td ltx_align_right">50.4</td>
<td id="S6.T2.1.1.8.8.3" class="ltx_td ltx_align_right ltx_border_r">51.7</td>
<td id="S6.T2.1.1.8.8.4" class="ltx_td ltx_align_right">40.3</td>
<td id="S6.T2.1.1.8.8.5" class="ltx_td ltx_align_right">50.5</td>
<td id="S6.T2.1.1.8.8.6" class="ltx_td ltx_align_right ltx_border_r">54.9</td>
<td id="S6.T2.1.1.8.8.7" class="ltx_td ltx_align_right">61.0</td>
<td id="S6.T2.1.1.8.8.8" class="ltx_td ltx_align_right ltx_border_r">56.8</td>
<td id="S6.T2.1.1.8.8.9" class="ltx_td ltx_align_right">54.3</td>
<td id="S6.T2.1.1.8.8.10" class="ltx_td ltx_align_right ltx_border_r">61.8</td>
<td id="S6.T2.1.1.8.8.11" class="ltx_td ltx_align_right"><span id="S6.T2.1.1.8.8.11.1" class="ltx_text ltx_font_bold">63.3</span></td>
</tr>
<tr id="S6.T2.1.1.9.9" class="ltx_tr">
<td id="S6.T2.1.1.9.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_t"><span id="S6.T2.1.1.9.9.1.1" class="ltx_text ltx_font_bold">Ave.</span></td>
<td id="S6.T2.1.1.9.9.2" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">51.8</td>
<td id="S6.T2.1.1.9.9.3" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">53.7</td>
<td id="S6.T2.1.1.9.9.4" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">50.7</td>
<td id="S6.T2.1.1.9.9.5" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">56.1</td>
<td id="S6.T2.1.1.9.9.6" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">57.3</td>
<td id="S6.T2.1.1.9.9.7" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">63.3</td>
<td id="S6.T2.1.1.9.9.8" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">62.8</td>
<td id="S6.T2.1.1.9.9.9" class="ltx_td ltx_align_right ltx_border_b ltx_border_t">61.4</td>
<td id="S6.T2.1.1.9.9.10" class="ltx_td ltx_align_right ltx_border_b ltx_border_r ltx_border_t">64.6</td>
<td id="S6.T2.1.1.9.9.11" class="ltx_td ltx_align_right ltx_border_b ltx_border_t"><span id="S6.T2.1.1.9.9.11.1" class="ltx_text ltx_font_bold">68.4</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span><span id="S6.T2.3.1" class="ltx_text ltx_font_bold">Benchmark results</span>: comparing the effectiveness of OFA to random initialization before multilingual adaptive fine-tuning (MAFT)</figcaption>
</figure>
<div id="S6.p1" class="ltx_para ltx_noindent">
<p id="S6.p1.1" class="ltx_p">Table<a href="#S6.T2" title="Table 2 ‣ 6 Results and Discussion ‣ AngOFA: Leveraging OFA Embedding Initialization and Synthetic Data for Angolan Language Model" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the performance of our baseline models using the <span id="S6.p1.1.1" class="ltx_text ltx_font_bold">weighted F1 metric</span>. We discuss our key findings below:</p>
</div>
<section id="S6.SS0.SSS0.Px1" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">Region-specific PLMs are better than those pre-trained from scratch with many languages</h5>

<div id="S6.SS0.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSS0.Px1.p1.4" class="ltx_p">Our results shows that <span id="S6.SS0.SSS0.Px1.p1.4.1" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>created with MAFT performed better than XLM-R, AfroXLMR, Serengeti and Glot-500 with <math id="S6.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="+5.5" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S6.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mo id="S6.SS0.SSS0.Px1.p1.1.m1.1.1a" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">+</mo><mn id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">5.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1"><plus id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1"></plus><cn type="float" id="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.1.m1.1.1.2">5.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.1.m1.1c">+5.5</annotation></semantics></math>, <math id="S6.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="+1.2" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S6.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mo id="S6.SS0.SSS0.Px1.p1.2.m2.1.1a" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">+</mo><mn id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">1.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1"><plus id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1"></plus><cn type="float" id="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.2.m2.1.1.2">1.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.2.m2.1c">+1.2</annotation></semantics></math>, <math id="S6.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="+3.6" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.3.m3.1a"><mrow id="S6.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.cmml"><mo id="S6.SS0.SSS0.Px1.p1.3.m3.1.1a" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">+</mo><mn id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.2" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml">3.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1"><plus id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1"></plus><cn type="float" id="S6.SS0.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.3.m3.1.1.2">3.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.3.m3.1c">+3.6</annotation></semantics></math>, <math id="S6.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="+6.6" display="inline"><semantics id="S6.SS0.SSS0.Px1.p1.4.m4.1a"><mrow id="S6.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.cmml"><mo id="S6.SS0.SSS0.Px1.p1.4.m4.1.1a" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">+</mo><mn id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.2" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml">6.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px1.p1.4.m4.1b"><apply id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1"><plus id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1"></plus><cn type="float" id="S6.SS0.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S6.SS0.SSS0.Px1.p1.4.m4.1.1.2">6.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px1.p1.4.m4.1c">+6.6</annotation></semantics></math> points respectively. The last two PLMs have been pre-trained on 500+ languages with few Angolan languages but performed worse than AfroXLMR (adapted through MAFT to 20 languages), and <span id="S6.SS0.SSS0.Px1.p1.4.2" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>(adapted to five Angolan languages). This shows that region-specific PLMs covering related languages within the same language family can be more effective.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px2" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">MAFT results can be boosted by leveraging synthetic monolingual data</h5>

<div id="S6.SS0.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSS0.Px2.p1.1" class="ltx_p">By incorporating additional synthetic data, <span id="S6.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>(+SYN data) performance improved by <math id="S6.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="+5.5" display="inline"><semantics id="S6.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S6.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mo id="S6.SS0.SSS0.Px2.p1.1.m1.1.1a" xref="S6.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">+</mo><mn id="S6.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">5.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px2.p1.1.m1.1.1"><plus id="S6.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px2.p1.1.m1.1.1"></plus><cn type="float" id="S6.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px2.p1.1.m1.1.1.2">5.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px2.p1.1.m1.1c">+5.5</annotation></semantics></math> over the <span id="S6.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>without synthetic data. However, it failed to beat the performance of AfroXLMR-base-76L that has been trained on 76 African languages including all Angolan languages except for Luba-Kasai with the largest data. Our experiment showed that the adapted PLM to 76 languages performed better than Serengeti pre-trained on 500 languages, which further shows that we can create better PLMs to cover more languages through adaptation without the expensive process of pre-training from scratch.</p>
</div>
</section>
<section id="S6.SS0.SSS0.Px3" class="ltx_paragraph">
<h5 class="ltx_title ltx_title_paragraph">OFA embedding initialization with larger data is more effective</h5>

<div id="S6.SS0.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p id="S6.SS0.SSS0.Px3.p1.4" class="ltx_p">Models initialized with OFA demonstrated a consistent improvement compared with other baselines. This indicates that OFA, which explicitly leverages information encoded in the embeddings of the source model and external multilingual embeddings, is superior to random initialization. Notably, <span id="S6.SS0.SSS0.Px3.p1.4.1" class="ltx_text ltx_font_smallcaps">AngOFA</span>’s advantage over OFA is accentuated by its access to a significantly larger corpus of data for the respective languages through the use of synthetic data. Without the additional synthetic data <span id="S6.SS0.SSS0.Px3.p1.4.2" class="ltx_text ltx_font_smallcaps">AngOFA </span>performed worse than <span id="S6.SS0.SSS0.Px3.p1.4.3" class="ltx_text ltx_font_smallcaps">OFA</span> pre-trained on 500 languages with a drop of <math id="S6.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="-3.2" display="inline"><semantics id="S6.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S6.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mo id="S6.SS0.SSS0.Px3.p1.1.m1.1.1a" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.cmml">−</mo><mn id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">3.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1"><minus id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1"></minus><cn type="float" id="S6.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S6.SS0.SSS0.Px3.p1.1.m1.1.1.2">3.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px3.p1.1.m1.1c">-3.2</annotation></semantics></math>. However, when we trained on the synthetic data, <span id="S6.SS0.SSS0.Px3.p1.4.4" class="ltx_text ltx_font_smallcaps">AngOFA </span>achieved the best overall performance with <math id="S6.SS0.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="+16.6" display="inline"><semantics id="S6.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="S6.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mo id="S6.SS0.SSS0.Px3.p1.2.m2.1.1a" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.cmml">+</mo><mn id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">16.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1"><plus id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1"></plus><cn type="float" id="S6.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S6.SS0.SSS0.Px3.p1.2.m2.1.1.2">16.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px3.p1.2.m2.1c">+16.6</annotation></semantics></math> over XLM-R, <math id="S6.SS0.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="+12.3" display="inline"><semantics id="S6.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="S6.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mo id="S6.SS0.SSS0.Px3.p1.3.m3.1.1a" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.cmml">+</mo><mn id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">12.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1"><plus id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1"></plus><cn type="float" id="S6.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S6.SS0.SSS0.Px3.p1.3.m3.1.1.2">12.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px3.p1.3.m3.1c">+12.3</annotation></semantics></math> over AfroXLMR, and <math id="S6.SS0.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="+5.6" display="inline"><semantics id="S6.SS0.SSS0.Px3.p1.4.m4.1a"><mrow id="S6.SS0.SSS0.Px3.p1.4.m4.1.1" xref="S6.SS0.SSS0.Px3.p1.4.m4.1.1.cmml"><mo id="S6.SS0.SSS0.Px3.p1.4.m4.1.1a" xref="S6.SS0.SSS0.Px3.p1.4.m4.1.1.cmml">+</mo><mn id="S6.SS0.SSS0.Px3.p1.4.m4.1.1.2" xref="S6.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml">5.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS0.SSS0.Px3.p1.4.m4.1b"><apply id="S6.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.4.m4.1.1"><plus id="S6.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S6.SS0.SSS0.Px3.p1.4.m4.1.1"></plus><cn type="float" id="S6.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S6.SS0.SSS0.Px3.p1.4.m4.1.1.2">5.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS0.SSS0.Px3.p1.4.m4.1c">+5.6</annotation></semantics></math> over <span id="S6.SS0.SSS0.Px3.p1.4.5" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>(with synthetic data).</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion and Future work</h2>

<div id="S7.p1" class="ltx_para ltx_noindent">
<p id="S7.p1.1" class="ltx_p">This paper introduces four multilingual PLMs models tailored for Angolan languages. Our experimental findings illustrate that employing informed embedding initialization significantly enhances the performance of a MAFT model in downstream tasks. While models initialized with OFA exhibit superior results compared to their counterparts, even in the case where <span id="S7.p1.1.1" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>finetuned on a larger corpus of data for the respective languages performs poorly as compared to OFA finetuned on a smaller corpus. Nevertheless, the specific factors contributing to <span id="S7.p1.1.2" class="ltx_text ltx_font_smallcaps">AngXLM-R </span>’s superiority over OFA, especially in the context of Luba-Kassai, raise intriguing questions about the primary determinants influencing the performance of models in downstream tasks, including considerations like dataset size versus informed embedding initialization. These questions are left for future investigation. Furthermore, we aim to expand the application of OFA to more African languages for further exploration.</p>
</div>
<section id="S7.SS0.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Acknowledgments</h4>

<div id="S7.SS0.SSSx1.p1" class="ltx_para ltx_noindent">
<p id="S7.SS0.SSSx1.p1.1" class="ltx_p">This work was supported in part by Oracle Cloud credits and related resources provided by Oracle. David Adelani acknowledges the support of DeepMind Academic Fellowship programme.</p>
</div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adebara et al. (2023)</span>
<span class="ltx_bibblock">
Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides
Alcoba Inciarte.

</span>
<span class="ltx_bibblock">SERENGETI: Massively multilingual language models for Africa.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.),
<em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>,
pp.  1498–1537, Toronto, Canada, July 2023. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-acl.97</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.findings-acl.97" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.findings-acl.97</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. (2022)</span>
<span class="ltx_bibblock">
David Adelani, Graham Neubig, Sebastian Ruder, Shruti Rijhwani, Michael
Beukman, Chester Palen-Michel, Constantine Lignos, Jesujoba Alabi,
Shamsuddeen Muhammad, Peter Nabende, Cheikh M. Bamba Dione, Andiswa Bukula,
Rooweither Mabuya, Bonaventure F. P. Dossou, Blessing Sibanda, Happy Buzaaba,
Jonathan Mukiibi, Godson Kalipe, Derguene Mbaye, Amelia Taylor, Fatoumata
Kabore, Chris Chinenye Emezue, Anuoluwapo Aremu, Perez Ogayo, Catherine
Gitau, Edwin Munkoh-Buabeng, Victoire Memdjokam Koagne, Allahsera Auguste
Tapo, Tebogo Macucwa, Vukosi Marivate, Mboning Tchiaze Elvis, Tajuddeen
Gwadabe, Tosin Adewumi, Orevaoghene Ahia, Joyce Nakatumba-Nabende, Neo Lerato
Mokono, Ignatius Ezeani, Chiamaka Chukwuneke, Mofetoluwa Oluwaseun Adeyemi,
Gilles Quentin Hacheme, Idris Abdulmumin, Odunayo Ogundepo, Oreen Yousuf,
Tatiana Moteu, and Dietrich Klakow.

</span>
<span class="ltx_bibblock">MasakhaNER 2.0: Africa-centric transfer learning for named
entity recognition.

</span>
<span class="ltx_bibblock">In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.),
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing</em>, pp.  4488–4508, Abu Dhabi, United Arab Emirates,
December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.emnlp-main.298</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.emnlp-main.298" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.emnlp-main.298</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. (2021)</span>
<span class="ltx_bibblock">
David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D’souza, Julia
Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba, Shruti
Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime, Shamsuddeen H.
Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez Ogayo, Aremu
Anuoluwapo, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi, Seid Muhie
Yimam, Tajuddeen Rabiu Gwadabe, Ignatius Ezeani, Rubungo Andre Niyongabo,
Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba Ngom, Tosin
Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki, Emmanuel Anebi,
Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala, Samuel Oyerinde,
Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede, Yvonne Wambui, Victor
Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele Awokoya, Mouhamadane
MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi Nwaike, Degaga Wolde,
Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia, Bonaventure F. P. Dossou,
Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye Diallo, Adewale Akinfaderin,
Tendai Marengereke, and Salomey Osei.

</span>
<span class="ltx_bibblock">MasakhaNER: Named entity recognition for African languages.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Transactions of the Association for Computational Linguistics</em>,
9:1116–1131, 2021.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/tacl˙a˙00416</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.tacl-1.66" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.tacl-1.66</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adelani et al. (2023)</span>
<span class="ltx_bibblock">
David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba O.
Alabi, Yanke Mao, Haonan Gao, and Annie En-Shiun Lee.

</span>
<span class="ltx_bibblock">Sib-200: A simple, inclusive, and big evaluation dataset for topic
classification in 200+ languages and dialects, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2309.07445" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2309.07445</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alabi et al. (2022)</span>
<span class="ltx_bibblock">
Jesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow.

</span>
<span class="ltx_bibblock">Adapting pre-trained language models to African languages via
multilingual adaptive fine-tuning.

</span>
<span class="ltx_bibblock">In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James
Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia
Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan
Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond,
and Seung-Hoon Na (eds.), <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 29th International
Conference on Computational Linguistics</em>, pp.  4336–4349, Gyeongju,
Republic of Korea, October 2022. International Committee on Computational
Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.coling-1.382" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.coling-1.382</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Unsupervised cross-lingual representation learning at scale.

</span>
<span class="ltx_bibblock">In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault
(eds.), <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics</em>, pp.  8440–8451, Online, July 2020. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.acl-main.747</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2020.acl-main.747" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2020.acl-main.747</a>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio (eds.),
<em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2019 Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies,
Volume 1 (Long and Short Papers)</em>, pp.  4171–4186, Minneapolis, Minnesota,
June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1423</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/N19-1423" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/N19-1423</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dobler &amp; de Melo (2023)</span>
<span class="ltx_bibblock">
Konstantin Dobler and Gerard de Melo.

</span>
<span class="ltx_bibblock">FOCUS: Effective embedding initialization for monolingual
specialization of multilingual models.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali (eds.),
<em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural
Language Processing</em>, pp.  13440–13454, Singapore, December 2023.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.emnlp-main.829</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.emnlp-main.829" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.emnlp-main.829</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dossou et al. (2022)</span>
<span class="ltx_bibblock">
Bonaventure F. P. Dossou, Atnafu Lambebo Tonja, Oreen Yousuf, Salomey Osei,
Abigail Oppong, Iyanuoluwa Shode, Oluwabusayo Olufunke Awoyomi, and Chris
Emezue.

</span>
<span class="ltx_bibblock">AfroLM: A self-active learning-based multilingual pretrained
language model for 23 African languages.

</span>
<span class="ltx_bibblock">In Angela Fan, Iryna Gurevych, Yufang Hou, Zornitsa Kozareva, Sasha
Luccioni, Nafise Sadat Moosavi, Sujith Ravi, Gyuwan Kim, Roy Schwartz, and
Andreas Rücklé (eds.), <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of The Third Workshop on
Simple and Efficient Natural Language Processing (SustaiNLP)</em>, pp.  52–64,
Abu Dhabi, United Arab Emirates (Hybrid), December 2022. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.sustainlp-1.11</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.sustainlp-1.11" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.sustainlp-1.11</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al. (2023)</span>
<span class="ltx_bibblock">
Kshitij Gupta, Benjamin Thérien, Adam Ibrahim, Mats Leon Richter,
Quentin Gregory Anthony, Eugene Belilovsky, Irina Rish, and Timothée
Lesort.

</span>
<span class="ltx_bibblock">Continual pre-training of large language models: How to re-warm your
model?

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Workshop on Efficient Systems for Foundation Models @
ICML2023</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=pg7PUJe0Tl" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=pg7PUJe0Tl</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ImaniGooghari et al. (2023)</span>
<span class="ltx_bibblock">
Ayyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini,
Masoud Jalili Sabet, Nora Kassner, Chunlan Ma, Helmut Schmid, André
Martins, François Yvon, and Hinrich Schütze.

</span>
<span class="ltx_bibblock">Glot500: Scaling multilingual corpora and language models to 500
languages.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.),
<em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 61st Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pp.  1082–1117,
Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.acl-long.61</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.acl-long.61" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.acl-long.61</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi et al. (2022)</span>
<span class="ltx_bibblock">
Tom Kocmi, Rachel Bawden, Ondřej Bojar, Anton Dvorkovich, Christian
Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz,
Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto
Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Novák, Martin Popel,
and Maja Popović.

</span>
<span class="ltx_bibblock">Findings of the 2022 conference on machine translation (WMT22).

</span>
<span class="ltx_bibblock">In Philipp Koehn, Loïc Barrault, Ondřej Bojar, Fethi
Bougares, Rajen Chatterjee, Marta R. Costa-jussà, Christian Federmann,
Mark Fishel, Alexander Fraser, Markus Freitag, Yvette Graham, Roman
Grundkiewicz, Paco Guzman, Barry Haddow, Matthias Huck, Antonio Jimeno Yepes,
Tom Kocmi, André Martins, Makoto Morishita, Christof Monz, Masaaki
Nagata, Toshiaki Nakazawa, Matteo Negri, Aurélie Névéol, Mariana
Neves, Martin Popel, Marco Turchi, and Marcos Zampieri (eds.),
<em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>,
pp.  1–45, Abu Dhabi, United Arab Emirates (Hybrid), December 2022.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.wmt-1.1" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.wmt-1.1</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023a)</span>
<span class="ltx_bibblock">
Yihong Liu, Peiqin Lin, Mingyang Wang, and Hinrich Schütze.

</span>
<span class="ltx_bibblock">Ofa: A framework of initializing unseen subword embeddings for
efficient large-scale multilingual continued pretraining, 2023a.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2311.08849" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2311.08849</a>.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023b)</span>
<span class="ltx_bibblock">
Yihong Liu, Haotian Ye, Leonie Weissweiler, Renhao Pei, and Hinrich Schuetze.

</span>
<span class="ltx_bibblock">Crosslingual transfer learning for low-resource languages based on
multilingual colexification graphs.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">The 2023 Conference on Empirical Methods in Natural Language
Processing</em>, 2023b.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=Tn5hALAaA4" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=Tn5hALAaA4</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. (2023)</span>
<span class="ltx_bibblock">
Chunlan Ma, Ayyoob ImaniGooghari, Haotian Ye, Ehsaneddin Asgari, and Hinrich
Schütze.

</span>
<span class="ltx_bibblock">Taxi1500: A multilingual dataset for text classification in 1500
languages, 2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2305.08487" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2305.08487</a>.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minixhofer et al. (2022)</span>
<span class="ltx_bibblock">
Benjamin Minixhofer, Fabian Paischer, and Navid Rekabsaz.

</span>
<span class="ltx_bibblock">WECHSEL: Effective initialization of subword embeddings for
cross-lingual transfer of monolingual language models.

</span>
<span class="ltx_bibblock">In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir
Meza Ruiz (eds.), <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies</em>, pp.  3992–4006, Seattle, United States, July 2022.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.naacl-main.293</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.naacl-main.293" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.naacl-main.293</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muhammad et al. (2023)</span>
<span class="ltx_bibblock">
Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Seid Muhie Yimam, David Ifeoluwa
Adelani, Ibrahim Said Ahmad, Nedjma Ousidhoum, Abinew Ali Ayele, Saif
Mohammad, Meriem Beloucif, and Sebastian Ruder.

</span>
<span class="ltx_bibblock">SemEval-2023 task 12: Sentiment analysis for African languages
(AfriSenti-SemEval).

</span>
<span class="ltx_bibblock">In Atul Kr. Ojha, A. Seza Doğruöz, Giovanni Da San Martino,
Harish Tayyar Madabushi, Ritesh Kumar, and Elisa Sartori (eds.),
<em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 17th International Workshop on Semantic Evaluation
(SemEval-2023)</em>, pp.  2319–2337, Toronto, Canada, July 2023. Association
for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.semeval-1.315</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.semeval-1.315" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.semeval-1.315</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">NLLB-Team et al. (2022)</span>
<span class="ltx_bibblock">
NLLB-Team, Marta Ruiz Costa-jussà, James Cross, Onur cCelebi, Maha Elbayad,
Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Alison Youngblood,
Bapi Akula, Loïc Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,
John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon L.
Spruit, C. Tran, Pierre Yves Andrews, Necip Fazil Ayan, Shruti Bhosale,
Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzm’an,
Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem,
Holger Schwenk, and Jeff Wang.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">ArXiv</em>, abs/2207.04672, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://api.semanticscholar.org/CorpusID:250425961" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://api.semanticscholar.org/CorpusID:250425961</a>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ogueji et al. (2021)</span>
<span class="ltx_bibblock">
Kelechi Ogueji, Yuxin Zhu, and Jimmy Lin.

</span>
<span class="ltx_bibblock">Small data? no problem! exploring the viability of pretrained
multilingual language models for low-resourced languages.

</span>
<span class="ltx_bibblock">In Duygu Ataman, Alexandra Birch, Alexis Conneau, Orhan Firat,
Sebastian Ruder, and Gozde Gul Sahin (eds.), <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 1st
Workshop on Multilingual Representation Learning</em>, pp.  116–126, Punta
Cana, Dominican Republic, November 2021. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.mrl-1.11</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.mrl-1.11" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.mrl-1.11</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfeiffer et al. (2021)</span>
<span class="ltx_bibblock">
Jonas Pfeiffer, Ivan Vulić, Iryna Gurevych, and Sebastian Ruder.

</span>
<span class="ltx_bibblock">UNKs everywhere: Adapting multilingual language models to new
scripts.

</span>
<span class="ltx_bibblock">In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott
Wen-tau Yih (eds.), <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing</em>, pp.  10186–10203, Online and Punta
Cana, Dominican Republic, November 2021. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.800</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.emnlp-main.800" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.emnlp-main.800</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid et al. (2021)</span>
<span class="ltx_bibblock">
Machel Reid, Junjie Hu, Graham Neubig, and Yutaka Matsuo.

</span>
<span class="ltx_bibblock">AfroMT: Pretraining strategies and reproducible benchmarks for
translation of 8 African languages.

</span>
<span class="ltx_bibblock">In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott
Wen-tau Yih (eds.), <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical
Methods in Natural Language Processing</em>, pp.  1306–1320, Online and Punta
Cana, Dominican Republic, November 2021. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.99</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.emnlp-main.99" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.emnlp-main.99</a>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smith (1949)</span>
<span class="ltx_bibblock">
Edwin W. Smith.

</span>
<span class="ltx_bibblock">The classification of the bantu languages. by malcolm guthrie, ph.d.
published for the international african institute by the oxford university
press, 1948. pp. 91. map. 8s. 6d. net.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">Africa</em>, 19(1):73–74, 1949.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.2307/1156267</span>.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sugiyama &amp; Yoshinaga (2019)</span>
<span class="ltx_bibblock">
Amane Sugiyama and Naoki Yoshinaga.

</span>
<span class="ltx_bibblock">Data augmentation using back-translation for context-aware neural
machine translation.

</span>
<span class="ltx_bibblock">In Andrei Popescu-Belis, Sharid Loáiciga, Christian Hardmeier,
and Deyi Xiong (eds.), <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Proceedings of the Fourth Workshop on Discourse
in Machine Translation (DiscoMT 2019)</em>, pp.  35–44, Hong Kong, China,
November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D19-6504</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/D19-6504" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/D19-6504</a>.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al. (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won
Chung, Sharan Narang, Dani Yogatama, Ashish Vaswani, and Donald Metzler.

</span>
<span class="ltx_bibblock">Scale efficiently: Insights from pretraining and finetuning
transformers.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://openreview.net/forum?id=f2OYVDyfIB" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://openreview.net/forum?id=f2OYVDyfIB</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, and Dong Yu.

</span>
<span class="ltx_bibblock">Improving pre-trained multilingual model with vocabulary expansion.

</span>
<span class="ltx_bibblock">In Mohit Bansal and Aline Villavicencio (eds.), <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">Proceedings of
the 23rd Conference on Computational Natural Language Learning (CoNLL)</em>, pp. 316–327, Hong Kong, China, November 2019. Association for Computational
Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/K19-1030</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/K19-1030" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/K19-1030</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Xinyi Wang, Sebastian Ruder, and Graham Neubig.

</span>
<span class="ltx_bibblock">Expanding pretrained models to thousands more languages via
lexicon-based adaptation.

</span>
<span class="ltx_bibblock">In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.),
<em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers)</em>, pp.  863–877, Dublin,
Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.acl-long.61</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2022.acl-long.61" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2022.acl-long.61</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Workshop et al. (2023)</span>
<span class="ltx_bibblock">
BigScience Workshop, :, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie
Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha
Luccioni, François Yvon, Matthias Gallé, Jonathan Tow, Alexander M. Rush,
Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang,
Benoît Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji
Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu
Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo
Laurençon, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel,
Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna
Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue,
Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani,
Dragomir Radev, Eduardo González Ponferrada, Efrat Levkovizh, Ethan Kim,
Eyal Bar Natan, Francesco De Toni, Gérard Dupont, Germán Kruszewski, Giada
Pistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran, Ian Yu, Idris Abdulmumin,
Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse
Dodge, Jian Zhu, Jonathan Chang, Jörg Frohberg, Joseph Tobing, Joydeep
Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro Von Werra, Leon
Weber, Long Phan, Loubna Ben allal, Ludovic Tanguy, Manan Dey, Manuel Romero
Muñoz, Maraim Masoud, María Grandury, Mario Šaško, Max Huang, Maximin
Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad A.
Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis,
Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Henderson,
Pierre Colombo, Priscilla Amuok, Quentin Lhoest, Rheza Harliman, Rishi
Bommasani, Roberto Luis López, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,
Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma,
Shayne Longpre, Somaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Sydney
Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev,
Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid
Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si,
Davut Emre Taşar, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee,
Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti
Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik
Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful
Bari, Maged S. Al-shaibani, Matteo Manica, Nihal Nayak, Ryan Teehan, Samuel
Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali
Bers, Thibault Fevry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiangru
Tang, Zheng-Xin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri, Hadar Tojarieh,
Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong
Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max
Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette,
Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre
Cornette, Pierre François Lavallée, Rémi Lacroix, Samyam Rajbhandari,
Sanchit Gandhi, Shaden Smith, Stéphane Requena, Suraj Patil, Tim Dettmers,
Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun
Subramonian, Aurélie Névéol, Charles Lovering, Dan Garrette, Deepak
Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli
Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo,
Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken
Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton
Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen
Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina,
Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov,
Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger,
Zdeněk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy
Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo
Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin
Ajibade, Bharat Saxena, Carlos Muñoz Ferrandis, Daniel McDuff, Danish
Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward
Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib
Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko,
Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis Sanz, Livia Dutra,
Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha
Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis
Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An,
Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav
Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach
Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima
Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang,
Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clémentine Fourrier,
Daniel León Periñán, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio
Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns,
Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas
Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani,
Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc
Pàmies, Maria A Castillo, Marianna Nezhurina, Mario Sänger, Matthias
Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihaljcic,
Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg,
Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya
Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su,
Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu
Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan
Schweter, Sushil Bharati, Tanmay Laud, Théo Gigant, Tomoya Kainuma, Wojciech
Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin
Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and
Thomas Wolf.

</span>
<span class="ltx_bibblock">Bloom: A 176b-parameter open-access multilingual language model,
2023.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://arxiv.org/abs/2211.05100" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2211.05100</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al. (2019)</span>
<span class="ltx_bibblock">
Mengzhou Xia, Xiang Kong, Antonios Anastasopoulos, and Graham Neubig.

</span>
<span class="ltx_bibblock">Generalized data augmentation for low-resource translation.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez (eds.),
<em id="bib.bib28.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</em>, pp.  5786–5796, Florence, Italy, July 2019.
Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1579</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/P19-1579" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/P19-1579</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al. (2021)</span>
<span class="ltx_bibblock">
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, and Colin Raffel.

</span>
<span class="ltx_bibblock">mT5: A massively multilingual pre-trained text-to-text transformer.

</span>
<span class="ltx_bibblock">In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek
Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty,
and Yichao Zhou (eds.), <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computational Linguistics: Human
Language Technologies</em>, pp.  483–498, Online, June 2021. Association for
Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.naacl-main.41</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2021.naacl-main.41" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2021.naacl-main.41</a>.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.02533" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.02534" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.02534">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.02534" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.02535" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 18:16:23 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
