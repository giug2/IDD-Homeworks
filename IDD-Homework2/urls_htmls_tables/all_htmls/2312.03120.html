<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2312.03120] The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning</title><meta property="og:description" content="With the advance of the powerful heterogeneous, parallel and distributed
computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scien…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2312.03120">

<!--Generated on Tue Feb 27 15:06:40 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p">[1]<span id="p1.1.1" class="ltx_ERROR undefined">\fnm</span>Omer <span id="p1.1.2" class="ltx_ERROR undefined">\sur</span>Subasi



[1]<span id="p1.1.3" class="ltx_ERROR undefined">\orgdiv</span>High Performance Computing Group, <span id="p1.1.4" class="ltx_ERROR undefined">\orgname</span>Pacific Northwest National Laboratory, <span id="p1.1.5" class="ltx_ERROR undefined">\orgaddress</span><span id="p1.1.6" class="ltx_ERROR undefined">\street</span>902 Battelle Blvd, <span id="p1.1.7" class="ltx_ERROR undefined">\city</span>Richland, <span id="p1.1.8" class="ltx_ERROR undefined">\postcode</span>99354, <span id="p1.1.9" class="ltx_ERROR undefined">\state</span>WA, <span id="p1.1.10" class="ltx_ERROR undefined">\country</span>USA</p>
</div>
<h1 class="ltx_title ltx_title_document">The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:omer.subasi@pnnl.gov">omer.subasi@pnnl.gov</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id1.1.id1" class="ltx_ERROR undefined">\fnm</span>Oceane <span id="id2.2.id2" class="ltx_ERROR undefined">\sur</span>Bel
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:obel@pnnl.gov">obel@pnnl.gov</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id3.1.id1" class="ltx_ERROR undefined">\fnm</span>Joseph <span id="id4.2.id2" class="ltx_ERROR undefined">\sur</span>Manzano
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:joseph.manzano@pnnl.gov">joseph.manzano@pnnl.gov</a>
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span id="id5.1.id1" class="ltx_ERROR undefined">\fnm</span>Kevin <span id="id6.2.id2" class="ltx_ERROR undefined">\sur</span>Barker
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:kevin.barker@pnnl.gov">kevin.barker@pnnl.gov</a>
</span>
<span class="ltx_contact ltx_role_affiliation">*
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id7.id1" class="ltx_p">With the advance of the powerful heterogeneous, parallel and distributed
computing systems and ever increasing immense amount of data, machine learning has become an indispensable part of cutting-edge technology, scientific research and consumer products. In this study, we present a review of modern machine and deep learning. We provide a high-level overview for the latest advanced machine learning algorithms, applications, and frameworks. Our discussion encompasses parallel distributed learning, deep learning as well as federated learning. As a result, our work serves as an introductory text to the vast field of modern machine learning.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>Machine Learning, Distributed Machine Learning, Deep Learning, Federated Learning, Parallel and Distributed Computing.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Over the last decade,
Machine Learning (ML) has been applied to ever increasing immense amount of data that is becoming available as
more people become daily users of internet, mobile and wireless networks.
Coupled with the significant advances in deep learning (DL), ML has found more complex applications: from medical to machine translation and speech recognition, to intelligent object recognition, and to smart cities <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.
Modern parallel and heterogeneous computing systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> have enabled such applications by supporting highly parallel training.
These large-scale and distributed systems therefore have become the backbone of modern ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Federated Learning (FL), as a sub-field of DL, has emerged as a distributed learning solution to provide data privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>. Ever since its inception <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, FL has been studied extensively and adapted widely <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this study, we review the current landscape of modern ML systems and applications, and offer an overview as a self-contained text. While there are many surveys on large-scale <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, distributed ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, DL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, and FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>, we instead provide a high-level joint view of modern parallel and distributed ML and FL. In this way, our work differentiates itself from the existing literature. In brief, our study</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">presents the concepts and methods of ML and DL.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">discusses the parallelism and scaling approaches of large-scale distributed ML. Moreover, it explores the communication aspects, such as costs, topologies, and networking, of parallel and distributed training and inference.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">introduces FL, its applications and aggregation methods. It then elaborates on the security and privacy aspects as well as the existing platforms and datasets.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">summarizes open research questions in the modern landscape of parallel and distributed ML, DL and FL.</p>
</div>
</li>
</ul>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> outlines and summarizes our study.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2312.03120/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="152" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The outline of our review.</figcaption>
</figure>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our study is organized as follows:
Section <a href="#S2" title="2 Related Work ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> overviews the related work on large-scale and distributed ML.
Section <a href="#S3" title="3 Machine Learning (ML) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides the background on ML.
Section <a href="#S4" title="4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> discusses distributed ML.
Section <a href="#S5" title="5 Federated Learning (FL) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents FL.
Section <a href="#S6" title="6 Open Questions and Challenges ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> summarizes the existing open challenges.
Finally, Section <a href="#S7" title="7 Conclusions ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> concludes our review.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Surveys pertaining to parallel, distributed and large scale ML have been very numerous in the literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. Our work is different and unique because it provides an introductory review of the latest joint landscape of ML, DL and FL.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Different than the general surveys such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, some surveys offer in depth cost and comparisons of algorithms and methods both theoretically and empirically <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p">Many studies focus on distributed DL. Some of them are <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Moreover, there exists a significant number of surveys that focus on specific types of models such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> for graph neural networks (GNNs), <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> for Internet-of-Things (IoTs), <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> for wireless networks, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> for mobile and 5G networks or for specific target environments such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> for unmanned aerial vehicles (UAV).</p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">FL literature unsurprisingly offers many surveys.
Some of the latest surveys are <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Among studies having specific topics,
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> surveys privacy and security methods for FL, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> discusses block chain-based FL.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> presents differential privacy for FL.
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> offers a survey of FL for IoT.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Machine Learning (ML)</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we first overview ML in terms of concepts and goals.
Then we review various ML algorithms.
Finally, we discuss the existing modern ML frameworks.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Introduction to ML</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">ML is the process of learning from data to perform complex tasks
for which there is no known deterministic and algorithmic solution, or
building such a solution is not practical. For instance, developing a deterministic
algorithm based on rules to detect
spam emails is highly impractical. It is not possible to know
the exact list of the detection rules. In addition, these rules most often change over time.
Since the list of the rules may be ever-increasing and even contradictory,
the maintenance of such algorithms would require constant labor.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The ML process is mainly two-fold: Training and prediction (inference).
In the training phase, the parameters of a learning model are optimized based on data.
In the prediction phase, the trained model is deployed to perform predictions on new data.
While in most cases the training and prediction phases are mutually exclusive, in
incremental learning cases, they are coupled together.
The models in these cases are continuously trained and make predictions.
Figure <a href="#S3.F2" title="Figure 2 ‣ 3.1 Introduction to ML ‣ 3 Machine Learning (ML) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> visualizes the training and prediction phases.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The main goal of ML is to generalize such that it performs well with unseen data. However, this goal contradicts its optimization goal in which ML tries to minimize the training loss with the training data. As a result, the well-known bias-variance problem emerges. If an ML model over-fits the training data, that is, having high variance, it performs poorly with the unseen data.
On the other hand, if the model under-fits, that is, having high bias, it does not learn important patterns or regularities in the data. Over-fitting typically happens when a model is too complex for the underlying problem. In contrast, under-fitting happens when the model is too simple. Figure <a href="#S3.F3" title="Figure 3 ‣ 3.1 Introduction to ML ‣ 3 Machine Learning (ML) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> depicts the bias-variance trade-off.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2312.03120/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="244" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>ML phases: Training and prediction (inference).</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2312.03120/assets/x3.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="346" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Bias-variance trade-off. Model complexity with respect to bias and variance.</figcaption>
</figure>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">In the following, we present different types of ML tasks. After that, we look into different problems that ML can solve.
Then, we review widely used ML algorithms and methods.
Finally, we survey the existing ML platforms that are not supported with specialized hardware and not suited for DL or FL.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2312.03120/assets/x4.png" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="323" height="136" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>An artificial neural network example.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>ML Algorithms</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">ML algorithms can be categorized by the format and requirements of data (external feedback),
by the type of problems they are designed for (target problem), and by the techniques they use (algorithmic approaches).</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">It is worth noting that there is another way of categorizing ML: online and offline. In offline learning, the entire training data is available prior to training. This is the most common application of ML. In online learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>, either the entire data is not available beforehand or it is computationally infeasible to perform training over the entire data at once. An example of the former is sequential training such as time series analysis in financial markets. An example of the latter is learning with a very large dataset which does not fit into the memory and consequently, training becomes prohibitive.</p>
</div>
<section id="S3.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>External feedback</h4>

<div id="S3.SS2.SSS1.p1" class="ltx_para">
<p id="S3.SS2.SSS1.p1.1" class="ltx_p">ML algorithms can be classified based on the external feedback as follows:</p>
</div>
<div id="S3.SS2.SSS1.p2" class="ltx_para">
<p id="S3.SS2.SSS1.p2.1" class="ltx_p">Supervised Learning:
Learning is performed by feeding labeled input data
so that a model’s parameters are optimized.
Labeled data can be desired classes, categories, or numerical outputs corresponding to the training instances.
During training, the optimization is achieved by minimizing a predetermined cost function.
After training, the trained model is deployed to predict
the outputs of new instances.
An example supervised learning is to classify newly seen handwritten digits by training with the labeled digits.</p>
</div>
<div id="S3.SS2.SSS1.p3" class="ltx_para">
<p id="S3.SS2.SSS1.p3.1" class="ltx_p">Unsupervised Learning:
The goal of unsupervised learning is to find structures and patterns in unlabeled data. This means that in unsupervised learning, the data does not possess desired outputs.
As an example of unsupervised learning, clustering aims to find similar groups (clusters) in given data.
Dimensionality reduction is another example of unsupervised learning where the goal is to find a subset of key features that describes the data well.</p>
</div>
<div id="S3.SS2.SSS1.p4" class="ltx_para">
<p id="S3.SS2.SSS1.p4.1" class="ltx_p">Semi-supervised Learning:
In semi-supervised learning, the amount of labeled data is small while the amount of
unlabeled data is large. Clustering algorithms are typically used to propagate existing
labels to the unlabeled data. An assumption of semi-supervised learning is that similar data shares the same label.</p>
</div>
<div id="S3.SS2.SSS1.p5" class="ltx_para">
<p id="S3.SS2.SSS1.p5.1" class="ltx_p">Reinforcement Learning:
Reinforcement learning is applied when an agent interacts with an environment.
Based on the observations it makes, the agent takes actions. The actions are rewarded or
penalized according to a reward function. Applications of reinforcement learning lie in the fields such as game theory, robotics and industrial automation.</p>
</div>
</section>
<section id="S3.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Target Problem</h4>

<div id="S3.SS2.SSS2.p1" class="ltx_para">
<p id="S3.SS2.SSS2.p1.1" class="ltx_p">Under this categorization, ML algorithms are grouped according to the kind of problems they are designed to solve.</p>
</div>
<div id="S3.SS2.SSS2.p2" class="ltx_para">
<p id="S3.SS2.SSS2.p2.1" class="ltx_p">In classification problems, the aim is to correctly categorize data instances into the known classes.</p>
</div>
<div id="S3.SS2.SSS2.p3" class="ltx_para">
<p id="S3.SS2.SSS2.p3.1" class="ltx_p">In regression problems, the goal is to estimate the value of a variable based on other variables (features).</p>
</div>
<div id="S3.SS2.SSS2.p4" class="ltx_para">
<p id="S3.SS2.SSS2.p4.1" class="ltx_p">Clustering finds the distinct groups of similar data instances based on a selected similarity metric.</p>
</div>
<div id="S3.SS2.SSS2.p5" class="ltx_para">
<p id="S3.SS2.SSS2.p5.1" class="ltx_p">Anomaly and novelty detection is used to find
data instances that are significantly different than others. These instances are called outliers. In anomaly detection, training data consists of both outliers and regular (expected) data instances.
In novelty detection, on the other hand, the goal is used to detect unseen data where training data is free of outliers.</p>
</div>
<div id="S3.SS2.SSS2.p6" class="ltx_para">
<p id="S3.SS2.SSS2.p6.1" class="ltx_p">Dimensionality reduction is used to reduce the number features of the training data. In dimensionality reduction, if a subset of the original set of the features is selected, it is called feature selection. In contrast,
if features are combined into new ones, it is called feature extraction.
Dimensionality reduction can also be used to decrease computational costs of training. Furthermore, it can also be used to prevent over-fitting. The problem of over-fitting with high-dimensional data is famously known as the curse of dimensionality. The curse of dimensionality arises due to data sparsity in high dimensional spaces.</p>
</div>
</section>
<section id="S3.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Algorithmic Approaches</h4>

<div id="S3.SS2.SSS3.p1" class="ltx_para">
<p id="S3.SS2.SSS3.p1.1" class="ltx_p">ML algorithms can be categorized based on algorithmic approaches that they employ.</p>
</div>
<div id="S3.SS2.SSS3.p2" class="ltx_para">
<p id="S3.SS2.SSS3.p2.1" class="ltx_p">Stochastic Gradient Decent (SGD) based algorithms are optimized based on a loss function of the outputs of
the model parameters in the opposite direction of the gradient.
Because at each training step a random subset of data is used, this optimization method is
called stochastic.
Many common ML algorithms are optimized with SGD such as artificial neural networks.</p>
</div>
<div id="S3.SS2.SSS3.p3" class="ltx_para">
<p id="S3.SS2.SSS3.p3.1" class="ltx_p">Support Vector Machines (SVMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> are typically used when the input data is not linearly separable in its original space. They
map the input data to high dimensional spaces where it becomes linearly separable.
SVMs can be used for classification, regression, and novelty detection.</p>
</div>
<div id="S3.SS2.SSS3.p4" class="ltx_para">
<p id="S3.SS2.SSS3.p4.1" class="ltx_p">Artificial Neural Networks (ANNs) are constructed by multiple layers of nodes (neurons) that have inputs, outputs, corresponding feature weights, and an activation function. Layers can be input, hidden, and output layers. ANNs have recently been very successful in tasks such as image classification, object detection, and natural language processing. Figure <a href="#S3.F4" title="Figure 4 ‣ 3.1 Introduction to ML ‣ 3 Machine Learning (ML) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> depicts an example of an ANN.
Some well-known types of ANNs include:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> are deep neural networks that incorporate convolutions and pooling. While convolutions help with learning local data, pooling help with learning abstract features. CNNs have been extremely successful in tasks such as image classification, object detection, and image segmentation.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Recurrent Neural Networks (RNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> maintains a temporal state of sequence data.
The temporal state may hold short-term or long-term memory. RNNs are used in tasks such as time series forecasting, natural language processing, and anomaly detection.</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Autoencoders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite> are ANNs that learn latent representations of input data with no supervision. They are used for dimensionality reduction and visualization of high dimensional data.</p>
</div>
</li>
<li id="S3.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i4.p1" class="ltx_para">
<p id="S3.I1.i4.p1.1" class="ltx_p">Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> are (originally unsupervised) neural networks used to generate data based on a game between a generator and discriminator network. They have been successfully applied in supervised and semi-supervised learning.</p>
</div>
</li>
<li id="S3.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i5.p1" class="ltx_para">
<p id="S3.I1.i5.p1.1" class="ltx_p">Graph Neural Networks (GNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite> are a type of ANNs designed to perform learning and prediction on data described by graphs. GNNs provide an easy way to do node, edge, and graph level ML tasks.</p>
</div>
</li>
<li id="S3.I1.i6" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i6.p1" class="ltx_para">
<p id="S3.I1.i6.p1.1" class="ltx_p">Self-Organizing Maps (SOMs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> are neural networks which produce a low dimensional
representation of high dimensional data. SOMs are used for visualization, clustering, and classification. The training is unsupervised where after random initialization,
neurons compete against each other.</p>
</div>
</li>
<li id="S3.I1.i7" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i7.p1" class="ltx_para">
<p id="S3.I1.i7.p1.1" class="ltx_p">Boltzmann Machines <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>]</cite> are fully connected ANNs which, unlike other ANNs, have probabilistic activation functions. Neurons output 1 or 0 based on
Boltzmann distribution. Boltzmann Machines can be used for classifying, denoising, or completing images.</p>
</div>
</li>
<li id="S3.I1.i8" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i8.p1" class="ltx_para">
<p id="S3.I1.i8.p1.1" class="ltx_p">Deep Belief Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>]</cite> are stacked Boltzmann Machines designed to tackle larger and more complex learning challenges. They are used for semi-supervised learning.</p>
</div>
</li>
<li id="S3.I1.i9" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I1.i9.p1" class="ltx_para">
<p id="S3.I1.i9.p1.1" class="ltx_p">Hopfield Networks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>]</cite> are fully connected networks that are used for tasks such as character recognition.</p>
</div>
</li>
</ul>
</div>
<div id="S3.SS2.SSS3.p5" class="ltx_para">
<p id="S3.SS2.SSS3.p5.1" class="ltx_p">Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref">42</a>]</cite> are a class of DL models that has shown extraordinary success in many ML fields including natural language processing and computer vision. Transformers were first introduced by a landmark paper from Google <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> which were based on a novel mechanism called <span id="S3.SS2.SSS3.p5.1.1" class="ltx_text ltx_font_italic">Attention</span>. At its core, a transformer is an encoder-decoder model. The success of Transformers has become a regular news-headliner such as the release of GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite> and ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite>.</p>
</div>
<div id="S3.SS2.SSS3.p6" class="ltx_para">
<p id="S3.SS2.SSS3.p6.1" class="ltx_p">Rule-based algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>]</cite> use a set of rules to learn patterns from the input data. They are typically easier to interpret than other ML algorithms.
Decision trees are the most well-known rule-based algorithms.</p>
</div>
<div id="S3.SS2.SSS3.p7" class="ltx_para">
<p id="S3.SS2.SSS3.p7.1" class="ltx_p">Evolutionary algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref">47</a>]</cite>
use ideas from biological evolution.
In evolutionary algorithms, the target problem is represented by a set of properties.
The performance metric is called fitness function.
Based on fitness scores, the set of properties is mutated and crossed over.
These algorithms iterate until accurate estimates are obtained.
Evolutionary algorithms can also be used to create other algorithms such as neural networks.</p>
</div>
<div id="S3.SS2.SSS3.p8" class="ltx_para">
<p id="S3.SS2.SSS3.p8.1" class="ltx_p">Semantic and Topic algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>]</cite> are used to learn specific semantic patterns and distinct relationships in the input data.
An example application of these algorithms is to find the topics and relate them to each other in a given set of documents.</p>
</div>
<div id="S3.SS2.SSS3.p9" class="ltx_para">
<p id="S3.SS2.SSS3.p9.1" class="ltx_p">Ensemble algorithms combine other algorithms to obtain a solution that performs better than the individual algorithms. Different ways to build ensembles are:</p>
<ul id="S3.I2" class="ltx_itemize">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Bagging combines multiple classifiers and uses voting to determine the final output.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Boosting is a technique that trains the subsequent models with the data instances misclassified by the preceding models in the chain.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Stacking is the process where a model trains with the outputs of the preceding models in a chain of several models. Stacking typically reduces the classification variance.</p>
</div>
</li>
<li id="S3.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S3.I2.i4.p1" class="ltx_para">
<p id="S3.I2.i4.p1.1" class="ltx_p">Random Forests combine multiple decision trees and output an (weighted) average of the outputs of the individual trees.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Existing ML Frameworks</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">In this section, we present the existing ML platforms that are not supported with specialized hardware and typically not suited for DL or FL. We then briefly mention the popular ML services in the cloud.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">Scikit-Learn <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref">50</a>]</cite> is the most popular open-source Python library that offers an extensive suite of ML algorithms. The library is very well maintained and provides a comprehensive set of algorithms, methods, pre-processing, pipelining, model selection and hyper-parameter search capabilities.
It provides interfaces to work with NumPy and SciPy packages.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">Weka <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> is a general-purpose and popular Java ML library. It provides a large collection of algorithms and visualization tools. Weka supports numerous tasks such as pre-processing, classification, regression, clustering and visualization.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p">XGBoost <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> is a scalable and distributed gradient boosting library based on decision trees. It implements parallel ML algorithms for classification, regression and ranking tasks.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.1" class="ltx_p">Shogun <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref">53</a>]</cite> is a research-oriented open-source ML library. It offers a large number of ML algorithms and cross-platform support by providing bindings with other languages and environments such as Python, Octave, R, Java. Shogun’s core library is implemented in C++.</p>
</div>
<div id="S3.SS3.p6" class="ltx_para">
<p id="S3.SS3.p6.1" class="ltx_p">LibSVM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib54" title="" class="ltx_ref">54</a>]</cite> is a specialized C/C++ library for SVMs. It provides interfaces for Python, R, MATLAB and many others.</p>
</div>
<div id="S3.SS3.p7" class="ltx_para">
<p id="S3.SS3.p7.1" class="ltx_p">Many companies offer standard ML and distributed ML services. Moreover, these services often include the support for GPUs and other ML specific hardware. Popular cloud ML services are
Google’s Cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>]</cite>, Microsoft Azure <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>, Amazon’s SageMaker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib57" title="" class="ltx_ref">57</a>]</cite> and the IBM Watson Cloud <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Distributed Machine Learning</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this section, we introduce large-scale distributed ML.
We then explore different types of parallelisms used in distributed training.
Next, we dive into vertical scaling techniques. After that, we present the optimizations for communications in distributed ML.
Then we continue with the communication topologies and synchronization models. Finally, we conclude this section by the discussion of the existing distributed ML frameworks.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">As a side note, we use <span id="S4.p2.1.1" class="ltx_text ltx_font_italic">client</span> and <span id="S4.p2.1.2" class="ltx_text ltx_font_italic">participant</span> interchangeably in the rest of the paper.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Introduction to Distributed ML</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">Distributed ML is proposed to utilize distributed and heterogeneous computing systems to
solve large and complex problems where a solution cannot be obtained by a single standalone homogeneous computing device.
Distributed ML
offers two different approaches. The first is to use heterogeneous resources available
in a single computing system such as Graphical Processing Units (GPUs). This is called vertical scaling.
The second is to use multiple machines to solve larger problems and to support fault-tolerance. This is called horizontal scaling.</p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p">GPUs have been the most common mean of vertical scaling. Given sufficient parallelism, it has been shown that GPUs significantly accelerate training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.
For instance, NVIDIA GPUs have been popular in accelerating ML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite>.
Vendors such as Google have implemented their own specific hardware accelerators. Tensor Processing Units (TPUs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>
are designed specifically for this purpose. Others such as Graphcore <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib62" title="" class="ltx_ref">62</a>]</cite> and SambaNova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite> have followed this trend with sophisticated dataflow-based hardware designs and powerful system software tool-chains.</p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p">In contrast to vertical scaling, horizontal scaling corresponds to distributed training and inference across multiple machines. Horizontal scaling enables ML solutions to handle applications and data that do not fit in the resources of a single machine.
Additionally, the usage of multiple machines typically accelerate training and inference.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Parallelisms in Distributed Training and Inference</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">There are three types of parallelisms used in distributed training. These are data, model and
pipeline parallelism.</p>
</div>
<figure id="S4.F5" class="ltx_figure"><img src="/html/2312.03120/assets/x5.png" id="S4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="225" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Data parallelism for a deep neural network.</figcaption>
</figure>
<section id="S4.SS2.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Data Parallelism</h4>

<div id="S4.SS2.SSS1.p1" class="ltx_para">
<p id="S4.SS2.SSS1.p1.1" class="ltx_p">In data parallelism,
the same ML model is trained with different subsets of the data in parallel at different computing resources. Once all computing resources finish the assigned training, the models are accumulated and an average model is obtained. Then, this average model is distributed back to each computing resource for the subsequent rounds of training.
Figure <a href="#S4.F5" title="Figure 5 ‣ 4.2 Parallelisms in Distributed Training and Inference ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> depicts data parallelism with two parallel resources.</p>
</div>
<div id="S4.SS2.SSS1.p2" class="ltx_para">
<p id="S4.SS2.SSS1.p2.1" class="ltx_p">The main advantage of data parallelism is that it is applicable to any distributed ML model without requiring expert/domain knowledge. It is also very scalable for
compute-intensive models, such as CNNs.
One disadvantage of data parallelism is that model synchronization may become a bottleneck.
Another disadvantage occurs when the model does not fit in the memory of a single device.</p>
</div>
</section>
<section id="S4.SS2.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Model Parallelism</h4>

<div id="S4.SS2.SSS2.p1" class="ltx_para">
<p id="S4.SS2.SSS2.p1.1" class="ltx_p">In model parallelism, the model is partitioned and distributed to different computing resources. The data is distributed as well according to the model distribution. When there is a dependency among the computing resources, synchronization is needed for the parameters (weights) to be shared consistently.
Figure <a href="#S4.F6" title="Figure 6 ‣ 4.2.2 Model Parallelism ‣ 4.2 Parallelisms in Distributed Training and Inference ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows model parallelism where two resources are used. An important note is that in the figure, every time that a dashed line crosses a resource boundary, at least one synchronization event must take place to ensure data consistency.</p>
</div>
<div id="S4.SS2.SSS2.p2" class="ltx_para">
<p id="S4.SS2.SSS2.p2.1" class="ltx_p">The main advantage of model parallelism is that models take less memory in each single resource (device).
Its main disadvantage is that the model partitioning is often nontrivial. Another disadvantage is the potential intensive communications among the resources.</p>
</div>
<figure id="S4.F6" class="ltx_figure"><img src="/html/2312.03120/assets/x6.png" id="S4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Model parallelism for a deep neural network.</figcaption>
</figure>
</section>
<section id="S4.SS2.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.3 </span>Pipeline Parallelism</h4>

<div id="S4.SS2.SSS3.p1" class="ltx_para">
<p id="S4.SS2.SSS3.p1.1" class="ltx_p">Pipeline parallelism combines model and data parallelisms.
It distributes the model and data in a such a way that there is a pipeline among the computing resources in which each resource has a different part of the model.
Pipeline parallelism maintains the advantages of model parallelism while increasing the resource utilization.
Figure <a href="#S4.F7" title="Figure 7 ‣ 4.2.3 Pipeline Parallelism ‣ 4.2 Parallelisms in Distributed Training and Inference ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> illustrates pipeline parallelism.</p>
</div>
<figure id="S4.F7" class="ltx_figure"><img src="/html/2312.03120/assets/x7.png" id="S4.F7.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="322" height="174" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Pipeline parallelism for a deep neural network.</figcaption>
</figure>
<figure id="S4.F8" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.03120/assets/x8.png" id="S4.F8.sf1.g1" class="ltx_graphics ltx_img_square" width="138" height="125" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Centralized</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.03120/assets/x9.png" id="S4.F8.sf2.g1" class="ltx_graphics ltx_img_landscape" width="138" height="103" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Hierarchical</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S4.F8.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2312.03120/assets/x10.png" id="S4.F8.sf3.g1" class="ltx_graphics ltx_img_landscape" width="138" height="78" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(c) </span>Distributed</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Different topologies of distributed ML.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Vertical Optimization Approaches</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.1" class="ltx_p">We have discussed the types of parallelisms used in distributed ML above. Now, we explore three vertical optimization approaches. They are model simplification, optimization approximation, and communication optimization approaches.</p>
</div>
<section id="S4.SS3.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Model Simplification</h4>

<div id="S4.SS3.SSS1.p1" class="ltx_para">
<p id="S4.SS3.SSS1.p1.1" class="ltx_p">Model simplification refers to the reformulation of a target model to decrease its computational complexity as a way of achieving efficiency. Model simplification can be further divided into categories based on the type of the ML models.
These models can be based on kernels, trees, graphs and deep neural networks. Table <a href="#S4.T1" title="Table 1 ‣ 4.3.1 Model Simplification ‣ 4.3 Vertical Optimization Approaches ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>
summarizes the model simplification techniques.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Model Type</th>
<th id="S4.T1.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Techniques</th>
<th id="S4.T1.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Existing Work</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Kernel-based Models</th>
<td id="S4.T1.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T1.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.2.1.2.1.1" class="ltx_tr">
<td id="S4.T1.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Sampling-based</td>
</tr>
<tr id="S4.T1.1.2.1.2.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Projection-based</td>
</tr>
</table>
</td>
<td id="S4.T1.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>, <a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite></td>
</tr>
<tr id="S4.T1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Tree-based Models</th>
<td id="S4.T1.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T1.1.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.3.2.2.1.1" class="ltx_tr">
<td id="S4.T1.1.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Rule sampling</td>
</tr>
<tr id="S4.T1.1.3.2.2.1.2" class="ltx_tr">
<td id="S4.T1.1.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Feature sampling</td>
</tr>
</table>
</td>
<td id="S4.T1.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>, <a href="#bib.bib69" title="" class="ltx_ref">69</a>, <a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite></td>
</tr>
<tr id="S4.T1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Graph-based Models</th>
<td id="S4.T1.1.4.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T1.1.4.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.4.3.2.1.1" class="ltx_tr">
<td id="S4.T1.1.4.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Sparse graph construction</td>
</tr>
<tr id="S4.T1.1.4.3.2.1.2" class="ltx_tr">
<td id="S4.T1.1.4.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Anchor graph based optimization</td>
</tr>
</table>
</td>
<td id="S4.T1.1.4.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>, <a href="#bib.bib71" title="" class="ltx_ref">71</a>, <a href="#bib.bib72" title="" class="ltx_ref">72</a>, <a href="#bib.bib73" title="" class="ltx_ref">73</a>, <a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite></td>
</tr>
<tr id="S4.T1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Deep Neural Network Models</th>
<td id="S4.T1.1.5.4.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S4.T1.1.5.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T1.1.5.4.2.1.1" class="ltx_tr">
<td id="S4.T1.1.5.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Efficient activation functions</td>
</tr>
<tr id="S4.T1.1.5.4.2.1.2" class="ltx_tr">
<td id="S4.T1.1.5.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Filter factorization and grouping</td>
</tr>
</table>
</td>
<td id="S4.T1.1.5.4.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>, <a href="#bib.bib76" title="" class="ltx_ref">76</a>, <a href="#bib.bib77" title="" class="ltx_ref">77</a>, <a href="#bib.bib78" title="" class="ltx_ref">78</a>, <a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Model simplifications for different ML models.</figcaption>
</figure>
<div id="S4.SS3.SSS1.p2" class="ltx_para">
<p id="S4.SS3.SSS1.p2.1" class="ltx_p">Simplifications for kernel-based models are made by sampling-based or projection-based approximations. While sampling-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>, <a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> approximate kernel matrices by random samples, projection-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib66" title="" class="ltx_ref">66</a>, <a href="#bib.bib67" title="" class="ltx_ref">67</a>]</cite> use Gaussian or sparse random projections to map the data features to low dimensional sub-spaces.</p>
</div>
<div id="S4.SS3.SSS1.p3" class="ltx_para">
<p id="S4.SS3.SSS1.p3.1" class="ltx_p">Performance and scalability improvements for tree-based models, such as decision trees and random forests, are commonly based on rule <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib68" title="" class="ltx_ref">68</a>]</cite> or feature sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib69" title="" class="ltx_ref">69</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS1.p4" class="ltx_para">
<p id="S4.SS3.SSS1.p4.1" class="ltx_p">Graph-based simplifications are developed for graph-based models where nodes represent the data instances and edges represent the similarity between the instances. In these models, the cost of training comes from two main sources: graph construction and the label matrix inversion.
For sparse graphs, graph construction constitutes the main cost of training. This is
because when label propagation is used, it lowers the cost of the inversion of the label matrix and it becomes less costly than graph construction. As a result, graph construction dominates the main computational cost.
To construct sparse graphs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib70" title="" class="ltx_ref">70</a>]</cite>, hashing methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib71" title="" class="ltx_ref">71</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib72" title="" class="ltx_ref">72</a>]</cite> are often used.
Different than sparse graph models, there are also graph models that are built by anchor graphs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib73" title="" class="ltx_ref">73</a>]</cite>. An anchor graph is a hierarchical representation of a target graph. It is built with a small subset of the instances. This small subset is used to retain the similarities between all instances. In such a representation, the label matrix inversion is the main cost of training.
To reduce the cost of the matrix inversion, the pruning of anchors’ adjacency <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib74" title="" class="ltx_ref">74</a>]</cite> is a common technique.</p>
</div>
<div id="S4.SS3.SSS1.p5" class="ltx_para">
<p id="S4.SS3.SSS1.p5.1" class="ltx_p">Performance improvements for deep neural networks can be achieved in two different ways. First, activation functions, such as Rectified Linear Unit (ReLU) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib75" title="" class="ltx_ref">75</a>]</cite> and its variants <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib76" title="" class="ltx_ref">76</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib77" title="" class="ltx_ref">77</a>]</cite>, can be employed instead of the expensive functions, such as sigmoid and tanh, which use the exponential function. Other techniques, specifically for CNNs, involve depth-wise filter factorization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib78" title="" class="ltx_ref">78</a>]</cite> and group-wise convolutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib79" title="" class="ltx_ref">79</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Optimization Approximation</h4>

<div id="S4.SS3.SSS2.p1" class="ltx_para">
<p id="S4.SS3.SSS2.p1.1" class="ltx_p">Optimization approximation is a family of techniques that are
used to reduce the cost of the optimization related
computations, i.e., gradient computations, for training. It is generally realized by computing the gradients with a small number of instances or parameters instead of all instances or parameters.
Care has to be taken since such approximations can lead to longer convergence times, local extrema, or even non-convergence. Optimization approximation can be categorized based on the specific optimization algorithm that is being used: Mini-batch gradient descent, coordinate descent, and numerical integration based on Markov chain Monte Carlo. Table <a href="#S4.T2" title="Table 2 ‣ 4.3.2 Optimization Approximation ‣ 4.3 Vertical Optimization Approaches ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the existing techniques.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Categories</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Techniques</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Existing Work</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<th id="S4.T2.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Mini-batch gradient descent</th>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T2.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.2.1.2.1.1" class="ltx_tr">
<td id="S4.T2.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Adaptive sampling</td>
</tr>
<tr id="S4.T2.1.2.1.2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Adaptive learning rates</td>
</tr>
<tr id="S4.T2.1.2.1.2.1.3" class="ltx_tr">
<td id="S4.T2.1.2.1.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Gradient corrections</td>
</tr>
</table>
</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T2.1.2.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.2.1.3.1.1" class="ltx_tr">
<td id="S4.T2.1.2.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>
</td>
</tr>
<tr id="S4.T2.1.2.1.3.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>
</td>
</tr>
<tr id="S4.T2.1.2.1.3.1.3" class="ltx_tr">
<td id="S4.T2.1.2.1.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>
</td>
</tr>
<tr id="S4.T2.1.2.1.3.1.4" class="ltx_tr">
<td id="S4.T2.1.2.1.3.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>
</td>
</tr>
<tr id="S4.T2.1.2.1.3.1.5" class="ltx_tr">
<td id="S4.T2.1.2.1.3.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<th id="S4.T2.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Coordinate gradient descent</th>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T2.1.3.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.3.2.2.1.1" class="ltx_tr">
<td id="S4.T2.1.3.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Rule sampling</td>
</tr>
<tr id="S4.T2.1.3.2.2.1.2" class="ltx_tr">
<td id="S4.T2.1.3.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Feature sampling</td>
</tr>
</table>
</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T2.1.3.2.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.3.2.3.1.1" class="ltx_tr">
<td id="S4.T2.1.3.2.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>
</td>
</tr>
<tr id="S4.T2.1.3.2.3.1.2" class="ltx_tr">
<td id="S4.T2.1.3.2.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite>
</td>
</tr>
<tr id="S4.T2.1.3.2.3.1.3" class="ltx_tr">
<td id="S4.T2.1.3.2.3.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<th id="S4.T2.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Bayesian optimization</th>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S4.T2.1.4.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T2.1.4.3.2.1.1" class="ltx_tr">
<td id="S4.T2.1.4.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Sparse graph construction</td>
</tr>
<tr id="S4.T2.1.4.3.2.1.2" class="ltx_tr">
<td id="S4.T2.1.4.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Anchor graph based optimization</td>
</tr>
</table>
</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Optimization approximation based techniques.</figcaption>
</figure>
<div id="S4.SS3.SSS2.p2" class="ltx_para">
<p id="S4.SS3.SSS2.p2.1" class="ltx_p">Techniques that are used for mini-batch gradient descent approximations are adaptive sampling of mini-batches, adaptive learning rates, and the improvements in gradient approximations.
Adaptive sampling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib80" title="" class="ltx_ref">80</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib81" title="" class="ltx_ref">81</a>]</cite> for mini-batches takes the data distribution and gradient contributions into account rather than just using random batches of samples or making a gradual increase in the batch size <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib82" title="" class="ltx_ref">82</a>]</cite>.
Learning rates are also crucial in terms of achieving fast convergence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib83" title="" class="ltx_ref">83</a>]</cite>.
Adaptive learning rates can boost the speed and quality of convergence <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib84" title="" class="ltx_ref">84</a>]</cite>. Further adaptive adjustments are shown to be effective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib85" title="" class="ltx_ref">85</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib86" title="" class="ltx_ref">86</a>]</cite>.
Complementary to adaptive sampling or adaptive learning rates, reducing the variance of gradients and computing more accurate gradients are shown to be effective and efficient in achieving fast convergence. Such methods use average gradients or look-ahead corrections of gradients <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib87" title="" class="ltx_ref">87</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib88" title="" class="ltx_ref">88</a>]</cite>.
In addition to the accurate first-order gradients, higher-order gradients may be needed due to ill-conditioning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib90" title="" class="ltx_ref">90</a>]</cite>. Hessian matrices are estimated by the high-order gradients to make convergence possible <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib89" title="" class="ltx_ref">89</a>]</cite>.</p>
</div>
<div id="S4.SS3.SSS2.p3" class="ltx_para">
<p id="S4.SS3.SSS2.p3.1" class="ltx_p">Coordinate gradient descent are targeted at the problems where the instances are high dimensional, such as recommender systems <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib91" title="" class="ltx_ref">91</a>]</cite> and natural language processing <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib92" title="" class="ltx_ref">92</a>]</cite>. To speed up the optimizations performed by coordinate gradient descent, a small number of parameters can be selected at each iteration. Random selection of parameters has shown to be effective <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib93" title="" class="ltx_ref">93</a>]</cite>
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. Parameter selection can also be based on the first and/or second-order gradients information <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib95" title="" class="ltx_ref">95</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib96" title="" class="ltx_ref">96</a>]</cite>. Another approach for speedup is to use extrapolation steps during the optimization phase <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib94" title="" class="ltx_ref">94</a>]</cite>. If the optimization problem is non-convex, then studies such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib98" title="" class="ltx_ref">98</a>]</cite> present specific solutions. For instance, Li and Lin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib97" title="" class="ltx_ref">97</a>]</cite> propose
an extended variant of accelerated proximal gradient method.</p>
</div>
<div id="S4.SS3.SSS2.p4" class="ltx_para">
<p id="S4.SS3.SSS2.p4.1" class="ltx_p">Finally, Bayesian optimization methods are commonly based on Markov chain Monte Carlo <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib99" title="" class="ltx_ref">99</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib100" title="" class="ltx_ref">100</a>]</cite>.
Such methods employ stochastic mini-batches due to the high cost of the acceptance tests <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib101" title="" class="ltx_ref">101</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS3.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Communication Optimization Approaches</h4>

<div id="S4.SS3.SSS3.p1" class="ltx_para">
<p id="S4.SS3.SSS3.p1.1" class="ltx_p">Optimizations to reduce communication costs constitute another option to those for computation.
In these optimizations, compression of gradients is one of the two main ideas.
Some studies compress each gradient component to just 1 bit <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>. Others map
gradients to a discrete set of values <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite> or sketch gradients into buckets and then encode them <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite>.
Some proposals only communicate gradients that are bigger than a certain threshold <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>.
A combination of gradient compression and low-precision learning has been shown to further reduce the communication costs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>.
The other main idea for the optimization of communication is gradient delaying <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>.
Ho et. al. explore the usage of gradient delays for stale synchronous parallel communications.
Zheng et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite> on the other hand compute approximate second-order gradients and overlap these computations with the delays to enhance the communication efficiency. Zhang, Choromanska, and LeCun <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite> define an elastic relationship between the local and global model to avoid local minima as gradient transfers are delayed. Different than these studies, McMahan and Streeter <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite> introduce communication optimizations for online learning.</p>
</div>
<div id="S4.SS3.SSS3.p2" class="ltx_para">
<p id="S4.SS3.SSS3.p2.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ 4.3.3 Communication Optimization Approaches ‣ 4.3 Vertical Optimization Approaches ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> summarizes these techniques.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Categories</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Techniques</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Existing Work</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<th id="S4.T3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Communications</th>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S4.T3.1.2.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.2.1.2.1.1" class="ltx_tr">
<td id="S4.T3.1.2.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Gradient compression</td>
</tr>
<tr id="S4.T3.1.2.1.2.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Gradient delay</td>
</tr>
</table>
</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S4.T3.1.2.1.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T3.1.2.1.3.1.1" class="ltx_tr">
<td id="S4.T3.1.2.1.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib102" title="" class="ltx_ref">102</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib103" title="" class="ltx_ref">103</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib104" title="" class="ltx_ref">104</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib105" title="" class="ltx_ref">105</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib106" title="" class="ltx_ref">106</a>]</cite>
</td>
</tr>
<tr id="S4.T3.1.2.1.3.1.2" class="ltx_tr">
<td id="S4.T3.1.2.1.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib108" title="" class="ltx_ref">108</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib109" title="" class="ltx_ref">109</a>]</cite>, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib110" title="" class="ltx_ref">110</a>]</cite>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Communication optimization approaches.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Communication Topology</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">In a distributed ML system, the computing resources (clusters) can be structured in
different ways. The types of topologies that the resources use can be categorized into three: centralized, hierarchical, and fully distributed (decentralized). Figure <a href="#S4.F8" title="Figure 8 ‣ 4.2.3 Pipeline Parallelism ‣ 4.2 Parallelisms in Distributed Training and Inference ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> depicts these topologies. Table <a href="#S4.T4" title="Table 4 ‣ 4.4.3 Fully Distributed Topology ‣ 4.4 Communication Topology ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes our discussion.</p>
</div>
<section id="S4.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Centralized Topology</h4>

<div id="S4.SS4.SSS1.p1" class="ltx_para">
<p id="S4.SS4.SSS1.p1.1" class="ltx_p">In this topology, the computation of the global model parameters, gradient averaging and communications with the distributed nodes/clients are performed at a central server. Every distributed client directly communicates with the central server and works with its local data only. A major disadvantage of a centralized topology is that the central server constitutes a single point of failure and a computational bottleneck. Advantages of a centralized topology are the ease of its implementation and inspection.
Figure <a href="#S4.F8" title="Figure 8 ‣ 4.2.3 Pipeline Parallelism ‣ 4.2 Parallelisms in Distributed Training and Inference ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (a) presents an example of this topology.</p>
</div>
</section>
<section id="S4.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Hierarchical Topology</h4>

<div id="S4.SS4.SSS2.p1" class="ltx_para">
<p id="S4.SS4.SSS2.p1.1" class="ltx_p">The computations and aggregation of the global model parameters are performed in a stage-wise and hierarchical way.
Each child node only communicates with its parent.
These topologies offer higher scalability than the centralized counterparts and easier manageability than the distributed counterparts.
Figure <a href="#S4.F8" title="Figure 8 ‣ 4.2.3 Pipeline Parallelism ‣ 4.2 Parallelisms in Distributed Training and Inference ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (b) depicts a hierarchical topology.</p>
</div>
</section>
<section id="S4.SS4.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Fully Distributed Topology</h4>

<div id="S4.SS4.SSS3.p1" class="ltx_para">
<p id="S4.SS4.SSS3.p1.1" class="ltx_p">Every participant maintains a local copy of the global model in a fully distributed topology. Participants directly communicate with each other. Compared to the centralized and hierarchical topologies, scalability is much higher and the single points of failure are eliminated. However, the implementation of these topologies is relatively more complex.
Figure <a href="#S4.F8" title="Figure 8 ‣ 4.2.3 Pipeline Parallelism ‣ 4.2 Parallelisms in Distributed Training and Inference ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> (c) shows this topology.</p>
</div>
<figure id="S4.T4" class="ltx_table">
<table id="S4.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T4.1.1.1" class="ltx_tr">
<th id="S4.T4.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Topology</th>
<td id="S4.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Complexity</td>
<td id="S4.T4.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Scalability</td>
<td id="S4.T4.1.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Manageability</td>
<td id="S4.T4.1.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Single Point Failures</td>
<td id="S4.T4.1.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Latency</td>
</tr>
<tr id="S4.T4.1.2.2" class="ltx_tr">
<th id="S4.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Centralized</th>
<td id="S4.T4.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Low</td>
<td id="S4.T4.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Low</td>
<td id="S4.T4.1.2.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">High</td>
<td id="S4.T4.1.2.2.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes</td>
<td id="S4.T4.1.2.2.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Low</td>
</tr>
<tr id="S4.T4.1.3.3" class="ltx_tr">
<th id="S4.T4.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">Hierarchical</th>
<td id="S4.T4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Medium</td>
<td id="S4.T4.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Medium</td>
<td id="S4.T4.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Medium</td>
<td id="S4.T4.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Yes</td>
<td id="S4.T4.1.3.3.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Medium</td>
</tr>
<tr id="S4.T4.1.4.4" class="ltx_tr">
<th id="S4.T4.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Fully Distributed</th>
<td id="S4.T4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">High</td>
<td id="S4.T4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">High</td>
<td id="S4.T4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">Low</td>
<td id="S4.T4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">No</td>
<td id="S4.T4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">High</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of different communication topologies.</figcaption>
</figure>
</section>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Synchronization Models</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">Synchronization models are techniques to guide and perform synchronization between parallel computations and communications.
These models seek to establish the best trade-off between fast updates and accurate models. To do fast updates, lower levels of synchronization are required.
In comparison, to obtain accurate models, higher levels of synchronization are needed.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">As far as ML is concerned, stochastic gradient descent is one of the most popular algorithms for the optimization during the training phase. As discussed below,
variants of stochastic gradient descent have been implemented in accordance with the underlying synchronization model. Therefore, those variants constitute practical examples for the corresponding synchronization model.</p>
</div>
<section id="S4.SS5.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Bulk Synchronous Parallel</h4>

<div id="S4.SS5.SSS1.p1" class="ltx_para">
<p id="S4.SS5.SSS1.p1.1" class="ltx_p">It is a synchronization model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib111" title="" class="ltx_ref">111</a>]</cite> where synchronization happens between each computation and communication phase. Since this model is serializable by construction, the final output is guaranteed to be correct.
However, when there are discrepancy between the progress of parallel workers, the faster workers have to wait for the slower ones. This can
result in significant synchronization overhead.</p>
</div>
</section>
<section id="S4.SS5.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Stale Synchronous Parallel</h4>

<div id="S4.SS5.SSS2.p1" class="ltx_para">
<p id="S4.SS5.SSS2.p1.1" class="ltx_p">This synchronization model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib107" title="" class="ltx_ref">107</a>]</cite> allows the faster workers continue with their version of data for an additional but limited number of iterations to reduce the synchronization overheads due to the wait on the slower workers.
While this can help reduce the overheads, data consistency and model convergence may become difficult to establish.</p>
</div>
</section>
<section id="S4.SS5.SSS3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.3 </span>Approximate Synchronous Parallel</h4>

<div id="S4.SS5.SSS3.p1" class="ltx_para">
<p id="S4.SS5.SSS3.p1.1" class="ltx_p">In this model, synchronization is sometimes omitted or delayed to reduce the overheads.
However, the accuracy and consistency of a model may deteriorate if care is not taken.
An advantage of approximate synchronicity is that when a parameter update is insignificant, the server can delay synchronization as much as possible.
A disadvantage is that selecting which updates are significant or not is typically difficult to do.
As an example of the application of this model, Gaia <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib112" title="" class="ltx_ref">112</a>]</cite> is an approximate synchronous parallel ML system.</p>
</div>
</section>
<section id="S4.SS5.SSS4" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.4 </span>Asynchronous Parallel</h4>

<div id="S4.SS5.SSS4.p1" class="ltx_para">
<p id="S4.SS5.SSS4.p1.1" class="ltx_p">This synchronization model omits all synchronizations among the workers.
While these omissions may significantly reduce the computation time and communication overhead, asynchronous communications may cause ML models to produce incorrect outputs.
To give an example application,
HOGWILD algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib113" title="" class="ltx_ref">113</a>]</cite> are developed based on asynchronous communications.</p>
</div>
</section>
</section>
<section id="S4.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Existing Distributed Learning Frameworks</h3>

<div id="S4.SS6.p1" class="ltx_para">
<p id="S4.SS6.p1.1" class="ltx_p">There are many ML frameworks that provide distributed ML algorithms and utilities. The most popular distributed implementations are Tensorflow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>, <a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>, PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>, <a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>, MXNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>, <a href="#bib.bib120" title="" class="ltx_ref">120</a>]</cite>, Horovod <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite>, Baidu <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite>, Dianne <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite>, CNTK <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> and Theano <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite>.
Table <a href="#S4.T5" title="Table 5 ‣ 4.6 Existing Distributed Learning Frameworks ‣ 4 Distributed Machine Learning ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> summarizes these frameworks.
Other than the ML frameworks above, some general-purpose distributed computing libraries, such as Apache Spark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> and Hadoop <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>, also support distributed ML.</p>
</div>
<figure id="S4.T5" class="ltx_table">
<table id="S4.T5.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T5.1.1.1" class="ltx_tr">
<td id="S4.T5.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Frameworks</td>
<td id="S4.T5.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Pros</td>
<td id="S4.T5.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Cons</td>
<td id="S4.T5.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Parallelism</td>
</tr>
<tr id="S4.T5.1.2.2" class="ltx_tr">
<td id="S4.T5.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Tensorflow</td>
<td id="S4.T5.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.2.2.2.1.1" class="ltx_tr">
<td id="S4.T5.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Most popular.</td>
</tr>
<tr id="S4.T5.1.2.2.2.1.2" class="ltx_tr">
<td id="S4.T5.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Strong support by Google.</td>
</tr>
<tr id="S4.T5.1.2.2.2.1.3" class="ltx_tr">
<td id="S4.T5.1.2.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Efficient and scalable
CPU,</td>
</tr>
<tr id="S4.T5.1.2.2.2.1.4" class="ltx_tr">
<td id="S4.T5.1.2.2.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">multi-GPU,</td>
</tr>
<tr id="S4.T5.1.2.2.2.1.5" class="ltx_tr">
<td id="S4.T5.1.2.2.2.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left">mobile implementations.</td>
</tr>
<tr id="S4.T5.1.2.2.2.1.6" class="ltx_tr">
<td id="S4.T5.1.2.2.2.1.6.1" class="ltx_td ltx_nopad_r ltx_align_left">Various training strategies:</td>
</tr>
<tr id="S4.T5.1.2.2.2.1.7" class="ltx_tr">
<td id="S4.T5.1.2.2.2.1.7.1" class="ltx_td ltx_nopad_r ltx_align_left">Multi-worker, Parameter server…</td>
</tr>
</table>
</td>
<td id="S4.T5.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Difficult to use API</td>
<td id="S4.T5.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data, Model</td>
</tr>
<tr id="S4.T5.1.3.3" class="ltx_tr">
<td id="S4.T5.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">PyTorch</td>
<td id="S4.T5.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.3.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.3.3.2.1.1" class="ltx_tr">
<td id="S4.T5.1.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Dynamic computation graph</td>
</tr>
<tr id="S4.T5.1.3.3.2.1.2" class="ltx_tr">
<td id="S4.T5.1.3.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Automatic differentiation</td>
</tr>
<tr id="S4.T5.1.3.3.2.1.3" class="ltx_tr">
<td id="S4.T5.1.3.3.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Support of remote procedure calls</td>
</tr>
</table>
</td>
<td id="S4.T5.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No support for mobile</td>
<td id="S4.T5.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.3.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.3.3.4.1.1" class="ltx_tr">
<td id="S4.T5.1.3.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Data, Model,</td>
</tr>
<tr id="S4.T5.1.3.3.4.1.2" class="ltx_tr">
<td id="S4.T5.1.3.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Pipeline</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.4.4" class="ltx_tr">
<td id="S4.T5.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">MXNet</td>
<td id="S4.T5.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.4.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.4.4.2.1.1" class="ltx_tr">
<td id="S4.T5.1.4.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">High scalability</td>
</tr>
<tr id="S4.T5.1.4.4.2.1.2" class="ltx_tr">
<td id="S4.T5.1.4.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Support of many languages:</td>
</tr>
<tr id="S4.T5.1.4.4.2.1.3" class="ltx_tr">
<td id="S4.T5.1.4.4.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">C++, Python, Julia, R</td>
</tr>
<tr id="S4.T5.1.4.4.2.1.4" class="ltx_tr">
<td id="S4.T5.1.4.4.2.1.4.1" class="ltx_td ltx_nopad_r ltx_align_left">Usage of symbolic</td>
</tr>
<tr id="S4.T5.1.4.4.2.1.5" class="ltx_tr">
<td id="S4.T5.1.4.4.2.1.5.1" class="ltx_td ltx_nopad_r ltx_align_left">and imperative programming</td>
</tr>
</table>
</td>
<td id="S4.T5.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Difficult to use API</td>
<td id="S4.T5.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data</td>
</tr>
<tr id="S4.T5.1.5.5" class="ltx_tr">
<td id="S4.T5.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Horovod</td>
<td id="S4.T5.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.5.5.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.5.5.2.1.1" class="ltx_tr">
<td id="S4.T5.1.5.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Easy to use</td>
</tr>
<tr id="S4.T5.1.5.5.2.1.2" class="ltx_tr">
<td id="S4.T5.1.5.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Supports Tensorflow, Keras,</td>
</tr>
<tr id="S4.T5.1.5.5.2.1.3" class="ltx_tr">
<td id="S4.T5.1.5.5.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">PyTorch, and MXNet</td>
</tr>
</table>
</td>
<td id="S4.T5.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Lacks fault tolerance</td>
<td id="S4.T5.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.5.5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.5.5.4.1.1" class="ltx_tr">
<td id="S4.T5.1.5.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Data Model</td>
</tr>
<tr id="S4.T5.1.5.5.4.1.2" class="ltx_tr">
<td id="S4.T5.1.5.5.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Pipeline</td>
</tr>
</table>
</td>
</tr>
<tr id="S4.T5.1.6.6" class="ltx_tr">
<td id="S4.T5.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Baidu</td>
<td id="S4.T5.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Commercial ML and DL solutions</td>
<td id="S4.T5.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.6.6.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.6.6.3.1.1" class="ltx_tr">
<td id="S4.T5.1.6.6.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Limited scalability</td>
</tr>
<tr id="S4.T5.1.6.6.3.1.2" class="ltx_tr">
<td id="S4.T5.1.6.6.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">No support for fault-tolerance</td>
</tr>
</table>
</td>
<td id="S4.T5.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data, Pipeline</td>
</tr>
<tr id="S4.T5.1.7.7" class="ltx_tr">
<td id="S4.T5.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Dianne</td>
<td id="S4.T5.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Java based development platform</td>
<td id="S4.T5.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">No other languages</td>
<td id="S4.T5.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data, Model</td>
</tr>
<tr id="S4.T5.1.8.8" class="ltx_tr">
<td id="S4.T5.1.8.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">CNTK</td>
<td id="S4.T5.1.8.8.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.8.8.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.8.8.2.1.1" class="ltx_tr">
<td id="S4.T5.1.8.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Open-source</td>
</tr>
<tr id="S4.T5.1.8.8.2.1.2" class="ltx_tr">
<td id="S4.T5.1.8.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Efficient and high-performing</td>
</tr>
</table>
</td>
<td id="S4.T5.1.8.8.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S4.T5.1.8.8.3.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.8.8.3.1.1" class="ltx_tr">
<td id="S4.T5.1.8.8.3.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">No longer actively developed</td>
</tr>
<tr id="S4.T5.1.8.8.3.1.2" class="ltx_tr">
<td id="S4.T5.1.8.8.3.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Limited mobile support</td>
</tr>
</table>
</td>
<td id="S4.T5.1.8.8.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data, Model</td>
</tr>
<tr id="S4.T5.1.9.9" class="ltx_tr">
<td id="S4.T5.1.9.9.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Theano</td>
<td id="S4.T5.1.9.9.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S4.T5.1.9.9.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S4.T5.1.9.9.2.1.1" class="ltx_tr">
<td id="S4.T5.1.9.9.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Open-source and cross-platform</td>
</tr>
<tr id="S4.T5.1.9.9.2.1.2" class="ltx_tr">
<td id="S4.T5.1.9.9.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Powerful numerical library</td>
</tr>
</table>
</td>
<td id="S4.T5.1.9.9.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Discontinued</td>
<td id="S4.T5.1.9.9.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Data</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Existing distributed learning platforms.</figcaption>
</figure>
<div id="S4.SS6.p2" class="ltx_para">
<p id="S4.SS6.p2.1" class="ltx_p">Tensorflow <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib114" title="" class="ltx_ref">114</a>]</cite> is a free and open-source software library developed for ML and DL by Google.
In fact, Tensorflow is the most popular library among the DL libraries. It supports distributed learning with several distribution strategies, such as mirrored, multi-worker and parameter server, that are either data or model parallel <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib115" title="" class="ltx_ref">115</a>, <a href="#bib.bib116" title="" class="ltx_ref">116</a>]</cite>. The library provides efficient and scalable ML implementations for CPUs, multi-GPUs and mobile devices.</p>
</div>
<div id="S4.SS6.p3" class="ltx_para">
<p id="S4.SS6.p3.1" class="ltx_p">PyTorch <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib117" title="" class="ltx_ref">117</a>]</cite> is another free and open-source framework based on the Torch Library developed by Meta. It is a popular framework for scientific research and provides automatic differentiation and dynamic computation graphs. It supports distributed learning mainly in two ways with torch.distributed package <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib118" title="" class="ltx_ref">118</a>]</cite>. First, same as the Tensorflow mirrored strategy, PyTorch offers distributed data-parallel training which is based on the single-program and multiple-data paradigm. Second, for the cases that do not fit into data parallelism, PyTorch provides Remote Procedure Call (RPC) based distributed training. Examples of these types of distributed training are parameter server, pipeline parallelism, and reinforcement learning with multiple agents and observers.</p>
</div>
<div id="S4.SS6.p4" class="ltx_para">
<p id="S4.SS6.p4.1" class="ltx_p">MXNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib119" title="" class="ltx_ref">119</a>]</cite> is an open-source DL framework for research prototyping and production. It offers data-parallel distributed learning with parameter servers. MXNet allows mixing both symbolic and imperative programming for computational efficiency and scalability. MXNet supports many programming languages such as C++, Python, R and Julia.</p>
</div>
<div id="S4.SS6.p5" class="ltx_para">
<p id="S4.SS6.p5.1" class="ltx_p">Horovod <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib121" title="" class="ltx_ref">121</a>]</cite> is a distributed wrapper DL framework for TensorFlow, Keras, PyTorch, and Apache MXNet. Horovod is often easy to use because it only requires an addition of a small number of library calls to the source code. Horovod supports data, model and pipeline parallelisms.</p>
</div>
<div id="S4.SS6.p6" class="ltx_para">
<p id="S4.SS6.p6.1" class="ltx_p">Baidu <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib122" title="" class="ltx_ref">122</a>]</cite> was started as an easy-to-use, efficient distributed DL platform. It supports large-scale ML and can train hundreds of machines in parallel with GPUs. Baidu offers various commercial solutions, such as machine translation, recommender systems, image classification and segmentation.</p>
</div>
<div id="S4.SS6.p7" class="ltx_para">
<p id="S4.SS6.p7.1" class="ltx_p">Dianne <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib123" title="" class="ltx_ref">123</a>]</cite> is a distributed and ANNs-focused software framework based on OSGi which is a dynamic module system for Java. Dianne supports both model and data parallelisms and offers UI-based functionality.</p>
</div>
<div id="S4.SS6.p8" class="ltx_para">
<p id="S4.SS6.p8.1" class="ltx_p">The Microsoft Cognitive Toolkit (CNTK) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib124" title="" class="ltx_ref">124</a>]</cite> is open-source software for commercial-grade DL. However, it is no longer actively developed.
It supports distributed learning through parallel Stochastic Gradient Descent (SGD)
algorithms. CNTK implements the following four parallel SGD algorithms: Data-parallel,
block momentum, model averaging, and asynchronous data-parallel SGD.</p>
</div>
<div id="S4.SS6.p9" class="ltx_para">
<p id="S4.SS6.p9.1" class="ltx_p">Theano <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib125" title="" class="ltx_ref">125</a>]</cite> was a popular open-source Python library to define, optimize and evaluate mathematical expressions. It has support for efficient multi-dimensional arrays. Developed by Universite de Montreal, it is no longer used widely.
Theano supports data-parallel distributed learning by both synchronous and asynchronous training. It also supports multi-GPU multi-machine distributed training.</p>
</div>
<div id="S4.SS6.p10" class="ltx_para">
<p id="S4.SS6.p10.1" class="ltx_p">General-purpose distributed frameworks that are based on MapReduce programming model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib128" title="" class="ltx_ref">128</a>]</cite>, such as Apache Spark <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib126" title="" class="ltx_ref">126</a>]</cite> and Apache Hadoop <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib127" title="" class="ltx_ref">127</a>]</cite>, supports distributed ML algorithms, applications and utilities.
Apache Spark is one of the most popular implementations of MapReduce.
It includes MLlib <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib129" title="" class="ltx_ref">129</a>]</cite> which is an open-source scalable distributed ML library. MLlib consists of widely-used ML algorithms and utilities for classification, regression, clustering, and dimensionality reduction tasks.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Federated Learning (FL)</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this section, we first introduce FL.
We then present the existing aggregation algorithms in detail.
After that, we discuss the security and privacy aspects of FL. We conclude this section by the available FL platforms and datasets.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Introduction to FL</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> is a variant of ML where training a model is done by distributed clients that individually train local models. Once local models are trained, all local model parameters are sent to a central server which then calculates the average of the parameters (weights) to compute an average model. This average model is then communicated back to the clients for subsequent local training.
FL performs distributed training without sharing private client data.</p>
</div>
<figure id="S5.F9" class="ltx_figure"><img src="/html/2312.03120/assets/x11.png" id="S5.F9.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="315" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Federated Learning Overview.</figcaption>
</figure>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p">FL can be categorized based on how data partitioning is done.
Horizontal FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> refers to the case where the clients share the same feature space but have different sample spaces. This is similar to data parallelism.
An example of horizontal FL is wake-up voice recognition on smartphones. Users with different types of voices (different sample spaces) speak the same wake-up command (same feature space).</p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p">Vertical FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib130" title="" class="ltx_ref">130</a>]</cite> takes place where the clients share the same sample space but have different feature spaces. As an example, the common customers (same sample space) of a bank and an e-commerce company (different feature spaces) join the training of an FL model for optimizing personal loans.</p>
</div>
<div id="S5.SS1.p4" class="ltx_para">
<p id="S5.SS1.p4.1" class="ltx_p">Finally, Federated Transfer Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib131" title="" class="ltx_ref">131</a>]</cite> refers to the case where both the sample and the feature spaces are different. Federated transfer learning transfers features from different feature spaces to the same representation to train a model with the data of different clients.
An example is disease diagnosis by many different collaborating countries with multiple hospitals which have different patients (different sample spaces) with different medication tests (different feature spaces).</p>
</div>
<div id="S5.SS1.p5" class="ltx_para">
<p id="S5.SS1.p5.1" class="ltx_p">Distributed machine learning and FL have some fundamental differences. These are:</p>
<ul id="S5.I1" class="ltx_itemize">
<li id="S5.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i1.p1" class="ltx_para">
<p id="S5.I1.i1.p1.1" class="ltx_p">While distributed machine learning’s main goal is to minimize the computational costs and achieve high scalability, FL’s main goal is to provide privacy and security for the user/client data. As a result, FL is designed such that user/client data is never shared.</p>
</div>
</li>
<li id="S5.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i2.p1" class="ltx_para">
<p id="S5.I1.i2.p1.1" class="ltx_p">Distributed learning assumes that the user data is independent and identically distributed (i.i.d). On the other hand, FL assumes non-i.i.d because users typically have different data distributions and types.</p>
</div>
</li>
<li id="S5.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I1.i3.p1" class="ltx_para">
<p id="S5.I1.i3.p1.1" class="ltx_p">Distributed learning is performed based on aggregating client data, which is then distributed to different clients for training and inference. Contrarily, FL utilizes decentralized data. The client data is never shared and is never aggregated on a central server.</p>
</div>
</li>
</ul>
</div>
<figure id="alg1" class="ltx_float ltx_float_algorithm ltx_framed ltx_framed_top">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_float"><span id="alg1.2.1.1" class="ltx_text ltx_font_bold">Algorithm 1</span> </span> Federated learning: client and server functions</figcaption>
<div id="alg1.3" class="ltx_listing ltx_listing">
<div id="alg1.l1" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l1.1.1.1" class="ltx_text" style="font-size:80%;">1:</span></span><math id="alg1.l1.m1.1" class="ltx_Math" alttext="i\leftarrow isClient" display="inline"><semantics id="alg1.l1.m1.1a"><mrow id="alg1.l1.m1.1.1" xref="alg1.l1.m1.1.1.cmml"><mi id="alg1.l1.m1.1.1.2" xref="alg1.l1.m1.1.1.2.cmml">i</mi><mo stretchy="false" id="alg1.l1.m1.1.1.1" xref="alg1.l1.m1.1.1.1.cmml">←</mo><mrow id="alg1.l1.m1.1.1.3" xref="alg1.l1.m1.1.1.3.cmml"><mi id="alg1.l1.m1.1.1.3.2" xref="alg1.l1.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m1.1.1.3.1" xref="alg1.l1.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m1.1.1.3.3" xref="alg1.l1.m1.1.1.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m1.1.1.3.1a" xref="alg1.l1.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m1.1.1.3.4" xref="alg1.l1.m1.1.1.3.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m1.1.1.3.1b" xref="alg1.l1.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m1.1.1.3.5" xref="alg1.l1.m1.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m1.1.1.3.1c" xref="alg1.l1.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m1.1.1.3.6" xref="alg1.l1.m1.1.1.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m1.1.1.3.1d" xref="alg1.l1.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m1.1.1.3.7" xref="alg1.l1.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m1.1.1.3.1e" xref="alg1.l1.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m1.1.1.3.8" xref="alg1.l1.m1.1.1.3.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m1.1.1.3.1f" xref="alg1.l1.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l1.m1.1.1.3.9" xref="alg1.l1.m1.1.1.3.9.cmml">t</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m1.1b"><apply id="alg1.l1.m1.1.1.cmml" xref="alg1.l1.m1.1.1"><ci id="alg1.l1.m1.1.1.1.cmml" xref="alg1.l1.m1.1.1.1">←</ci><ci id="alg1.l1.m1.1.1.2.cmml" xref="alg1.l1.m1.1.1.2">𝑖</ci><apply id="alg1.l1.m1.1.1.3.cmml" xref="alg1.l1.m1.1.1.3"><times id="alg1.l1.m1.1.1.3.1.cmml" xref="alg1.l1.m1.1.1.3.1"></times><ci id="alg1.l1.m1.1.1.3.2.cmml" xref="alg1.l1.m1.1.1.3.2">𝑖</ci><ci id="alg1.l1.m1.1.1.3.3.cmml" xref="alg1.l1.m1.1.1.3.3">𝑠</ci><ci id="alg1.l1.m1.1.1.3.4.cmml" xref="alg1.l1.m1.1.1.3.4">𝐶</ci><ci id="alg1.l1.m1.1.1.3.5.cmml" xref="alg1.l1.m1.1.1.3.5">𝑙</ci><ci id="alg1.l1.m1.1.1.3.6.cmml" xref="alg1.l1.m1.1.1.3.6">𝑖</ci><ci id="alg1.l1.m1.1.1.3.7.cmml" xref="alg1.l1.m1.1.1.3.7">𝑒</ci><ci id="alg1.l1.m1.1.1.3.8.cmml" xref="alg1.l1.m1.1.1.3.8">𝑛</ci><ci id="alg1.l1.m1.1.1.3.9.cmml" xref="alg1.l1.m1.1.1.3.9">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m1.1c">i\leftarrow isClient</annotation></semantics></math> or <math id="alg1.l1.m2.1" class="ltx_Math" alttext="isServer" display="inline"><semantics id="alg1.l1.m2.1a"><mrow id="alg1.l1.m2.1.1" xref="alg1.l1.m2.1.1.cmml"><mi id="alg1.l1.m2.1.1.2" xref="alg1.l1.m2.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m2.1.1.1" xref="alg1.l1.m2.1.1.1.cmml">​</mo><mi id="alg1.l1.m2.1.1.3" xref="alg1.l1.m2.1.1.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m2.1.1.1a" xref="alg1.l1.m2.1.1.1.cmml">​</mo><mi id="alg1.l1.m2.1.1.4" xref="alg1.l1.m2.1.1.4.cmml">S</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m2.1.1.1b" xref="alg1.l1.m2.1.1.1.cmml">​</mo><mi id="alg1.l1.m2.1.1.5" xref="alg1.l1.m2.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m2.1.1.1c" xref="alg1.l1.m2.1.1.1.cmml">​</mo><mi id="alg1.l1.m2.1.1.6" xref="alg1.l1.m2.1.1.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m2.1.1.1d" xref="alg1.l1.m2.1.1.1.cmml">​</mo><mi id="alg1.l1.m2.1.1.7" xref="alg1.l1.m2.1.1.7.cmml">v</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m2.1.1.1e" xref="alg1.l1.m2.1.1.1.cmml">​</mo><mi id="alg1.l1.m2.1.1.8" xref="alg1.l1.m2.1.1.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l1.m2.1.1.1f" xref="alg1.l1.m2.1.1.1.cmml">​</mo><mi id="alg1.l1.m2.1.1.9" xref="alg1.l1.m2.1.1.9.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="alg1.l1.m2.1b"><apply id="alg1.l1.m2.1.1.cmml" xref="alg1.l1.m2.1.1"><times id="alg1.l1.m2.1.1.1.cmml" xref="alg1.l1.m2.1.1.1"></times><ci id="alg1.l1.m2.1.1.2.cmml" xref="alg1.l1.m2.1.1.2">𝑖</ci><ci id="alg1.l1.m2.1.1.3.cmml" xref="alg1.l1.m2.1.1.3">𝑠</ci><ci id="alg1.l1.m2.1.1.4.cmml" xref="alg1.l1.m2.1.1.4">𝑆</ci><ci id="alg1.l1.m2.1.1.5.cmml" xref="alg1.l1.m2.1.1.5">𝑒</ci><ci id="alg1.l1.m2.1.1.6.cmml" xref="alg1.l1.m2.1.1.6">𝑟</ci><ci id="alg1.l1.m2.1.1.7.cmml" xref="alg1.l1.m2.1.1.7">𝑣</ci><ci id="alg1.l1.m2.1.1.8.cmml" xref="alg1.l1.m2.1.1.8">𝑒</ci><ci id="alg1.l1.m2.1.1.9.cmml" xref="alg1.l1.m2.1.1.9">𝑟</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l1.m2.1c">isServer</annotation></semantics></math>

</div>
<div id="alg1.l2" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l2.1.1.1" class="ltx_text" style="font-size:80%;">2:</span></span><math id="alg1.l2.m1.1" class="ltx_Math" alttext="E\leftarrow totalEpochs" display="inline"><semantics id="alg1.l2.m1.1a"><mrow id="alg1.l2.m1.1.1" xref="alg1.l2.m1.1.1.cmml"><mi id="alg1.l2.m1.1.1.2" xref="alg1.l2.m1.1.1.2.cmml">E</mi><mo stretchy="false" id="alg1.l2.m1.1.1.1" xref="alg1.l2.m1.1.1.1.cmml">←</mo><mrow id="alg1.l2.m1.1.1.3" xref="alg1.l2.m1.1.1.3.cmml"><mi id="alg1.l2.m1.1.1.3.2" xref="alg1.l2.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.3" xref="alg1.l2.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1a" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.4" xref="alg1.l2.m1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1b" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.5" xref="alg1.l2.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1c" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.6" xref="alg1.l2.m1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1d" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.7" xref="alg1.l2.m1.1.1.3.7.cmml">E</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1e" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.8" xref="alg1.l2.m1.1.1.3.8.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1f" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.9" xref="alg1.l2.m1.1.1.3.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1g" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.10" xref="alg1.l2.m1.1.1.3.10.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1h" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.11" xref="alg1.l2.m1.1.1.3.11.cmml">h</mi><mo lspace="0em" rspace="0em" id="alg1.l2.m1.1.1.3.1i" xref="alg1.l2.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l2.m1.1.1.3.12" xref="alg1.l2.m1.1.1.3.12.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l2.m1.1b"><apply id="alg1.l2.m1.1.1.cmml" xref="alg1.l2.m1.1.1"><ci id="alg1.l2.m1.1.1.1.cmml" xref="alg1.l2.m1.1.1.1">←</ci><ci id="alg1.l2.m1.1.1.2.cmml" xref="alg1.l2.m1.1.1.2">𝐸</ci><apply id="alg1.l2.m1.1.1.3.cmml" xref="alg1.l2.m1.1.1.3"><times id="alg1.l2.m1.1.1.3.1.cmml" xref="alg1.l2.m1.1.1.3.1"></times><ci id="alg1.l2.m1.1.1.3.2.cmml" xref="alg1.l2.m1.1.1.3.2">𝑡</ci><ci id="alg1.l2.m1.1.1.3.3.cmml" xref="alg1.l2.m1.1.1.3.3">𝑜</ci><ci id="alg1.l2.m1.1.1.3.4.cmml" xref="alg1.l2.m1.1.1.3.4">𝑡</ci><ci id="alg1.l2.m1.1.1.3.5.cmml" xref="alg1.l2.m1.1.1.3.5">𝑎</ci><ci id="alg1.l2.m1.1.1.3.6.cmml" xref="alg1.l2.m1.1.1.3.6">𝑙</ci><ci id="alg1.l2.m1.1.1.3.7.cmml" xref="alg1.l2.m1.1.1.3.7">𝐸</ci><ci id="alg1.l2.m1.1.1.3.8.cmml" xref="alg1.l2.m1.1.1.3.8">𝑝</ci><ci id="alg1.l2.m1.1.1.3.9.cmml" xref="alg1.l2.m1.1.1.3.9">𝑜</ci><ci id="alg1.l2.m1.1.1.3.10.cmml" xref="alg1.l2.m1.1.1.3.10">𝑐</ci><ci id="alg1.l2.m1.1.1.3.11.cmml" xref="alg1.l2.m1.1.1.3.11">ℎ</ci><ci id="alg1.l2.m1.1.1.3.12.cmml" xref="alg1.l2.m1.1.1.3.12">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l2.m1.1c">E\leftarrow totalEpochs</annotation></semantics></math>

</div>
<div id="alg1.l3" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l3.1.1.1" class="ltx_text" style="font-size:80%;">3:</span></span><math id="alg1.l3.m1.1" class="ltx_Math" alttext="B\leftarrow totalNumBatches" display="inline"><semantics id="alg1.l3.m1.1a"><mrow id="alg1.l3.m1.1.1" xref="alg1.l3.m1.1.1.cmml"><mi id="alg1.l3.m1.1.1.2" xref="alg1.l3.m1.1.1.2.cmml">B</mi><mo stretchy="false" id="alg1.l3.m1.1.1.1" xref="alg1.l3.m1.1.1.1.cmml">←</mo><mrow id="alg1.l3.m1.1.1.3" xref="alg1.l3.m1.1.1.3.cmml"><mi id="alg1.l3.m1.1.1.3.2" xref="alg1.l3.m1.1.1.3.2.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.3" xref="alg1.l3.m1.1.1.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1a" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.4" xref="alg1.l3.m1.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1b" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.5" xref="alg1.l3.m1.1.1.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1c" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.6" xref="alg1.l3.m1.1.1.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1d" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.7" xref="alg1.l3.m1.1.1.3.7.cmml">N</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1e" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.8" xref="alg1.l3.m1.1.1.3.8.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1f" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.9" xref="alg1.l3.m1.1.1.3.9.cmml">m</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1g" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.10" xref="alg1.l3.m1.1.1.3.10.cmml">B</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1h" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.11" xref="alg1.l3.m1.1.1.3.11.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1i" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.12" xref="alg1.l3.m1.1.1.3.12.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1j" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.13" xref="alg1.l3.m1.1.1.3.13.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1k" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.14" xref="alg1.l3.m1.1.1.3.14.cmml">h</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1l" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.15" xref="alg1.l3.m1.1.1.3.15.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l3.m1.1.1.3.1m" xref="alg1.l3.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l3.m1.1.1.3.16" xref="alg1.l3.m1.1.1.3.16.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l3.m1.1b"><apply id="alg1.l3.m1.1.1.cmml" xref="alg1.l3.m1.1.1"><ci id="alg1.l3.m1.1.1.1.cmml" xref="alg1.l3.m1.1.1.1">←</ci><ci id="alg1.l3.m1.1.1.2.cmml" xref="alg1.l3.m1.1.1.2">𝐵</ci><apply id="alg1.l3.m1.1.1.3.cmml" xref="alg1.l3.m1.1.1.3"><times id="alg1.l3.m1.1.1.3.1.cmml" xref="alg1.l3.m1.1.1.3.1"></times><ci id="alg1.l3.m1.1.1.3.2.cmml" xref="alg1.l3.m1.1.1.3.2">𝑡</ci><ci id="alg1.l3.m1.1.1.3.3.cmml" xref="alg1.l3.m1.1.1.3.3">𝑜</ci><ci id="alg1.l3.m1.1.1.3.4.cmml" xref="alg1.l3.m1.1.1.3.4">𝑡</ci><ci id="alg1.l3.m1.1.1.3.5.cmml" xref="alg1.l3.m1.1.1.3.5">𝑎</ci><ci id="alg1.l3.m1.1.1.3.6.cmml" xref="alg1.l3.m1.1.1.3.6">𝑙</ci><ci id="alg1.l3.m1.1.1.3.7.cmml" xref="alg1.l3.m1.1.1.3.7">𝑁</ci><ci id="alg1.l3.m1.1.1.3.8.cmml" xref="alg1.l3.m1.1.1.3.8">𝑢</ci><ci id="alg1.l3.m1.1.1.3.9.cmml" xref="alg1.l3.m1.1.1.3.9">𝑚</ci><ci id="alg1.l3.m1.1.1.3.10.cmml" xref="alg1.l3.m1.1.1.3.10">𝐵</ci><ci id="alg1.l3.m1.1.1.3.11.cmml" xref="alg1.l3.m1.1.1.3.11">𝑎</ci><ci id="alg1.l3.m1.1.1.3.12.cmml" xref="alg1.l3.m1.1.1.3.12">𝑡</ci><ci id="alg1.l3.m1.1.1.3.13.cmml" xref="alg1.l3.m1.1.1.3.13">𝑐</ci><ci id="alg1.l3.m1.1.1.3.14.cmml" xref="alg1.l3.m1.1.1.3.14">ℎ</ci><ci id="alg1.l3.m1.1.1.3.15.cmml" xref="alg1.l3.m1.1.1.3.15">𝑒</ci><ci id="alg1.l3.m1.1.1.3.16.cmml" xref="alg1.l3.m1.1.1.3.16">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l3.m1.1c">B\leftarrow totalNumBatches</annotation></semantics></math>

</div>
<div id="alg1.l4" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l4.1.1.1" class="ltx_text" style="font-size:80%;">4:</span></span><math id="alg1.l4.m1.1" class="ltx_Math" alttext="\eta\leftarrow learningRate" display="inline"><semantics id="alg1.l4.m1.1a"><mrow id="alg1.l4.m1.1.1" xref="alg1.l4.m1.1.1.cmml"><mi id="alg1.l4.m1.1.1.2" xref="alg1.l4.m1.1.1.2.cmml">η</mi><mo stretchy="false" id="alg1.l4.m1.1.1.1" xref="alg1.l4.m1.1.1.1.cmml">←</mo><mrow id="alg1.l4.m1.1.1.3" xref="alg1.l4.m1.1.1.3.cmml"><mi id="alg1.l4.m1.1.1.3.2" xref="alg1.l4.m1.1.1.3.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.3" xref="alg1.l4.m1.1.1.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1a" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.4" xref="alg1.l4.m1.1.1.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1b" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.5" xref="alg1.l4.m1.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1c" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.6" xref="alg1.l4.m1.1.1.3.6.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1d" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.7" xref="alg1.l4.m1.1.1.3.7.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1e" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.8" xref="alg1.l4.m1.1.1.3.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1f" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.9" xref="alg1.l4.m1.1.1.3.9.cmml">g</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1g" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.10" xref="alg1.l4.m1.1.1.3.10.cmml">R</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1h" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.11" xref="alg1.l4.m1.1.1.3.11.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1i" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.12" xref="alg1.l4.m1.1.1.3.12.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l4.m1.1.1.3.1j" xref="alg1.l4.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l4.m1.1.1.3.13" xref="alg1.l4.m1.1.1.3.13.cmml">e</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l4.m1.1b"><apply id="alg1.l4.m1.1.1.cmml" xref="alg1.l4.m1.1.1"><ci id="alg1.l4.m1.1.1.1.cmml" xref="alg1.l4.m1.1.1.1">←</ci><ci id="alg1.l4.m1.1.1.2.cmml" xref="alg1.l4.m1.1.1.2">𝜂</ci><apply id="alg1.l4.m1.1.1.3.cmml" xref="alg1.l4.m1.1.1.3"><times id="alg1.l4.m1.1.1.3.1.cmml" xref="alg1.l4.m1.1.1.3.1"></times><ci id="alg1.l4.m1.1.1.3.2.cmml" xref="alg1.l4.m1.1.1.3.2">𝑙</ci><ci id="alg1.l4.m1.1.1.3.3.cmml" xref="alg1.l4.m1.1.1.3.3">𝑒</ci><ci id="alg1.l4.m1.1.1.3.4.cmml" xref="alg1.l4.m1.1.1.3.4">𝑎</ci><ci id="alg1.l4.m1.1.1.3.5.cmml" xref="alg1.l4.m1.1.1.3.5">𝑟</ci><ci id="alg1.l4.m1.1.1.3.6.cmml" xref="alg1.l4.m1.1.1.3.6">𝑛</ci><ci id="alg1.l4.m1.1.1.3.7.cmml" xref="alg1.l4.m1.1.1.3.7">𝑖</ci><ci id="alg1.l4.m1.1.1.3.8.cmml" xref="alg1.l4.m1.1.1.3.8">𝑛</ci><ci id="alg1.l4.m1.1.1.3.9.cmml" xref="alg1.l4.m1.1.1.3.9">𝑔</ci><ci id="alg1.l4.m1.1.1.3.10.cmml" xref="alg1.l4.m1.1.1.3.10">𝑅</ci><ci id="alg1.l4.m1.1.1.3.11.cmml" xref="alg1.l4.m1.1.1.3.11">𝑎</ci><ci id="alg1.l4.m1.1.1.3.12.cmml" xref="alg1.l4.m1.1.1.3.12">𝑡</ci><ci id="alg1.l4.m1.1.1.3.13.cmml" xref="alg1.l4.m1.1.1.3.13">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l4.m1.1c">\eta\leftarrow learningRate</annotation></semantics></math>

</div>
<div id="alg1.l5" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l5.1.1.1" class="ltx_text" style="font-size:80%;">5:</span></span><math id="alg1.l5.m1.1" class="ltx_Math" alttext="w\leftarrow initialWeights" display="inline"><semantics id="alg1.l5.m1.1a"><mrow id="alg1.l5.m1.1.1" xref="alg1.l5.m1.1.1.cmml"><mi id="alg1.l5.m1.1.1.2" xref="alg1.l5.m1.1.1.2.cmml">w</mi><mo stretchy="false" id="alg1.l5.m1.1.1.1" xref="alg1.l5.m1.1.1.1.cmml">←</mo><mrow id="alg1.l5.m1.1.1.3" xref="alg1.l5.m1.1.1.3.cmml"><mi id="alg1.l5.m1.1.1.3.2" xref="alg1.l5.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.3" xref="alg1.l5.m1.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1a" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.4" xref="alg1.l5.m1.1.1.3.4.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1b" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.5" xref="alg1.l5.m1.1.1.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1c" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.6" xref="alg1.l5.m1.1.1.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1d" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.7" xref="alg1.l5.m1.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1e" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.8" xref="alg1.l5.m1.1.1.3.8.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1f" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.9" xref="alg1.l5.m1.1.1.3.9.cmml">W</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1g" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.10" xref="alg1.l5.m1.1.1.3.10.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1h" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.11" xref="alg1.l5.m1.1.1.3.11.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1i" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.12" xref="alg1.l5.m1.1.1.3.12.cmml">g</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1j" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.13" xref="alg1.l5.m1.1.1.3.13.cmml">h</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1k" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.14" xref="alg1.l5.m1.1.1.3.14.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l5.m1.1.1.3.1l" xref="alg1.l5.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l5.m1.1.1.3.15" xref="alg1.l5.m1.1.1.3.15.cmml">s</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l5.m1.1b"><apply id="alg1.l5.m1.1.1.cmml" xref="alg1.l5.m1.1.1"><ci id="alg1.l5.m1.1.1.1.cmml" xref="alg1.l5.m1.1.1.1">←</ci><ci id="alg1.l5.m1.1.1.2.cmml" xref="alg1.l5.m1.1.1.2">𝑤</ci><apply id="alg1.l5.m1.1.1.3.cmml" xref="alg1.l5.m1.1.1.3"><times id="alg1.l5.m1.1.1.3.1.cmml" xref="alg1.l5.m1.1.1.3.1"></times><ci id="alg1.l5.m1.1.1.3.2.cmml" xref="alg1.l5.m1.1.1.3.2">𝑖</ci><ci id="alg1.l5.m1.1.1.3.3.cmml" xref="alg1.l5.m1.1.1.3.3">𝑛</ci><ci id="alg1.l5.m1.1.1.3.4.cmml" xref="alg1.l5.m1.1.1.3.4">𝑖</ci><ci id="alg1.l5.m1.1.1.3.5.cmml" xref="alg1.l5.m1.1.1.3.5">𝑡</ci><ci id="alg1.l5.m1.1.1.3.6.cmml" xref="alg1.l5.m1.1.1.3.6">𝑖</ci><ci id="alg1.l5.m1.1.1.3.7.cmml" xref="alg1.l5.m1.1.1.3.7">𝑎</ci><ci id="alg1.l5.m1.1.1.3.8.cmml" xref="alg1.l5.m1.1.1.3.8">𝑙</ci><ci id="alg1.l5.m1.1.1.3.9.cmml" xref="alg1.l5.m1.1.1.3.9">𝑊</ci><ci id="alg1.l5.m1.1.1.3.10.cmml" xref="alg1.l5.m1.1.1.3.10">𝑒</ci><ci id="alg1.l5.m1.1.1.3.11.cmml" xref="alg1.l5.m1.1.1.3.11">𝑖</ci><ci id="alg1.l5.m1.1.1.3.12.cmml" xref="alg1.l5.m1.1.1.3.12">𝑔</ci><ci id="alg1.l5.m1.1.1.3.13.cmml" xref="alg1.l5.m1.1.1.3.13">ℎ</ci><ci id="alg1.l5.m1.1.1.3.14.cmml" xref="alg1.l5.m1.1.1.3.14">𝑡</ci><ci id="alg1.l5.m1.1.1.3.15.cmml" xref="alg1.l5.m1.1.1.3.15">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l5.m1.1c">w\leftarrow initialWeights</annotation></semantics></math>

</div>
<div id="alg1.l6" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l6.1.1.1" class="ltx_text" style="font-size:80%;">6:</span></span><span id="alg1.l6.2" class="ltx_text ltx_font_bold">if</span> <math id="alg1.l6.m1.1" class="ltx_Math" alttext="i=isClient" display="inline"><semantics id="alg1.l6.m1.1a"><mrow id="alg1.l6.m1.1.1" xref="alg1.l6.m1.1.1.cmml"><mi id="alg1.l6.m1.1.1.2" xref="alg1.l6.m1.1.1.2.cmml">i</mi><mo id="alg1.l6.m1.1.1.1" xref="alg1.l6.m1.1.1.1.cmml">=</mo><mrow id="alg1.l6.m1.1.1.3" xref="alg1.l6.m1.1.1.3.cmml"><mi id="alg1.l6.m1.1.1.3.2" xref="alg1.l6.m1.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.3.1" xref="alg1.l6.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.3.3" xref="alg1.l6.m1.1.1.3.3.cmml">s</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.3.1a" xref="alg1.l6.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.3.4" xref="alg1.l6.m1.1.1.3.4.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.3.1b" xref="alg1.l6.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.3.5" xref="alg1.l6.m1.1.1.3.5.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.3.1c" xref="alg1.l6.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.3.6" xref="alg1.l6.m1.1.1.3.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.3.1d" xref="alg1.l6.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.3.7" xref="alg1.l6.m1.1.1.3.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.3.1e" xref="alg1.l6.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.3.8" xref="alg1.l6.m1.1.1.3.8.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l6.m1.1.1.3.1f" xref="alg1.l6.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l6.m1.1.1.3.9" xref="alg1.l6.m1.1.1.3.9.cmml">t</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l6.m1.1b"><apply id="alg1.l6.m1.1.1.cmml" xref="alg1.l6.m1.1.1"><eq id="alg1.l6.m1.1.1.1.cmml" xref="alg1.l6.m1.1.1.1"></eq><ci id="alg1.l6.m1.1.1.2.cmml" xref="alg1.l6.m1.1.1.2">𝑖</ci><apply id="alg1.l6.m1.1.1.3.cmml" xref="alg1.l6.m1.1.1.3"><times id="alg1.l6.m1.1.1.3.1.cmml" xref="alg1.l6.m1.1.1.3.1"></times><ci id="alg1.l6.m1.1.1.3.2.cmml" xref="alg1.l6.m1.1.1.3.2">𝑖</ci><ci id="alg1.l6.m1.1.1.3.3.cmml" xref="alg1.l6.m1.1.1.3.3">𝑠</ci><ci id="alg1.l6.m1.1.1.3.4.cmml" xref="alg1.l6.m1.1.1.3.4">𝐶</ci><ci id="alg1.l6.m1.1.1.3.5.cmml" xref="alg1.l6.m1.1.1.3.5">𝑙</ci><ci id="alg1.l6.m1.1.1.3.6.cmml" xref="alg1.l6.m1.1.1.3.6">𝑖</ci><ci id="alg1.l6.m1.1.1.3.7.cmml" xref="alg1.l6.m1.1.1.3.7">𝑒</ci><ci id="alg1.l6.m1.1.1.3.8.cmml" xref="alg1.l6.m1.1.1.3.8">𝑛</ci><ci id="alg1.l6.m1.1.1.3.9.cmml" xref="alg1.l6.m1.1.1.3.9">𝑡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l6.m1.1c">i=isClient</annotation></semantics></math> <span id="alg1.l6.3" class="ltx_text ltx_font_bold">then</span>

</div>
<div id="alg1.l7" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l7.1.1.1" class="ltx_text" style="font-size:80%;">7:</span></span>     <span id="alg1.l7.2" class="ltx_text ltx_font_bold">function</span> <span id="alg1.l7.3" class="ltx_text ltx_font_smallcaps">UpdateClientWeight</span>(w,k) 

</div>
<div id="alg1.l8" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l8.1.1.1" class="ltx_text" style="font-size:80%;">8:</span></span>         <span id="alg1.l8.2" class="ltx_text ltx_font_bold">for</span> epochs <math id="alg1.l8.m1.1" class="ltx_Math" alttext="e" display="inline"><semantics id="alg1.l8.m1.1a"><mi id="alg1.l8.m1.1.1" xref="alg1.l8.m1.1.1.cmml">e</mi><annotation-xml encoding="MathML-Content" id="alg1.l8.m1.1b"><ci id="alg1.l8.m1.1.1.cmml" xref="alg1.l8.m1.1.1">𝑒</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m1.1c">e</annotation></semantics></math> from 1 to <math id="alg1.l8.m2.1" class="ltx_Math" alttext="E" display="inline"><semantics id="alg1.l8.m2.1a"><mi id="alg1.l8.m2.1.1" xref="alg1.l8.m2.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="alg1.l8.m2.1b"><ci id="alg1.l8.m2.1.1.cmml" xref="alg1.l8.m2.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l8.m2.1c">E</annotation></semantics></math> <span id="alg1.l8.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l9" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l9.1.1.1" class="ltx_text" style="font-size:80%;">9:</span></span>              <span id="alg1.l9.2" class="ltx_text ltx_font_bold">for</span> batchs <math id="alg1.l9.m1.1" class="ltx_Math" alttext="b" display="inline"><semantics id="alg1.l9.m1.1a"><mi id="alg1.l9.m1.1.1" xref="alg1.l9.m1.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m1.1b"><ci id="alg1.l9.m1.1.1.cmml" xref="alg1.l9.m1.1.1">𝑏</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m1.1c">b</annotation></semantics></math> from 1 to <math id="alg1.l9.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="alg1.l9.m2.1a"><mi id="alg1.l9.m2.1.1" xref="alg1.l9.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="alg1.l9.m2.1b"><ci id="alg1.l9.m2.1.1.cmml" xref="alg1.l9.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l9.m2.1c">B</annotation></semantics></math> <span id="alg1.l9.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l10" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l10.1.1.1" class="ltx_text" style="font-size:80%;">10:</span></span>                  <math id="alg1.l10.m1.2" class="ltx_Math" alttext="w\leftarrow w-\eta\nabla l(w,b)" display="inline"><semantics id="alg1.l10.m1.2a"><mrow id="alg1.l10.m1.2.3" xref="alg1.l10.m1.2.3.cmml"><mi id="alg1.l10.m1.2.3.2" xref="alg1.l10.m1.2.3.2.cmml">w</mi><mo stretchy="false" id="alg1.l10.m1.2.3.1" xref="alg1.l10.m1.2.3.1.cmml">←</mo><mrow id="alg1.l10.m1.2.3.3" xref="alg1.l10.m1.2.3.3.cmml"><mi id="alg1.l10.m1.2.3.3.2" xref="alg1.l10.m1.2.3.3.2.cmml">w</mi><mo id="alg1.l10.m1.2.3.3.1" xref="alg1.l10.m1.2.3.3.1.cmml">−</mo><mrow id="alg1.l10.m1.2.3.3.3" xref="alg1.l10.m1.2.3.3.3.cmml"><mi id="alg1.l10.m1.2.3.3.3.2" xref="alg1.l10.m1.2.3.3.3.2.cmml">η</mi><mo lspace="0.167em" rspace="0em" id="alg1.l10.m1.2.3.3.3.1" xref="alg1.l10.m1.2.3.3.3.1.cmml">​</mo><mrow id="alg1.l10.m1.2.3.3.3.3" xref="alg1.l10.m1.2.3.3.3.3.cmml"><mo rspace="0.167em" id="alg1.l10.m1.2.3.3.3.3.1" xref="alg1.l10.m1.2.3.3.3.3.1.cmml">∇</mo><mi id="alg1.l10.m1.2.3.3.3.3.2" xref="alg1.l10.m1.2.3.3.3.3.2.cmml">l</mi></mrow><mo lspace="0em" rspace="0em" id="alg1.l10.m1.2.3.3.3.1a" xref="alg1.l10.m1.2.3.3.3.1.cmml">​</mo><mrow id="alg1.l10.m1.2.3.3.3.4.2" xref="alg1.l10.m1.2.3.3.3.4.1.cmml"><mo stretchy="false" id="alg1.l10.m1.2.3.3.3.4.2.1" xref="alg1.l10.m1.2.3.3.3.4.1.cmml">(</mo><mi id="alg1.l10.m1.1.1" xref="alg1.l10.m1.1.1.cmml">w</mi><mo id="alg1.l10.m1.2.3.3.3.4.2.2" xref="alg1.l10.m1.2.3.3.3.4.1.cmml">,</mo><mi id="alg1.l10.m1.2.2" xref="alg1.l10.m1.2.2.cmml">b</mi><mo stretchy="false" id="alg1.l10.m1.2.3.3.3.4.2.3" xref="alg1.l10.m1.2.3.3.3.4.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l10.m1.2b"><apply id="alg1.l10.m1.2.3.cmml" xref="alg1.l10.m1.2.3"><ci id="alg1.l10.m1.2.3.1.cmml" xref="alg1.l10.m1.2.3.1">←</ci><ci id="alg1.l10.m1.2.3.2.cmml" xref="alg1.l10.m1.2.3.2">𝑤</ci><apply id="alg1.l10.m1.2.3.3.cmml" xref="alg1.l10.m1.2.3.3"><minus id="alg1.l10.m1.2.3.3.1.cmml" xref="alg1.l10.m1.2.3.3.1"></minus><ci id="alg1.l10.m1.2.3.3.2.cmml" xref="alg1.l10.m1.2.3.3.2">𝑤</ci><apply id="alg1.l10.m1.2.3.3.3.cmml" xref="alg1.l10.m1.2.3.3.3"><times id="alg1.l10.m1.2.3.3.3.1.cmml" xref="alg1.l10.m1.2.3.3.3.1"></times><ci id="alg1.l10.m1.2.3.3.3.2.cmml" xref="alg1.l10.m1.2.3.3.3.2">𝜂</ci><apply id="alg1.l10.m1.2.3.3.3.3.cmml" xref="alg1.l10.m1.2.3.3.3.3"><ci id="alg1.l10.m1.2.3.3.3.3.1.cmml" xref="alg1.l10.m1.2.3.3.3.3.1">∇</ci><ci id="alg1.l10.m1.2.3.3.3.3.2.cmml" xref="alg1.l10.m1.2.3.3.3.3.2">𝑙</ci></apply><interval closure="open" id="alg1.l10.m1.2.3.3.3.4.1.cmml" xref="alg1.l10.m1.2.3.3.3.4.2"><ci id="alg1.l10.m1.1.1.cmml" xref="alg1.l10.m1.1.1">𝑤</ci><ci id="alg1.l10.m1.2.2.cmml" xref="alg1.l10.m1.2.2">𝑏</ci></interval></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l10.m1.2c">w\leftarrow w-\eta\nabla l(w,b)</annotation></semantics></math>

</div>
<div id="alg1.l11" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l11.1.1.1" class="ltx_text" style="font-size:80%;">11:</span></span>              <span id="alg1.l11.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l11.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l12" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l12.1.1.1" class="ltx_text" style="font-size:80%;">12:</span></span>         <span id="alg1.l12.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l12.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l13" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l13.1.1.1" class="ltx_text" style="font-size:80%;">13:</span></span>         <span id="alg1.l13.2" class="ltx_text ltx_font_bold">return</span> w to the server

</div>
<div id="alg1.l14" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l14.1.1.1" class="ltx_text" style="font-size:80%;">14:</span></span>     <span id="alg1.l14.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l14.3" class="ltx_text ltx_font_bold">function</span>
</div>
<div id="alg1.l15" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l15.1.1.1" class="ltx_text" style="font-size:80%;">15:</span></span><span id="alg1.l15.2" class="ltx_text ltx_font_bold">else</span>
</div>
<div id="alg1.l16" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l16.1.1.1" class="ltx_text" style="font-size:80%;">16:</span></span>     <span id="alg1.l16.2" class="ltx_text ltx_font_bold">function</span> <span id="alg1.l16.3" class="ltx_text ltx_font_smallcaps">ServerUpdateWeight</span> 

</div>
<div id="alg1.l17" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l17.1.1.1" class="ltx_text" style="font-size:80%;">17:</span></span>         <math id="alg1.l17.m1.1" class="ltx_Math" alttext="t\leftarrow currentRoundID" display="inline"><semantics id="alg1.l17.m1.1a"><mrow id="alg1.l17.m1.1.1" xref="alg1.l17.m1.1.1.cmml"><mi id="alg1.l17.m1.1.1.2" xref="alg1.l17.m1.1.1.2.cmml">t</mi><mo stretchy="false" id="alg1.l17.m1.1.1.1" xref="alg1.l17.m1.1.1.1.cmml">←</mo><mrow id="alg1.l17.m1.1.1.3" xref="alg1.l17.m1.1.1.3.cmml"><mi id="alg1.l17.m1.1.1.3.2" xref="alg1.l17.m1.1.1.3.2.cmml">c</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.3" xref="alg1.l17.m1.1.1.3.3.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1a" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.4" xref="alg1.l17.m1.1.1.3.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1b" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.5" xref="alg1.l17.m1.1.1.3.5.cmml">r</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1c" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.6" xref="alg1.l17.m1.1.1.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1d" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.7" xref="alg1.l17.m1.1.1.3.7.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1e" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.8" xref="alg1.l17.m1.1.1.3.8.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1f" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.9" xref="alg1.l17.m1.1.1.3.9.cmml">R</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1g" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.10" xref="alg1.l17.m1.1.1.3.10.cmml">o</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1h" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.11" xref="alg1.l17.m1.1.1.3.11.cmml">u</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1i" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.12" xref="alg1.l17.m1.1.1.3.12.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1j" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.13" xref="alg1.l17.m1.1.1.3.13.cmml">d</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1k" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.14" xref="alg1.l17.m1.1.1.3.14.cmml">I</mi><mo lspace="0em" rspace="0em" id="alg1.l17.m1.1.1.3.1l" xref="alg1.l17.m1.1.1.3.1.cmml">​</mo><mi id="alg1.l17.m1.1.1.3.15" xref="alg1.l17.m1.1.1.3.15.cmml">D</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l17.m1.1b"><apply id="alg1.l17.m1.1.1.cmml" xref="alg1.l17.m1.1.1"><ci id="alg1.l17.m1.1.1.1.cmml" xref="alg1.l17.m1.1.1.1">←</ci><ci id="alg1.l17.m1.1.1.2.cmml" xref="alg1.l17.m1.1.1.2">𝑡</ci><apply id="alg1.l17.m1.1.1.3.cmml" xref="alg1.l17.m1.1.1.3"><times id="alg1.l17.m1.1.1.3.1.cmml" xref="alg1.l17.m1.1.1.3.1"></times><ci id="alg1.l17.m1.1.1.3.2.cmml" xref="alg1.l17.m1.1.1.3.2">𝑐</ci><ci id="alg1.l17.m1.1.1.3.3.cmml" xref="alg1.l17.m1.1.1.3.3">𝑢</ci><ci id="alg1.l17.m1.1.1.3.4.cmml" xref="alg1.l17.m1.1.1.3.4">𝑟</ci><ci id="alg1.l17.m1.1.1.3.5.cmml" xref="alg1.l17.m1.1.1.3.5">𝑟</ci><ci id="alg1.l17.m1.1.1.3.6.cmml" xref="alg1.l17.m1.1.1.3.6">𝑒</ci><ci id="alg1.l17.m1.1.1.3.7.cmml" xref="alg1.l17.m1.1.1.3.7">𝑛</ci><ci id="alg1.l17.m1.1.1.3.8.cmml" xref="alg1.l17.m1.1.1.3.8">𝑡</ci><ci id="alg1.l17.m1.1.1.3.9.cmml" xref="alg1.l17.m1.1.1.3.9">𝑅</ci><ci id="alg1.l17.m1.1.1.3.10.cmml" xref="alg1.l17.m1.1.1.3.10">𝑜</ci><ci id="alg1.l17.m1.1.1.3.11.cmml" xref="alg1.l17.m1.1.1.3.11">𝑢</ci><ci id="alg1.l17.m1.1.1.3.12.cmml" xref="alg1.l17.m1.1.1.3.12">𝑛</ci><ci id="alg1.l17.m1.1.1.3.13.cmml" xref="alg1.l17.m1.1.1.3.13">𝑑</ci><ci id="alg1.l17.m1.1.1.3.14.cmml" xref="alg1.l17.m1.1.1.3.14">𝐼</ci><ci id="alg1.l17.m1.1.1.3.15.cmml" xref="alg1.l17.m1.1.1.3.15">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l17.m1.1c">t\leftarrow currentRoundID</annotation></semantics></math>

</div>
<div id="alg1.l18" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l18.1.1.1" class="ltx_text" style="font-size:80%;">18:</span></span>         <span id="alg1.l18.2" class="ltx_text ltx_font_bold">for</span> <math id="alg1.l18.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="alg1.l18.m1.1a"><mi id="alg1.l18.m1.1.1" xref="alg1.l18.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="alg1.l18.m1.1b"><ci id="alg1.l18.m1.1.1.cmml" xref="alg1.l18.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m1.1c">k</annotation></semantics></math> in sub batch of <math id="alg1.l18.m2.1" class="ltx_Math" alttext="K" display="inline"><semantics id="alg1.l18.m2.1a"><mi id="alg1.l18.m2.1.1" xref="alg1.l18.m2.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="alg1.l18.m2.1b"><ci id="alg1.l18.m2.1.1.cmml" xref="alg1.l18.m2.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="alg1.l18.m2.1c">K</annotation></semantics></math> clients <span id="alg1.l18.3" class="ltx_text ltx_font_bold">do</span>

</div>
<div id="alg1.l19" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l19.1.1.1" class="ltx_text" style="font-size:80%;">19:</span></span>              the following is done in parallel

</div>
<div id="alg1.l20" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l20.1.1.1" class="ltx_text" style="font-size:80%;">20:</span></span>              <math id="alg1.l20.m1.2" class="ltx_Math" alttext="w^{k}_{t+1}\leftarrow UpdateClientWeight(k,w^{k}_{t})" display="inline"><semantics id="alg1.l20.m1.2a"><mrow id="alg1.l20.m1.2.2" xref="alg1.l20.m1.2.2.cmml"><msubsup id="alg1.l20.m1.2.2.3" xref="alg1.l20.m1.2.2.3.cmml"><mi id="alg1.l20.m1.2.2.3.2.2" xref="alg1.l20.m1.2.2.3.2.2.cmml">w</mi><mrow id="alg1.l20.m1.2.2.3.3" xref="alg1.l20.m1.2.2.3.3.cmml"><mi id="alg1.l20.m1.2.2.3.3.2" xref="alg1.l20.m1.2.2.3.3.2.cmml">t</mi><mo id="alg1.l20.m1.2.2.3.3.1" xref="alg1.l20.m1.2.2.3.3.1.cmml">+</mo><mn id="alg1.l20.m1.2.2.3.3.3" xref="alg1.l20.m1.2.2.3.3.3.cmml">1</mn></mrow><mi id="alg1.l20.m1.2.2.3.2.3" xref="alg1.l20.m1.2.2.3.2.3.cmml">k</mi></msubsup><mo stretchy="false" id="alg1.l20.m1.2.2.2" xref="alg1.l20.m1.2.2.2.cmml">←</mo><mrow id="alg1.l20.m1.2.2.1" xref="alg1.l20.m1.2.2.1.cmml"><mi id="alg1.l20.m1.2.2.1.3" xref="alg1.l20.m1.2.2.1.3.cmml">U</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.4" xref="alg1.l20.m1.2.2.1.4.cmml">p</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2a" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.5" xref="alg1.l20.m1.2.2.1.5.cmml">d</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2b" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.6" xref="alg1.l20.m1.2.2.1.6.cmml">a</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2c" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.7" xref="alg1.l20.m1.2.2.1.7.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2d" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.8" xref="alg1.l20.m1.2.2.1.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2e" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.9" xref="alg1.l20.m1.2.2.1.9.cmml">C</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2f" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.10" xref="alg1.l20.m1.2.2.1.10.cmml">l</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2g" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.11" xref="alg1.l20.m1.2.2.1.11.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2h" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.12" xref="alg1.l20.m1.2.2.1.12.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2i" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.13" xref="alg1.l20.m1.2.2.1.13.cmml">n</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2j" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.14" xref="alg1.l20.m1.2.2.1.14.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2k" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.15" xref="alg1.l20.m1.2.2.1.15.cmml">W</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2l" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.16" xref="alg1.l20.m1.2.2.1.16.cmml">e</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2m" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.17" xref="alg1.l20.m1.2.2.1.17.cmml">i</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2n" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.18" xref="alg1.l20.m1.2.2.1.18.cmml">g</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2o" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.19" xref="alg1.l20.m1.2.2.1.19.cmml">h</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2p" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mi id="alg1.l20.m1.2.2.1.20" xref="alg1.l20.m1.2.2.1.20.cmml">t</mi><mo lspace="0em" rspace="0em" id="alg1.l20.m1.2.2.1.2q" xref="alg1.l20.m1.2.2.1.2.cmml">​</mo><mrow id="alg1.l20.m1.2.2.1.1.1" xref="alg1.l20.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="alg1.l20.m1.2.2.1.1.1.2" xref="alg1.l20.m1.2.2.1.1.2.cmml">(</mo><mi id="alg1.l20.m1.1.1" xref="alg1.l20.m1.1.1.cmml">k</mi><mo id="alg1.l20.m1.2.2.1.1.1.3" xref="alg1.l20.m1.2.2.1.1.2.cmml">,</mo><msubsup id="alg1.l20.m1.2.2.1.1.1.1" xref="alg1.l20.m1.2.2.1.1.1.1.cmml"><mi id="alg1.l20.m1.2.2.1.1.1.1.2.2" xref="alg1.l20.m1.2.2.1.1.1.1.2.2.cmml">w</mi><mi id="alg1.l20.m1.2.2.1.1.1.1.3" xref="alg1.l20.m1.2.2.1.1.1.1.3.cmml">t</mi><mi id="alg1.l20.m1.2.2.1.1.1.1.2.3" xref="alg1.l20.m1.2.2.1.1.1.1.2.3.cmml">k</mi></msubsup><mo stretchy="false" id="alg1.l20.m1.2.2.1.1.1.4" xref="alg1.l20.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="alg1.l20.m1.2b"><apply id="alg1.l20.m1.2.2.cmml" xref="alg1.l20.m1.2.2"><ci id="alg1.l20.m1.2.2.2.cmml" xref="alg1.l20.m1.2.2.2">←</ci><apply id="alg1.l20.m1.2.2.3.cmml" xref="alg1.l20.m1.2.2.3"><csymbol cd="ambiguous" id="alg1.l20.m1.2.2.3.1.cmml" xref="alg1.l20.m1.2.2.3">subscript</csymbol><apply id="alg1.l20.m1.2.2.3.2.cmml" xref="alg1.l20.m1.2.2.3"><csymbol cd="ambiguous" id="alg1.l20.m1.2.2.3.2.1.cmml" xref="alg1.l20.m1.2.2.3">superscript</csymbol><ci id="alg1.l20.m1.2.2.3.2.2.cmml" xref="alg1.l20.m1.2.2.3.2.2">𝑤</ci><ci id="alg1.l20.m1.2.2.3.2.3.cmml" xref="alg1.l20.m1.2.2.3.2.3">𝑘</ci></apply><apply id="alg1.l20.m1.2.2.3.3.cmml" xref="alg1.l20.m1.2.2.3.3"><plus id="alg1.l20.m1.2.2.3.3.1.cmml" xref="alg1.l20.m1.2.2.3.3.1"></plus><ci id="alg1.l20.m1.2.2.3.3.2.cmml" xref="alg1.l20.m1.2.2.3.3.2">𝑡</ci><cn type="integer" id="alg1.l20.m1.2.2.3.3.3.cmml" xref="alg1.l20.m1.2.2.3.3.3">1</cn></apply></apply><apply id="alg1.l20.m1.2.2.1.cmml" xref="alg1.l20.m1.2.2.1"><times id="alg1.l20.m1.2.2.1.2.cmml" xref="alg1.l20.m1.2.2.1.2"></times><ci id="alg1.l20.m1.2.2.1.3.cmml" xref="alg1.l20.m1.2.2.1.3">𝑈</ci><ci id="alg1.l20.m1.2.2.1.4.cmml" xref="alg1.l20.m1.2.2.1.4">𝑝</ci><ci id="alg1.l20.m1.2.2.1.5.cmml" xref="alg1.l20.m1.2.2.1.5">𝑑</ci><ci id="alg1.l20.m1.2.2.1.6.cmml" xref="alg1.l20.m1.2.2.1.6">𝑎</ci><ci id="alg1.l20.m1.2.2.1.7.cmml" xref="alg1.l20.m1.2.2.1.7">𝑡</ci><ci id="alg1.l20.m1.2.2.1.8.cmml" xref="alg1.l20.m1.2.2.1.8">𝑒</ci><ci id="alg1.l20.m1.2.2.1.9.cmml" xref="alg1.l20.m1.2.2.1.9">𝐶</ci><ci id="alg1.l20.m1.2.2.1.10.cmml" xref="alg1.l20.m1.2.2.1.10">𝑙</ci><ci id="alg1.l20.m1.2.2.1.11.cmml" xref="alg1.l20.m1.2.2.1.11">𝑖</ci><ci id="alg1.l20.m1.2.2.1.12.cmml" xref="alg1.l20.m1.2.2.1.12">𝑒</ci><ci id="alg1.l20.m1.2.2.1.13.cmml" xref="alg1.l20.m1.2.2.1.13">𝑛</ci><ci id="alg1.l20.m1.2.2.1.14.cmml" xref="alg1.l20.m1.2.2.1.14">𝑡</ci><ci id="alg1.l20.m1.2.2.1.15.cmml" xref="alg1.l20.m1.2.2.1.15">𝑊</ci><ci id="alg1.l20.m1.2.2.1.16.cmml" xref="alg1.l20.m1.2.2.1.16">𝑒</ci><ci id="alg1.l20.m1.2.2.1.17.cmml" xref="alg1.l20.m1.2.2.1.17">𝑖</ci><ci id="alg1.l20.m1.2.2.1.18.cmml" xref="alg1.l20.m1.2.2.1.18">𝑔</ci><ci id="alg1.l20.m1.2.2.1.19.cmml" xref="alg1.l20.m1.2.2.1.19">ℎ</ci><ci id="alg1.l20.m1.2.2.1.20.cmml" xref="alg1.l20.m1.2.2.1.20">𝑡</ci><interval closure="open" id="alg1.l20.m1.2.2.1.1.2.cmml" xref="alg1.l20.m1.2.2.1.1.1"><ci id="alg1.l20.m1.1.1.cmml" xref="alg1.l20.m1.1.1">𝑘</ci><apply id="alg1.l20.m1.2.2.1.1.1.1.cmml" xref="alg1.l20.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l20.m1.2.2.1.1.1.1.1.cmml" xref="alg1.l20.m1.2.2.1.1.1.1">subscript</csymbol><apply id="alg1.l20.m1.2.2.1.1.1.1.2.cmml" xref="alg1.l20.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="alg1.l20.m1.2.2.1.1.1.1.2.1.cmml" xref="alg1.l20.m1.2.2.1.1.1.1">superscript</csymbol><ci id="alg1.l20.m1.2.2.1.1.1.1.2.2.cmml" xref="alg1.l20.m1.2.2.1.1.1.1.2.2">𝑤</ci><ci id="alg1.l20.m1.2.2.1.1.1.1.2.3.cmml" xref="alg1.l20.m1.2.2.1.1.1.1.2.3">𝑘</ci></apply><ci id="alg1.l20.m1.2.2.1.1.1.1.3.cmml" xref="alg1.l20.m1.2.2.1.1.1.1.3">𝑡</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l20.m1.2c">w^{k}_{t+1}\leftarrow UpdateClientWeight(k,w^{k}_{t})</annotation></semantics></math>

</div>
<div id="alg1.l21" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l21.1.1.1" class="ltx_text" style="font-size:80%;">21:</span></span>         <span id="alg1.l21.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l21.3" class="ltx_text ltx_font_bold">for</span>
</div>
<div id="alg1.l22" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l22.1.1.1" class="ltx_text" style="font-size:80%;">22:</span></span>         <math id="alg1.l22.m1.1" class="ltx_Math" alttext="w_{t+1}\leftarrow\frac{\sum_{k=1}^{K}w^{k}_{t+1}}{K}" display="inline"><semantics id="alg1.l22.m1.1a"><mrow id="alg1.l22.m1.1.1" xref="alg1.l22.m1.1.1.cmml"><msub id="alg1.l22.m1.1.1.2" xref="alg1.l22.m1.1.1.2.cmml"><mi id="alg1.l22.m1.1.1.2.2" xref="alg1.l22.m1.1.1.2.2.cmml">w</mi><mrow id="alg1.l22.m1.1.1.2.3" xref="alg1.l22.m1.1.1.2.3.cmml"><mi id="alg1.l22.m1.1.1.2.3.2" xref="alg1.l22.m1.1.1.2.3.2.cmml">t</mi><mo id="alg1.l22.m1.1.1.2.3.1" xref="alg1.l22.m1.1.1.2.3.1.cmml">+</mo><mn id="alg1.l22.m1.1.1.2.3.3" xref="alg1.l22.m1.1.1.2.3.3.cmml">1</mn></mrow></msub><mo stretchy="false" id="alg1.l22.m1.1.1.1" xref="alg1.l22.m1.1.1.1.cmml">←</mo><mfrac id="alg1.l22.m1.1.1.3" xref="alg1.l22.m1.1.1.3.cmml"><mrow id="alg1.l22.m1.1.1.3.2" xref="alg1.l22.m1.1.1.3.2.cmml"><mstyle displaystyle="false" id="alg1.l22.m1.1.1.3.2.1" xref="alg1.l22.m1.1.1.3.2.1.cmml"><msubsup id="alg1.l22.m1.1.1.3.2.1a" xref="alg1.l22.m1.1.1.3.2.1.cmml"><mo id="alg1.l22.m1.1.1.3.2.1.2.2" xref="alg1.l22.m1.1.1.3.2.1.2.2.cmml">∑</mo><mrow id="alg1.l22.m1.1.1.3.2.1.2.3" xref="alg1.l22.m1.1.1.3.2.1.2.3.cmml"><mi id="alg1.l22.m1.1.1.3.2.1.2.3.2" xref="alg1.l22.m1.1.1.3.2.1.2.3.2.cmml">k</mi><mo id="alg1.l22.m1.1.1.3.2.1.2.3.1" xref="alg1.l22.m1.1.1.3.2.1.2.3.1.cmml">=</mo><mn id="alg1.l22.m1.1.1.3.2.1.2.3.3" xref="alg1.l22.m1.1.1.3.2.1.2.3.3.cmml">1</mn></mrow><mi id="alg1.l22.m1.1.1.3.2.1.3" xref="alg1.l22.m1.1.1.3.2.1.3.cmml">K</mi></msubsup></mstyle><msubsup id="alg1.l22.m1.1.1.3.2.2" xref="alg1.l22.m1.1.1.3.2.2.cmml"><mi id="alg1.l22.m1.1.1.3.2.2.2.2" xref="alg1.l22.m1.1.1.3.2.2.2.2.cmml">w</mi><mrow id="alg1.l22.m1.1.1.3.2.2.3" xref="alg1.l22.m1.1.1.3.2.2.3.cmml"><mi id="alg1.l22.m1.1.1.3.2.2.3.2" xref="alg1.l22.m1.1.1.3.2.2.3.2.cmml">t</mi><mo id="alg1.l22.m1.1.1.3.2.2.3.1" xref="alg1.l22.m1.1.1.3.2.2.3.1.cmml">+</mo><mn id="alg1.l22.m1.1.1.3.2.2.3.3" xref="alg1.l22.m1.1.1.3.2.2.3.3.cmml">1</mn></mrow><mi id="alg1.l22.m1.1.1.3.2.2.2.3" xref="alg1.l22.m1.1.1.3.2.2.2.3.cmml">k</mi></msubsup></mrow><mi id="alg1.l22.m1.1.1.3.3" xref="alg1.l22.m1.1.1.3.3.cmml">K</mi></mfrac></mrow><annotation-xml encoding="MathML-Content" id="alg1.l22.m1.1b"><apply id="alg1.l22.m1.1.1.cmml" xref="alg1.l22.m1.1.1"><ci id="alg1.l22.m1.1.1.1.cmml" xref="alg1.l22.m1.1.1.1">←</ci><apply id="alg1.l22.m1.1.1.2.cmml" xref="alg1.l22.m1.1.1.2"><csymbol cd="ambiguous" id="alg1.l22.m1.1.1.2.1.cmml" xref="alg1.l22.m1.1.1.2">subscript</csymbol><ci id="alg1.l22.m1.1.1.2.2.cmml" xref="alg1.l22.m1.1.1.2.2">𝑤</ci><apply id="alg1.l22.m1.1.1.2.3.cmml" xref="alg1.l22.m1.1.1.2.3"><plus id="alg1.l22.m1.1.1.2.3.1.cmml" xref="alg1.l22.m1.1.1.2.3.1"></plus><ci id="alg1.l22.m1.1.1.2.3.2.cmml" xref="alg1.l22.m1.1.1.2.3.2">𝑡</ci><cn type="integer" id="alg1.l22.m1.1.1.2.3.3.cmml" xref="alg1.l22.m1.1.1.2.3.3">1</cn></apply></apply><apply id="alg1.l22.m1.1.1.3.cmml" xref="alg1.l22.m1.1.1.3"><divide id="alg1.l22.m1.1.1.3.1.cmml" xref="alg1.l22.m1.1.1.3"></divide><apply id="alg1.l22.m1.1.1.3.2.cmml" xref="alg1.l22.m1.1.1.3.2"><apply id="alg1.l22.m1.1.1.3.2.1.cmml" xref="alg1.l22.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="alg1.l22.m1.1.1.3.2.1.1.cmml" xref="alg1.l22.m1.1.1.3.2.1">superscript</csymbol><apply id="alg1.l22.m1.1.1.3.2.1.2.cmml" xref="alg1.l22.m1.1.1.3.2.1"><csymbol cd="ambiguous" id="alg1.l22.m1.1.1.3.2.1.2.1.cmml" xref="alg1.l22.m1.1.1.3.2.1">subscript</csymbol><sum id="alg1.l22.m1.1.1.3.2.1.2.2.cmml" xref="alg1.l22.m1.1.1.3.2.1.2.2"></sum><apply id="alg1.l22.m1.1.1.3.2.1.2.3.cmml" xref="alg1.l22.m1.1.1.3.2.1.2.3"><eq id="alg1.l22.m1.1.1.3.2.1.2.3.1.cmml" xref="alg1.l22.m1.1.1.3.2.1.2.3.1"></eq><ci id="alg1.l22.m1.1.1.3.2.1.2.3.2.cmml" xref="alg1.l22.m1.1.1.3.2.1.2.3.2">𝑘</ci><cn type="integer" id="alg1.l22.m1.1.1.3.2.1.2.3.3.cmml" xref="alg1.l22.m1.1.1.3.2.1.2.3.3">1</cn></apply></apply><ci id="alg1.l22.m1.1.1.3.2.1.3.cmml" xref="alg1.l22.m1.1.1.3.2.1.3">𝐾</ci></apply><apply id="alg1.l22.m1.1.1.3.2.2.cmml" xref="alg1.l22.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="alg1.l22.m1.1.1.3.2.2.1.cmml" xref="alg1.l22.m1.1.1.3.2.2">subscript</csymbol><apply id="alg1.l22.m1.1.1.3.2.2.2.cmml" xref="alg1.l22.m1.1.1.3.2.2"><csymbol cd="ambiguous" id="alg1.l22.m1.1.1.3.2.2.2.1.cmml" xref="alg1.l22.m1.1.1.3.2.2">superscript</csymbol><ci id="alg1.l22.m1.1.1.3.2.2.2.2.cmml" xref="alg1.l22.m1.1.1.3.2.2.2.2">𝑤</ci><ci id="alg1.l22.m1.1.1.3.2.2.2.3.cmml" xref="alg1.l22.m1.1.1.3.2.2.2.3">𝑘</ci></apply><apply id="alg1.l22.m1.1.1.3.2.2.3.cmml" xref="alg1.l22.m1.1.1.3.2.2.3"><plus id="alg1.l22.m1.1.1.3.2.2.3.1.cmml" xref="alg1.l22.m1.1.1.3.2.2.3.1"></plus><ci id="alg1.l22.m1.1.1.3.2.2.3.2.cmml" xref="alg1.l22.m1.1.1.3.2.2.3.2">𝑡</ci><cn type="integer" id="alg1.l22.m1.1.1.3.2.2.3.3.cmml" xref="alg1.l22.m1.1.1.3.2.2.3.3">1</cn></apply></apply></apply><ci id="alg1.l22.m1.1.1.3.3.cmml" xref="alg1.l22.m1.1.1.3.3">𝐾</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="alg1.l22.m1.1c">w_{t+1}\leftarrow\frac{\sum_{k=1}^{K}w^{k}_{t+1}}{K}</annotation></semantics></math>

</div>
<div id="alg1.l23" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l23.1.1.1" class="ltx_text" style="font-size:80%;">23:</span></span>     <span id="alg1.l23.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l23.3" class="ltx_text ltx_font_bold">function</span>
</div>
<div id="alg1.l24" class="ltx_listingline">
<span class="ltx_tag ltx_tag_listingline"><span id="alg1.l24.1.1.1" class="ltx_text" style="font-size:80%;">24:</span></span><span id="alg1.l24.2" class="ltx_text ltx_font_bold">end</span> <span id="alg1.l24.3" class="ltx_text ltx_font_bold">if</span>
</div>
</div>
</figure>
<div id="S5.SS1.p6" class="ltx_para">
<p id="S5.SS1.p6.2" class="ltx_p">We now discuss the very first FL algorithm, FedAvg, proposed by
McMahan et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Algorithm <a href="#alg1" title="Algorithm 1 ‣ 5.1 Introduction to FL ‣ 5 Federated Learning (FL) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> describes FedAvg.
It shows the action taken by the server and clients during a round of FL. The clients train the model with their data. Once trained, the weights are sent to the server as described by the <math id="S5.SS1.p6.1.m1.1" class="ltx_Math" alttext="UpdateClientWeight" display="inline"><semantics id="S5.SS1.p6.1.m1.1a"><mrow id="S5.SS1.p6.1.m1.1.1" xref="S5.SS1.p6.1.m1.1.1.cmml"><mi id="S5.SS1.p6.1.m1.1.1.2" xref="S5.SS1.p6.1.m1.1.1.2.cmml">U</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.3" xref="S5.SS1.p6.1.m1.1.1.3.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1a" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.4" xref="S5.SS1.p6.1.m1.1.1.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1b" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.5" xref="S5.SS1.p6.1.m1.1.1.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1c" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.6" xref="S5.SS1.p6.1.m1.1.1.6.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1d" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.7" xref="S5.SS1.p6.1.m1.1.1.7.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1e" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.8" xref="S5.SS1.p6.1.m1.1.1.8.cmml">C</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1f" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.9" xref="S5.SS1.p6.1.m1.1.1.9.cmml">l</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1g" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.10" xref="S5.SS1.p6.1.m1.1.1.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1h" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.11" xref="S5.SS1.p6.1.m1.1.1.11.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1i" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.12" xref="S5.SS1.p6.1.m1.1.1.12.cmml">n</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1j" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.13" xref="S5.SS1.p6.1.m1.1.1.13.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1k" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.14" xref="S5.SS1.p6.1.m1.1.1.14.cmml">W</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1l" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.15" xref="S5.SS1.p6.1.m1.1.1.15.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1m" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.16" xref="S5.SS1.p6.1.m1.1.1.16.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1n" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.17" xref="S5.SS1.p6.1.m1.1.1.17.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1o" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.18" xref="S5.SS1.p6.1.m1.1.1.18.cmml">h</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.1.m1.1.1.1p" xref="S5.SS1.p6.1.m1.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.1.m1.1.1.19" xref="S5.SS1.p6.1.m1.1.1.19.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.1.m1.1b"><apply id="S5.SS1.p6.1.m1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1"><times id="S5.SS1.p6.1.m1.1.1.1.cmml" xref="S5.SS1.p6.1.m1.1.1.1"></times><ci id="S5.SS1.p6.1.m1.1.1.2.cmml" xref="S5.SS1.p6.1.m1.1.1.2">𝑈</ci><ci id="S5.SS1.p6.1.m1.1.1.3.cmml" xref="S5.SS1.p6.1.m1.1.1.3">𝑝</ci><ci id="S5.SS1.p6.1.m1.1.1.4.cmml" xref="S5.SS1.p6.1.m1.1.1.4">𝑑</ci><ci id="S5.SS1.p6.1.m1.1.1.5.cmml" xref="S5.SS1.p6.1.m1.1.1.5">𝑎</ci><ci id="S5.SS1.p6.1.m1.1.1.6.cmml" xref="S5.SS1.p6.1.m1.1.1.6">𝑡</ci><ci id="S5.SS1.p6.1.m1.1.1.7.cmml" xref="S5.SS1.p6.1.m1.1.1.7">𝑒</ci><ci id="S5.SS1.p6.1.m1.1.1.8.cmml" xref="S5.SS1.p6.1.m1.1.1.8">𝐶</ci><ci id="S5.SS1.p6.1.m1.1.1.9.cmml" xref="S5.SS1.p6.1.m1.1.1.9">𝑙</ci><ci id="S5.SS1.p6.1.m1.1.1.10.cmml" xref="S5.SS1.p6.1.m1.1.1.10">𝑖</ci><ci id="S5.SS1.p6.1.m1.1.1.11.cmml" xref="S5.SS1.p6.1.m1.1.1.11">𝑒</ci><ci id="S5.SS1.p6.1.m1.1.1.12.cmml" xref="S5.SS1.p6.1.m1.1.1.12">𝑛</ci><ci id="S5.SS1.p6.1.m1.1.1.13.cmml" xref="S5.SS1.p6.1.m1.1.1.13">𝑡</ci><ci id="S5.SS1.p6.1.m1.1.1.14.cmml" xref="S5.SS1.p6.1.m1.1.1.14">𝑊</ci><ci id="S5.SS1.p6.1.m1.1.1.15.cmml" xref="S5.SS1.p6.1.m1.1.1.15">𝑒</ci><ci id="S5.SS1.p6.1.m1.1.1.16.cmml" xref="S5.SS1.p6.1.m1.1.1.16">𝑖</ci><ci id="S5.SS1.p6.1.m1.1.1.17.cmml" xref="S5.SS1.p6.1.m1.1.1.17">𝑔</ci><ci id="S5.SS1.p6.1.m1.1.1.18.cmml" xref="S5.SS1.p6.1.m1.1.1.18">ℎ</ci><ci id="S5.SS1.p6.1.m1.1.1.19.cmml" xref="S5.SS1.p6.1.m1.1.1.19">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.1.m1.1c">UpdateClientWeight</annotation></semantics></math> function on line <a href="#alg1.l7" title="In Algorithm 1 ‣ 5.1 Introduction to FL ‣ 5 Federated Learning (FL) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>. Once the server receives the weights from the clients, which is done in parallel, it averages out all the weights and sends the average weights back to each clients, as seen in the <math id="S5.SS1.p6.2.m2.1" class="ltx_Math" alttext="ServerUpdateWeight" display="inline"><semantics id="S5.SS1.p6.2.m2.1a"><mrow id="S5.SS1.p6.2.m2.1.1" xref="S5.SS1.p6.2.m2.1.1.cmml"><mi id="S5.SS1.p6.2.m2.1.1.2" xref="S5.SS1.p6.2.m2.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.3" xref="S5.SS1.p6.2.m2.1.1.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1a" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.4" xref="S5.SS1.p6.2.m2.1.1.4.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1b" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.5" xref="S5.SS1.p6.2.m2.1.1.5.cmml">v</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1c" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.6" xref="S5.SS1.p6.2.m2.1.1.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1d" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.7" xref="S5.SS1.p6.2.m2.1.1.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1e" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.8" xref="S5.SS1.p6.2.m2.1.1.8.cmml">U</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1f" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.9" xref="S5.SS1.p6.2.m2.1.1.9.cmml">p</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1g" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.10" xref="S5.SS1.p6.2.m2.1.1.10.cmml">d</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1h" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.11" xref="S5.SS1.p6.2.m2.1.1.11.cmml">a</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1i" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.12" xref="S5.SS1.p6.2.m2.1.1.12.cmml">t</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1j" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.13" xref="S5.SS1.p6.2.m2.1.1.13.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1k" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.14" xref="S5.SS1.p6.2.m2.1.1.14.cmml">W</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1l" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.15" xref="S5.SS1.p6.2.m2.1.1.15.cmml">e</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1m" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.16" xref="S5.SS1.p6.2.m2.1.1.16.cmml">i</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1n" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.17" xref="S5.SS1.p6.2.m2.1.1.17.cmml">g</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1o" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.18" xref="S5.SS1.p6.2.m2.1.1.18.cmml">h</mi><mo lspace="0em" rspace="0em" id="S5.SS1.p6.2.m2.1.1.1p" xref="S5.SS1.p6.2.m2.1.1.1.cmml">​</mo><mi id="S5.SS1.p6.2.m2.1.1.19" xref="S5.SS1.p6.2.m2.1.1.19.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p6.2.m2.1b"><apply id="S5.SS1.p6.2.m2.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1"><times id="S5.SS1.p6.2.m2.1.1.1.cmml" xref="S5.SS1.p6.2.m2.1.1.1"></times><ci id="S5.SS1.p6.2.m2.1.1.2.cmml" xref="S5.SS1.p6.2.m2.1.1.2">𝑆</ci><ci id="S5.SS1.p6.2.m2.1.1.3.cmml" xref="S5.SS1.p6.2.m2.1.1.3">𝑒</ci><ci id="S5.SS1.p6.2.m2.1.1.4.cmml" xref="S5.SS1.p6.2.m2.1.1.4">𝑟</ci><ci id="S5.SS1.p6.2.m2.1.1.5.cmml" xref="S5.SS1.p6.2.m2.1.1.5">𝑣</ci><ci id="S5.SS1.p6.2.m2.1.1.6.cmml" xref="S5.SS1.p6.2.m2.1.1.6">𝑒</ci><ci id="S5.SS1.p6.2.m2.1.1.7.cmml" xref="S5.SS1.p6.2.m2.1.1.7">𝑟</ci><ci id="S5.SS1.p6.2.m2.1.1.8.cmml" xref="S5.SS1.p6.2.m2.1.1.8">𝑈</ci><ci id="S5.SS1.p6.2.m2.1.1.9.cmml" xref="S5.SS1.p6.2.m2.1.1.9">𝑝</ci><ci id="S5.SS1.p6.2.m2.1.1.10.cmml" xref="S5.SS1.p6.2.m2.1.1.10">𝑑</ci><ci id="S5.SS1.p6.2.m2.1.1.11.cmml" xref="S5.SS1.p6.2.m2.1.1.11">𝑎</ci><ci id="S5.SS1.p6.2.m2.1.1.12.cmml" xref="S5.SS1.p6.2.m2.1.1.12">𝑡</ci><ci id="S5.SS1.p6.2.m2.1.1.13.cmml" xref="S5.SS1.p6.2.m2.1.1.13">𝑒</ci><ci id="S5.SS1.p6.2.m2.1.1.14.cmml" xref="S5.SS1.p6.2.m2.1.1.14">𝑊</ci><ci id="S5.SS1.p6.2.m2.1.1.15.cmml" xref="S5.SS1.p6.2.m2.1.1.15">𝑒</ci><ci id="S5.SS1.p6.2.m2.1.1.16.cmml" xref="S5.SS1.p6.2.m2.1.1.16">𝑖</ci><ci id="S5.SS1.p6.2.m2.1.1.17.cmml" xref="S5.SS1.p6.2.m2.1.1.17">𝑔</ci><ci id="S5.SS1.p6.2.m2.1.1.18.cmml" xref="S5.SS1.p6.2.m2.1.1.18">ℎ</ci><ci id="S5.SS1.p6.2.m2.1.1.19.cmml" xref="S5.SS1.p6.2.m2.1.1.19">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p6.2.m2.1c">ServerUpdateWeight</annotation></semantics></math> function on line <a href="#alg1.l16" title="In Algorithm 1 ‣ 5.1 Introduction to FL ‣ 5 Federated Learning (FL) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a>. Training is repeated if the data changes. This is to keep the weights updated.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>FL Applications</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">FL has a wide range of applications across different domains and settings. Some of them are:</p>
<ul id="S5.I2" class="ltx_itemize">
<li id="S5.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i1.p1" class="ltx_para">
<p id="S5.I2.i1.p1.1" class="ltx_p">Smartphones: FL has been used to develop ML applications for smartphones such as next-word prediction, and face and voice recognition.</p>
</div>
</li>
<li id="S5.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i2.p1" class="ltx_para">
<p id="S5.I2.i2.p1.1" class="ltx_p">Healthcare: FL has been applied successfully for research problems in medical studies such as drug discovery and brain tumor segmentation.</p>
</div>
</li>
<li id="S5.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i3.p1" class="ltx_para">
<p id="S5.I2.i3.p1.1" class="ltx_p">The internet of things (IoT): IoT is a network of digital or mechanical computing objects that have sensors, software, and other computing technologies. IoT exchanges data with other devices and systems over the internet to perform specific learning tasks. Applications of FL in IoT include autonomous driving and intrusion and anomaly detection.</p>
</div>
</li>
<li id="S5.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S5.I2.i4.p1" class="ltx_para">
<p id="S5.I2.i4.p1.1" class="ltx_p">Finance: FL has been adopted to detect/identify financial crimes such as fraudulent loans and money laundering.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>FL Aggregation Algorithms</h3>

<div id="S5.SS3.p1" class="ltx_para">
<p id="S5.SS3.p1.1" class="ltx_p">In FL, due to data parallelism and horizontal FL, aggregation algorithms are needed to aggregate the models or gradients between the participants.
As stated above,
the very first aggregation algorithm, called Federated Averaging (FedAvg), was introduced by McMahan et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> who essentially kick-started FL itself. FedAvg computes the global model parameters by averaging the parameter updates of the participants. Once the global parameters are computed and updated, these parameters are communicated back to the participants. FedAvg is a straightforward algorithm however, it is biased toward the participants who have favorable network conditions.</p>
</div>
<div id="S5.SS3.p2" class="ltx_para">
<p id="S5.SS3.p2.1" class="ltx_p">Aggregation algorithms have been studied extensively for centralized topologies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>, <a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite>.
To decrease the communication overheads, Liu et. al. propose the Federated Stochastic Block Coordinate Descent (FedBCD) algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib132" title="" class="ltx_ref">132</a>]</cite> in which each participant makes multiple local updates before synchronizing with other participants.
Differently, FedOpt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib133" title="" class="ltx_ref">133</a>]</cite> uses gradient compression to reduce communication overhead while sacrificing accuracy.
Furthermore, for edge devices where computational resources are limited, algorithms such as FedGKT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib134" title="" class="ltx_ref">134</a>]</cite> are developed.</p>
</div>
<div id="S5.SS3.p3" class="ltx_para">
<p id="S5.SS3.p3.1" class="ltx_p">A significant objective in FL is to provide fairness.
Fairness means that the clients equally contribute to the global model with respect to certain metrics. Researchers have proposed algorithms such as Stochastic Agnostic Federated Learning (SAFL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib135" title="" class="ltx_ref">135</a>]</cite> and FedMGDA+ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib136" title="" class="ltx_ref">136</a>]</cite> to achieve fairness.</p>
</div>
<div id="S5.SS3.p4" class="ltx_para">
<p id="S5.SS3.p4.1" class="ltx_p">Adaptive FL and its impact on convergence and accuracy have been explored in various recent works.
ADAGRAD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib137" title="" class="ltx_ref">137</a>]</cite> offers an adaptive approach to ML optimization compared to FedAvg. ADAGRAD and its variants dynamically choose server and client learning rates and momentum parameters during training. Mime Lite <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib138" title="" class="ltx_ref">138</a>]</cite> is a closely related study where adaptive learning rates and momenta are reported to improve accuracy.</p>
</div>
<div id="S5.SS3.p5" class="ltx_para">
<p id="S5.SS3.p5.1" class="ltx_p">Some recent aggregation algorithms support heterogeneity of participant data. FedProx <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib139" title="" class="ltx_ref">139</a>]</cite> is such an algorithm used for FL over heterogeneous data and resources. SCAFFOLD <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib140" title="" class="ltx_ref">140</a>]</cite> is another algorithm that accounts for heterogeneous data while reducing the number of rounds to converge.
FedAtt <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib141" title="" class="ltx_ref">141</a>]</cite> accounts for the client contributions by attending to the importance of their model updates. The attention is quantified by the similarity between the server model and the client model in a layer-wise manner.
FedNova <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib142" title="" class="ltx_ref">142</a>]</cite> proposes a normalized averaging method as a way to avoid objective inconsistencies and to achieve fast convergence for highly heterogeneous clients.</p>
</div>
<div id="S5.SS3.p6" class="ltx_para">
<p id="S5.SS3.p6.1" class="ltx_p">Personalization is another important consideration in FL.
There has been extensive research on personalized FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib143" title="" class="ltx_ref">143</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib144" title="" class="ltx_ref">144</a>]</cite>.
Tan et. al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib145" title="" class="ltx_ref">145</a>]</cite> offer a survey of the latest personalization techniques.</p>
</div>
<div id="S5.SS3.p7" class="ltx_para">
<p id="S5.SS3.p7.1" class="ltx_p">When the topology of the clients in FL is hierarchical, an aggregation algorithm needs to take the hierarchy into account. Numerous hierarchical aggregation algorithms have been proposed for such settings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>, <a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite>.
Among hierarchical solutions, SPAHM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib146" title="" class="ltx_ref">146</a>]</cite> and PFNM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib147" title="" class="ltx_ref">147</a>]</cite> are Bayesian FL methods.
Similarly, for decentralized topologies, decentralized algorithms have been developed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib148" title="" class="ltx_ref">148</a>, <a href="#bib.bib149" title="" class="ltx_ref">149</a>]</cite>.</p>
</div>
<div id="S5.SS3.p8" class="ltx_para">
<p id="S5.SS3.p8.1" class="ltx_p">Considering fault-tolerance in FL, Krum <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib150" title="" class="ltx_ref">150</a>]</cite> is an aggregation scheme that is reportedly resilient to Byzantine failures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib151" title="" class="ltx_ref">151</a>]</cite> where computing processes fail arbitrarily and failure symptoms are different for different observers. For these types of failures, more fault-tolerant FL studies are needed.</p>
</div>
</section>
<section id="S5.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Security and Privacy in FL</h3>

<figure id="S5.T6" class="ltx_table">
<table id="S5.T6.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T6.1.1.1" class="ltx_tr">
<td id="S5.T6.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Defense Type</td>
<td id="S5.T6.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Addressed Attacks</td>
<td id="S5.T6.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Potential Negative Effects</td>
</tr>
<tr id="S5.T6.1.2.2" class="ltx_tr">
<td id="S5.T6.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T6.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.2.2.1.1.1" class="ltx_tr">
<td id="S5.T6.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Differential privacy</td>
</tr>
<tr id="S5.T6.1.2.2.1.1.2" class="ltx_tr">
<td id="S5.T6.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite></td>
</tr>
</table>
</td>
<td id="S5.T6.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T6.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.2.2.2.1.1" class="ltx_tr">
<td id="S5.T6.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Data Poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib157" title="" class="ltx_ref">157</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.2.2.2.1.2" class="ltx_tr">
<td id="S5.T6.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Model Poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.2.2.2.1.3" class="ltx_tr">
<td id="S5.T6.1.2.2.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T6.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Decreased model utility</td>
</tr>
<tr id="S5.T6.1.3.3" class="ltx_tr">
<td id="S5.T6.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T6.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.3.3.1.1.1" class="ltx_tr">
<td id="S5.T6.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Homomorphic encryption</td>
</tr>
<tr id="S5.T6.1.3.3.1.1.2" class="ltx_tr">
<td id="S5.T6.1.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite></td>
</tr>
</table>
</td>
<td id="S5.T6.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T6.1.3.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.3.3.2.1.1" class="ltx_tr">
<td id="S5.T6.1.3.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.3.3.2.1.2" class="ltx_tr">
<td id="S5.T6.1.3.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">GAN-based attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T6.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High computational costs</td>
</tr>
<tr id="S5.T6.1.4.4" class="ltx_tr">
<td id="S5.T6.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T6.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.4.4.1.1.1" class="ltx_tr">
<td id="S5.T6.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Trusted execution environments</td>
</tr>
<tr id="S5.T6.1.4.4.1.1.2" class="ltx_tr">
<td id="S5.T6.1.4.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite></td>
</tr>
</table>
</td>
<td id="S5.T6.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T6.1.4.4.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.4.4.2.1.1" class="ltx_tr">
<td id="S5.T6.1.4.4.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.4.4.2.1.2" class="ltx_tr">
<td id="S5.T6.1.4.4.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Model Poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T6.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Specialized hardware</td>
</tr>
<tr id="S5.T6.1.5.5" class="ltx_tr">
<td id="S5.T6.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T6.1.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.5.5.1.1.1" class="ltx_tr">
<td id="S5.T6.1.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Secure Multi-party Computation</td>
</tr>
<tr id="S5.T6.1.5.5.1.1.2" class="ltx_tr">
<td id="S5.T6.1.5.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite></td>
</tr>
</table>
</td>
<td id="S5.T6.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T6.1.5.5.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.5.5.2.1.1" class="ltx_tr">
<td id="S5.T6.1.5.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">GAN-based attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.5.5.2.1.2" class="ltx_tr">
<td id="S5.T6.1.5.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.5.5.2.1.3" class="ltx_tr">
<td id="S5.T6.1.5.5.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Eavesdropping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T6.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High computational costs</td>
</tr>
<tr id="S5.T6.1.6.6" class="ltx_tr">
<td id="S5.T6.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>
</td>
<td id="S5.T6.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Blockchain attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib173" title="" class="ltx_ref">173</a>, <a href="#bib.bib174" title="" class="ltx_ref">174</a>]</cite>
</td>
<td id="S5.T6.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High resource costs</td>
</tr>
<tr id="S5.T6.1.7.7" class="ltx_tr">
<td id="S5.T6.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Data anonymization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>
</td>
<td id="S5.T6.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T6.1.7.7.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.7.7.2.1.1" class="ltx_tr">
<td id="S5.T6.1.7.7.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">GAN-based attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.7.7.2.1.2" class="ltx_tr">
<td id="S5.T6.1.7.7.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T6.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Decreased data usability</td>
</tr>
<tr id="S5.T6.1.8.8" class="ltx_tr">
<td id="S5.T6.1.8.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Anomaly detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite>
</td>
<td id="S5.T6.1.8.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S5.T6.1.8.8.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T6.1.8.8.2.1.1" class="ltx_tr">
<td id="S5.T6.1.8.8.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Data Poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib157" title="" class="ltx_ref">157</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.8.8.2.1.2" class="ltx_tr">
<td id="S5.T6.1.8.8.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Model Poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite>
</td>
</tr>
<tr id="S5.T6.1.8.8.2.1.3" class="ltx_tr">
<td id="S5.T6.1.8.8.2.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Free-riders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>
</td>
</tr>
</table>
</td>
<td id="S5.T6.1.8.8.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Detection latency</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Attacks and defenses in FL.</figcaption>
</figure>
<div id="S5.SS4.p1" class="ltx_para">
<p id="S5.SS4.p1.1" class="ltx_p">The security of FL entails ensuring the triad of confidentiality, integrity and availability of its data and models, and particularly, data privacy. Privacy is defined as the protection of the raw data against the information leakage. In this section, we first summarize the attack types and then the defensive actions and methods existing in the FL literature <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>.</p>
</div>
<section id="S5.SS4.SSS1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1 </span>Attacks</h4>

<div id="S5.SS4.SSS1.p1" class="ltx_para">
<p id="S5.SS4.SSS1.p1.1" class="ltx_p">There are numerous attack types in FL.
Poisoning attacks aim to tamper with and/or alter the data or the model. Data poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib156" title="" class="ltx_ref">156</a>, <a href="#bib.bib157" title="" class="ltx_ref">157</a>, <a href="#bib.bib158" title="" class="ltx_ref">158</a>]</cite> refers to altering the features in the training data or generating false data to degrade the performance of a model on the unseen data. Model poisoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib159" title="" class="ltx_ref">159</a>, <a href="#bib.bib160" title="" class="ltx_ref">160</a>, <a href="#bib.bib161" title="" class="ltx_ref">161</a>]</cite> refers to the modification of the model parameters and/or the fabrication of false weights that are communicated between the participants and the servers.</p>
</div>
<div id="S5.SS4.SSS1.p2" class="ltx_para">
<p id="S5.SS4.SSS1.p2.1" class="ltx_p">Backdoor attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib177" title="" class="ltx_ref">177</a>, <a href="#bib.bib178" title="" class="ltx_ref">178</a>]</cite> inject malicious instructions into the models while not impacting their expected performance. These attacks are non-transparent and notoriously difficult to detect.</p>
</div>
<div id="S5.SS4.SSS1.p3" class="ltx_para">
<p id="S5.SS4.SSS1.p3.1" class="ltx_p">Inference attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib162" title="" class="ltx_ref">162</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite> involve gaining knowledge of the sensitive information of the participants, the training data or the model through the communications occurring during training or inference. Membership inference attacks aim to learn if a sample has been used as a training instance. Property inference attacks aim to learn the meta-characteristics of the training data. Class representative inference attacks aim to learn representative samples of a target class.</p>
</div>
<div id="S5.SS4.SSS1.p4" class="ltx_para">
<p id="S5.SS4.SSS1.p4.1" class="ltx_p">Generative Adversarial Networks (GANs) based attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib166" title="" class="ltx_ref">166</a>, <a href="#bib.bib167" title="" class="ltx_ref">167</a>, <a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite> are used to launch poisoning attacks where GANs generate the altered or false data and/or model parameters.</p>
</div>
<div id="S5.SS4.SSS1.p5" class="ltx_para">
<p id="S5.SS4.SSS1.p5.1" class="ltx_p">There are many other attack types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>, such free-riders <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite> and Eavesdropping <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>, <a href="#bib.bib163" title="" class="ltx_ref">163</a>]</cite>.</p>
</div>
</section>
<section id="S5.SS4.SSS2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2 </span>Defenses</h4>

<div id="S5.SS4.SSS2.p1" class="ltx_para">
<p id="S5.SS4.SSS2.p1.1" class="ltx_p">The most commonly used attack defense mechanisms can be categorized by the usage of trusted execution environments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>, homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>, differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>, and possibly some combinations of them.
There are many other techniques which are based on GANs <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib181" title="" class="ltx_ref">181</a>]</cite>, anomaly detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite>, secure multi-party computation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite>, data anonymization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>, and blockchains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>.</p>
</div>
<div id="S5.SS4.SSS2.p2" class="ltx_para">
<p id="S5.SS4.SSS2.p2.1" class="ltx_p">A trusted execution environment <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite> is an (hardware/software) architecture where the program execution is secured and information leakage is not possible. Such architectures use specialized designs to prevent unauthorized accesses as well as privacy violations in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite>.</p>
</div>
<div id="S5.SS4.SSS2.p3" class="ltx_para">
<p id="S5.SS4.SSS2.p3.1" class="ltx_p">Homomorphic encryption <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite> is a certain type of encryption in which the decryption of the results of the computations performed on the encrypted data is the same as the result of the same computations performed on the unencrypted data. Homomorphic encryption has various levels depending on the whether addition and/or multiplication is supported. It has been adapted for data privacy in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib164" title="" class="ltx_ref">164</a>, <a href="#bib.bib165" title="" class="ltx_ref">165</a>]</cite>.</p>
</div>
<div id="S5.SS4.SSS2.p4" class="ltx_para">
<p id="S5.SS4.SSS2.p4.1" class="ltx_p">Differential privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>]</cite> is a technique for achieving data privacy by adding noise to raw data. It is commonly used in FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite>.</p>
</div>
<div id="S5.SS4.SSS2.p5" class="ltx_para">
<p id="S5.SS4.SSS2.p5.1" class="ltx_p">Table <a href="#S5.T6" title="Table 6 ‣ 5.4 Security and Privacy in FL ‣ 5 Federated Learning (FL) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reviews attacks and defenses in FL. In addition, Table <a href="#S5.T7" title="Table 7 ‣ 5.4.2 Defenses ‣ 5.4 Security and Privacy in FL ‣ 5 Federated Learning (FL) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> compares the defense mechanisms in terms of the strength of the protection, the computational and communication efficiency, robustness, scalability, and generalizability of a mechanism.</p>
</div>
<figure id="S5.T7" class="ltx_table">
<table id="S5.T7.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S5.T7.1.1.1" class="ltx_tr">
<td id="S5.T7.1.1.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Defense Type</td>
<td id="S5.T7.1.1.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Protection</td>
<td id="S5.T7.1.1.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Efficiency</td>
<td id="S5.T7.1.1.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Robustness</td>
<td id="S5.T7.1.1.1.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Scalability</td>
<td id="S5.T7.1.1.1.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Generalizability</td>
</tr>
<tr id="S5.T7.1.2.2" class="ltx_tr">
<td id="S5.T7.1.2.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T7.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.2.2.1.1.1" class="ltx_tr">
<td id="S5.T7.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Differential privacy</td>
</tr>
<tr id="S5.T7.1.2.2.1.1.2" class="ltx_tr">
<td id="S5.T7.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>, <a href="#bib.bib152" title="" class="ltx_ref">152</a>, <a href="#bib.bib153" title="" class="ltx_ref">153</a>, <a href="#bib.bib154" title="" class="ltx_ref">154</a>, <a href="#bib.bib155" title="" class="ltx_ref">155</a>]</cite></td>
</tr>
</table>
</td>
<td id="S5.T7.1.2.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.2.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.2.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.2.2.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.2.2.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
</tr>
<tr id="S5.T7.1.3.3" class="ltx_tr">
<td id="S5.T7.1.3.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T7.1.3.3.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.3.3.1.1.1" class="ltx_tr">
<td id="S5.T7.1.3.3.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Homomorphic encryption</td>
</tr>
<tr id="S5.T7.1.3.3.1.1.2" class="ltx_tr">
<td id="S5.T7.1.3.3.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib182" title="" class="ltx_ref">182</a>]</cite></td>
</tr>
</table>
</td>
<td id="S5.T7.1.3.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.3.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Low</td>
<td id="S5.T7.1.3.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.3.3.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Low</td>
<td id="S5.T7.1.3.3.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
</tr>
<tr id="S5.T7.1.4.4" class="ltx_tr">
<td id="S5.T7.1.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T7.1.4.4.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.4.4.1.1.1" class="ltx_tr">
<td id="S5.T7.1.4.4.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Trusted execution environments</td>
</tr>
<tr id="S5.T7.1.4.4.1.1.2" class="ltx_tr">
<td id="S5.T7.1.4.4.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib169" title="" class="ltx_ref">169</a>, <a href="#bib.bib170" title="" class="ltx_ref">170</a>]</cite></td>
</tr>
</table>
</td>
<td id="S5.T7.1.4.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Medium</td>
<td id="S5.T7.1.4.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.4.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Medium</td>
<td id="S5.T7.1.4.4.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Low</td>
<td id="S5.T7.1.4.4.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
</tr>
<tr id="S5.T7.1.5.5" class="ltx_tr">
<td id="S5.T7.1.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T7.1.5.5.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T7.1.5.5.1.1.1" class="ltx_tr">
<td id="S5.T7.1.5.5.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Secure multi-party computations</td>
</tr>
<tr id="S5.T7.1.5.5.1.1.2" class="ltx_tr">
<td id="S5.T7.1.5.5.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left"><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib171" title="" class="ltx_ref">171</a>]</cite></td>
</tr>
</table>
</td>
<td id="S5.T7.1.5.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.5.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Medium</td>
<td id="S5.T7.1.5.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.5.5.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Low</td>
<td id="S5.T7.1.5.5.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Medium</td>
</tr>
<tr id="S5.T7.1.6.6" class="ltx_tr">
<td id="S5.T7.1.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Blockchain <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib172" title="" class="ltx_ref">172</a>]</cite>
</td>
<td id="S5.T7.1.6.6.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.6.6.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Low</td>
<td id="S5.T7.1.6.6.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Medium</td>
<td id="S5.T7.1.6.6.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.6.6.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Medium</td>
</tr>
<tr id="S5.T7.1.7.7" class="ltx_tr">
<td id="S5.T7.1.7.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">Data anonymization <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib175" title="" class="ltx_ref">175</a>]</cite>
</td>
<td id="S5.T7.1.7.7.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Medium</td>
<td id="S5.T7.1.7.7.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Medium</td>
<td id="S5.T7.1.7.7.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Low</td>
<td id="S5.T7.1.7.7.5" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.7.7.6" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">High</td>
</tr>
<tr id="S5.T7.1.8.8" class="ltx_tr">
<td id="S5.T7.1.8.8.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Anomaly detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib176" title="" class="ltx_ref">176</a>]</cite>
</td>
<td id="S5.T7.1.8.8.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Medium</td>
<td id="S5.T7.1.8.8.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.8.8.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Medium</td>
<td id="S5.T7.1.8.8.5" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">High</td>
<td id="S5.T7.1.8.8.6" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Low</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparison of the defense mechanisms in terms of the strength of the protection, the computational and communication efficiency, robustness, scalability, and generalizability of a mechanism.</figcaption>
</figure>
</section>
</section>
<section id="S5.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Existing FL Frameworks</h3>

<div id="S5.SS5.p1" class="ltx_para">
<p id="S5.SS5.p1.1" class="ltx_p">The most widely used FL frameworks are
TensorFlow Federated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>, <a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite>, IBM Federated Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite>, NVIDIA FLARE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite>, FedML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite>, Federated AI Technology Enabler (FATE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite>, PySyft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite>, and Open Federated Learning (OpenFL) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite>. Table <a href="#S5.T8" title="Table 8 ‣ 5.5 Existing FL Frameworks ‣ 5 Federated Learning (FL) ‣ The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> summarizes the existing FL frameworks.</p>
</div>
<figure id="S5.T8" class="ltx_table">
<table id="S5.T8.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T8.1.1.1" class="ltx_tr">
<th id="S5.T8.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Frameworks</th>
<th id="S5.T8.1.1.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Aggregation Algorithm</th>
<th id="S5.T8.1.1.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Parallelism</th>
<th id="S5.T8.1.1.1.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Privacy and Security</th>
</tr>
<tr id="S5.T8.1.2.2" class="ltx_tr">
<th id="S5.T8.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">
<table id="S5.T8.1.2.2.1.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.2.2.1.1.1" class="ltx_tr">
<td id="S5.T8.1.2.2.1.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">TensorFlow Federated</td>
</tr>
<tr id="S5.T8.1.2.2.1.1.2" class="ltx_tr">
<td id="S5.T8.1.2.2.1.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Keras Federated</td>
</tr>
</table>
</th>
<th id="S5.T8.1.2.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">
<table id="S5.T8.1.2.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.2.2.2.1.1" class="ltx_tr">
<td id="S5.T8.1.2.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">FedAvg, FedProx,</td>
</tr>
<tr id="S5.T8.1.2.2.2.1.2" class="ltx_tr">
<td id="S5.T8.1.2.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">FedSGD, Mime Lite</td>
</tr>
</table>
</th>
<th id="S5.T8.1.2.2.3" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Data, Model</th>
<th id="S5.T8.1.2.2.4" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t">Differential privacy</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T8.1.3.1" class="ltx_tr">
<td id="S5.T8.1.3.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">IBM Federated</td>
<td id="S5.T8.1.3.1.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.3.1.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.3.1.2.1.1" class="ltx_tr">
<td id="S5.T8.1.3.1.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">FedAvg, SPAHM,</td>
</tr>
<tr id="S5.T8.1.3.1.2.1.2" class="ltx_tr">
<td id="S5.T8.1.3.1.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">PFNM, Krum</td>
</tr>
</table>
</td>
<td id="S5.T8.1.3.1.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data, Model</td>
<td id="S5.T8.1.3.1.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.3.1.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.3.1.4.1.1" class="ltx_tr">
<td id="S5.T8.1.3.1.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Differential privacy</td>
</tr>
<tr id="S5.T8.1.3.1.4.1.2" class="ltx_tr">
<td id="S5.T8.1.3.1.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Secure multi-party computation</td>
</tr>
<tr id="S5.T8.1.3.1.4.1.3" class="ltx_tr">
<td id="S5.T8.1.3.1.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Homomorphic encryptions</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.4.2" class="ltx_tr">
<td id="S5.T8.1.4.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">NVIDIA FLARE</td>
<td id="S5.T8.1.4.2.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.4.2.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.4.2.2.1.1" class="ltx_tr">
<td id="S5.T8.1.4.2.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">FedAvg, FedProx,</td>
</tr>
<tr id="S5.T8.1.4.2.2.1.2" class="ltx_tr">
<td id="S5.T8.1.4.2.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">SCAFFOLD</td>
</tr>
</table>
</td>
<td id="S5.T8.1.4.2.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data</td>
<td id="S5.T8.1.4.2.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.4.2.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.4.2.4.1.1" class="ltx_tr">
<td id="S5.T8.1.4.2.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Differential privacy</td>
</tr>
<tr id="S5.T8.1.4.2.4.1.2" class="ltx_tr">
<td id="S5.T8.1.4.2.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Homomorphic encryption</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.5.3" class="ltx_tr">
<td id="S5.T8.1.5.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FedML</td>
<td id="S5.T8.1.5.3.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.5.3.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.5.3.2.1.1" class="ltx_tr">
<td id="S5.T8.1.5.3.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">FedAvg, FedOpt,</td>
</tr>
<tr id="S5.T8.1.5.3.2.1.2" class="ltx_tr">
<td id="S5.T8.1.5.3.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">FedNova, FedGKT</td>
</tr>
</table>
</td>
<td id="S5.T8.1.5.3.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data, Model</td>
<td id="S5.T8.1.5.3.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.5.3.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.5.3.4.1.1" class="ltx_tr">
<td id="S5.T8.1.5.3.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Differential privacy</td>
</tr>
<tr id="S5.T8.1.5.3.4.1.2" class="ltx_tr">
<td id="S5.T8.1.5.3.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Cryptography</td>
</tr>
<tr id="S5.T8.1.5.3.4.1.3" class="ltx_tr">
<td id="S5.T8.1.5.3.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Coding approaches</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.6.4" class="ltx_tr">
<td id="S5.T8.1.6.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">FATE</td>
<td id="S5.T8.1.6.4.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">FedAvg</td>
<td id="S5.T8.1.6.4.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data, Pipeline</td>
<td id="S5.T8.1.6.4.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.6.4.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.6.4.4.1.1" class="ltx_tr">
<td id="S5.T8.1.6.4.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Homomorphic encryption</td>
</tr>
<tr id="S5.T8.1.6.4.4.1.2" class="ltx_tr">
<td id="S5.T8.1.6.4.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">RSA</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.7.5" class="ltx_tr">
<td id="S5.T8.1.7.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">PySyft</td>
<td id="S5.T8.1.7.5.2" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.7.5.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.7.5.2.1.1" class="ltx_tr">
<td id="S5.T8.1.7.5.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">FedAvg, FedProx,</td>
</tr>
<tr id="S5.T8.1.7.5.2.1.2" class="ltx_tr">
<td id="S5.T8.1.7.5.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">FedSGD</td>
</tr>
</table>
</td>
<td id="S5.T8.1.7.5.3" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">Data, Model</td>
<td id="S5.T8.1.7.5.4" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">
<table id="S5.T8.1.7.5.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.7.5.4.1.1" class="ltx_tr">
<td id="S5.T8.1.7.5.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Differential privacy</td>
</tr>
<tr id="S5.T8.1.7.5.4.1.2" class="ltx_tr">
<td id="S5.T8.1.7.5.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">Homomorphic encryption</td>
</tr>
</table>
</td>
</tr>
<tr id="S5.T8.1.8.6" class="ltx_tr">
<td id="S5.T8.1.8.6.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t">OpenFL</td>
<td id="S5.T8.1.8.6.2" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S5.T8.1.8.6.2.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.8.6.2.1.1" class="ltx_tr">
<td id="S5.T8.1.8.6.2.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">FedAvg,</td>
</tr>
<tr id="S5.T8.1.8.6.2.1.2" class="ltx_tr">
<td id="S5.T8.1.8.6.2.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">FedADAGRAD</td>
</tr>
</table>
</td>
<td id="S5.T8.1.8.6.3" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">Data</td>
<td id="S5.T8.1.8.6.4" class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t">
<table id="S5.T8.1.8.6.4.1" class="ltx_tabular ltx_align_middle">
<tr id="S5.T8.1.8.6.4.1.1" class="ltx_tr">
<td id="S5.T8.1.8.6.4.1.1.1" class="ltx_td ltx_nopad_r ltx_align_left">Trusted execution environments</td>
</tr>
<tr id="S5.T8.1.8.6.4.1.2" class="ltx_tr">
<td id="S5.T8.1.8.6.4.1.2.1" class="ltx_td ltx_nopad_r ltx_align_left">RSA</td>
</tr>
<tr id="S5.T8.1.8.6.4.1.3" class="ltx_tr">
<td id="S5.T8.1.8.6.4.1.3.1" class="ltx_td ltx_nopad_r ltx_align_left">Differential privacy</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Existing FL platforms.</figcaption>
</figure>
<div id="S5.SS5.p2" class="ltx_para">
<p id="S5.SS5.p2.1" class="ltx_p">TensorFlow Federated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib183" title="" class="ltx_ref">183</a>]</cite> (and Keras Federated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib184" title="" class="ltx_ref">184</a>]</cite>) is an open-source framework for FL by Google. It enables researchers to simulate FL algorithms. FedAvg, FedProx, FedSGD, and Mime Lite are some of the FL aggregation algorithms that are readily available. TensorFlow Federated supports data and model parallelisms.
It provides differential privacy as a privacy measure. TensorFlow Federated has two main APIs. FL API offers built-in algorithms. FL Core API offers a set of lower-level functionalities for new algorithms to be implemented.</p>
</div>
<div id="S5.SS5.p3" class="ltx_para">
<p id="S5.SS5.p3.1" class="ltx_p">IBM Federated Learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib185" title="" class="ltx_ref">185</a>]</cite> provides support for FL and DL models written in Keras, PyTorch and TensorFlow.
FedAvg, SPAHM, PFNM, and Krum are among the available aggregation algorithms. IBM FL supports data and model parallelism. In addition,
differential privacy, secure multi-party computation and homomorphic encryption are available defenses for ensuring privacy and security.
IBM FL also offers the implementations of several topologies and communication protocols.</p>
</div>
<div id="S5.SS5.p4" class="ltx_para">
<p id="S5.SS5.p4.1" class="ltx_p">NVIDIA FLARE (Federated Learning Application Runtime Environment) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib186" title="" class="ltx_ref">186</a>]</cite> is a modular open-source software development kit (SDK) for FL which offers secure and privacy-preserving distributed learning. FLARE provides FL algorithms such as
FedAvg, FedProx and SCAFFOLD. It offers differential privacy and homomorphic encryption. FLARE SDK has several components, such as a simulator for prototyping, secure management tools for provisioning and deployment and an API for extensions.</p>
</div>
<div id="S5.SS5.p5" class="ltx_para">
<p id="S5.SS5.p5.1" class="ltx_p">FedML <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib187" title="" class="ltx_ref">187</a>]</cite> framework offers a wide-range of cross-platform FL capabilities including natural language processing, computer vision, and GNNs. FedAvg, FedOpt, FedNova and FedGKT are the supported FL algorithms. FedML offers defense mechanisms such as differential privacy, cryptography routines, and several coding
methods. It supports data and model parallel distributed learning.
FedML models can be trained and deployed at the edge or on the cloud.</p>
</div>
<div id="S5.SS5.p6" class="ltx_para">
<p id="S5.SS5.p6.1" class="ltx_p">FATE <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib188" title="" class="ltx_ref">188</a>]</cite> is an open-source platform initiated by WeBank, a bank based in Shenzhen, China. It provides a diverse set of FL algorithms, such as tree-based algorithms, DL, and transfer learning. It offers a set of modules consisting of an ML algorithms library, a high-performance serving system, an end-to-end pipeline system, a multi-party communication network system, and a module for cloud technologies. FATE provides homomorphic encryption and RSA for secure and privacy preserving training. FATE supports data and pipeline parallelisms.</p>
</div>
<div id="S5.SS5.p7" class="ltx_para">
<p id="S5.SS5.p7.1" class="ltx_p">PySyft <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib189" title="" class="ltx_ref">189</a>]</cite> is an open-source multi-language library that provides secure and private DL and FL in Python for frameworks such as PyTorch, Tensorflow and Keras. It supports differential privacy and homomorphic encryption. FedAvg, FedProx and FedSGD are among the available aggregation algorithms. Training can be data or model parallel.</p>
</div>
<div id="S5.SS5.p8" class="ltx_para">
<p id="S5.SS5.p8.1" class="ltx_p">OpenFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib190" title="" class="ltx_ref">190</a>]</cite> is an open-source Python framework originally developed by Intel Labs. It provides a set of workflows for the researchers to experiment with FL. FedAvg and ADAGRAD algorithms are built-in. OpenFL’s capabilities include trusted execution environments, RSA, differential privacy.</p>
</div>
</section>
<section id="S5.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6 </span>FL Datasets</h3>

<div id="S5.SS6.p1" class="ltx_para">
<p id="S5.SS6.p1.1" class="ltx_p">As FL research progresses, new datasets are being built. One of the most well-known datasets for FL is
the LEAF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib191" title="" class="ltx_ref">191</a>]</cite>. It is a suite of open-source federated datasets. There are a total of six different datasets.
One of the datasets, called FEMNIST, is built for image classification. Sentiment140, which consists of Tweets, is a dataset for sentiment analysis. Shakespeare is a text dataset of Shakespeare Dialogues which is used for next character prediction. Celeba is an image classification dataset of celebrity images. There is a synthetic classification dataset which is generated for the FL models that are device-dependant.
Lastly, the Reddit comments dataset is used for next word prediction.</p>
</div>
<div id="S5.SS6.p2" class="ltx_para">
<p id="S5.SS6.p2.1" class="ltx_p">TensorFlow Federated <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib192" title="" class="ltx_ref">192</a>]</cite> offers several datasets to support FL simulations. While some of its datasets are the same as those of LEAF, there are also different datasets, such as the federated CIFAR-100 dataset, the FLAIR dataset, and the federated Google Landmark v2 dataset.</p>
</div>
<div id="S5.SS6.p3" class="ltx_para">
<p id="S5.SS6.p3.1" class="ltx_p">Street Dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib193" title="" class="ltx_ref">193</a>]</cite> is a real-world image dataset. It contains images generated from street cameras. A total of seven object categories annotated with bounding boxes. This dataset is built for object detection tasks.</p>
</div>
<div id="S5.SS6.p4" class="ltx_para">
<p id="S5.SS6.p4.1" class="ltx_p">CC-19 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib194" title="" class="ltx_ref">194</a>]</cite> is a new dataset related to the latest family of coronavirus (COVID-19). It contains the Computed Tomography (CT) scan of subjects and is built for image classification.</p>
</div>
<div id="S5.SS6.p5" class="ltx_para">
<p id="S5.SS6.p5.1" class="ltx_p">FedTADBench <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib195" title="" class="ltx_ref">195</a>]</cite> offers three different datasets to evaluate time series anomaly detection algorithms.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Open Questions and Challenges</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In this section, we summarize the challenges that ML and FL face. We only present major problems. This is because there is a large number of open problems, and we choose to keep our presentation concise and focused.</p>
</div>
<section id="S6.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>Challenges for Parallel and Distributed ML</h3>

<div id="S6.SS1.p1" class="ltx_para">
<p id="S6.SS1.p1.1" class="ltx_p">The major challenges with parallel and distributed ML are related to performance, fault-tolerance, security and privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib196" title="" class="ltx_ref">196</a>, <a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite>.</p>
</div>
<div id="S6.SS1.p2" class="ltx_para">
<p id="S6.SS1.p2.1" class="ltx_p">Typically, in distributed and parallel training, additional resources are used to decrease wall-clock time <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib198" title="" class="ltx_ref">198</a>]</cite>. Such additional resources can be multiple machines, multiple GPUs and high-end communication networks. As a result, the decrease in wall-clock time may not
compensate for the additional resources or their energy consumption. Therefore, research studies, such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, are needed to investigate this trade-off with different applications and system architectures.</p>
</div>
<div id="S6.SS1.p3" class="ltx_para">
<p id="S6.SS1.p3.1" class="ltx_p">Distributed and parallel ML platforms, especially those executed on high-performance computing systems, often consider fault-tolerance as a second-class concern.
However, given the sizes of the latest large-scale computing systems, failures are common; not rare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite>. As a result, efficient checkpointing and/or replication solutions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib199" title="" class="ltx_ref">199</a>]</cite> are needed to recover from errors and to limit the amount of lost computation due to a failure.</p>
</div>
<div id="S6.SS1.p4" class="ltx_para">
<p id="S6.SS1.p4.1" class="ltx_p">Ensuring security and privacy for distributed and parallel ML has consistently been a serious concern <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib196" title="" class="ltx_ref">196</a>]</cite>. While FL was devised for the privacy of user data, there have been many novel types of attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib179" title="" class="ltx_ref">179</a>, <a href="#bib.bib180" title="" class="ltx_ref">180</a>]</cite>. These attacks include adversarial <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib168" title="" class="ltx_ref">168</a>]</cite>, poisoning, evasion, backdoor, and integrity attacks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib180" title="" class="ltx_ref">180</a>, <a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite>. As such attacks get sophisticated, so must their defenses.
Moreover, the systematic deployment of the defenses to the physical systems as well as the evaluation of these deployments have not studied well <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib197" title="" class="ltx_ref">197</a>]</cite>.
Furthermore, there is a lack of the rigorous efficiency and efficacy studies of attack defense mechanisms. As a result of these issues, security and privacy for ML remain an open problem.</p>
</div>
</section>
<section id="S6.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Challenges for FL</h3>

<div id="S6.SS2.p1" class="ltx_para">
<p id="S6.SS2.p1.1" class="ltx_p">The main challenges in FL are two-fold <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib200" title="" class="ltx_ref">200</a>]</cite>: explainability and interpretability, and federated GNNs.
Explainability and interpretability refer to the understanding of the contributions of the clients or the data features. For instance,
Shapley values are proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib201" title="" class="ltx_ref">201</a>]</cite> to quantify the impact of the features on the model output.
Zheng et. al. propose a quantified ranking of features <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib202" title="" class="ltx_ref">202</a>]</cite>.
Similarly, there are studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib203" title="" class="ltx_ref">203</a>]</cite> targeting vertical FL.
Several works introduce tailored measures of interpretability such as
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib204" title="" class="ltx_ref">204</a>]</cite> defining a measure based on the gradients. However, in general, the problem of explainability and interpretability remains open because
i) ensuring privacy while building explainable models is not trivial,
ii) the aggregation of the local parameters obscures interpretability,
iii) there is a lack of datasets that are not composed of images or text, and
iv) there is a lack of a general framework for explainable federated models.</p>
</div>
<div id="S6.SS2.p2" class="ltx_para">
<p id="S6.SS2.p2.1" class="ltx_p">Research for FL with GNNs
<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib205" title="" class="ltx_ref">205</a>, <a href="#bib.bib206" title="" class="ltx_ref">206</a>, <a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite> has recently started.
For instance, FedGraphNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib205" title="" class="ltx_ref">205</a>]</cite> provides an FL benchmark system to evaluate various graph models, algorithms and datasets. Another example is GraphFL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib207" title="" class="ltx_ref">207</a>]</cite> which is designed to classify nodes on graphs. However, many questions are still waiting to be solved, such as the
protection against malicious attacks, interpretability, lack of modern graph neural frameworks for FL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib208" title="" class="ltx_ref">208</a>]</cite>.</p>
</div>
</section>
</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusions</h2>

<div id="S7.p1" class="ltx_para">
<p id="S7.p1.1" class="ltx_p">In this work, we provided a review of modern large-scale, parallel and distributed ML: the state-of-the-art algorithms, optimization methods, types of parallelisms, communication topologies, synchronization models, and the existing frameworks. Moreover, we reviewed FL.
We discussed various aggregation algorithms in FL. In addition, we reviewed the security and privacy aspects including various types of attacks and defense mechanisms. Moreover, we explored the existing FL frameworks and datasets. We concluded our study with the open research problems and challenges in large-scale distributed ML and FL. The major challenges are typically related to performance, security, privacy, explainability, portability, and fault-tolerance.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgment</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This work was supported by the U.S. DOE Office of Science, Office of Advanced Scientific Computing Research, under award 66150: ”CENATE - Center for Advanced Architecture Evaluation” project. The Pacific Northwest National Laboratory is operated by Battelle for the U.S. Department of Energy under contract DE-AC05-76RL01830.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Shi Dong, Ping Wang, and Khushnood Abbas.

</span>
<span class="ltx_bibblock">A survey on deep learning and its applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Computer Science Review</span>, 40:100379, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Iqbal H Sarker.

</span>
<span class="ltx_bibblock">Deep learning: a comprehensive overview on techniques, taxonomy,
applications and research directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">SN Computer Science</span>, 2(6):420, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Siddharth
Samsi, and Jeremy Kepner.

</span>
<span class="ltx_bibblock">Survey of machine learning accelerators.

</span>
<span class="ltx_bibblock">In <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">2020 IEEE High Performance Extreme Computing Conference
(HPEC)</span>, pages 1–12, 2020.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Siddharth
Samsi, and Jeremy K̃epner.

</span>
<span class="ltx_bibblock">Survey and benchmarking of machine learning accelerators.

</span>
<span class="ltx_bibblock">In <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">2019 IEEE High Performance Extreme Computing Conference
(HPEC)</span>, pages 1–9, 2019.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Sathwika Bavikadi, Abhijitt Dhavlle, Amlan Ganguly, Anand Haridass, Hagar
Hendy, Cory Merkel, Vijay Janapa Reddi, Purab Ranjan Sutradhar, Arun Joseph,
and Sai Manoj Pudukotai Dinakarrao.

</span>
<span class="ltx_bibblock">A survey on machine learning accelerators and evolutionary hardware
platforms.

</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">IEEE Design &amp; Test</span>, 39(3):91–116, 2022.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Meng Wang, Weijie Fu, Xiangnan He, Shijie Hao, and Xindong Wu.

</span>
<span class="ltx_bibblock">A survey on large-scale machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>,
34(6):2574–2594, 2022.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Joost Verbraeken, Matthijs Wolting, Jonathan Katzy, Jeroen Kloppenburg, Tim
Verbelen, and Jan S. Rellermeyer.

</span>
<span class="ltx_bibblock">A survey on distributed machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">ACM Comput. Surv.</span>, 53(2), mar 2020.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Lucy Ellen Lwakatare, Aiswarya Raj, Ivica Crnkovic, Jan Bosch, and
Helena Holmström Olsson.

</span>
<span class="ltx_bibblock">Large-scale machine learning systems in real-world industrial
settings: A review of challenges and solutions.

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">Information and Software Technology</span>, 127:106368, 2020.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and
Blaise Agüera y Arcas.

</span>
<span class="ltx_bibblock">Communication-efficient learning of deep networks from decentralized
data.

</span>
<span class="ltx_bibblock">In Aarti Singh and Xiaojin (Jerry) Zhu, editors, <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of
the 20th International Conference on Artificial Intelligence and Statistics,
AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA</span>, volume 54 of
<span id="bib.bib9.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages 1273–1282. PMLR,
2017.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi Xiong, and Dejing
Dou.

</span>
<span class="ltx_bibblock">From distributed machine learning to federated learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Knowl. Inf. Syst.</span>, 64(4):885–917, apr 2022.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Chen Zhang, Yu Xie, Hang Bai, Bin Yu, Weihong Li, and Yuan Gao.

</span>
<span class="ltx_bibblock">A survey on federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Knowledge-Based Systems</span>, 216:106775, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu, and
Bingsheng He.

</span>
<span class="ltx_bibblock">A survey on federated learning systems: Vision, hype and reality for
data privacy and protection.

</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>,
35(4):3347–3366, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Dinh C. Nguyen, Ming Ding, Pubudu N. Pathirana, Aruna Seneviratne, Jun Li, and
H. Vincent Poor.

</span>
<span class="ltx_bibblock">Federated learning for internet of things: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</span>, 23(3):1622–1658,
2021.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Shaveta Dargan, Munish Kumar, Maruthi Rohit Ayyagari, and Gulshan Kumar.

</span>
<span class="ltx_bibblock">A survey of deep learning and its applications: a new paradigm to
machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Archives of Computational Methods in Engineering</span>,
27:1071–1092, 2020.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Iqbal H Sarker.

</span>
<span class="ltx_bibblock">Machine learning: Algorithms, real-world applications and research
directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">SN computer science</span>, 2(3):160, 2021.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Tal Ben-Nun and Torsten Hoefler.

</span>
<span class="ltx_bibblock">Demystifying parallel and distributed deep learning: An in-depth
concurrency analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">ACM Comput. Surv.</span>, 52(4), aug 2019.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Yuhao Zhang, Frank McQuillan, Nandish Jayaram, Nikhil Kak, Ekta Khanna, Orhan
Kislal, Domino Valdano, and Arun Kumar.

</span>
<span class="ltx_bibblock">Distributed deep learning on data systems: A comparative analysis of
approaches.

</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proc. VLDB Endow.</span>, 14(10):1769–1782, jun 2021.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Matthias Langer, Zhen He, Wenny Rahayu, and Yanbo Xue.

</span>
<span class="ltx_bibblock">Distributed training of deep learning models: A taxonomic
perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Parallel and Distributed Systems</span>,
31(12):2802–2818, 2020.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ruben Mayer and Hans-Arno Jacobsen.

</span>
<span class="ltx_bibblock">Scalable deep learning on distributed infrastructures: Challenges,
techniques, and tools.

</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">ACM Comput. Surv.</span>, 53(1), feb 2020.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Omar Abdel Wahab, Azzam Mourad, Hadi Otrok, and Tarik Taleb.

</span>
<span class="ltx_bibblock">Federated machine learning: Survey, multi-level classification,
desirable criteria and future directions in communication and networking
systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</span>, 23(2):1342–1397,
2021.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Yingxia Shao, Hongzheng Li, Xizhi Gu, Hongbo Yin, Yawen Li, Xupeng Miao, Wentao
Zhang, Bin Cui, and Lei Chen.

</span>
<span class="ltx_bibblock">Distributed graph neural network training: A survey, 2022.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Haozhao Wang, Zhihao Qu, Qihua Zhou, Haobo Zhang, Boyuan Luo, Wenchao Xu, Song
Guo, and Ruixuan Li.

</span>
<span class="ltx_bibblock">A comprehensive survey on training acceleration for large machine
learning models in iot.

</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 9(2):939–963, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Shuyan Hu, Xiaojing Chen, Wei Ni, Ekram Hossain, and Xin Wang.

</span>
<span class="ltx_bibblock">Distributed machine learning for wireless communication networks:
Techniques, architectures, and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</span>, 23(3):1458–1493,
2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Omar Nassef, Wenting Sun, Hakimeh Purmehdi, Mallik Tatipamula, and Toktam
Mahmoodi.

</span>
<span class="ltx_bibblock">A survey: Distributed machine learning for 5g and beyond.

</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Comput. Netw.</span>, 207(C), apr 2022.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Yahao Ding, Zhaohui Yang, Quoc-Viet Pham, Zhaoyang Zhang, and Mohammad
Shikh-Bahaei.

</span>
<span class="ltx_bibblock">Distributed machine learning for uav swarms: Computing, sensing, and
semantics, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Viraaji Mothukuri, Reza M. Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
Dehghantanha, and Gautam Srivastava.

</span>
<span class="ltx_bibblock">A survey on security and privacy of federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Future Generation Computer Systems</span>, 115:619–640, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Youyang Qu, Md Palash Uddin, Chenquan Gan, Yong Xiang, Longxiang Gao, and John
Yearwood.

</span>
<span class="ltx_bibblock">Blockchain-enabled federated learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys</span>, 55(4):1–35, 2022.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Ahmed El Ouadrhiri and Ahmed Abdelhadi.

</span>
<span class="ltx_bibblock">Differential privacy for deep and federated learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 10:22359–22380, 2022.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Steven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao.

</span>
<span class="ltx_bibblock">Online learning: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 459:249–289, 2021.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard
Scholkopf.

</span>
<span class="ltx_bibblock">Support vector machines.

</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text ltx_font_italic">IEEE Intelligent Systems and their applications</span>, 13(4):18–28,
1998.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Asifullah Khan, Anabia Sohail, Umme Zahoora, and Aqsa Saeed Qureshi.

</span>
<span class="ltx_bibblock">A survey of the recent architectures of deep convolutional neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Artificial intelligence review</span>, 53(8):5455–5516, 2020.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yong Yu, Xiaosheng Si, Changhua Hu, and Jianxun Zhang.

</span>
<span class="ltx_bibblock">A review of recurrent neural networks: Lstm cells and network
architectures.

</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Neural computation</span>, 31(7):1235–1270, 2019.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Dor Bank, Noam Koenigstein, and Raja Giryes.

</span>
<span class="ltx_bibblock">Autoencoders.

</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.05991</span>, 2020.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio.

</span>
<span class="ltx_bibblock">Generative adversarial networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 63(11):139–144, 2020.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu
Philip.

</span>
<span class="ltx_bibblock">A comprehensive survey on graph neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">IEEE transactions on neural networks and learning systems</span>,
32(1):4–24, 2020.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Marc M Van Hulle.

</span>
<span class="ltx_bibblock">Self-organizing maps.

</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Handbook of natural computing</span>, 1:585–622, 2012.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Geoffrey E Hinton, Terrence J Sejnowski, and David H Ackley.

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">Boltzmann machines: Constraint satisfaction networks that
learn</span>.

</span>
<span class="ltx_bibblock">Carnegie-Mellon University, Department of Computer Science
Pittsburgh, PA, 1984.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Geoffrey E Hinton, Terrence J Sejnowski, et al.

</span>
<span class="ltx_bibblock">Learning and relearning in boltzmann machines.

</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text ltx_font_italic">Parallel distributed processing: Explorations in the
microstructure of cognition</span>, 1(282-317):2, 1986.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Deep belief networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text ltx_font_italic">Scholarpedia</span>, 4(5):5947, 2009.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
John J Hopfield.

</span>
<span class="ltx_bibblock">Hopfield network.

</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text ltx_font_italic">Scholarpedia</span>, 2(5):1977, 2007.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Hubert Ramsauer, Bernhard Schäfl, Johannes Lehner, Philipp Seidl, Michael
Widrich, Thomas Adler, Lukas Gruber, Markus Holzleitner, Milena Pavlović,
Geir Kjetil Sandve, et al.

</span>
<span class="ltx_bibblock">Hopfield networks is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2008.02217</span>, 2020.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">A survey of transformers.

</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text ltx_font_italic">AI Open</span>, 2022.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, Accessed on 03-29-2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Chatgpt, Accessed on 03-29-2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Sholom M Weiss and Nitin Indurkhya.

</span>
<span class="ltx_bibblock">Rule-based machine learning methods for functional prediction.

</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text ltx_font_italic">Journal of Artificial Intelligence Research</span>, 3:383–403, 1995.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Akbar Telikani, Amirhessam Tahmassebi, Wolfgang Banzhaf, and Amir H. Gandomi.

</span>
<span class="ltx_bibblock">Evolutionary machine learning: A survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text ltx_font_italic">ACM Comput. Surv.</span>, 54(8), oct 2021.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Rubayyi Alghamdi and Khalid Alfalqi.

</span>
<span class="ltx_bibblock">A survey of topic modeling in text mining.

</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text ltx_font_italic">Int. J. Adv. Comput. Sci. Appl.(IJACSA)</span>, 6(1), 2015.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Pooja Kherwa and Poonam Bansal.

</span>
<span class="ltx_bibblock">Topic modeling: A comprehensive review.

</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text ltx_font_italic">EAI Endorsed Transactions on Scalable Information Systems</span>,
7(24), 7 2019.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel,
Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau,
Matthieu Brucher, Matthieu Perrot, and Édouard Duchesnay.

</span>
<span class="ltx_bibblock">Scikit-learn: Machine learning in python.

</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 12(85):2825–2830, 2011.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Ian H. Witten, Eibe Frank, Mark A. Hall, and Christopher J. Pal.

</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text ltx_font_italic">Data Mining, Fourth Edition: Practical Machine Learning Tools
and Techniques</span>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 4th edition,
2016.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Tianqi Chen and Carlos Guestrin.

</span>
<span class="ltx_bibblock">Xgboost: A scalable tree boosting system.

</span>
<span class="ltx_bibblock">In <span id="bib.bib52.1.1" class="ltx_text ltx_font_italic">Proceedings of the 22nd acm sigkdd international conference
on knowledge discovery and data mining</span>, pages 785–794, 2016.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Sören Sonnenburg, Gunnar Ratsch, Sebastian Henschel, Christian Widmer,
Jonas Behr, Alexander Zien, Fabio de Bona, Alexander Binder, Christian Gehl,
and Vojtech Franc.

</span>
<span class="ltx_bibblock">The shogun machine learning toolbox.

</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 11(60):1799–1802, 2010.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Chih-Chung Chang and Chih-Jen Lin.

</span>
<span class="ltx_bibblock">Libsvm: a library for support vector machines.

</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text ltx_font_italic">ACM transactions on intelligent systems and technology (TIST)</span>,
2(3):1–27, 2011.

</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Google.

</span>
<span class="ltx_bibblock">Google cloud, Accessed on 03-27-2023.

</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Microsoft.

</span>
<span class="ltx_bibblock">Microsoft azure, Accessed on 03-27-2023.

</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Amazon.

</span>
<span class="ltx_bibblock">Machine learning on aws, Accessed on 03-27-2023.

</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
IBM Watson.

</span>
<span class="ltx_bibblock">Ibm watson assistant, Accessed on 03-27-2023.

</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Sharan Chetlur, Cliff Woolley, Philippe Vandermersch, Jonathan Cohen, John
Tran, Bryan Catanzaro, and Evan Shelhamer.

</span>
<span class="ltx_bibblock">cudnn: Efficient primitives for deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1410.0759, 2014.

</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Norman P. Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, Rick
Boyle, Pierre-luc Cantin, Clifford Chao, Chris Clark, Jeremy Coriell, Mike
Daley, Matt Dau, Jeffrey Dean, Ben Gelb, Tara Vazir Ghaemmaghami, Rajendra
Gottipati, William Gulland, Robert Hagmann, C. Richard Ho, Doug Hogberg, John
Hu, Robert Hundt, Dan Hurt, Julian Ibarz, Aaron Jaffey, Alek Jaworski,
Alexander Kaplan, Harshit Khaitan, Daniel Killebrew, Andy Koch, Naveen Kumar,
Steve Lacy, James Laudon, James Law, Diemthu Le, Chris Leary, Zhuyuan Liu,
Kyle Lucke, Alan Lundin, Gordon MacKean, Adriana Maggiore, Maire Mahony,
Kieran Miller, Rahul Nagarajan, Ravi Narayanaswami, Ray Ni, Kathy Nix, Thomas
Norrie, Mark Omernick, Narayana Penukonda, Andy Phelps, Jonathan Ross, Matt
Ross, Amir Salek, Emad Samadiani, Chris Severn, Gregory Sizikov, Matthew
Snelham, Jed Souter, Dan Steinberg, Andy Swing, Mercedes Tan, Gregory
Thorson, Bo Tian, Horia Toma, Erick Tuttle, Vijay Vasudevan, Richard Walter,
Walter Wang, Eric Wilcox, and Doe Hyun Yoon.

</span>
<span class="ltx_bibblock">In-datacenter performance analysis of a tensor processing unit.

</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text ltx_font_italic">SIGARCH Comput. Archit. News</span>, 45(2):1–12, jun 2017.

</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Rengan Xu, Frank Han, and Quy Ta.

</span>
<span class="ltx_bibblock">Deep learning at scale on nvidia v100 accelerators.

</span>
<span class="ltx_bibblock">In <span id="bib.bib61.1.1" class="ltx_text ltx_font_italic">2018 IEEE/ACM Performance Modeling, Benchmarking and
Simulation of High Performance Computer Systems (PMBS)</span>, pages 23–32, 2018.

</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza.

</span>
<span class="ltx_bibblock">Dissecting the graphcore IPU architecture via microbenchmarking.

</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1912.03413, 2019.

</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Murali Emani, Venkatram Vishwanath, Corey Adams, Michael E. Papka, Rick
Stevens, Laura Florescu, Sumti Jairath, William Liu, Tejas Nama, and Arvind
Sujeeth.

</span>
<span class="ltx_bibblock">Accelerating scientific applications with sambanova reconfigurable
dataflow architecture.

</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text ltx_font_italic">Computing in Science &amp; Engineering</span>, 23(2):114–119, 2021.

</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Sanjiv Kumar, Mehryar Mohri, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Sampling methods for the nyström method.

</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text ltx_font_italic">The Journal of Machine Learning Research</span>, 13(1):981–1006,
2012.

</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Djallel Bouneffouf and Inanc Birol.

</span>
<span class="ltx_bibblock">Sampling with minimum sum of squared similarities for nystrom-based
large scale spectral clustering.

</span>
<span class="ltx_bibblock">In <span id="bib.bib65.1.1" class="ltx_text ltx_font_italic">IJCAI</span>, pages 2313–2319, 2015.

</span>
</li>
<li id="bib.bib66" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert.

</span>
<span class="ltx_bibblock">A randomized algorithm for the decomposition of matrices.

</span>
<span class="ltx_bibblock"><span id="bib.bib66.1.1" class="ltx_text ltx_font_italic">Applied and Computational Harmonic Analysis</span>, 30(1):47–68,
2011.

</span>
</li>
<li id="bib.bib67" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Ali Rahimi and Benjamin Recht.

</span>
<span class="ltx_bibblock">Random features for large-scale kernel machines.

</span>
<span class="ltx_bibblock"><span id="bib.bib67.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 20, 2007.

</span>
</li>
<li id="bib.bib68" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Jia Deng, Sanjeev Satheesh, Alexander Berg, and Fei Li.

</span>
<span class="ltx_bibblock">Fast and balanced: Efficient label tree learning for large scale
object recognition.

</span>
<span class="ltx_bibblock"><span id="bib.bib68.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 24, 2011.

</span>
</li>
<li id="bib.bib69" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Yael Ben-Haim and Elad Tom-Tov.

</span>
<span class="ltx_bibblock">A streaming parallel decision tree algorithm.

</span>
<span class="ltx_bibblock"><span id="bib.bib69.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 11(2), 2010.

</span>
</li>
<li id="bib.bib70" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Wei Liu, Junfeng He, and Shih-Fu Chang.

</span>
<span class="ltx_bibblock">Large graph construction for scalable semi-supervised learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib70.1.1" class="ltx_text ltx_font_italic">Proceedings of the 27th international conference on machine
learning (ICML-10)</span>, pages 679–686. Citeseer, 2010.

</span>
</li>
<li id="bib.bib71" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Yan-Ming Zhang, Kaizhu Huang, Guanggang Geng, and Cheng-Lin Liu.

</span>
<span class="ltx_bibblock">Fast k nn graph construction with locality sensitive hashing.

</span>
<span class="ltx_bibblock">In <span id="bib.bib71.1.1" class="ltx_text ltx_font_italic">Machine Learning and Knowledge Discovery in Databases:
European Conference, ECML PKDD 2013, Prague, Czech Republic, September 23-27,
2013, Proceedings, Part II 13</span>, pages 660–674. Springer, 2013.

</span>
</li>
<li id="bib.bib72" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Jingdong Wang, Ting Zhang, Nicu Sebe, Heng Tao Shen, et al.

</span>
<span class="ltx_bibblock">A survey on learning to hash.

</span>
<span class="ltx_bibblock"><span id="bib.bib72.1.1" class="ltx_text ltx_font_italic">IEEE transactions on pattern analysis and machine intelligence</span>,
40(4):769–790, 2017.

</span>
</li>
<li id="bib.bib73" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Meng Wang, Weijie Fu, Shijie Hao, Hengchang Liu, and Xindong Wu.

</span>
<span class="ltx_bibblock">Learning on big graph: Label inference and regularization with anchor
hierarchy.

</span>
<span class="ltx_bibblock"><span id="bib.bib73.1.1" class="ltx_text ltx_font_italic">IEEE transactions on knowledge and data engineering</span>,
29(5):1101–1114, 2017.

</span>
</li>
<li id="bib.bib74" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Meng Wang, Weijie Fu, Shijie Hao, Dacheng Tao, and Xindong Wu.

</span>
<span class="ltx_bibblock">Scalable semi-supervised learning by efficient anchor graph
regularization.

</span>
<span class="ltx_bibblock"><span id="bib.bib74.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Knowledge and Data Engineering</span>,
28(7):1864–1877, 2016.

</span>
</li>
<li id="bib.bib75" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Vinod Nair and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Rectified linear units improve restricted boltzmann machines.

</span>
<span class="ltx_bibblock">In <span id="bib.bib75.1.1" class="ltx_text ltx_font_italic">Proceedings of the 27th international conference on machine
learning (ICML-10)</span>, pages 807–814, 2010.

</span>
</li>
<li id="bib.bib76" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al.

</span>
<span class="ltx_bibblock">Rectifier nonlinearities improve neural network acoustic models.

</span>
<span class="ltx_bibblock">In <span id="bib.bib76.1.1" class="ltx_text ltx_font_italic">Proc. icml</span>, volume 30, page 3. Atlanta, Georgia, USA, 2013.

</span>
</li>
<li id="bib.bib77" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Shan Sung Liew, Mohamed Khalil-Hani, and Rabia Bakhteri.

</span>
<span class="ltx_bibblock">Bounded activation functions for enhanced training stability of deep
neural networks on visual pattern recognition problems.

</span>
<span class="ltx_bibblock"><span id="bib.bib77.1.1" class="ltx_text ltx_font_italic">Neurocomputing</span>, 216:718–734, 2016.

</span>
</li>
<li id="bib.bib78" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
Tobias Weyand, Marco Andreetto, and Hartwig Adam.

</span>
<span class="ltx_bibblock">Mobilenets: Efficient convolutional neural networks for mobile vision
applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib78.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1704.04861</span>, 2017.

</span>
</li>
<li id="bib.bib79" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.

</span>
<span class="ltx_bibblock">Imagenet classification with deep convolutional neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib79.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 60(6):84–90, 2017.

</span>
</li>
<li id="bib.bib80" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Siddharth Gopal.

</span>
<span class="ltx_bibblock">Adaptive sampling for sgd by exploiting side information.

</span>
<span class="ltx_bibblock">In <span id="bib.bib80.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
364–372. PMLR, 2016.

</span>
</li>
<li id="bib.bib81" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua
Bengio.

</span>
<span class="ltx_bibblock">Variance reduction in sgd by distributed importance sampling.

</span>
<span class="ltx_bibblock"><span id="bib.bib81.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1511.06481</span>, 2015.

</span>
</li>
<li id="bib.bib82" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz
Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.

</span>
<span class="ltx_bibblock">Accurate, large minibatch sgd: Training imagenet in 1 hour.

</span>
<span class="ltx_bibblock"><span id="bib.bib82.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1706.02677</span>, 2017.

</span>
</li>
<li id="bib.bib83" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Yoshua Bengio.

</span>
<span class="ltx_bibblock">Practical recommendations for gradient-based training of deep
architectures.

</span>
<span class="ltx_bibblock"><span id="bib.bib83.1.1" class="ltx_text ltx_font_italic">Neural Networks: Tricks of the Trade: Second Edition</span>, pages
437–478, 2012.

</span>
</li>
<li id="bib.bib84" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar.

</span>
<span class="ltx_bibblock">On the convergence of adam and beyond.

</span>
<span class="ltx_bibblock"><span id="bib.bib84.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1904.09237</span>, 2019.

</span>
</li>
<li id="bib.bib85" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
John Duchi, Elad Hazan, and Yoram Singer.

</span>
<span class="ltx_bibblock">Adaptive subgradient methods for online learning and stochastic
optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib85.1.1" class="ltx_text ltx_font_italic">Journal of machine learning research</span>, 12(7), 2011.

</span>
</li>
<li id="bib.bib86" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Matthew D Zeiler.

</span>
<span class="ltx_bibblock">Adadelta: an adaptive learning rate method.

</span>
<span class="ltx_bibblock"><span id="bib.bib86.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1212.5701</span>, 2012.

</span>
</li>
<li id="bib.bib87" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Ning Qian.

</span>
<span class="ltx_bibblock">On the momentum term in gradient descent learning algorithms.

</span>
<span class="ltx_bibblock"><span id="bib.bib87.1.1" class="ltx_text ltx_font_italic">Neural networks</span>, 12(1):145–151, 1999.

</span>
</li>
<li id="bib.bib88" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Yu Nesterov.

</span>
<span class="ltx_bibblock">Gradient methods for minimizing composite functions.

</span>
<span class="ltx_bibblock"><span id="bib.bib88.1.1" class="ltx_text ltx_font_italic">Mathematical programming</span>, 140(1):125–161, 2013.

</span>
</li>
<li id="bib.bib89" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
Shiliang Sun, Zehui Cao, Han Zhu, and Jing Zhao.

</span>
<span class="ltx_bibblock">A survey of optimization methods from a machine learning perspective.

</span>
<span class="ltx_bibblock"><span id="bib.bib89.1.1" class="ltx_text ltx_font_italic">IEEE transactions on cybernetics</span>, 50(8):3668–3681, 2019.

</span>
</li>
<li id="bib.bib90" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
Quoc V Le, Jiquan Ngiam, Adam Coates, Abhik Lahiri, Bobby Prochnow, and
Andrew Y Ng.

</span>
<span class="ltx_bibblock">On optimization methods for deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib90.1.1" class="ltx_text ltx_font_italic">Proceedings of the 28th International Conference on
International Conference on Machine Learning</span>, pages 265–272, 2011.

</span>
</li>
<li id="bib.bib91" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
Immanuel Bayer, Xiangnan He, Bhargav Kanagal, and Steffen Rendle.

</span>
<span class="ltx_bibblock">A generic coordinate descent framework for learning from implicit
feedback.

</span>
<span class="ltx_bibblock">In <span id="bib.bib91.1.1" class="ltx_text ltx_font_italic">Proceedings of the 26th International Conference on World
Wide Web</span>, pages 1341–1350, 2017.

</span>
</li>
<li id="bib.bib92" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
Guo-Xun Yuan, Chia-Hua Ho, and Chih-Jen Lin.

</span>
<span class="ltx_bibblock">Recent advances of large-scale linear classification.

</span>
<span class="ltx_bibblock"><span id="bib.bib92.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE</span>, 100(9):2584–2603, 2012.

</span>
</li>
<li id="bib.bib93" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
Julie Nutini, Mark Schmidt, Issam Laradji, Michael Friedlander, and Hoyt
Koepke.

</span>
<span class="ltx_bibblock">Coordinate descent converges faster with the gauss-southwell rule
than random selection.

</span>
<span class="ltx_bibblock">In <span id="bib.bib93.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
1632–1641. PMLR, 2015.

</span>
</li>
<li id="bib.bib94" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
Yu Nesterov.

</span>
<span class="ltx_bibblock">Efficiency of coordinate descent methods on huge-scale optimization
problems.

</span>
<span class="ltx_bibblock"><span id="bib.bib94.1.1" class="ltx_text ltx_font_italic">SIAM Journal on Optimization</span>, 22(2):341–362, 2012.

</span>
</li>
<li id="bib.bib95" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
Hao-Jun Michael Shi, Shenyinying Tu, Yangyang Xu, and Wotao Yin.

</span>
<span class="ltx_bibblock">A primer on coordinate descent algorithms.

</span>
<span class="ltx_bibblock"><span id="bib.bib95.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1610.00040</span>, 2016.

</span>
</li>
<li id="bib.bib96" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
Sangwoon Yun and Kim-Chuan Toh.

</span>
<span class="ltx_bibblock">A coordinate gradient descent method for l1-regularized convex
minimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib96.1.1" class="ltx_text ltx_font_italic">Computational Optimization and Applications</span>, 48(2):273–307,
2011.

</span>
</li>
<li id="bib.bib97" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Huan Li and Zhouchen Lin.

</span>
<span class="ltx_bibblock">Accelerated proximal gradient methods for nonconvex programming.

</span>
<span class="ltx_bibblock"><span id="bib.bib97.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib98" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al.

</span>
<span class="ltx_bibblock">Distributed optimization and statistical learning via the alternating
direction method of multipliers.

</span>
<span class="ltx_bibblock"><span id="bib.bib98.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Machine learning</span>,
3(1):1–122, 2011.

</span>
</li>
<li id="bib.bib99" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
Siddhartha Chib and Edward Greenberg.

</span>
<span class="ltx_bibblock">Understanding the metropolis-hastings algorithm.

</span>
<span class="ltx_bibblock"><span id="bib.bib99.1.1" class="ltx_text ltx_font_italic">The american statistician</span>, 49(4):327–335, 1995.

</span>
</li>
<li id="bib.bib100" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
Thomas L Griffiths and Mark Steyvers.

</span>
<span class="ltx_bibblock">Finding scientific topics.

</span>
<span class="ltx_bibblock"><span id="bib.bib100.1.1" class="ltx_text ltx_font_italic">Proceedings of the National academy of Sciences</span>,
101(suppl_1):5228–5235, 2004.

</span>
</li>
<li id="bib.bib101" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
Sungjin Ahn, Anoop Korattikara, and Max Welling.

</span>
<span class="ltx_bibblock">Bayesian posterior sampling via stochastic gradient fisher scoring.

</span>
<span class="ltx_bibblock"><span id="bib.bib101.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1206.6380</span>, 2012.

</span>
</li>
<li id="bib.bib102" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.

</span>
<span class="ltx_bibblock">1-bit stochastic gradient descent and its application to
data-parallel distributed training of speech dnns.

</span>
<span class="ltx_bibblock">In <span id="bib.bib102.1.1" class="ltx_text ltx_font_italic">Fifteenth annual conference of the international speech
communication association</span>, 2014.

</span>
</li>
<li id="bib.bib103" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.

</span>
<span class="ltx_bibblock">Qsgd: Communication-efficient sgd via gradient quantization and
encoding.

</span>
<span class="ltx_bibblock"><span id="bib.bib103.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib104" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.

</span>
<span class="ltx_bibblock">Terngrad: Ternary gradients to reduce communication in distributed
deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib104.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib105" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally.

</span>
<span class="ltx_bibblock">Deep gradient compression: Reducing the communication bandwidth for
distributed training.

</span>
<span class="ltx_bibblock"><span id="bib.bib105.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1712.01887</span>, 2017.

</span>
</li>
<li id="bib.bib106" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji Liu, and Ce Zhang.

</span>
<span class="ltx_bibblock">Zipml: Training linear models with end-to-end low precision, and a
little bit of deep learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib106.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4035–4043. PMLR, 2017.

</span>
</li>
<li id="bib.bib107" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B
Gibbons, Garth A Gibson, Greg Ganger, and Eric P Xing.

</span>
<span class="ltx_bibblock">More effective distributed ml via a stale synchronous parallel
parameter server.

</span>
<span class="ltx_bibblock"><span id="bib.bib107.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 26, 2013.

</span>
</li>
<li id="bib.bib108" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and
Tie-Yan Liu.

</span>
<span class="ltx_bibblock">Asynchronous stochastic gradient descent with delay compensation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib108.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
4120–4129. PMLR, 2017.

</span>
</li>
<li id="bib.bib109" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
Sixin Zhang, Anna E Choromanska, and Yann LeCun.

</span>
<span class="ltx_bibblock">Deep learning with elastic averaging sgd.

</span>
<span class="ltx_bibblock"><span id="bib.bib109.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 28, 2015.

</span>
</li>
<li id="bib.bib110" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Brendan McMahan and Matthew Streeter.

</span>
<span class="ltx_bibblock">Delay-tolerant algorithms for asynchronous distributed online
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib110.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>, 27, 2014.

</span>
</li>
<li id="bib.bib111" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
Leslie G Valiant.

</span>
<span class="ltx_bibblock">A bridging model for parallel computation.

</span>
<span class="ltx_bibblock"><span id="bib.bib111.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 33(8):103–111, 1990.

</span>
</li>
<li id="bib.bib112" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
Kevin Hsieh, Aaron Harlap, Nandita Vijaykumar, Dimitris Konomis, Gregory R
Ganger, Phillip B Gibbons, and Onur Mutlu.

</span>
<span class="ltx_bibblock">Gaia: Geo-distributed machine learning approaching lan speeds.

</span>
<span class="ltx_bibblock">In <span id="bib.bib112.1.1" class="ltx_text ltx_font_italic">NSDI</span>, pages 629–647, 2017.

</span>
</li>
<li id="bib.bib113" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
Christopher De Sa, Ce Zhang, Kunle Olukotun, and Christopher Ré.

</span>
<span class="ltx_bibblock">Taming the wild: A unified analysis of hogwild!-style algorithms,
2015.

</span>
</li>
<li id="bib.bib114" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard,
Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh
Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal
Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,
Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and
Xiaoqiang Zheng.

</span>
<span class="ltx_bibblock">TensorFlow: Large-scale machine learning on heterogeneous systems,
2015.

</span>
<span class="ltx_bibblock">Software available from tensorflow.org.

</span>
</li>
<li id="bib.bib115" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
Tensorflow.

</span>
<span class="ltx_bibblock">Distributed training with tensorflow.

</span>
<span class="ltx_bibblock">Available at
<a target="_blank" href="https://www.tensorflow.org/guide/distributed_training" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/guide/distributed_training</a>, Last accessed
09.01.2022.

</span>
</li>
<li id="bib.bib116" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
Keras.

</span>
<span class="ltx_bibblock">Distributed training with keras.

</span>
<span class="ltx_bibblock">Available at
<a target="_blank" href="https://www.tensorflow.org/tutorials/distribute/keras" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/tutorials/distribute/keras</a>, Last accessed
09.01.2022.

</span>
</li>
<li id="bib.bib117" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith
Chintala.

</span>
<span class="ltx_bibblock">Pytorch: An imperative style, high-performance deep learning library.

</span>
<span class="ltx_bibblock">In <span id="bib.bib117.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems 32</span>, pages
8024–8035. Curran Associates, Inc., 2019.

</span>
</li>
<li id="bib.bib118" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
PyTorch.

</span>
<span class="ltx_bibblock">Pytorch distributed overview.

</span>
<span class="ltx_bibblock">Available at
<a target="_blank" href="https://pytorch.org/tutorials/beginner/dist_overview.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://pytorch.org/tutorials/beginner/dist_overview.html</a>, Last
accessed 09.01.2022.

</span>
</li>
<li id="bib.bib119" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
Bing Xu, Chiyuan Zhang, and Zheng Zhang.

</span>
<span class="ltx_bibblock">Mxnet: A flexible and efficient machine learning library for
heterogeneous distributed systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib119.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1512.01274, 2015.

</span>
</li>
<li id="bib.bib120" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
Amith R Mamidala, Georgios Kollias, Chris Ward, and Fausto Artico.

</span>
<span class="ltx_bibblock">Mxnet-mpi: Embedding mpi parallelism in parameter server task model
for scaling deep learning, 2018.

</span>
</li>
<li id="bib.bib121" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
Alexander Sergeev and Mike Del Balso.

</span>
<span class="ltx_bibblock">Horovod: fast and easy distributed deep learning in tensorflow.

</span>
<span class="ltx_bibblock"><span id="bib.bib121.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1802.05799, 2018.

</span>
</li>
<li id="bib.bib122" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Andrew Gibiansky.

</span>
<span class="ltx_bibblock">Bringing hpc techniques to deep learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib122.1.1" class="ltx_text ltx_font_italic">Baidu Research, Tech. Rep.</span>, 2017.

</span>
</li>
<li id="bib.bib123" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
Elias De Coninck, Steven Bohez, Sam Leroux, Tim Verbelen, Bert Vankeirsbilck,
Pieter Simoens, and Bart Dhoedt.

</span>
<span class="ltx_bibblock">Dianne: a modular framework for designing, training and deploying
deep neural networks on heterogeneous distributed infrastructure.

</span>
<span class="ltx_bibblock"><span id="bib.bib123.1.1" class="ltx_text ltx_font_italic">Journal of Systems and Software</span>, 141:52–65, 2018.

</span>
</li>
<li id="bib.bib124" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Frank Seide and Amit Agarwal.

</span>
<span class="ltx_bibblock">Cntk: Microsoft’s open-source deep-learning toolkit.

</span>
<span class="ltx_bibblock">In <span id="bib.bib124.1.1" class="ltx_text ltx_font_italic">Proceedings of the 22nd ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining</span>, KDD ’16, page 2135, New York, NY,
USA, 2016. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib125" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
James Bergstra, Olivier Breuleux, Frédéric Bastien, Pascal Lamblin, Razvan
Pascanu, Guillaume Desjardins, Joseph Turian, David Warde-farley, and Yoshua
Bengio.

</span>
<span class="ltx_bibblock">Theano: A cpu and gpu math compiler in python.

</span>
<span class="ltx_bibblock">In <span id="bib.bib125.1.1" class="ltx_text ltx_font_italic">Proceedings of the 9th Python in Science Conference</span>, pages
3–10, 2010.

</span>
</li>
<li id="bib.bib126" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Matei Zaharia, Reynold S Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J
Franklin, et al.

</span>
<span class="ltx_bibblock">Apache spark: a unified engine for big data processing.

</span>
<span class="ltx_bibblock"><span id="bib.bib126.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 59(11):56–65, 2016.

</span>
</li>
<li id="bib.bib127" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
Apache Software Foundation.

</span>
<span class="ltx_bibblock">Hadoop.

</span>
</li>
<li id="bib.bib128" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
Jeffrey Dean and Sanjay Ghemawat.

</span>
<span class="ltx_bibblock">Mapreduce: simplified data processing on large clusters.

</span>
<span class="ltx_bibblock"><span id="bib.bib128.1.1" class="ltx_text ltx_font_italic">Communications of the ACM</span>, 51(1):107–113, 2008.

</span>
</li>
<li id="bib.bib129" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman,
Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, Doris Xin,
Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, and Ameet
Talwalkar.

</span>
<span class="ltx_bibblock">Mllib: Machine learning in apache spark.

</span>
<span class="ltx_bibblock"><span id="bib.bib129.1.1" class="ltx_text ltx_font_italic">Journal of Machine Learning Research</span>, 17(34):1–7, 2016.

</span>
</li>
<li id="bib.bib130" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong.

</span>
<span class="ltx_bibblock">Federated machine learning: Concept and applications.

</span>
<span class="ltx_bibblock"><span id="bib.bib130.1.1" class="ltx_text ltx_font_italic">ACM Transactions on Intelligent Systems and Technology (TIST)</span>,
10(2):1–19, 2019.

</span>
</li>
<li id="bib.bib131" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
Yang Liu, Yan Kang, Chaoping Xing, Tianjian Chen, and Qiang Yang.

</span>
<span class="ltx_bibblock">A secure federated transfer learning framework.

</span>
<span class="ltx_bibblock"><span id="bib.bib131.1.1" class="ltx_text ltx_font_italic">IEEE Intelligent Systems</span>, 35(4):70–82, 2020.

</span>
</li>
<li id="bib.bib132" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
Yang Liu, Xinwei Zhang, Yan Kang, Liping Li, Tianjian Chen, Mingyi Hong, and
Qiang Yang.

</span>
<span class="ltx_bibblock">Fedbcd: A communication-efficient collaborative learning framework
for distributed features.

</span>
<span class="ltx_bibblock"><span id="bib.bib132.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Signal Processing</span>, 70:4277–4290, 2022.

</span>
</li>
<li id="bib.bib133" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
Muhammad Asad, Ahmed Moustafa, and Takayuki Ito.

</span>
<span class="ltx_bibblock">Fedopt: Towards communication efficiency and privacy preservation in
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib133.1.1" class="ltx_text ltx_font_italic">Applied Sciences</span>, 10(8):2864, 2020.

</span>
</li>
<li id="bib.bib134" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
Chaoyang He, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Group knowledge transfer: Federated learning of large cnns at the
edge.

</span>
<span class="ltx_bibblock"><span id="bib.bib134.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
33:14068–14080, 2020.

</span>
</li>
<li id="bib.bib135" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Agnostic federated learning, 2019.

</span>
</li>
<li id="bib.bib136" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
Zeou Hu, Kiarash Shaloudegi, Guojun Zhang, and Yaoliang Yu.

</span>
<span class="ltx_bibblock">Federated learning meets multi-objective optimization, 2020.

</span>
</li>
<li id="bib.bib137" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush,
Jakub Konečnỳ, Sanjiv Kumar, and H Brendan McMahan.

</span>
<span class="ltx_bibblock">Adaptive federated optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib137.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.00295</span>, 2020.

</span>
</li>
<li id="bib.bib138" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank
Reddi, Sebastian U Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">Breaking the centralized barrier for cross-device federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib138.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems</span>,
34:28663–28676, 2021.

</span>
</li>
<li id="bib.bib139" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
Virginia Smith.

</span>
<span class="ltx_bibblock">Federated optimization in heterogeneous networks, 2018.

</span>
</li>
<li id="bib.bib140" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
Stich, and Ananda Theertha Suresh.

</span>
<span class="ltx_bibblock">SCAFFOLD: Stochastic controlled averaging for federated learning.

</span>
<span class="ltx_bibblock">In Hal Daumé III and Aarti Singh, editors, <span id="bib.bib140.1.1" class="ltx_text ltx_font_italic">Proceedings of the
37th International Conference on Machine Learning</span>, volume 119 of <span id="bib.bib140.2.2" class="ltx_text ltx_font_italic">Proceedings of Machine Learning Research</span>, pages 5132–5143. PMLR, 13–18 Jul
2020.

</span>
</li>
<li id="bib.bib141" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
Shaoxiong Ji, Shirui Pan, Guodong Long, Xue Li, Jing Jiang, and Zi Huang.

</span>
<span class="ltx_bibblock">Learning private neural language modeling with attentive aggregation.

</span>
<span class="ltx_bibblock">In <span id="bib.bib141.1.1" class="ltx_text ltx_font_italic">2019 International Joint Conference on Neural Networks
(IJCNN)</span>, pages 1–8, 2019.

</span>
</li>
<li id="bib.bib142" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor.

</span>
<span class="ltx_bibblock">Tackling the objective inconsistency problem in heterogeneous
federated optimization.

</span>
<span class="ltx_bibblock"><span id="bib.bib142.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>,
33:7611–7623, 2020.

</span>
</li>
<li id="bib.bib143" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi.

</span>
<span class="ltx_bibblock">Adaptive personalized federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib143.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2003.13461</span>, 2020.

</span>
</li>
<li id="bib.bib144" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik.

</span>
<span class="ltx_bibblock">Personalized federated learning using hypernetworks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib144.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
9489–9502. PMLR, 2021.

</span>
</li>
<li id="bib.bib145" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Alysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang.

</span>
<span class="ltx_bibblock">Towards personalized federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib145.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Neural Networks and Learning Systems</span>,
2022.

</span>
</li>
<li id="bib.bib146" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, and Nghia
Hoang.

</span>
<span class="ltx_bibblock">Statistical model aggregation via parameter matching.

</span>
<span class="ltx_bibblock"><span id="bib.bib146.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 32, 2019.

</span>
</li>
<li id="bib.bib147" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia
Hoang, and Yasaman Khazaeni.

</span>
<span class="ltx_bibblock">Bayesian nonparametric federated learning of neural networks.

</span>
<span class="ltx_bibblock">In <span id="bib.bib147.1.1" class="ltx_text ltx_font_italic">International conference on machine learning</span>, pages
7252–7261. PMLR, 2019.

</span>
</li>
<li id="bib.bib148" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
István Hegedűs, Gábor Danner, and Márk Jelasity.

</span>
<span class="ltx_bibblock">Decentralized learning works: An empirical comparison of gossip
learning and federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib148.1.1" class="ltx_text ltx_font_italic">Journal of Parallel and Distributed Computing</span>, 148:109–124,
2021.

</span>
</li>
<li id="bib.bib149" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
Hao Ye, Le Liang, and Geoffrey Ye Li.

</span>
<span class="ltx_bibblock">Decentralized federated learning with unreliable communications.

</span>
<span class="ltx_bibblock"><span id="bib.bib149.1.1" class="ltx_text ltx_font_italic">IEEE Journal of Selected Topics in Signal Processing</span>,
16(3):487–500, 2022.

</span>
</li>
<li id="bib.bib150" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.

</span>
<span class="ltx_bibblock">Machine learning with adversaries: Byzantine tolerant gradient
descent.

</span>
<span class="ltx_bibblock"><span id="bib.bib150.1.1" class="ltx_text ltx_font_italic">Advances in neural information processing systems</span>, 30, 2017.

</span>
</li>
<li id="bib.bib151" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
Leslie Lamport, Robert Shostak, and Marshall Pease.

</span>
<span class="ltx_bibblock">The byzantine generals problem.

</span>
<span class="ltx_bibblock">In <span id="bib.bib151.1.1" class="ltx_text ltx_font_italic">Concurrency: the works of leslie lamport</span>, pages 203–226.
ACM New York, NY, USA, 2019.

</span>
</li>
<li id="bib.bib152" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
Cynthia Dwork.

</span>
<span class="ltx_bibblock">Differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib152.1.1" class="ltx_text ltx_font_italic">Automata, Languages and Programming: 33rd International
Colloquium, ICALP 2006, Venice, Italy, July 10-14, 2006, Proceedings, Part II
33</span>, pages 1–12. Springer, 2006.

</span>
</li>
<li id="bib.bib153" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
Cynthia Dwork, Aaron Roth, et al.

</span>
<span class="ltx_bibblock">The algorithmic foundations of differential privacy.

</span>
<span class="ltx_bibblock"><span id="bib.bib153.1.1" class="ltx_text ltx_font_italic">Foundations and Trends® in Theoretical Computer
Science</span>, 9(3–4):211–407, 2014.

</span>
</li>
<li id="bib.bib154" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Kang Wei, Jun Li, Ming Ding, Chuan Ma, Howard H Yang, Farhad Farokhi, Shi Jin,
Tony QS Quek, and H Vincent Poor.

</span>
<span class="ltx_bibblock">Federated learning with differential privacy: Algorithms and
performance analysis.

</span>
<span class="ltx_bibblock"><span id="bib.bib154.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
15:3454–3469, 2020.

</span>
</li>
<li id="bib.bib155" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
Stacey Truex, Ling Liu, Ka-Ho Chow, Mehmet Emre Gursoy, and Wenqi Wei.

</span>
<span class="ltx_bibblock">Ldp-fed: Federated learning with local differential privacy.

</span>
<span class="ltx_bibblock">In <span id="bib.bib155.1.1" class="ltx_text ltx_font_italic">Proceedings of the Third ACM International Workshop on Edge
Systems, Analytics and Networking</span>, pages 61–66, 2020.

</span>
</li>
<li id="bib.bib156" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
Vale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu.

</span>
<span class="ltx_bibblock">Data poisoning attacks against federated learning systems.

</span>
<span class="ltx_bibblock">In <span id="bib.bib156.1.1" class="ltx_text ltx_font_italic">Computer Security–ESORICS 2020: 25th European Symposium on
Research in Computer Security, ESORICS 2020, Guildford, UK, September 14–18,
2020, Proceedings, Part I 25</span>, pages 480–501. Springer, 2020.

</span>
</li>
<li id="bib.bib157" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
Florian Nuding and Rudolf Mayer.

</span>
<span class="ltx_bibblock">Data poisoning in sequential and parallel federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib157.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2022 ACM on International Workshop on
Security and Privacy Analytics</span>, IWSPA ’22, page 24–34, 2022.

</span>
</li>
<li id="bib.bib158" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Gan Sun, Yang Cong, Jiahua Dong, Qiang Wang, Lingjuan Lyu, and Ji Liu.

</span>
<span class="ltx_bibblock">Data poisoning attacks on federated machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib158.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 9(13):11365–11375, 2022.

</span>
</li>
<li id="bib.bib159" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Ashwinee Panda, Saeed Mahloujifar, Arjun Nitin Bhagoji, Supriyo Chakraborty,
and Prateek Mittal.

</span>
<span class="ltx_bibblock">Sparsefed: Mitigating model poisoning attacks in federated learning
with sparsification.

</span>
<span class="ltx_bibblock">In <span id="bib.bib159.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</span>, pages 7587–7624. PMLR, 2022.

</span>
</li>
<li id="bib.bib160" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
Xiaoyu Cao and Neil Zhenqiang Gong.

</span>
<span class="ltx_bibblock">Mpaf: Model poisoning attacks to federated learning based on fake
clients.

</span>
<span class="ltx_bibblock">In <span id="bib.bib160.1.1" class="ltx_text ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span>, pages 3396–3404, 2022.

</span>
</li>
<li id="bib.bib161" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock">
Zhuoran Ma, Jianfeng Ma, Yinbin Miao, Yingjiu Li, and Robert H. Deng.

</span>
<span class="ltx_bibblock">Shieldfl: Mitigating model poisoning attacks in privacy-preserving
federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib161.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Information Forensics and Security</span>,
17:1639–1654, 2022.

</span>
</li>
<li id="bib.bib162" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock">
Milad Nasr, Reza Shokri, and Amir Houmansadr.

</span>
<span class="ltx_bibblock">Comprehensive privacy analysis of deep learning: Passive and active
white-box inference attacks against centralized and federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib162.1.1" class="ltx_text ltx_font_italic">2019 IEEE symposium on security and privacy (SP)</span>, pages
739–753. IEEE, 2019.

</span>
</li>
<li id="bib.bib163" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock">
Pengrui Liu, Xiangrui Xu, and Wei Wang.

</span>
<span class="ltx_bibblock">Threats, attacks and defenses to federated learning: issues, taxonomy
and perspectives.

</span>
<span class="ltx_bibblock"><span id="bib.bib163.1.1" class="ltx_text ltx_font_italic">Cybersecurity</span>, 5(1):1–19, 2022.

</span>
</li>
<li id="bib.bib164" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock">
Chengliang Zhang, Suyi Li, Junzhe Xia, Wei Wang, Feng Yan, and Yang Liu.

</span>
<span class="ltx_bibblock">Batchcrypt: Efficient homomorphic encryption for cross-silo federated
learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib164.1.1" class="ltx_text ltx_font_italic">Proceedings of the 2020 USENIX Annual Technical Conference
(USENIX ATC 2020)</span>, 2020.

</span>
</li>
<li id="bib.bib165" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock">
Stephen Hardy, Wilko Henecka, Hamish Ivey-Law, Richard Nock, Giorgio Patrini,
Guillaume Smith, and Brian Thorne.

</span>
<span class="ltx_bibblock">Private federated learning on vertically partitioned data via entity
resolution and additively homomorphic encryption.

</span>
<span class="ltx_bibblock"><span id="bib.bib165.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1711.10677</span>, 2017.

</span>
</li>
<li id="bib.bib166" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock">
Poongodi Manoharan, Ranjan Walia, Celestine Iwendi, Tariq Ahamed Ahanger,
ST Suganthi, MM Kamruzzaman, Sami Bourouis, Wajdi Alhakami, and Mounir Hamdi.

</span>
<span class="ltx_bibblock">Svm-based generative adverserial networks for federated learning and
edge computing attack model and outpoising.

</span>
<span class="ltx_bibblock"><span id="bib.bib166.1.1" class="ltx_text ltx_font_italic">Expert Systems</span>, page e13072, 2022.

</span>
</li>
<li id="bib.bib167" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock">
Jiale Zhang, Bing Chen, Xiang Cheng, Huynh Thi Thanh Binh, and Shui Yu.

</span>
<span class="ltx_bibblock">Poisongan: Generative poisoning attacks against federated learning in
edge computing systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib167.1.1" class="ltx_text ltx_font_italic">IEEE Internet of Things Journal</span>, 8(5):3310–3322, 2020.

</span>
</li>
<li id="bib.bib168" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock">
Ishai Rosenberg, Asaf Shabtai, Yuval Elovici, and Lior Rokach.

</span>
<span class="ltx_bibblock">Adversarial machine learning attacks and defense methods in the cyber
security domain.

</span>
<span class="ltx_bibblock"><span id="bib.bib168.1.1" class="ltx_text ltx_font_italic">ACM Computing Surveys (CSUR)</span>, 54(5):1–36, 2021.

</span>
</li>
<li id="bib.bib169" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock">
Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard Marin, Diego Perino, and
Nicolas Kourtellis.

</span>
<span class="ltx_bibblock">Ppfl: privacy-preserving federated learning with trusted execution
environments.

</span>
<span class="ltx_bibblock">In <span id="bib.bib169.1.1" class="ltx_text ltx_font_italic">Proceedings of the 19th annual international conference on
mobile systems, applications, and services</span>, pages 94–108, 2021.

</span>
</li>
<li id="bib.bib170" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock">
Yu Chen, Fang Luo, Tong Li, Tao Xiang, Zheli Liu, and Jin Li.

</span>
<span class="ltx_bibblock">A training-integrity privacy-preserving federated learning scheme
with trusted execution environment.

</span>
<span class="ltx_bibblock"><span id="bib.bib170.1.1" class="ltx_text ltx_font_italic">Information Sciences</span>, 522:69–79, 2020.

</span>
</li>
<li id="bib.bib171" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock">
Vaikkunth Mugunthan, Antigoni Polychroniadou, David Byrd, and Tucker Hybinette
Balch.

</span>
<span class="ltx_bibblock">Smpai: Secure multi-party computation for federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib171.1.1" class="ltx_text ltx_font_italic">Proceedings of the NeurIPS 2019 Workshop on Robust AI in
Financial Services</span>, 2019.

</span>
</li>
<li id="bib.bib172" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock">
Yunlong Lu, Xiaohong Huang, Yueyue Dai, Sabita Maharjan, and Yan Zhang.

</span>
<span class="ltx_bibblock">Blockchain and federated learning for privacy-preserved data sharing
in industrial iot.

</span>
<span class="ltx_bibblock"><span id="bib.bib172.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Industrial Informatics</span>, 16(6):4177–4186,
2019.

</span>
</li>
<li id="bib.bib173" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock">
Muhammad Saad, Jeffrey Spaulding, Laurent Njilla, Charles Kamhoua, Sachin
Shetty, DaeHun Nyang, and David Mohaisen.

</span>
<span class="ltx_bibblock">Exploring the attack surface of blockchain: A comprehensive survey.

</span>
<span class="ltx_bibblock"><span id="bib.bib173.1.1" class="ltx_text ltx_font_italic">IEEE Communications Surveys &amp; Tutorials</span>, 22(3):1977–2008,
2020.

</span>
</li>
<li id="bib.bib174" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock">
Yourong Chen, Hao Chen, Yang Zhang, Meng Han, Madhuri Siddula, and Zhipeng Cai.

</span>
<span class="ltx_bibblock">A survey on blockchain systems: Attacks, defenses, and privacy
preservation.

</span>
<span class="ltx_bibblock"><span id="bib.bib174.1.1" class="ltx_text ltx_font_italic">High-Confidence Computing</span>, 2(2):100048, 2022.

</span>
</li>
<li id="bib.bib175" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock">
Olivia Choudhury, Aris Gkoulalas-Divanis, Theodoros Salonidis, Issa Sylla,
Yoonyoung Park, Grace Hsu, and Amar Das.

</span>
<span class="ltx_bibblock">Anonymizing data for privacy-preserving federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib175.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2002.09096</span>, 2020.

</span>
</li>
<li id="bib.bib176" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock">
Suyi Li, Yong Cheng, Yang Liu, Wei Wang, and Tianjian Chen.

</span>
<span class="ltx_bibblock">Abnormal client behavior detection in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib176.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1910.09933</span>, 2019.

</span>
</li>
<li id="bib.bib177" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock">
Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
Shmatikov.

</span>
<span class="ltx_bibblock">How to backdoor federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib177.1.1" class="ltx_text ltx_font_italic">International Conference on Artificial Intelligence and
Statistics</span>, pages 2938–2948. PMLR, 2020.

</span>
</li>
<li id="bib.bib178" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock">
Xueluan Gong, Yanjiao Chen, Qian Wang, and Weihan Kong.

</span>
<span class="ltx_bibblock">Backdoor attacks and defenses in federated learning:
State-of-the-art, taxonomy, and future directions.

</span>
<span class="ltx_bibblock"><span id="bib.bib178.1.1" class="ltx_text ltx_font_italic">IEEE Wireless Communications</span>, 2022.

</span>
</li>
<li id="bib.bib179" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock">
Maria Rigaki and Sebastian Garcia.

</span>
<span class="ltx_bibblock">A survey of privacy attacks in machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib179.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2007.07646</span>, 2020.

</span>
</li>
<li id="bib.bib180" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock">
Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild,
Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein.

</span>
<span class="ltx_bibblock">Dataset security for machine learning: Data poisoning, backdoor
attacks, and defenses.

</span>
<span class="ltx_bibblock"><span id="bib.bib180.1.1" class="ltx_text ltx_font_italic">IEEE Transactions on Pattern Analysis and Machine Intelligence</span>,
45(2):1563–1580, 2022.

</span>
</li>
<li id="bib.bib181" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock">
Ying Zhao, Junjun Chen, Jiale Zhang, Di Wu, Jian Teng, and Shui Yu.

</span>
<span class="ltx_bibblock">Pdgan: A novel poisoning defense method in federated learning using
generative adversarial network.

</span>
<span class="ltx_bibblock">In <span id="bib.bib181.1.1" class="ltx_text ltx_font_italic">Algorithms and Architectures for Parallel Processing: 19th
International Conference, ICA3PP</span>, page 595–609, 2019.

</span>
</li>
<li id="bib.bib182" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock">
Caroline Fontaine and Fabien Galand.

</span>
<span class="ltx_bibblock">A survey of homomorphic encryption for nonspecialists.

</span>
<span class="ltx_bibblock"><span id="bib.bib182.1.1" class="ltx_text ltx_font_italic">EURASIP Journal on Information Security</span>, 2007:1–10, 2007.

</span>
</li>
<li id="bib.bib183" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock">
Tensorflow.

</span>
<span class="ltx_bibblock">Federated learning.

</span>
<span class="ltx_bibblock">Available at
<a target="_blank" href="https://www.tensorflow.org/federated/federated_learning" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated/federated_learning</a>, Last accessed
09.01.2022.

</span>
</li>
<li id="bib.bib184" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock">
Keras.

</span>
<span class="ltx_bibblock">Federated learning for image classification.

</span>
<span class="ltx_bibblock">Available at <a target="_blank" href="https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated/tutorials/federated_learning_for_image_classification</a>, Last accessed 09.01.2022.

</span>
</li>
<li id="bib.bib185" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock">
Heiko Ludwig, Nathalie Baracaldo, Gegi Thomas, Yi Zhou, Ali Anwar, Shashank
Rajamoni, Yuya Jeremy Ong, Jayaram Radhakrishnan, Ashish Verma, Mathieu Sinn,
Mark Purcell, Ambrish Rawat, Tran Ngoc Minh, Naoise Holohan, Supriyo
Chakraborty, Shalisha Witherspoon, Dean Steuer, Laura Wynter, Hifaz Hassan,
Sean Laguna, Mikhail Yurochkin, Mayank Agarwal, Ebube Chuba, and Annie Abay.

</span>
<span class="ltx_bibblock">IBM federated learning: an enterprise framework white paper V0.1.

</span>
<span class="ltx_bibblock"><span id="bib.bib185.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/2007.10987, 2020.

</span>
</li>
<li id="bib.bib186" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock">
Holger R. Roth, Yan Cheng, Yuhong Wen, Isaac Yang, Ziyue Xu, Yuan-Ting Hsieh,
Kristopher Kersten, Ahmed Harouni, Can Zhao, Kevin Lu, Zhihong Zhang, Wenqi
Li, Andriy Myronenko, Dong Yang, Sean Yang, Nicola Rieke, Abood Quraini,
Chester Chen, Daguang Xu, Nic Ma, Prerna Dogra, Mona Flores, and Andrew Feng.

</span>
<span class="ltx_bibblock">Nvidia flare: Federated learning from simulation to real-world, 2022.

</span>
</li>
<li id="bib.bib187" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock">
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang,
Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, Peilin Zhao, Yan Kang,
Yang Liu, Ramesh Raskar, Qiang Yang, Murali Annavaram, and Salman Avestimehr.

</span>
<span class="ltx_bibblock">Fedml: A research library and benchmark for federated machine
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib187.1.1" class="ltx_text ltx_font_italic">Advances in Neural Information Processing Systems, Best Paper
Award at Federate Learning Workshop</span>, 2020.

</span>
</li>
<li id="bib.bib188" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock">
Yang Liu, Tao Fan, Tianjian Chen, Qian Xu, and Qiang Yang.

</span>
<span class="ltx_bibblock">Fate: An industrial grade platform for collaborative learning with
data protection.

</span>
<span class="ltx_bibblock"><span id="bib.bib188.1.1" class="ltx_text ltx_font_italic">J. Mach. Learn. Res.</span>, 22(1), jul 2022.

</span>
</li>
<li id="bib.bib189" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock">
Alexander Ziller, Andrew Trask, Antonio Lopardo, Benjamin Szymkow, Bobby
Wagner, Emma Bluemke, Jean-Mickael Nounahon, Jonathan Passerat-Palmbach,
Kritika Prakash, Nick Rose, Théo Ryffel, Zarreen Naowal Reza, and
Georgios Kaissis.

</span>
<span class="ltx_bibblock"><span id="bib.bib189.1.1" class="ltx_text ltx_font_italic">PySyft: A Library for Easy Federated Learning</span>, pages 111–139.

</span>
<span class="ltx_bibblock">Springer International Publishing, Cham, 2021.

</span>
</li>
<li id="bib.bib190" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock">
Patrick Foley, Micah J Sheller, Brandon Edwards, Sarthak Pati, Walter Riviera,
Mansi Sharma, Prakash Narayana Moorthy, Shi-han Wang, Jason Martin, Parsa
Mirhaji, Prashant Shah, and Spyridon Bakas.

</span>
<span class="ltx_bibblock">Openfl: the open federated learning library.

</span>
<span class="ltx_bibblock"><span id="bib.bib190.1.1" class="ltx_text ltx_font_italic">Physics in Medicine &amp; Biology</span>, 2022.

</span>
</li>
<li id="bib.bib191" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock">
Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný,
H. Brendan McMahan, Virginia Smith, and Ameet Talwalkar.

</span>
<span class="ltx_bibblock">Leaf: A benchmark for federated settings, 2018.

</span>
</li>
<li id="bib.bib192" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock">
Tensorflow.

</span>
<span class="ltx_bibblock">Datasets for running tensorflow federated simulations.

</span>
<span class="ltx_bibblock">Available at
<a target="_blank" href="https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.tensorflow.org/federated/api_docs/python/tff/simulation/datasets</a>,
Last accessed 02.21.2022.

</span>
</li>
<li id="bib.bib193" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock">
Jiahuan Luo, Xueyang Wu, Yun Luo, Anbu Huang, Yunfeng Huang, Yang Liu, and
Qiang Yang.

</span>
<span class="ltx_bibblock">Real-world image datasets for federated learning, 2019.

</span>
</li>
<li id="bib.bib194" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock">
Rajesh Kumar, Abdullah Aman Khan, Jay Kumar, Noorbakhsh Amiri Golilarz, Simin
Zhang, Yang Ting, Chengyu Zheng, Wenyong Wang, et al.

</span>
<span class="ltx_bibblock">Blockchain-federated-learning and deep learning models for covid-19
detection using ct imaging.

</span>
<span class="ltx_bibblock"><span id="bib.bib194.1.1" class="ltx_text ltx_font_italic">IEEE Sensors Journal</span>, 21(14):16301–16314, 2021.

</span>
</li>
<li id="bib.bib195" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock">
Fanxing Liu, Cheng Zeng, Le Zhang, Yingjie Zhou, Qing Mu, Yanru Zhang, Ling
Zhang, and Ce Zhu.

</span>
<span class="ltx_bibblock">Fedtadbench: Federated time-series anomaly detection benchmark, 2022.

</span>
</li>
<li id="bib.bib196" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock">
Nikolaos Pitropakis, Emmanouil Panaousis, Thanassis Giannetsos, Eleftherios
Anastasiadis, and George Loukas.

</span>
<span class="ltx_bibblock">A taxonomy and survey of attacks against machine learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib196.1.1" class="ltx_text ltx_font_italic">Computer Science Review</span>, 34:100199, 2019.

</span>
</li>
<li id="bib.bib197" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock">
Mingfu Xue, Chengxiang Yuan, Heyi Wu, Yushu Zhang, and Weiqiang Liu.

</span>
<span class="ltx_bibblock">Machine learning security: Threats, countermeasures, and evaluations.

</span>
<span class="ltx_bibblock"><span id="bib.bib197.1.1" class="ltx_text ltx_font_italic">IEEE Access</span>, 8:74720–74742, 2020.

</span>
</li>
<li id="bib.bib198" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock">
Salem Alqahtani and Murat Demirbas.

</span>
<span class="ltx_bibblock">Performance analysis and comparison of distributed machine learning
systems.

</span>
<span class="ltx_bibblock"><span id="bib.bib198.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1909.02061</span>, 2019.

</span>
</li>
<li id="bib.bib199" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock">
Aurick Qiao, Bryon Aragam, Bingjing Zhang, and Eric Xing.

</span>
<span class="ltx_bibblock">Fault tolerance in iterative-convergent machine learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib199.1.1" class="ltx_text ltx_font_italic">International Conference on Machine Learning</span>, pages
5220–5230. PMLR, 2019.

</span>
</li>
<li id="bib.bib200" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock">
Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
Bennis, Arjun Nitin Bhagoji, Kallista A. Bonawitz, Zachary Charles, Graham
Cormode, Rachel Cummings, Rafael G. L. D’Oliveira, Salim El Rouayheb, David
Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi,
Phillip B. Gibbons, Marco Gruteser, Zaïd Harchaoui, Chaoyang He, Lie
He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi,
Gauri Joshi, Mikhail Khodak, Jakub Konečný, Aleksandra Korolova,
Farinaz Koushanfar, Sanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek
Mittal, Mehryar Mohri, Richard Nock, Ayfer Özgür, Rasmus Pagh,
Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang
Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian
Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang
Yang, Felix X. Yu, Han Yu, and Sen Zhao.

</span>
<span class="ltx_bibblock">Advances and open problems in federated learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib200.1.1" class="ltx_text ltx_font_italic">CoRR</span>, abs/1912.04977, 2019.

</span>
</li>
<li id="bib.bib201" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock">
Guan Wang.

</span>
<span class="ltx_bibblock">Interpret federated learning with shapley values.

</span>
<span class="ltx_bibblock"><span id="bib.bib201.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1905.04519</span>, 2019.

</span>
</li>
<li id="bib.bib202" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock">
Fanglan Zheng, Kun Li, Jiang Tian, Xiaojia Xiang, et al.

</span>
<span class="ltx_bibblock">A vertical federated learning method for interpretable scorecard and
its application in credit scoring.

</span>
<span class="ltx_bibblock"><span id="bib.bib202.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2009.06218</span>, 2020.

</span>
</li>
<li id="bib.bib203" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock">
Xiaolin Chen, Shuai Zhou, Bei Guan, Kai Yang, Hao Fao, Hu Wang, and Yongji
Wang.

</span>
<span class="ltx_bibblock">Fed-eini: An efficient and interpretable inference framework for
decision tree ensembles in vertical federated learning.

</span>
<span class="ltx_bibblock">In <span id="bib.bib203.1.1" class="ltx_text ltx_font_italic">2021 IEEE international conference on big data (big data)</span>,
pages 1242–1248. IEEE, 2021.

</span>
</li>
<li id="bib.bib204" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock">
Zhe Li, Honglong Chen, Zhichen Ni, and Huajie Shao.

</span>
<span class="ltx_bibblock">Balancing privacy protection and interpretability in federated
learning.

</span>
<span class="ltx_bibblock"><span id="bib.bib204.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2302.08044</span>, 2023.

</span>
</li>
<li id="bib.bib205" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock">
Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Carl Yang, Han Xie, Lichao
Sun, Lifang He, Liangwei Yang, Philip S Yu, Yu Rong, et al.

</span>
<span class="ltx_bibblock">Fedgraphnn: A federated learning system and benchmark for graph
neural networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib205.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2104.07145</span>, 2021.

</span>
</li>
<li id="bib.bib206" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock">
Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Murali Annavaram, and Salman
Avestimehr.

</span>
<span class="ltx_bibblock">Spreadgnn: Serverless multi-task federated learning for graph neural
networks.

</span>
<span class="ltx_bibblock"><span id="bib.bib206.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2106.02743</span>, 2021.

</span>
</li>
<li id="bib.bib207" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock">
Binghui Wang, Ang Li, Meng Pang, Hai Li, and Yiran Chen.

</span>
<span class="ltx_bibblock">Graphfl: A federated learning framework for semi-supervised node
classification on graphs.

</span>
<span class="ltx_bibblock">In <span id="bib.bib207.1.1" class="ltx_text ltx_font_italic">2022 IEEE International Conference on Data Mining (ICDM)</span>,
pages 498–507. IEEE, 2022.

</span>
</li>
<li id="bib.bib208" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock">
Rui Liu and Han Yu.

</span>
<span class="ltx_bibblock">Federated graph neural networks: Overview, techniques and challenges.

</span>
<span class="ltx_bibblock"><span id="bib.bib208.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:2202.07256</span>, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2312.03119" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2312.03120" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2312.03120">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2312.03120" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2312.03121" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 15:06:40 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
