<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2404.10378] Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data</title><meta property="og:description" content="Synthetic data is gaining increasing relevance for training machine learning models. This is mainly motivated due to several factors such as the lack of real data and intra-class variability, time and errors produced i…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2404.10378">

<!--Generated on Sun May  5 15:42:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Second Edition FRCSyn Challenge at CVPR 2024:
<br class="ltx_break">Face Recognition Challenge in the Era of Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ivan DeAndres-Tame<sup id="id88.2.id1" class="ltx_sup"><span id="id88.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruben Tolosana<sup id="id89.2.id1" class="ltx_sup"><span id="id89.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pietro Melzi<sup id="id90.2.id1" class="ltx_sup"><span id="id90.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ruben Vera-Rodriguez<sup id="id91.2.id1" class="ltx_sup"><span id="id91.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Minchul Kim<sup id="id92.2.id1" class="ltx_sup"><span id="id92.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christian Rathgeb<sup id="id93.2.id1" class="ltx_sup"><span id="id93.2.id1.1" class="ltx_text ltx_font_italic">3</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaoming Liu<sup id="id94.2.id1" class="ltx_sup"><span id="id94.2.id1.1" class="ltx_text ltx_font_italic">2</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aythami Morales<sup id="id95.2.id1" class="ltx_sup"><span id="id95.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julian Fierrez<sup id="id96.2.id1" class="ltx_sup"><span id="id96.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Javier Ortega-Garcia<sup id="id97.2.id1" class="ltx_sup"><span id="id97.2.id1.1" class="ltx_text ltx_font_italic">1</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhizhou Zhong<sup id="id98.2.id1" class="ltx_sup"><span id="id98.2.id1.1" class="ltx_text ltx_font_italic">4</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuge Huang<sup id="id99.2.id1" class="ltx_sup"><span id="id99.2.id1.1" class="ltx_text ltx_font_italic">5</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuxi Mi<sup id="id100.2.id1" class="ltx_sup"><span id="id100.2.id1.1" class="ltx_text ltx_font_italic">4</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shouhong Ding<sup id="id101.2.id1" class="ltx_sup"><span id="id101.2.id1.1" class="ltx_text ltx_font_italic">5</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuigeng Zhou<sup id="id102.2.id1" class="ltx_sup"><span id="id102.2.id1.1" class="ltx_text ltx_font_italic">4</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuai He<sup id="id103.2.id1" class="ltx_sup"><span id="id103.2.id1.1" class="ltx_text ltx_font_italic">6</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Lingzhi Fu<sup id="id104.2.id1" class="ltx_sup"><span id="id104.2.id1.1" class="ltx_text ltx_font_italic">6</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Heng Cong<sup id="id105.2.id1" class="ltx_sup"><span id="id105.2.id1.1" class="ltx_text ltx_font_italic">6</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rongyu Zhang<sup id="id106.2.id1" class="ltx_sup"><span id="id106.2.id1.1" class="ltx_text ltx_font_italic">6</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhihong Xiao<sup id="id107.2.id1" class="ltx_sup"><span id="id107.2.id1.1" class="ltx_text ltx_font_italic">6</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Evgeny Smirnov<sup id="id108.2.id1" class="ltx_sup"><span id="id108.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anton Pimenov<sup id="id109.2.id1" class="ltx_sup"><span id="id109.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aleksei Grigorev<sup id="id110.2.id1" class="ltx_sup"><span id="id110.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Denis Timoshenko<sup id="id111.2.id1" class="ltx_sup"><span id="id111.2.id1.1" class="ltx_text ltx_font_italic">7</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kaleb Mesfin Asfaw<sup id="id112.2.id1" class="ltx_sup"><span id="id112.2.id1.1" class="ltx_text ltx_font_italic">8</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cheng Yaw Low<sup id="id113.2.id1" class="ltx_sup"><span id="id113.2.id1.1" class="ltx_text ltx_font_italic">9</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hao Liu<sup id="id114.2.id1" class="ltx_sup"><span id="id114.2.id1.1" class="ltx_text ltx_font_italic">10</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chuyi Wang<sup id="id115.2.id1" class="ltx_sup"><span id="id115.2.id1.1" class="ltx_text ltx_font_italic">10</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qing Zuo<sup id="id116.2.id1" class="ltx_sup"><span id="id116.2.id1.1" class="ltx_text ltx_font_italic">10</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhixiang He<sup id="id117.2.id1" class="ltx_sup"><span id="id117.2.id1.1" class="ltx_text ltx_font_italic">10</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hatef Otroshi Shahreza<sup id="id118.2.id1" class="ltx_sup"><span id="id118.2.id1.1" class="ltx_text ltx_font_italic">11,12</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Anjith George<sup id="id119.2.id1" class="ltx_sup"><span id="id119.2.id1.1" class="ltx_text ltx_font_italic">11</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alexander Unnervik<sup id="id120.2.id1" class="ltx_sup"><span id="id120.2.id1.1" class="ltx_text ltx_font_italic">11,12</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Parsa Rahimi<sup id="id121.2.id1" class="ltx_sup"><span id="id121.2.id1.1" class="ltx_text ltx_font_italic">11,12</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sébastien Marcel<sup id="id122.2.id1" class="ltx_sup"><span id="id122.2.id1.1" class="ltx_text ltx_font_italic">11,13</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pedro C. Neto<sup id="id123.2.id1" class="ltx_sup"><span id="id123.2.id1.1" class="ltx_text ltx_font_italic">14,15</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Marco Huber<sup id="id124.2.id1" class="ltx_sup"><span id="id124.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jan Niklas Kolf<sup id="id125.2.id1" class="ltx_sup"><span id="id125.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Naser Damer<sup id="id126.2.id1" class="ltx_sup"><span id="id126.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Fadi Boutros<sup id="id127.2.id1" class="ltx_sup"><span id="id127.2.id1.1" class="ltx_text ltx_font_italic">16</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jaime S. Cardoso<sup id="id128.2.id1" class="ltx_sup"><span id="id128.2.id1.1" class="ltx_text ltx_font_italic">14,15</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ana F. Sequeira<sup id="id129.2.id1" class="ltx_sup"><span id="id129.2.id1.1" class="ltx_text ltx_font_italic">14,15</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrea Atzori<sup id="id130.2.id1" class="ltx_sup"><span id="id130.2.id1.1" class="ltx_text ltx_font_italic">17</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Gianni Fenu<sup id="id131.2.id1" class="ltx_sup"><span id="id131.2.id1.1" class="ltx_text ltx_font_italic">17</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mirko Marras<sup id="id132.2.id1" class="ltx_sup"><span id="id132.2.id1.1" class="ltx_text ltx_font_italic">17</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Vitomir Štruc<sup id="id133.2.id1" class="ltx_sup"><span id="id133.2.id1.1" class="ltx_text ltx_font_italic">18</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiang Yu<sup id="id134.2.id1" class="ltx_sup"><span id="id134.2.id1.1" class="ltx_text ltx_font_italic">19</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhangjie Li<sup id="id135.2.id1" class="ltx_sup"><span id="id135.2.id1.1" class="ltx_text ltx_font_italic">19,20</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jichun Li<sup id="id136.2.id1" class="ltx_sup"><span id="id136.2.id1.1" class="ltx_text ltx_font_italic">19</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Weisong Zhao<sup id="id137.2.id1" class="ltx_sup"><span id="id137.2.id1.1" class="ltx_text ltx_font_italic">21</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhen Lei<sup id="id138.2.id1" class="ltx_sup"><span id="id138.2.id1.1" class="ltx_text ltx_font_italic">22</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiangyu Zhu<sup id="id139.2.id1" class="ltx_sup"><span id="id139.2.id1.1" class="ltx_text ltx_font_italic">22</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiao-Yu Zhang<sup id="id140.2.id1" class="ltx_sup"><span id="id140.2.id1.1" class="ltx_text ltx_font_italic">21</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bernardo Biesseck<sup id="id141.2.id1" class="ltx_sup"><span id="id141.2.id1.1" class="ltx_text ltx_font_italic">23,24</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pedro Vidal<sup id="id142.2.id1" class="ltx_sup"><span id="id142.2.id1.1" class="ltx_text ltx_font_italic">23</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Luiz Coelho<sup id="id143.2.id1" class="ltx_sup"><span id="id143.2.id1.1" class="ltx_text ltx_font_italic">25</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Roger Granada<sup id="id144.2.id1" class="ltx_sup"><span id="id144.2.id1.1" class="ltx_text ltx_font_italic">25</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">David Menotti<sup id="id145.2.id1" class="ltx_sup"><span id="id145.2.id1.1" class="ltx_text ltx_font_italic">23</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><sup id="id146.26.id1" class="ltx_sup"><span id="id146.26.id1.1" class="ltx_text ltx_font_italic">1</span></sup>Universidad Autonoma de Madrid, Spain
<sup id="id147.27.id2" class="ltx_sup"><span id="id147.27.id2.1" class="ltx_text ltx_font_italic">2</span></sup>Michigan State University, USA
<sup id="id148.28.id3" class="ltx_sup"><span id="id148.28.id3.1" class="ltx_text ltx_font_italic">3</span></sup>Hochschule Darmstadt, Germany
<br class="ltx_break"><sup id="id149.29.id4" class="ltx_sup"><span id="id149.29.id4.1" class="ltx_text ltx_font_italic">4</span></sup>Fudan University, China
<sup id="id150.30.id5" class="ltx_sup"><span id="id150.30.id5.1" class="ltx_text ltx_font_italic">5</span></sup>Tencent Youtu Lab, China
<sup id="id151.31.id6" class="ltx_sup"><span id="id151.31.id6.1" class="ltx_text ltx_font_italic">6</span></sup>Interactive Entertainment Group of Netease Inc, China
<br class="ltx_break"><sup id="id152.32.id7" class="ltx_sup"><span id="id152.32.id7.1" class="ltx_text ltx_font_italic">7</span></sup>ID R&amp;D Inc., USA
<sup id="id153.33.id8" class="ltx_sup"><span id="id153.33.id8.1" class="ltx_text ltx_font_italic">8</span></sup>Korea Advanced Institute of Science &amp; Technology, Korea
<sup id="id154.34.id9" class="ltx_sup"><span id="id154.34.id9.1" class="ltx_text ltx_font_italic">9</span></sup>Institute for Basic Science, Korea
<br class="ltx_break"><sup id="id155.35.id10" class="ltx_sup"><span id="id155.35.id10.1" class="ltx_text ltx_font_italic">10</span></sup>China Telecom AI, China
<sup id="id156.36.id11" class="ltx_sup"><span id="id156.36.id11.1" class="ltx_text ltx_font_italic">11</span></sup>Idiap Research Institute, Switzerland
<sup id="id157.37.id12" class="ltx_sup"><span id="id157.37.id12.1" class="ltx_text ltx_font_italic">12</span></sup>EPFL, Switzerland
<sup id="id158.38.id13" class="ltx_sup"><span id="id158.38.id13.1" class="ltx_text ltx_font_italic">13</span></sup>Université de Lausanne, Switzerland
<br class="ltx_break"><sup id="id159.39.id14" class="ltx_sup"><span id="id159.39.id14.1" class="ltx_text ltx_font_italic">14</span></sup>INESC TEC, Portugal
<sup id="id160.40.id15" class="ltx_sup"><span id="id160.40.id15.1" class="ltx_text ltx_font_italic">15</span></sup>Universidade do Porto, Portugal
<sup id="id161.41.id16" class="ltx_sup"><span id="id161.41.id16.1" class="ltx_text ltx_font_italic">16</span></sup>Fraunhofer IGD, Germany
<sup id="id162.42.id17" class="ltx_sup"><span id="id162.42.id17.1" class="ltx_text ltx_font_italic">17</span></sup>University of Cagliari, Italy
<br class="ltx_break"><sup id="id163.43.id18" class="ltx_sup"><span id="id163.43.id18.1" class="ltx_text ltx_font_italic">18</span></sup>University of Ljubljana, Slovenia
<sup id="id164.44.id19" class="ltx_sup"><span id="id164.44.id19.1" class="ltx_text ltx_font_italic">19</span></sup>Samsung Electronics (China) R&amp;D Centre, China
<br class="ltx_break"><sup id="id165.45.id20" class="ltx_sup"><span id="id165.45.id20.1" class="ltx_text ltx_font_italic">20</span></sup>University of Science and Technology, China
<sup id="id166.46.id21" class="ltx_sup"><span id="id166.46.id21.1" class="ltx_text ltx_font_italic">21</span></sup>IIE, CAS, China
<sup id="id167.47.id22" class="ltx_sup"><span id="id167.47.id22.1" class="ltx_text ltx_font_italic">22</span></sup>MAIS, CASIA, China
<br class="ltx_break"><sup id="id168.48.id23" class="ltx_sup"><span id="id168.48.id23.1" class="ltx_text ltx_font_italic">23</span></sup>Federal University of Paraná, Brazil
<sup id="id169.49.id24" class="ltx_sup"><span id="id169.49.id24.1" class="ltx_text ltx_font_italic">24</span></sup>Federal Institute of Mato Grosso, Brazil
<sup id="id170.50.id25" class="ltx_sup"><span id="id170.50.id25.1" class="ltx_text ltx_font_italic">25</span></sup>unico - idTech, Brazil
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id87.4" class="ltx_p">Synthetic data is gaining increasing relevance for training machine learning models. This is mainly motivated due to several factors such as the lack of real data and intra-class variability, time and errors produced in manual labeling, and in some cases privacy concerns, among others. This paper presents an overview of the 2<math id="id84.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="id84.1.m1.1a"><msup id="id84.1.m1.1.1" xref="id84.1.m1.1.1.cmml"><mi id="id84.1.m1.1.1a" xref="id84.1.m1.1.1.cmml"></mi><mtext id="id84.1.m1.1.1.1" xref="id84.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="id84.1.m1.1b"><apply id="id84.1.m1.1.1.cmml" xref="id84.1.m1.1.1"><ci id="id84.1.m1.1.1.1a.cmml" xref="id84.1.m1.1.1.1"><mtext mathsize="70%" id="id84.1.m1.1.1.1.cmml" xref="id84.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id84.1.m1.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) organized at CVPR 2024. FRCSyn aims to investigate the use of synthetic data in face recognition to address current technological limitations, including data privacy concerns, demographic biases, generalization to novel scenarios, and performance constraints in challenging situations such as aging, pose variations, and occlusions. Unlike the 1<math id="id85.2.m2.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="id85.2.m2.1a"><msup id="id85.2.m2.1.1" xref="id85.2.m2.1.1.cmml"><mi id="id85.2.m2.1.1a" xref="id85.2.m2.1.1.cmml"></mi><mtext id="id85.2.m2.1.1.1" xref="id85.2.m2.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="id85.2.m2.1b"><apply id="id85.2.m2.1.1.cmml" xref="id85.2.m2.1.1"><ci id="id85.2.m2.1.1.1a.cmml" xref="id85.2.m2.1.1.1"><mtext mathsize="70%" id="id85.2.m2.1.1.1.cmml" xref="id85.2.m2.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id85.2.m2.1c">{}^{\text{st}}</annotation></semantics></math> edition, in which synthetic data from DCFace and GANDiffFace methods was only allowed to train face recognition systems, in this 2<math id="id86.3.m3.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="id86.3.m3.1a"><msup id="id86.3.m3.1.1" xref="id86.3.m3.1.1.cmml"><mi id="id86.3.m3.1.1a" xref="id86.3.m3.1.1.cmml"></mi><mtext id="id86.3.m3.1.1.1" xref="id86.3.m3.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="id86.3.m3.1b"><apply id="id86.3.m3.1.1.cmml" xref="id86.3.m3.1.1"><ci id="id86.3.m3.1.1.1a.cmml" xref="id86.3.m3.1.1.1"><mtext mathsize="70%" id="id86.3.m3.1.1.1.cmml" xref="id86.3.m3.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id86.3.m3.1c">{}^{\text{nd}}</annotation></semantics></math> edition we propose new sub-tasks that allow participants to explore novel face generative methods. The outcomes of the 2<math id="id87.4.m4.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="id87.4.m4.1a"><msup id="id87.4.m4.1.1" xref="id87.4.m4.1.1.cmml"><mi id="id87.4.m4.1.1a" xref="id87.4.m4.1.1.cmml"></mi><mtext id="id87.4.m4.1.1.1" xref="id87.4.m4.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="id87.4.m4.1b"><apply id="id87.4.m4.1.1.cmml" xref="id87.4.m4.1.1"><ci id="id87.4.m4.1.1.1a.cmml" xref="id87.4.m4.1.1.1"><mtext mathsize="70%" id="id87.4.m4.1.1.1.cmml" xref="id87.4.m4.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="id87.4.m4.1c">{}^{\text{nd}}</annotation></semantics></math> FRCSyn Challenge, along with the proposed experimental protocol and benchmarking contribute significantly to the application of synthetic data to face recognition.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2404.10378/assets/Img/gandiff_examples.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="556" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Examples of synthetic identities and variations for different demographic groups using GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>.</span></figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Face biometrics is a very popular area in the fields of Computer Vision and Pattern Recognition, finding applications in diverse domains such as person recognition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">47</span></a>, <a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">13</span></a>]</cite>, healthcare <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">4</span></a>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">17</span></a>]</cite>, or e-learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">10</span></a>]</cite>, among others. In the last years, with the rapid evolution of deep learning, we have witnessed a considerable performance improvement in areas such as face recognition (FR) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, outperforming the state-of-the-art on established benchmarks. However, FR technology has still room for improvement in several research directions, such as explainability <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">11</span></a>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">9</span></a>]</cite>, demographic bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">42</span></a>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">28</span></a>]</cite>, privacy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">32</span></a>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">29</span></a>]</cite>, and robustness against challenging conditions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>, <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">e.g.,</span> aging, pose variations, illumination, occlusions, etc.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Synthetic data has recently appeared as a good solution to mitigate some of these drawbacks, allowing the generation of <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">i)</span> a huge number of facial images from different non-existent identities, and <span id="S1.p2.1.2" class="ltx_text ltx_font_italic">ii)</span> variability in terms of demographic attributes and scenario conditions. Several approaches have been proposed in the last couple of years for the synthesis of face images, considering state-of-the-art deep learning methods such as Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, Diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>, <a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, the combination of GAN and Diffusion models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, or alternative methods <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>. Examples of synthetic face images generated using GANDiffFace are shown in Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.3" class="ltx_p">However, beyond the generation of novel and realistic synthetic faces, a critical aspect lies in the possible application and benefits of synthetic data to better train FR technology. Recent preliminary studies in the literature have shown the existence of a performance gap between FR systems trained solely on synthetic data and those trained on real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>. Nevertheless, the results achieved in the 1<math id="S1.p3.1.m1.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S1.p3.1.m1.1a"><msup id="S1.p3.1.m1.1.1" xref="S1.p3.1.m1.1.1.cmml"><mi id="S1.p3.1.m1.1.1a" xref="S1.p3.1.m1.1.1.cmml"></mi><mtext id="S1.p3.1.m1.1.1.1" xref="S1.p3.1.m1.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p3.1.m1.1b"><apply id="S1.p3.1.m1.1.1.cmml" xref="S1.p3.1.m1.1.1"><ci id="S1.p3.1.m1.1.1.1a.cmml" xref="S1.p3.1.m1.1.1.1"><mtext mathsize="70%" id="S1.p3.1.m1.1.1.1.cmml" xref="S1.p3.1.m1.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.1.m1.1c">{}^{\text{st}}</annotation></semantics></math> edition of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) demonstrate the importance of using synthetic data by itself or in combination with real data to mitigate challenges in FR such as demographic bias <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>. It is important to highlight that in the 1<math id="S1.p3.2.m2.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S1.p3.2.m2.1a"><msup id="S1.p3.2.m2.1.1" xref="S1.p3.2.m2.1.1.cmml"><mi id="S1.p3.2.m2.1.1a" xref="S1.p3.2.m2.1.1.cmml"></mi><mtext id="S1.p3.2.m2.1.1.1" xref="S1.p3.2.m2.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p3.2.m2.1b"><apply id="S1.p3.2.m2.1.1.cmml" xref="S1.p3.2.m2.1.1"><ci id="S1.p3.2.m2.1.1.1a.cmml" xref="S1.p3.2.m2.1.1.1"><mtext mathsize="70%" id="S1.p3.2.m2.1.1.1.cmml" xref="S1.p3.2.m2.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.2.m2.1c">{}^{\text{st}}</annotation></semantics></math> edition of the FRCSyn Challenge, only synthetic data from DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> methods was allowed to train FR systems. In addition to novel generative methods, another possible improvement of the FR technology could be related to the specific design and training process, taking into account the domain gap between real and synthetic data in some scenarios. For example, we observed in the 1<math id="S1.p3.3.m3.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S1.p3.3.m3.1a"><msup id="S1.p3.3.m3.1.1" xref="S1.p3.3.m3.1.1.cmml"><mi id="S1.p3.3.m3.1.1a" xref="S1.p3.3.m3.1.1.cmml"></mi><mtext id="S1.p3.3.m3.1.1.1" xref="S1.p3.3.m3.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p3.3.m3.1b"><apply id="S1.p3.3.m3.1.1.cmml" xref="S1.p3.3.m3.1.1"><ci id="S1.p3.3.m3.1.1.1a.cmml" xref="S1.p3.3.m3.1.1.1"><mtext mathsize="70%" id="S1.p3.3.m3.1.1.1.cmml" xref="S1.p3.3.m3.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p3.3.m3.1c">{}^{\text{st}}</annotation></semantics></math> edition of the FRCSyn Challenge that the majority of the teams considered the same deep learning architectures (<span id="S1.p3.3.1" class="ltx_text ltx_font_italic">e.g.,</span> ResNet-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite>) and loss functions (<span id="S1.p3.3.2" class="ltx_text ltx_font_italic">e.g.,</span> AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>), popularly considered in FR systems trained with real data.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.3" class="ltx_p">To promote the proposal of novel face generative methods and the creation of face synthetic databases, as well as specific approaches to better train FR systems with synthetic data, we have organized the 2<math id="S1.p4.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S1.p4.1.m1.1a"><msup id="S1.p4.1.m1.1.1" xref="S1.p4.1.m1.1.1.cmml"><mi id="S1.p4.1.m1.1.1a" xref="S1.p4.1.m1.1.1.cmml"></mi><mtext id="S1.p4.1.m1.1.1.1" xref="S1.p4.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p4.1.m1.1b"><apply id="S1.p4.1.m1.1.1.cmml" xref="S1.p4.1.m1.1.1"><ci id="S1.p4.1.m1.1.1.1a.cmml" xref="S1.p4.1.m1.1.1.1"><mtext mathsize="70%" id="S1.p4.1.m1.1.1.1.cmml" xref="S1.p4.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.1.m1.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the FRCSyn Challenge as part of CVPR 2024<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://frcsyn.github.io/CVPR2024.html" title="" class="ltx_ref ltx_href">https://frcsyn.github.io/CVPR2024.html</a></span></span></span>. In this 2<math id="S1.p4.2.m2.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S1.p4.2.m2.1a"><msup id="S1.p4.2.m2.1.1" xref="S1.p4.2.m2.1.1.cmml"><mi id="S1.p4.2.m2.1.1a" xref="S1.p4.2.m2.1.1.cmml"></mi><mtext id="S1.p4.2.m2.1.1.1" xref="S1.p4.2.m2.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p4.2.m2.1b"><apply id="S1.p4.2.m2.1.1.cmml" xref="S1.p4.2.m2.1.1"><ci id="S1.p4.2.m2.1.1.1a.cmml" xref="S1.p4.2.m2.1.1.1"><mtext mathsize="70%" id="S1.p4.2.m2.1.1.1.cmml" xref="S1.p4.2.m2.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.2.m2.1c">{}^{\text{nd}}</annotation></semantics></math> edition, we introduce new sub-tasks enabling participants to train FR systems utilizing synthetic data obtained with the generative frameworks of their choice, offering more freedom compared to the 1<math id="S1.p4.3.m3.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S1.p4.3.m3.1a"><msup id="S1.p4.3.m3.1.1" xref="S1.p4.3.m3.1.1.cmml"><mi id="S1.p4.3.m3.1.1a" xref="S1.p4.3.m3.1.1.cmml"></mi><mtext id="S1.p4.3.m3.1.1.1" xref="S1.p4.3.m3.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p4.3.m3.1b"><apply id="S1.p4.3.m3.1.1.cmml" xref="S1.p4.3.m3.1.1"><ci id="S1.p4.3.m3.1.1.1a.cmml" xref="S1.p4.3.m3.1.1.1"><mtext mathsize="70%" id="S1.p4.3.m3.1.1.1.cmml" xref="S1.p4.3.m3.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p4.3.m3.1c">{}^{\text{st}}</annotation></semantics></math> edition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>. In addition, we also consider new sub-tasks featured with different experimental settings, to investigate how FR systems can be trained in both constrained and unconstrained scenarios concerning the amount of synthetic training data. The FRCSyn Challenge aims to answer the following research questions:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">What are the limits of FR technology trained only with synthetic data?</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Can the use of synthetic data be beneficial to reduce the current limitations in FR technology?</p>
</div>
</li>
</ol>
<p id="S1.p4.4" class="ltx_p">These research questions have gained significant importance, particularly after the discontinuation of popular real FR databases due to privacy concerns<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://exposing.ai/about/news/" title="" class="ltx_ref ltx_href">https://exposing.ai/about/news/</a> <span id="footnote2.1" class="ltx_text ltx_font_italic">(March, 2024)</span></span></span></span> and the introduction of new regulatory laws<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://artificialintelligenceact.eu" title="" class="ltx_ref ltx_href">https://artificialintelligenceact.eu</a> <span id="footnote3.1" class="ltx_text ltx_font_italic">(March, 2024)</span></span></span></span>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.2" class="ltx_p">The remainder of the paper is organized as follows. Section <a href="#S2" title="2 FRCSyn Challenge: Databases ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> focuses on the databases considered in this 2<math id="S1.p5.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S1.p5.1.m1.1a"><msup id="S1.p5.1.m1.1.1" xref="S1.p5.1.m1.1.1.cmml"><mi id="S1.p5.1.m1.1.1a" xref="S1.p5.1.m1.1.1.cmml"></mi><mtext id="S1.p5.1.m1.1.1.1" xref="S1.p5.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p5.1.m1.1b"><apply id="S1.p5.1.m1.1.1.cmml" xref="S1.p5.1.m1.1.1"><ci id="S1.p5.1.m1.1.1.1a.cmml" xref="S1.p5.1.m1.1.1.1"><mtext mathsize="70%" id="S1.p5.1.m1.1.1.1.cmml" xref="S1.p5.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.1.m1.1c">{}^{\text{nd}}</annotation></semantics></math> edition. Section <a href="#S3" title="3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> explains the experimental setup of the challenge, including the different tasks and sub-tasks, the experimental protocol, metrics, and restrictions. In Section <a href="#S4" title="4 FRCSyn Challenge: Systems Description ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>, we describe the approaches proposed by the top-6 participating teams. Section <a href="#S5" title="5 FRCSyn Challenge: Results ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> presents the best results achieved in the different tasks and sub-tasks of the 2<math id="S1.p5.2.m2.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S1.p5.2.m2.1a"><msup id="S1.p5.2.m2.1.1" xref="S1.p5.2.m2.1.1.cmml"><mi id="S1.p5.2.m2.1.1a" xref="S1.p5.2.m2.1.1.cmml"></mi><mtext id="S1.p5.2.m2.1.1.1" xref="S1.p5.2.m2.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S1.p5.2.m2.1b"><apply id="S1.p5.2.m2.1.1.cmml" xref="S1.p5.2.m2.1.1"><ci id="S1.p5.2.m2.1.1.1a.cmml" xref="S1.p5.2.m2.1.1.1"><mtext mathsize="70%" id="S1.p5.2.m2.1.1.1.cmml" xref="S1.p5.2.m2.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.p5.2.m2.1c">{}^{\text{nd}}</annotation></semantics></math> edition, emphasizing the key results of the challenge. Finally, in Section <a href="#S6" title="6 Conclusion ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>, we provide some conclusions, highlighting potential future research directions in the field.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>FRCSyn Challenge: Databases</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Synthetic Databases</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.3" class="ltx_p">One of the main novelties of this 2<math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><msup id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1a" xref="S2.SS1.p1.1.m1.1.1.cmml"></mi><mtext id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><ci id="S2.SS1.p1.1.m1.1.1.1a.cmml" xref="S2.SS1.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the FRCSyn Challenge is the absence of restrictions on the generative methods allowed to create synthetic data, unlike the 1<math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msup id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1a" xref="S2.SS1.p1.2.m2.1.1.cmml"></mi><mtext id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><ci id="S2.SS1.p1.2.m2.1.1.1a.cmml" xref="S2.SS1.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">{}^{\text{st}}</annotation></semantics></math> edition in which only synthetic data created using DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> and GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite> methods was allowed. As a reference, after the registration in the challenge, we provided all the participants with a list of possible state-of-the-art generative frameworks, including DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, DigiFace-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, IDiff-Face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, ID3PM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">20</span></a>]</cite>, SFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">51</span></a>]</cite>, SYNFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">36</span></a>]</cite>, and ITI-GEN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">49</span></a>]</cite>. In addition, we also motivate participants to propose novel face generative methods. In the 2<math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><msup id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mi id="S2.SS1.p1.3.m3.1.1a" xref="S2.SS1.p1.3.m3.1.1.cmml"></mi><mtext id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><ci id="S2.SS1.p1.3.m3.1.1.1a.cmml" xref="S2.SS1.p1.3.m3.1.1.1"><mtext mathsize="70%" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the FRCSyn Challenge, synthetic data is exclusively utilized in the training stage of FR technology, replicating realistic operational scenarios.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Real Databases</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.2" class="ltx_p">For the training of the FR systems (depending on the sub-task, please see Section <a href="#S3.SS1" title="3.1 Tasks ‣ 3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a> for more details), participants are allowed to use only <span id="S2.SS2.p1.2.1" class="ltx_text ltx_font_bold">CASIA-WebFace</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>. This database contains <math id="S2.SS2.p1.1.m1.2" class="ltx_Math" alttext="494,414" display="inline"><semantics id="S2.SS2.p1.1.m1.2a"><mrow id="S2.SS2.p1.1.m1.2.3.2" xref="S2.SS2.p1.1.m1.2.3.1.cmml"><mn id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">494</mn><mo id="S2.SS2.p1.1.m1.2.3.2.1" xref="S2.SS2.p1.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS2.p1.1.m1.2.2" xref="S2.SS2.p1.1.m1.2.2.cmml">414</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.2b"><list id="S2.SS2.p1.1.m1.2.3.1.cmml" xref="S2.SS2.p1.1.m1.2.3.2"><cn type="integer" id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">494</cn><cn type="integer" id="S2.SS2.p1.1.m1.2.2.cmml" xref="S2.SS2.p1.1.m1.2.2">414</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.2c">494,414</annotation></semantics></math> face images of <math id="S2.SS2.p1.2.m2.2" class="ltx_Math" alttext="10,575" display="inline"><semantics id="S2.SS2.p1.2.m2.2a"><mrow id="S2.SS2.p1.2.m2.2.3.2" xref="S2.SS2.p1.2.m2.2.3.1.cmml"><mn id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">10</mn><mo id="S2.SS2.p1.2.m2.2.3.2.1" xref="S2.SS2.p1.2.m2.2.3.1.cmml">,</mo><mn id="S2.SS2.p1.2.m2.2.2" xref="S2.SS2.p1.2.m2.2.2.cmml">575</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.2b"><list id="S2.SS2.p1.2.m2.2.3.1.cmml" xref="S2.SS2.p1.2.m2.2.3.2"><cn type="integer" id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">10</cn><cn type="integer" id="S2.SS2.p1.2.m2.2.2.cmml" xref="S2.SS2.p1.2.m2.2.2">575</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.2c">10,575</annotation></semantics></math> real identities collected from the web.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">For the final evaluation of the proposed FR systems, we use the same four real databases of the 1<math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><msup id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml"><mi id="S2.SS2.p2.1.m1.1.1a" xref="S2.SS2.p2.1.m1.1.1.cmml"></mi><mtext id="S2.SS2.p2.1.m1.1.1.1" xref="S2.SS2.p2.1.m1.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><apply id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1"><ci id="S2.SS2.p2.1.m1.1.1.1a.cmml" xref="S2.SS2.p2.1.m1.1.1.1"><mtext mathsize="70%" id="S2.SS2.p2.1.m1.1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">{}^{\text{st}}</annotation></semantics></math> edition of the FRCSyn Challenge <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>: <span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_italic">i)</span> <span id="S2.SS2.p2.1.2" class="ltx_text ltx_font_bold">BUPT-BalancedFace</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>, designed to address performance disparities across different ethnic groups; <span id="S2.SS2.p2.1.3" class="ltx_text ltx_font_italic">ii)</span> <span id="S2.SS2.p2.1.4" class="ltx_text ltx_font_bold">AgeDB</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, including facial images of the same subjects at different ages; <span id="S2.SS2.p2.1.5" class="ltx_text ltx_font_italic">iii)</span> <span id="S2.SS2.p2.1.6" class="ltx_text ltx_font_bold">CFP-FP</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, presenting facial images from subjects with great changes in pose, including both frontal and profile images; and <span id="S2.SS2.p2.1.7" class="ltx_text ltx_font_italic">iv)</span> <span id="S2.SS2.p2.1.8" class="ltx_text ltx_font_bold">ROF</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>, consisting of occluded faces with both upper and lower face occlusions.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>FRCSyn Challenge: Setup</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Tasks</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.3" class="ltx_p">Similar to the 1<math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msup id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1a" xref="S3.SS1.p1.1.m1.1.1.cmml"></mi><mtext id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><ci id="S3.SS1.p1.1.m1.1.1.1a.cmml" xref="S3.SS1.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">{}^{\text{st}}</annotation></semantics></math> edition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>, the challenge has been hosted on Codalab<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://codalab.lisn.upsaclay.fr/competitions/16970" title="" class="ltx_ref ltx_href">https://codalab.lisn.upsaclay.fr/competitions/16970</a></span></span></span>. In this 2<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msup id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1a" xref="S3.SS1.p1.2.m2.1.1.cmml"></mi><mtext id="S3.SS1.p1.2.m2.1.1.1" xref="S3.SS1.p1.2.m2.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><ci id="S3.SS1.p1.2.m2.1.1.1a.cmml" xref="S3.SS1.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">{}^{\text{nd}}</annotation></semantics></math> edition we also explore the application of synthetic data into the training of FR systems, with a specific focus on addressing two critical aspects in current FR technology: <span id="S3.SS1.p1.3.1" class="ltx_text ltx_font_italic">i)</span> mitigating demographic bias, and <span id="S3.SS1.p1.3.2" class="ltx_text ltx_font_italic">ii)</span> enhancing overall performance under challenging conditions that include variations in age and pose, the presence of occlusions, and diverse demographic groups. To investigate these two areas, we propose two distinct tasks, each comprising three sub-tasks considering different types (real/synthetic) and amounts of data for training the FR systems. Consequently, the 2<math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><msup id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1a" xref="S3.SS1.p1.3.m3.1.1.cmml"></mi><mtext id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><ci id="S3.SS1.p1.3.m3.1.1.1a.cmml" xref="S3.SS1.p1.3.m3.1.1.1"><mtext mathsize="70%" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the FRCSyn Challenge comprises 6 different sub-tasks. In Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Tasks ‣ 3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we summarize the key aspects of the experimental protocol, metrics, and restrictions for each sub-task.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p"><span id="S3.SS1.p2.1.1" class="ltx_text ltx_font_bold">Task 1:</span> The first proposed task focuses on the use of synthetic data to mitigate demographic biases in FR systems. To assess the effectiveness of the proposed systems, we generate lists of mated and non-mated comparisons using subjects from the BUPT-BalancedFace database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>. We take into account eight demographic groups obtained from the combination of four ethnic groups (White, Black, Asian, and Indian) and two genders (Male and Female), and keep these groups balanced in the number of comparisons. In the case of non-mated comparisons, we only consider pairs of subjects within the same demographic group, as these hold greater relevance than non-mated comparisons involving subjects from different demographic groups.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Task 2:</span> The second proposed task focuses on utilizing synthetic data to enhance the overall performance of FR systems under challenging conditions. To assess the effectiveness of the proposed systems, we utilize lists of mated and non-mated comparisons selected from subjects from the different evaluation databases, each one designed to address specific challenges in FR. Specifically, BUPT-BalancedFace is used to consider diverse demographic groups, whereas AgeDB, CFP-FP, and ROF to assess age, pose, and occlusion, respectively.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<div id="S3.T1.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:243.5pt;height:222pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-139.8pt,127.5pt) scale(0.465367811202035,0.465367811202035) ;">
<table id="S3.T1.6.6" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.6.6.7.1" class="ltx_tr">
<td id="S3.T1.6.6.7.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.7.1.1.1" class="ltx_text ltx_font_bold">Task 1:</span> synthetic data for <span id="S3.T1.6.6.7.1.1.2" class="ltx_text ltx_font_bold">demographic bias mitigation</span>
</td>
</tr>
<tr id="S3.T1.6.6.8.2" class="ltx_tr">
<td id="S3.T1.6.6.8.2.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.8.2.1.1" class="ltx_text ltx_font_bold">Baseline</span>: training with only CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>.</td>
</tr>
<tr id="S3.T1.6.6.9.3" class="ltx_tr">
<td id="S3.T1.6.6.9.3.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.9.3.1.1" class="ltx_text ltx_font_bold">Metrics</span>: accuracy (for each demographic group).</td>
</tr>
<tr id="S3.T1.6.6.10.4" class="ltx_tr">
<td id="S3.T1.6.6.10.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.10.4.1.1" class="ltx_text ltx_font_bold">Ranking</span>: average vs SD accuracy, see Section <a href="#S3.SS3" title="3.3 Evaluation Metrics ‣ 3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> for more details.</td>
</tr>
<tr id="S3.T1.6.6.11.5" class="ltx_tr">
<td id="S3.T1.6.6.11.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.11.5.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 1.1: [<span id="S3.T1.6.6.11.5.1.1.1" class="ltx_text" style="color:#32963D;">constrained</span>]</span> training exclusively with <span id="S3.T1.6.6.11.5.1.2" class="ltx_text ltx_font_bold">synthetic</span> data</td>
</tr>
<tr id="S3.T1.3.3.3" class="ltx_tr">
<td id="S3.T1.3.3.3.3" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.3.3.3.3.1" class="ltx_text ltx_font_bold">Train</span>: maximum <math id="S3.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="500K" display="inline"><semantics id="S3.T1.1.1.1.1.m1.1a"><mrow id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml"><mn id="S3.T1.1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.1.m1.1.1.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S3.T1.1.1.1.1.m1.1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.1.cmml">​</mo><mi id="S3.T1.1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1"><times id="S3.T1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.1.m1.1.1.2">500</cn><ci id="S3.T1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">500K</annotation></semantics></math> face images (<span id="S3.T1.3.3.3.3.2" class="ltx_text ltx_font_italic">e.g.,</span> <math id="S3.T1.2.2.2.2.m2.1" class="ltx_Math" alttext="10K" display="inline"><semantics id="S3.T1.2.2.2.2.m2.1a"><mrow id="S3.T1.2.2.2.2.m2.1.1" xref="S3.T1.2.2.2.2.m2.1.1.cmml"><mn id="S3.T1.2.2.2.2.m2.1.1.2" xref="S3.T1.2.2.2.2.m2.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S3.T1.2.2.2.2.m2.1.1.1" xref="S3.T1.2.2.2.2.m2.1.1.1.cmml">​</mo><mi id="S3.T1.2.2.2.2.m2.1.1.3" xref="S3.T1.2.2.2.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m2.1b"><apply id="S3.T1.2.2.2.2.m2.1.1.cmml" xref="S3.T1.2.2.2.2.m2.1.1"><times id="S3.T1.2.2.2.2.m2.1.1.1.cmml" xref="S3.T1.2.2.2.2.m2.1.1.1"></times><cn type="integer" id="S3.T1.2.2.2.2.m2.1.1.2.cmml" xref="S3.T1.2.2.2.2.m2.1.1.2">10</cn><ci id="S3.T1.2.2.2.2.m2.1.1.3.cmml" xref="S3.T1.2.2.2.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m2.1c">10K</annotation></semantics></math> identities and <math id="S3.T1.3.3.3.3.m3.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S3.T1.3.3.3.3.m3.1a"><mn id="S3.T1.3.3.3.3.m3.1.1" xref="S3.T1.3.3.3.3.m3.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m3.1b"><cn type="integer" id="S3.T1.3.3.3.3.m3.1.1.cmml" xref="S3.T1.3.3.3.3.m3.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m3.1c">50</annotation></semantics></math> images per identity).</td>
</tr>
<tr id="S3.T1.6.6.12.6" class="ltx_tr">
<td id="S3.T1.6.6.12.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.12.6.1.1" class="ltx_text ltx_font_bold">Eval</span>: BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite>.</td>
</tr>
<tr id="S3.T1.6.6.13.7" class="ltx_tr">
<td id="S3.T1.6.6.13.7.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.13.7.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 1.2: [<span id="S3.T1.6.6.13.7.1.1.1" class="ltx_text" style="color:#32963D;">unconstrained</span>]</span> training exclusively with <span id="S3.T1.6.6.13.7.1.2" class="ltx_text ltx_font_bold">synthetic</span> data</td>
</tr>
<tr id="S3.T1.6.6.14.8" class="ltx_tr">
<td id="S3.T1.6.6.14.8.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.14.8.1.1" class="ltx_text ltx_font_bold">Train</span>: no restrictions in terms of the number of face images.</td>
</tr>
<tr id="S3.T1.6.6.15.9" class="ltx_tr">
<td id="S3.T1.6.6.15.9.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.15.9.1.1" class="ltx_text ltx_font_bold">Eval</span>: BUPT-BalancedFace.</td>
</tr>
<tr id="S3.T1.6.6.16.10" class="ltx_tr">
<td id="S3.T1.6.6.16.10.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.16.10.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 1.3: [<span id="S3.T1.6.6.16.10.1.1.1" class="ltx_text" style="color:#32963D;">constrained</span>]</span> training with <span id="S3.T1.6.6.16.10.1.2" class="ltx_text ltx_font_bold">real and synthetic</span> data</td>
</tr>
<tr id="S3.T1.4.4.4" class="ltx_tr">
<td id="S3.T1.4.4.4.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.4.4.4.1.1" class="ltx_text ltx_font_bold">Train</span>: CASIA-WebFace, and maximum <math id="S3.T1.4.4.4.1.m1.1" class="ltx_Math" alttext="500K" display="inline"><semantics id="S3.T1.4.4.4.1.m1.1a"><mrow id="S3.T1.4.4.4.1.m1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.cmml"><mn id="S3.T1.4.4.4.1.m1.1.1.2" xref="S3.T1.4.4.4.1.m1.1.1.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S3.T1.4.4.4.1.m1.1.1.1" xref="S3.T1.4.4.4.1.m1.1.1.1.cmml">​</mo><mi id="S3.T1.4.4.4.1.m1.1.1.3" xref="S3.T1.4.4.4.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><apply id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1"><times id="S3.T1.4.4.4.1.m1.1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.4.4.4.1.m1.1.1.2.cmml" xref="S3.T1.4.4.4.1.m1.1.1.2">500</cn><ci id="S3.T1.4.4.4.1.m1.1.1.3.cmml" xref="S3.T1.4.4.4.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">500K</annotation></semantics></math> face synthetic images.</td>
</tr>
<tr id="S3.T1.6.6.17.11" class="ltx_tr">
<td id="S3.T1.6.6.17.11.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.17.11.1.1" class="ltx_text ltx_font_bold">Eval</span>: BUPT-BalancedFace.</td>
</tr>
<tr id="S3.T1.6.6.18.12" class="ltx_tr">
<td id="S3.T1.6.6.18.12.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt">
<span id="S3.T1.6.6.18.12.1.1" class="ltx_text ltx_font_bold">Task 2:</span> synthetic data for <span id="S3.T1.6.6.18.12.1.2" class="ltx_text ltx_font_bold">overall performance improvement</span>
</td>
</tr>
<tr id="S3.T1.6.6.19.13" class="ltx_tr">
<td id="S3.T1.6.6.19.13.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.19.13.1.1" class="ltx_text ltx_font_bold">Baseline</span>: training with only CASIA-WebFace.</td>
</tr>
<tr id="S3.T1.6.6.20.14" class="ltx_tr">
<td id="S3.T1.6.6.20.14.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.20.14.1.1" class="ltx_text ltx_font_bold">Metrics</span>: accuracy (for each evaluation database).</td>
</tr>
<tr id="S3.T1.6.6.21.15" class="ltx_tr">
<td id="S3.T1.6.6.21.15.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.21.15.1.1" class="ltx_text ltx_font_bold">Ranking</span>: average accuracy, see Section <a href="#S3.SS3" title="3.3 Evaluation Metrics ‣ 3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a> for more details.</td>
</tr>
<tr id="S3.T1.6.6.22.16" class="ltx_tr">
<td id="S3.T1.6.6.22.16.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.22.16.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 2.1: [<span id="S3.T1.6.6.22.16.1.1.1" class="ltx_text" style="color:#32963D;">constrained</span>]</span> training with only <span id="S3.T1.6.6.22.16.1.2" class="ltx_text ltx_font_bold">synthetic</span> data</td>
</tr>
<tr id="S3.T1.5.5.5" class="ltx_tr">
<td id="S3.T1.5.5.5.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.5.5.5.1.1" class="ltx_text ltx_font_bold">Train</span>: maximum <math id="S3.T1.5.5.5.1.m1.1" class="ltx_Math" alttext="500K" display="inline"><semantics id="S3.T1.5.5.5.1.m1.1a"><mrow id="S3.T1.5.5.5.1.m1.1.1" xref="S3.T1.5.5.5.1.m1.1.1.cmml"><mn id="S3.T1.5.5.5.1.m1.1.1.2" xref="S3.T1.5.5.5.1.m1.1.1.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S3.T1.5.5.5.1.m1.1.1.1" xref="S3.T1.5.5.5.1.m1.1.1.1.cmml">​</mo><mi id="S3.T1.5.5.5.1.m1.1.1.3" xref="S3.T1.5.5.5.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.1.m1.1b"><apply id="S3.T1.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1"><times id="S3.T1.5.5.5.1.m1.1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.5.5.5.1.m1.1.1.2.cmml" xref="S3.T1.5.5.5.1.m1.1.1.2">500</cn><ci id="S3.T1.5.5.5.1.m1.1.1.3.cmml" xref="S3.T1.5.5.5.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.1.m1.1c">500K</annotation></semantics></math> face images.</td>
</tr>
<tr id="S3.T1.6.6.23.17" class="ltx_tr">
<td id="S3.T1.6.6.23.17.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.23.17.1.1" class="ltx_text ltx_font_bold">Eval</span>: BUPT-BalancedFace, AgeDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, and ROF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>.</td>
</tr>
<tr id="S3.T1.6.6.24.18" class="ltx_tr">
<td id="S3.T1.6.6.24.18.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.24.18.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 2.2: [<span id="S3.T1.6.6.24.18.1.1.1" class="ltx_text" style="color:#32963D;">unconstrained</span>]</span> training with only <span id="S3.T1.6.6.24.18.1.2" class="ltx_text ltx_font_bold">synthetic</span> data</td>
</tr>
<tr id="S3.T1.6.6.25.19" class="ltx_tr">
<td id="S3.T1.6.6.25.19.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.25.19.1.1" class="ltx_text ltx_font_bold">Train</span>: no restrictions in terms of the number of face images.</td>
</tr>
<tr id="S3.T1.6.6.26.20" class="ltx_tr">
<td id="S3.T1.6.6.26.20.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.26.20.1.1" class="ltx_text ltx_font_bold">Eval</span>: BUPT-BalancedFace, AgeDB, CFP-FP, and ROF.</td>
</tr>
<tr id="S3.T1.6.6.27.21" class="ltx_tr">
<td id="S3.T1.6.6.27.21.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.6.6.27.21.1.1" class="ltx_text ltx_font_bold" style="color:#0000FF;">Sub-Task 2.3: [<span id="S3.T1.6.6.27.21.1.1.1" class="ltx_text" style="color:#32963D;">constrained</span>]</span> training with <span id="S3.T1.6.6.27.21.1.2" class="ltx_text ltx_font_bold">real and synthetic</span> data</td>
</tr>
<tr id="S3.T1.6.6.6" class="ltx_tr">
<td id="S3.T1.6.6.6.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.6.1.1" class="ltx_text ltx_font_bold">Train</span>: CASIA-WebFace, and maximum <math id="S3.T1.6.6.6.1.m1.1" class="ltx_Math" alttext="500K" display="inline"><semantics id="S3.T1.6.6.6.1.m1.1a"><mrow id="S3.T1.6.6.6.1.m1.1.1" xref="S3.T1.6.6.6.1.m1.1.1.cmml"><mn id="S3.T1.6.6.6.1.m1.1.1.2" xref="S3.T1.6.6.6.1.m1.1.1.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S3.T1.6.6.6.1.m1.1.1.1" xref="S3.T1.6.6.6.1.m1.1.1.1.cmml">​</mo><mi id="S3.T1.6.6.6.1.m1.1.1.3" xref="S3.T1.6.6.6.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.1.m1.1b"><apply id="S3.T1.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.1.m1.1.1"><times id="S3.T1.6.6.6.1.m1.1.1.1.cmml" xref="S3.T1.6.6.6.1.m1.1.1.1"></times><cn type="integer" id="S3.T1.6.6.6.1.m1.1.1.2.cmml" xref="S3.T1.6.6.6.1.m1.1.1.2">500</cn><ci id="S3.T1.6.6.6.1.m1.1.1.3.cmml" xref="S3.T1.6.6.6.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.1.m1.1c">500K</annotation></semantics></math> face synthetic images.</td>
</tr>
<tr id="S3.T1.6.6.28.22" class="ltx_tr">
<td id="S3.T1.6.6.28.22.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r">   <span id="S3.T1.6.6.28.22.1.1" class="ltx_text ltx_font_bold">Eval</span>: BUPT-BalancedFace, AgeDB, CFP-FP, and ROF.</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S3.T1.10.2.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S3.T1.8.1" class="ltx_text" style="font-size:90%;">Tasks and sub-tasks for the 2<math id="S3.T1.8.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S3.T1.8.1.m1.1b"><msup id="S3.T1.8.1.m1.1.1" xref="S3.T1.8.1.m1.1.1.cmml"><mi id="S3.T1.8.1.m1.1.1b" xref="S3.T1.8.1.m1.1.1.cmml"></mi><mtext id="S3.T1.8.1.m1.1.1.1" xref="S3.T1.8.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.T1.8.1.m1.1c"><apply id="S3.T1.8.1.m1.1.1.cmml" xref="S3.T1.8.1.m1.1.1"><ci id="S3.T1.8.1.m1.1.1.1a.cmml" xref="S3.T1.8.1.m1.1.1.1"><mtext mathsize="70%" id="S3.T1.8.1.m1.1.1.1.cmml" xref="S3.T1.8.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.1.m1.1d">{}^{\text{nd}}</annotation></semantics></math> FRCSyn Challenge and their respective metrics and databases. SD = Standard Deviation.</span></figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experimental protocol</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p"><span id="S3.SS2.p1.1.1" class="ltx_text ltx_font_bold">Training:</span> The 6 sub-tasks introduced in the 2<math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><msup id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml"><mi id="S3.SS2.p1.1.m1.1.1a" xref="S3.SS2.p1.1.m1.1.1.cmml"></mi><mtext id="S3.SS2.p1.1.m1.1.1.1" xref="S3.SS2.p1.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><apply id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"><ci id="S3.SS2.p1.1.m1.1.1.1a.cmml" xref="S3.SS2.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S3.SS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the FRCSyn Challenge are mutually independent. This implies that participants have the flexibility to participate in any number of sub-tasks based on their preferences. For each selected sub-task, participants are required to develop and train the same FR system twice: <span id="S3.SS2.p1.1.2" class="ltx_text ltx_font_italic">i)</span> using the authorized real database exclusively, i.e. CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, and <span id="S3.SS2.p1.1.3" class="ltx_text ltx_font_italic">ii)</span> following the specific requirements of the chosen sub-task, as summarized in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Tasks ‣ 3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. According to this protocol, participants must provide both the <span id="S3.SS2.p1.1.4" class="ltx_text ltx_font_italic">baseline system</span> and the <span id="S3.SS2.p1.1.5" class="ltx_text ltx_font_italic">proposed system</span> for the specific sub-task. The baseline system plays a critical role in evaluating the impact of synthetic data on training and serves as a reference point for comparing against the conventional practice of training solely with real databases. To maintain consistency, the baseline FR system, trained exclusively with real data, and the proposed FR system, trained according to the specifications of the selected sub-task, must have the same architecture.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Evaluation:</span> In each sub-task, participants received the comparison files comprising both mated and non-mated comparisons, which are used to evaluate the performance of their proposed FR systems. Task 1 involves a single comparison file containing balanced comparisons of different demographic groups of the BUPT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> database, while Task 2 comprises four comparison files, each corresponding to each of the specific real-world databases considered (i.e., BUPT, AgeDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, and ROF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite>). During the evaluation of each sub-task, participants are required to submit via Codalab three files per database: <span id="S3.SS2.p2.1.2" class="ltx_text ltx_font_italic">i)</span> the scores of the baseline system, <span id="S3.SS2.p2.1.3" class="ltx_text ltx_font_italic">ii)</span> the scores of the proposed system, and <span id="S3.SS2.p2.1.4" class="ltx_text ltx_font_italic">iii)</span> the decision threshold for each FR system <span id="S3.SS2.p2.1.5" class="ltx_text ltx_font_italic">(i.e.,</span> baseline and proposed). The submitted scores must fall within the range of <math id="S3.SS2.p2.1.m1.2" class="ltx_Math" alttext="[0,1]" display="inline"><semantics id="S3.SS2.p2.1.m1.2a"><mrow id="S3.SS2.p2.1.m1.2.3.2" xref="S3.SS2.p2.1.m1.2.3.1.cmml"><mo stretchy="false" id="S3.SS2.p2.1.m1.2.3.2.1" xref="S3.SS2.p2.1.m1.2.3.1.cmml">[</mo><mn id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">0</mn><mo id="S3.SS2.p2.1.m1.2.3.2.2" xref="S3.SS2.p2.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS2.p2.1.m1.2.2" xref="S3.SS2.p2.1.m1.2.2.cmml">1</mn><mo stretchy="false" id="S3.SS2.p2.1.m1.2.3.2.3" xref="S3.SS2.p2.1.m1.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.2b"><interval closure="closed" id="S3.SS2.p2.1.m1.2.3.1.cmml" xref="S3.SS2.p2.1.m1.2.3.2"><cn type="integer" id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">0</cn><cn type="integer" id="S3.SS2.p2.1.m1.2.2.cmml" xref="S3.SS2.p2.1.m1.2.2">1</cn></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.2c">[0,1]</annotation></semantics></math>, with lower scores indicating non-mated comparisons, and vice versa.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Evaluation Metrics</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.3" class="ltx_p">We evaluate FR systems using a protocol based on lists of mated and non-mated comparisons for each sub-task and database. From the scores and thresholds provided by participants, we calculate the binary decision and the verification accuracy. Additionally, we calculate the gap to real (GAP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> as follows: <math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\text{GAP}=\left(\text{REAL}-\text{SYN}\right)/\text{SYN}" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mtext id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3a.cmml">GAP</mtext><mo id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">=</mo><mrow id="S3.SS3.p1.1.m1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.cmml"><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml"><mo id="S3.SS3.p1.1.m1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.p1.1.m1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml"><mtext id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2a.cmml">REAL</mtext><mo id="S3.SS3.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml">−</mo><mtext id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3a.cmml">SYN</mtext></mrow><mo id="S3.SS3.p1.1.m1.1.1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.SS3.p1.1.m1.1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.1.2.cmml">/</mo><mtext id="S3.SS3.p1.1.m1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.3a.cmml">SYN</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><eq id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2"></eq><ci id="S3.SS3.p1.1.m1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.3"><mtext id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">GAP</mtext></ci><apply id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1"><divide id="S3.SS3.p1.1.m1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.2"></divide><apply id="S3.SS3.p1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1"><minus id="S3.SS3.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.1"></minus><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2a.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2"><mtext id="S3.SS3.p1.1.m1.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.2">REAL</mtext></ci><ci id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3"><mtext id="S3.SS3.p1.1.m1.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1.1.1.3">SYN</mtext></ci></apply><ci id="S3.SS3.p1.1.m1.1.1.1.3a.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3"><mtext id="S3.SS3.p1.1.m1.1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3">SYN</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\text{GAP}=\left(\text{REAL}-\text{SYN}\right)/\text{SYN}</annotation></semantics></math>, with <span id="S3.SS3.p1.3.1" class="ltx_text ltx_markedasmath">REAL</span> representing the verification accuracy of the baseline system and <span id="S3.SS3.p1.3.2" class="ltx_text ltx_markedasmath">SYN</span> the verification accuracy of the proposed system, trained with synthetic (or real + synthetic) data. Other metrics such as False Non-Match Rate (FNMR) at 1% False Match Rate (FMR), which are very popular for the analysis of FR systems in real-world applications, can also be computed from the scores provided by participants. Due to the lack of space, comprehensive evaluations of the proposed systems will be conducted in subsequent studies, including FNMRs and metrics for each demographic group and database used for evaluation. Next, we explain how participants are ranked in the different tasks.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Task 1:</span> To rank participants and determine the winners of Sub-Tasks 1.1, 1.2, and 1.3, we closely examine the trade-off between the average (AVG) and standard deviation (SD) of the verification accuracy across the eight demographic groups defined in Section <a href="#S3.SS1" title="3.1 Tasks ‣ 3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. We define the trade-off metric (TO) as follows: <math id="S3.SS3.p2.1.m1.1" class="ltx_Math" alttext="\text{TO}=\text{AVG}-\text{SD}" display="inline"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mtext id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2a.cmml">TO</mtext><mo id="S3.SS3.p2.1.m1.1.1.1" xref="S3.SS3.p2.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml"><mtext id="S3.SS3.p2.1.m1.1.1.3.2" xref="S3.SS3.p2.1.m1.1.1.3.2a.cmml">AVG</mtext><mo id="S3.SS3.p2.1.m1.1.1.3.1" xref="S3.SS3.p2.1.m1.1.1.3.1.cmml">−</mo><mtext id="S3.SS3.p2.1.m1.1.1.3.3" xref="S3.SS3.p2.1.m1.1.1.3.3a.cmml">SD</mtext></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><eq id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1.1"></eq><ci id="S3.SS3.p2.1.m1.1.1.2a.cmml" xref="S3.SS3.p2.1.m1.1.1.2"><mtext id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">TO</mtext></ci><apply id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3"><minus id="S3.SS3.p2.1.m1.1.1.3.1.cmml" xref="S3.SS3.p2.1.m1.1.1.3.1"></minus><ci id="S3.SS3.p2.1.m1.1.1.3.2a.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2"><mtext id="S3.SS3.p2.1.m1.1.1.3.2.cmml" xref="S3.SS3.p2.1.m1.1.1.3.2">AVG</mtext></ci><ci id="S3.SS3.p2.1.m1.1.1.3.3a.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3"><mtext id="S3.SS3.p2.1.m1.1.1.3.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3.3">SD</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\text{TO}=\text{AVG}-\text{SD}</annotation></semantics></math>. This metric corresponds to plotting the average accuracy on the x-axis and the standard deviation on the y-axis in 2D space. We draw multiple 45-degree parallel lines to find the winning team whose performance falls to the far right side of these lines. With this proposed metric, we reward FR systems that achieve good levels of performance and fairness simultaneously, unlike common benchmarks based only on recognition performance. The standard deviation of verification accuracy across demographic groups is a common metric for assessing bias and should be reported by any work addressing demographic bias mitigation.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Task 2:</span> To rank participants and determine the winners of Sub-Tasks 2.1, 2.2, and 2.3, we consider the average verification accuracy across the four databases used for evaluation, described in Section <a href="#S3.SS1" title="3.1 Tasks ‣ 3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>. This approach allows us to evaluate four challenging aspects of FR simultaneously: <span id="S3.SS3.p3.1.2" class="ltx_text ltx_font_italic">i)</span> diverse demographic groups, <span id="S3.SS3.p3.1.3" class="ltx_text ltx_font_italic">ii)</span> pose variations, <span id="S3.SS3.p3.1.4" class="ltx_text ltx_font_italic">iii)</span> aging, and <span id="S3.SS3.p3.1.5" class="ltx_text ltx_font_italic">iv)</span> presence of occlusions providing a comprehensive evaluation of FR systems in real operational scenarios.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Restrictions</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">Regarding the FR system, participants have the freedom to choose any architecture for each sub-task, provided that the system’s number of Floating Point Operations Per Second (FLOPs) does not exceed 50 GFLOPs. This threshold has been established to facilitate the exploration of innovative architectures and encourage the use of diverse models while preventing the dominance of excessively large models. Participants are also free to utilize their preferred training modality, with the requirement that only the specified databases are used for training. This means that no additional databases can be employed during the training phase, such as to adapt the verification thresholds. Participants are allowed to use non-face databases for pre-training purposes and employ traditional data augmentation techniques using the authorized training databases. Regarding the synthetic data used to train the FR system in each sub-task, we allow participants to use any existing/novel database and face generative framework, regardless of how the model is trained.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">To maintain the integrity of the evaluation process, the organizers reserve the right to disqualify participants if anomalous results are detected or if participants fail to adhere to the challenge’s rules.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>FRCSyn Challenge: Systems Description</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.3" class="ltx_p">The 2<math id="S4.p1.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S4.p1.1.m1.1a"><msup id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1a" xref="S4.p1.1.m1.1.1.cmml"></mi><mtext id="S4.p1.1.m1.1.1.1" xref="S4.p1.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><ci id="S4.p1.1.m1.1.1.1a.cmml" xref="S4.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the FRCSyn Challenge received significant interest, with <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="78" display="inline"><semantics id="S4.p1.2.m2.1a"><mn id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml">78</mn><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><cn type="integer" id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1">78</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">78</annotation></semantics></math> international teams correctly registered, comprising research groups from both industry and academia. These teams work in various domains, including FR, generative AI, and other aspects of computer vision, such as demographic fairness and domain adaptation. Finally, <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="23" display="inline"><semantics id="S4.p1.3.m3.1a"><mn id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml">23</mn><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><cn type="integer" id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1">23</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">23</annotation></semantics></math> teams submitted their scores, receiving all sub-tasks great attention. The submitting teams are geographically distributed, with fourteen teams from Asia, six teams from Europe, and three teams from America. Table <a href="#S4.T2" title="Table 2 ‣ 4 FRCSyn Challenge: Systems Description ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides a general overview of the teams that ranked among the top-6 in at least one sub-task, including the sub-tasks in which they participated. Next, we describe briefly the approaches proposed for each team.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div id="S4.T2.11" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:148.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-52.6pt,33.6pt) scale(0.689337629124974,0.689337629124974) ;">
<table id="S4.T2.11.11" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.11.11.12.1" class="ltx_tr">
<th id="S4.T2.11.11.12.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S4.T2.11.11.12.1.1.1" class="ltx_text ltx_font_bold">Team</span></th>
<th id="S4.T2.11.11.12.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.11.11.12.1.2.1" class="ltx_text ltx_font_bold">Affiliations</span></th>
<th id="S4.T2.11.11.12.1.3" class="ltx_td ltx_align_left ltx_th ltx_th_column"><span id="S4.T2.11.11.12.1.3.1" class="ltx_text ltx_font_bold">Country</span></th>
<th id="S4.T2.11.11.12.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.11.11.12.1.4.1" class="ltx_text ltx_font_bold">Sub-Tasks</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1.2" class="ltx_td ltx_align_left ltx_border_t">ADMIS</td>
<td id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_border_t"><math id="S4.T2.1.1.1.1.m1.2" class="ltx_Math" alttext="4,5" display="inline"><semantics id="S4.T2.1.1.1.1.m1.2a"><mrow id="S4.T2.1.1.1.1.m1.2.3.2" xref="S4.T2.1.1.1.1.m1.2.3.1.cmml"><mn id="S4.T2.1.1.1.1.m1.1.1" xref="S4.T2.1.1.1.1.m1.1.1.cmml">4</mn><mo id="S4.T2.1.1.1.1.m1.2.3.2.1" xref="S4.T2.1.1.1.1.m1.2.3.1.cmml">,</mo><mn id="S4.T2.1.1.1.1.m1.2.2" xref="S4.T2.1.1.1.1.m1.2.2.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.1.m1.2b"><list id="S4.T2.1.1.1.1.m1.2.3.1.cmml" xref="S4.T2.1.1.1.1.m1.2.3.2"><cn type="integer" id="S4.T2.1.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.1.m1.1.1">4</cn><cn type="integer" id="S4.T2.1.1.1.1.m1.2.2.cmml" xref="S4.T2.1.1.1.1.m1.2.2">5</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.1.m1.2c">4,5</annotation></semantics></math></td>
<td id="S4.T2.1.1.1.3" class="ltx_td ltx_align_left ltx_border_t">China</td>
<td id="S4.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t">all</td>
</tr>
<tr id="S4.T2.2.2.2" class="ltx_tr">
<td id="S4.T2.2.2.2.2" class="ltx_td ltx_align_left">OPDAI</td>
<td id="S4.T2.2.2.2.1" class="ltx_td ltx_align_center"><math id="S4.T2.2.2.2.1.m1.1" class="ltx_Math" alttext="6" display="inline"><semantics id="S4.T2.2.2.2.1.m1.1a"><mn id="S4.T2.2.2.2.1.m1.1.1" xref="S4.T2.2.2.2.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.1.m1.1b"><cn type="integer" id="S4.T2.2.2.2.1.m1.1.1.cmml" xref="S4.T2.2.2.2.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.1.m1.1c">6</annotation></semantics></math></td>
<td id="S4.T2.2.2.2.3" class="ltx_td ltx_align_left">China</td>
<td id="S4.T2.2.2.2.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S4.T2.3.3.3" class="ltx_tr">
<td id="S4.T2.3.3.3.2" class="ltx_td ltx_align_left">ID R&amp;D</td>
<td id="S4.T2.3.3.3.1" class="ltx_td ltx_align_center"><math id="S4.T2.3.3.3.1.m1.1" class="ltx_Math" alttext="7" display="inline"><semantics id="S4.T2.3.3.3.1.m1.1a"><mn id="S4.T2.3.3.3.1.m1.1.1" xref="S4.T2.3.3.3.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.1.m1.1b"><cn type="integer" id="S4.T2.3.3.3.1.m1.1.1.cmml" xref="S4.T2.3.3.3.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.1.m1.1c">7</annotation></semantics></math></td>
<td id="S4.T2.3.3.3.3" class="ltx_td ltx_align_left">USA</td>
<td id="S4.T2.3.3.3.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S4.T2.4.4.4" class="ltx_tr">
<td id="S4.T2.4.4.4.2" class="ltx_td ltx_align_left">K-IBS-DS</td>
<td id="S4.T2.4.4.4.1" class="ltx_td ltx_align_center"><math id="S4.T2.4.4.4.1.m1.2" class="ltx_Math" alttext="8,9" display="inline"><semantics id="S4.T2.4.4.4.1.m1.2a"><mrow id="S4.T2.4.4.4.1.m1.2.3.2" xref="S4.T2.4.4.4.1.m1.2.3.1.cmml"><mn id="S4.T2.4.4.4.1.m1.1.1" xref="S4.T2.4.4.4.1.m1.1.1.cmml">8</mn><mo id="S4.T2.4.4.4.1.m1.2.3.2.1" xref="S4.T2.4.4.4.1.m1.2.3.1.cmml">,</mo><mn id="S4.T2.4.4.4.1.m1.2.2" xref="S4.T2.4.4.4.1.m1.2.2.cmml">9</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.4.1.m1.2b"><list id="S4.T2.4.4.4.1.m1.2.3.1.cmml" xref="S4.T2.4.4.4.1.m1.2.3.2"><cn type="integer" id="S4.T2.4.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.4.1.m1.1.1">8</cn><cn type="integer" id="S4.T2.4.4.4.1.m1.2.2.cmml" xref="S4.T2.4.4.4.1.m1.2.2">9</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.4.1.m1.2c">8,9</annotation></semantics></math></td>
<td id="S4.T2.4.4.4.3" class="ltx_td ltx_align_left">South Korea</td>
<td id="S4.T2.4.4.4.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S4.T2.5.5.5" class="ltx_tr">
<td id="S4.T2.5.5.5.2" class="ltx_td ltx_align_left">CTAI</td>
<td id="S4.T2.5.5.5.1" class="ltx_td ltx_align_center"><math id="S4.T2.5.5.5.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.T2.5.5.5.1.m1.1a"><mn id="S4.T2.5.5.5.1.m1.1.1" xref="S4.T2.5.5.5.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.5.1.m1.1b"><cn type="integer" id="S4.T2.5.5.5.1.m1.1.1.cmml" xref="S4.T2.5.5.5.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.5.1.m1.1c">10</annotation></semantics></math></td>
<td id="S4.T2.5.5.5.3" class="ltx_td ltx_align_left">China</td>
<td id="S4.T2.5.5.5.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S4.T2.6.6.6" class="ltx_tr">
<td id="S4.T2.6.6.6.2" class="ltx_td ltx_align_left">Idiap-SynthDistill</td>
<td id="S4.T2.6.6.6.1" class="ltx_td ltx_align_center"><math id="S4.T2.6.6.6.1.m1.3" class="ltx_Math" alttext="11,12,13" display="inline"><semantics id="S4.T2.6.6.6.1.m1.3a"><mrow id="S4.T2.6.6.6.1.m1.3.4.2" xref="S4.T2.6.6.6.1.m1.3.4.1.cmml"><mn id="S4.T2.6.6.6.1.m1.1.1" xref="S4.T2.6.6.6.1.m1.1.1.cmml">11</mn><mo id="S4.T2.6.6.6.1.m1.3.4.2.1" xref="S4.T2.6.6.6.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.6.6.6.1.m1.2.2" xref="S4.T2.6.6.6.1.m1.2.2.cmml">12</mn><mo id="S4.T2.6.6.6.1.m1.3.4.2.2" xref="S4.T2.6.6.6.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.6.6.6.1.m1.3.3" xref="S4.T2.6.6.6.1.m1.3.3.cmml">13</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.6.1.m1.3b"><list id="S4.T2.6.6.6.1.m1.3.4.1.cmml" xref="S4.T2.6.6.6.1.m1.3.4.2"><cn type="integer" id="S4.T2.6.6.6.1.m1.1.1.cmml" xref="S4.T2.6.6.6.1.m1.1.1">11</cn><cn type="integer" id="S4.T2.6.6.6.1.m1.2.2.cmml" xref="S4.T2.6.6.6.1.m1.2.2">12</cn><cn type="integer" id="S4.T2.6.6.6.1.m1.3.3.cmml" xref="S4.T2.6.6.6.1.m1.3.3">13</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.6.1.m1.3c">11,12,13</annotation></semantics></math></td>
<td id="S4.T2.6.6.6.3" class="ltx_td ltx_align_left">Switzerland</td>
<td id="S4.T2.6.6.6.4" class="ltx_td ltx_align_center">1.2 - 2.2</td>
</tr>
<tr id="S4.T2.7.7.7" class="ltx_tr">
<td id="S4.T2.7.7.7.2" class="ltx_td ltx_align_left">INESC-IGD</td>
<td id="S4.T2.7.7.7.1" class="ltx_td ltx_align_center"><math id="S4.T2.7.7.7.1.m1.3" class="ltx_Math" alttext="14,15,16" display="inline"><semantics id="S4.T2.7.7.7.1.m1.3a"><mrow id="S4.T2.7.7.7.1.m1.3.4.2" xref="S4.T2.7.7.7.1.m1.3.4.1.cmml"><mn id="S4.T2.7.7.7.1.m1.1.1" xref="S4.T2.7.7.7.1.m1.1.1.cmml">14</mn><mo id="S4.T2.7.7.7.1.m1.3.4.2.1" xref="S4.T2.7.7.7.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.7.7.7.1.m1.2.2" xref="S4.T2.7.7.7.1.m1.2.2.cmml">15</mn><mo id="S4.T2.7.7.7.1.m1.3.4.2.2" xref="S4.T2.7.7.7.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.7.7.7.1.m1.3.3" xref="S4.T2.7.7.7.1.m1.3.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.7.1.m1.3b"><list id="S4.T2.7.7.7.1.m1.3.4.1.cmml" xref="S4.T2.7.7.7.1.m1.3.4.2"><cn type="integer" id="S4.T2.7.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.7.1.m1.1.1">14</cn><cn type="integer" id="S4.T2.7.7.7.1.m1.2.2.cmml" xref="S4.T2.7.7.7.1.m1.2.2">15</cn><cn type="integer" id="S4.T2.7.7.7.1.m1.3.3.cmml" xref="S4.T2.7.7.7.1.m1.3.3">16</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.7.1.m1.3c">14,15,16</annotation></semantics></math></td>
<td id="S4.T2.7.7.7.3" class="ltx_td ltx_align_left">Portugal and Germany</td>
<td id="S4.T2.7.7.7.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S4.T2.8.8.8" class="ltx_tr">
<td id="S4.T2.8.8.8.2" class="ltx_td ltx_align_left">UNICA-IGD-LSI</td>
<td id="S4.T2.8.8.8.1" class="ltx_td ltx_align_center"><math id="S4.T2.8.8.8.1.m1.3" class="ltx_Math" alttext="16,17,18" display="inline"><semantics id="S4.T2.8.8.8.1.m1.3a"><mrow id="S4.T2.8.8.8.1.m1.3.4.2" xref="S4.T2.8.8.8.1.m1.3.4.1.cmml"><mn id="S4.T2.8.8.8.1.m1.1.1" xref="S4.T2.8.8.8.1.m1.1.1.cmml">16</mn><mo id="S4.T2.8.8.8.1.m1.3.4.2.1" xref="S4.T2.8.8.8.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.8.8.8.1.m1.2.2" xref="S4.T2.8.8.8.1.m1.2.2.cmml">17</mn><mo id="S4.T2.8.8.8.1.m1.3.4.2.2" xref="S4.T2.8.8.8.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.8.8.8.1.m1.3.3" xref="S4.T2.8.8.8.1.m1.3.3.cmml">18</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.8.1.m1.3b"><list id="S4.T2.8.8.8.1.m1.3.4.1.cmml" xref="S4.T2.8.8.8.1.m1.3.4.2"><cn type="integer" id="S4.T2.8.8.8.1.m1.1.1.cmml" xref="S4.T2.8.8.8.1.m1.1.1">16</cn><cn type="integer" id="S4.T2.8.8.8.1.m1.2.2.cmml" xref="S4.T2.8.8.8.1.m1.2.2">17</cn><cn type="integer" id="S4.T2.8.8.8.1.m1.3.3.cmml" xref="S4.T2.8.8.8.1.m1.3.3">18</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.8.1.m1.3c">16,17,18</annotation></semantics></math></td>
<td id="S4.T2.8.8.8.3" class="ltx_td ltx_align_left">Italy, Germany, Slovenia</td>
<td id="S4.T2.8.8.8.4" class="ltx_td ltx_align_center">all</td>
</tr>
<tr id="S4.T2.9.9.9" class="ltx_tr">
<td id="S4.T2.9.9.9.2" class="ltx_td ltx_align_left">SRCN_AIVL</td>
<td id="S4.T2.9.9.9.1" class="ltx_td ltx_align_center"><math id="S4.T2.9.9.9.1.m1.4" class="ltx_Math" alttext="19,20,21,22" display="inline"><semantics id="S4.T2.9.9.9.1.m1.4a"><mrow id="S4.T2.9.9.9.1.m1.4.5.2" xref="S4.T2.9.9.9.1.m1.4.5.1.cmml"><mn id="S4.T2.9.9.9.1.m1.1.1" xref="S4.T2.9.9.9.1.m1.1.1.cmml">19</mn><mo id="S4.T2.9.9.9.1.m1.4.5.2.1" xref="S4.T2.9.9.9.1.m1.4.5.1.cmml">,</mo><mn id="S4.T2.9.9.9.1.m1.2.2" xref="S4.T2.9.9.9.1.m1.2.2.cmml">20</mn><mo id="S4.T2.9.9.9.1.m1.4.5.2.2" xref="S4.T2.9.9.9.1.m1.4.5.1.cmml">,</mo><mn id="S4.T2.9.9.9.1.m1.3.3" xref="S4.T2.9.9.9.1.m1.3.3.cmml">21</mn><mo id="S4.T2.9.9.9.1.m1.4.5.2.3" xref="S4.T2.9.9.9.1.m1.4.5.1.cmml">,</mo><mn id="S4.T2.9.9.9.1.m1.4.4" xref="S4.T2.9.9.9.1.m1.4.4.cmml">22</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.9.1.m1.4b"><list id="S4.T2.9.9.9.1.m1.4.5.1.cmml" xref="S4.T2.9.9.9.1.m1.4.5.2"><cn type="integer" id="S4.T2.9.9.9.1.m1.1.1.cmml" xref="S4.T2.9.9.9.1.m1.1.1">19</cn><cn type="integer" id="S4.T2.9.9.9.1.m1.2.2.cmml" xref="S4.T2.9.9.9.1.m1.2.2">20</cn><cn type="integer" id="S4.T2.9.9.9.1.m1.3.3.cmml" xref="S4.T2.9.9.9.1.m1.3.3">21</cn><cn type="integer" id="S4.T2.9.9.9.1.m1.4.4.cmml" xref="S4.T2.9.9.9.1.m1.4.4">22</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.9.1.m1.4c">19,20,21,22</annotation></semantics></math></td>
<td id="S4.T2.9.9.9.3" class="ltx_td ltx_align_left">China</td>
<td id="S4.T2.9.9.9.4" class="ltx_td ltx_align_center">1.1</td>
</tr>
<tr id="S4.T2.10.10.10" class="ltx_tr">
<td id="S4.T2.10.10.10.2" class="ltx_td ltx_align_left">CBSR-Samsung</td>
<td id="S4.T2.10.10.10.1" class="ltx_td ltx_align_center"><math id="S4.T2.10.10.10.1.m1.3" class="ltx_Math" alttext="19,21,22" display="inline"><semantics id="S4.T2.10.10.10.1.m1.3a"><mrow id="S4.T2.10.10.10.1.m1.3.4.2" xref="S4.T2.10.10.10.1.m1.3.4.1.cmml"><mn id="S4.T2.10.10.10.1.m1.1.1" xref="S4.T2.10.10.10.1.m1.1.1.cmml">19</mn><mo id="S4.T2.10.10.10.1.m1.3.4.2.1" xref="S4.T2.10.10.10.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.10.10.10.1.m1.2.2" xref="S4.T2.10.10.10.1.m1.2.2.cmml">21</mn><mo id="S4.T2.10.10.10.1.m1.3.4.2.2" xref="S4.T2.10.10.10.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.10.10.10.1.m1.3.3" xref="S4.T2.10.10.10.1.m1.3.3.cmml">22</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.10.1.m1.3b"><list id="S4.T2.10.10.10.1.m1.3.4.1.cmml" xref="S4.T2.10.10.10.1.m1.3.4.2"><cn type="integer" id="S4.T2.10.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.10.1.m1.1.1">19</cn><cn type="integer" id="S4.T2.10.10.10.1.m1.2.2.cmml" xref="S4.T2.10.10.10.1.m1.2.2">21</cn><cn type="integer" id="S4.T2.10.10.10.1.m1.3.3.cmml" xref="S4.T2.10.10.10.1.m1.3.3">22</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.10.1.m1.3c">19,21,22</annotation></semantics></math></td>
<td id="S4.T2.10.10.10.3" class="ltx_td ltx_align_left">China</td>
<td id="S4.T2.10.10.10.4" class="ltx_td ltx_align_center">1.3 - 2.3</td>
</tr>
<tr id="S4.T2.11.11.11" class="ltx_tr">
<td id="S4.T2.11.11.11.2" class="ltx_td ltx_align_left ltx_border_b">BOVIFOCR-UFPR</td>
<td id="S4.T2.11.11.11.1" class="ltx_td ltx_align_center ltx_border_b"><math id="S4.T2.11.11.11.1.m1.3" class="ltx_Math" alttext="23,24,25" display="inline"><semantics id="S4.T2.11.11.11.1.m1.3a"><mrow id="S4.T2.11.11.11.1.m1.3.4.2" xref="S4.T2.11.11.11.1.m1.3.4.1.cmml"><mn id="S4.T2.11.11.11.1.m1.1.1" xref="S4.T2.11.11.11.1.m1.1.1.cmml">23</mn><mo id="S4.T2.11.11.11.1.m1.3.4.2.1" xref="S4.T2.11.11.11.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.11.11.11.1.m1.2.2" xref="S4.T2.11.11.11.1.m1.2.2.cmml">24</mn><mo id="S4.T2.11.11.11.1.m1.3.4.2.2" xref="S4.T2.11.11.11.1.m1.3.4.1.cmml">,</mo><mn id="S4.T2.11.11.11.1.m1.3.3" xref="S4.T2.11.11.11.1.m1.3.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.11.1.m1.3b"><list id="S4.T2.11.11.11.1.m1.3.4.1.cmml" xref="S4.T2.11.11.11.1.m1.3.4.2"><cn type="integer" id="S4.T2.11.11.11.1.m1.1.1.cmml" xref="S4.T2.11.11.11.1.m1.1.1">23</cn><cn type="integer" id="S4.T2.11.11.11.1.m1.2.2.cmml" xref="S4.T2.11.11.11.1.m1.2.2">24</cn><cn type="integer" id="S4.T2.11.11.11.1.m1.3.3.cmml" xref="S4.T2.11.11.11.1.m1.3.3">25</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.11.1.m1.3c">23,24,25</annotation></semantics></math></td>
<td id="S4.T2.11.11.11.3" class="ltx_td ltx_align_left ltx_border_b">Brazil</td>
<td id="S4.T2.11.11.11.4" class="ltx_td ltx_align_center ltx_border_b">1.2 - 2.1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T2.13.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.14.2" class="ltx_text" style="font-size:90%;">Description of the teams that ranked among the top-6 in at least one sub-task, ordered by the average rank in all the sub-tasks. The numbers reported in the column ‘affiliations’ refer to the ones provided in the title page.</span></figcaption>
</figure>
<section id="S4.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">ADMIS (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px1.p1.6" class="ltx_p">They used an IDiff-Face-based <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> Latent Diffusion Model (LDM) to synthesize face images. Specifically, they trained an identity-conditioned LDM using ID embeddings extracted from CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> with a pre-trained ElasticFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> IResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> model. As the LDM takes the ID embeddings as context, they employed an unconditional Denoising Diffusion Probabilistic Model (DDPM) trained on the FFHQ database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">21</span></a>]</cite> as a context generator. This produced <math id="S4.SS0.SSS0.Px1.p1.1.m1.1" class="ltx_Math" alttext="400K" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">400</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.2">400</cn><ci id="S4.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.1.m1.1c">400K</annotation></semantics></math> images, from which they extracted approximately <math id="S4.SS0.SSS0.Px1.p1.2.m2.1" class="ltx_Math" alttext="30K" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml">30</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.2">30</cn><ci id="S4.SS0.SSS0.Px1.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.2.m2.1c">30K</annotation></semantics></math> unique ID embeddings with a <math id="S4.SS0.SSS0.Px1.p1.3.m3.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.3.m3.1b"><cn type="float" id="S4.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.3.m3.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.3.m3.1c">0.3</annotation></semantics></math> similarity threshold using the pre-trained ElasticFace model, creating a context database. Furthermore, they accelerated the sampling process of the LDM using a DDIM <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">41</span></a>]</cite>. For the training of the FR model, they generated <math id="S4.SS0.SSS0.Px1.p1.4.m4.1" class="ltx_Math" alttext="49" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">49</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.4.m4.1b"><cn type="integer" id="S4.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.4.m4.1.1">49</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.4.m4.1c">49</annotation></semantics></math> images for each context. They adopted the ID oversampling strategy from DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> and performed it five times for each ID to enhance consistency. As a result, <math id="S4.SS0.SSS0.Px1.p1.5.m5.1" class="ltx_Math" alttext="10K" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.5.m5.1a"><mrow id="S4.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px1.p1.5.m5.1.1.cmml"><mn id="S4.SS0.SSS0.Px1.p1.5.m5.1.1.2" xref="S4.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.5.m5.1.1.1" xref="S4.SS0.SSS0.Px1.p1.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.5.m5.1.1.3" xref="S4.SS0.SSS0.Px1.p1.5.m5.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.5.m5.1b"><apply id="S4.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.5.m5.1.1"><times id="S4.SS0.SSS0.Px1.p1.5.m5.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.5.m5.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px1.p1.5.m5.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.5.m5.1.1.2">10</cn><ci id="S4.SS0.SSS0.Px1.p1.5.m5.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.5.m5.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.5.m5.1c">10K</annotation></semantics></math> contexts were utilized for Sub-Tasks 1.1 and 2.1, while <math id="S4.SS0.SSS0.Px1.p1.6.m6.1" class="ltx_Math" alttext="30K" display="inline"><semantics id="S4.SS0.SSS0.Px1.p1.6.m6.1a"><mrow id="S4.SS0.SSS0.Px1.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px1.p1.6.m6.1.1.cmml"><mn id="S4.SS0.SSS0.Px1.p1.6.m6.1.1.2" xref="S4.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml">30</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px1.p1.6.m6.1.1.1" xref="S4.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px1.p1.6.m6.1.1.3" xref="S4.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px1.p1.6.m6.1b"><apply id="S4.SS0.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.6.m6.1.1"><times id="S4.SS0.SSS0.Px1.p1.6.m6.1.1.1.cmml" xref="S4.SS0.SSS0.Px1.p1.6.m6.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px1.p1.6.m6.1.1.2.cmml" xref="S4.SS0.SSS0.Px1.p1.6.m6.1.1.2">30</cn><ci id="S4.SS0.SSS0.Px1.p1.6.m6.1.1.3.cmml" xref="S4.SS0.SSS0.Px1.p1.6.m6.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px1.p1.6.m6.1c">30K</annotation></semantics></math> for Sub-Tasks 1.2 and 2.2. For Sub-Tasks 1.3 and 2.3, they expanded Sub-Tasks 1.1 and 2.1 with the CASIA-WebFace database. They applied the ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> loss and random cropping augmentation during training. Both the baseline and proposed models used IResNet-101 architectures <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite>.</p>
</div>
<div id="S4.SS0.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px1.p2.1" class="ltx_p">Code: <a target="_blank" href="https://github.com/zzzweakman/CVPR24_FRCSyn_ADMIS" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/zzzweakman/CVPR24_FRCSyn_ADMIS</a></p>
</div>
</section>
<section id="S4.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">OPDAI (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px2.p1.4" class="ltx_p">They initially used the data provided by DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, generating then <math id="S4.SS0.SSS0.Px2.p1.1.m1.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><cn type="integer" id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">10</annotation></semantics></math> more face images for each ID with large pose variations and occlusions using Photomaker <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">25</span></a>]</cite>. They randomly replaced these images in the original DCFace data to ensure that the total number of samples meets the requirement of <math id="S4.SS0.SSS0.Px2.p1.2.m2.1" class="ltx_Math" alttext="500K" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.2">500</cn><ci id="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.2.m2.1c">500K</annotation></semantics></math>. During the Photomaker inference, they adopted a batch size of <math id="S4.SS0.SSS0.Px2.p1.3.m3.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.3.m3.1b"><cn type="integer" id="S4.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.3.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.3.m3.1c">1</annotation></semantics></math> and used random prompts including age, pose, and image quality to ensure the diversity of the generated samples. For Sub-Tasks 1.2 and 2.2, they combined this data with the <math id="S4.SS0.SSS0.Px2.p1.4.m4.1" class="ltx_Math" alttext="1.2M" display="inline"><semantics id="S4.SS0.SSS0.Px2.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml"><mn id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml">1.2</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1"><times id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.1"></times><cn type="float" id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.2">1.2</cn><ci id="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.4.m4.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.4.m4.1c">1.2M</annotation></semantics></math> version of DCFace, while for Sub-Tasks 1.3 and 2.3, it was merged with CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>. For Sub-Tasks 1.2, 1.3, 2.2, and 2.3 they did not merge nor denoise samples from different databases, following the Partial FC approach <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">1</span></a>]</cite>. Also, they obtained the loss of different databases in independent AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> heads, calculating the final loss as the average of the multiple heads. Both baseline and proposed models are based on IResNet-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> architectures, with horizontal flipping.</p>
</div>
<div id="S4.SS0.SSS0.Px2.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px2.p2.1" class="ltx_p">Code: <a target="_blank" href="https://github.com/mightycatty/frcsyn_cvpr2024.git" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/mightycatty/frcsyn_cvpr2024.git</a></p>
</div>
</section>
<section id="S4.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">ID R&amp;D (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px3.p1.6" class="ltx_p">To generate the synthetic data, they used two models trained on WebFace42M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">53</span></a>]</cite>, one based on Hourglass Diffusion Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> and the other on StyleNAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite>, enhanced with a FR model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite>. They used classifier weights of the trained Prototype Memory <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">40</span></a>]</cite> to get <math id="S4.SS0.SSS0.Px3.p1.1.m1.1" class="ltx_Math" alttext="50K" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.2">50</cn><ci id="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.1.m1.1c">50K</annotation></semantics></math> identity vectors, of which <math id="S4.SS0.SSS0.Px3.p1.2.m2.1" class="ltx_Math" alttext="20K" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml">20</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.2">20</cn><ci id="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.2.m2.1c">20K</annotation></semantics></math> were randomly selected and <math id="S4.SS0.SSS0.Px3.p1.3.m3.1" class="ltx_Math" alttext="30K" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.3.m3.1a"><mrow id="S4.SS0.SSS0.Px3.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml">30</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.3.m3.1b"><apply id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1"><times id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.2">30</cn><ci id="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.3.m3.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.3.m3.1c">30K</annotation></semantics></math> were uniformly sampled from the <math id="S4.SS0.SSS0.Px3.p1.4.m4.1" class="ltx_Math" alttext="1K" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px3.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.cmml"><mn id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1"><times id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.2">1</cn><ci id="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="S4.SS0.SSS0.Px3.p1.4.m4.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.4.m4.1c">1K</annotation></semantics></math> clusters obtained using k-means, to get demographic diversity. For each identity, they generated <math id="S4.SS0.SSS0.Px3.p1.5.m5.1" class="ltx_Math" alttext="5" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px3.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.5.m5.1b"><cn type="integer" id="S4.SS0.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.5.m5.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.5.m5.1c">5</annotation></semantics></math> images using each of the two generative models. This data was used to train IResNet-200 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> with UniFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> loss for <math id="S4.SS0.SSS0.Px3.p1.6.m6.1" class="ltx_Math" alttext="28" display="inline"><semantics id="S4.SS0.SSS0.Px3.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px3.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.1.cmml">28</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px3.p1.6.m6.1b"><cn type="integer" id="S4.SS0.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px3.p1.6.m6.1.1">28</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px3.p1.6.m6.1c">28</annotation></semantics></math> epochs. One network was trained with color, geometric augmentations, and FaceMix-B <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">16</span></a>]</cite>, and the other network used only random horizontal flipping. These two networks were combined in an “ensemble”, where the first one received the original image, and the second one a mirrored copy. They used the same model for Sub-Tasks 1.1, 1.2, 2.1 and 2.2. For Sub-Tasks 1.3 and 2.3, they combined the synthetic data and CASIA-WebFace, training two models, one on the mixed data, and the other on the CASIA-WebFace.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">K-IBS-DS (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px4.p1.6" class="ltx_p">Inspired by SlackedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>, they made two modifications to enhance the AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> FR classifier. First, they made a more reliable weight initialization for uniformity across identity prototypes in the unit sphere and replaced the L2-norm with the face recognizability index from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">26</span></a>]</cite>. Regarding the synthetic data, they used DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> with <math id="S4.SS0.SSS0.Px4.p1.1.m1.1" class="ltx_Math" alttext="500K" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px4.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.2">500</cn><ci id="S4.SS0.SSS0.Px4.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px4.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.1.m1.1c">500K</annotation></semantics></math> and <math id="S4.SS0.SSS0.Px4.p1.2.m2.1" class="ltx_Math" alttext="1.2M" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px4.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml">1.2</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.1"></times><cn type="float" id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.2">1.2</cn><ci id="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px4.p1.2.m2.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.2.m2.1c">1.2M</annotation></semantics></math> face images (depending on the sub-task). The training stage was in line with <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, including optimizer, learning rate, etc. For Sub-Tasks 1.3 and 2.3, the first <math id="S4.SS0.SSS0.Px4.p1.3.m3.1" class="ltx_Math" alttext="10K" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.3.m3.1a"><mrow id="S4.SS0.SSS0.Px4.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.cmml"><mn id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.2" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.2.cmml">10</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.1" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.3" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.3.m3.1b"><apply id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1"><times id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.2">10</cn><ci id="S4.SS0.SSS0.Px4.p1.3.m3.1.1.3.cmml" xref="S4.SS0.SSS0.Px4.p1.3.m3.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.3.m3.1c">10K</annotation></semantics></math> subjects of the CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> were assigned for training, and the remaining ones for performance validation using random pairs with challenging conditions (identified based on the poorest L2-norm values <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>). The final score is obtained by aggregating the comparison scores of ResNet with Squeeze-and-Excitation (SE) blocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> models of <math id="S4.SS0.SSS0.Px4.p1.4.m4.1" class="ltx_Math" alttext="50" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px4.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px4.p1.4.m4.1.1.cmml">50</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.4.m4.1b"><cn type="integer" id="S4.SS0.SSS0.Px4.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.4.m4.1.1">50</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.4.m4.1c">50</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px4.p1.5.m5.1" class="ltx_Math" alttext="100" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px4.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px4.p1.5.m5.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.5.m5.1b"><cn type="integer" id="S4.SS0.SSS0.Px4.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.5.m5.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.5.m5.1c">100</annotation></semantics></math>, and <math id="S4.SS0.SSS0.Px4.p1.6.m6.1" class="ltx_Math" alttext="152" display="inline"><semantics id="S4.SS0.SSS0.Px4.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px4.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px4.p1.6.m6.1.1.cmml">152</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px4.p1.6.m6.1b"><cn type="integer" id="S4.SS0.SSS0.Px4.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px4.p1.6.m6.1.1">152</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px4.p1.6.m6.1c">152</annotation></semantics></math> layers, along with the horizontally flipped instances through score fusion.</p>
</div>
<div id="S4.SS0.SSS0.Px4.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px4.p2.1" class="ltx_p">Code: <a target="_blank" href="https://github.com/kalebmes/cvpr_frcsyn" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/kalebmes/cvpr_frcsyn</a></p>
</div>
</section>
<section id="S4.SS0.SSS0.Px5" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CTAI (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px5.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px5.p1.2" class="ltx_p">By analyzing popular synthetic data, they found that intra-class and inter-class noise was widely present. Data cleaning can effectively remove the bad examples of synthetic data and retain important images from a large amount of synthetic data. In order to select the optimal synthetic data, they first trained an IResNet-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> model with Squeeze-and-Excitation (SE) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">19</span></a>]</cite> blocks using CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> to extract features of synthetic images from DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, and DigiFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>. Subsequently, they used DBSCAN clustering to segregate intra-class noise and removed IDs with a class center feature cosine similarity greater than <math id="S4.SS0.SSS0.Px5.p1.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px5.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.1.m1.1b"><cn type="float" id="S4.SS0.SSS0.Px5.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.1.m1.1c">0.5</annotation></semantics></math>. Finally, they used the cleaned synthetic data merged with CASIA-WebFace to finetune the IResNet-100 for a second data refinement. From the final refined synthetic dataset, they sampled <math id="S4.SS0.SSS0.Px5.p1.2.m2.1" class="ltx_Math" alttext="500K" display="inline"><semantics id="S4.SS0.SSS0.Px5.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px5.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px5.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.2">500</cn><ci id="S4.SS0.SSS0.Px5.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px5.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px5.p1.2.m2.1c">500K</annotation></semantics></math> face images while retaining as many IDs as possible to build their synthetic training set. Regarding the FR model, in particular Sub-Task 2.3 in which they achieved their highest position among all sub-tasks, they trained IResNet-100 with AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> loss (A1) and CosFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> loss (A2) with mask and occlusion augmentation on CASIA-WebFace and the refined synthetic data. They used an ensemble of A1, A2, and a model trained with only synthetic data. Furthermore, data augmentation was employed to enhance all features.</p>
</div>
<div id="S4.SS0.SSS0.Px5.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px5.p2.1" class="ltx_p">Code: <a target="_blank" href="https://github.com/liuhao-lh/FRCSyn-Challenge" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/liuhao-lh/FRCSyn-Challenge</a></p>
</div>
</section>
<section id="S4.SS0.SSS0.Px6" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Idiap-SynthDistill (Sub-Tasks 1.2 and 2.2):</h4>

<div id="S4.SS0.SSS0.Px6.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px6.p1.1" class="ltx_p">The proposed method was based on SynthDistill <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>, which is an end-to-end approach, generating synthetic images and training the FR model in the same training loop. Instead of using the pre-trained model in a separate step, they directly used it in the training loop for supervision, while a new student FR model was trained fully using synthetic data generated from a StyleGAN model. For generating synthetic images, they trained StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">22</span></a>]</cite> with the CASIA-WebFace database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> and then dynamically generated synthetic images during training based on the training loss. For the dynamic image generation, they used the training loss to find the most difficult synthetic image in each batch, and then they generated a new batch of synthetic images by re-sampling the most difficult samples. Regarding the FR model, they used a model with the IResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> architecture and trained it with synthetic data using SynthDistill. They used the Adam optimizer with an initial learning rate of <math id="S4.SS0.SSS0.Px6.p1.1.m1.1" class="ltx_Math" alttext="0.001" display="inline"><semantics id="S4.SS0.SSS0.Px6.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px6.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1.cmml">0.001</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px6.p1.1.m1.1b"><cn type="float" id="S4.SS0.SSS0.Px6.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px6.p1.1.m1.1.1">0.001</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px6.p1.1.m1.1c">0.001</annotation></semantics></math> and trained their student model with the same loss function as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite>. For thresholding, a subset of DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> was used to determine the optimal threshold for maximizing verification accuracy, using a 10-fold cross-validation approach based on a random selection of identities and comparison pairs.</p>
</div>
<div id="S4.SS0.SSS0.Px6.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px6.p2.1" class="ltx_p">Code: <a target="_blank" href="https://gitlab.idiap.ch/bob/bob.paper.ijcb2023_synthdistill" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://gitlab.idiap.ch/bob/bob.paper.ijcb2023_synthdistill</a></p>
</div>
</section>
<section id="S4.SS0.SSS0.Px7" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">INESC-IGD (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px7.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px7.p1.8" class="ltx_p">In all sub-tasks they trained a ResNet-100 with ElasticCosFac-Plus loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite> using the settings presented in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">5</span></a>]</cite>. For the training dataset, DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite>, IDiff-Face Uniform, and IDiff-Face Two-stage <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> datasets were merged and their images were labeled with ethnicity labels using a similar approach to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">35</span></a>]</cite>.
For Sub-Tasks 1.1 and 2.1, they created a synthetic training dataset containing <math id="S4.SS0.SSS0.Px7.p1.1.m1.1" class="ltx_Math" alttext="500K" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px7.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1.2.cmml">500</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px7.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px7.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px7.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px7.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px7.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1.2">500</cn><ci id="S4.SS0.SSS0.Px7.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.1.m1.1c">500K</annotation></semantics></math> face images by sampling <math id="S4.SS0.SSS0.Px7.p1.2.m2.1" class="ltx_Math" alttext="7K" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px7.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1.2.cmml">7</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px7.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px7.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px7.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px7.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px7.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1.2">7</cn><ci id="S4.SS0.SSS0.Px7.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.2.m2.1c">7K</annotation></semantics></math> balanced identities, in terms of ethnicity labels. For Sub-Tasks 1.2 and 2.2, they created a synthetic training dataset containing <math id="S4.SS0.SSS0.Px7.p1.3.m3.1" class="ltx_Math" alttext="2.1M" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.3.m3.1a"><mrow id="S4.SS0.SSS0.Px7.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.3.m3.1.1.2" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1.2.cmml">2.1</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px7.p1.3.m3.1.1.1" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px7.p1.3.m3.1.1.3" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.3.m3.1b"><apply id="S4.SS0.SSS0.Px7.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1"><times id="S4.SS0.SSS0.Px7.p1.3.m3.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1.1"></times><cn type="float" id="S4.SS0.SSS0.Px7.p1.3.m3.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1.2">2.1</cn><ci id="S4.SS0.SSS0.Px7.p1.3.m3.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.3.m3.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.3.m3.1c">2.1M</annotation></semantics></math> face images by sampling <math id="S4.SS0.SSS0.Px7.p1.4.m4.1" class="ltx_Math" alttext="50K" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px7.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.2.cmml">50</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.3" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1"><times id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.2">50</cn><ci id="S4.SS0.SSS0.Px7.p1.4.m4.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.4.m4.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.4.m4.1c">50K</annotation></semantics></math> identities from the training datasets. For Sub-Tasks 1.3 and 2.3, two instances of ResNet-100 were trained on CASIA-WebFace and a subset of synthetic datasets (<math id="S4.SS0.SSS0.Px7.p1.5.m5.1" class="ltx_Math" alttext="400K" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.5.m5.1a"><mrow id="S4.SS0.SSS0.Px7.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.5.m5.1.1.2" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1.2.cmml">400</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px7.p1.5.m5.1.1.1" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px7.p1.5.m5.1.1.3" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.5.m5.1b"><apply id="S4.SS0.SSS0.Px7.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1"><times id="S4.SS0.SSS0.Px7.p1.5.m5.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px7.p1.5.m5.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1.2">400</cn><ci id="S4.SS0.SSS0.Px7.p1.5.m5.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.5.m5.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.5.m5.1c">400K</annotation></semantics></math> images of <math id="S4.SS0.SSS0.Px7.p1.6.m6.1" class="ltx_Math" alttext="9K" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.6.m6.1a"><mrow id="S4.SS0.SSS0.Px7.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.cmml"><mn id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.2" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.2.cmml">9</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.1" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.6.m6.1b"><apply id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1"><times id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.2.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.2">9</cn><ci id="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3.cmml" xref="S4.SS0.SSS0.Px7.p1.6.m6.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.6.m6.1c">9K</annotation></semantics></math> identities), respectively. The synthetic datasets were sampled from DCFace and IDiff-Face. During the testing phase of Sub-Tasks 1.3 and 2.3, feature embeddings were obtained from trained models and the weighted sum of <math id="S4.SS0.SSS0.Px7.p1.7.m7.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px7.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px7.p1.7.m7.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.7.m7.1b"><cn type="float" id="S4.SS0.SSS0.Px7.p1.7.m7.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.7.m7.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.7.m7.1c">0.5</annotation></semantics></math> score-level fusion was utilized. During the FR training of all sub-tasks, the training datasets were augmented using the RandAug utilized in IDiff-Face and occluded augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">34</span></a>]</cite> with probabilities of <math id="S4.SS0.SSS0.Px7.p1.8.m8.1" class="ltx_Math" alttext="0.4" display="inline"><semantics id="S4.SS0.SSS0.Px7.p1.8.m8.1a"><mn id="S4.SS0.SSS0.Px7.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px7.p1.8.m8.1.1.cmml">0.4</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px7.p1.8.m8.1b"><cn type="float" id="S4.SS0.SSS0.Px7.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px7.p1.8.m8.1.1">0.4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px7.p1.8.m8.1c">0.4</annotation></semantics></math>.</p>
</div>
<div id="S4.SS0.SSS0.Px7.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px7.p2.1" class="ltx_p">Code: <a target="_blank" href="https://github.com/NetoPedro/Equilibrium-Face-Recognition" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/NetoPedro/Equilibrium-Face-Recognition</a></p>
</div>
</section>
<section id="S4.SS0.SSS0.Px8" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">UNICA-IGD-LSI (All sub-tasks):</h4>

<div id="S4.SS0.SSS0.Px8.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px8.p1.11" class="ltx_p">They used the DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> synthetic dataset as it led to remarkable performance gains under well-known evaluation benchmarks for face verification, while combined with real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>. They trained a ResNet-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> network using CosFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite> with a margin penalty of 0.35 and a scale term of 64. The similarity mean difference between real-only and synthetic-only samples was scaled and added to the loss value. They trained the model for <math id="S4.SS0.SSS0.Px8.p1.1.m1.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px8.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.1.m1.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.1.m1.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.1.m1.1c">40</annotation></semantics></math> epochs with a batch size of <math id="S4.SS0.SSS0.Px8.p1.2.m2.1" class="ltx_Math" alttext="512" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px8.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px8.p1.2.m2.1.1.cmml">512</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.2.m2.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.2.m2.1.1">512</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.2.m2.1c">512</annotation></semantics></math> and an initial learning rate of <math id="S4.SS0.SSS0.Px8.p1.3.m3.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px8.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px8.p1.3.m3.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.3.m3.1b"><cn type="float" id="S4.SS0.SSS0.Px8.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.3.m3.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.3.m3.1c">0.1</annotation></semantics></math>, which was divided by <math id="S4.SS0.SSS0.Px8.p1.4.m4.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px8.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px8.p1.4.m4.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.4.m4.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.4.m4.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.4.m4.1c">10</annotation></semantics></math> after <math id="S4.SS0.SSS0.Px8.p1.5.m5.1" class="ltx_Math" alttext="10" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.5.m5.1a"><mn id="S4.SS0.SSS0.Px8.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px8.p1.5.m5.1.1.cmml">10</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.5.m5.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.5.m5.1.1">10</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.5.m5.1c">10</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px8.p1.6.m6.1" class="ltx_Math" alttext="22" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.6.m6.1a"><mn id="S4.SS0.SSS0.Px8.p1.6.m6.1.1" xref="S4.SS0.SSS0.Px8.p1.6.m6.1.1.cmml">22</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.6.m6.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.6.m6.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.6.m6.1.1">22</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.6.m6.1c">22</annotation></semantics></math>, <math id="S4.SS0.SSS0.Px8.p1.7.m7.1" class="ltx_Math" alttext="30" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.7.m7.1a"><mn id="S4.SS0.SSS0.Px8.p1.7.m7.1.1" xref="S4.SS0.SSS0.Px8.p1.7.m7.1.1.cmml">30</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.7.m7.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.7.m7.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.7.m7.1.1">30</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.7.m7.1c">30</annotation></semantics></math>, and <math id="S4.SS0.SSS0.Px8.p1.8.m8.1" class="ltx_Math" alttext="40" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.8.m8.1a"><mn id="S4.SS0.SSS0.Px8.p1.8.m8.1.1" xref="S4.SS0.SSS0.Px8.p1.8.m8.1.1.cmml">40</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.8.m8.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.8.m8.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.8.m8.1.1">40</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.8.m8.1c">40</annotation></semantics></math> epochs. During the training phase, the synthetic samples were augmented using RandAugment with <math id="S4.SS0.SSS0.Px8.p1.9.m9.1" class="ltx_Math" alttext="4" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.9.m9.1a"><mn id="S4.SS0.SSS0.Px8.p1.9.m9.1.1" xref="S4.SS0.SSS0.Px8.p1.9.m9.1.1.cmml">4</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.9.m9.1b"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.9.m9.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.9.m9.1.1">4</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.9.m9.1c">4</annotation></semantics></math> operations and a magnitude of 16, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">7</span></a>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2</span></a>]</cite>. For Sub-Tasks 1.3 and 2.3, the chosen synthetic dataset was combined with CASIA-Webface <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>, obtaining a total of <math id="S4.SS0.SSS0.Px8.p1.10.m10.1" class="ltx_Math" alttext="1M" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.10.m10.1a"><mrow id="S4.SS0.SSS0.Px8.p1.10.m10.1.1" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1.cmml"><mn id="S4.SS0.SSS0.Px8.p1.10.m10.1.1.2" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px8.p1.10.m10.1.1.1" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px8.p1.10.m10.1.1.3" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.10.m10.1b"><apply id="S4.SS0.SSS0.Px8.p1.10.m10.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1"><times id="S4.SS0.SSS0.Px8.p1.10.m10.1.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px8.p1.10.m10.1.1.2.cmml" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1.2">1</cn><ci id="S4.SS0.SSS0.Px8.p1.10.m10.1.1.3.cmml" xref="S4.SS0.SSS0.Px8.p1.10.m10.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.10.m10.1c">1M</annotation></semantics></math> images from <math id="S4.SS0.SSS0.Px8.p1.11.m11.2" class="ltx_Math" alttext="20,572" display="inline"><semantics id="S4.SS0.SSS0.Px8.p1.11.m11.2a"><mrow id="S4.SS0.SSS0.Px8.p1.11.m11.2.3.2" xref="S4.SS0.SSS0.Px8.p1.11.m11.2.3.1.cmml"><mn id="S4.SS0.SSS0.Px8.p1.11.m11.1.1" xref="S4.SS0.SSS0.Px8.p1.11.m11.1.1.cmml">20</mn><mo id="S4.SS0.SSS0.Px8.p1.11.m11.2.3.2.1" xref="S4.SS0.SSS0.Px8.p1.11.m11.2.3.1.cmml">,</mo><mn id="S4.SS0.SSS0.Px8.p1.11.m11.2.2" xref="S4.SS0.SSS0.Px8.p1.11.m11.2.2.cmml">572</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px8.p1.11.m11.2b"><list id="S4.SS0.SSS0.Px8.p1.11.m11.2.3.1.cmml" xref="S4.SS0.SSS0.Px8.p1.11.m11.2.3.2"><cn type="integer" id="S4.SS0.SSS0.Px8.p1.11.m11.1.1.cmml" xref="S4.SS0.SSS0.Px8.p1.11.m11.1.1">20</cn><cn type="integer" id="S4.SS0.SSS0.Px8.p1.11.m11.2.2.cmml" xref="S4.SS0.SSS0.Px8.p1.11.m11.2.2">572</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px8.p1.11.m11.2c">20,572</annotation></semantics></math> identities.</p>
</div>
<div id="S4.SS0.SSS0.Px8.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px8.p2.1" class="ltx_p">Code: <a target="_blank" href="https://github.com/atzoriandrea/FRCSyn2" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/atzoriandrea/FRCSyn2</a></p>
</div>
</section>
<section id="S4.SS0.SSS0.Px9" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">SRCN_AIVL (Sub-Task 1.1):</h4>

<div id="S4.SS0.SSS0.Px9.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px9.p1.4" class="ltx_p">They selected <math id="S4.SS0.SSS0.Px9.p1.1.m1.1" class="ltx_Math" alttext="400K" display="inline"><semantics id="S4.SS0.SSS0.Px9.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px9.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px9.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px9.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px9.p1.1.m1.1.1.2.cmml">400</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px9.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px9.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px9.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px9.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px9.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px9.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px9.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px9.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px9.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px9.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px9.p1.1.m1.1.1.2">400</cn><ci id="S4.SS0.SSS0.Px9.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px9.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px9.p1.1.m1.1c">400K</annotation></semantics></math> samples from the DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> database and labeled the ethnicity of each subject, as they considered that the racial distribution gap may lead to bad performance in testing. Based on this insight, they trained IDiff-Face <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite> with CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite> database generating <math id="S4.SS0.SSS0.Px9.p1.2.m2.1" class="ltx_Math" alttext="100K" display="inline"><semantics id="S4.SS0.SSS0.Px9.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px9.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px9.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px9.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px9.p1.2.m2.1.1.2.cmml">100</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px9.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px9.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px9.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px9.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px9.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px9.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px9.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px9.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px9.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px9.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px9.p1.2.m2.1.1.2">100</cn><ci id="S4.SS0.SSS0.Px9.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px9.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px9.p1.2.m2.1c">100K</annotation></semantics></math> synthetic face images of specific races. Regarding the FR system, they used two custom ResNet-101 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> trained with AdaFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> function. The models were trained for <math id="S4.SS0.SSS0.Px9.p1.3.m3.1" class="ltx_Math" alttext="60" display="inline"><semantics id="S4.SS0.SSS0.Px9.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px9.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px9.p1.3.m3.1.1.cmml">60</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px9.p1.3.m3.1b"><cn type="integer" id="S4.SS0.SSS0.Px9.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px9.p1.3.m3.1.1">60</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px9.p1.3.m3.1c">60</annotation></semantics></math> epochs with an initial learning rate of <math id="S4.SS0.SSS0.Px9.p1.4.m4.1" class="ltx_Math" alttext="0.1" display="inline"><semantics id="S4.SS0.SSS0.Px9.p1.4.m4.1a"><mn id="S4.SS0.SSS0.Px9.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px9.p1.4.m4.1.1.cmml">0.1</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px9.p1.4.m4.1b"><cn type="float" id="S4.SS0.SSS0.Px9.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px9.p1.4.m4.1.1">0.1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px9.p1.4.m4.1c">0.1</annotation></semantics></math>, which was adjusted at predefined milestones. Their training data underwent further preprocessing, including padding crop augmentation, low-resolution augmentation, photometric augmentation, random grayscale, and normalization. For the inference, data preprocessing involved an MTCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">50</span></a>]</cite> and resizing all data. After cropping and alignment, they fed the image and the flipped image into the two models. After obtaining the two feature embeddings, they combined them and performed the similarity calculation with these embeddings.</p>
</div>
<div id="S4.SS0.SSS0.Px9.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px9.p2.1" class="ltx_p">Code: <a target="_blank" href="https://github.com/Value-Jack/2nd-Edition-FRCSyn" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/Value-Jack/2nd-Edition-FRCSyn</a></p>
</div>
</section>
<section id="S4.SS0.SSS0.Px10" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">CBSR-Samsung (Sub-Tasks 1.3 and 2.3):</h4>

<div id="S4.SS0.SSS0.Px10.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px10.p1.5" class="ltx_p">They first trained a FR model using CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>. Then, they used it to de-overlap DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> from CASIA, as DCFace was trained using that real database. For the synthetic dataset, they compared the performance of models trained with three synthetic datasets, including GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, DCFace, and IDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, and finally selected DCFace as the only synthetic training set. They created a validation dataset including three subsets for three different testing scenarios: <span id="S4.SS0.SSS0.Px10.p1.5.1" class="ltx_text ltx_font_italic">i)</span> random sample pairs from DCFace; <span id="S4.SS0.SSS0.Px10.p1.5.2" class="ltx_text ltx_font_italic">ii)</span> randomly positioned vertical bar masks to the images to simulate the self-occlusion due to pose; and <span id="S4.SS0.SSS0.Px10.p1.5.3" class="ltx_text ltx_font_italic">iii)</span> add a mask and sunglasses to images by detecting the landmarks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">45</span></a>]</cite>. All validation subsets consist of <math id="S4.SS0.SSS0.Px10.p1.1.m1.1" class="ltx_Math" alttext="6K" display="inline"><semantics id="S4.SS0.SSS0.Px10.p1.1.m1.1a"><mrow id="S4.SS0.SSS0.Px10.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px10.p1.1.m1.1.1.cmml"><mn id="S4.SS0.SSS0.Px10.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px10.p1.1.m1.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px10.p1.1.m1.1.1.1" xref="S4.SS0.SSS0.Px10.p1.1.m1.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px10.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px10.p1.1.m1.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px10.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px10.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.1.m1.1.1"><times id="S4.SS0.SSS0.Px10.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.1.m1.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px10.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px10.p1.1.m1.1.1.2">6</cn><ci id="S4.SS0.SSS0.Px10.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px10.p1.1.m1.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px10.p1.1.m1.1c">6K</annotation></semantics></math> positive pairs and <math id="S4.SS0.SSS0.Px10.p1.2.m2.1" class="ltx_Math" alttext="6K" display="inline"><semantics id="S4.SS0.SSS0.Px10.p1.2.m2.1a"><mrow id="S4.SS0.SSS0.Px10.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px10.p1.2.m2.1.1.cmml"><mn id="S4.SS0.SSS0.Px10.p1.2.m2.1.1.2" xref="S4.SS0.SSS0.Px10.p1.2.m2.1.1.2.cmml">6</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px10.p1.2.m2.1.1.1" xref="S4.SS0.SSS0.Px10.p1.2.m2.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px10.p1.2.m2.1.1.3" xref="S4.SS0.SSS0.Px10.p1.2.m2.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px10.p1.2.m2.1b"><apply id="S4.SS0.SSS0.Px10.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.2.m2.1.1"><times id="S4.SS0.SSS0.Px10.p1.2.m2.1.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.2.m2.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px10.p1.2.m2.1.1.2.cmml" xref="S4.SS0.SSS0.Px10.p1.2.m2.1.1.2">6</cn><ci id="S4.SS0.SSS0.Px10.p1.2.m2.1.1.3.cmml" xref="S4.SS0.SSS0.Px10.p1.2.m2.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px10.p1.2.m2.1c">6K</annotation></semantics></math> negative pairs. Finally, they concatenated these subsets as the validation set. Subsequently, they conducted an intra-class clustering for all datasets using DBSCAN (<math id="S4.SS0.SSS0.Px10.p1.3.m3.1" class="ltx_Math" alttext="0.3" display="inline"><semantics id="S4.SS0.SSS0.Px10.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px10.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px10.p1.3.m3.1.1.cmml">0.3</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px10.p1.3.m3.1b"><cn type="float" id="S4.SS0.SSS0.Px10.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.3.m3.1.1">0.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px10.p1.3.m3.1c">0.3</annotation></semantics></math> threshold) and removed the samples that were separated from the class center. They merged the refined datasets and trained IResNet-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> with AdaFace loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite>. In addition, they adopted two augmentation strategies, <span id="S4.SS0.SSS0.Px10.p1.5.4" class="ltx_text ltx_font_italic">i.e.,</span> photometric augmentation and rescaling. After that, they trained two FR models using occlusion augmentation with <math id="S4.SS0.SSS0.Px10.p1.4.m4.1" class="ltx_Math" alttext="10\%" display="inline"><semantics id="S4.SS0.SSS0.Px10.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px10.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px10.p1.4.m4.1.1.cmml"><mn id="S4.SS0.SSS0.Px10.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px10.p1.4.m4.1.1.2.cmml">10</mn><mo id="S4.SS0.SSS0.Px10.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px10.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px10.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px10.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.4.m4.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px10.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px10.p1.4.m4.1.1.2.cmml" xref="S4.SS0.SSS0.Px10.p1.4.m4.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px10.p1.4.m4.1c">10\%</annotation></semantics></math> and <math id="S4.SS0.SSS0.Px10.p1.5.m5.1" class="ltx_Math" alttext="30\%" display="inline"><semantics id="S4.SS0.SSS0.Px10.p1.5.m5.1a"><mrow id="S4.SS0.SSS0.Px10.p1.5.m5.1.1" xref="S4.SS0.SSS0.Px10.p1.5.m5.1.1.cmml"><mn id="S4.SS0.SSS0.Px10.p1.5.m5.1.1.2" xref="S4.SS0.SSS0.Px10.p1.5.m5.1.1.2.cmml">30</mn><mo id="S4.SS0.SSS0.Px10.p1.5.m5.1.1.1" xref="S4.SS0.SSS0.Px10.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px10.p1.5.m5.1b"><apply id="S4.SS0.SSS0.Px10.p1.5.m5.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.5.m5.1.1"><csymbol cd="latexml" id="S4.SS0.SSS0.Px10.p1.5.m5.1.1.1.cmml" xref="S4.SS0.SSS0.Px10.p1.5.m5.1.1.1">percent</csymbol><cn type="integer" id="S4.SS0.SSS0.Px10.p1.5.m5.1.1.2.cmml" xref="S4.SS0.SSS0.Px10.p1.5.m5.1.1.2">30</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px10.p1.5.m5.1c">30\%</annotation></semantics></math> probability, respectively. Finally, they submitted the average similarity score of the two models.</p>
</div>
</section>
<section id="S4.SS0.SSS0.Px11" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">BOVIFOCR-UFPR (Sub-Tasks 1.2 and 2.1):</h4>

<div id="S4.SS0.SSS0.Px11.p1" class="ltx_para">
<p id="S4.SS0.SSS0.Px11.p1.4" class="ltx_p">They chose DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> as the synthetic dataset and ResNet-100 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> as the backbone, trained with the ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> loss function. The images used for training were augmented using a Random Flip with a probability of <math id="S4.SS0.SSS0.Px11.p1.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S4.SS0.SSS0.Px11.p1.1.m1.1a"><mn id="S4.SS0.SSS0.Px11.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px11.p1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px11.p1.1.m1.1b"><cn type="float" id="S4.SS0.SSS0.Px11.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px11.p1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px11.p1.1.m1.1c">0.5</annotation></semantics></math>. They also applied random erasing and RandAugment as additional augmentations. The model was trained using the Insightface library for <math id="S4.SS0.SSS0.Px11.p1.2.m2.1" class="ltx_Math" alttext="20" display="inline"><semantics id="S4.SS0.SSS0.Px11.p1.2.m2.1a"><mn id="S4.SS0.SSS0.Px11.p1.2.m2.1.1" xref="S4.SS0.SSS0.Px11.p1.2.m2.1.1.cmml">20</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px11.p1.2.m2.1b"><cn type="integer" id="S4.SS0.SSS0.Px11.p1.2.m2.1.1.cmml" xref="S4.SS0.SSS0.Px11.p1.2.m2.1.1">20</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px11.p1.2.m2.1c">20</annotation></semantics></math> epochs within a batch size of <math id="S4.SS0.SSS0.Px11.p1.3.m3.1" class="ltx_Math" alttext="128" display="inline"><semantics id="S4.SS0.SSS0.Px11.p1.3.m3.1a"><mn id="S4.SS0.SSS0.Px11.p1.3.m3.1.1" xref="S4.SS0.SSS0.Px11.p1.3.m3.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px11.p1.3.m3.1b"><cn type="integer" id="S4.SS0.SSS0.Px11.p1.3.m3.1.1.cmml" xref="S4.SS0.SSS0.Px11.p1.3.m3.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px11.p1.3.m3.1c">128</annotation></semantics></math>, running for approximately <math id="S4.SS0.SSS0.Px11.p1.4.m4.1" class="ltx_Math" alttext="78K" display="inline"><semantics id="S4.SS0.SSS0.Px11.p1.4.m4.1a"><mrow id="S4.SS0.SSS0.Px11.p1.4.m4.1.1" xref="S4.SS0.SSS0.Px11.p1.4.m4.1.1.cmml"><mn id="S4.SS0.SSS0.Px11.p1.4.m4.1.1.2" xref="S4.SS0.SSS0.Px11.p1.4.m4.1.1.2.cmml">78</mn><mo lspace="0em" rspace="0em" id="S4.SS0.SSS0.Px11.p1.4.m4.1.1.1" xref="S4.SS0.SSS0.Px11.p1.4.m4.1.1.1.cmml">​</mo><mi id="S4.SS0.SSS0.Px11.p1.4.m4.1.1.3" xref="S4.SS0.SSS0.Px11.p1.4.m4.1.1.3.cmml">K</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px11.p1.4.m4.1b"><apply id="S4.SS0.SSS0.Px11.p1.4.m4.1.1.cmml" xref="S4.SS0.SSS0.Px11.p1.4.m4.1.1"><times id="S4.SS0.SSS0.Px11.p1.4.m4.1.1.1.cmml" xref="S4.SS0.SSS0.Px11.p1.4.m4.1.1.1"></times><cn type="integer" id="S4.SS0.SSS0.Px11.p1.4.m4.1.1.2.cmml" xref="S4.SS0.SSS0.Px11.p1.4.m4.1.1.2">78</cn><ci id="S4.SS0.SSS0.Px11.p1.4.m4.1.1.3.cmml" xref="S4.SS0.SSS0.Px11.p1.4.m4.1.1.3">𝐾</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px11.p1.4.m4.1c">78K</annotation></semantics></math> iterations.</p>
</div>
<div id="S4.SS0.SSS0.Px11.p2" class="ltx_para ltx_noindent">
<p id="S4.SS0.SSS0.Px11.p2.1" class="ltx_p">Code: <a target="_blank" href="https://github.com/PedroBVidal/insightface" title="" class="ltx_ref ltx_href" style="font-size:80%;">https://github.com/PedroBVidal/insightface</a></p>
</div>
<figure id="S4.T3" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T3.2" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:136.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-6.7pt,3.9pt) scale(0.945691078010635,0.945691078010635) ;">
<table id="S4.T3.2.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.2.1.1.1" class="ltx_tr">
<th id="S4.T3.2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6"><span id="S4.T3.2.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 1.1 (Bias Mitigation): Synthetic Data (Constrained)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.2.1.2.1" class="ltx_tr">
<th id="S4.T3.2.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T3.2.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T3.2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TO [%]</span></td>
<td id="S4.T3.2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T3.2.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SD [%]</span></td>
<td id="S4.T3.2.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T3.2.1.3.2" class="ltx_tr">
<th id="S4.T3.2.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.3.2.1.1" class="ltx_text" style="font-size:80%;">1</span></th>
<th id="S4.T3.2.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.2.1.3.2.2.1" class="ltx_text" style="font-size:80%;">ID R&amp;D</span></th>
<td id="S4.T3.2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.3.2.3.1" class="ltx_text" style="font-size:80%;">96.73</span></td>
<td id="S4.T3.2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.3.2.4.1" class="ltx_text" style="font-size:80%;">97.55</span></td>
<td id="S4.T3.2.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.3.2.5.1" class="ltx_text" style="font-size:80%;">0.82</span></td>
<td id="S4.T3.2.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.2.1.3.2.6.1" class="ltx_text" style="font-size:80%;">-5.31</span></td>
</tr>
<tr id="S4.T3.2.1.4.3" class="ltx_tr">
<th id="S4.T3.2.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.2.1.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T3.2.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.4.3.2.1" class="ltx_text" style="font-size:80%;">ADMIS</span></th>
<td id="S4.T3.2.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.3.1" class="ltx_text" style="font-size:80%;">94.30</span></td>
<td id="S4.T3.2.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.4.1" class="ltx_text" style="font-size:80%;">95.10</span></td>
<td id="S4.T3.2.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.5.1" class="ltx_text" style="font-size:80%;">0.80</span></td>
<td id="S4.T3.2.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.4.3.6.1" class="ltx_text" style="font-size:80%;">1.47</span></td>
</tr>
<tr id="S4.T3.2.1.5.4" class="ltx_tr">
<th id="S4.T3.2.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.2.1.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T3.2.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.5.4.2.1" class="ltx_text" style="font-size:80%;">SRCN_AIVL</span></th>
<td id="S4.T3.2.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.3.1" class="ltx_text" style="font-size:80%;">94.06</span></td>
<td id="S4.T3.2.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.4.1" class="ltx_text" style="font-size:80%;">95.12</span></td>
<td id="S4.T3.2.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.5.1" class="ltx_text" style="font-size:80%;">1.07</span></td>
<td id="S4.T3.2.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.5.4.6.1" class="ltx_text" style="font-size:80%;">-0.54</span></td>
</tr>
<tr id="S4.T3.2.1.6.5" class="ltx_tr">
<th id="S4.T3.2.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.2.1.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T3.2.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.6.5.2.1" class="ltx_text" style="font-size:80%;">OPDAI</span></th>
<td id="S4.T3.2.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.3.1" class="ltx_text" style="font-size:80%;">93.75</span></td>
<td id="S4.T3.2.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.4.1" class="ltx_text" style="font-size:80%;">94.92</span></td>
<td id="S4.T3.2.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.5.1" class="ltx_text" style="font-size:80%;">1.17</span></td>
<td id="S4.T3.2.1.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.6.5.6.1" class="ltx_text" style="font-size:80%;">1.02</span></td>
</tr>
<tr id="S4.T3.2.1.7.6" class="ltx_tr">
<th id="S4.T3.2.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.2.1.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T3.2.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.2.1.7.6.2.1" class="ltx_text" style="font-size:80%;">CTAI</span></th>
<td id="S4.T3.2.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.3.1" class="ltx_text" style="font-size:80%;">93.21</span></td>
<td id="S4.T3.2.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.4.1" class="ltx_text" style="font-size:80%;">94.74</span></td>
<td id="S4.T3.2.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.5.1" class="ltx_text" style="font-size:80%;">1.53</span></td>
<td id="S4.T3.2.1.7.6.6" class="ltx_td ltx_align_center"><span id="S4.T3.2.1.7.6.6.1" class="ltx_text" style="font-size:80%;">-0.63</span></td>
</tr>
<tr id="S4.T3.2.1.8.7" class="ltx_tr">
<th id="S4.T3.2.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.2.1.8.7.1.1" class="ltx_text" style="font-size:80%;">6</span></th>
<th id="S4.T3.2.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.2.1.8.7.2.1" class="ltx_text" style="font-size:80%;">K-IBS-DS</span></th>
<td id="S4.T3.2.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.2.1.8.7.3.1" class="ltx_text" style="font-size:80%;">92.91</span></td>
<td id="S4.T3.2.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.2.1.8.7.4.1" class="ltx_text" style="font-size:80%;">94.11</span></td>
<td id="S4.T3.2.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.2.1.8.7.5.1" class="ltx_text" style="font-size:80%;">1.20</span></td>
<td id="S4.T3.2.1.8.7.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.2.1.8.7.6.1" class="ltx_text" style="font-size:80%;">1.58</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T3.3" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:139.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-4.1pt,2.4pt) scale(0.966345049761116,0.966345049761116) ;">
<table id="S4.T3.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.3.1.1.1" class="ltx_tr">
<th id="S4.T3.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6"><span id="S4.T3.3.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 1.2 (Bias Mitigation): Synthetic Data (Unconstrained)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.3.1.2.1" class="ltx_tr">
<th id="S4.T3.3.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.3.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T3.3.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.3.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T3.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TO [%]</span></td>
<td id="S4.T3.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T3.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SD [%]</span></td>
<td id="S4.T3.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T3.3.1.3.2" class="ltx_tr">
<th id="S4.T3.3.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.3.1.3.2.1.1" class="ltx_text" style="font-size:80%;">1</span></th>
<th id="S4.T3.3.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.3.1.3.2.2.1" class="ltx_text" style="font-size:80%;">ID R&amp;D</span></th>
<td id="S4.T3.3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.3.2.3.1" class="ltx_text" style="font-size:80%;">96.73</span></td>
<td id="S4.T3.3.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.3.2.4.1" class="ltx_text" style="font-size:80%;">97.55</span></td>
<td id="S4.T3.3.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.3.2.5.1" class="ltx_text" style="font-size:80%;">0.82</span></td>
<td id="S4.T3.3.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.3.1.3.2.6.1" class="ltx_text" style="font-size:80%;">-5.31</span></td>
</tr>
<tr id="S4.T3.3.1.4.3" class="ltx_tr">
<th id="S4.T3.3.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.3.1.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T3.3.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.4.3.2.1" class="ltx_text" style="font-size:80%;">ADMIS</span></th>
<td id="S4.T3.3.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.4.3.3.1" class="ltx_text" style="font-size:80%;">95.72</span></td>
<td id="S4.T3.3.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.4.3.4.1" class="ltx_text" style="font-size:80%;">96.50</span></td>
<td id="S4.T3.3.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.4.3.5.1" class="ltx_text" style="font-size:80%;">0.78</span></td>
<td id="S4.T3.3.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.4.3.6.1" class="ltx_text" style="font-size:80%;">-0.56</span></td>
</tr>
<tr id="S4.T3.3.1.5.4" class="ltx_tr">
<th id="S4.T3.3.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.3.1.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T3.3.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.5.4.2.1" class="ltx_text" style="font-size:80%;">OPDAI</span></th>
<td id="S4.T3.3.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.5.4.3.1" class="ltx_text" style="font-size:80%;">94.12</span></td>
<td id="S4.T3.3.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.5.4.4.1" class="ltx_text" style="font-size:80%;">95.22</span></td>
<td id="S4.T3.3.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.5.4.5.1" class="ltx_text" style="font-size:80%;">1.11</span></td>
<td id="S4.T3.3.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.5.4.6.1" class="ltx_text" style="font-size:80%;">0.71</span></td>
</tr>
<tr id="S4.T3.3.1.6.5" class="ltx_tr">
<th id="S4.T3.3.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.3.1.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T3.3.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.6.5.2.1" class="ltx_text" style="font-size:80%;">INESC-IGD</span></th>
<td id="S4.T3.3.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.6.5.3.1" class="ltx_text" style="font-size:80%;">94.05</span></td>
<td id="S4.T3.3.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.6.5.4.1" class="ltx_text" style="font-size:80%;">95.22</span></td>
<td id="S4.T3.3.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.6.5.5.1" class="ltx_text" style="font-size:80%;">1.17</span></td>
<td id="S4.T3.3.1.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.6.5.6.1" class="ltx_text" style="font-size:80%;">1.04</span></td>
</tr>
<tr id="S4.T3.3.1.7.6" class="ltx_tr">
<th id="S4.T3.3.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.3.1.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T3.3.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.3.1.7.6.2.1" class="ltx_text" style="font-size:80%;">K-IBS-DS</span></th>
<td id="S4.T3.3.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.7.6.3.1" class="ltx_text" style="font-size:80%;">93.72</span></td>
<td id="S4.T3.3.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.7.6.4.1" class="ltx_text" style="font-size:80%;">94.88</span></td>
<td id="S4.T3.3.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.7.6.5.1" class="ltx_text" style="font-size:80%;">1.16</span></td>
<td id="S4.T3.3.1.7.6.6" class="ltx_td ltx_align_center"><span id="S4.T3.3.1.7.6.6.1" class="ltx_text" style="font-size:80%;">0.77</span></td>
</tr>
<tr id="S4.T3.3.1.8.7" class="ltx_tr">
<th id="S4.T3.3.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.3.1.8.7.1.1" class="ltx_text" style="font-size:80%;">6</span></th>
<th id="S4.T3.3.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.3.1.8.7.2.1" class="ltx_text" style="font-size:80%;">CTAI</span></th>
<td id="S4.T3.3.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.3.1.8.7.3.1" class="ltx_text" style="font-size:80%;">93.21</span></td>
<td id="S4.T3.3.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.3.1.8.7.4.1" class="ltx_text" style="font-size:80%;">94.74</span></td>
<td id="S4.T3.3.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.3.1.8.7.5.1" class="ltx_text" style="font-size:80%;">1.53</span></td>
<td id="S4.T3.3.1.8.7.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.3.1.8.7.6.1" class="ltx_text" style="font-size:80%;">-0.63</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T3.4" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:130pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.6pt,7.0pt) scale(0.902444847767449,0.902444847767449) ;">
<table id="S4.T3.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.4.1.1.1" class="ltx_tr">
<th id="S4.T3.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="6"><span id="S4.T3.4.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 1.3 (Bias Mitigation): Synthetic + Real Data (Constrained)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.4.1.2.1" class="ltx_tr">
<th id="S4.T3.4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.4.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T3.4.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.4.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T3.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">TO [%]</span></td>
<td id="S4.T3.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T3.4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.2.1.5.1" class="ltx_text ltx_font_bold" style="font-size:80%;">SD [%]</span></td>
<td id="S4.T3.4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.2.1.6.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T3.4.1.3.2" class="ltx_tr">
<th id="S4.T3.4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.4.1.3.2.1.1" class="ltx_text" style="font-size:80%;">1</span></th>
<th id="S4.T3.4.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.4.1.3.2.2.1" class="ltx_text" style="font-size:80%;">ADMIS</span></th>
<td id="S4.T3.4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.3.2.3.1" class="ltx_text" style="font-size:80%;">96.50</span></td>
<td id="S4.T3.4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.3.2.4.1" class="ltx_text" style="font-size:80%;">97.25</span></td>
<td id="S4.T3.4.1.3.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.3.2.5.1" class="ltx_text" style="font-size:80%;">0.75</span></td>
<td id="S4.T3.4.1.3.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.4.1.3.2.6.1" class="ltx_text" style="font-size:80%;">-1.33</span></td>
</tr>
<tr id="S4.T3.4.1.4.3" class="ltx_tr">
<th id="S4.T3.4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.4.1.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T3.4.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.4.1.4.3.2.1" class="ltx_text" style="font-size:80%;">K-IBS-DS</span></th>
<td id="S4.T3.4.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.4.3.3.1" class="ltx_text" style="font-size:80%;">96.17</span></td>
<td id="S4.T3.4.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.4.3.4.1" class="ltx_text" style="font-size:80%;">96.92</span></td>
<td id="S4.T3.4.1.4.3.5" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.4.3.5.1" class="ltx_text" style="font-size:80%;">0.75</span></td>
<td id="S4.T3.4.1.4.3.6" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.4.3.6.1" class="ltx_text" style="font-size:80%;">-1.37</span></td>
</tr>
<tr id="S4.T3.4.1.5.4" class="ltx_tr">
<th id="S4.T3.4.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.4.1.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T3.4.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.4.1.5.4.2.1" class="ltx_text" style="font-size:80%;">UNICA-IGD-LSI</span></th>
<td id="S4.T3.4.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.5.4.3.1" class="ltx_text" style="font-size:80%;">96.00</span></td>
<td id="S4.T3.4.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.5.4.4.1" class="ltx_text" style="font-size:80%;">96.70</span></td>
<td id="S4.T3.4.1.5.4.5" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.5.4.5.1" class="ltx_text" style="font-size:80%;">0.70</span></td>
<td id="S4.T3.4.1.5.4.6" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.5.4.6.1" class="ltx_text" style="font-size:80%;">-5.33</span></td>
</tr>
<tr id="S4.T3.4.1.6.5" class="ltx_tr">
<th id="S4.T3.4.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.4.1.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T3.4.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.4.1.6.5.2.1" class="ltx_text" style="font-size:80%;">OPDAI</span></th>
<td id="S4.T3.4.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.6.5.3.1" class="ltx_text" style="font-size:80%;">95.96</span></td>
<td id="S4.T3.4.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.6.5.4.1" class="ltx_text" style="font-size:80%;">96.80</span></td>
<td id="S4.T3.4.1.6.5.5" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.6.5.5.1" class="ltx_text" style="font-size:80%;">0.84</span></td>
<td id="S4.T3.4.1.6.5.6" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.6.5.6.1" class="ltx_text" style="font-size:80%;">-0.03</span></td>
</tr>
<tr id="S4.T3.4.1.7.6" class="ltx_tr">
<th id="S4.T3.4.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.4.1.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T3.4.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.4.1.7.6.2.1" class="ltx_text" style="font-size:80%;">INESC-IGD</span></th>
<td id="S4.T3.4.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.7.6.3.1" class="ltx_text" style="font-size:80%;">95.65</span></td>
<td id="S4.T3.4.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.7.6.4.1" class="ltx_text" style="font-size:80%;">96.33</span></td>
<td id="S4.T3.4.1.7.6.5" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.7.6.5.1" class="ltx_text" style="font-size:80%;">0.67</span></td>
<td id="S4.T3.4.1.7.6.6" class="ltx_td ltx_align_center"><span id="S4.T3.4.1.7.6.6.1" class="ltx_text" style="font-size:80%;">-0.12</span></td>
</tr>
<tr id="S4.T3.4.1.8.7" class="ltx_tr">
<th id="S4.T3.4.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.4.1.8.7.1.1" class="ltx_text" style="font-size:80%;">6</span></th>
<th id="S4.T3.4.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.4.1.8.7.2.1" class="ltx_text" style="font-size:80%;">CBSR-Samsung</span></th>
<td id="S4.T3.4.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.4.1.8.7.3.1" class="ltx_text" style="font-size:80%;">95.57</span></td>
<td id="S4.T3.4.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.4.1.8.7.4.1" class="ltx_text" style="font-size:80%;">96.54</span></td>
<td id="S4.T3.4.1.8.7.5" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.4.1.8.7.5.1" class="ltx_text" style="font-size:80%;">0.97</span></td>
<td id="S4.T3.4.1.8.7.6" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.4.1.8.7.6.1" class="ltx_text" style="font-size:80%;">-24.43</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T3.5" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:173.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(19.9pt,-14.8pt) scale(1.20551993588065,1.20551993588065) ;">
<table id="S4.T3.5.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.5.1.1.1" class="ltx_tr">
<th id="S4.T3.5.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="4"><span id="S4.T3.5.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 2.1 (Overall Improvement): Synthetic Data (Constrained)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.5.1.2.1" class="ltx_tr">
<th id="S4.T3.5.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.5.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T3.5.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.5.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T3.5.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.5.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T3.5.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.5.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T3.5.1.3.2" class="ltx_tr">
<th id="S4.T3.5.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.5.1.3.2.1.1" class="ltx_text" style="font-size:80%;">1</span></th>
<th id="S4.T3.5.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.5.1.3.2.2.1" class="ltx_text" style="font-size:80%;">OPDAI</span></th>
<td id="S4.T3.5.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.5.1.3.2.3.1" class="ltx_text" style="font-size:80%;">91.93</span></td>
<td id="S4.T3.5.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.5.1.3.2.4.1" class="ltx_text" style="font-size:80%;">3.09</span></td>
</tr>
<tr id="S4.T3.5.1.4.3" class="ltx_tr">
<th id="S4.T3.5.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.5.1.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T3.5.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.5.1.4.3.2.1" class="ltx_text" style="font-size:80%;">ID R&amp;D</span></th>
<td id="S4.T3.5.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.5.1.4.3.3.1" class="ltx_text" style="font-size:80%;">91.86</span></td>
<td id="S4.T3.5.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.5.1.4.3.4.1" class="ltx_text" style="font-size:80%;">2.99</span></td>
</tr>
<tr id="S4.T3.5.1.5.4" class="ltx_tr">
<th id="S4.T3.5.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.5.1.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T3.5.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.5.1.5.4.2.1" class="ltx_text" style="font-size:80%;">ADMIS</span></th>
<td id="S4.T3.5.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.5.1.5.4.3.1" class="ltx_text" style="font-size:80%;">91.19</span></td>
<td id="S4.T3.5.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.5.1.5.4.4.1" class="ltx_text" style="font-size:80%;">2.78</span></td>
</tr>
<tr id="S4.T3.5.1.6.5" class="ltx_tr">
<th id="S4.T3.5.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.5.1.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T3.5.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.5.1.6.5.2.1" class="ltx_text" style="font-size:80%;">K-IBS-DS</span></th>
<td id="S4.T3.5.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.5.1.6.5.3.1" class="ltx_text" style="font-size:80%;">91.05</span></td>
<td id="S4.T3.5.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.5.1.6.5.4.1" class="ltx_text" style="font-size:80%;">2.60</span></td>
</tr>
<tr id="S4.T3.5.1.7.6" class="ltx_tr">
<th id="S4.T3.5.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.5.1.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T3.5.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.5.1.7.6.2.1" class="ltx_text" style="font-size:80%;">CTAI</span></th>
<td id="S4.T3.5.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.5.1.7.6.3.1" class="ltx_text" style="font-size:80%;">90.59</span></td>
<td id="S4.T3.5.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.5.1.7.6.4.1" class="ltx_text" style="font-size:80%;">-1.94</span></td>
</tr>
<tr id="S4.T3.5.1.8.7" class="ltx_tr">
<th id="S4.T3.5.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.5.1.8.7.1.1" class="ltx_text" style="font-size:80%;">6</span></th>
<th id="S4.T3.5.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.5.1.8.7.2.1" class="ltx_text" style="font-size:80%;">BOVIFOCR-UFPR</span></th>
<td id="S4.T3.5.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.5.1.8.7.3.1" class="ltx_text" style="font-size:80%;">89.97</span></td>
<td id="S4.T3.5.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.5.1.8.7.4.1" class="ltx_text" style="font-size:80%;">3.71</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T3.6" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:179.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(23.1pt,-17.7pt) scale(1.24597070763106,1.24597070763106) ;">
<table id="S4.T3.6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.6.1.1.1" class="ltx_tr">
<th id="S4.T3.6.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="4"><span id="S4.T3.6.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 2.2 (Overall Improvement): Synthetic Data (Unconstrained)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.6.1.2.1" class="ltx_tr">
<th id="S4.T3.6.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.6.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T3.6.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.6.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T3.6.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T3.6.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T3.6.1.3.2" class="ltx_tr">
<th id="S4.T3.6.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.6.1.3.2.1.1" class="ltx_text" style="font-size:80%;">1</span></th>
<th id="S4.T3.6.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.6.1.3.2.2.1" class="ltx_text" style="font-size:80%;">Idiap-SynthDistill</span></th>
<td id="S4.T3.6.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.1.3.2.3.1" class="ltx_text" style="font-size:80%;">93.50</span></td>
<td id="S4.T3.6.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.6.1.3.2.4.1" class="ltx_text" style="font-size:80%;">-0.05</span></td>
</tr>
<tr id="S4.T3.6.1.4.3" class="ltx_tr">
<th id="S4.T3.6.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.6.1.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T3.6.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.6.1.4.3.2.1" class="ltx_text" style="font-size:80%;">ADMIS</span></th>
<td id="S4.T3.6.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.1.4.3.3.1" class="ltx_text" style="font-size:80%;">92.92</span></td>
<td id="S4.T3.6.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.1.4.3.4.1" class="ltx_text" style="font-size:80%;">0.21</span></td>
</tr>
<tr id="S4.T3.6.1.5.4" class="ltx_tr">
<th id="S4.T3.6.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.6.1.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T3.6.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.6.1.5.4.2.1" class="ltx_text" style="font-size:80%;">OPDAI</span></th>
<td id="S4.T3.6.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.1.5.4.3.1" class="ltx_text" style="font-size:80%;">92.04</span></td>
<td id="S4.T3.6.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.1.5.4.4.1" class="ltx_text" style="font-size:80%;">3.00</span></td>
</tr>
<tr id="S4.T3.6.1.6.5" class="ltx_tr">
<th id="S4.T3.6.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.6.1.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T3.6.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.6.1.6.5.2.1" class="ltx_text" style="font-size:80%;">ID R&amp;D</span></th>
<td id="S4.T3.6.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.1.6.5.3.1" class="ltx_text" style="font-size:80%;">91.86</span></td>
<td id="S4.T3.6.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.1.6.5.4.1" class="ltx_text" style="font-size:80%;">2.99</span></td>
</tr>
<tr id="S4.T3.6.1.7.6" class="ltx_tr">
<th id="S4.T3.6.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.6.1.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T3.6.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.6.1.7.6.2.1" class="ltx_text" style="font-size:80%;">K-IBS-DS</span></th>
<td id="S4.T3.6.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.6.1.7.6.3.1" class="ltx_text" style="font-size:80%;">91.61</span></td>
<td id="S4.T3.6.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.6.1.7.6.4.1" class="ltx_text" style="font-size:80%;">1.96</span></td>
</tr>
<tr id="S4.T3.6.1.8.7" class="ltx_tr">
<th id="S4.T3.6.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.6.1.8.7.1.1" class="ltx_text" style="font-size:80%;">6</span></th>
<th id="S4.T3.6.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.6.1.8.7.2.1" class="ltx_text" style="font-size:80%;">CTAI</span></th>
<td id="S4.T3.6.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.6.1.8.7.3.1" class="ltx_text" style="font-size:80%;">90.59</span></td>
<td id="S4.T3.6.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.6.1.8.7.4.1" class="ltx_text" style="font-size:80%;">-1.94</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.T3.7" class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" style="width:233.5pt;height:186.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(26.4pt,-21.1pt) scale(1.29271948519508,1.29271948519508) ;">
<table id="S4.T3.7.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.7.1.1.1" class="ltx_tr">
<th id="S4.T3.7.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row" colspan="4"><span id="S4.T3.7.1.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Sub-Task 2.3 (Overall Improvement): Synthetic + Real Data (Constrained)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.7.1.2.1" class="ltx_tr">
<th id="S4.T3.7.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.7.1.2.1.1.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Pos.</span></th>
<th id="S4.T3.7.1.2.1.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.7.1.2.1.2.1" class="ltx_text ltx_font_bold" style="font-size:80%;">Team</span></th>
<td id="S4.T3.7.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.7.1.2.1.3.1" class="ltx_text ltx_font_bold" style="font-size:80%;">AVG [%]</span></td>
<td id="S4.T3.7.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.7.1.2.1.4.1" class="ltx_text ltx_font_bold" style="font-size:80%;">GAP [%]</span></td>
</tr>
<tr id="S4.T3.7.1.3.2" class="ltx_tr">
<th id="S4.T3.7.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.7.1.3.2.1.1" class="ltx_text" style="font-size:80%;">1</span></th>
<th id="S4.T3.7.1.3.2.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T3.7.1.3.2.2.1" class="ltx_text" style="font-size:80%;">K-IBS-DS</span></th>
<td id="S4.T3.7.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.7.1.3.2.3.1" class="ltx_text" style="font-size:80%;">95.42</span></td>
<td id="S4.T3.7.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="S4.T3.7.1.3.2.4.1" class="ltx_text" style="font-size:80%;">-2.15</span></td>
</tr>
<tr id="S4.T3.7.1.4.3" class="ltx_tr">
<th id="S4.T3.7.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.7.1.4.3.1.1" class="ltx_text" style="font-size:80%;">2</span></th>
<th id="S4.T3.7.1.4.3.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.7.1.4.3.2.1" class="ltx_text" style="font-size:80%;">OPDAI</span></th>
<td id="S4.T3.7.1.4.3.3" class="ltx_td ltx_align_center"><span id="S4.T3.7.1.4.3.3.1" class="ltx_text" style="font-size:80%;">95.23</span></td>
<td id="S4.T3.7.1.4.3.4" class="ltx_td ltx_align_center"><span id="S4.T3.7.1.4.3.4.1" class="ltx_text" style="font-size:80%;">-0.52</span></td>
</tr>
<tr id="S4.T3.7.1.5.4" class="ltx_tr">
<th id="S4.T3.7.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.7.1.5.4.1.1" class="ltx_text" style="font-size:80%;">3</span></th>
<th id="S4.T3.7.1.5.4.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.7.1.5.4.2.1" class="ltx_text" style="font-size:80%;">CTAI</span></th>
<td id="S4.T3.7.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T3.7.1.5.4.3.1" class="ltx_text" style="font-size:80%;">94.56</span></td>
<td id="S4.T3.7.1.5.4.4" class="ltx_td ltx_align_center"><span id="S4.T3.7.1.5.4.4.1" class="ltx_text" style="font-size:80%;">-6.01</span></td>
</tr>
<tr id="S4.T3.7.1.6.5" class="ltx_tr">
<th id="S4.T3.7.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.7.1.6.5.1.1" class="ltx_text" style="font-size:80%;">4</span></th>
<th id="S4.T3.7.1.6.5.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.7.1.6.5.2.1" class="ltx_text" style="font-size:80%;">CBSR-Samsung</span></th>
<td id="S4.T3.7.1.6.5.3" class="ltx_td ltx_align_center"><span id="S4.T3.7.1.6.5.3.1" class="ltx_text" style="font-size:80%;">94.20</span></td>
<td id="S4.T3.7.1.6.5.4" class="ltx_td ltx_align_center"><span id="S4.T3.7.1.6.5.4.1" class="ltx_text" style="font-size:80%;">-4.40</span></td>
</tr>
<tr id="S4.T3.7.1.7.6" class="ltx_tr">
<th id="S4.T3.7.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row"><span id="S4.T3.7.1.7.6.1.1" class="ltx_text" style="font-size:80%;">5</span></th>
<th id="S4.T3.7.1.7.6.2" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T3.7.1.7.6.2.1" class="ltx_text" style="font-size:80%;">ADMIS</span></th>
<td id="S4.T3.7.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T3.7.1.7.6.3.1" class="ltx_text" style="font-size:80%;">94.15</span></td>
<td id="S4.T3.7.1.7.6.4" class="ltx_td ltx_align_center"><span id="S4.T3.7.1.7.6.4.1" class="ltx_text" style="font-size:80%;">-1.10</span></td>
</tr>
<tr id="S4.T3.7.1.8.7" class="ltx_tr">
<th id="S4.T3.7.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.7.1.8.7.1.1" class="ltx_text" style="font-size:80%;">6</span></th>
<th id="S4.T3.7.1.8.7.2" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T3.7.1.8.7.2.1" class="ltx_text" style="font-size:80%;">ID R&amp;D</span></th>
<td id="S4.T3.7.1.8.7.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.7.1.8.7.3.1" class="ltx_text" style="font-size:80%;">94.05</span></td>
<td id="S4.T3.7.1.8.7.4" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T3.7.1.8.7.4.1" class="ltx_text" style="font-size:80%;">0.07</span></td>
</tr>
</tbody>
</table>
</span></div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table"><span id="S4.T3.11.1.1" class="ltx_text" style="font-size:113%;">Table 3</span>: </span><span id="S4.T3.12.2" class="ltx_text" style="font-size:113%;">Ranking for the six sub-tasks, according to the metrics described in Section <a href="#S3.SS3" title="3.3 Evaluation Metrics ‣ 3 FRCSyn Challenge: Setup ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>. TO = Trade-Off, AVG = Average accuracy, SD = Standard Deviation of accuracy, GAP = Gap to Real.</span></figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>FRCSyn Challenge: Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">Table <a href="#S4.T3" title="Table 3 ‣ BOVIFOCR-UFPR (Sub-Tasks 1.2 and 2.1): ‣ 4 FRCSyn Challenge: Systems Description ‣ Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> presents the rankings for the different sub-tasks considered in the 2<math id="S5.p1.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S5.p1.1.m1.1a"><msup id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><mi id="S5.p1.1.m1.1.1a" xref="S5.p1.1.m1.1.1.cmml"></mi><mtext id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><ci id="S5.p1.1.m1.1.1.1a.cmml" xref="S5.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the FRCSyn Challenge. In general, the rankings for Sub-Tasks 1.1, 1.2, and 1.3 (bias mitigation), corresponding to the descending order of TO, closely align with the ascending order of SD (<span id="S5.p1.1.1" class="ltx_text ltx_font_italic">i.e.,</span> from less to more biased FR systems). Notably, the winner of Sub-Tasks 1.1 and 1.2, ID R&amp;D (96.73% TO), exhibits a considerable negative GAP value (-5.31%), indicating higher accuracy when training the FR system with synthetic data compared to real data (<span id="S5.p1.1.2" class="ltx_text ltx_font_italic">i.e.,</span> CASIA-WebFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">48</span></a>]</cite>). Furthermore, when the limitation in the number of synthetic images is removed (<span id="S5.p1.1.3" class="ltx_text ltx_font_italic">i.e.,</span> Sub-Task 1.2), the TO value of most FR systems increases, obtaining a performance improvement and fairness simultaneously. For example, for the ADMIS team (top-2) the TO value increases to 95.72% (<span id="S5.p1.1.4" class="ltx_text ltx_font_italic">i.e.,</span> 1.42% TO general improvement from Sub-Tasks 1.1 to 1.2), with a GAP value of -0.56%. These results highlight the advantages of synthetic data, including the potential for generating an infinite number of face images to reduce bias in current FR technology. Finally, for completeness, we analyze in Sub-Task 1.3 the case of adding real and synthetic data to the FR training process. In general, we can observe better TO values, in addition to negative GAP values for all the top-6 teams, <span id="S5.p1.1.5" class="ltx_text ltx_font_italic">e.g.,</span> ADMIS (96.50% TO, -1.33 GAP), K-IBS-DS (96.17% TO, -1.37% GAP), and UNICA-IGD-LSI (96.00% TO, -5.33% GAP). These results prove that the combination of synthetic and real data achieves higher FR performance compared to training only with real data. In addition, it is also interesting to compare the best results achieved in Sub-Task 1.2, <span id="S5.p1.1.6" class="ltx_text ltx_font_italic">i.e.,</span> unconstrained synthetic data, and Sub-Task 1.3, <span id="S5.p1.1.7" class="ltx_text ltx_font_italic">i.e.,</span> constrained synthetic + real data. The ID R&amp;D team achieves 96.73% TO in Sub-Task 1.2 whereas ADMIS achieves 96.50% TO in Sub-Task 1.3, proving that it is possible to obtain better results using only unlimited synthetic data than including real data.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">For Task 2, the average accuracy across databases in the different sub-tasks is lower than the accuracy achieved for BUPT-BalancedFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">46</span></a>]</cite> in Task 1, emphasizing the additional challenges introduced by AgeDB <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">33</span></a>]</cite>, CFP-FP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">37</span></a>]</cite>, and ROF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">15</span></a>]</cite> real databases considered for evaluation. Also, although good results are achieved in Sub-Task 2.1 when training only with synthetic data (<span id="S5.p2.1.1" class="ltx_text ltx_font_italic">e.g.,</span> 91.93% AVG for OPDAI), the positive GAP values provided by most of the top-6 teams are the greatest from all the sub-tasks, indicating that synthetic data alone currently struggles to completely replace real data for training FR systems. Nevertheless, in Sub-Task 2.2 in which there are no restrictions in the number of synthetic images to use, the Idiap-SynthDistill team (top-1) achieves much better results (93.50% AVG) with a GAP value of -0.05, proving that unlimited synthetic data by itself can even outperform limited real data. Finally, in Sub-Task 2.3, most of the teams report better AVG and higher negative GAP values (<span id="S5.p2.1.2" class="ltx_text ltx_font_italic">e.g.,</span> 95.42% AVG and -2.15% GAP for the K-IBS-DS team, top-1), which suggests that synthetic data combined with real data can mitigate existing limitations within FR technology.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">Finally, analyzing the contributions of all eleven top teams, a notable trend emerges, showing the prevalence of well-established methodologies. ResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">18</span></a>]</cite> or IResNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">14</span></a>]</cite> backbones were chosen by all the teams for their wide adoption in state-of-the-art FR approaches. The AdaFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">23</span></a>]</cite> and ArcFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">12</span></a>]</cite> loss functions were widely used, featuring in the approaches of most of the teams, except for ID R&amp;D which used the recent UniFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">52</span></a>]</cite> or UNICA which used CosFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">44</span></a>]</cite>. Notably, all the teams used DCFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">24</span></a>]</cite> alone or combined with other databases like GANDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">27</span></a>]</cite>, DigiFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">3</span></a>]</cite>, or IDiffFace <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">6</span></a>]</cite>, considering also interesting approaches based on synthetic data cleaning and selection for some teams such as CTAI and CBSR-Samsung. ID R&amp;D and Idiap-SynthDistill were the only teams that used different approaches to generate the synthetic data. In particular, an Hourglass Diffusion Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">8</span></a>]</cite> and StyleNAT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">43</span></a>]</cite> by the ID R&amp;D team, and dynamic image generation using StyleGAN2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">38</span></a>]</cite> by the Idiap-SynthDistill team.</p>
</div>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.4" class="ltx_p">The 2<math id="S6.p1.1.m1.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S6.p1.1.m1.1a"><msup id="S6.p1.1.m1.1.1" xref="S6.p1.1.m1.1.1.cmml"><mi id="S6.p1.1.m1.1.1a" xref="S6.p1.1.m1.1.1.cmml"></mi><mtext id="S6.p1.1.m1.1.1.1" xref="S6.p1.1.m1.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.p1.1.m1.1b"><apply id="S6.p1.1.m1.1.1.cmml" xref="S6.p1.1.m1.1.1"><ci id="S6.p1.1.m1.1.1.1a.cmml" xref="S6.p1.1.m1.1.1.1"><mtext mathsize="70%" id="S6.p1.1.m1.1.1.1.cmml" xref="S6.p1.1.m1.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.1.m1.1c">{}^{\text{nd}}</annotation></semantics></math> edition of the FRCSyn Challenge has presented a comprehensive exploration of the applications of synthetic data in FR, effectively addressing existing limitations in the field. In this 2<math id="S6.p1.2.m2.1" class="ltx_Math" alttext="{}^{\text{nd}}" display="inline"><semantics id="S6.p1.2.m2.1a"><msup id="S6.p1.2.m2.1.1" xref="S6.p1.2.m2.1.1.cmml"><mi id="S6.p1.2.m2.1.1a" xref="S6.p1.2.m2.1.1.cmml"></mi><mtext id="S6.p1.2.m2.1.1.1" xref="S6.p1.2.m2.1.1.1a.cmml">nd</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.p1.2.m2.1b"><apply id="S6.p1.2.m2.1.1.cmml" xref="S6.p1.2.m2.1.1"><ci id="S6.p1.2.m2.1.1.1a.cmml" xref="S6.p1.2.m2.1.1.1"><mtext mathsize="70%" id="S6.p1.2.m2.1.1.1.cmml" xref="S6.p1.2.m2.1.1.1">nd</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.2.m2.1c">{}^{\text{nd}}</annotation></semantics></math> edition, two additional sub-tasks have been introduced, showing that impressive results can be achieved using unlimited synthetic data, even outperforming in some cases the scenario of training with real data. With an increased number of participants in this edition, we have witnessed a considerable performance improvement in all sub-tasks in comparison to the 1<math id="S6.p1.3.m3.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S6.p1.3.m3.1a"><msup id="S6.p1.3.m3.1.1" xref="S6.p1.3.m3.1.1.cmml"><mi id="S6.p1.3.m3.1.1a" xref="S6.p1.3.m3.1.1.cmml"></mi><mtext id="S6.p1.3.m3.1.1.1" xref="S6.p1.3.m3.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.p1.3.m3.1b"><apply id="S6.p1.3.m3.1.1.cmml" xref="S6.p1.3.m3.1.1"><ci id="S6.p1.3.m3.1.1.1a.cmml" xref="S6.p1.3.m3.1.1.1"><mtext mathsize="70%" id="S6.p1.3.m3.1.1.1.cmml" xref="S6.p1.3.m3.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.3.m3.1c">{}^{\text{st}}</annotation></semantics></math> edition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">30</span></a>]</cite>. This has been possible thanks to the proposal of novel methods to generate and select better synthetic data, as well as FR models and loss functions. These approaches can be compared across a variety of sub-tasks, with many being reproducible thanks to the materials made available by the participating teams. Future works will be oriented to a more detailed analysis of the results and comparison with recent challenges in the topic, such as SDFR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">39</span></a>]</cite>. We also plan to transform the CodaLab platform into an ongoing challenge, similar to what we did in the 1<math id="S6.p1.4.m4.1" class="ltx_Math" alttext="{}^{\text{st}}" display="inline"><semantics id="S6.p1.4.m4.1a"><msup id="S6.p1.4.m4.1.1" xref="S6.p1.4.m4.1.1.cmml"><mi id="S6.p1.4.m4.1.1a" xref="S6.p1.4.m4.1.1.cmml"></mi><mtext id="S6.p1.4.m4.1.1.1" xref="S6.p1.4.m4.1.1.1a.cmml">st</mtext></msup><annotation-xml encoding="MathML-Content" id="S6.p1.4.m4.1b"><apply id="S6.p1.4.m4.1.1.cmml" xref="S6.p1.4.m4.1.1"><ci id="S6.p1.4.m4.1.1.1a.cmml" xref="S6.p1.4.m4.1.1.1"><mtext mathsize="70%" id="S6.p1.4.m4.1.1.1.cmml" xref="S6.p1.4.m4.1.1.1">st</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.p1.4.m4.1c">{}^{\text{st}}</annotation></semantics></math> edition <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">31</span></a>]</cite>.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p"><span id="Sx1.p1.1.1" class="ltx_text" style="font-size:80%;">This study has received funding from the European Union’s Horizon 2020 TReSPAsS-ETN (No 860813) and is supported by INTER-ACTION (PID2021-126521OB-I00 MICINN/FEDER), Cátedra ENIA UAM-VERIDAS en IA Responsable (NextGenerationEU PRTR TSI-100927-2023-2) and R&amp;D Agreement DGGC/ UAM/FUAM for Biometrics and Cybersecurity.
It is also supported by the German Federal Ministry of Education and Research and the Hessian Ministry of Higher Education, Research, Science and the Arts within their joint support of the National Research Center for Applied Cybersecurity ATHENE.
K-IBS-DS was supported by the Institute for Basic Science, Republic of Korea (IBS-R029-C2). UNICA-IGD-LSI was supported by the ARIS program P2-0250B.</span>

<span id="Sx1.p1.1.2" class="ltx_text" style="font-size:90%;"></span></p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">An et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, and Ying Fu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Partial FC: Training 10 Million Identities on a Single Machine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF International Conference on Computer Vision Workshops</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Atzori et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, and Mirko Marras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">If It’s Not Enough, Make It So: Reducing Authentic Data Demand in Face Recognition through Synthetic Faces.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib2.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. 18th International Conference on Automatic Face and Gesture Recognition</em><span id="bib.bib2.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Bae et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Gwangbin Bae, Martin de La Gorce, Tadas Baltrušaitis, Charlie Hewitt, Dong Chen, Julien Valentin, Roberto Cipolla, and Jingjing Shen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">DigiFace-1M: 1 Million Digital Face Images for Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib3.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Winter Conference on Applications of Computer Vision</em><span id="bib.bib3.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Bisogni et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Carmen Bisogni, Aniello Castiglione, Sanoar Hossain, Fabio Narducci, and Saiyed Umer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Impact of Deep Learning Approaches on Facial Expression Recognition in Healthcare Industries.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib4.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Industrial Informatics</em><span id="bib.bib4.10.2" class="ltx_text" style="font-size:90%;">, 18(8):5619–5627, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Boutros et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Fadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">ElasticFace: Elastic Margin Loss for Deep Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.5.5.1" class="ltx_text" style="font-size:90%;">Boutros et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.7.1" class="ltx_text" style="font-size:90%;">
Fadi Boutros, Jonas Henry Grebe, Arjan Kuijper, and Naser Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.8.1" class="ltx_text" style="font-size:90%;">IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib6.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib6.11.3" class="ltx_text" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Boutros et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Fadi Boutros, Marcel Klemt, Meiling Fang, Arjan Kuijper, and Naser Damer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">Unsupervised Face Recognition using Unlabeled Synthetic Data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib7.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. 17th International Conference on Automatic Face and Gesture Recognition</em><span id="bib.bib7.11.3" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.5.5.1" class="ltx_text" style="font-size:90%;">Crowson et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">
Katherine Crowson, Stefan Andreas Baumann, Alex Birch, Tanishq Mathew Abraham, Daniel Z Kaplan, and Enrico Shippole.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass Diffusion Transformers.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib8.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2401.11605</em><span id="bib.bib8.10.2" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Crum et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Colton R. Crum, Patrick Tinsley, Aidan Boyd, Jacob Piland, Christopher Sweet, Timothy Kelley, Kevin Bowyer, and Adam Czajka.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">Explain To Me: Salience-Based Explainability for Synthetic Face Detection Models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib9.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Artificial Intelligence</em><span id="bib.bib9.10.2" class="ltx_text" style="font-size:90%;">, pages 1–12, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Daza et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Roberto Daza, Luis F Gomez, Aythami Morales, Julian Fierrez, Ruben Tolosana, Ruth Cobos, and Javier Ortega-Garcia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">MATT: Multimodal Attention Level Estimation for e-learning Platforms.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib10.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. AAAI Workshop on Artificial Intelligence for Education</em><span id="bib.bib10.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Deandres-Tame et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Ivan Deandres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Julian Fierrez, and Javier Ortega-Garcia.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">How Good Is ChatGPT at Face Biometrics? A First Look Into Recognition, Soft Biometrics, and Explainability.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib11.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Access</em><span id="bib.bib11.10.2" class="ltx_text" style="font-size:90%;">, 12:34390–34401, 2024.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Deng et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">ArcFace: Additive Angular Margin Loss for Deep Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib12.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib12.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Du et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Hang Du, Hailin Shi, Dan Zeng, Xiao-Ping Zhang, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib13.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Comput. Surv.</em><span id="bib.bib13.10.2" class="ltx_text" style="font-size:90%;">, 54(10s), 2022.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.5.5.1" class="ltx_text" style="font-size:90%;">Duta et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">
Ionut Cosmin Duta, Li Liu, Fan Zhu, and Ling Shao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">Improved Residual Networks for Image and Video Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib14.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. 25th International Conference on Pattern Recognition</em><span id="bib.bib14.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.12.10.1" class="ltx_text" style="font-size:90%;">Erak</span><math id="bib.bib15.3.3.m1.1" class="ltx_Math" alttext="\iota" display="inline"><semantics id="bib.bib15.3.3.m1.1a"><mi mathsize="90%" id="bib.bib15.3.3.m1.1.1" xref="bib.bib15.3.3.m1.1.1.cmml">ι</mi><annotation-xml encoding="MathML-Content" id="bib.bib15.3.3.m1.1b"><ci id="bib.bib15.3.3.m1.1.1.cmml" xref="bib.bib15.3.3.m1.1.1">𝜄</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.3.3.m1.1c">\iota</annotation></semantics></math><span id="bib.bib15.13.11.2" class="ltx_text" style="font-size:90%;">n et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.15.1" class="ltx_text" style="font-size:90%;">
Mustafa Ekrem Erak</span><math id="bib.bib15.4.m1.1" class="ltx_Math" alttext="\iota" display="inline"><semantics id="bib.bib15.4.m1.1a"><mi mathsize="90%" id="bib.bib15.4.m1.1.1" xref="bib.bib15.4.m1.1.1.cmml">ι</mi><annotation-xml encoding="MathML-Content" id="bib.bib15.4.m1.1b"><ci id="bib.bib15.4.m1.1.1.cmml" xref="bib.bib15.4.m1.1.1">𝜄</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.4.m1.1c">\iota</annotation></semantics></math><span id="bib.bib15.16.2" class="ltx_text" style="font-size:90%;">n, Uğur Demir, and Haz</span><math id="bib.bib15.5.m2.1" class="ltx_Math" alttext="\iota" display="inline"><semantics id="bib.bib15.5.m2.1a"><mi mathsize="90%" id="bib.bib15.5.m2.1.1" xref="bib.bib15.5.m2.1.1.cmml">ι</mi><annotation-xml encoding="MathML-Content" id="bib.bib15.5.m2.1b"><ci id="bib.bib15.5.m2.1.1.cmml" xref="bib.bib15.5.m2.1.1">𝜄</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib15.5.m2.1c">\iota</annotation></semantics></math><span id="bib.bib15.17.3" class="ltx_text" style="font-size:90%;">m Kemal Ekenel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.18.1" class="ltx_text" style="font-size:90%;">On Recognizing Occluded Faces in the Wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.19.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib15.20.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. International Conference of the Biometrics Special Interest Group</em><span id="bib.bib15.21.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Garaev et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Nikita Garaev, Evgeny Smirnov, Vasiliy Galyuk, and Evgeny Lukyanets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">FaceMix: Transferring Local Regions for Data Augmentation in Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. Neural Information Processing</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Gomez et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Luis F. Gomez, Aythami Morales, Juan R. Orozco-Arroyave, Roberto Daza, and Julian Fierrez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Improving Parkinson Detection Using Dynamic Features From Evoked Expressions in Video.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">He et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">Deep Residual Learning for Image Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib18.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib18.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Hu et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Jie Hu, Li Shen, and Gang Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Squeeze-and-Excitation Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.5.5.1" class="ltx_text" style="font-size:90%;">Kansy et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">
Manuel Kansy, Anton Raël, Graziana Mignone, Jacek Naruniec, Christopher Schroers, Markus Gross, and Romann M. Weber.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">Controllable Inversion of Black-Box Face Recognition Models via Diffusion.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF International Conference on Computer Vision Workshops</em><span id="bib.bib20.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Karras et al. [2019]</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Samuli Laine, and Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">A Style-Based Generator Architecture for Generative Adversarial Networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF conference on Computer Vision and Pattern Recognition</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Karras et al. [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Analyzing and Improving the Image Quality of StyleGAN.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib22.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib22.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Minchul Kim, Anil K. Jain, and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">AdaFace: Quality Adaptive Margin for Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib23.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib23.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Kim et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Minchul Kim, Feng Liu, Anil Jain, and Xiaoming Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">DCFace: Synthetic Face Generation with Dual Condition Diffusion Model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib24.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib24.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Li et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Low et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Cheng Yaw Low, Jacky Chen Long Chai, Jaewoo Park, Kyeongjin Ann, and Meeyoung Cha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">SlackedFace: Learning a Slacked Margin for Low-Resolution Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib26.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. 34th British Machine Vision Conference</em><span id="bib.bib26.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.5.5.1" class="ltx_text" style="font-size:90%;">Melzi et al. [2023a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">
Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Dominik Lawatsch, Florian Domin, and Maxim Schaubert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">GANDiffFace: Controllable Generation of Synthetic Datasets for Face Recognition with Realistic Variations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib27.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF International Conference on Computer Vision Workshops</em><span id="bib.bib27.11.3" class="ltx_text" style="font-size:90%;">, 2023a.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Melzi et al. [2023b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Pietro Melzi, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami Morales, Dominik Lawatsch, Florian Domin, and Maxim Schaubert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Synthetic Data for the Mitigation of Demographic Biases in Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE International Joint Conference on Biometrics</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, 2023b.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Melzi et al. [2023c]</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Pietro Melzi, Hatef Otroshi Shahreza, Christian Rathgeb, Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Sébastien Marcel, and Christoph Busch.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">Multi-IVE: Privacy Enhancement of Multiple Soft-Biometrics in Face Embeddings.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Winter Conference on Applications of Computer Vision Workshops</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, 2023c.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.5.5.1" class="ltx_text" style="font-size:90%;">Melzi et al. [2024a]</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">
Pietro Melzi, Ruben Tolosana, Ruben Vera-Rodriguez, Minchul Kim, Christian Rathgeb, Xiaoming Liu, Ivan DeAndres-Tame, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Weisong Zhao, Xiangyu Zhu, Zheyu Yan, Xiao-Yu Zhang, Jinlin Wu, Zhen Lei, Suvidha Tripathi, Mahak Kothari, Md Haider Zama, Debayan Deb, Bernardo Biesseck, Pedro Vidal, Roger Granada, Guilherme Fickel, Gustavo Führ, David Menotti, Alexander Unnervik, Anjith George, Christophe Ecabert, Hatef Otroshi Shahreza, Parsa Rahimi, Sébastien Marcel, Ioannis Sarridis, Christos Koutlis, Georgia Baltsou, Symeon Papadopoulos, Christos Diou, Nicolò Di Domenico, Guido Borghi, Lorenzo Pellegrini, Enrique Mas-Candela, Ángela Sánchez-Pérez, Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, and Mirko Marras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib30.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. of the IEEE/CVF Winter Conference on Applications of Computer Vision Workshops</em><span id="bib.bib30.11.3" class="ltx_text" style="font-size:90%;">, 2024a.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Melzi et al. [2024b]</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Pietro Melzi, Ruben Tolosana, Ruben Vera-Rodriguez, Minchul Kim, Christian Rathgeb, Xiaoming Liu, Ivan DeAndres-Tame, Aythami Morales, Julian Fierrez, Javier Ortega-Garcia, Weisong Zhao, Xiangyu Zhu, Zheyu Yan, Xiao-Yu Zhang, Jinlin Wu, Zhen Lei, Suvidha Tripathi, Mahak Kothari, Md Haider Zama, Debayan Deb, Bernardo Biesseck, Pedro Vidal, Roger Granada, Guilherme Fickel, Gustavo Führ, David Menotti, Alexander Unnervik, Anjith George, Christophe Ecabert, Hatef Otroshi Shahreza, Parsa Rahimi, Sébastien Marcel, Ioannis Sarridis, Christos Koutlis, Georgia Baltsou, Symeon Papadopoulos, Christos Diou, Nicolò Di Domenico, Guido Borghi, Lorenzo Pellegrini, Enrique Mas-Candela, Ángela Sánchez-Pérez, Andrea Atzori, Fadi Boutros, Naser Damer, Gianni Fenu, and Mirko Marras.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.8.1" class="ltx_text" style="font-size:90%;">FRCSyn-onGoing: Benchmarking and Comprehensive Evaluation of Real and Synthetic Data to Improve Face Recognition Systems.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Information Fusion</em><span id="bib.bib31.10.2" class="ltx_text" style="font-size:90%;">, 107:102322, 2024b.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Morales et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Aythami Morales, Julian Fierrez, Ruben Vera-Rodriguez, and Ruben Tolosana.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">SensitiveNets: Learning Agnostic Representations with Application to Face Images.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Pattern Analysis and Machine Intelligence</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 43(6):2158–2164, 2021.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">Moschoglou et al. [2017]</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">AgeDB: The First Manually Collected, In-The-Wild Age Database.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib33.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</em><span id="bib.bib33.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Neto et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Pedro C. Neto, Fadi Boutros, Joao Ribeiro Pinto, Naser Damer, Ana F. Sequeira, Jaime S. Cardoso, Messaoud Bengherabi, Abderaouf Bousnat, Sana Boucheta, Nesrine Hebbadj, Mustafa Ekrem Erakın, Uğur Demir, Hazım Kemal Ekenel, Pedro Beber De Queiroz Vidal, and David Menotti.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">OCFR 2022: Competition on Occluded Face Recognition from Synthetically Generated Structure-Aware Occlusions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib34.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE International Joint Conference on Biometrics</em><span id="bib.bib34.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">Neto et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
Pedro C. Neto, Eduarda Caldeira, Jaime S. Cardoso, and Ana F. Sequeira.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Compressed Models Decompress Race Biases: What Quantized Models Forget for Fair Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib35.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. International Conference of the Biometrics Special Interest Group</em><span id="bib.bib35.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">Qiu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
Haibo Qiu, Baosheng Yu, Dihong Gong, Zhifeng Li, Wei Liu, and Dacheng Tao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">SynFace: Face Recognition with Synthetic Data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib36.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib36.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.5.5.1" class="ltx_text" style="font-size:90%;">Sengupta et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">
Soumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, Vishal M Patel, Rama Chellappa, and David W Jacobs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">Frontal to Profile Face Verification in the Wild.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib37.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Winter Conference on Applications of Computer Vision</em><span id="bib.bib37.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.5.5.1" class="ltx_text" style="font-size:90%;">Shahreza et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">
Hatef Otroshi Shahreza, Anjith George, and Sébastien Marcel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib38.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE International Joint Conference on Biometrics</em><span id="bib.bib38.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.5.5.1" class="ltx_text" style="font-size:90%;">Shahreza et al. [2024]</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">
Hatef Otroshi Shahreza, Christophe Ecabert, Anjith George, Alexander Unnervik, Sébastien Marcel, Nicolò Di Domenico, Guido Borghi, Davide Maltoni, Fadi Boutros, Julia Vogel, Naser Damer, Ángela Sánchez-Pérez, EnriqueMas-Candela, Jorge Calvo-Zaragoza, Bernardo Biesseck, Pedro Vidal, Roger Granada, David Menotti, Ivan DeAndres-Tame, Simone Maurizio La Cava, Sara Concas, Pietro Melzi, Ruben Tolosana, Ruben Vera-Rodriguez, Gianpaolo Perelli, Giulia Orrù, Gian Luca Marcialis, and Julian Fierrez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">SDFR: Synthetic Data for Face Recognition Competition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib39.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. 18th IEEE International Conference on Automatic Face and Gesture Recognition</em><span id="bib.bib39.11.3" class="ltx_text" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Smirnov et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Evgeny Smirnov, Nikita Garaev, Vasiliy Galyuk, and Evgeny Lukyanets.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Prototype Memory for Large-Scale Face Representation Learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib40.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Access</em><span id="bib.bib40.10.2" class="ltx_text" style="font-size:90%;">, 10:12031–12046, 2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Song et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Jiaming Song, Chenlin Meng, and Stefano Ermon.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Denoising Diffusion Implicit Models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib41.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. International Conference on Learning Representations</em><span id="bib.bib41.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Terhörst et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Philipp Terhörst, Jan Niklas Kolf, Marco Huber, Florian Kirchbuchner, Naser Damer, Aythami Morales Moreno, Julian Fierrez, and Arjan Kuijper.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">A Comprehensive Study on Face Recognition Biases Beyond Demographics.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Technology and Society</em><span id="bib.bib42.10.2" class="ltx_text" style="font-size:90%;">, 3(1):16–30, 2021.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Walton et al. [2022]</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Steven Walton, Ali Hassani, Xingqian Xu, Zhangyang Wang, and Humphrey Shi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">StyleNAT: Giving Each Head a New Perspective.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2211.05770</em><span id="bib.bib43.10.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2018]</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">CosFace: Large Margin Cosine Loss for Deep Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib44.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib44.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Wang et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Jun Wang, Yinglu Liu, Yibo Hu, Hailin Shi, and Tao Mei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">FaceX-Zoo: A PyTorch Toolbox for Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. 29th ACM International Conference on Multimedia</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.4.4.1" class="ltx_text" style="font-size:90%;">Wang and Deng [2020]</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.6.1" class="ltx_text" style="font-size:90%;">
Mei Wang and Weihong Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">Mitigating Bias in Face Recognition Using Skewness-Aware Reinforcement Learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib46.10.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.4.4.1" class="ltx_text" style="font-size:90%;">Wang and Deng [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.6.1" class="ltx_text" style="font-size:90%;">
Mei Wang and Weihong Deng.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">Deep Face Recognition: A Survey.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib47.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Neurocomputing</em><span id="bib.bib47.9.2" class="ltx_text" style="font-size:90%;">, 429:215–244, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Yi et al. [2014]</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Learning Face Representation from Scratch.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib48.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1411.7923</em><span id="bib.bib48.10.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Cheng Zhang, Xuanbai Chen, Siqi Chai, Chen Henry Wu, Dmitry Lagun, Thabo Beeler, and Fernando De la Torre.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">ITI-GEN: Inclusive Text-to-Image Generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib49.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib49.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. [2016]</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib50.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Signal Processing Letters</em><span id="bib.bib50.10.2" class="ltx_text" style="font-size:90%;">, 23(10):1499–1503, 2016.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Zhong et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Yaoyao Zhong, Weihong Deng, Jiani Hu, Dongyue Zhao, Xian Li, and Dongchao Wen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">SFace: Sigmoid-Constrained Hypersphere Loss for Robust Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE Transactions on Image Processing</em><span id="bib.bib51.10.2" class="ltx_text" style="font-size:90%;">, 30:2587–2598, 2021.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Zhou et al. [2023]</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Jiancan Zhou, Xi Jia, Qiufu Li, Linlin Shen, and Jinming Duan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">UniFace: Unified Cross-Entropy Loss for Deep Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib52.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF International Conference on Computer Vision</em><span id="bib.bib52.11.3" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.5.5.1" class="ltx_text" style="font-size:90%;">Zhu et al. [2021]</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">
Zheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, and Jie Zhou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">WebFace260M: A Benchmark Unveiling the Power of Million-Scale Deep Face Recognition.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib53.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</em><span id="bib.bib53.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2404.10377" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2404.10378" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2404.10378">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2404.10378" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2404.10379" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Sun May  5 15:42:43 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
