<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2206.13240] A Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data</title><meta property="og:description" content="Automatic Speech Recognition(ASR) has been dominated by deep learning-based end-to-end speech recognition models. These approaches require large amounts of labeled data in the form of audio-text pairs. Moreover, these …">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2206.13240">

<!--Generated on Mon Mar 11 17:57:21 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Raviraj Joshi 
<br class="ltx_break">Flipkart, Bengaluru 
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter">raviraj.j@flipkart.com</span> 
<br class="ltx_break"><span id="id2.2.id2" class="ltx_ERROR undefined">\And</span>Anupam Singh 
<br class="ltx_break">Flipkart, Bengaluru 
<br class="ltx_break"><span id="id3.3.id3" class="ltx_text ltx_font_typewriter">anupam.s@flipkart.com</span> 
<br class="ltx_break">
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Automatic Speech Recognition(ASR) has been dominated by deep learning-based end-to-end speech recognition models. These approaches require large amounts of labeled data in the form of audio-text pairs. Moreover, these models are more susceptible to domain shift as compared to traditional models. It is common practice to train generic ASR models and then adapt them to target domains using comparatively smaller data sets. We consider a more extreme case of domain adaptation where text-only corpus is available. In this work, we propose a simple baseline technique for domain adaptation in end-to-end speech recognition models. We convert the text-only corpus to audio data using single speaker Text to Speech (TTS) engine. The parallel data in the target domain is then used to fine-tune the final dense layer of generic ASR models. We show that single speaker synthetic TTS data coupled with final dense layer only fine-tuning provides reasonable improvements in word error rates. We use text data from address and e-commerce search domains to show the effectiveness of our low-cost baseline approach on CTC and attention-based models.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">End-to-end speech recognition models simplify the speech recognition process by folding multiple components into a single model. These models directly convert the speech utterance into the spoken text <cite class="ltx_cite ltx_citemacro_cite">He et al. (<a href="#bib.bib8" title="" class="ltx_ref">2019</a>)</cite>. The major end-to-end architectures include CTC-based, attention-based, and transducer-based approaches <cite class="ltx_cite ltx_citemacro_cite">Graves et al. (<a href="#bib.bib7" title="" class="ltx_ref">2013</a>); Graves (<a href="#bib.bib5" title="" class="ltx_ref">2012</a>); Chan et al. (<a href="#bib.bib2" title="" class="ltx_ref">2015</a>)</cite>. These all neural approaches are competitive in terms of performance however they require a large amount of supervised data to achieve generalization. A model trained on a single application domain doesn’t work well on other target domains. Examples of such applications domains include e-commerce, voice search, medical, etc. Since it is not feasible to prepare supervised data for all the application domains, it is common to train models on large out of domain corpus followed by a small amount of in-domain finetuning <cite class="ltx_cite ltx_citemacro_cite">Bell et al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>. However, this approach still requires the availability of small labeled data. In the most basic form, the unlabelled text data from the target domain can be used to build domain-specific language models (LMs). The domain LMs are combined with the end-to-end ASR model using shallow fusion <cite class="ltx_cite ltx_citemacro_cite">Kannan et al. (<a href="#bib.bib11" title="" class="ltx_ref">2018</a>); Shan et al. (<a href="#bib.bib23" title="" class="ltx_ref">2019</a>); Meng et al. (<a href="#bib.bib16" title="" class="ltx_ref">2021</a>)</cite>. This approach has limited benefits since the main ASR model is not tuned to the target domain. Another popular technique is to prepare synthetic data using a Text to Speech (TTS) system and the target domain text data <cite class="ltx_cite ltx_citemacro_cite">Sim et al. (<a href="#bib.bib25" title="" class="ltx_ref">2019</a>)</cite>. This requires a sophisticated multi-speaker TTS system followed by the addition of representative noise to make the data usable. The idea is to make synthetic data as close as the real-world data. However, this approach is prone to overfitting as the synthetic data does not exactly resemble real-world noisy conditions. Different fine-tuning approaches have been explored using synthetic data to alleviate the over-fitting problem.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In this work, we are concerned with domain adaptation techniques when a text-only corpus from the target domain is available <cite class="ltx_cite ltx_citemacro_cite">Gao et al. (<a href="#bib.bib4" title="" class="ltx_ref">2021</a>)</cite>. We present a simple baseline approach using single speaker synthetic TTS data followed by final dense layer only fine-tuning. The synthetic data is created using a single speaker TTS system which is commonly available and also easier to build in-house. The data is not subjected to any noise and is directly used to fine-tune the neural network. Although such single speaker data is easy to build it is not usable for the training of end-to-end networks. We, therefore, propose dense-only fine-tuning for effective fine-tuning. The approach solely relies on final dense layer fine-tuning to avoid over-fitting on single speaker and acoustic conditions. We refer to the dense layer projecting the intermediate embedding onto vocabulary space as the final dense layer. Since the acoustic encoder of the neural network is frozen, the network only learns about the linguistic characteristic of the target domain. Similar approaches have been explored in literature where only the decoder part of the neural network is fine-tuned. However, this approach is not applicable to CTC-based neural networks <cite class="ltx_cite ltx_citemacro_cite">Graves et al. (<a href="#bib.bib6" title="" class="ltx_ref">2006</a>)</cite> which do not follow an encoder-decoder architecture. We present our approach in the context of CTC and Listen-Attend-Spell (LAS) based neural network architectures. For LAS-based network, we also compare dense only and decoder only fine-tuning. We consider the text from address (for delivery of e-commerce products) domain and voice search (of e-commerce products) domain <cite class="ltx_cite ltx_citemacro_cite">Joshi and Kannan (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite> for fine-tuning the model trained on a generic multi-domain dataset. Although encoder only fine-tuning has been widely studied in the literature <cite class="ltx_cite ltx_citemacro_cite">Mimura et al. (<a href="#bib.bib17" title="" class="ltx_ref">2018</a>)</cite>, this is the first work to exploit dense-only fine-tuning which is more relevant to the CTC-based systems. Moreover, we demonstrate a way to build an ASR system for the Address domain which is not explored in the literature.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our work is at the intersection of data augmentation using the TTS system and domain adaptation. In this section, we review the recent work in these two areas. The synthetic data generated using the TTS system was used to improve the recognition of out of vocabulary (OOV) words in <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a href="#bib.bib27" title="" class="ltx_ref">2021</a>)</cite>. Both synthetic data containing OOV words and original data were used together to train the best RNN-T model. Encoder freezing and elastic weight consolidation were further shown to provide extra benefits. Similarly, <cite class="ltx_cite ltx_citemacro_cite">Peyser et al. (<a href="#bib.bib19" title="" class="ltx_ref">2019</a>)</cite> used a TTS system to generate numeric training data and improve the ASR performance on the out of vocabulary numeric sequences. The importance of data augmentation over semi-supervised learning was shown in <cite class="ltx_cite ltx_citemacro_cite">Laptev et al. (<a href="#bib.bib13" title="" class="ltx_ref">2020</a>)</cite>. In this work, the TTS system was trained on the same supervised ASR data set and used to generate synthesized samples on a wider set. The work also highlights the importance of multi-speaker TTS systems and noise addition to build usable systems. Other data augmentation techniques like spec augment <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite> were shown to be complementary with TTS based augmentation in <cite class="ltx_cite ltx_citemacro_cite">Rossenbach et al. (<a href="#bib.bib22" title="" class="ltx_ref">2020</a>)</cite>. Effective training strategies for using synthetic data were proposed in <cite class="ltx_cite ltx_citemacro_cite">Fazel et al. (<a href="#bib.bib3" title="" class="ltx_ref">2021</a>)</cite>. In order to avoid catastrophic forgetting, multi-stage training was used. The encoder layers were frozen in the initial stage followed by full fine-tuning in later stages. An elastic penalty was also added to the loss function so to avoid large deviation in learned parameters.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Similar approaches have been proposed for domain adaptation as well with a bais towards fine-tuning based transfer learning approaches. An LSTM-based domain classifier was trained to select an appropriate domain adapted language model in <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a href="#bib.bib15" title="" class="ltx_ref">2021</a>)</cite>. The corresponding domain-specific language model was used for second pass re-scoring. The transfer learning approaches for domain adaptation and cross-language adaptation were evaluated in <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a href="#bib.bib9" title="" class="ltx_ref">2020</a>)</cite>. They compare the fine-tuning of the pre-trained QuartzNet model with the corresponding model trained from scratch. They concluded that large pre-trained models performed better than small pre-trained models and the models trained from scratch. Another form of transfer learning involves partial fine-tuning of the model instead of the entire model. The decoder only fine-tuning for domain adaptation in Listen-Attend-Spell (LAS) based model was evaluated in <cite class="ltx_cite ltx_citemacro_cite">Ueno et al. (<a href="#bib.bib26" title="" class="ltx_ref">2018</a>)</cite>. The model is first trained on the source domain followed by decoder only fine-tuning on the target domain. The partial fine-tuning is shown to work better than the full fine-tuning and from the models trained from scratch. An adaptation technique specific to RNN-T networks using text-only data was proposed in <cite class="ltx_cite ltx_citemacro_cite">Pylkkönen et al. (<a href="#bib.bib21" title="" class="ltx_ref">2021</a>)</cite>. The prediction network of RNN-T is viewed as a neural language model and is adapted using text-only corpus while keeping the encoder and joint network fixed. Another approach for adapting RNN-T network using text-only data was proposed in <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a href="#bib.bib14" title="" class="ltx_ref">2020</a>)</cite>. The fine-tuning of prediction and the joint network was performed using synthetic TTS domain-specific data. Partial fine-tuning was shown to work better than full fine-tuning approaches.
These works mainly used RNNT-based systems and employ a multi-context multi-speaker TTS system. In this work, we use a single speaker TTS system with a focus on CTC and attention-based models. Moreover, we focus on dense only fine-tuning instead of decoder fine-tuning studied in these works.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The flow of our process is depicted in Figure <a href="#S3.F1" title="Figure 1 ‣ 3.1 Model Architecture ‣ 3 Methodology ‣ A Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. We follow a simple pre-training and fine-tuning approach. The model is first trained on general out-of-domain data. The target domain text data is converted into audio using a single speaker TTS engine. The synthetic samples are then used to fine-tune the final dense layers of ASR models. We consider two model types i.e CTC based models and Attention-based models. The model architecture and TTS system description are provided in the following sub-sections.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Model Architecture</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We consider CTC and attention-based ASR models which follow the same pre-processing steps <cite class="ltx_cite ltx_citemacro_cite">Joshi and Kannan (<a href="#bib.bib10" title="" class="ltx_ref">2021</a>)</cite>. The audio is segmented into 20ms chunks with an overlap of 10ms. Log-mel features are computed and provided as input to the model. Standard spec-augment is used for time and frequency masking of the spectrograms <cite class="ltx_cite ltx_citemacro_cite">Park et al. (<a href="#bib.bib18" title="" class="ltx_ref">2019</a>)</cite>. An 80-dimensional log mel feature is computed per time step. Three consecutive features are stacked to give a final feature of size 240. The output vocabulary size consists of sub-word units of size 5000. The sentence piece library is used to train the subword model using the generic out-of-domain data <cite class="ltx_cite ltx_citemacro_cite">Kudo (<a href="#bib.bib12" title="" class="ltx_ref">2018</a>)</cite>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The CTC-based model consists of a series of stacked LSTM layers followed by a final dense layer projecting the hidden vectors onto the vocabulary space. The LSTM consists of 700 units at all levels. A total of 12 LSTM layers are present with a final dense layer of size 700 x 5001. The final vocabulary element is reserved for the blank token. The CTC loss function is used to train the model.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">The attention-based model follows a transformer LAS architecture. It consists of 10 encoder layers and 2 decoder layers. All the layers are standard transformer blocks. The internal model dimension is 512 units and the feed-forward dimension is 2048 units. Each block has 4 attention heads with 1024 units each. The final dense layer on the decoder side has a size of 512 x 5000. The same generic vocabulary is used for all the experiments. This sequence to sequence model is trained using the cross-entropy loss function.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">A single speaker TTS system is used to generate the synthetic data. The system is based on Tacotron2 architecture <cite class="ltx_cite ltx_citemacro_cite">Shen et al. (<a href="#bib.bib24" title="" class="ltx_ref">2018</a>)</cite> and a Clarinet <cite class="ltx_cite ltx_citemacro_cite">Ping et al. (<a href="#bib.bib20" title="" class="ltx_ref">2018</a>)</cite> based vocoder. The Tacotron2 sub-system converts a sequence of phonemes to a mel-spectrogram. The generated mel-spectrogram is converted into a time-domain using a Clarinet-style vocoder. In-house single speaker studio recordings are used to train this model. Both Hindi and English queries were recorded using the voice of the same artist and the text was represented in Devnagari script. The TTS system could therefore be used to convert both English and Hindi text to audio.</p>
</div>
<figure id="S3.F1" class="ltx_figure"><img src="/html/2206.13240/assets/domainflow2.png" id="S3.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="515" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Domain Adaptation Process</figcaption>
</figure>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.2.1" class="ltx_text ltx_font_bold">Test WER</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S3.T1.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.3.1.1" class="ltx_p"><span id="S3.T1.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Test WER</span> +</span>
<span id="S3.T1.1.1.1.3.1.2" class="ltx_p"><span id="S3.T1.1.1.1.3.1.2.1" class="ltx_text ltx_font_bold">LM Rescoring</span></span>
</span>
</th>
<th id="S3.T1.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T1.1.1.1.4.1" class="ltx_text ltx_font_bold">N-Best WER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.2.1" class="ltx_tr">
<td id="S3.T1.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">LAS-Gen</td>
<td id="S3.T1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">25.31</td>
<td id="S3.T1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">22.18</td>
<td id="S3.T1.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">13.71</td>
</tr>
<tr id="S3.T1.1.3.2" class="ltx_tr">
<td id="S3.T1.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t">LAS-Dense</td>
<td id="S3.T1.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">16.25</td>
<td id="S3.T1.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">15.55</td>
<td id="S3.T1.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">7.6</td>
</tr>
<tr id="S3.T1.1.4.3" class="ltx_tr">
<td id="S3.T1.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t">LAS-Decoder</td>
<td id="S3.T1.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">13.65</td>
<td id="S3.T1.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T1.1.4.3.3.1" class="ltx_text ltx_font_bold">13.36</span></td>
<td id="S3.T1.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">5.82</td>
</tr>
<tr id="S3.T1.1.5.4" class="ltx_tr">
<td id="S3.T1.1.5.4.1" class="ltx_td ltx_align_center ltx_border_t">CTC-Gen</td>
<td id="S3.T1.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">31.84</td>
<td id="S3.T1.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">25.58</td>
<td id="S3.T1.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">13.83</td>
</tr>
<tr id="S3.T1.1.6.5" class="ltx_tr">
<td id="S3.T1.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">CTC-Dense</td>
<td id="S3.T1.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">20.32</td>
<td id="S3.T1.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">17.66</td>
<td id="S3.T1.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">8.24</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Word Error Rate(WER) for different model variations using Voice Search Domain. The N-Best WER indicates the best WER in the top N=10 beams.</figcaption>
</figure>
<figure id="S3.T2" class="ltx_table">
<table id="S3.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T2.1.1.1" class="ltx_tr">
<th id="S3.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S3.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.2.1" class="ltx_text ltx_font_bold">Test WER</span></th>
<th id="S3.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t">
<span id="S3.T2.1.1.1.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T2.1.1.1.3.1.1" class="ltx_p"><span id="S3.T2.1.1.1.3.1.1.1" class="ltx_text ltx_font_bold">Test WER</span> +</span>
<span id="S3.T2.1.1.1.3.1.2" class="ltx_p"><span id="S3.T2.1.1.1.3.1.2.1" class="ltx_text ltx_font_bold">LM Rescoring</span></span>
</span>
</th>
<th id="S3.T2.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t"><span id="S3.T2.1.1.1.4.1" class="ltx_text ltx_font_bold">N-Best WER</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T2.1.2.1" class="ltx_tr">
<td id="S3.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_t">LAS-Gen</td>
<td id="S3.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">39.42</td>
<td id="S3.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">31.62</td>
<td id="S3.T2.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">25.35</td>
</tr>
<tr id="S3.T2.1.3.2" class="ltx_tr">
<td id="S3.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_t">LAS-Dense</td>
<td id="S3.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_t">22.57</td>
<td id="S3.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_t">16.38</td>
<td id="S3.T2.1.3.2.4" class="ltx_td ltx_align_center ltx_border_t">11.01</td>
</tr>
<tr id="S3.T2.1.4.3" class="ltx_tr">
<td id="S3.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_t">LAS-Decoder</td>
<td id="S3.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_t">18.96</td>
<td id="S3.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="S3.T2.1.4.3.3.1" class="ltx_text ltx_font_bold">12.54</span></td>
<td id="S3.T2.1.4.3.4" class="ltx_td ltx_align_center ltx_border_t">8.17</td>
</tr>
<tr id="S3.T2.1.5.4" class="ltx_tr">
<td id="S3.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_t">CTC-Gen</td>
<td id="S3.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_t">31.08</td>
<td id="S3.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_t">22.81</td>
<td id="S3.T2.1.5.4.4" class="ltx_td ltx_align_center ltx_border_t">19.74</td>
</tr>
<tr id="S3.T2.1.6.5" class="ltx_tr">
<td id="S3.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">CTC-Dense</td>
<td id="S3.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">22.43</td>
<td id="S3.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">15.42</td>
<td id="S3.T2.1.6.5.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_t">12.15</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Word Error Rate(WER) for different model variations using Address Domain. The N-Best WER indicates the best WER in the top N=10 beams.</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Dataset Details</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The train data consists of a multi-domain generic audio corpus and two domain-specific synthetic data sets. The generic data consists of crowdsourced read speech corpus. It consists of around 4 million samples amounting to 6500 hours of data. The domain-specific data were synthetically created using a single speaker TTS engine. The two domains under consideration are the voice search domain and address domain. The search domain corresponds to the Flipkart e-commerce product search domain. The address domain corresponds to the pan India delivery address. Both the domains contain around 3 million samples which are approximately 4000 hours of data for VS domain and 5000 for the address domain. The address domain queries are longer as compared to voice search queries. All the datasets consist of queries in both English and Hindi. All the text is represented in the Devanagari script. The test data was real-world domain-specific data recorded on the Flipkart application. The test data is multi-speaker data recorded in a noisy environment and it is very different than the single speaker TTS data recorded in noise-free studio settings. The test audio data was manually transcribed by the operations team. The voice search test data consisted of 25000 examples and address test data had 7000 examples. Except for linguistic overlap, the synthetic train and real test datasets represent completely different environments, and hence improvements reported in this work are not dependent on the quality of the TTS system as long as it is a single speaker.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In this work, we evaluate dense only fine-tuning baseline for CTC and attention-based models. The domain adaptation approach is presented on two datasets from voice search and address domain. The word error rates(WER) is used to compare the different approaches. The WER is word-level Levenshtein distance between ground truth text and output text. The results for voice search domain and address domain are shown in Table <a href="#S3.T1" title="Table 1 ‣ 3.1 Model Architecture ‣ 3 Methodology ‣ A Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Table <a href="#S3.T2" title="Table 2 ‣ 3.1 Model Architecture ‣ 3 Methodology ‣ A Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> respectively. The models are first trained on the generic multi-domain dataset and represented as CTC-Gen and LAS-Gen. These pre-trained models are then fine-tuned single speaker synthetic dataset. We show that dense only fine-tuning provides considerable improvement in accuracy while at the same time avoiding over-fitting on single speaker data. The dense-finetuned models are referred to as CTC-Dense and LAS-Dense. We also evaluate decoder-only fine-tuning for LAS models termed as LAS-Decoder. We report WER with and without external language model rescoring. A kenLM based language model is trained using text transcripts for both the domains individually. The N-Best WER is computed by picking the best beam from the top N=10 beam elements.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">The results show that LAS-Dense provides around 30% relative improvement in WER over LAS-Gen for VS domain and around 50% relative improvement for the address domain. The LAS-Decoder further improves the results by 14% for VS domain and 23% for the address domain. Similarly, CTC-Dense provides an improvement of 30% and 32% for VS and address domain respectively over CTC-Gen. Note that the WER of LAS-Gen evaluated on address domain is considerably high as compared to VS domain. Moreover, this simple fine-tuning and LM-rescoring provides high improvements in WER. This shows that the text distribution of address data is very different from the initial multi-domain data. Also, the variety of named entities is very high in address data as compared to VS data. Overall we show that dense only fine-tuning can provide us a reasonable baseline for domain adaptation. For encoder-decoder architectures, decoder fine-tuning serves as a better option. This is expected as the encoder part can also be seen as the acoustic network is frozen and the decoder network which can be seen as a contextual language model is fine-tuned. For CTC-based networks, we observe that extending fine-tuning to even a single lower LSTM layer results in over-fitting and degradation in performance. Therefore for CTC networks dense only fine-tuning is the optimal approach to avoid over-fitting.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In conclusion, we demonstrate a simple baseline approach for domain adaptation using a text-only corpus from the target domain. We show that the final dense layer only fine-tuning using single speaker TTS data provides considerable improvements over the generic model. The results are shown on two different domains of voice search and address domain. For both CTC and attention-based models we show that dense-only fine-tuning is a reasonable approach for domain adaptation. Although the technique is more relevant to CTC-based models it can also be used with encoder-decoder type models. For encoder-decoder models, the decoder only fine-tuning performs better.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bell et al. (2020)</span>
<span class="ltx_bibblock">
Peter Bell, Joachim Fainberg, Ondrej Klejch, Jinyu Li, Steve Renals, and Pawel
Swietojanski. 2020.

</span>
<span class="ltx_bibblock">Adaptation algorithms for neural network-based speech recognition: An
overview.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">IEEE Open Journal of Signal Processing</em>, 2:33–66.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al. (2015)</span>
<span class="ltx_bibblock">
William Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. 2015.

</span>
<span class="ltx_bibblock">Listen, attend and spell.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1508.01211</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fazel et al. (2021)</span>
<span class="ltx_bibblock">
Amin Fazel, Wei Yang, Yulan Liu, Roberto Barra-Chicote, Yixiong Meng, Roland
Maas, and Jasha Droppo. 2021.

</span>
<span class="ltx_bibblock">Synthasr: Unlocking synthetic data for speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2106.07803</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2021)</span>
<span class="ltx_bibblock">
Changfeng Gao, Gaofeng Cheng, Runyan Yang, Han Zhu, Pengyuan Zhang, and
Yonghong Yan. 2021.

</span>
<span class="ltx_bibblock">Pre-training transformer decoder for end-to-end asr model with
unpaired text data.

</span>
<span class="ltx_bibblock">In <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6543–6547. IEEE.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves (2012)</span>
<span class="ltx_bibblock">
Alex Graves. 2012.

</span>
<span class="ltx_bibblock">Sequence transduction with recurrent neural networks.

</span>
<span class="ltx_bibblock"><em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1211.3711</em>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al. (2006)</span>
<span class="ltx_bibblock">
Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen
Schmidhuber. 2006.

</span>
<span class="ltx_bibblock">Connectionist temporal classification: labelling unsegmented sequence
data with recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 23rd international conference on Machine
learning</em>, pages 369–376.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graves et al. (2013)</span>
<span class="ltx_bibblock">
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. 2013.

</span>
<span class="ltx_bibblock">Speech recognition with deep recurrent neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">2013 IEEE international conference on acoustics, speech and
signal processing</em>, pages 6645–6649. Ieee.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al. (2019)</span>
<span class="ltx_bibblock">
Yanzhang He, Tara N Sainath, Rohit Prabhavalkar, Ian McGraw, Raziel Alvarez,
Ding Zhao, David Rybach, Anjuli Kannan, Yonghui Wu, Ruoming Pang, et al.
2019.

</span>
<span class="ltx_bibblock">Streaming end-to-end speech recognition for mobile devices.

</span>
<span class="ltx_bibblock">In <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 6381–6385. IEEE.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2020)</span>
<span class="ltx_bibblock">
Jocelyn Huang, Oleksii Kuchaiev, Patrick O’Neill, Vitaly Lavrukhin, Jason Li,
Adriana Flores, Georg Kucsko, and Boris Ginsburg. 2020.

</span>
<span class="ltx_bibblock">Cross-language transfer learning, continuous learning, and domain
adaptation for end-to-end automatic speech recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2005.04290</em>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi and Kannan (2021)</span>
<span class="ltx_bibblock">
Raviraj Joshi and Venkateshan Kannan. 2021.

</span>
<span class="ltx_bibblock">Attention based end to end speech recognition for voice search in
hindi and english.

</span>
<span class="ltx_bibblock">In <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">Forum for Information Retrieval Evaluation</em>, pages 107–113.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kannan et al. (2018)</span>
<span class="ltx_bibblock">
Anjuli Kannan, Yonghui Wu, Patrick Nguyen, Tara N Sainath, Zhijeng Chen, and
Rohit Prabhavalkar. 2018.

</span>
<span class="ltx_bibblock">An analysis of incorporating an external language model into a
sequence-to-sequence model.

</span>
<span class="ltx_bibblock">In <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>, pages 1–5828. IEEE.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kudo (2018)</span>
<span class="ltx_bibblock">
Taku Kudo. 2018.

</span>
<span class="ltx_bibblock">Subword regularization: Improving neural network translation models
with multiple subword candidates.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 56th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pages 66–75.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laptev et al. (2020)</span>
<span class="ltx_bibblock">
Aleksandr Laptev, Roman Korostik, Aleksey Svischev, Andrei Andrusenko, Ivan
Medennikov, and Sergey Rybin. 2020.

</span>
<span class="ltx_bibblock">You do not need more data: Improving end-to-end speech recognition by
text-to-speech data augmentation.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2020 13th International Congress on Image and Signal
Processing, BioMedical Engineering and Informatics (CISP-BMEI)</em>, pages
439–444. IEEE.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2020)</span>
<span class="ltx_bibblock">
Jinyu Li, Rui Zhao, Zhong Meng, Yanqing Liu, Wenning Wei, Sarangarajan
Parthasarathy, Vadim Mazalov, Zhenghao Wang, Lei He, Sheng Zhao, et al. 2020.

</span>
<span class="ltx_bibblock">Developing rnn-t models surpassing high-performance hybrid models
with customization capability.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2007.15188</em>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021)</span>
<span class="ltx_bibblock">
Linda Liu, Yile Gu, Aditya Gourav, Ankur Gandhe, Shashank Kalmane, Denis
Filimonov, Ariya Rastrow, and Ivan Bulyko. 2021.

</span>
<span class="ltx_bibblock">Domain-aware neural language models for speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 7373–7377. IEEE.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Meng et al. (2021)</span>
<span class="ltx_bibblock">
Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda,
Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong. 2021.

</span>
<span class="ltx_bibblock">Internal language model estimation for domain-adaptive end-to-end
speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">2021 IEEE Spoken Language Technology Workshop (SLT)</em>, pages
243–250. IEEE.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mimura et al. (2018)</span>
<span class="ltx_bibblock">
Masato Mimura, Sei Ueno, Hirofumi Inaguma, Shinsuke Sakai, and Tatsuya
Kawahara. 2018.

</span>
<span class="ltx_bibblock">Leveraging sequence-to-sequence speech synthesis for enhancing
acoustic-to-word speech recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">2018 IEEE Spoken Language Technology Workshop (SLT)</em>, pages
477–484. IEEE.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2019)</span>
<span class="ltx_bibblock">
Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D
Cubuk, and Quoc V Le. 2019.

</span>
<span class="ltx_bibblock">Specaugment: A simple data augmentation method for automatic speech
recognition.

</span>
<span class="ltx_bibblock"><em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1904.08779</em>.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peyser et al. (2019)</span>
<span class="ltx_bibblock">
Cal Peyser, Hao Zhang, Tara N Sainath, and Zelin Wu. 2019.

</span>
<span class="ltx_bibblock">Improving performance of end-to-end asr on numeric sequences.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1907.01372</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ping et al. (2018)</span>
<span class="ltx_bibblock">
Wei Ping, Kainan Peng, and Jitong Chen. 2018.

</span>
<span class="ltx_bibblock">Clarinet: Parallel wave generation in end-to-end text-to-speech.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.07281</em>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pylkkönen et al. (2021)</span>
<span class="ltx_bibblock">
Janne Pylkkönen, Antti Ukkonen, Juho Kilpikoski, Samu Tamminen, and Hannes
Heikinheimo. 2021.

</span>
<span class="ltx_bibblock">Fast text-only domain adaptation of rnn-transducer prediction
network.

</span>
<span class="ltx_bibblock"><em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2104.11127</em>.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rossenbach et al. (2020)</span>
<span class="ltx_bibblock">
Nick Rossenbach, Albert Zeyer, Ralf Schlüter, and Hermann Ney. 2020.

</span>
<span class="ltx_bibblock">Generating synthetic audio data for attention-based speech
recognition systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">ICASSP 2020-2020 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 7069–7073. IEEE.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shan et al. (2019)</span>
<span class="ltx_bibblock">
Changhao Shan, Chao Weng, Guangsen Wang, Dan Su, Min Luo, Dong Yu, and Lei Xie.
2019.

</span>
<span class="ltx_bibblock">Component fusion: Learning replaceable language model component for
end-to-end speech recognition system.

</span>
<span class="ltx_bibblock">In <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">ICASSP 2019-2019 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 5361–5635. IEEE.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al. (2018)</span>
<span class="ltx_bibblock">
Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike Schuster, Navdeep Jaitly,
Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al.
2018.

</span>
<span class="ltx_bibblock">Natural tts synthesis by conditioning wavenet on mel spectrogram
predictions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">2018 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP)</em>, pages 4779–4783. IEEE.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sim et al. (2019)</span>
<span class="ltx_bibblock">
Khe Chai Sim, Françoise Beaufays, Arnaud Benard, Dhruv Guliani, Andreas
Kabel, Nikhil Khare, Tamar Lucassen, Petr Zadrazil, Harry Zhang, Leif
Johnson, et al. 2019.

</span>
<span class="ltx_bibblock">Personalization of end-to-end speech recognition on mobile devices
for named entities.

</span>
<span class="ltx_bibblock">In <em id="bib.bib25.1.1" class="ltx_emph ltx_font_italic">2019 IEEE Automatic Speech Recognition and Understanding
Workshop (ASRU)</em>, pages 23–30. IEEE.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ueno et al. (2018)</span>
<span class="ltx_bibblock">
Sei Ueno, Takafumi Moriya, Masato Mimura, Shinsuke Sakai, Yusuke Shinohara,
Yoshikazu Yamaguchi, Yushi Aono, and Tatsuya Kawahara. 2018.

</span>
<span class="ltx_bibblock">Encoder transfer for attention-based acoustic-to-word speech
recognition.

</span>
<span class="ltx_bibblock">In <em id="bib.bib26.1.1" class="ltx_emph ltx_font_italic">INTERSPEECH</em>, pages 2424–2428.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2021)</span>
<span class="ltx_bibblock">
Xianrui Zheng, Yulan Liu, Deniz Gunceler, and Daniel Willett. 2021.

</span>
<span class="ltx_bibblock">Using synthetic audio to improve the recognition of out-of-vocabulary
words in end-to-end asr systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib27.1.1" class="ltx_emph ltx_font_italic">ICASSP 2021-2021 IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP)</em>, pages 5674–5678. IEEE.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2206.13239" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2206.13240" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2206.13240">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2206.13240" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2206.13241" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 17:57:21 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
