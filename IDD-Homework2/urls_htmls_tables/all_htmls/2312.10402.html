<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2312.10402] Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion</title><meta property="og:description" content="Automatic Music Transcription (AMT) is a vital technology in the field of music information processing. Despite recent enhancements in performance due to machine learning techniques, current methods typically attain hi…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2312.10402">

<!--Generated on Tue Feb 27 11:35:46 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Annotation-free Automatic Music Transcription
with Scalable Synthetic Data and Adversarial
Domain Confusion</h1>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Automatic Music Transcription (AMT) is a vital technology in the field of music information processing. Despite recent enhancements in performance due to machine learning techniques, current methods typically attain high accuracy in domains where abundant annotated data is available.
Addressing domains with low or no resources continues to be an unresolved challenge.
To tackle this issue, we propose a transcription model that does not require any MIDI-audio paired data through the utilization of scalable synthetic audio for pre-training and adversarial domain confusion using unannotated real audio.
In experiments, we evaluate methods under the real-world application scenario where training datasets do not include the MIDI annotation of audio in the target data domain. Our proposed method achieved competitive performance relative to established baseline methods, despite not utilizing any real datasets of paired MIDI-audio.
Additionally, ablation studies have provided insights into the scalability of this approach and the forthcoming challenges in the field of AMT research.</p>
</div>
<span id="footnotex1" class="ltx_note ltx_role_footnotetext"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>These two authors contributed equally to this work</span></span></span>
<div id="p1" class="ltx_para">
<p id="p1.1" class="ltx_p"><span id="p1.1.1" class="ltx_text ltx_font_bold ltx_font_italic">Index Terms<span id="p1.1.1.1" class="ltx_text ltx_font_upright">— </span></span>
automatic music transcription, adversarial domain confusion, synthetic data, low-resource</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Automatic Music Transcription (AMT) is a significant technology in the realm of music information processing. Generally, it seeks to convert music audio into a symbolic representation automatically, typically in the form of musical notes, pitch, timing, velocity, and other pertinent musical elements.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Constructing a practical AMT system necessitates a model adaptable to any instrument, a challenging task due to the complex overtone structures and dynamic articulations unique to each instrument and playing style.
The current most effective approach to solve this complex problem is supervised learning with large-scale models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. While various studies have been conducted and have contributed to significant performance improvements, this approach generally requires large annotated datasets.
However, as of now, no dataset exists with an adequate amount of data and diversity to tackle the complexity of such AMT tasks.
Therefore, most existing methods either specialize in a specific instrument type, like the piano, for which datasets are plentiful <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>, or they aim to facilitate multitasking by amalgamating existing datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.
While this multitasking approach has seen some success, data-rich domains like piano still consistently yield strong results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. This performance may rely on the quality and quantity of currently available datasets and fails to achieve genuinely general performance across data domains not used in training.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">One primary reason for the dataset shortage is the challenge of annotation. Supervised learning for AMT necessitates ground truth MIDI data corresponding to the audio, detailing the timing and pitch of the played notes.
The most straightforward methods to obtain this data are through expert human annotation or by capturing MIDI-audio data simultaneously with a MIDI instrument.
However, neither method is efficient, and many instrument domains still lack a substantial collection of annotated data for transcription <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. Additionally, the recent music industry has witnessed a diversification of tones in songs, owing to the rise of Desk Top Music (DTM) and other technologies.
Merely amalgamating existing annotated data offers limited scope for enhancing generalization.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To solve this problem, we propose a transcription model with adversarial domain confusion of scalable synthetic and real domain data, achieving <span id="S1.p4.1.1" class="ltx_text ltx_font_bold">annotation-free AMT</span> where the model does not require any MIDI-audio paired data for training.
The domain confusion used here is a method of learning to make it difficult for a model to distinguish between different target domains.
By applying this method to synthetic domain audio and real domain audio, we fine-tune the transcription model pre-trained in the synthetic domain to the real domain.
In this process, synthetic domain data is generated by rendering MIDI data and one-shot audio data, and we only use unannotated audio data for real domain data. This enables a workflow that completely eliminates the need for MIDI-audio paired data.
In addition, we propose a scalable audio synthetic method using mixing of the one-shot audio. By using this method, we can train models with complex and diverse input patterns.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">We conduct experiments in the <span id="S1.p5.1.1" class="ltx_text ltx_font_bold">target-annotation-free setting</span> where training datasets do not include the MIDI annotation of audio in the target data domain.
The scores of our proposed method are competitive with those of established baseline methods, which utilize annotated data from other real domains, even though we do not use any MIDI-audio paired datasets for training.
We also perform ablation studies to examine the relationship between the timbre and MIDI variations of synthetic audio and their impact on transcription performance.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Automatic Music Transcription (AMT)</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">AMT has been extensively studied and researched.
It started with signal processing approaches like classical NMF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, and in recent years, machine learning-based approaches have led to significant performance improvements <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>.
Particularly recently, large-scale networks utilizing transformer models have demonstrated high performance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
Because of the aforementioned lack of a generic dataset, many studies have focused their approach on specific instrument types such as piano <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, guitar <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and so on.
As approaches to multitasking, some studies have combined multiple datasets to train a single model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">Several approaches have been developed to address less annotated (low-resource) domains.
Cheuk et al. used unannotated music recordings in semi-supervised learning for AMT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Wu et al. proposed a hybrid-supervised framework, the masked frame-level autoencoder (MFAE) to understand generic representations of low-resource instruments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>.
Bittner et al. proposed an “instrument-agnostic” method aimed at achieving general performance, independent of instrument types <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
Cheuk et al. solved the transcription task using diffusion generative models, which boosts performance by pretraining on pianoroll data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
Contrary to previous work, we propose a model that does not rely on real MIDI-audio paired datasets and demonstrates effective performance in the target-annotation-free setting.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Synthetic audio</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">In the field of music information processing, using synthesized audio in training is valuable for several reasons, particularly its ease of acquisition.
One method involves synthesizing MIDI datasets using one-shot audio to present a dataset specifically designed for instrument retrieval tasks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>.
Slakh <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> is an audio-MIDI paired dataset that renders music MIDI data with 187 different timbres using professional-grade sample-based virtual instruments, constituting the largest dataset suitable for transcription.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.1" class="ltx_p">Beyond Slakh, several studies have also employed synthetic audio for AMT.
Kim et al. utilized synthetic audio generated by oscillators to augment the guitar transcription datasets <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>.
Maman et al. utilized synthesized and unaligned piano audio data for the expectation maximization framework to enable training on in-the-wild recordings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.
Simon et al. adopted a strategy of creating annotated polyphonic sounds by using monophonic sounds and their transcription results <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>.
In this paper, we generate synthetic audio by rendering various MIDI with one-shot audio of diverse tones.
This approach leverages the accessibility, scalability, and customizability of MIDI and one-shot audio.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Deep domain adaptation and confusion</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">In recent years, domain adaptation approaches that perform adversarial training to align the representation of source data with that of target data were proposed <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite>.
Particularly, the technique of training models to learn domain-invariant representations is known as <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">domain confusion</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>.
Tzeng et al. later proposed using a discriminator to achieve domain confusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para">
<p id="S2.SS3.p2.1" class="ltx_p">In the context of music information processing, domain adaptation and confusion have been applied to various tasks.
Bittner et al. studied microphone mismatch in automatic piano transcription using domain adaptation techniques <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>. Lordelo et al. introduced adversarial domain adaptation to improve harmonic-percussive source separation in different domains <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>.
Narita et al. used domain confusion to extract pitch-invariant information, equating pitch differences with domain differences <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>.
Mor et al. achieved music translation by domain confusion of sound features of different genres and different instruments <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>.
In this study, we demonstrate that employing domain confusion can make synthetic and real domains indistinguishable, thereby enhancing the transcription performance of real data.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Synthetic audio</h3>

<figure id="S3.F1" class="ltx_figure">
<p id="S3.F1.1.1" class="ltx_p ltx_minipage ltx_align_bottom" style="width:433.6pt;"><span id="S3.F1.1.1.1" class="ltx_text"><img src="/html/2312.10402/assets/model10.png" id="S3.F1.1.1.1.g1" class="ltx_graphics ltx_img_landscape" width="687" height="163" alt="Refer to caption"></span></p>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.3.1.1" class="ltx_text ltx_font_bold">Fig. 1</span>: </span>Our proposed method.</figcaption>
</figure>
<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.7" class="ltx_p">The goal of this study is to build a transcription model achieving annotation-free AMT where the model does not require any MIDI-audio paired data for training.
In this process, we generate the synthesized audio in a simple manner using MIDI data and one-shot audio data.
First, to enhance timbre patterns, we mix audio samples as follows.
Two one-shot audio samples are randomly selected with different timbres from a one-shot dataset, where one is a sample of the main timbre (<math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="T_{\mathrm{main}}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><msub id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml">main</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">𝑇</ci><ci id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3">main</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">T_{\mathrm{main}}</annotation></semantics></math>) and the other is that of the sub timbre (<math id="S3.SS1.p1.2.m2.1" class="ltx_Math" alttext="T_{\mathrm{sub}}" display="inline"><semantics id="S3.SS1.p1.2.m2.1a"><msub id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">T</mi><mi id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">sub</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑇</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">sub</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">T_{\mathrm{sub}}</annotation></semantics></math>).
For each pitch, samples of mixed timbres are calculated by <math id="S3.SS1.p1.3.m3.1" class="ltx_Math" alttext="T_{\mathrm{main}}+\alpha T_{\mathrm{sub}}" display="inline"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><msub id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2.2" xref="S3.SS1.p1.3.m3.1.1.2.2.cmml">T</mi><mi id="S3.SS1.p1.3.m3.1.1.2.3" xref="S3.SS1.p1.3.m3.1.1.2.3.cmml">main</mi></msub><mo id="S3.SS1.p1.3.m3.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.cmml">+</mo><mrow id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.3.2.cmml">α</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.3.m3.1.1.3.1" xref="S3.SS1.p1.3.m3.1.1.3.1.cmml">​</mo><msub id="S3.SS1.p1.3.m3.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.3.3.2" xref="S3.SS1.p1.3.m3.1.1.3.3.2.cmml">T</mi><mi id="S3.SS1.p1.3.m3.1.1.3.3.3" xref="S3.SS1.p1.3.m3.1.1.3.3.3.cmml">sub</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><plus id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1"></plus><apply id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2.2">𝑇</ci><ci id="S3.SS1.p1.3.m3.1.1.2.3.cmml" xref="S3.SS1.p1.3.m3.1.1.2.3">main</ci></apply><apply id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3"><times id="S3.SS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.1"></times><ci id="S3.SS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.2">𝛼</ci><apply id="S3.SS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.3.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.2">𝑇</ci><ci id="S3.SS1.p1.3.m3.1.1.3.3.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3.3.3">sub</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">T_{\mathrm{main}}+\alpha T_{\mathrm{sub}}</annotation></semantics></math>.
The mixing rate <math id="S3.SS1.p1.4.m4.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\alpha</annotation></semantics></math> is randomized to increase timbre variation from a uniform distribution within the [0, 2] range, as determined by preliminary experiments. Following this process, we normalize the mixed one-shot audio to fall within a range of -1 to 1.
By doing this, <math id="S3.SS1.p1.5.m5.1" class="ltx_Math" alttext="n(n-1)/2" display="inline"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml"><mrow id="S3.SS1.p1.5.m5.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.5.m5.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.2.cmml">​</mo><mrow id="S3.SS1.p1.5.m5.1.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS1.p1.5.m5.1.1.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS1.p1.5.m5.1.1.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.5.m5.1.1.1.1.1.1.2" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.2.cmml">n</mi><mo id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.cmml">−</mo><mn id="S3.SS1.p1.5.m5.1.1.1.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo stretchy="false" id="S3.SS1.p1.5.m5.1.1.1.1.1.3" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p1.5.m5.1.1.2" xref="S3.SS1.p1.5.m5.1.1.2.cmml">/</mo><mn id="S3.SS1.p1.5.m5.1.1.3" xref="S3.SS1.p1.5.m5.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1"><divide id="S3.SS1.p1.5.m5.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.2"></divide><apply id="S3.SS1.p1.5.m5.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1"><times id="S3.SS1.p1.5.m5.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.2"></times><ci id="S3.SS1.p1.5.m5.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.3">𝑛</ci><apply id="S3.SS1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1"><minus id="S3.SS1.p1.5.m5.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.1"></minus><ci id="S3.SS1.p1.5.m5.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.2">𝑛</ci><cn type="integer" id="S3.SS1.p1.5.m5.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.1.1.1.1.3">1</cn></apply></apply><cn type="integer" id="S3.SS1.p1.5.m5.1.1.3.cmml" xref="S3.SS1.p1.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">n(n-1)/2</annotation></semantics></math> (+ difference of <math id="S3.SS1.p1.6.m6.1" class="ltx_Math" alttext="\alpha" display="inline"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">\alpha</annotation></semantics></math>) different timbres can be created using <math id="S3.SS1.p1.7.m7.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">n</annotation></semantics></math> different timbres. This has the effect of increasing the tonal diversity of the input data.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.4" class="ltx_p">Next, this mixed one-shot audio and MIDI data are used to generate the synthetic audio.
For the instrument group of one-shot audio <math id="S3.SS1.p2.1.m1.1" class="ltx_Math" alttext="T_{\mathrm{main}}" display="inline"><semantics id="S3.SS1.p2.1.m1.1a"><msub id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml">main</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">𝑇</ci><ci id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3">main</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">T_{\mathrm{main}}</annotation></semantics></math>, the MIDI segment of the corresponding MIDI program number is chosen: 1-8 for keyboard, 9-16 for mallet, 17-24 for organ, 25-32 and 105-112 for guitar, 33-40 for bass, 41-56 for strings, 57-64 for brass, 65-72 for reed, 73-80 for flute, and 81-96 for both synth and vocal.
From the MIDI segment, we can obtain onset-time, offset-time, pitch, and velocity information for each note.
The one-shot audio is trimmed according to the note pitch and length (defined as offset-time minus onset-time) for each note.
If the length of a note exceeds the length of the one-shot audio, we simply use the full length of the audio.
The velocity (<math id="S3.SS1.p2.2.m2.1" class="ltx_Math" alttext="v" display="inline"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">v</annotation></semantics></math>) of each note determines the amplitude of each one-shot audio (<math id="S3.SS1.p2.3.m3.1" class="ltx_Math" alttext="a" display="inline"><semantics id="S3.SS1.p2.3.m3.1a"><mi id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><ci id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">a</annotation></semantics></math>) by <math id="S3.SS1.p2.4.m4.2" class="ltx_Math" alttext="a=\log(1+v/127)/\log 2" display="inline"><semantics id="S3.SS1.p2.4.m4.2a"><mrow id="S3.SS1.p2.4.m4.2.2" xref="S3.SS1.p2.4.m4.2.2.cmml"><mi id="S3.SS1.p2.4.m4.2.2.3" xref="S3.SS1.p2.4.m4.2.2.3.cmml">a</mi><mo id="S3.SS1.p2.4.m4.2.2.2" xref="S3.SS1.p2.4.m4.2.2.2.cmml">=</mo><mrow id="S3.SS1.p2.4.m4.2.2.1" xref="S3.SS1.p2.4.m4.2.2.1.cmml"><mrow id="S3.SS1.p2.4.m4.2.2.1.1.1" xref="S3.SS1.p2.4.m4.2.2.1.1.2.cmml"><mi id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml">log</mi><mo id="S3.SS1.p2.4.m4.2.2.1.1.1a" xref="S3.SS1.p2.4.m4.2.2.1.1.2.cmml">⁡</mo><mrow id="S3.SS1.p2.4.m4.2.2.1.1.1.1" xref="S3.SS1.p2.4.m4.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.SS1.p2.4.m4.2.2.1.1.1.1.2" xref="S3.SS1.p2.4.m4.2.2.1.1.2.cmml">(</mo><mrow id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.cmml"><mn id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.2" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.2.cmml">1</mn><mo id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.1" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.1.cmml">+</mo><mrow id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.cmml"><mi id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.2" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.2.cmml">v</mi><mo id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.1" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.1.cmml">/</mo><mn id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.3" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.3.cmml">127</mn></mrow></mrow><mo stretchy="false" id="S3.SS1.p2.4.m4.2.2.1.1.1.1.3" xref="S3.SS1.p2.4.m4.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.SS1.p2.4.m4.2.2.1.2" xref="S3.SS1.p2.4.m4.2.2.1.2.cmml">/</mo><mrow id="S3.SS1.p2.4.m4.2.2.1.3" xref="S3.SS1.p2.4.m4.2.2.1.3.cmml"><mi id="S3.SS1.p2.4.m4.2.2.1.3.1" xref="S3.SS1.p2.4.m4.2.2.1.3.1.cmml">log</mi><mo lspace="0.167em" id="S3.SS1.p2.4.m4.2.2.1.3a" xref="S3.SS1.p2.4.m4.2.2.1.3.cmml">⁡</mo><mn id="S3.SS1.p2.4.m4.2.2.1.3.2" xref="S3.SS1.p2.4.m4.2.2.1.3.2.cmml">2</mn></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.2b"><apply id="S3.SS1.p2.4.m4.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2"><eq id="S3.SS1.p2.4.m4.2.2.2.cmml" xref="S3.SS1.p2.4.m4.2.2.2"></eq><ci id="S3.SS1.p2.4.m4.2.2.3.cmml" xref="S3.SS1.p2.4.m4.2.2.3">𝑎</ci><apply id="S3.SS1.p2.4.m4.2.2.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1"><divide id="S3.SS1.p2.4.m4.2.2.1.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.2"></divide><apply id="S3.SS1.p2.4.m4.2.2.1.1.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1"><log id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"></log><apply id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1"><plus id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.1"></plus><cn type="integer" id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.2">1</cn><apply id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3"><divide id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.1"></divide><ci id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.2">𝑣</ci><cn type="integer" id="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.3.cmml" xref="S3.SS1.p2.4.m4.2.2.1.1.1.1.1.3.3">127</cn></apply></apply></apply><apply id="S3.SS1.p2.4.m4.2.2.1.3.cmml" xref="S3.SS1.p2.4.m4.2.2.1.3"><log id="S3.SS1.p2.4.m4.2.2.1.3.1.cmml" xref="S3.SS1.p2.4.m4.2.2.1.3.1"></log><cn type="integer" id="S3.SS1.p2.4.m4.2.2.1.3.2.cmml" xref="S3.SS1.p2.4.m4.2.2.1.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.2c">a=\log(1+v/127)/\log 2</annotation></semantics></math>.
Finally, audio samples for each note are overlaid and added together to produce the musical audio.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para">
<p id="S3.SS1.p3.1" class="ltx_p">In general, detection of offset time is a difficult problem in transcription due to the release time.
The release time corresponds to the period following the offset time , which is not explicitly indicated by MIDI data.
Additionally, this varies across different instruments and is influenced by audio effects, like reverb.
In this study, the release time is defined randomly within a specific range based on the program number stored in the MIDI data: 0.1 to 1.0 seconds for keyboard, organ, mallet, brass, synth, and vocal, 0.8 to 1.0 seconds for strings, reed, and flute, 0.1 to 0.5 seconds for guitar, and 0.1 to 0.2 seconds for bass.</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Contrary to synthetic audio, real audio often undergoes some degree of compression during the recording and production process.
To accommodate this issue, we apply a limiting process to the synthesized audio. We choose limiting over compression due to its higher computational efficiency and its better capability in managing challenging scenarios. The limiting is applied to the normalized audio with a probability of 80%, with the threshold being set randomly from a uniform distribution in the range of 0.5 to 1.0.
After completing this process, normalization is applied again.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Transcription architecture</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">This section explains the transcription architecture as illustrated in Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 Synthetic audio ‣ 3 Model ‣ Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
For input data, we use 2.56-second audio segments sampled in 16,000Hz, converted into mel spectrograms (window length is 2,048, hop length is 160, the number of mel bins is 384, and the time resolution is 10ms).
Each time step of the mel spectrogram is fed into the fully-connected layer, the positional embedding is added, and then fed into the transformer as proposed in Gardner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p">For output data, we employ absolute time tokens based on MIDI-like representation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> and supplemented with an end-tie token as proposed in Gardner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> to deal with notes that span separated audio segments. The tokens we use are pitch (128 values), time (256 values), note ON/OFF (2 values), begin of sequence/end of sequence (2 values), and end-tie (1 value). Note that velocity is not a prediction target in this study.
The vanilla transformer is employed as our transcription model <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>.
Based on preliminary experiments, the transformer’s internal parameters were set as follows: the dimension of the fully-connected layer is set to 1,024, the number of heads to 6, the number of encoder layers to 2, the number of decoder layers to 3, the embedding dimension to 384, and the learning rate to 1e-4.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">After the pre-training, we perform domain confusion using synthetic audio data and real audio data as fine-tuning.
First, both real and synthetic audio data are input into the transformer encoder. The encoder outputs from both synthetic and real data are transmitted into the discriminator, a three-layer fully-connected network, and the encoder output only from synthetic data is transmitted into the transformer decoder.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Our goal is to bridge the gap between the synthetic and real domains in terms of timbral features, rather than the characteristics of note sequences.
We assume that the discriminator can focus on information other than the note sequence by inputting a short time range of the encoder output to the discriminator. To achieve this, 0.1-second intervals are randomly chosen from the encoder output and used as input to the discriminator.
The learning rates for the transformer and the discriminator are set to 1e-5 and 1e-4, respectively.
As shown on the right side of Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 Synthetic audio ‣ 3 Model ‣ Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, the following objectives are alternately optimized:</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<table id="S5.EGx1" class="ltx_equationgroup ltx_eqn_align ltx_eqn_table">

<tbody id="S3.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1.m1.2" class="ltx_Math" alttext="\displaystyle\underset{C}{\min}\&gt;\mathrm{BCE}(0,C(E(\bm{x}_{\mathrm{S}})))+" display="inline"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><munder accentunder="true" id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml"><mi id="S3.E1.m1.2.2.1.3.2" xref="S3.E1.m1.2.2.1.3.2.cmml">min</mi><mo id="S3.E1.m1.2.2.1.3.1" xref="S3.E1.m1.2.2.1.3.1.cmml">𝐶</mo></munder><mo lspace="0.387em" rspace="0em" id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml">​</mo><mi id="S3.E1.m1.2.2.1.4" xref="S3.E1.m1.2.2.1.4.cmml">BCE</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.2a" xref="S3.E1.m1.2.2.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1" xref="S3.E1.m1.2.2.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">(</mo><mn id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">0</mn><mo id="S3.E1.m1.2.2.1.1.1.3" xref="S3.E1.m1.2.2.1.1.2.cmml">,</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml">S</mi></msub><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m1.2.2.1.1.1.4" xref="S3.E1.m1.2.2.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">+</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2">limit-from</csymbol><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><times id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"></times><apply id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3"><ci id="S3.E1.m1.2.2.1.3.1.cmml" xref="S3.E1.m1.2.2.1.3.1">𝐶</ci><min id="S3.E1.m1.2.2.1.3.2.cmml" xref="S3.E1.m1.2.2.1.3.2"></min></apply><ci id="S3.E1.m1.2.2.1.4.cmml" xref="S3.E1.m1.2.2.1.4">BCE</ci><interval closure="open" id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1"><cn type="integer" id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">0</cn><apply id="S3.E1.m1.2.2.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.3">𝐶</ci><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.3">𝐸</ci><apply id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1.1.1.1.1.3">S</ci></apply></apply></apply></interval></apply><plus id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">\displaystyle\underset{C}{\min}\&gt;\mathrm{BCE}(0,C(E(\bm{x}_{\mathrm{S}})))+</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E1.m2.2" class="ltx_Math" alttext="\displaystyle\mathrm{BCE}(1,C(E(\bm{x}_{\mathrm{R}})))," display="inline"><semantics id="S3.E1.m2.2a"><mrow id="S3.E1.m2.2.2.1" xref="S3.E1.m2.2.2.1.1.cmml"><mrow id="S3.E1.m2.2.2.1.1" xref="S3.E1.m2.2.2.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.3" xref="S3.E1.m2.2.2.1.1.3.cmml">BCE</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.2.2.1.1.2" xref="S3.E1.m2.2.2.1.1.2.cmml">​</mo><mrow id="S3.E1.m2.2.2.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.2.cmml">(</mo><mn id="S3.E1.m2.1.1" xref="S3.E1.m2.1.1.cmml">1</mn><mo id="S3.E1.m2.2.2.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.2.cmml">,</mo><mrow id="S3.E1.m2.2.2.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.2.2.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m2.2.2.1.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">R</mi></msub><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.3" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E1.m2.2.2.1.1.1.1.4" xref="S3.E1.m2.2.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E1.m2.2.2.1.2" xref="S3.E1.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m2.2b"><apply id="S3.E1.m2.2.2.1.1.cmml" xref="S3.E1.m2.2.2.1"><times id="S3.E1.m2.2.2.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.2"></times><ci id="S3.E1.m2.2.2.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.3">BCE</ci><interval closure="open" id="S3.E1.m2.2.2.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1"><cn type="integer" id="S3.E1.m2.1.1.cmml" xref="S3.E1.m2.1.1">1</cn><apply id="S3.E1.m2.2.2.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1"><times id="S3.E1.m2.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.2"></times><ci id="S3.E1.m2.2.2.1.1.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.3">𝐶</ci><apply id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1"><times id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.3">𝐸</ci><apply id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3">R</ci></apply></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m2.2c">\displaystyle\mathrm{BCE}(1,C(E(\bm{x}_{\mathrm{R}}))),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.Ex1.m1.4" class="ltx_Math" alttext="\displaystyle\underset{E,D}{\min}\&gt;\mathrm{CE}(\bm{i}(\bm{x}_{\mathrm{S}}),D(E(\bm{x}_{\mathrm{S}})))+" display="inline"><semantics id="S3.Ex1.m1.4a"><mrow id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4.cmml"><mrow id="S3.Ex1.m1.4.4.2" xref="S3.Ex1.m1.4.4.2.cmml"><munder accentunder="true" id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml"><mi id="S3.Ex1.m1.2.2.3" xref="S3.Ex1.m1.2.2.3.cmml">min</mi><mrow id="S3.Ex1.m1.2.2.2.4" xref="S3.Ex1.m1.2.2.2.3.cmml"><mi id="S3.Ex1.m1.1.1.1.1" xref="S3.Ex1.m1.1.1.1.1.cmml">E</mi><mo id="S3.Ex1.m1.2.2.2.4.1" xref="S3.Ex1.m1.2.2.2.3.cmml">,</mo><mi id="S3.Ex1.m1.2.2.2.2" xref="S3.Ex1.m1.2.2.2.2.cmml">D</mi></mrow></munder><mo lspace="0.387em" rspace="0em" id="S3.Ex1.m1.4.4.2.3" xref="S3.Ex1.m1.4.4.2.3.cmml">​</mo><mi id="S3.Ex1.m1.4.4.2.4" xref="S3.Ex1.m1.4.4.2.4.cmml">CE</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.2.3a" xref="S3.Ex1.m1.4.4.2.3.cmml">​</mo><mrow id="S3.Ex1.m1.4.4.2.2.2" xref="S3.Ex1.m1.4.4.2.2.3.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.2.2.2.3" xref="S3.Ex1.m1.4.4.2.2.3.cmml">(</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.1.3.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.3.3.1.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.1.2.cmml">​</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.3.3.1.1.1.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.3.cmml">S</mi></msub><mo stretchy="false" id="S3.Ex1.m1.3.3.1.1.1.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.4.4.2.2.2.4" xref="S3.Ex1.m1.4.4.2.2.3.cmml">,</mo><mrow id="S3.Ex1.m1.4.4.2.2.2.2" xref="S3.Ex1.m1.4.4.2.2.2.2.cmml"><mi id="S3.Ex1.m1.4.4.2.2.2.2.3" xref="S3.Ex1.m1.4.4.2.2.2.2.3.cmml">D</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.2.2.2.2.2" xref="S3.Ex1.m1.4.4.2.2.2.2.2.cmml">​</mo><mrow id="S3.Ex1.m1.4.4.2.2.2.2.1.1" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.2.2.2.2.1.1.2" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.cmml"><mi id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.3" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.2" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.2" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.2" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.3" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.3.cmml">S</mi></msub><mo stretchy="false" id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.3" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex1.m1.4.4.2.2.2.2.1.1.3" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex1.m1.4.4.2.2.2.5" xref="S3.Ex1.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m1.4.4.4" xref="S3.Ex1.m1.4.4.4.cmml">+</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.4b"><apply id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4"><csymbol cd="latexml" id="S3.Ex1.m1.4.4.3.cmml" xref="S3.Ex1.m1.4.4">limit-from</csymbol><apply id="S3.Ex1.m1.4.4.2.cmml" xref="S3.Ex1.m1.4.4.2"><times id="S3.Ex1.m1.4.4.2.3.cmml" xref="S3.Ex1.m1.4.4.2.3"></times><apply id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2"><list id="S3.Ex1.m1.2.2.2.3.cmml" xref="S3.Ex1.m1.2.2.2.4"><ci id="S3.Ex1.m1.1.1.1.1.cmml" xref="S3.Ex1.m1.1.1.1.1">𝐸</ci><ci id="S3.Ex1.m1.2.2.2.2.cmml" xref="S3.Ex1.m1.2.2.2.2">𝐷</ci></list><min id="S3.Ex1.m1.2.2.3.cmml" xref="S3.Ex1.m1.2.2.3"></min></apply><ci id="S3.Ex1.m1.4.4.2.4.cmml" xref="S3.Ex1.m1.4.4.2.4">CE</ci><interval closure="open" id="S3.Ex1.m1.4.4.2.2.3.cmml" xref="S3.Ex1.m1.4.4.2.2.2"><apply id="S3.Ex1.m1.3.3.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1"><times id="S3.Ex1.m1.3.3.1.1.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.2"></times><ci id="S3.Ex1.m1.3.3.1.1.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.3">𝒊</ci><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.1.3">S</ci></apply></apply><apply id="S3.Ex1.m1.4.4.2.2.2.2.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2"><times id="S3.Ex1.m1.4.4.2.2.2.2.2.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.2"></times><ci id="S3.Ex1.m1.4.4.2.2.2.2.3.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.3">𝐷</ci><apply id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1"><times id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.2"></times><ci id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.3.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.3">𝐸</ci><apply id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.4.4.2.2.2.2.1.1.1.1.1.1.3">S</ci></apply></apply></apply></interval></apply><plus id="S3.Ex1.m1.4.4.4.cmml" xref="S3.Ex1.m1.4.4.4"></plus></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.4c">\displaystyle\underset{E,D}{\min}\&gt;\mathrm{CE}(\bm{i}(\bm{x}_{\mathrm{S}}),D(E(\bm{x}_{\mathrm{S}})))+</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.Ex1.m2.2" class="ltx_Math" alttext="\displaystyle\lambda\,\mathrm{BCE}(0.5,C(E(\bm{x}_{\mathrm{S}})))" display="inline"><semantics id="S3.Ex1.m2.2a"><mrow id="S3.Ex1.m2.2.2" xref="S3.Ex1.m2.2.2.cmml"><mi id="S3.Ex1.m2.2.2.3" xref="S3.Ex1.m2.2.2.3.cmml">λ</mi><mo lspace="0.170em" rspace="0em" id="S3.Ex1.m2.2.2.2" xref="S3.Ex1.m2.2.2.2.cmml">​</mo><mi id="S3.Ex1.m2.2.2.4" xref="S3.Ex1.m2.2.2.4.cmml">BCE</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.2.2.2a" xref="S3.Ex1.m2.2.2.2.cmml">​</mo><mrow id="S3.Ex1.m2.2.2.1.1" xref="S3.Ex1.m2.2.2.1.2.cmml"><mo stretchy="false" id="S3.Ex1.m2.2.2.1.1.2" xref="S3.Ex1.m2.2.2.1.2.cmml">(</mo><mn id="S3.Ex1.m2.1.1" xref="S3.Ex1.m2.1.1.cmml">0.5</mn><mo id="S3.Ex1.m2.2.2.1.1.3" xref="S3.Ex1.m2.2.2.1.2.cmml">,</mo><mrow id="S3.Ex1.m2.2.2.1.1.1" xref="S3.Ex1.m2.2.2.1.1.1.cmml"><mi id="S3.Ex1.m2.2.2.1.1.1.3" xref="S3.Ex1.m2.2.2.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.2.2.1.1.1.2" xref="S3.Ex1.m2.2.2.1.1.1.2.cmml">​</mo><mrow id="S3.Ex1.m2.2.2.1.1.1.1.1" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m2.2.2.1.1.1.1.1.2" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m2.2.2.1.1.1.1.1.1" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m2.2.2.1.1.1.1.1.1.3" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.Ex1.m2.2.2.1.1.1.1.1.1.2" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml">S</mi></msub><mo stretchy="false" id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex1.m2.2.2.1.1.1.1.1.3" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.Ex1.m2.2.2.1.1.4" xref="S3.Ex1.m2.2.2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m2.2b"><apply id="S3.Ex1.m2.2.2.cmml" xref="S3.Ex1.m2.2.2"><times id="S3.Ex1.m2.2.2.2.cmml" xref="S3.Ex1.m2.2.2.2"></times><ci id="S3.Ex1.m2.2.2.3.cmml" xref="S3.Ex1.m2.2.2.3">𝜆</ci><ci id="S3.Ex1.m2.2.2.4.cmml" xref="S3.Ex1.m2.2.2.4">BCE</ci><interval closure="open" id="S3.Ex1.m2.2.2.1.2.cmml" xref="S3.Ex1.m2.2.2.1.1"><cn type="float" id="S3.Ex1.m2.1.1.cmml" xref="S3.Ex1.m2.1.1">0.5</cn><apply id="S3.Ex1.m2.2.2.1.1.1.cmml" xref="S3.Ex1.m2.2.2.1.1.1"><times id="S3.Ex1.m2.2.2.1.1.1.2.cmml" xref="S3.Ex1.m2.2.2.1.1.1.2"></times><ci id="S3.Ex1.m2.2.2.1.1.1.3.cmml" xref="S3.Ex1.m2.2.2.1.1.1.3">𝐶</ci><apply id="S3.Ex1.m2.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.2.2.1.1.1.1.1"><times id="S3.Ex1.m2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.2"></times><ci id="S3.Ex1.m2.2.2.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.3">𝐸</ci><apply id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m2.2.2.1.1.1.1.1.1.1.1.1.3">S</ci></apply></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m2.2c">\displaystyle\lambda\,\mathrm{BCE}(0.5,C(E(\bm{x}_{\mathrm{S}})))</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2.m1.1" class="ltx_Math" alttext="\displaystyle+" display="inline"><semantics id="S3.E2.m1.1a"><mo id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">+</mo><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><plus id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"></plus></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle+</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math id="S3.E2.m2.2" class="ltx_Math" alttext="\displaystyle\lambda\,\mathrm{BCE}(0.5,C(E(\bm{x}_{\mathrm{R}})))," display="inline"><semantics id="S3.E2.m2.2a"><mrow id="S3.E2.m2.2.2.1" xref="S3.E2.m2.2.2.1.1.cmml"><mrow id="S3.E2.m2.2.2.1.1" xref="S3.E2.m2.2.2.1.1.cmml"><mi id="S3.E2.m2.2.2.1.1.3" xref="S3.E2.m2.2.2.1.1.3.cmml">λ</mi><mo lspace="0.170em" rspace="0em" id="S3.E2.m2.2.2.1.1.2" xref="S3.E2.m2.2.2.1.1.2.cmml">​</mo><mi id="S3.E2.m2.2.2.1.1.4" xref="S3.E2.m2.2.2.1.1.4.cmml">BCE</mi><mo lspace="0em" rspace="0em" id="S3.E2.m2.2.2.1.1.2a" xref="S3.E2.m2.2.2.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.2.2.1.1.1.1" xref="S3.E2.m2.2.2.1.1.1.2.cmml"><mo stretchy="false" id="S3.E2.m2.2.2.1.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.2.cmml">(</mo><mn id="S3.E2.m2.1.1" xref="S3.E2.m2.1.1.cmml">0.5</mn><mo id="S3.E2.m2.2.2.1.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.2.cmml">,</mo><mrow id="S3.E2.m2.2.2.1.1.1.1.1" xref="S3.E2.m2.2.2.1.1.1.1.1.cmml"><mi id="S3.E2.m2.2.2.1.1.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.1.1.3.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E2.m2.2.2.1.1.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.2.2.1.1.1.1.1.1.1" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m2.2.2.1.1.1.1.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.3.cmml">E</mi><mo lspace="0em" rspace="0em" id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml">R</mi></msub><mo stretchy="false" id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m2.2.2.1.1.1.1.1.1.1.3" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S3.E2.m2.2.2.1.1.1.1.4" xref="S3.E2.m2.2.2.1.1.1.2.cmml">)</mo></mrow></mrow><mo id="S3.E2.m2.2.2.1.2" xref="S3.E2.m2.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m2.2b"><apply id="S3.E2.m2.2.2.1.1.cmml" xref="S3.E2.m2.2.2.1"><times id="S3.E2.m2.2.2.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.2"></times><ci id="S3.E2.m2.2.2.1.1.3.cmml" xref="S3.E2.m2.2.2.1.1.3">𝜆</ci><ci id="S3.E2.m2.2.2.1.1.4.cmml" xref="S3.E2.m2.2.2.1.1.4">BCE</ci><interval closure="open" id="S3.E2.m2.2.2.1.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.1.1"><cn type="float" id="S3.E2.m2.1.1.cmml" xref="S3.E2.m2.1.1">0.5</cn><apply id="S3.E2.m2.2.2.1.1.1.1.1.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1"><times id="S3.E2.m2.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.2"></times><ci id="S3.E2.m2.2.2.1.1.1.1.1.3.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.3">𝐶</ci><apply id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1"><times id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.2"></times><ci id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.3">𝐸</ci><apply id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.2">𝒙</ci><ci id="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m2.2.2.1.1.1.1.1.1.1.1.1.1.1.3">R</ci></apply></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m2.2c">\displaystyle\lambda\,\mathrm{BCE}(0.5,C(E(\bm{x}_{\mathrm{R}}))),</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p id="S3.SS2.p5.15" class="ltx_p">where <math id="S3.SS2.p5.1.m1.1" class="ltx_Math" alttext="\mathrm{CE}" display="inline"><semantics id="S3.SS2.p5.1.m1.1a"><mi id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml">CE</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><ci id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">CE</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">\mathrm{CE}</annotation></semantics></math> denotes cross entropy loss and <math id="S3.SS2.p5.2.m2.1" class="ltx_Math" alttext="\mathrm{BCE}" display="inline"><semantics id="S3.SS2.p5.2.m2.1a"><mi id="S3.SS2.p5.2.m2.1.1" xref="S3.SS2.p5.2.m2.1.1.cmml">BCE</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.2.m2.1b"><ci id="S3.SS2.p5.2.m2.1.1.cmml" xref="S3.SS2.p5.2.m2.1.1">BCE</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.2.m2.1c">\mathrm{BCE}</annotation></semantics></math> denotes binary cross entropy loss. <math id="S3.SS2.p5.3.m3.1" class="ltx_Math" alttext="\bm{x}_{\mathrm{S}}" display="inline"><semantics id="S3.SS2.p5.3.m3.1a"><msub id="S3.SS2.p5.3.m3.1.1" xref="S3.SS2.p5.3.m3.1.1.cmml"><mi id="S3.SS2.p5.3.m3.1.1.2" xref="S3.SS2.p5.3.m3.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.SS2.p5.3.m3.1.1.3" xref="S3.SS2.p5.3.m3.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.3.m3.1b"><apply id="S3.SS2.p5.3.m3.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.3.m3.1.1.1.cmml" xref="S3.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p5.3.m3.1.1.2.cmml" xref="S3.SS2.p5.3.m3.1.1.2">𝒙</ci><ci id="S3.SS2.p5.3.m3.1.1.3.cmml" xref="S3.SS2.p5.3.m3.1.1.3">S</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.3.m3.1c">\bm{x}_{\mathrm{S}}</annotation></semantics></math> and <math id="S3.SS2.p5.4.m4.1" class="ltx_Math" alttext="\bm{x}_{\mathrm{R}}" display="inline"><semantics id="S3.SS2.p5.4.m4.1a"><msub id="S3.SS2.p5.4.m4.1.1" xref="S3.SS2.p5.4.m4.1.1.cmml"><mi id="S3.SS2.p5.4.m4.1.1.2" xref="S3.SS2.p5.4.m4.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.SS2.p5.4.m4.1.1.3" xref="S3.SS2.p5.4.m4.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.4.m4.1b"><apply id="S3.SS2.p5.4.m4.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.4.m4.1.1.1.cmml" xref="S3.SS2.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p5.4.m4.1.1.2.cmml" xref="S3.SS2.p5.4.m4.1.1.2">𝒙</ci><ci id="S3.SS2.p5.4.m4.1.1.3.cmml" xref="S3.SS2.p5.4.m4.1.1.3">R</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.4.m4.1c">\bm{x}_{\mathrm{R}}</annotation></semantics></math> are the input mel spectrograms of synthetic and real data, respectively. <math id="S3.SS2.p5.5.m5.1" class="ltx_Math" alttext="E" display="inline"><semantics id="S3.SS2.p5.5.m5.1a"><mi id="S3.SS2.p5.5.m5.1.1" xref="S3.SS2.p5.5.m5.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.5.m5.1b"><ci id="S3.SS2.p5.5.m5.1.1.cmml" xref="S3.SS2.p5.5.m5.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.5.m5.1c">E</annotation></semantics></math>, <math id="S3.SS2.p5.6.m6.1" class="ltx_Math" alttext="D" display="inline"><semantics id="S3.SS2.p5.6.m6.1a"><mi id="S3.SS2.p5.6.m6.1.1" xref="S3.SS2.p5.6.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.6.m6.1b"><ci id="S3.SS2.p5.6.m6.1.1.cmml" xref="S3.SS2.p5.6.m6.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.6.m6.1c">D</annotation></semantics></math>, and <math id="S3.SS2.p5.7.m7.1" class="ltx_Math" alttext="C" display="inline"><semantics id="S3.SS2.p5.7.m7.1a"><mi id="S3.SS2.p5.7.m7.1.1" xref="S3.SS2.p5.7.m7.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.7.m7.1b"><ci id="S3.SS2.p5.7.m7.1.1.cmml" xref="S3.SS2.p5.7.m7.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.7.m7.1c">C</annotation></semantics></math> are encoder, decoder, and discriminator, respectively. <math id="S3.SS2.p5.8.m8.1" class="ltx_Math" alttext="\bm{i}(\bm{x}_{\mathrm{S}})" display="inline"><semantics id="S3.SS2.p5.8.m8.1a"><mrow id="S3.SS2.p5.8.m8.1.1" xref="S3.SS2.p5.8.m8.1.1.cmml"><mi id="S3.SS2.p5.8.m8.1.1.3" xref="S3.SS2.p5.8.m8.1.1.3.cmml">𝒊</mi><mo lspace="0em" rspace="0em" id="S3.SS2.p5.8.m8.1.1.2" xref="S3.SS2.p5.8.m8.1.1.2.cmml">​</mo><mrow id="S3.SS2.p5.8.m8.1.1.1.1" xref="S3.SS2.p5.8.m8.1.1.1.1.1.cmml"><mo stretchy="false" id="S3.SS2.p5.8.m8.1.1.1.1.2" xref="S3.SS2.p5.8.m8.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p5.8.m8.1.1.1.1.1" xref="S3.SS2.p5.8.m8.1.1.1.1.1.cmml"><mi id="S3.SS2.p5.8.m8.1.1.1.1.1.2" xref="S3.SS2.p5.8.m8.1.1.1.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.SS2.p5.8.m8.1.1.1.1.1.3" xref="S3.SS2.p5.8.m8.1.1.1.1.1.3.cmml">S</mi></msub><mo stretchy="false" id="S3.SS2.p5.8.m8.1.1.1.1.3" xref="S3.SS2.p5.8.m8.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.8.m8.1b"><apply id="S3.SS2.p5.8.m8.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1"><times id="S3.SS2.p5.8.m8.1.1.2.cmml" xref="S3.SS2.p5.8.m8.1.1.2"></times><ci id="S3.SS2.p5.8.m8.1.1.3.cmml" xref="S3.SS2.p5.8.m8.1.1.3">𝒊</ci><apply id="S3.SS2.p5.8.m8.1.1.1.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.8.m8.1.1.1.1.1.1.cmml" xref="S3.SS2.p5.8.m8.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p5.8.m8.1.1.1.1.1.2.cmml" xref="S3.SS2.p5.8.m8.1.1.1.1.1.2">𝒙</ci><ci id="S3.SS2.p5.8.m8.1.1.1.1.1.3.cmml" xref="S3.SS2.p5.8.m8.1.1.1.1.1.3">S</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.8.m8.1c">\bm{i}(\bm{x}_{\mathrm{S}})</annotation></semantics></math> is the one-hot vector indicating ground truth MIDI tokens of input <math id="S3.SS2.p5.9.m9.1" class="ltx_Math" alttext="\bm{x}_{\mathrm{S}}" display="inline"><semantics id="S3.SS2.p5.9.m9.1a"><msub id="S3.SS2.p5.9.m9.1.1" xref="S3.SS2.p5.9.m9.1.1.cmml"><mi id="S3.SS2.p5.9.m9.1.1.2" xref="S3.SS2.p5.9.m9.1.1.2.cmml">𝒙</mi><mi mathvariant="normal" id="S3.SS2.p5.9.m9.1.1.3" xref="S3.SS2.p5.9.m9.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.9.m9.1b"><apply id="S3.SS2.p5.9.m9.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.9.m9.1.1.1.cmml" xref="S3.SS2.p5.9.m9.1.1">subscript</csymbol><ci id="S3.SS2.p5.9.m9.1.1.2.cmml" xref="S3.SS2.p5.9.m9.1.1.2">𝒙</ci><ci id="S3.SS2.p5.9.m9.1.1.3.cmml" xref="S3.SS2.p5.9.m9.1.1.3">S</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.9.m9.1c">\bm{x}_{\mathrm{S}}</annotation></semantics></math>, and <math id="S3.SS2.p5.10.m10.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS2.p5.10.m10.1a"><mn id="S3.SS2.p5.10.m10.1.1" xref="S3.SS2.p5.10.m10.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.10.m10.1b"><cn type="integer" id="S3.SS2.p5.10.m10.1.1.cmml" xref="S3.SS2.p5.10.m10.1.1">0</cn></annotation-xml></semantics></math> and <math id="S3.SS2.p5.11.m11.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p5.11.m11.1a"><mn id="S3.SS2.p5.11.m11.1.1" xref="S3.SS2.p5.11.m11.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.11.m11.1b"><cn type="integer" id="S3.SS2.p5.11.m11.1.1.cmml" xref="S3.SS2.p5.11.m11.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.11.m11.1c">1</annotation></semantics></math> are the ground truth labels of the input class (<math id="S3.SS2.p5.12.m12.1" class="ltx_Math" alttext="0" display="inline"><semantics id="S3.SS2.p5.12.m12.1a"><mn id="S3.SS2.p5.12.m12.1.1" xref="S3.SS2.p5.12.m12.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.12.m12.1b"><cn type="integer" id="S3.SS2.p5.12.m12.1.1.cmml" xref="S3.SS2.p5.12.m12.1.1">0</cn></annotation-xml></semantics></math> for synthetic and <math id="S3.SS2.p5.13.m13.1" class="ltx_Math" alttext="1" display="inline"><semantics id="S3.SS2.p5.13.m13.1a"><mn id="S3.SS2.p5.13.m13.1.1" xref="S3.SS2.p5.13.m13.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.13.m13.1b"><cn type="integer" id="S3.SS2.p5.13.m13.1.1.cmml" xref="S3.SS2.p5.13.m13.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.13.m13.1c">1</annotation></semantics></math> for real). <math id="S3.SS2.p5.14.m14.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p5.14.m14.1a"><mi id="S3.SS2.p5.14.m14.1.1" xref="S3.SS2.p5.14.m14.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.14.m14.1b"><ci id="S3.SS2.p5.14.m14.1.1.cmml" xref="S3.SS2.p5.14.m14.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.14.m14.1c">\lambda</annotation></semantics></math> balances the transcription loss and the domain confusion loss. The value of <math id="S3.SS2.p5.15.m15.1" class="ltx_Math" alttext="\lambda" display="inline"><semantics id="S3.SS2.p5.15.m15.1a"><mi id="S3.SS2.p5.15.m15.1.1" xref="S3.SS2.p5.15.m15.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.15.m15.1b"><ci id="S3.SS2.p5.15.m15.1.1.cmml" xref="S3.SS2.p5.15.m15.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.15.m15.1c">\lambda</annotation></semantics></math> is set to 0.01.</p>
</div>
<div id="S3.SS2.p6" class="ltx_para">
<p id="S3.SS2.p6.1" class="ltx_p">The classification loss Eq. (<a href="#S3.E1" title="In 3.2 Transcription architecture ‣ 3 Model ‣ Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) updates the discriminator’s parameters to improve the discrimination performance between synthetic and real data.
The first term in Eq. (<a href="#S3.E2" title="In 3.2 Transcription architecture ‣ 3 Model ‣ Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), the transcription loss, is associated with transcription performance.
The second and third terms in Eq. (<a href="#S3.E2" title="In 3.2 Transcription architecture ‣ 3 Model ‣ Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) are the adversarial losses that perform domain confusion to make the output of the discriminator closer to 0.5.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiment</h2>

<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.2" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.2.1.1" class="ltx_tr">
<td id="S4.T1.2.1.1.1" class="ltx_td ltx_border_l ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3">Real Data</td>
<td id="S4.T1.2.1.1.3" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" colspan="2">Our Synth Data</td>
<td id="S4.T1.2.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Guitarset</td>
<td id="S4.T1.2.1.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Maestro</td>
<td id="S4.T1.2.1.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Molina</td>
<td id="S4.T1.2.1.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Phenicx</td>
<td id="S4.T1.2.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Slakh</td>
</tr>
<tr id="S4.T1.2.2.2" class="ltx_tr">
<td id="S4.T1.2.2.2.1" class="ltx_td ltx_border_l ltx_border_r"></td>
<td id="S4.T1.2.2.2.2" class="ltx_td ltx_align_center ltx_border_r">a</td>
<td id="S4.T1.2.2.2.3" class="ltx_td ltx_align_center ltx_border_r">b</td>
<td id="S4.T1.2.2.2.4" class="ltx_td ltx_align_center ltx_border_r">c</td>
<td id="S4.T1.2.2.2.5" class="ltx_td ltx_align_center ltx_border_r">MIDI</td>
<td id="S4.T1.2.2.2.6" class="ltx_td ltx_align_center ltx_border_rr">Timbre</td>
<td id="S4.T1.2.2.2.7" class="ltx_td ltx_align_center ltx_border_r">Fn  Ac  F</td>
<td id="S4.T1.2.2.2.8" class="ltx_td ltx_align_center ltx_border_r">Fn  Ac  F</td>
<td id="S4.T1.2.2.2.9" class="ltx_td ltx_align_center ltx_border_r">Fn  Ac  F</td>
<td id="S4.T1.2.2.2.10" class="ltx_td ltx_align_center ltx_border_r">Fn  Ac  F</td>
<td id="S4.T1.2.2.2.11" class="ltx_td ltx_align_center ltx_border_r">Fn  Ac  F</td>
</tr>
<tr id="S4.T1.2.3.3" class="ltx_tr">
<td id="S4.T1.2.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">Bittner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>
</td>
<td id="S4.T1.2.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✓</td>
<td id="S4.T1.2.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✓</td>
<td id="S4.T1.2.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✓</td>
<td id="S4.T1.2.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">—</td>
<td id="S4.T1.2.3.3.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">—</td>
<td id="S4.T1.2.3.3.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.79 .70 .56</td>
<td id="S4.T1.2.3.3.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.71 .38 .11</td>
<td id="S4.T1.2.3.3.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#CCFFCC;"><span id="S4.T1.2.3.3.9.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFCC;">.55<span id="S4.T1.2.3.3.9.1.1" class="ltx_text ltx_font_medium"> .60 </span>.38</span></td>
<td id="S4.T1.2.3.3.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#CCFFCC;"><span id="S4.T1.2.3.3.10.1" class="ltx_text" style="background-color:#CCFFCC;">.51 .50 <span id="S4.T1.2.3.3.10.1.1" class="ltx_text ltx_font_bold">.36</span></span></td>
<td id="S4.T1.2.3.3.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">.43 .40 .23</td>
</tr>
<tr id="S4.T1.2.4.4" class="ltx_tr">
<td id="S4.T1.2.4.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>
</td>
<td id="S4.T1.2.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.4.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.4.4.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">—</td>
<td id="S4.T1.2.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.4.4.7.1" class="ltx_text" style="background-color:#CCFFCC;">.59 .43 .27</span></td>
<td id="S4.T1.2.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.30 .39 .07</td>
<td id="S4.T1.2.4.4.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.4.4.9.1" class="ltx_text" style="background-color:#CCFFCC;">.31 .48 .11</span></td>
<td id="S4.T1.2.4.4.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.4.4.10.1" class="ltx_text" style="background-color:#CCFFCC;">.12 .13 .05</span></td>
<td id="S4.T1.2.4.4.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.4.4.11.1" class="ltx_text" style="background-color:#CCFFCC;">.23 .40 .07</span></td>
</tr>
<tr id="S4.T1.2.5.5" class="ltx_tr">
<td id="S4.T1.2.5.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Gardner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>
</td>
<td id="S4.T1.2.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.5.5.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">—</td>
<td id="S4.T1.2.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.5.5.7.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFCC;">.78<span id="S4.T1.2.5.5.7.1.1" class="ltx_text ltx_font_medium">  —  —</span></span></td>
<td id="S4.T1.2.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.5.5.8.1" class="ltx_text" style="background-color:#CCFFCC;">.28  —  —</span></td>
<td id="S4.T1.2.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.5.5.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.5.5.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.5.5.11.1" class="ltx_text" style="background-color:#CCFFCC;">.14* —  —</span></td>
</tr>
<tr id="S4.T1.2.6.6" class="ltx_tr">
<td id="S4.T1.2.6.6.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Simon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>
</td>
<td id="S4.T1.2.6.6.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.6.6.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.6.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.6.6.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.6.6.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">—</td>
<td id="S4.T1.2.6.6.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.6.6.7.1" class="ltx_text" style="background-color:#CCFFCC;">.71  —  —</span></td>
<td id="S4.T1.2.6.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.6.6.8.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFCC;">.83<span id="S4.T1.2.6.6.8.1.1" class="ltx_text ltx_font_medium">  —  —</span></span></td>
<td id="S4.T1.2.6.6.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.6.6.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.6.6.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.6.6.11.1" class="ltx_text" style="background-color:#CCFFCC;">.45* —  —</span></td>
</tr>
<tr id="S4.T1.2.7.7" class="ltx_tr">
<td id="S4.T1.2.7.7.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Real-Mix</td>
<td id="S4.T1.2.7.7.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.7.7.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.7.7.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.7.7.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.7.7.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">—</td>
<td id="S4.T1.2.7.7.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.80 .76 .58</td>
<td id="S4.T1.2.7.7.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.90 .57 .54</td>
<td id="S4.T1.2.7.7.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.78 .78 .58</td>
<td id="S4.T1.2.7.7.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.66 .66 .40</td>
<td id="S4.T1.2.7.7.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">.79 .79 .60</td>
</tr>
<tr id="S4.T1.2.8.8" class="ltx_tr">
<td id="S4.T1.2.8.8.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Real-Omit</td>
<td id="S4.T1.2.8.8.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.8.8.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.8.8.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.8.8.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">—</td>
<td id="S4.T1.2.8.8.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">—</td>
<td id="S4.T1.2.8.8.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.8.8.7.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFCC;">.78<span id="S4.T1.2.8.8.7.1.1" class="ltx_text ltx_font_medium"> .64 </span>.43</span></td>
<td id="S4.T1.2.8.8.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.8.8.8.1" class="ltx_text" style="background-color:#CCFFCC;">.51 .47 <span id="S4.T1.2.8.8.8.1.1" class="ltx_text ltx_framed ltx_framed_underline">.22</span></span></td>
<td id="S4.T1.2.8.8.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.8.8.9.1" class="ltx_text" style="background-color:#CCFFCC;">.26 .66 .13</span></td>
<td id="S4.T1.2.8.8.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.8.8.10.1" class="ltx_text" style="background-color:#CCFFCC;">.61 .51 .17</span></td>
<td id="S4.T1.2.8.8.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.8.8.11.1" class="ltx_text" style="background-color:#CCFFCC;">.35 .42 .13</span></td>
</tr>
<tr id="S4.T1.2.9.9" class="ltx_tr">
<td id="S4.T1.2.9.9.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt">Synthetic-DC</td>
<td id="S4.T1.2.9.9.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">✓</td>
<td id="S4.T1.2.9.9.3" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T1.2.9.9.4" class="ltx_td ltx_border_r ltx_border_tt"></td>
<td id="S4.T1.2.9.9.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt">126K</td>
<td id="S4.T1.2.9.9.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">313K+</td>
<td id="S4.T1.2.9.9.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#CCFFCC;">
<span id="S4.T1.2.9.9.7.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#CCFFCC;">.77</span><span id="S4.T1.2.9.9.7.2" class="ltx_text" style="background-color:#CCFFCC;"> <span id="S4.T1.2.9.9.7.2.1" class="ltx_text ltx_font_bold">.67</span> <span id="S4.T1.2.9.9.7.2.2" class="ltx_text ltx_framed ltx_framed_underline">.40</span></span>
</td>
<td id="S4.T1.2.9.9.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#CCFFCC;">
<span id="S4.T1.2.9.9.8.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#CCFFCC;">.57</span><span id="S4.T1.2.9.9.8.2" class="ltx_text" style="background-color:#CCFFCC;"> <span id="S4.T1.2.9.9.8.2.1" class="ltx_text ltx_font_bold">.50</span> <span id="S4.T1.2.9.9.8.2.2" class="ltx_text ltx_font_bold">.23</span></span>
</td>
<td id="S4.T1.2.9.9.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#CCFFCC;"><span id="S4.T1.2.9.9.9.1" class="ltx_text" style="background-color:#CCFFCC;">.50 .66 .24</span></td>
<td id="S4.T1.2.9.9.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#CCFFCC;"><span id="S4.T1.2.9.9.10.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFCC;">.74<span id="S4.T1.2.9.9.10.1.1" class="ltx_text ltx_font_medium"> .61 <span id="S4.T1.2.9.9.10.1.1.1" class="ltx_text ltx_framed ltx_framed_underline">.32</span></span></span></td>
<td id="S4.T1.2.9.9.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" style="background-color:#CCFFCC;"><span id="S4.T1.2.9.9.11.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFCC;">.57<span id="S4.T1.2.9.9.11.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T1.2.9.9.11.1.1.1" class="ltx_text ltx_framed ltx_framed_underline">.57</span> </span>.31</span></td>
</tr>
<tr id="S4.T1.2.10.10" class="ltx_tr">
<td id="S4.T1.2.10.10.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Synthetic-DA</td>
<td id="S4.T1.2.10.10.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">✓</td>
<td id="S4.T1.2.10.10.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.10.10.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.10.10.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">126K</td>
<td id="S4.T1.2.10.10.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">313K+</td>
<td id="S4.T1.2.10.10.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.10.10.7.1" class="ltx_text" style="background-color:#CCFFCC;">.64 .60 .27</span></td>
<td id="S4.T1.2.10.10.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.10.10.8.1" class="ltx_text" style="background-color:#CCFFCC;">.50 <span id="S4.T1.2.10.10.8.1.1" class="ltx_text ltx_framed ltx_framed_underline">.49</span> .20</span></td>
<td id="S4.T1.2.10.10.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.10.10.9.1" class="ltx_text" style="background-color:#CCFFCC;">.47 .67 .23</span></td>
<td id="S4.T1.2.10.10.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.10.10.10.1" class="ltx_text" style="background-color:#CCFFCC;">.64 <span id="S4.T1.2.10.10.10.1.1" class="ltx_text ltx_framed ltx_framed_underline">.63</span> .28</span></td>
<td id="S4.T1.2.10.10.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.10.10.11.1" class="ltx_text" style="background-color:#CCFFCC;">.54 .54 .28</span></td>
</tr>
<tr id="S4.T1.2.11.11" class="ltx_tr">
<td id="S4.T1.2.11.11.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Synthetic-L</td>
<td id="S4.T1.2.11.11.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.11.11.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.11.11.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.11.11.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">126K</td>
<td id="S4.T1.2.11.11.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">313K+</td>
<td id="S4.T1.2.11.11.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.11.11.7.1" class="ltx_text" style="background-color:#CCFFCC;">.73 <span id="S4.T1.2.11.11.7.1.1" class="ltx_text ltx_framed ltx_framed_underline">.66</span> .38</span></td>
<td id="S4.T1.2.11.11.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.11.11.8.1" class="ltx_text" style="background-color:#CCFFCC;">.54 <span id="S4.T1.2.11.11.8.1.1" class="ltx_text ltx_framed ltx_framed_underline">.49</span> <span id="S4.T1.2.11.11.8.1.2" class="ltx_text ltx_framed ltx_framed_underline">.22</span></span></td>
<td id="S4.T1.2.11.11.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;">
<span id="S4.T1.2.11.11.9.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#CCFFCC;">.52</span><span id="S4.T1.2.11.11.9.2" class="ltx_text" style="background-color:#CCFFCC;"> .67 <span id="S4.T1.2.11.11.9.2.1" class="ltx_text ltx_framed ltx_framed_underline">.25</span></span>
</td>
<td id="S4.T1.2.11.11.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;">
<span id="S4.T1.2.11.11.10.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#CCFFCC;">.72</span><span id="S4.T1.2.11.11.10.2" class="ltx_text" style="background-color:#CCFFCC;"> <span id="S4.T1.2.11.11.10.2.1" class="ltx_text ltx_font_bold">.66</span> <span id="S4.T1.2.11.11.10.2.2" class="ltx_text ltx_font_bold">.36</span></span>
</td>
<td id="S4.T1.2.11.11.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;">
<span id="S4.T1.2.11.11.11.1" class="ltx_text ltx_framed ltx_framed_underline" style="background-color:#CCFFCC;">.56</span><span id="S4.T1.2.11.11.11.2" class="ltx_text" style="background-color:#CCFFCC;"> .55 <span id="S4.T1.2.11.11.11.2.1" class="ltx_text ltx_framed ltx_framed_underline">.30</span></span>
</td>
</tr>
<tr id="S4.T1.2.12.12" class="ltx_tr">
<td id="S4.T1.2.12.12.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Synthetic-M</td>
<td id="S4.T1.2.12.12.2" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.12.12.3" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.12.12.4" class="ltx_td ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.12.12.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">126K</td>
<td id="S4.T1.2.12.12.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">792</td>
<td id="S4.T1.2.12.12.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.12.12.7.1" class="ltx_text" style="background-color:#CCFFCC;">.71 .65 .35</span></td>
<td id="S4.T1.2.12.12.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.12.12.8.1" class="ltx_text" style="background-color:#CCFFCC;">.55 .47 .21</span></td>
<td id="S4.T1.2.12.12.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.12.12.9.1" class="ltx_text" style="background-color:#CCFFCC;">.43 <span id="S4.T1.2.12.12.9.1.1" class="ltx_text ltx_framed ltx_framed_underline">.68</span> .20</span></td>
<td id="S4.T1.2.12.12.10" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.12.12.10.1" class="ltx_text" style="background-color:#CCFFCC;">.70 .60 .25</span></td>
<td id="S4.T1.2.12.12.11" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.12.12.11.1" class="ltx_text ltx_font_bold" style="background-color:#CCFFCC;">.57<span id="S4.T1.2.12.12.11.1.1" class="ltx_text ltx_font_medium"> <span id="S4.T1.2.12.12.11.1.1.1" class="ltx_text ltx_framed ltx_framed_underline">.57</span> </span>.31</span></td>
</tr>
<tr id="S4.T1.2.13.13" class="ltx_tr">
<td id="S4.T1.2.13.13.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">Synthetic-S</td>
<td id="S4.T1.2.13.13.2" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.13.13.3" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.13.13.4" class="ltx_td ltx_border_b ltx_border_r ltx_border_t"></td>
<td id="S4.T1.2.13.13.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">13K</td>
<td id="S4.T1.2.13.13.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t">792</td>
<td id="S4.T1.2.13.13.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.13.13.7.1" class="ltx_text" style="background-color:#CCFFCC;">.65 .63 .29</span></td>
<td id="S4.T1.2.13.13.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.13.13.8.1" class="ltx_text" style="background-color:#CCFFCC;">.40 .41 .11</span></td>
<td id="S4.T1.2.13.13.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.13.13.9.1" class="ltx_text" style="background-color:#CCFFCC;">.45 <span id="S4.T1.2.13.13.9.1.1" class="ltx_text ltx_font_bold">.69</span> .20</span></td>
<td id="S4.T1.2.13.13.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.13.13.10.1" class="ltx_text" style="background-color:#CCFFCC;">.68 .61 .30</span></td>
<td id="S4.T1.2.13.13.11" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" style="background-color:#CCFFCC;"><span id="S4.T1.2.13.13.11.1" class="ltx_text" style="background-color:#CCFFCC;">.56 <span id="S4.T1.2.13.13.11.1.1" class="ltx_text ltx_font_bold">.61</span> .29</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S4.T1.10.1.1" class="ltx_text ltx_font_bold">Table 1</span>: </span>Results of the experiment.
Scores highlighted in green pertain to the target-annotation-free setting, where the MIDI annotation of the target data domain for evaluation is not used for training to test the generalization capability. Among the scores under the target-annotation-free setting, we mark the best result with <span id="S4.T1.11.2" class="ltx_text ltx_font_bold">boldface</span> and the
second-best result by <span id="S4.T1.12.3" class="ltx_text ltx_framed ltx_framed_underline">underlining</span>. Our proposed method is <span id="S4.T1.13.4" class="ltx_text ltx_font_italic">Synthetic-DC</span>. <span id="S4.T1.14.5" class="ltx_text ltx_font_italic">Synthetic-DA</span>, <span id="S4.T1.15.6" class="ltx_text ltx_font_italic">L</span>, <span id="S4.T1.16.7" class="ltx_text ltx_font_italic">M</span>, and <span id="S4.T1.17.8" class="ltx_text ltx_font_italic">S</span> constitute an ablation study of the components integral to our proposed method, examining their individual and collective contributions to the model’s performance.
‘Real Data’ ‘a’, ‘b’, and ‘c’ each indicate whether the models are trained using real audio, real audio with MIDI annotation, and the extra real audio with MIDI annotation, respectively.
The asterisk (*) indicates the evaluation results for multitrack audio.
Our proposed method performs well under the target-annotation-free setting. The performance of synthetic methods tends to improve as the number of MIDI and audio variations used in the model increases.</figcaption>
</figure>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation metrics</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">We used mir-eval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> as the evaluation code, and three evaluation metrics were used. The first metric is the note-level F measure (<span id="S4.SS1.p1.1.1" class="ltx_text ltx_font_italic">F</span>), where a note is deemed correct if its pitch is correct, the onset error is within 50 ms, and the offset error is within 50 ms or 20 percent of the note length. The second metric is note-level F measure no offset (<span id="S4.SS1.p1.1.2" class="ltx_text ltx_font_italic">Fn</span>), evaluated similarly to F, but ignores offsets. The third metric is frame level accuracy (<span id="S4.SS1.p1.1.3" class="ltx_text ltx_font_italic">Ac</span>).
Given the inherent challenges in offset estimation, this study primarily focuses on <span id="S4.SS1.p1.1.4" class="ltx_text ltx_font_italic">Fn</span>, following <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Dataset</h3>

<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p">The NSynth dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> and Lakh MIDI dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> is used to synthesize the audio. The NSynth dataset contains large amount of 4 seconds one-shot samples with 11 instrument types, consisting of 1,006 different timbres, all collected from commercial sample libraries.
The Lakh MIDI dataset is a large MIDI dataset of 176,581 tracks collected from the Internet. We use the largest version, LMD-full, while omitting drums and percussion tracks</p>
</div>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p">For evaluation purposes, we use Guitarset (guitar) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, Maestro (piano) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, Molina (vocal) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>, Phenicx (orchestra) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>]</cite>, and Slakh (mixture) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite>.
In the case of Phenicx, we use instrumental grouped stems and midi. Note that Lakh MIDI dataset contains MIDI data for Slakh, but still no MIDI-audio pair in Slakh is used for our proposed synthetic method.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Baselines</h3>

<div id="S4.SS3.p1" class="ltx_para">
<p id="S4.SS3.p1.3" class="ltx_p">Bittner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, Gardner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, and Simon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> are selected as the state-of-the-art baselines, which perform the target-annotation-free setting where training datasets do not include the MIDI annotation of audio in the target data domain. Bittner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> employed the same datasets for evaluation as ours, and the result for Phenicx is under the target-annotation-free setting.
While they also did not use the Molina (vocal) dataset for training, they instead utilized the iKala (vocal) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite> dataset.
Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite> trained the model with Maestro and MusicNet <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> dataset, so the results are target-annotation-free setting except for Maestro.
Gardner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> and Simon et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> demonstrated results in the target-annotation-free setting for Guitarset, Maestro, and Slakh. We only show <span id="S4.SS3.p1.3.1" class="ltx_text ltx_font_italic">Fn</span> scores for them because they did not use <span id="S4.SS3.p1.3.2" class="ltx_text ltx_font_italic">F</span> and <span id="S4.SS3.p1.3.3" class="ltx_text ltx_font_italic">Ac</span> for evaluation.
To demonstrate the effectiveness of our method with synthetic audio, we present the results of combining annotated real-domain datasets using the proposed transformer model. Here we do not apply domain confusion.
<span id="S4.SS3.p1.3.4" class="ltx_text ltx_font_italic">Real-Mix</span> is trained by combining five real audio datasets. <span id="S4.SS3.p1.3.5" class="ltx_text ltx_font_italic">Real-Omit</span> is trained by combining four real audio datasets without one real audio dataset of the target domain to create the target-annotation-free setting.
For dataset combination, we adopt the balanced sampling scheme described in Gardner et al. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. If a dataset <math id="S4.SS3.p1.1.m1.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">i</annotation></semantics></math> has <math id="S4.SS3.p1.2.m2.1" class="ltx_Math" alttext="n_{i}" display="inline"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">n</mi><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">𝑛</ci><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">n_{i}</annotation></semantics></math> examples, we sample examples from that dataset with probability <math id="S4.SS3.p1.3.m3.1" class="ltx_Math" alttext="(n_{i}/\sum_{j}n_{j})^{0.3}" display="inline"><semantics id="S4.SS3.p1.3.m3.1a"><msup id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mrow id="S4.SS3.p1.3.m3.1.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.1.1.cmml"><mo stretchy="false" id="S4.SS3.p1.3.m3.1.1.1.1.2" xref="S4.SS3.p1.3.m3.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS3.p1.3.m3.1.1.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.1.1.cmml"><msub id="S4.SS3.p1.3.m3.1.1.1.1.1.2" xref="S4.SS3.p1.3.m3.1.1.1.1.1.2.cmml"><mi id="S4.SS3.p1.3.m3.1.1.1.1.1.2.2" xref="S4.SS3.p1.3.m3.1.1.1.1.1.2.2.cmml">n</mi><mi id="S4.SS3.p1.3.m3.1.1.1.1.1.2.3" xref="S4.SS3.p1.3.m3.1.1.1.1.1.2.3.cmml">i</mi></msub><mo rspace="0.055em" id="S4.SS3.p1.3.m3.1.1.1.1.1.1" xref="S4.SS3.p1.3.m3.1.1.1.1.1.1.cmml">/</mo><mrow id="S4.SS3.p1.3.m3.1.1.1.1.1.3" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.cmml"><msub id="S4.SS3.p1.3.m3.1.1.1.1.1.3.1" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.cmml"><mo id="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.2" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.2.cmml">∑</mo><mi id="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.3" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.3.cmml">j</mi></msub><msub id="S4.SS3.p1.3.m3.1.1.1.1.1.3.2" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.cmml"><mi id="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.2" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.2.cmml">n</mi><mi id="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.3" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.3.cmml">j</mi></msub></mrow></mrow><mo stretchy="false" id="S4.SS3.p1.3.m3.1.1.1.1.3" xref="S4.SS3.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow><mn id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">0.3</mn></msup><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1">superscript</csymbol><apply id="S4.SS3.p1.3.m3.1.1.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1"><divide id="S4.SS3.p1.3.m3.1.1.1.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.1"></divide><apply id="S4.SS3.p1.3.m3.1.1.1.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.1.1.1.2.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.3.m3.1.1.1.1.1.2.2.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.2.2">𝑛</ci><ci id="S4.SS3.p1.3.m3.1.1.1.1.1.2.3.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S4.SS3.p1.3.m3.1.1.1.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3"><apply id="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.1">subscript</csymbol><sum id="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.2"></sum><ci id="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.1.3">𝑗</ci></apply><apply id="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.1.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.2">subscript</csymbol><ci id="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.2.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.2">𝑛</ci><ci id="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.3.cmml" xref="S4.SS3.p1.3.m3.1.1.1.1.1.3.2.3">𝑗</ci></apply></apply></apply><cn type="float" id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">0.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">(n_{i}/\sum_{j}n_{j})^{0.3}</annotation></semantics></math>.
Although these baselines and <span id="S4.SS3.p1.3.6" class="ltx_text ltx_font_italic">Real-Omit</span> do not use the target domain data for training, they utilize annotated real domain data for training. They may be operating under more favorable conditions compared to our proposed method that does not use MIDI-audio paired dataset.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Proposed method</h3>

<div id="S4.SS4.p1" class="ltx_para">
<p id="S4.SS4.p1.1" class="ltx_p">Our proposed method is denoted as <span id="S4.SS4.p1.1.1" class="ltx_text ltx_font_italic">Synthetic-DC</span>.
The model is trained using synthetic data and unannotated real audio data of the target domain.
In an ablation study, domain adaptation is also conducted using the same transcription architecture, denoted as <span id="S4.SS4.p1.1.2" class="ltx_text ltx_font_italic">Synthetic-DA</span>.
While we train the model to make the discriminator output closer to 0.5 in domain confusion as in Eq. (<a href="#S3.E2" title="In 3.2 Transcription architecture ‣ 3 Model ‣ Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>), we train the model to make the discriminator output closer to 1.0 in domain adaptation.
Additionally, several ablation studies are conducted to evaluate the scalability of the synthetic audio.
<span id="S4.SS4.p1.1.3" class="ltx_text ltx_font_italic">Synthetic-L</span> uses Lakh MIDI dataset and mixed NSynth dataset, same as <span id="S4.SS4.p1.1.4" class="ltx_text ltx_font_italic">Synthetic-DC</span> except for domain confusion. <span id="S4.SS4.p1.1.5" class="ltx_text ltx_font_italic">Synthetic-M</span> uses Lakh MIDI dataset and non-mixed NSynth dataset. <span id="S4.SS4.p1.1.6" class="ltx_text ltx_font_italic">Synthetic-S</span> uses Slakh MIDI dataset and non-mixed NSynth dataset. <span id="S4.SS4.p1.1.7" class="ltx_text ltx_font_italic">Synthetic-L</span>, <span id="S4.SS4.p1.1.8" class="ltx_text ltx_font_italic">M</span>, and <span id="S4.SS4.p1.1.9" class="ltx_text ltx_font_italic">S</span> employ neither domain confusion nor domain adaptation.</p>
</div>
</section>
<section id="S4.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Results</h3>

<div id="S4.SS5.p1" class="ltx_para">
<p id="S4.SS5.p1.1" class="ltx_p">The results of our proposed method and the baselines are shown in Table <a href="#S4.T1" title="Table 1 ‣ 4 Experiment ‣ Annotation-free Automatic Music Transcription with Scalable Synthetic Data and Adversarial Domain Confusion" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.
Scores highlighted in green pertain to the target-annotation-free setting, where the annotation in the target domain for evaluation is not used for training. Among the scores under the target-annotation-free setting, we mark the best result with <span id="S4.SS5.p1.1.1" class="ltx_text ltx_font_bold">boldface</span> and the
second-best result by <span id="S4.SS5.p1.1.2" class="ltx_text ltx_framed ltx_framed_underline">underlining</span>.
In ‘Real Data’ categories, ‘a’, ‘b’, and ‘c’ each indicate whether the models are trained using real audio, real audio with MIDI annotation, and the extra real audio with MIDI annotation, respectively.</p>
</div>
<div id="S4.SS5.p2" class="ltx_para">
<p id="S4.SS5.p2.1" class="ltx_p">Despite the absence of annotated real domain data, the scores for our proposed <span id="S4.SS5.p2.1.1" class="ltx_text ltx_font_italic">Synthetic-DC</span> are competitive with those of established baseline methods which utilize annotated real data. Within a group of strong baseline methods and those identified through ablation studies, our method frequently yields the best or second-best results in the target-annotation-free setting, thereby demonstrating consistently stable performance.
Comparing synthetic methods, <span id="S4.SS5.p2.1.2" class="ltx_text ltx_font_italic">Synthetic-DC</span> scores are higher for many datasets, indicating the effectiveness of domain confusion for transcription tasks.
On the other hand, domain confusion tends to be less effective in bridging larger domain gaps, such as those found in the vocal dataset (Molina). This reduced efficacy is believed to stem from the unique structural characteristics of pronunciation.
In addition, comparing <span id="S4.SS5.p2.1.3" class="ltx_text ltx_font_italic">Real-Mix</span> and <span id="S4.SS5.p2.1.4" class="ltx_text ltx_font_italic">Real-Omit</span>, <span id="S4.SS5.p2.1.5" class="ltx_text ltx_font_italic">Real-Omit</span> scores tend to decrease significantly. This suggests that training by mixing multiple datasets, which has conventionally been used as an approach to multitasking, actually has weak generalizability to domains other than the ones used for training. In contrast, the proposed method consistently produces robust and stable results without using annotations of the target domain, showing a glimpse of generalizability.</p>
</div>
<div id="S4.SS5.p3" class="ltx_para">
<p id="S4.SS5.p3.1" class="ltx_p">For the <span id="S4.SS5.p3.1.1" class="ltx_text ltx_font_italic">Synthetic-L, M, and S</span>, performance tends to improve as MIDI and timbre variations increase.
While the result for the Maestro dataset shows significant performance improvement with increased MIDI variations, the result for the Molina dataset shows significant performance improvement with increased timbre variations.
This implies that various instrument domains present distinct transcription challenges, and our synthetic approach is apt for adjusting the MIDI and timbre variations as required.</p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We proposed a transcription model that utilizes scalable synthetic audio and adversarial domain confusion to achieve annotation-free AMT which does not require any MIDI-audio paired data.
Through experiments on several datasets, we have shown that our proposed method performs well in the target-annotation-free setting. We also found that adversarial domain confusion using timbre information is useful for transcription.
The ablation studies indicate that distinct instrument domains present unique challenges, a critical consideration for future research targeting a universal model.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">In this study, we utilized a public MIDI dataset and a public one-shot dataset for training.
However, such datasets are also published elsewhere, available for free or purchase, and can even be created through rendering with specialized software because they are often used in music production.
Additionally, our domain confusion fine-tuning method can be applied using only the target domain audio data without MIDI annotation.
We believe that by selecting and amalgamating a collection of such readily accessible datasets, we can construct models customized to particular objectives, thereby circumventing the need for manually annotating audio with MIDI for the specific domains we aim to transcribe.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
C. Hawthorne et al.,

</span>
<span class="ltx_bibblock">“Sequence-to-sequence piano transcription with transformers,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2021.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Gardner et al.,

</span>
<span class="ltx_bibblock">“MT3: multi-task multitrack music transcription,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib2.1.1" class="ltx_text ltx_font_italic">Proceedings of ICLR</span>, 2021.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
K. Toyama et al.,

</span>
<span class="ltx_bibblock">“Automatic piano transcription with hierarchical frequency-time transformer,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib3.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2023.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R. M. Bittner et al.,

</span>
<span class="ltx_bibblock">“A lightweight instrument-agnostic model for polyphonic note transcription and multipitch estimation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib4.1.1" class="ltx_text ltx_font_italic">Proceedings of ICASSP</span>, 2022.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
W. Lu et al.,

</span>
<span class="ltx_bibblock">“Multitrack music transcription with a time-frequency perceiver,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib5.1.1" class="ltx_text ltx_font_italic">Proceedings of ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
L. Su and Y.-H. Yang,

</span>
<span class="ltx_bibblock">“Escaping from the abyss of manual annotation: New methodology of building polyphonic datasets for automatic music transcription,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib6.1.1" class="ltx_text ltx_font_italic">Proceedings of CMMR</span>, 2015.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P. Smaragdis and J.C. Brown,

</span>
<span class="ltx_bibblock">“Non-negative matrix fac-torization for polyphonic music transcription,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib7.1.1" class="ltx_text ltx_font_italic">Proceedings of WASPAA</span>, 2003.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
E. Benetos et al.,

</span>
<span class="ltx_bibblock">“Automatic music transcription: An overview,”

</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text ltx_font_italic">IEEE Signal Process. Magazine</span>, 2019.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
B. Maman and A. H. Bermano,

</span>
<span class="ltx_bibblock">“Unaligned supervision for automatic music transcription in-the-wild,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib9.1.1" class="ltx_text ltx_font_italic">Proceedings of ICML</span>, 2022.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
S. Kim et al.,

</span>
<span class="ltx_bibblock">“Sequence-to-sequence network training methods for automatic guitar transcription with tokenized outputs,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib10.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
K. W. Cheuk et al.,

</span>
<span class="ltx_bibblock">“Reconvat: A semi-supervised automatic music transcription framework for low-resource real-world data,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib11.1.1" class="ltx_text ltx_font_italic">Proceedings of ACM MM</span>, 2021.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Y. Wu et al.,

</span>
<span class="ltx_bibblock">“MFAE: Masked frame-level autoencoder with hybrid-supervision for low-resource music transcription,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib12.1.1" class="ltx_text ltx_font_italic">Proceedings of ICME</span>, 2023.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
K. W. Cheuk et al.,

</span>
<span class="ltx_bibblock">“Diffroll: Diffusion-based generative music transcription with unsupervised pretraining capability,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib13.1.1" class="ltx_text ltx_font_italic">Proceedings of ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
K. Kim et al.,

</span>
<span class="ltx_bibblock">“Show me the instruments: Musical instrument retrieval from mixture audio,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib14.1.1" class="ltx_text ltx_font_italic">Proceedings of ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
E. Manilow et al.,

</span>
<span class="ltx_bibblock">“Cutting music source separation some Slakh: A dataset to study the impact of training data quality and quantity,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib15.1.1" class="ltx_text ltx_font_italic">Proceedings of WASPAA</span>, 2019.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S. Ian et al.,

</span>
<span class="ltx_bibblock">“Scaling polyphonic transcription with mixtures of monophonic transcriptions,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib16.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2022.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
E. Tzeng et al.,

</span>
<span class="ltx_bibblock">“Adversarial discriminative domain adaptation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib17.1.1" class="ltx_text ltx_font_italic">Proceedings of CVPR</span>, 2017.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
L. Chen et al.,

</span>
<span class="ltx_bibblock">“Reusing the task-specific classifier as a discriminator: Discriminator-free adversarial domain adaptation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib18.1.1" class="ltx_text ltx_font_italic">Proceedings of CVPR</span>, 2022.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
H. Rangwani et al.,

</span>
<span class="ltx_bibblock">“A closer look at smoothness in domain adversarial training,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib19.1.1" class="ltx_text ltx_font_italic">Proceedings of ICML</span>, 2022.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
E. Tzeng et al.,

</span>
<span class="ltx_bibblock">“Deep domain confusion: Maximizing for domain invariance,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib20.1.1" class="ltx_text ltx_font_italic">Proceedings of CVPR</span>, 2014.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
E. Tzeng et al.,

</span>
<span class="ltx_bibblock">“Simultaneous deep transfer across domains and tasks,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib21.1.1" class="ltx_text ltx_font_italic">Proceedings of ICCV</span>, 2015.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
F. Bittner et al.,

</span>
<span class="ltx_bibblock">“Multi-pitch estimation meets microphone mismatch: Applicability of domain adaptation,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib22.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2022.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
C. Lordelo et al.,

</span>
<span class="ltx_bibblock">“Adversarial unsupervised domain adaptation for harmonic-percussive source separation,”

</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text ltx_font_italic">IEEE Signal Process. Lett.</span>, 2021.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
G. Narita et al.,

</span>
<span class="ltx_bibblock">“GANStrument: Adversarial instrument sound synthesis with pitch-invariant instance conditioning,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib24.1.1" class="ltx_text ltx_font_italic">Proceedings of ICASSP</span>, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
N. Mor et al.,

</span>
<span class="ltx_bibblock">“A universal music translation network,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib25.1.1" class="ltx_text ltx_font_italic">Proceedings of ICLR</span>, 2019.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
N. Fradet et al.,

</span>
<span class="ltx_bibblock">“MidiTok: A python package for MIDI file tokenization,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib26.1.1" class="ltx_text ltx_font_italic">Late-Breaking Demo of ISMIR</span>, 2021.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Vaswani et al.,

</span>
<span class="ltx_bibblock">“Attention is all you need,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib27.1.1" class="ltx_text ltx_font_italic">Proceedings of NIPS</span>, 2017.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Y.-T. Wu et al.,

</span>
<span class="ltx_bibblock">“Multi-instrument automatic music transcription with self-attention-based instance segmenta- tion,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib28.1.1" class="ltx_text ltx_font_italic">Proceedings of ICML</span>, 2020.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
C. Raffel et al.,

</span>
<span class="ltx_bibblock">“mir_eval: A transparent implementation of common mir metrics,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib29.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2014.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Engel et al.,

</span>
<span class="ltx_bibblock">“Neural audio synthesis of musical notes with wavenet autoencoders,”

</span>
<span class="ltx_bibblock">2017.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
T. Bertin-Mahieux et al.,

</span>
<span class="ltx_bibblock">“The million song dataset,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib31.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2011.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Q. Xi et al.,

</span>
<span class="ltx_bibblock">“Guitarset: A dataset for guitar transcription,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib32.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2018.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
C. Hawthorne et al.,

</span>
<span class="ltx_bibblock">“Enabling factorized piano music modeling and generation with the MAESTRO dataset,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib33.1.1" class="ltx_text ltx_font_italic">Proceedings of ICLR</span>, 2019.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
E. Molina et al.,

</span>
<span class="ltx_bibblock">“Evaluation framework for automatic singing transcription,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib34.1.1" class="ltx_text ltx_font_italic">Proceedings of ISMIR</span>, 2014.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
M. Miron et al.,

</span>
<span class="ltx_bibblock">“Score-informed source separation for multi-channel orchestral recordings,”

</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text ltx_font_italic">Electrical and Computer Engineering</span>, 2016.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T.-S. Chan et al.,

</span>
<span class="ltx_bibblock">“Vocal activity informed singing voice separation with the ikala dataset,”

</span>
<span class="ltx_bibblock">in <span id="bib.bib36.1.1" class="ltx_text ltx_font_italic">Proceedings of ICASSP</span>, 2015.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
John Thickstun et al.,

</span>
<span class="ltx_bibblock">“Learning features of music from scratch.,”

</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text ltx_font_italic">arXiv preprint arXiv:1611.09827</span>, 2016.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2312.10401" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2312.10402" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2312.10402">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2312.10402" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2312.10403" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Feb 27 11:35:46 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
