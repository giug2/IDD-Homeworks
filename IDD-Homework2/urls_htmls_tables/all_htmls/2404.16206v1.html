<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations</title>
<!--Generated on Tue Apr 30 19:16:25 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Knowledge Graphs Text Mining." lang="en" name="keywords"/>
<base href="/html/2404.16206v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S1" title="In Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S2" title="In Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background and Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S2.SS1" title="In 2 Background and Related Work â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Graph Structural Approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S2.SS2" title="In 2 Background and Related Work â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Textual Content Approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S2.SS3" title="In 2 Background and Related Work â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Hybrid Approaches</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S3" title="In Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S3.SS1" title="In 3 Methodology â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Structural Representation Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S3.SS2" title="In 3 Methodology â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Text Encoding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S3.SS3" title="In 3 Methodology â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Relation Prediction</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S4" title="In Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S4.SS1" title="In 4 Experiments â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S4.SS2" title="In 4 Experiments â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Comparison Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S4.SS3" title="In 4 Experiments â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Main Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S4.SS4" title="In 4 Experiments â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S5" title="In Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>School of Computing, University of Georgia

<br class="ltx_break"/><span class="ltx_note ltx_role_email" id="id1.1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">email: </span>{sakher.a,kkochut}@uga.edu</span></span></span>
<br class="ltx_break"/></span></span></span>
<h1 class="ltx_title ltx_title_document">Knowledge Graph Completion using Structural and Textual Embeddings<span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">thanks: </span>Accepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sakher Khalil Alqaaidi 
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Krzysztof Kochut
</span><span class="ltx_author_notes">11</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id2.id1">Knowledge Graphs (KGs) are widely employed in artificial intelligence applications, such as question-answering and recommendation systems. However, KGs are frequently found to be incomplete. While much of the existing literature focuses on predicting missing nodes for given incomplete KG triples, there remains an opportunity to complete KGs by exploring relations between existing nodes, a task known as relation prediction. In this study, we propose a relations prediction model that harnesses both textual and structural information within KGs. Our approach integrates walks-based embeddings with language model embeddings to effectively represent nodes. We demonstrate that our model achieves competitive results in the relation prediction task when evaluated on a widely used dataset.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Knowledge Graphs Text Mining.
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Knowledge Graphs (KGs) store informational facts in a structured format of connected triples. A triple consists of a semantic relation that binds a head (subject) entity node with a tail (object) entity node. KGs have been widely employed in several artificial intelligence applications on a global data scale, such as question-answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib33" title="">33</a>]</cite> and recommendation systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib24" title="">24</a>]</cite>. Due to their enormous sizes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib2" title="">2</a>]</cite>, Knowledge Graphs suffer from being incomplete <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib32" title="">32</a>]</cite>.
Given that the KG incompleteness problem affects its utilization, several works proposed Knowledge Graph Completion (KGC) models. These works tackled three types of KGC, that are Link Prediction (LP), Relation Prediction (RP), and Triple Classification (TC). The LP task aims at finding the missing head or tail node in a triple. The RP task aims at finding the valid relations for a given pair of head and tail nodes. The TC task aims at determining the plausibility of a complete triple.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">After surveying the literature, we found that much of the literature works focused on the LP task, whereas the RP task remained a secondary objective in some models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib23" title="">23</a>]</cite>. A few models proposed an RP variant based on a link prediction-oriented design <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib14" title="">14</a>]</cite>. However, we see that the RP task holds equal importance to the LP task and can effectively contribute to KGC. That is by identifying all the possible relations between node pairs. For instance, if <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">Apple</span> and <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">California</span> are two nodes in a knowledge graph and connected using the relation <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">has_store_in</span>, we could be interested in exploring other possible relations between them. For example, one relation that can be established between them is <span class="ltx_text ltx_font_italic" id="S1.p2.1.4">headquarters_in</span> after identifying <span class="ltx_text ltx_font_italic" id="S1.p2.1.5">Apple</span> as the node for the technological company.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Recently, KGC models have achieved remarkable accomplishments by representing graphs in the form of multi-dimensional float vectors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib23" title="">23</a>]</cite>. Therefore, KGC models can be categorized into three groups according to the representation type used. First, KGC models utilized the graphâ€™s structural information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib25" title="">25</a>]</cite>. Second, KGC models utilized the nodesâ€™ meta information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib34" title="">34</a>]</cite>. Third, KGC models utilized a combination of the two information types <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib35" title="">35</a>]</cite>. In terms of meta information utilization, several models have used the textual content within node entity names as input for language models, enabling the generation of representation embeddings. This approach has demonstrated efficacy, particularly in inductive settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib6" title="">6</a>]</cite>, that is when prediction is conducted for nodes not seen during model training. Notably, Masked Language Models (MLMs) showed astonishing results in the KGC task using nodesâ€™ text. However, these language models suffer from a costly fine-tuning phase in terms of computational resource usage, such as time and memory. In contrast, Pre-trained Language Models (PLMs) have significantly lower computational load and still provide meaningful representation of words without training the model again.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Although language models cap present powerful text representation, employing the graph structural information is still crucial in the KGC task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib29" title="">29</a>]</cite>, primarily due to the entity ambiguity problem encountered when encoding text content. Particularly, because language models cannot represent an entity efficiently relying on short text. Similar to the previous <span class="ltx_text ltx_font_italic" id="S1.p4.1.1">Apple</span> and <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">California</span> example, language model embeddings for the <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">Apple</span> node cannot lead to determining whether it is the technological company or just the fruit without having additional clues other than the node name. Otherwise, the relation <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">planted_in</span> would be predicted because <span class="ltx_text ltx_font_italic" id="S1.p4.1.5">Apple</span> could also be a node for the fruit. Indeed, determining the node identity would be possible only if the text content is detailed as in this sentence: <span class="ltx_text ltx_font_italic" id="S1.p4.1.6">Apple is a technology company headquartered in California</span>. However, such detailed description is not usually provided in KGs without employing connections to external knowledge resources. In contrast, the nodeâ€™s structural details can support revealing the nodeâ€™s identity depending on topological information, such as the neighborhood and the walks obtained from there <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To this end, we propose a <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">r</span>elations <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">p</span>rediction model (RPEST) that utilizes the <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">e</span>mbeddings of <span class="ltx_text ltx_font_bold" id="S1.p5.1.4">s</span>tructural and <span class="ltx_text ltx_font_bold" id="S1.p5.1.5">t</span>extual features in knowledge graphs. Our model employs a walk-based graph structure algorithm, namely Node2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib9" title="">9</a>]</cite>, to replace the language model fine-tuning step with structural embeddings training. Additionally, we exploit pre-trained language models efficiently to capture text contextualized representation and to avoid the costly fine-tuning phase in MLMs. We show that our choice of language model has significantly lower overhead compared to masked language models. Notably, our model achieves competitive results in the relation prediction task when compared to the best accomplishments in Freebase, a widely used benchmark. Furthermore, we analyze the effectiveness of our modelâ€™s components in a dense ablation study. To the best of our knowledge, this is the first work that considers textual embeddings along with graph walks-based embeddings for the RP task. The code to reproduce the results is available online <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://github.com/sa5r/KGRP</span></span></span></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background and Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.19">Given a knowledge graph <math alttext="\mathcal{G}=\{\mathcal{E},\mathcal{R},\mathcal{T}\}" class="ltx_Math" display="inline" id="S2.p1.1.m1.3"><semantics id="S2.p1.1.m1.3a"><mrow id="S2.p1.1.m1.3.4" xref="S2.p1.1.m1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.3.4.2" xref="S2.p1.1.m1.3.4.2.cmml">ğ’¢</mi><mo id="S2.p1.1.m1.3.4.1" xref="S2.p1.1.m1.3.4.1.cmml">=</mo><mrow id="S2.p1.1.m1.3.4.3.2" xref="S2.p1.1.m1.3.4.3.1.cmml"><mo id="S2.p1.1.m1.3.4.3.2.1" stretchy="false" xref="S2.p1.1.m1.3.4.3.1.cmml">{</mo><mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.1.1" xref="S2.p1.1.m1.1.1.cmml">â„°</mi><mo id="S2.p1.1.m1.3.4.3.2.2" xref="S2.p1.1.m1.3.4.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.2.2" xref="S2.p1.1.m1.2.2.cmml">â„›</mi><mo id="S2.p1.1.m1.3.4.3.2.3" xref="S2.p1.1.m1.3.4.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.p1.1.m1.3.3" xref="S2.p1.1.m1.3.3.cmml">ğ’¯</mi><mo id="S2.p1.1.m1.3.4.3.2.4" stretchy="false" xref="S2.p1.1.m1.3.4.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.1.m1.3b"><apply id="S2.p1.1.m1.3.4.cmml" xref="S2.p1.1.m1.3.4"><eq id="S2.p1.1.m1.3.4.1.cmml" xref="S2.p1.1.m1.3.4.1"></eq><ci id="S2.p1.1.m1.3.4.2.cmml" xref="S2.p1.1.m1.3.4.2">ğ’¢</ci><set id="S2.p1.1.m1.3.4.3.1.cmml" xref="S2.p1.1.m1.3.4.3.2"><ci id="S2.p1.1.m1.1.1.cmml" xref="S2.p1.1.m1.1.1">â„°</ci><ci id="S2.p1.1.m1.2.2.cmml" xref="S2.p1.1.m1.2.2">â„›</ci><ci id="S2.p1.1.m1.3.3.cmml" xref="S2.p1.1.m1.3.3">ğ’¯</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.1.m1.3c">\mathcal{G}=\{\mathcal{E},\mathcal{R},\mathcal{T}\}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.1.m1.3d">caligraphic_G = { caligraphic_E , caligraphic_R , caligraphic_T }</annotation></semantics></math>, where <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.p1.2.m2.1"><semantics id="S2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.2.m2.1.1" xref="S2.p1.2.m2.1.1.cmml">â„°</mi><annotation-xml encoding="MathML-Content" id="S2.p1.2.m2.1b"><ci id="S2.p1.2.m2.1.1.cmml" xref="S2.p1.2.m2.1.1">â„°</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.2.m2.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.2.m2.1d">caligraphic_E</annotation></semantics></math> is a set of entity nodes, <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.p1.3.m3.1"><semantics id="S2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.3.m3.1.1" xref="S2.p1.3.m3.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content" id="S2.p1.3.m3.1b"><ci id="S2.p1.3.m3.1.1.cmml" xref="S2.p1.3.m3.1.1">â„›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.3.m3.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.3.m3.1d">caligraphic_R</annotation></semantics></math> is a set of relations of size <math alttext="k" class="ltx_Math" display="inline" id="S2.p1.4.m4.1"><semantics id="S2.p1.4.m4.1a"><mi id="S2.p1.4.m4.1.1" xref="S2.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.p1.4.m4.1b"><ci id="S2.p1.4.m4.1.1.cmml" xref="S2.p1.4.m4.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.p1.4.m4.1d">italic_k</annotation></semantics></math>, and <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S2.p1.5.m5.1"><semantics id="S2.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.5.m5.1.1" xref="S2.p1.5.m5.1.1.cmml">ğ’¯</mi><annotation-xml encoding="MathML-Content" id="S2.p1.5.m5.1b"><ci id="S2.p1.5.m5.1.1.cmml" xref="S2.p1.5.m5.1.1">ğ’¯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.5.m5.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.5.m5.1d">caligraphic_T</annotation></semantics></math> is a set of entities text. <math alttext="\mathcal{G}" class="ltx_Math" display="inline" id="S2.p1.6.m6.1"><semantics id="S2.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.6.m6.1.1" xref="S2.p1.6.m6.1.1.cmml">ğ’¢</mi><annotation-xml encoding="MathML-Content" id="S2.p1.6.m6.1b"><ci id="S2.p1.6.m6.1.1.cmml" xref="S2.p1.6.m6.1.1">ğ’¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.6.m6.1c">\mathcal{G}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.6.m6.1d">caligraphic_G</annotation></semantics></math> is constructed of facts; each fact is represented as a directed triple <math alttext="(h,r,t)" class="ltx_Math" display="inline" id="S2.p1.7.m7.3"><semantics id="S2.p1.7.m7.3a"><mrow id="S2.p1.7.m7.3.4.2" xref="S2.p1.7.m7.3.4.1.cmml"><mo id="S2.p1.7.m7.3.4.2.1" stretchy="false" xref="S2.p1.7.m7.3.4.1.cmml">(</mo><mi id="S2.p1.7.m7.1.1" xref="S2.p1.7.m7.1.1.cmml">h</mi><mo id="S2.p1.7.m7.3.4.2.2" xref="S2.p1.7.m7.3.4.1.cmml">,</mo><mi id="S2.p1.7.m7.2.2" xref="S2.p1.7.m7.2.2.cmml">r</mi><mo id="S2.p1.7.m7.3.4.2.3" xref="S2.p1.7.m7.3.4.1.cmml">,</mo><mi id="S2.p1.7.m7.3.3" xref="S2.p1.7.m7.3.3.cmml">t</mi><mo id="S2.p1.7.m7.3.4.2.4" stretchy="false" xref="S2.p1.7.m7.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.7.m7.3b"><vector id="S2.p1.7.m7.3.4.1.cmml" xref="S2.p1.7.m7.3.4.2"><ci id="S2.p1.7.m7.1.1.cmml" xref="S2.p1.7.m7.1.1">â„</ci><ci id="S2.p1.7.m7.2.2.cmml" xref="S2.p1.7.m7.2.2">ğ‘Ÿ</ci><ci id="S2.p1.7.m7.3.3.cmml" xref="S2.p1.7.m7.3.3">ğ‘¡</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.7.m7.3c">(h,r,t)</annotation><annotation encoding="application/x-llamapun" id="S2.p1.7.m7.3d">( italic_h , italic_r , italic_t )</annotation></semantics></math>, where <math alttext="h,t\in\mathcal{E}" class="ltx_Math" display="inline" id="S2.p1.8.m8.2"><semantics id="S2.p1.8.m8.2a"><mrow id="S2.p1.8.m8.2.3" xref="S2.p1.8.m8.2.3.cmml"><mrow id="S2.p1.8.m8.2.3.2.2" xref="S2.p1.8.m8.2.3.2.1.cmml"><mi id="S2.p1.8.m8.1.1" xref="S2.p1.8.m8.1.1.cmml">h</mi><mo id="S2.p1.8.m8.2.3.2.2.1" xref="S2.p1.8.m8.2.3.2.1.cmml">,</mo><mi id="S2.p1.8.m8.2.2" xref="S2.p1.8.m8.2.2.cmml">t</mi></mrow><mo id="S2.p1.8.m8.2.3.1" xref="S2.p1.8.m8.2.3.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.p1.8.m8.2.3.3" xref="S2.p1.8.m8.2.3.3.cmml">â„°</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.8.m8.2b"><apply id="S2.p1.8.m8.2.3.cmml" xref="S2.p1.8.m8.2.3"><in id="S2.p1.8.m8.2.3.1.cmml" xref="S2.p1.8.m8.2.3.1"></in><list id="S2.p1.8.m8.2.3.2.1.cmml" xref="S2.p1.8.m8.2.3.2.2"><ci id="S2.p1.8.m8.1.1.cmml" xref="S2.p1.8.m8.1.1">â„</ci><ci id="S2.p1.8.m8.2.2.cmml" xref="S2.p1.8.m8.2.2">ğ‘¡</ci></list><ci id="S2.p1.8.m8.2.3.3.cmml" xref="S2.p1.8.m8.2.3.3">â„°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.8.m8.2c">h,t\in\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.8.m8.2d">italic_h , italic_t âˆˆ caligraphic_E</annotation></semantics></math>, <math alttext="h" class="ltx_Math" display="inline" id="S2.p1.9.m9.1"><semantics id="S2.p1.9.m9.1a"><mi id="S2.p1.9.m9.1.1" xref="S2.p1.9.m9.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.p1.9.m9.1b"><ci id="S2.p1.9.m9.1.1.cmml" xref="S2.p1.9.m9.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.9.m9.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.p1.9.m9.1d">italic_h</annotation></semantics></math> is a head entity node, <math alttext="t" class="ltx_Math" display="inline" id="S2.p1.10.m10.1"><semantics id="S2.p1.10.m10.1a"><mi id="S2.p1.10.m10.1.1" xref="S2.p1.10.m10.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p1.10.m10.1b"><ci id="S2.p1.10.m10.1.1.cmml" xref="S2.p1.10.m10.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.10.m10.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.p1.10.m10.1d">italic_t</annotation></semantics></math> is a tail entity node, <math alttext="r\in\mathcal{R}" class="ltx_Math" display="inline" id="S2.p1.11.m11.1"><semantics id="S2.p1.11.m11.1a"><mrow id="S2.p1.11.m11.1.1" xref="S2.p1.11.m11.1.1.cmml"><mi id="S2.p1.11.m11.1.1.2" xref="S2.p1.11.m11.1.1.2.cmml">r</mi><mo id="S2.p1.11.m11.1.1.1" xref="S2.p1.11.m11.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S2.p1.11.m11.1.1.3" xref="S2.p1.11.m11.1.1.3.cmml">â„›</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.11.m11.1b"><apply id="S2.p1.11.m11.1.1.cmml" xref="S2.p1.11.m11.1.1"><in id="S2.p1.11.m11.1.1.1.cmml" xref="S2.p1.11.m11.1.1.1"></in><ci id="S2.p1.11.m11.1.1.2.cmml" xref="S2.p1.11.m11.1.1.2">ğ‘Ÿ</ci><ci id="S2.p1.11.m11.1.1.3.cmml" xref="S2.p1.11.m11.1.1.3">â„›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.11.m11.1c">r\in\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.11.m11.1d">italic_r âˆˆ caligraphic_R</annotation></semantics></math>, and <math alttext="r" class="ltx_Math" display="inline" id="S2.p1.12.m12.1"><semantics id="S2.p1.12.m12.1a"><mi id="S2.p1.12.m12.1.1" xref="S2.p1.12.m12.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S2.p1.12.m12.1b"><ci id="S2.p1.12.m12.1.1.cmml" xref="S2.p1.12.m12.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.12.m12.1c">r</annotation><annotation encoding="application/x-llamapun" id="S2.p1.12.m12.1d">italic_r</annotation></semantics></math> is a relation with a direction from <math alttext="h" class="ltx_Math" display="inline" id="S2.p1.13.m13.1"><semantics id="S2.p1.13.m13.1a"><mi id="S2.p1.13.m13.1.1" xref="S2.p1.13.m13.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.p1.13.m13.1b"><ci id="S2.p1.13.m13.1.1.cmml" xref="S2.p1.13.m13.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.13.m13.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.p1.13.m13.1d">italic_h</annotation></semantics></math> to <math alttext="t" class="ltx_Math" display="inline" id="S2.p1.14.m14.1"><semantics id="S2.p1.14.m14.1a"><mi id="S2.p1.14.m14.1.1" xref="S2.p1.14.m14.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p1.14.m14.1b"><ci id="S2.p1.14.m14.1.1.cmml" xref="S2.p1.14.m14.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.14.m14.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.p1.14.m14.1d">italic_t</annotation></semantics></math>; the direction is essential to hold the fact, thus <math alttext="(t,r,h)" class="ltx_Math" display="inline" id="S2.p1.15.m15.3"><semantics id="S2.p1.15.m15.3a"><mrow id="S2.p1.15.m15.3.4.2" xref="S2.p1.15.m15.3.4.1.cmml"><mo id="S2.p1.15.m15.3.4.2.1" stretchy="false" xref="S2.p1.15.m15.3.4.1.cmml">(</mo><mi id="S2.p1.15.m15.1.1" xref="S2.p1.15.m15.1.1.cmml">t</mi><mo id="S2.p1.15.m15.3.4.2.2" xref="S2.p1.15.m15.3.4.1.cmml">,</mo><mi id="S2.p1.15.m15.2.2" xref="S2.p1.15.m15.2.2.cmml">r</mi><mo id="S2.p1.15.m15.3.4.2.3" xref="S2.p1.15.m15.3.4.1.cmml">,</mo><mi id="S2.p1.15.m15.3.3" xref="S2.p1.15.m15.3.3.cmml">h</mi><mo id="S2.p1.15.m15.3.4.2.4" stretchy="false" xref="S2.p1.15.m15.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.15.m15.3b"><vector id="S2.p1.15.m15.3.4.1.cmml" xref="S2.p1.15.m15.3.4.2"><ci id="S2.p1.15.m15.1.1.cmml" xref="S2.p1.15.m15.1.1">ğ‘¡</ci><ci id="S2.p1.15.m15.2.2.cmml" xref="S2.p1.15.m15.2.2">ğ‘Ÿ</ci><ci id="S2.p1.15.m15.3.3.cmml" xref="S2.p1.15.m15.3.3">â„</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.15.m15.3c">(t,r,h)</annotation><annotation encoding="application/x-llamapun" id="S2.p1.15.m15.3d">( italic_t , italic_r , italic_h )</annotation></semantics></math> is not equivalent to <math alttext="(h,r,t)" class="ltx_Math" display="inline" id="S2.p1.16.m16.3"><semantics id="S2.p1.16.m16.3a"><mrow id="S2.p1.16.m16.3.4.2" xref="S2.p1.16.m16.3.4.1.cmml"><mo id="S2.p1.16.m16.3.4.2.1" stretchy="false" xref="S2.p1.16.m16.3.4.1.cmml">(</mo><mi id="S2.p1.16.m16.1.1" xref="S2.p1.16.m16.1.1.cmml">h</mi><mo id="S2.p1.16.m16.3.4.2.2" xref="S2.p1.16.m16.3.4.1.cmml">,</mo><mi id="S2.p1.16.m16.2.2" xref="S2.p1.16.m16.2.2.cmml">r</mi><mo id="S2.p1.16.m16.3.4.2.3" xref="S2.p1.16.m16.3.4.1.cmml">,</mo><mi id="S2.p1.16.m16.3.3" xref="S2.p1.16.m16.3.3.cmml">t</mi><mo id="S2.p1.16.m16.3.4.2.4" stretchy="false" xref="S2.p1.16.m16.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p1.16.m16.3b"><vector id="S2.p1.16.m16.3.4.1.cmml" xref="S2.p1.16.m16.3.4.2"><ci id="S2.p1.16.m16.1.1.cmml" xref="S2.p1.16.m16.1.1">â„</ci><ci id="S2.p1.16.m16.2.2.cmml" xref="S2.p1.16.m16.2.2">ğ‘Ÿ</ci><ci id="S2.p1.16.m16.3.3.cmml" xref="S2.p1.16.m16.3.3">ğ‘¡</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.16.m16.3c">(h,r,t)</annotation><annotation encoding="application/x-llamapun" id="S2.p1.16.m16.3d">( italic_h , italic_r , italic_t )</annotation></semantics></math>. The relation prediction task targets finding the probability of all relations in <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.p1.17.m17.1"><semantics id="S2.p1.17.m17.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p1.17.m17.1.1" xref="S2.p1.17.m17.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content" id="S2.p1.17.m17.1b"><ci id="S2.p1.17.m17.1.1.cmml" xref="S2.p1.17.m17.1.1">â„›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.17.m17.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.p1.17.m17.1d">caligraphic_R</annotation></semantics></math> for the given nodes <math alttext="h" class="ltx_Math" display="inline" id="S2.p1.18.m18.1"><semantics id="S2.p1.18.m18.1a"><mi id="S2.p1.18.m18.1.1" xref="S2.p1.18.m18.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.p1.18.m18.1b"><ci id="S2.p1.18.m18.1.1.cmml" xref="S2.p1.18.m18.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.18.m18.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.p1.18.m18.1d">italic_h</annotation></semantics></math> and <math alttext="t" class="ltx_Math" display="inline" id="S2.p1.19.m19.1"><semantics id="S2.p1.19.m19.1a"><mi id="S2.p1.19.m19.1.1" xref="S2.p1.19.m19.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.p1.19.m19.1b"><ci id="S2.p1.19.m19.1.1.cmml" xref="S2.p1.19.m19.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p1.19.m19.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.p1.19.m19.1d">italic_t</annotation></semantics></math>, formally:</p>
</div>
<div class="ltx_para" id="S2.p2">
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\phi(h,t)=Pr(\mathcal{R})" class="ltx_Math" display="block" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.4" xref="S2.E1.m1.3.4.cmml"><mrow id="S2.E1.m1.3.4.2" xref="S2.E1.m1.3.4.2.cmml"><mi id="S2.E1.m1.3.4.2.2" xref="S2.E1.m1.3.4.2.2.cmml">Ï•</mi><mo id="S2.E1.m1.3.4.2.1" xref="S2.E1.m1.3.4.2.1.cmml">â¢</mo><mrow id="S2.E1.m1.3.4.2.3.2" xref="S2.E1.m1.3.4.2.3.1.cmml"><mo id="S2.E1.m1.3.4.2.3.2.1" stretchy="false" xref="S2.E1.m1.3.4.2.3.1.cmml">(</mo><mi id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">h</mi><mo id="S2.E1.m1.3.4.2.3.2.2" xref="S2.E1.m1.3.4.2.3.1.cmml">,</mo><mi id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">t</mi><mo id="S2.E1.m1.3.4.2.3.2.3" stretchy="false" xref="S2.E1.m1.3.4.2.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E1.m1.3.4.1" xref="S2.E1.m1.3.4.1.cmml">=</mo><mrow id="S2.E1.m1.3.4.3" xref="S2.E1.m1.3.4.3.cmml"><mi id="S2.E1.m1.3.4.3.2" xref="S2.E1.m1.3.4.3.2.cmml">P</mi><mo id="S2.E1.m1.3.4.3.1" xref="S2.E1.m1.3.4.3.1.cmml">â¢</mo><mi id="S2.E1.m1.3.4.3.3" xref="S2.E1.m1.3.4.3.3.cmml">r</mi><mo id="S2.E1.m1.3.4.3.1a" xref="S2.E1.m1.3.4.3.1.cmml">â¢</mo><mrow id="S2.E1.m1.3.4.3.4.2" xref="S2.E1.m1.3.4.3.cmml"><mo id="S2.E1.m1.3.4.3.4.2.1" stretchy="false" xref="S2.E1.m1.3.4.3.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">â„›</mi><mo id="S2.E1.m1.3.4.3.4.2.2" stretchy="false" xref="S2.E1.m1.3.4.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.4.cmml" xref="S2.E1.m1.3.4"><eq id="S2.E1.m1.3.4.1.cmml" xref="S2.E1.m1.3.4.1"></eq><apply id="S2.E1.m1.3.4.2.cmml" xref="S2.E1.m1.3.4.2"><times id="S2.E1.m1.3.4.2.1.cmml" xref="S2.E1.m1.3.4.2.1"></times><ci id="S2.E1.m1.3.4.2.2.cmml" xref="S2.E1.m1.3.4.2.2">italic-Ï•</ci><interval closure="open" id="S2.E1.m1.3.4.2.3.1.cmml" xref="S2.E1.m1.3.4.2.3.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">â„</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ğ‘¡</ci></interval></apply><apply id="S2.E1.m1.3.4.3.cmml" xref="S2.E1.m1.3.4.3"><times id="S2.E1.m1.3.4.3.1.cmml" xref="S2.E1.m1.3.4.3.1"></times><ci id="S2.E1.m1.3.4.3.2.cmml" xref="S2.E1.m1.3.4.3.2">ğ‘ƒ</ci><ci id="S2.E1.m1.3.4.3.3.cmml" xref="S2.E1.m1.3.4.3.3">ğ‘Ÿ</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">â„›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\phi(h,t)=Pr(\mathcal{R})</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">italic_Ï• ( italic_h , italic_t ) = italic_P italic_r ( caligraphic_R )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.4">where <math alttext="\phi" class="ltx_Math" display="inline" id="S2.p3.1.m1.1"><semantics id="S2.p3.1.m1.1a"><mi id="S2.p3.1.m1.1.1" xref="S2.p3.1.m1.1.1.cmml">Ï•</mi><annotation-xml encoding="MathML-Content" id="S2.p3.1.m1.1b"><ci id="S2.p3.1.m1.1.1.cmml" xref="S2.p3.1.m1.1.1">italic-Ï•</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.1.m1.1c">\phi</annotation><annotation encoding="application/x-llamapun" id="S2.p3.1.m1.1d">italic_Ï•</annotation></semantics></math> is a probability values scoring function. In contrast, the link prediction task aims to find a missing head node <math alttext="(?,r,t)" class="ltx_Math" display="inline" id="S2.p3.2.m2.3"><semantics id="S2.p3.2.m2.3a"><mrow id="S2.p3.2.m2.3.4.2" xref="S2.p3.2.m2.3.4.1.cmml"><mo id="S2.p3.2.m2.3.4.2.1" stretchy="false" xref="S2.p3.2.m2.3.4.1.cmml">(</mo><mi id="S2.p3.2.m2.1.1" mathvariant="normal" xref="S2.p3.2.m2.1.1.cmml">?</mi><mo id="S2.p3.2.m2.3.4.2.2" xref="S2.p3.2.m2.3.4.1.cmml">,</mo><mi id="S2.p3.2.m2.2.2" xref="S2.p3.2.m2.2.2.cmml">r</mi><mo id="S2.p3.2.m2.3.4.2.3" xref="S2.p3.2.m2.3.4.1.cmml">,</mo><mi id="S2.p3.2.m2.3.3" xref="S2.p3.2.m2.3.3.cmml">t</mi><mo id="S2.p3.2.m2.3.4.2.4" stretchy="false" xref="S2.p3.2.m2.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.2.m2.3b"><vector id="S2.p3.2.m2.3.4.1.cmml" xref="S2.p3.2.m2.3.4.2"><ci id="S2.p3.2.m2.1.1.cmml" xref="S2.p3.2.m2.1.1">?</ci><ci id="S2.p3.2.m2.2.2.cmml" xref="S2.p3.2.m2.2.2">ğ‘Ÿ</ci><ci id="S2.p3.2.m2.3.3.cmml" xref="S2.p3.2.m2.3.3">ğ‘¡</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.2.m2.3c">(?,r,t)</annotation><annotation encoding="application/x-llamapun" id="S2.p3.2.m2.3d">( ? , italic_r , italic_t )</annotation></semantics></math> or a missing tail node <math alttext="(h,r,?)" class="ltx_Math" display="inline" id="S2.p3.3.m3.3"><semantics id="S2.p3.3.m3.3a"><mrow id="S2.p3.3.m3.3.4.2" xref="S2.p3.3.m3.3.4.1.cmml"><mo id="S2.p3.3.m3.3.4.2.1" stretchy="false" xref="S2.p3.3.m3.3.4.1.cmml">(</mo><mi id="S2.p3.3.m3.1.1" xref="S2.p3.3.m3.1.1.cmml">h</mi><mo id="S2.p3.3.m3.3.4.2.2" xref="S2.p3.3.m3.3.4.1.cmml">,</mo><mi id="S2.p3.3.m3.2.2" xref="S2.p3.3.m3.2.2.cmml">r</mi><mo id="S2.p3.3.m3.3.4.2.3" xref="S2.p3.3.m3.3.4.1.cmml">,</mo><mi id="S2.p3.3.m3.3.3" mathvariant="normal" xref="S2.p3.3.m3.3.3.cmml">?</mi><mo id="S2.p3.3.m3.3.4.2.4" stretchy="false" xref="S2.p3.3.m3.3.4.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.p3.3.m3.3b"><vector id="S2.p3.3.m3.3.4.1.cmml" xref="S2.p3.3.m3.3.4.2"><ci id="S2.p3.3.m3.1.1.cmml" xref="S2.p3.3.m3.1.1">â„</ci><ci id="S2.p3.3.m3.2.2.cmml" xref="S2.p3.3.m3.2.2">ğ‘Ÿ</ci><ci id="S2.p3.3.m3.3.3.cmml" xref="S2.p3.3.m3.3.3">?</ci></vector></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.3.m3.3c">(h,r,?)</annotation><annotation encoding="application/x-llamapun" id="S2.p3.3.m3.3d">( italic_h , italic_r , ? )</annotation></semantics></math> in a triple. Recent achievements in knowledge graph completion have followed a representation learning approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib4" title="">4</a>]</cite>. Specifically, models targeted learning a function that can efficiently represent knowledge graph triples in a multi-dimensional float values space <math alttext="\mathbb{R}" class="ltx_Math" display="inline" id="S2.p3.4.m4.1"><semantics id="S2.p3.4.m4.1a"><mi id="S2.p3.4.m4.1.1" xref="S2.p3.4.m4.1.1.cmml">â„</mi><annotation-xml encoding="MathML-Content" id="S2.p3.4.m4.1b"><ci id="S2.p3.4.m4.1.1.cmml" xref="S2.p3.4.m4.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p3.4.m4.1c">\mathbb{R}</annotation><annotation encoding="application/x-llamapun" id="S2.p3.4.m4.1d">blackboard_R</annotation></semantics></math>, i.e., continuous vector embeddings. KGC models can be categorized into methods based on the nodeâ€™s structural information, textual content information, or a combination of both.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Graph Structural Approaches</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.7">Several translation-based methods have achieved remarkable results in the link prediction task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib15" title="">15</a>]</cite>. TransE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib3" title="">3</a>]</cite> started a translation-based stream of methods for unsupervised graph embeddings learning. The translation technique uses distance-based scoring function to find close embedding values of head and tail nodes given the relation between them. Specifically, finding the minimum value of the embeddings of the three triple elements in this equation <math alttext="|h+r-t|" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.2.cmml"><mo id="S2.SS1.p1.1.m1.1.1.1.2" stretchy="false" xref="S2.SS1.p1.1.m1.1.1.2.1.cmml">|</mo><mrow id="S2.SS1.p1.1.m1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.cmml"><mrow id="S2.SS1.p1.1.m1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.1.1.2.cmml"><mi id="S2.SS1.p1.1.m1.1.1.1.1.2.2" xref="S2.SS1.p1.1.m1.1.1.1.1.2.2.cmml">h</mi><mo id="S2.SS1.p1.1.m1.1.1.1.1.2.1" xref="S2.SS1.p1.1.m1.1.1.1.1.2.1.cmml">+</mo><mi id="S2.SS1.p1.1.m1.1.1.1.1.2.3" xref="S2.SS1.p1.1.m1.1.1.1.1.2.3.cmml">r</mi></mrow><mo id="S2.SS1.p1.1.m1.1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S2.SS1.p1.1.m1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.1.1.3.cmml">t</mi></mrow><mo id="S2.SS1.p1.1.m1.1.1.1.3" stretchy="false" xref="S2.SS1.p1.1.m1.1.1.2.1.cmml">|</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.1"><abs id="S2.SS1.p1.1.m1.1.1.2.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.2"></abs><apply id="S2.SS1.p1.1.m1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1"><minus id="S2.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1"></minus><apply id="S2.SS1.p1.1.m1.1.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.2"><plus id="S2.SS1.p1.1.m1.1.1.1.1.2.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.2.1"></plus><ci id="S2.SS1.p1.1.m1.1.1.1.1.2.2.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.2.2">â„</ci><ci id="S2.SS1.p1.1.m1.1.1.1.1.2.3.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.2.3">ğ‘Ÿ</ci></apply><ci id="S2.SS1.p1.1.m1.1.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.3">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">|h+r-t|</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">| italic_h + italic_r - italic_t |</annotation></semantics></math>, where <math alttext="h,r," class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.3"><semantics id="S2.SS1.p1.2.m2.3a"><mrow id="S2.SS1.p1.2.m2.3.3.1"><mrow id="S2.SS1.p1.2.m2.3.3.1.1.2" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">h</mi><mo id="S2.SS1.p1.2.m2.3.3.1.1.2.1" xref="S2.SS1.p1.2.m2.3.3.1.1.1.cmml">,</mo><mi id="S2.SS1.p1.2.m2.2.2" xref="S2.SS1.p1.2.m2.2.2.cmml">r</mi></mrow><mo id="S2.SS1.p1.2.m2.3.3.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.3b"><list id="S2.SS1.p1.2.m2.3.3.1.1.1.cmml" xref="S2.SS1.p1.2.m2.3.3.1.1.2"><ci id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">â„</ci><ci id="S2.SS1.p1.2.m2.2.2.cmml" xref="S2.SS1.p1.2.m2.2.2">ğ‘Ÿ</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.3c">h,r,</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.3d">italic_h , italic_r ,</annotation></semantics></math> and <math alttext="t" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">italic_t</annotation></semantics></math> are represented in a d-dimensional space <math alttext="\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><msup id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml"><mi id="S2.SS1.p1.4.m4.1.1.2" xref="S2.SS1.p1.4.m4.1.1.2.cmml">â„</mi><mi id="S2.SS1.p1.4.m4.1.1.3" xref="S2.SS1.p1.4.m4.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">superscript</csymbol><ci id="S2.SS1.p1.4.m4.1.1.2.cmml" xref="S2.SS1.p1.4.m4.1.1.2">â„</ci><ci id="S2.SS1.p1.4.m4.1.1.3.cmml" xref="S2.SS1.p1.4.m4.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>, i.e., <math alttext="h,r,t\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.3"><semantics id="S2.SS1.p1.5.m5.3a"><mrow id="S2.SS1.p1.5.m5.3.4" xref="S2.SS1.p1.5.m5.3.4.cmml"><mrow id="S2.SS1.p1.5.m5.3.4.2.2" xref="S2.SS1.p1.5.m5.3.4.2.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml">h</mi><mo id="S2.SS1.p1.5.m5.3.4.2.2.1" xref="S2.SS1.p1.5.m5.3.4.2.1.cmml">,</mo><mi id="S2.SS1.p1.5.m5.2.2" xref="S2.SS1.p1.5.m5.2.2.cmml">r</mi><mo id="S2.SS1.p1.5.m5.3.4.2.2.2" xref="S2.SS1.p1.5.m5.3.4.2.1.cmml">,</mo><mi id="S2.SS1.p1.5.m5.3.3" xref="S2.SS1.p1.5.m5.3.3.cmml">t</mi></mrow><mo id="S2.SS1.p1.5.m5.3.4.1" xref="S2.SS1.p1.5.m5.3.4.1.cmml">âˆˆ</mo><msup id="S2.SS1.p1.5.m5.3.4.3" xref="S2.SS1.p1.5.m5.3.4.3.cmml"><mi id="S2.SS1.p1.5.m5.3.4.3.2" xref="S2.SS1.p1.5.m5.3.4.3.2.cmml">â„</mi><mi id="S2.SS1.p1.5.m5.3.4.3.3" xref="S2.SS1.p1.5.m5.3.4.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.3b"><apply id="S2.SS1.p1.5.m5.3.4.cmml" xref="S2.SS1.p1.5.m5.3.4"><in id="S2.SS1.p1.5.m5.3.4.1.cmml" xref="S2.SS1.p1.5.m5.3.4.1"></in><list id="S2.SS1.p1.5.m5.3.4.2.1.cmml" xref="S2.SS1.p1.5.m5.3.4.2.2"><ci id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">â„</ci><ci id="S2.SS1.p1.5.m5.2.2.cmml" xref="S2.SS1.p1.5.m5.2.2">ğ‘Ÿ</ci><ci id="S2.SS1.p1.5.m5.3.3.cmml" xref="S2.SS1.p1.5.m5.3.3">ğ‘¡</ci></list><apply id="S2.SS1.p1.5.m5.3.4.3.cmml" xref="S2.SS1.p1.5.m5.3.4.3"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.3.4.3.1.cmml" xref="S2.SS1.p1.5.m5.3.4.3">superscript</csymbol><ci id="S2.SS1.p1.5.m5.3.4.3.2.cmml" xref="S2.SS1.p1.5.m5.3.4.3.2">â„</ci><ci id="S2.SS1.p1.5.m5.3.4.3.3.cmml" xref="S2.SS1.p1.5.m5.3.4.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.3c">h,r,t\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.3d">italic_h , italic_r , italic_t âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>. Since TransE used a unified space for nodes and relations. TransR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib15" title="">15</a>]</cite> proposed a multi-relational spaces model to tackle the problem of nodes involved in multiple facts in the knowledge graph. Formally, each relation is represented in the space <math alttext="\mathbb{R}^{k}" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><msup id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">â„</mi><mi id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml">k</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">superscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">â„</ci><ci id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3">ğ‘˜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">\mathbb{R}^{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">blackboard_R start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT</annotation></semantics></math>, and the knowledge graph facts are represented in the space <math alttext="\mathbb{R}^{k\times d}" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m7.1"><semantics id="S2.SS1.p1.7.m7.1a"><msup id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">â„</mi><mrow id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml"><mi id="S2.SS1.p1.7.m7.1.1.3.2" xref="S2.SS1.p1.7.m7.1.1.3.2.cmml">k</mi><mo id="S2.SS1.p1.7.m7.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p1.7.m7.1.1.3.1.cmml">Ã—</mo><mi id="S2.SS1.p1.7.m7.1.1.3.3" xref="S2.SS1.p1.7.m7.1.1.3.3.cmml">d</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">superscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2">â„</ci><apply id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3"><times id="S2.SS1.p1.7.m7.1.1.3.1.cmml" xref="S2.SS1.p1.7.m7.1.1.3.1"></times><ci id="S2.SS1.p1.7.m7.1.1.3.2.cmml" xref="S2.SS1.p1.7.m7.1.1.3.2">ğ‘˜</ci><ci id="S2.SS1.p1.7.m7.1.1.3.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3.3">ğ‘‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">\mathbb{R}^{k\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m7.1d">blackboard_R start_POSTSUPERSCRIPT italic_k Ã— italic_d end_POSTSUPERSCRIPT</annotation></semantics></math>. However, this approach came at the cost of additional computation due to the extended spaces. TaRP is a relation prediction model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib5" title="">5</a>]</cite>. TaRP employed different translation-based methods such as TransE and RotateE, to generate representation embeddings. Additionally, TaRP incorporated the node type identification and encoding to boost the relation prediction results. A different course to generate node embeddings is using neural networks. Shallom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib7" title="">7</a>]</cite> proposed a shallow neural network layers for the relation prediction task.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Textual Content Approaches</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The textual content in KGs has been exploited in different ways. The nodeâ€™s long description helped in predicting the relations for nodes that were not seen during model training, i.e., inductive settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib26" title="">26</a>]</cite>. However, retrieving each nodeâ€™s text description requires additional steps and the employment of external knowledge bases.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib8" title="">8</a>]</cite> emerged as a masked language model with a state of the art performance on a variety of Natural Language Processing (NLP) tasks KG-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib34" title="">34</a>]</cite> was one of the first models to utilize BERT for the KGC task; KG-BERT targeted mainly the LP task. Additionally, two variants were proposed for the RP and TC tasks. Inspired by this work, BERTRL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib36" title="">36</a>]</cite> linearized the triples around each relation and trained the model to have better inductive performance. Nevertheless, language model-based approaches suffered from serious issues. BERT and its successors of masked language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib16" title="">16</a>]</cite>, incorporate a costly fine-tuning phase in terms of time and memory. In large language models (LLMs) such as GPT and Llama, the computation needs are worst <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.2">In contrast, pre-trained language models (PLMs) have significant lower computational load. Furthermore, PLMs provide meaningful representation of text words for new datasets without retraining the model. For instance, Glove <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib18" title="">18</a>]</cite> is a PLM that provides multi-dimensional vectors representation for a single word with <math alttext="O(n)" class="ltx_Math" display="inline" id="S2.SS2.p3.1.m1.1"><semantics id="S2.SS2.p3.1.m1.1a"><mrow id="S2.SS2.p3.1.m1.1.2" xref="S2.SS2.p3.1.m1.1.2.cmml"><mi id="S2.SS2.p3.1.m1.1.2.2" xref="S2.SS2.p3.1.m1.1.2.2.cmml">O</mi><mo id="S2.SS2.p3.1.m1.1.2.1" xref="S2.SS2.p3.1.m1.1.2.1.cmml">â¢</mo><mrow id="S2.SS2.p3.1.m1.1.2.3.2" xref="S2.SS2.p3.1.m1.1.2.cmml"><mo id="S2.SS2.p3.1.m1.1.2.3.2.1" stretchy="false" xref="S2.SS2.p3.1.m1.1.2.cmml">(</mo><mi id="S2.SS2.p3.1.m1.1.1" xref="S2.SS2.p3.1.m1.1.1.cmml">n</mi><mo id="S2.SS2.p3.1.m1.1.2.3.2.2" stretchy="false" xref="S2.SS2.p3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.1.m1.1b"><apply id="S2.SS2.p3.1.m1.1.2.cmml" xref="S2.SS2.p3.1.m1.1.2"><times id="S2.SS2.p3.1.m1.1.2.1.cmml" xref="S2.SS2.p3.1.m1.1.2.1"></times><ci id="S2.SS2.p3.1.m1.1.2.2.cmml" xref="S2.SS2.p3.1.m1.1.2.2">ğ‘‚</ci><ci id="S2.SS2.p3.1.m1.1.1.cmml" xref="S2.SS2.p3.1.m1.1.1">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.1.m1.1c">O(n)</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.1.m1.1d">italic_O ( italic_n )</annotation></semantics></math> complexity, where <math alttext="n" class="ltx_Math" display="inline" id="S2.SS2.p3.2.m2.1"><semantics id="S2.SS2.p3.2.m2.1a"><mi id="S2.SS2.p3.2.m2.1.1" xref="S2.SS2.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p3.2.m2.1b"><ci id="S2.SS2.p3.2.m2.1.1.cmml" xref="S2.SS2.p3.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p3.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p3.2.m2.1d">italic_n</annotation></semantics></math> is the PLMâ€™s vocabulary size. However, Glove operates at the word level, unlike MLMs and LLMs that can represent text sequences as contextualized embeddings. Text sequences can consist of mutli-word terms or sentences. A nodeâ€™s text content can include multiple words, such as â€˜Alfred Nobelâ€™.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Hybrid Approaches</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Here we review KGC models that employed a hybrid approach based on textual and structural representation. LASS model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib22" title="">22</a>]</cite> employed BERTâ€™s fine-tuning for the textual encoding and used the resulted loss values to construct the semantic embeddings. A variant for triple classification was proposed in the paper. StAR model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib29" title="">29</a>]</cite> targeted handling the overwhelming performance in language models by reusing the graph elementsâ€™ embeddings. However, the model achieved a satisfactory performance only after combining the its textual representation with an existing graph embeddings, such as RotateE. The KGLM model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib35" title="">35</a>]</cite> re-ran the original Roberta language model training with additional text generated from translating KG triples into normal sentences. The model added an entity/relation-type embedding layer to include the graph structure in training. In KEPLER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib31" title="">31</a>]</cite>, authors jointly optimized the language model training objective with the structural training objective for the link prediction task.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Some models achieved better results in the link prediction task through trying different negative sampling (contrastive learning) approaches in the text embeddings training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib11" title="">11</a>]</cite>. The SimKGC model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib30" title="">30</a>]</cite> incorporated the structured information by re-ranking candidates based on path distances between target nodes in the graph, whereas the MoCoSA model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib11" title="">11</a>]</cite> used the independently trained structural embeddings and fused them with the textual embeddings.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" id="S2.F1.g1" src=""/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Our modelâ€™s diagram showing the node representation component and the relations prediction component.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.4">Our model aims to find the probability of every predefined relation that can establish a fact by connecting a head node to a tail node. Formally, <math alttext="Pr(\mathcal{R}|h,t)" class="ltx_Math" display="inline" id="S3.p1.1.m1.3"><semantics id="S3.p1.1.m1.3a"><mrow id="S3.p1.1.m1.3.3" xref="S3.p1.1.m1.3.3.cmml"><mi id="S3.p1.1.m1.3.3.3" xref="S3.p1.1.m1.3.3.3.cmml">P</mi><mo id="S3.p1.1.m1.3.3.2" xref="S3.p1.1.m1.3.3.2.cmml">â¢</mo><mi id="S3.p1.1.m1.3.3.4" xref="S3.p1.1.m1.3.3.4.cmml">r</mi><mo id="S3.p1.1.m1.3.3.2a" xref="S3.p1.1.m1.3.3.2.cmml">â¢</mo><mrow id="S3.p1.1.m1.3.3.1.1" xref="S3.p1.1.m1.3.3.1.1.1.cmml"><mo id="S3.p1.1.m1.3.3.1.1.2" stretchy="false" xref="S3.p1.1.m1.3.3.1.1.1.cmml">(</mo><mrow id="S3.p1.1.m1.3.3.1.1.1" xref="S3.p1.1.m1.3.3.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.p1.1.m1.3.3.1.1.1.2" xref="S3.p1.1.m1.3.3.1.1.1.2.cmml">â„›</mi><mo fence="false" id="S3.p1.1.m1.3.3.1.1.1.1" xref="S3.p1.1.m1.3.3.1.1.1.1.cmml">|</mo><mrow id="S3.p1.1.m1.3.3.1.1.1.3.2" xref="S3.p1.1.m1.3.3.1.1.1.3.1.cmml"><mi id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">h</mi><mo id="S3.p1.1.m1.3.3.1.1.1.3.2.1" xref="S3.p1.1.m1.3.3.1.1.1.3.1.cmml">,</mo><mi id="S3.p1.1.m1.2.2" xref="S3.p1.1.m1.2.2.cmml">t</mi></mrow></mrow><mo id="S3.p1.1.m1.3.3.1.1.3" stretchy="false" xref="S3.p1.1.m1.3.3.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.3b"><apply id="S3.p1.1.m1.3.3.cmml" xref="S3.p1.1.m1.3.3"><times id="S3.p1.1.m1.3.3.2.cmml" xref="S3.p1.1.m1.3.3.2"></times><ci id="S3.p1.1.m1.3.3.3.cmml" xref="S3.p1.1.m1.3.3.3">ğ‘ƒ</ci><ci id="S3.p1.1.m1.3.3.4.cmml" xref="S3.p1.1.m1.3.3.4">ğ‘Ÿ</ci><apply id="S3.p1.1.m1.3.3.1.1.1.cmml" xref="S3.p1.1.m1.3.3.1.1"><csymbol cd="latexml" id="S3.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.p1.1.m1.3.3.1.1.1.1">conditional</csymbol><ci id="S3.p1.1.m1.3.3.1.1.1.2.cmml" xref="S3.p1.1.m1.3.3.1.1.1.2">â„›</ci><list id="S3.p1.1.m1.3.3.1.1.1.3.1.cmml" xref="S3.p1.1.m1.3.3.1.1.1.3.2"><ci id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1">â„</ci><ci id="S3.p1.1.m1.2.2.cmml" xref="S3.p1.1.m1.2.2">ğ‘¡</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.3c">Pr(\mathcal{R}|h,t)</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.3d">italic_P italic_r ( caligraphic_R | italic_h , italic_t )</annotation></semantics></math> where <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">â„›</mi><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><ci id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1">â„›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">caligraphic_R</annotation></semantics></math> is a set of predefined relations, <math alttext="h" class="ltx_Math" display="inline" id="S3.p1.3.m3.1"><semantics id="S3.p1.3.m3.1a"><mi id="S3.p1.3.m3.1.1" xref="S3.p1.3.m3.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.1b"><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.1c">h</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.1d">italic_h</annotation></semantics></math> is the head node, and <math alttext="t" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><mi id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><ci id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">italic_t</annotation></semantics></math> is the tail node. Our model takes as an input a combination of the node structural representation and the node textual representation. Our model conducts a supervised learning for a neural network to predict the relationsâ€™ probabilities. Accordingly, our model consists of two main components, the representation component and the relations prediction component. The former incorporates our approach to generate the structural representation and the textual representation, whereas the later incorporates a recurrent neural network and the prediction layer. Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S2.F1" title="Figure 1 â€£ 2.3 Hybrid Approaches â€£ 2 Background and Related Work â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">1</span></a> shows the architecture of our model.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Structural Representation Training</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.7">We incorporate the nodes structural information using the Node2Vec <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib9" title="">9</a>]</cite> model. Node2Vec has been known for its efficiency and for being versatile. The model is an unsupervised learning algorithm for graph features representation. Training in Node2Vec aims at optimizing a function that maximizes the log-probability of observing a neighborhood for a node, based on the neighborhood nodesâ€™ representation as the following:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\max\limits_{f}\sum_{u\in V}log\,Pr(N_{s}(u)|f(u))" class="ltx_Math" display="block" id="S3.Ex1.m1.3"><semantics id="S3.Ex1.m1.3a"><mrow id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml"><munder id="S3.Ex1.m1.3.3.3" xref="S3.Ex1.m1.3.3.3.cmml"><mi id="S3.Ex1.m1.3.3.3.2" xref="S3.Ex1.m1.3.3.3.2.cmml">max</mi><mi id="S3.Ex1.m1.3.3.3.3" xref="S3.Ex1.m1.3.3.3.3.cmml">f</mi></munder><mo id="S3.Ex1.m1.3.3.2" xref="S3.Ex1.m1.3.3.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.3.3.1" xref="S3.Ex1.m1.3.3.1.cmml"><munder id="S3.Ex1.m1.3.3.1.2" xref="S3.Ex1.m1.3.3.1.2.cmml"><mo id="S3.Ex1.m1.3.3.1.2.2" movablelimits="false" xref="S3.Ex1.m1.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex1.m1.3.3.1.2.3" xref="S3.Ex1.m1.3.3.1.2.3.cmml"><mi id="S3.Ex1.m1.3.3.1.2.3.2" xref="S3.Ex1.m1.3.3.1.2.3.2.cmml">u</mi><mo id="S3.Ex1.m1.3.3.1.2.3.1" xref="S3.Ex1.m1.3.3.1.2.3.1.cmml">âˆˆ</mo><mi id="S3.Ex1.m1.3.3.1.2.3.3" xref="S3.Ex1.m1.3.3.1.2.3.3.cmml">V</mi></mrow></munder><mrow id="S3.Ex1.m1.3.3.1.1" xref="S3.Ex1.m1.3.3.1.1.cmml"><mi id="S3.Ex1.m1.3.3.1.1.3" xref="S3.Ex1.m1.3.3.1.1.3.cmml">l</mi><mo id="S3.Ex1.m1.3.3.1.1.2" xref="S3.Ex1.m1.3.3.1.1.2.cmml">â¢</mo><mi id="S3.Ex1.m1.3.3.1.1.4" xref="S3.Ex1.m1.3.3.1.1.4.cmml">o</mi><mo id="S3.Ex1.m1.3.3.1.1.2a" xref="S3.Ex1.m1.3.3.1.1.2.cmml">â¢</mo><mi id="S3.Ex1.m1.3.3.1.1.5" xref="S3.Ex1.m1.3.3.1.1.5.cmml">g</mi><mo id="S3.Ex1.m1.3.3.1.1.2b" lspace="0.170em" xref="S3.Ex1.m1.3.3.1.1.2.cmml">â¢</mo><mi id="S3.Ex1.m1.3.3.1.1.6" xref="S3.Ex1.m1.3.3.1.1.6.cmml">P</mi><mo id="S3.Ex1.m1.3.3.1.1.2c" xref="S3.Ex1.m1.3.3.1.1.2.cmml">â¢</mo><mi id="S3.Ex1.m1.3.3.1.1.7" xref="S3.Ex1.m1.3.3.1.1.7.cmml">r</mi><mo id="S3.Ex1.m1.3.3.1.1.2d" xref="S3.Ex1.m1.3.3.1.1.2.cmml">â¢</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.cmml"><mo id="S3.Ex1.m1.3.3.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.cmml"><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.cmml"><msub id="S3.Ex1.m1.3.3.1.1.1.1.1.2.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.2.cmml">N</mi><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.3.cmml">s</mi></msub><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.2.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.1.cmml">â¢</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.2.3.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.cmml"><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.2.3.2.1" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.cmml">(</mo><mi id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml">u</mi><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.2.3.2.2" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.cmml">)</mo></mrow></mrow><mo fence="false" id="S3.Ex1.m1.3.3.1.1.1.1.1.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1.cmml">|</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.3" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m1.3.3.1.1.1.1.1.3.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3.2.cmml">f</mi><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.3.1" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3.1.cmml">â¢</mo><mrow id="S3.Ex1.m1.3.3.1.1.1.1.1.3.3.2" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3.cmml"><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.3.3.2.1" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3.cmml">(</mo><mi id="S3.Ex1.m1.2.2" xref="S3.Ex1.m1.2.2.cmml">u</mi><mo id="S3.Ex1.m1.3.3.1.1.1.1.1.3.3.2.2" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1.m1.3.3.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.3b"><apply id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3"><times id="S3.Ex1.m1.3.3.2.cmml" xref="S3.Ex1.m1.3.3.2"></times><apply id="S3.Ex1.m1.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.3.1.cmml" xref="S3.Ex1.m1.3.3.3">subscript</csymbol><max id="S3.Ex1.m1.3.3.3.2.cmml" xref="S3.Ex1.m1.3.3.3.2"></max><ci id="S3.Ex1.m1.3.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3.3">ğ‘“</ci></apply><apply id="S3.Ex1.m1.3.3.1.cmml" xref="S3.Ex1.m1.3.3.1"><apply id="S3.Ex1.m1.3.3.1.2.cmml" xref="S3.Ex1.m1.3.3.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.2.1.cmml" xref="S3.Ex1.m1.3.3.1.2">subscript</csymbol><sum id="S3.Ex1.m1.3.3.1.2.2.cmml" xref="S3.Ex1.m1.3.3.1.2.2"></sum><apply id="S3.Ex1.m1.3.3.1.2.3.cmml" xref="S3.Ex1.m1.3.3.1.2.3"><in id="S3.Ex1.m1.3.3.1.2.3.1.cmml" xref="S3.Ex1.m1.3.3.1.2.3.1"></in><ci id="S3.Ex1.m1.3.3.1.2.3.2.cmml" xref="S3.Ex1.m1.3.3.1.2.3.2">ğ‘¢</ci><ci id="S3.Ex1.m1.3.3.1.2.3.3.cmml" xref="S3.Ex1.m1.3.3.1.2.3.3">ğ‘‰</ci></apply></apply><apply id="S3.Ex1.m1.3.3.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1"><times id="S3.Ex1.m1.3.3.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.2"></times><ci id="S3.Ex1.m1.3.3.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.3">ğ‘™</ci><ci id="S3.Ex1.m1.3.3.1.1.4.cmml" xref="S3.Ex1.m1.3.3.1.1.4">ğ‘œ</ci><ci id="S3.Ex1.m1.3.3.1.1.5.cmml" xref="S3.Ex1.m1.3.3.1.1.5">ğ‘”</ci><ci id="S3.Ex1.m1.3.3.1.1.6.cmml" xref="S3.Ex1.m1.3.3.1.1.6">ğ‘ƒ</ci><ci id="S3.Ex1.m1.3.3.1.1.7.cmml" xref="S3.Ex1.m1.3.3.1.1.7">ğ‘Ÿ</ci><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.1">conditional</csymbol><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2"><times id="S3.Ex1.m1.3.3.1.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.1"></times><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.2.2.3">ğ‘ </ci></apply><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">ğ‘¢</ci></apply><apply id="S3.Ex1.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3"><times id="S3.Ex1.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3.1"></times><ci id="S3.Ex1.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m1.3.3.1.1.1.1.1.3.2">ğ‘“</ci><ci id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">ğ‘¢</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.3c">\max\limits_{f}\sum_{u\in V}log\,Pr(N_{s}(u)|f(u))</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.3d">roman_max start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_u âˆˆ italic_V end_POSTSUBSCRIPT italic_l italic_o italic_g italic_P italic_r ( italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_u ) | italic_f ( italic_u ) )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS1.p1.6">where <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">italic_V</annotation></semantics></math> is the nodes set in a graph <math alttext="\mathcal{G}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">ğ’¢</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">ğ’¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">\mathcal{G}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">caligraphic_G</annotation></semantics></math>, <math alttext="N_{s}(u)" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><mrow id="S3.SS1.p1.3.m3.1.2" xref="S3.SS1.p1.3.m3.1.2.cmml"><msub id="S3.SS1.p1.3.m3.1.2.2" xref="S3.SS1.p1.3.m3.1.2.2.cmml"><mi id="S3.SS1.p1.3.m3.1.2.2.2" xref="S3.SS1.p1.3.m3.1.2.2.2.cmml">N</mi><mi id="S3.SS1.p1.3.m3.1.2.2.3" xref="S3.SS1.p1.3.m3.1.2.2.3.cmml">s</mi></msub><mo id="S3.SS1.p1.3.m3.1.2.1" xref="S3.SS1.p1.3.m3.1.2.1.cmml">â¢</mo><mrow id="S3.SS1.p1.3.m3.1.2.3.2" xref="S3.SS1.p1.3.m3.1.2.cmml"><mo id="S3.SS1.p1.3.m3.1.2.3.2.1" stretchy="false" xref="S3.SS1.p1.3.m3.1.2.cmml">(</mo><mi id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml">u</mi><mo id="S3.SS1.p1.3.m3.1.2.3.2.2" stretchy="false" xref="S3.SS1.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.2.cmml" xref="S3.SS1.p1.3.m3.1.2"><times id="S3.SS1.p1.3.m3.1.2.1.cmml" xref="S3.SS1.p1.3.m3.1.2.1"></times><apply id="S3.SS1.p1.3.m3.1.2.2.cmml" xref="S3.SS1.p1.3.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.2.2.1.cmml" xref="S3.SS1.p1.3.m3.1.2.2">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.2.2.2.cmml" xref="S3.SS1.p1.3.m3.1.2.2.2">ğ‘</ci><ci id="S3.SS1.p1.3.m3.1.2.2.3.cmml" xref="S3.SS1.p1.3.m3.1.2.2.3">ğ‘ </ci></apply><ci id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">N_{s}(u)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ( italic_u )</annotation></semantics></math> is the neighborhood for a node <math alttext="u" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mi id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><ci id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">italic_u</annotation></semantics></math>, and <math alttext="f(u)" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mrow id="S3.SS1.p1.5.m5.1.2" xref="S3.SS1.p1.5.m5.1.2.cmml"><mi id="S3.SS1.p1.5.m5.1.2.2" xref="S3.SS1.p1.5.m5.1.2.2.cmml">f</mi><mo id="S3.SS1.p1.5.m5.1.2.1" xref="S3.SS1.p1.5.m5.1.2.1.cmml">â¢</mo><mrow id="S3.SS1.p1.5.m5.1.2.3.2" xref="S3.SS1.p1.5.m5.1.2.cmml"><mo id="S3.SS1.p1.5.m5.1.2.3.2.1" stretchy="false" xref="S3.SS1.p1.5.m5.1.2.cmml">(</mo><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">u</mi><mo id="S3.SS1.p1.5.m5.1.2.3.2.2" stretchy="false" xref="S3.SS1.p1.5.m5.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><apply id="S3.SS1.p1.5.m5.1.2.cmml" xref="S3.SS1.p1.5.m5.1.2"><times id="S3.SS1.p1.5.m5.1.2.1.cmml" xref="S3.SS1.p1.5.m5.1.2.1"></times><ci id="S3.SS1.p1.5.m5.1.2.2.cmml" xref="S3.SS1.p1.5.m5.1.2.2">ğ‘“</ci><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">ğ‘¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">f(u)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_f ( italic_u )</annotation></semantics></math> is the features representation for the node <math alttext="u" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.1"><semantics id="S3.SS1.p1.6.m6.1a"><mi id="S3.SS1.p1.6.m6.1.1" xref="S3.SS1.p1.6.m6.1.1.cmml">u</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.1b"><ci id="S3.SS1.p1.6.m6.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1">ğ‘¢</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.1c">u</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.1d">italic_u</annotation></semantics></math>. The source node neighborhood is constructed from sampling nodes identified following biased random walk strategies. The strategies could be either Breadth-First Sampling (BFS) or Depth-First Sampling (DFS). In the former, the sampled nodes are adjacent to the source node, whereas in the later, the sampled nodes are found while taking further steps in the walk paths, so it is not necessary for the nodes to be immediately connected to the source node, but a direct path should lead to the source node.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.2">Node2Vec is used in our model as a preliminary step before training the prediction neural network, that is described in section <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S3.SS3" title="3.3 Relation Prediction â€£ 3 Methodology â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">3.3</span></a>, this initial phase replaces the fine-tuning step in MLMs, which is not needed in our implementation as described in section <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S3.SS2" title="3.2 Text Encoding â€£ 3 Methodology â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">3.2</span></a>. After training the Node2Vec algorithm on the graph, we get a representation <math alttext="v^{{}^{\prime}}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.1"><semantics id="S3.SS1.p2.1.m1.1a"><msup id="S3.SS1.p2.1.m1.1.1" xref="S3.SS1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.p2.1.m1.1.1.2" xref="S3.SS1.p2.1.m1.1.1.2.cmml">v</mi><msup id="S3.SS1.p2.1.m1.1.1.3" xref="S3.SS1.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.p2.1.m1.1.1.3a" xref="S3.SS1.p2.1.m1.1.1.3.cmml"></mi><mo id="S3.SS1.p2.1.m1.1.1.3.1" xref="S3.SS1.p2.1.m1.1.1.3.1.cmml">â€²</mo></msup></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.1b"><apply id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">superscript</csymbol><ci id="S3.SS1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.p2.1.m1.1.1.2">ğ‘£</ci><apply id="S3.SS1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.p2.1.m1.1.1.3"><ci id="S3.SS1.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.p2.1.m1.1.1.3.1">â€²</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.1c">v^{{}^{\prime}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.1d">italic_v start_POSTSUPERSCRIPT start_FLOATSUPERSCRIPT â€² end_FLOATSUPERSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> of <math alttext="d" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><mi id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><ci id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">italic_d</annotation></semantics></math> vectors for each node as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S2.F1" title="Figure 1 â€£ 2.3 Hybrid Approaches â€£ 2 Background and Related Work â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Text Encoding</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We employ Glove <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib18" title="">18</a>]</cite> pre-trained word embeddings to represent the nodesâ€™ textual content. Glove language model is an unsupervised learning algorithm trained on a word-word co-occurrence probabilities using a global scale text corpus. The training objective in Glove is minimizing the difference between two words co-occurrence as the following:</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="J=\sum^{V}_{i,j=1}f(X_{ij})(w^{T}_{i}\tilde{w_{j}}+b_{i}+\tilde{b_{j}}-log\,X_%
{ij})^{2}" class="ltx_Math" display="block" id="S3.Ex2.m1.4"><semantics id="S3.Ex2.m1.4a"><mrow id="S3.Ex2.m1.4.4" xref="S3.Ex2.m1.4.4.cmml"><mi id="S3.Ex2.m1.4.4.4" xref="S3.Ex2.m1.4.4.4.cmml">J</mi><mo id="S3.Ex2.m1.4.4.3" rspace="0.111em" xref="S3.Ex2.m1.4.4.3.cmml">=</mo><mrow id="S3.Ex2.m1.4.4.2" xref="S3.Ex2.m1.4.4.2.cmml"><munderover id="S3.Ex2.m1.4.4.2.3" xref="S3.Ex2.m1.4.4.2.3.cmml"><mo id="S3.Ex2.m1.4.4.2.3.2.2" movablelimits="false" xref="S3.Ex2.m1.4.4.2.3.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex2.m1.2.2.2" xref="S3.Ex2.m1.2.2.2.cmml"><mrow id="S3.Ex2.m1.2.2.2.4.2" xref="S3.Ex2.m1.2.2.2.4.1.cmml"><mi id="S3.Ex2.m1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.cmml">i</mi><mo id="S3.Ex2.m1.2.2.2.4.2.1" xref="S3.Ex2.m1.2.2.2.4.1.cmml">,</mo><mi id="S3.Ex2.m1.2.2.2.2" xref="S3.Ex2.m1.2.2.2.2.cmml">j</mi></mrow><mo id="S3.Ex2.m1.2.2.2.3" xref="S3.Ex2.m1.2.2.2.3.cmml">=</mo><mn id="S3.Ex2.m1.2.2.2.5" xref="S3.Ex2.m1.2.2.2.5.cmml">1</mn></mrow><mi id="S3.Ex2.m1.4.4.2.3.2.3" xref="S3.Ex2.m1.4.4.2.3.2.3.cmml">V</mi></munderover><mrow id="S3.Ex2.m1.4.4.2.2" xref="S3.Ex2.m1.4.4.2.2.cmml"><mi id="S3.Ex2.m1.4.4.2.2.4" xref="S3.Ex2.m1.4.4.2.2.4.cmml">f</mi><mo id="S3.Ex2.m1.4.4.2.2.3" xref="S3.Ex2.m1.4.4.2.2.3.cmml">â¢</mo><mrow id="S3.Ex2.m1.3.3.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.1.1.1.cmml"><mo id="S3.Ex2.m1.3.3.1.1.1.1.2" stretchy="false" xref="S3.Ex2.m1.3.3.1.1.1.1.1.cmml">(</mo><msub id="S3.Ex2.m1.3.3.1.1.1.1.1" xref="S3.Ex2.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.1.1.2" xref="S3.Ex2.m1.3.3.1.1.1.1.1.2.cmml">X</mi><mrow id="S3.Ex2.m1.3.3.1.1.1.1.1.3" xref="S3.Ex2.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.Ex2.m1.3.3.1.1.1.1.1.3.2" xref="S3.Ex2.m1.3.3.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.Ex2.m1.3.3.1.1.1.1.1.3.1" xref="S3.Ex2.m1.3.3.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.Ex2.m1.3.3.1.1.1.1.1.3.3" xref="S3.Ex2.m1.3.3.1.1.1.1.1.3.3.cmml">j</mi></mrow></msub><mo id="S3.Ex2.m1.3.3.1.1.1.1.3" stretchy="false" xref="S3.Ex2.m1.3.3.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.Ex2.m1.4.4.2.2.3a" xref="S3.Ex2.m1.4.4.2.2.3.cmml">â¢</mo><msup id="S3.Ex2.m1.4.4.2.2.2" xref="S3.Ex2.m1.4.4.2.2.2.cmml"><mrow id="S3.Ex2.m1.4.4.2.2.2.1.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.cmml"><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.2" stretchy="false" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.cmml">(</mo><mrow id="S3.Ex2.m1.4.4.2.2.2.1.1.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.cmml"><mrow id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.cmml"><mrow id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.cmml"><msubsup id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.cmml"><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.2.cmml">w</mi><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.3.cmml">i</mi><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.3.cmml">T</mi></msubsup><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.1.cmml">â¢</mo><mover accent="true" id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.cmml"><msub id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.cmml"><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.2.cmml">w</mi><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.3.cmml">j</mi></msub><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.1.cmml">~</mo></mover></mrow><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.1.cmml">+</mo><msub id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.cmml"><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.2.cmml">b</mi><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.3.cmml">i</mi></msub><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.1a" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.1.cmml">+</mo><mover accent="true" id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.cmml"><msub id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.cmml"><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.2.cmml">b</mi><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.3.cmml">j</mi></msub><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.1.cmml">~</mo></mover></mrow><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.1.cmml">âˆ’</mo><mrow id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.cmml"><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.2.cmml">l</mi><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.1.cmml">â¢</mo><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.3.cmml">o</mi><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.1a" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.1.cmml">â¢</mo><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.4" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.4.cmml">g</mi><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.1b" lspace="0.170em" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.1.cmml">â¢</mo><msub id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.cmml"><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.2.cmml">X</mi><mrow id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.cmml"><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.2" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.2.cmml">i</mi><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.1" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.1.cmml">â¢</mo><mi id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.3" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.3.cmml">j</mi></mrow></msub></mrow></mrow><mo id="S3.Ex2.m1.4.4.2.2.2.1.1.3" stretchy="false" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.cmml">)</mo></mrow><mn id="S3.Ex2.m1.4.4.2.2.2.3" xref="S3.Ex2.m1.4.4.2.2.2.3.cmml">2</mn></msup></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.4b"><apply id="S3.Ex2.m1.4.4.cmml" xref="S3.Ex2.m1.4.4"><eq id="S3.Ex2.m1.4.4.3.cmml" xref="S3.Ex2.m1.4.4.3"></eq><ci id="S3.Ex2.m1.4.4.4.cmml" xref="S3.Ex2.m1.4.4.4">ğ½</ci><apply id="S3.Ex2.m1.4.4.2.cmml" xref="S3.Ex2.m1.4.4.2"><apply id="S3.Ex2.m1.4.4.2.3.cmml" xref="S3.Ex2.m1.4.4.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.3.1.cmml" xref="S3.Ex2.m1.4.4.2.3">subscript</csymbol><apply id="S3.Ex2.m1.4.4.2.3.2.cmml" xref="S3.Ex2.m1.4.4.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.3.2.1.cmml" xref="S3.Ex2.m1.4.4.2.3">superscript</csymbol><sum id="S3.Ex2.m1.4.4.2.3.2.2.cmml" xref="S3.Ex2.m1.4.4.2.3.2.2"></sum><ci id="S3.Ex2.m1.4.4.2.3.2.3.cmml" xref="S3.Ex2.m1.4.4.2.3.2.3">ğ‘‰</ci></apply><apply id="S3.Ex2.m1.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2"><eq id="S3.Ex2.m1.2.2.2.3.cmml" xref="S3.Ex2.m1.2.2.2.3"></eq><list id="S3.Ex2.m1.2.2.2.4.1.cmml" xref="S3.Ex2.m1.2.2.2.4.2"><ci id="S3.Ex2.m1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1">ğ‘–</ci><ci id="S3.Ex2.m1.2.2.2.2.cmml" xref="S3.Ex2.m1.2.2.2.2">ğ‘—</ci></list><cn id="S3.Ex2.m1.2.2.2.5.cmml" type="integer" xref="S3.Ex2.m1.2.2.2.5">1</cn></apply></apply><apply id="S3.Ex2.m1.4.4.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2"><times id="S3.Ex2.m1.4.4.2.2.3.cmml" xref="S3.Ex2.m1.4.4.2.2.3"></times><ci id="S3.Ex2.m1.4.4.2.2.4.cmml" xref="S3.Ex2.m1.4.4.2.2.4">ğ‘“</ci><apply id="S3.Ex2.m1.3.3.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.Ex2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.2">ğ‘‹</ci><apply id="S3.Ex2.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.3"><times id="S3.Ex2.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.3.1"></times><ci id="S3.Ex2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.3.2">ğ‘–</ci><ci id="S3.Ex2.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.Ex2.m1.3.3.1.1.1.1.1.3.3">ğ‘—</ci></apply></apply><apply id="S3.Ex2.m1.4.4.2.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.2.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2">superscript</csymbol><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1"><minus id="S3.Ex2.m1.4.4.2.2.2.1.1.1.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.1"></minus><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2"><plus id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.1"></plus><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2"><times id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.1"></times><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2">subscript</csymbol><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2">superscript</csymbol><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.2">ğ‘¤</ci><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.2.3">ğ‘‡</ci></apply><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.2.3">ğ‘–</ci></apply><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3"><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.1">~</ci><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2">subscript</csymbol><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.2">ğ‘¤</ci><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.2.3.2.3">ğ‘—</ci></apply></apply></apply><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3">subscript</csymbol><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.2">ğ‘</ci><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.3.3">ğ‘–</ci></apply><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4"><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.1">~</ci><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2">subscript</csymbol><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.2">ğ‘</ci><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.2.4.2.3">ğ‘—</ci></apply></apply></apply><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3"><times id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.1"></times><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.2">ğ‘™</ci><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.3">ğ‘œ</ci><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.4.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.4">ğ‘”</ci><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5"><csymbol cd="ambiguous" id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5">subscript</csymbol><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.2">ğ‘‹</ci><apply id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3"><times id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.1.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.1"></times><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.2.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.2">ğ‘–</ci><ci id="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.3.cmml" xref="S3.Ex2.m1.4.4.2.2.2.1.1.1.3.5.3.3">ğ‘—</ci></apply></apply></apply></apply><cn id="S3.Ex2.m1.4.4.2.2.2.3.cmml" type="integer" xref="S3.Ex2.m1.4.4.2.2.2.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.4c">J=\sum^{V}_{i,j=1}f(X_{ij})(w^{T}_{i}\tilde{w_{j}}+b_{i}+\tilde{b_{j}}-log\,X_%
{ij})^{2}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.4d">italic_J = âˆ‘ start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i , italic_j = 1 end_POSTSUBSCRIPT italic_f ( italic_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) ( italic_w start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT over~ start_ARG italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG + italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT + over~ start_ARG italic_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_ARG - italic_l italic_o italic_g italic_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.13">where <math alttext="V" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">italic_V</annotation></semantics></math> is the number of words in Gloveâ€™s vocabulary, <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mi id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><ci id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">italic_i</annotation></semantics></math> and <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_j</annotation></semantics></math> are identification numbers for two words, <math alttext="w_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">w</mi><mi id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">ğ‘¤</ci><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">w_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">italic_w start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="b_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">b</mi><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">ğ‘</ci><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">b_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">italic_b start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are the vector and the bias for word <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.1"><semantics id="S3.SS2.p3.6.m6.1a"><mi id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><ci id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.1d">italic_i</annotation></semantics></math>, <math alttext="w_{j}" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m7.1"><semantics id="S3.SS2.p3.7.m7.1a"><msub id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.2" xref="S3.SS2.p3.7.m7.1.1.2.cmml">w</mi><mi id="S3.SS2.p3.7.m7.1.1.3" xref="S3.SS2.p3.7.m7.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2">ğ‘¤</ci><ci id="S3.SS2.p3.7.m7.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">w_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.7.m7.1d">italic_w start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="b_{j}" class="ltx_Math" display="inline" id="S3.SS2.p3.8.m8.1"><semantics id="S3.SS2.p3.8.m8.1a"><msub id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">b</mi><mi id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml">j</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">ğ‘</ci><ci id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3">ğ‘—</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">b_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.8.m8.1d">italic_b start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> are the vector and bias for word <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p3.9.m9.1"><semantics id="S3.SS2.p3.9.m9.1a"><mi id="S3.SS2.p3.9.m9.1.1" xref="S3.SS2.p3.9.m9.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.1b"><ci id="S3.SS2.p3.9.m9.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.9.m9.1d">italic_j</annotation></semantics></math>, <math alttext="X_{ij}" class="ltx_Math" display="inline" id="S3.SS2.p3.10.m10.1"><semantics id="S3.SS2.p3.10.m10.1a"><msub id="S3.SS2.p3.10.m10.1.1" xref="S3.SS2.p3.10.m10.1.1.cmml"><mi id="S3.SS2.p3.10.m10.1.1.2" xref="S3.SS2.p3.10.m10.1.1.2.cmml">X</mi><mrow id="S3.SS2.p3.10.m10.1.1.3" xref="S3.SS2.p3.10.m10.1.1.3.cmml"><mi id="S3.SS2.p3.10.m10.1.1.3.2" xref="S3.SS2.p3.10.m10.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p3.10.m10.1.1.3.1" xref="S3.SS2.p3.10.m10.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p3.10.m10.1.1.3.3" xref="S3.SS2.p3.10.m10.1.1.3.3.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m10.1b"><apply id="S3.SS2.p3.10.m10.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.1.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1">subscript</csymbol><ci id="S3.SS2.p3.10.m10.1.1.2.cmml" xref="S3.SS2.p3.10.m10.1.1.2">ğ‘‹</ci><apply id="S3.SS2.p3.10.m10.1.1.3.cmml" xref="S3.SS2.p3.10.m10.1.1.3"><times id="S3.SS2.p3.10.m10.1.1.3.1.cmml" xref="S3.SS2.p3.10.m10.1.1.3.1"></times><ci id="S3.SS2.p3.10.m10.1.1.3.2.cmml" xref="S3.SS2.p3.10.m10.1.1.3.2">ğ‘–</ci><ci id="S3.SS2.p3.10.m10.1.1.3.3.cmml" xref="S3.SS2.p3.10.m10.1.1.3.3">ğ‘—</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m10.1c">X_{ij}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.10.m10.1d">italic_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT</annotation></semantics></math> is the number of times word <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p3.11.m11.1"><semantics id="S3.SS2.p3.11.m11.1a"><mi id="S3.SS2.p3.11.m11.1.1" xref="S3.SS2.p3.11.m11.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m11.1b"><ci id="S3.SS2.p3.11.m11.1.1.cmml" xref="S3.SS2.p3.11.m11.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m11.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.11.m11.1d">italic_i</annotation></semantics></math> occurred in the context of word <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p3.12.m12.1"><semantics id="S3.SS2.p3.12.m12.1a"><mi id="S3.SS2.p3.12.m12.1.1" xref="S3.SS2.p3.12.m12.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m12.1b"><ci id="S3.SS2.p3.12.m12.1.1.cmml" xref="S3.SS2.p3.12.m12.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m12.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.12.m12.1d">italic_j</annotation></semantics></math>, and <math alttext="f" class="ltx_Math" display="inline" id="S3.SS2.p3.13.m13.1"><semantics id="S3.SS2.p3.13.m13.1a"><mi id="S3.SS2.p3.13.m13.1.1" xref="S3.SS2.p3.13.m13.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.13.m13.1b"><ci id="S3.SS2.p3.13.m13.1.1.cmml" xref="S3.SS2.p3.13.m13.1.1">ğ‘“</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.13.m13.1c">f</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.13.m13.1d">italic_f</annotation></semantics></math> is a weighting function for the co-occurrences.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.7">We chose Glove due to it is superiority among other language models, such as the skip-gram and continuous bag-of-words (CBOW) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib17" title="">17</a>]</cite>, and for providing immediate text representation without fine-tuning. However, Glove works at the word level, accordingly, the pre-trained embeddings <math alttext="E^{d}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><msup id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">E</mi><mi id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">d</mi></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">superscript</csymbol><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">ğ¸</ci><ci id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">E^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_E start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> represent each word in Gloveâ€™s vocabulary with vectors of size <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><mi id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><ci id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">italic_d</annotation></semantics></math> dimensions. In our implementation, Glove encodes each node as <math alttext="n\times d" class="ltx_Math" display="inline" id="S3.SS2.p4.3.m3.1"><semantics id="S3.SS2.p4.3.m3.1a"><mrow id="S3.SS2.p4.3.m3.1.1" xref="S3.SS2.p4.3.m3.1.1.cmml"><mi id="S3.SS2.p4.3.m3.1.1.2" xref="S3.SS2.p4.3.m3.1.1.2.cmml">n</mi><mo id="S3.SS2.p4.3.m3.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p4.3.m3.1.1.1.cmml">Ã—</mo><mi id="S3.SS2.p4.3.m3.1.1.3" xref="S3.SS2.p4.3.m3.1.1.3.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.3.m3.1b"><apply id="S3.SS2.p4.3.m3.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1"><times id="S3.SS2.p4.3.m3.1.1.1.cmml" xref="S3.SS2.p4.3.m3.1.1.1"></times><ci id="S3.SS2.p4.3.m3.1.1.2.cmml" xref="S3.SS2.p4.3.m3.1.1.2">ğ‘›</ci><ci id="S3.SS2.p4.3.m3.1.1.3.cmml" xref="S3.SS2.p4.3.m3.1.1.3">ğ‘‘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.3.m3.1c">n\times d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.3.m3.1d">italic_n Ã— italic_d</annotation></semantics></math> two-dimensional vector matrix, where n is the number of words to be considered in each node. When the number of words in a node is greater than <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p4.4.m4.1"><semantics id="S3.SS2.p4.4.m4.1a"><mi id="S3.SS2.p4.4.m4.1.1" xref="S3.SS2.p4.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.4.m4.1b"><ci id="S3.SS2.p4.4.m4.1.1.cmml" xref="S3.SS2.p4.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.4.m4.1d">italic_n</annotation></semantics></math>, we omit the words of index <math alttext="(n+1)" class="ltx_Math" display="inline" id="S3.SS2.p4.5.m5.1"><semantics id="S3.SS2.p4.5.m5.1a"><mrow id="S3.SS2.p4.5.m5.1.1.1" xref="S3.SS2.p4.5.m5.1.1.1.1.cmml"><mo id="S3.SS2.p4.5.m5.1.1.1.2" stretchy="false" xref="S3.SS2.p4.5.m5.1.1.1.1.cmml">(</mo><mrow id="S3.SS2.p4.5.m5.1.1.1.1" xref="S3.SS2.p4.5.m5.1.1.1.1.cmml"><mi id="S3.SS2.p4.5.m5.1.1.1.1.2" xref="S3.SS2.p4.5.m5.1.1.1.1.2.cmml">n</mi><mo id="S3.SS2.p4.5.m5.1.1.1.1.1" xref="S3.SS2.p4.5.m5.1.1.1.1.1.cmml">+</mo><mn id="S3.SS2.p4.5.m5.1.1.1.1.3" xref="S3.SS2.p4.5.m5.1.1.1.1.3.cmml">1</mn></mrow><mo id="S3.SS2.p4.5.m5.1.1.1.3" stretchy="false" xref="S3.SS2.p4.5.m5.1.1.1.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.5.m5.1b"><apply id="S3.SS2.p4.5.m5.1.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1.1"><plus id="S3.SS2.p4.5.m5.1.1.1.1.1.cmml" xref="S3.SS2.p4.5.m5.1.1.1.1.1"></plus><ci id="S3.SS2.p4.5.m5.1.1.1.1.2.cmml" xref="S3.SS2.p4.5.m5.1.1.1.1.2">ğ‘›</ci><cn id="S3.SS2.p4.5.m5.1.1.1.1.3.cmml" type="integer" xref="S3.SS2.p4.5.m5.1.1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.5.m5.1c">(n+1)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.5.m5.1d">( italic_n + 1 )</annotation></semantics></math> and greater. In contrast, for nodes with number of words below <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.p4.6.m6.1"><semantics id="S3.SS2.p4.6.m6.1a"><mi id="S3.SS2.p4.6.m6.1.1" xref="S3.SS2.p4.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.6.m6.1b"><ci id="S3.SS2.p4.6.m6.1.1.cmml" xref="S3.SS2.p4.6.m6.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.6.m6.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.6.m6.1d">italic_n</annotation></semantics></math>, we append embeddings of zeros to keep the embeddings consistent; each padding embedding has <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.p4.7.m7.1"><semantics id="S3.SS2.p4.7.m7.1a"><mi id="S3.SS2.p4.7.m7.1.1" xref="S3.SS2.p4.7.m7.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.7.m7.1b"><ci id="S3.SS2.p4.7.m7.1.1.cmml" xref="S3.SS2.p4.7.m7.1.1">ğ‘‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.7.m7.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.7.m7.1d">italic_d</annotation></semantics></math> vectors.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">KG node names usually consist of multiple words. For instance, in Freebase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib2" title="">2</a>]</cite>, the average entity name length is 2.7 words. Although multi-word terms and sentences can be represented efficiently in MLMs and LLMs, PLMs still can present a contextualized encoding by employing a Recurrent Neural Network (RNN). Consequently, we employ a bidirectional long-short term memory (LSTM) layer in our modelâ€™s neural network. RNNs are capable of encoding the latent features in text sequences <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib21" title="">21</a>]</cite>. For the Out-of-Vocabulary (OOV) words problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib37" title="">37</a>]</cite>, we find the longest possible match for a word in Gloveâ€™s vocabulary or take the word letter embeddings average in the worst case. However, OOV words still occupy less than 2% for most datasets when using Glove, due to its large vocabulary.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">We enhance the recognition of the relation direction in the input by appending additional vector at the top of each nodeâ€™s representation, this approach has been proven to help predicting the proper relations based on the direction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib1" title="">1</a>]</cite>. The head node uses a fixed value in the added vector and the tail node uses the same value but multiplied by -1.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Relation Prediction</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The architecture of our modelâ€™s neural network ensures the utilization of the latent features in the combined node embeddings.
The input embeddings for each pair of nodes consist of a head node representation concatenated with a tail node representation. Each nodeâ€™s representation consists of the textual embeddings concatenated with the structural embeddings as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S2.F1" title="Figure 1 â€£ 2.3 Hybrid Approaches â€£ 2 Background and Related Work â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">1</span></a>. For each pair of nodes, the bidirectional LSTM layer generates new contextualized embeddings, that are forwarded to an attention layer to selectively focus on certain parts of the input data, allowing the model to weigh different inputs differently based on their relevance to the task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib28" title="">28</a>]</cite>. In the following dropout layer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib12" title="">12</a>]</cite> we enhance the generalization performance. Ultimately, the output layer uses the Sigmoid function <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib10" title="">10</a>]</cite> to give the probability of each relation. Our model computes the neural network training loss using the cross entropy loss function.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We evaluated our model on FB15K, a subset of the FreeBase dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib2" title="">2</a>]</cite>. FreeBase has been maintained by Google for several years and has been widely used for KG-related tasks; its content includes data collected from several sources such as Wikipedia. The last release of the complete dataset holds about 1.9 billion triples <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_text ltx_font_italic" id="footnote2.1">https://developers.google.com/freebase/</span></span></span></span>. The FB15K subset holds 1,345 relations and 14,951 entities. The number of triples in the training portion is 483,142, the validation portion has 50,000 triples, and the testing portion has 59,071 triples.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">We used PyTorch to train our model. The used hardware consisted of NVIDIA A100-SXM-80GB GPU node, a main memory of 50GB, and AMD EPYC MILAN (3rd gen) CPU processor; we utilized 8 cores of the CPU. Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S4.T1" title="Table 1 â€£ 4 Experiments â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">1</span></a> shows our training settings. We used Glove 300 dimensional word embeddings trained on 6 Billion tokens with a 400,000 words vocabulary. For the Node2Vec training, we used balanced settings in the biased walks. Specifically, we used equal parameters for the breadth-first and the depth-first sampling. For the model training optimization, we employed Adam algorithm. In the following sections we explain the evaluation metrics, the comparison models, the main results analysis, and the model components ablation study.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The training settings.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1">Settings</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.2">Value</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.1">Training Epochs</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T1.1.2.1.2">50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.1">Early stopping epochs</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.3.2.2">5</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T1.1.4.3.1">Batch size</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.4.3.2">32</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.4.1">LSTM features in the hidden state</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.5.4.2">400</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.5.1">LSTM recurrent layers</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.6.5.2">2</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.6.1">Dropout probability</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.7.6.2">15%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<td class="ltx_td ltx_align_left" id="S4.T1.1.8.7.1">Learning rate</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.8.7.2">0.0008</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<td class="ltx_td ltx_align_left" id="S4.T1.1.9.8.1">Optimizer decay</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.9.8.2">35%</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<td class="ltx_td ltx_align_left" id="S4.T1.1.10.9.1">Node representation padding</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.10.9.2">40</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<td class="ltx_td ltx_align_left" id="S4.T1.1.11.10.1">Node2Vec walk length</td>
<td class="ltx_td ltx_align_left" id="S4.T1.1.11.10.2">50</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.1.12.11.1">Node2Vec node walks</td>
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T1.1.12.11.2">50</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation Metrics</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We evaluate our model on two commonly used metrics. First, the mean rank of the ground truth relation in the predicted relation probabilities set. Items in the set follow a descending order. Accordingly, the lower the mean rank the better the result. Since any pair of nodes may incur multiple correct relations, the rank of a correct relation should be decremented by the number of the valid relations that appeared on top of the correct one. This evaluation behaviour is called the filtered settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib3" title="">3</a>]</cite> and is reported in our experiments along with the raw mean rank. The second evaluation metric is called Hits@1, that is the ratio of ground truth relations appeared as the first item in the ordered relations probability values. We also report the filtered settings for this metric.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Comparison Models</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">We perform a comparison with models that used the structural information and the textual information in the RP task. Although some models showed significant results in the same task, we excluded these results from our comparison because of the utilization of node information other than the node name, such as the description or the node entity type. For instance TaRP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib5" title="">5</a>]</cite> employed the node type information to achieve the reported performance. Nevertheless, most datasets do not have the node type information provided by default and that requires external knowledge bases with added load to retrieve and encode the type information. Additionally, TaRP did not generate embeddings for nodes but instead depended on existing models to do so. Similarly, the work in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib26" title="">26</a>]</cite> used the entity description of long text to achieve similar results. In the following we show the models we consider in our comparison.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">TransE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib3" title="">3</a>]</cite> was originally evaluated on the link prediction task. However, several works used the method for the RP task, and we use the reported rank and hits results in our comparison. TransE utilized the structural information to represent nodes. TransR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib15" title="">15</a>]</cite> was evaluated on the link prediction and triple classification tasks. Similarly, other works reported TransRâ€™s performance in the RP task. TransR also utilized the structural information to represent nodes. KG-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib34" title="">34</a>]</cite> reported their relation prediction performance in the FB15K dataset. As described in Section <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S2" title="2 Background and Related Work â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">2</span></a>, the modelâ€™s main design targeted the link prediction task. KG-BERT was also evaluated on the triple classification task. Shallom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib7" title="">7</a>]</cite> tackled the RP task and utilized neural networks embedding layers to represent nodes.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The results of our model and the comparison models on the FB15K dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.2">Mean Rank</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.3">Hits@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.4">Filtered Mean Rank</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T2.1.1.1.5">Filtered Hits@1</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T2.1.2.1.1">TransE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4">2.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.5">84</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T2.1.3.2.1">TransR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib15" title="">15</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.2">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3">42</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4">2.1</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.5">91</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<td class="ltx_td ltx_align_left" id="S4.T2.1.4.3.1">KG-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib34" title="">34</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.2">1.69</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.3">69</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4">1.25</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.5.1">96</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<td class="ltx_td ltx_align_left" id="S4.T2.1.5.4.1">Shallom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#bib.bib7" title="">7</a>]</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.2">1.59</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.3">73</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.4">1.26</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.5">95</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T2.1.6.5.1">Ours</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.6.5.2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.2.1">1.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.6.5.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.3.1">74</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.6.5.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.4.1">1.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T2.1.6.5.5">94</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The results of RPEST variants on the FB15K dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.1">Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.2">Mean Rank</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.3">Hits@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.4">Filtered Mean Rank</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.5">Filtered Hits@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.1.6">Epoch Time<sub class="ltx_sub" id="S4.T3.1.1.1.6.1">min</sub>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.2.1.1">RPEST<sub class="ltx_sub" id="S4.T3.1.2.1.1.1">Glove</sub>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.2">1.63</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.3">68</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.4">1.26</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.1.5.1">94</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.2.1.6.1">5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.2">
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.2.1">RPEST<sub class="ltx_sub" id="S4.T3.1.3.2.1.1">BERT</sub>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.2">1.70</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.3">67</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.4">1.31</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.5">92</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.2.6">13</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_b" id="S4.T3.1.4.3.1">RPEST</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.4.3.2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.2.1">1.53</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.4.3.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.3.1">74</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.4.3.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.4.1">1.18</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.4.3.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.4.3.5.1">94</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.1.4.3.6">6</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Main Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We show our main results in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S4.T2" title="Table 2 â€£ 4.2 Comparison Models â€£ 4 Experiments â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">2</span></a>. The results show superiority of our relation prediction model compared to the best results in other RP models. However, KG-BERT has equivalent filtered Hits@1 score. Nevertheless, our model beats the scores of the same model on the remaining metrics with good margin. The mean rank scores for TransE and TransR were not reported or found in any other study. We reason our modelâ€™s superiority by the combination of the structural and textual details for every node. More precisely, we find the efficient utilization of the textual content leading the performance to this level as we show in the ablation study.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">To evaluate the effectiveness of our modelâ€™s units, we created two variants to observe the model results after excluding some of its modules. Particularly, we created a variant that does not employ the nodesâ€™ structural information in the input. Therefore, we excluded the usage of Node2Vec in our model and we only kept Glove text encoder by creating a variant named RPEST<sub class="ltx_sub" id="S4.SS4.p1.1.1">Glove</sub>. Furthermore, we evaluated our choice of language models by replacing Glove with BERT language model in a variant named RPEST<sub class="ltx_sub" id="S4.SS4.p1.1.2">BERT</sub>. We show the evaluation metric scores of the different variants in Table <a class="ltx_ref" href="https://arxiv.org/html/2404.16206v1#S4.T3" title="Table 3 â€£ 4.2 Comparison Models â€£ 4 Experiments â€£ Knowledge Graph Completion using Structural and Textual EmbeddingsAccepted in AIAI 2024 - 20th International Conference on Artificial Intelligence Applications and Innovations"><span class="ltx_text ltx_ref_tag">3</span></a>. The table also shows the epoch time in minutes for each variant, and we rely on the time values to emphasize the performance difference.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">The lower scores of the RPEST<sub class="ltx_sub" id="S4.SS4.p2.1.1">Glove</sub> variant proofed the importance of employing the structural information in representing nodes. However, the variant had the fastest performance due to the reduced embeddings size after excluding Node2Vec vectors. On the other hand, the Glove variant scored better results compared to the RPEST<sub class="ltx_sub" id="S4.SS4.p2.1.2">BERT</sub> variant. We reason that by the efficient exploitation of the latent features in Gloveâ€™s embeddings using the bi-directional LSTM layer along with the attention layer and the employment of the additional node type vector. The node type vector enhanced the relation direction detection in our model. Regarding the slowest performing variant, the large embeddings size led to the reported speed in BERTâ€™s variant.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper we explain that the relation prediction task has equal importance to the link prediction task. We also show that the usage of nodesâ€™ content information along with the structural information is important to have efficient graph representation. Accordingly, we propose a relation prediction model that utilizes a walks-based algorithm to retrieve the node structural information along with the utilization of pre-trained language models to represent the textual content of the nodes. We argue that masked language models are costly in terms of computational resources and pre-trained language models can achieve equivalent results with effective exploitation. Our model does not require the nodesâ€™ description and shows competitive results when compared to the state of the art models in the same task.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Alqaaidi, S.K., Bozorgi, E., Kochut, K.J.: Multiple relations classification using imbalanced predictions adaptation. arXiv preprint arXiv:2309.13718 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bollacker, K., Evans, C., Paritosh, P., Sturge, T., Taylor, J.: Freebase: a collaboratively created graph database for structuring human knowledge. In: Proceedings of the 2008 ACM SIGMOD international conference on Management of data. pp. 1247â€“1250 (2008)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., Yakhnenko, O.: Translating embeddings for modeling multi-relational data. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib3.1.1">26</span> (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, Z., Wang, Y., Zhao, B., Cheng, J., Zhao, X., Duan, Z.: Knowledge graph completion: A review. Ieee Access <span class="ltx_text ltx_font_bold" id="bib.bib4.1.1">8</span>, 192435â€“192456 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Cui, Z., Kapanipathi, P., Talamadupula, K., Gao, T., Ji, Q.: Type-augmented relation prediction in knowledge graphs. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol.Â 35, pp. 7151â€“7159 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Daza, D., Cochez, M., Groth, P.: Inductive entity representations from text via link prediction. In: Proceedings of the Web Conference 2021. pp. 798â€“808 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Demir, C., Moussallem, D., Ngomo, A.C.N.: A shallow neural model for relation prediction. In: 2021 IEEE 15th International Conference on Semantic Computing (ICSC). pp. 179â€“182. IEEE (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Grover, A., Leskovec, J.: node2vec: Scalable feature learning for networks. In: Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 855â€“864 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Han, J., Moraga, C.: The influence of the sigmoid function parameters on the speed of backpropagation learning. In: International workshop on artificial neural networks. pp. 195â€“201. Springer (1995)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
He, J., Jia, L., Wang, L., Li, X., Xu, X.: Mocosa: Momentum contrast for knowledge graph completion with structure-augmented pre-trained language models. arXiv preprint arXiv:2308.08204 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Hinton, G.E., Srivastava, N., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.R.: Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580 (2012)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Hu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B., Catasta, M., Leskovec, J.: Open graph benchmark: Datasets for machine learning on graphs. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">33</span>, 22118â€“22133 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Lin, Y., Liu, Z., Luan, H., Sun, M., Rao, S., Liu, S.: Modeling relation paths for representation learning of knowledge bases. arXiv preprint arXiv:1506.00379 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Lin, Y., Liu, Z., Sun, M., Liu, Y., Zhu, X.: Learning entity and relation embeddings for knowledge graph completion. In: Proceedings of the AAAI conference on artificial intelligence. vol.Â 29 (2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Mikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Pennington, J., Socher, R., Manning, C.D.: Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP). pp. 1532â€“1543 (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: Online learning of social representations. In: Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 701â€“710 (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., etÂ al.: Language models are unsupervised multitask learners. OpenAI blog <span class="ltx_text ltx_font_bold" id="bib.bib20.1.1">1</span>(8), Â 9 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Sak, H., Senior, A., Beaufays, F.: Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition. arXiv preprint arXiv:1402.1128 (2014)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Shen, J., Wang, C., Gong, L., Song, D.: Joint language semantic and structure embedding for knowledge graph completion. arXiv preprint arXiv:2209.08721 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Shen, T., Zhang, F., Cheng, J.: A comprehensive overview of knowledge graph completion. Knowledge-Based Systems p. 109597 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Sheu, H.S., Li, S.: Context-aware graph embedding for session-based news recommendation. In: Proceedings of the 14th ACM Conference on Recommender Systems. pp. 657â€“662 (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Sun, Z., Deng, Z.H., Nie, J.Y., Tang, J.: Rotate: Knowledge graph embedding by relational rotation in complex space. arXiv preprint arXiv:1902.10197 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Tagawa, Y., Taniguchi, M., Miura, Y., Taniguchi, T., Ohkuma, T., Yamamoto, T., Nemoto, K.: Relation prediction for unseen-entities using entity-word graphs. In: Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13). pp. 11â€“16 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., etÂ al.: Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv. org/abs/2307.09288 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Å., Polosukhin, I.: Attention is all you need. Advances in neural information processing systems <span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">30</span> (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Wang, B., Shen, T., Long, G., Zhou, T., Wang, Y., Chang, Y.: Structure-augmented text representation learning for efficient knowledge graph completion. In: Proceedings of the Web Conference 2021. pp. 1737â€“1748 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Wang, L., Zhao, W., Wei, Z., Liu, J.: Simkgc: Simple contrastive knowledge graph completion with pre-trained language models. arXiv preprint arXiv:2203.02167 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Wang, X., Gao, T., Zhu, Z., Zhang, Z., Liu, Z., Li, J., Tang, J.: Kepler: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics <span class="ltx_text ltx_font_bold" id="bib.bib31.1.1">9</span>, 176â€“194 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Xu, J., Chen, K., Qiu, X., Huang, X.: Knowledge graph representation with jointly structural and textual encoding. arXiv preprint arXiv:1611.08661 (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Yani, M., Krisnadhi, A.A.: Challenges, techniques, and trends of simple knowledge graph question answering: a survey. Information <span class="ltx_text ltx_font_bold" id="bib.bib33.1.1">12</span>(7), Â 271 (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Yao, L., Mao, C., Luo, Y.: Kg-bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193 (2019)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Youn, J., Tagkopoulos, I.: Kglm: Integrating knowledge graph structure in language models for link prediction. arXiv preprint arXiv:2211.02744 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Zha, H., Chen, Z., Yan, X.: Inductive relation prediction by bert. In: Proceedings of the AAAI Conference on Artificial Intelligence. vol.Â 36, pp. 5923â€“5931 (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Zhuang, Z., Liang, Z., Rao, Y., Xie, H., Wang, F.L.: Out-of-vocabulary word embedding learning based on reading comprehension mechanism. Natural Language Processing Journal <span class="ltx_text ltx_font_bold" id="bib.bib37.1.1">5</span>, 100038 (2023)

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Apr 30 19:16:25 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
