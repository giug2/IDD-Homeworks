<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation</title>
<!--Generated on Mon May 20 15:34:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<!--Document created on 1.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.12119v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S1" title="In Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2" title="In Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Preliminaries</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS1" title="In 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Task Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS2" title="In 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Differentiable Search Index (DSI)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS3" title="In 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Item Indexing: LLMs Show Sufficient Item Content Knowledge</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS3.SSS1" title="In 2.3 Item Indexing: LLMs Show Sufficient Item Content Knowledge ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Observation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS3.SSS2" title="In 2.3 Item Indexing: LLMs Show Sufficient Item Content Knowledge ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Impact</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS4" title="In 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4 </span>Item Recommendation: LLMs Show Severe Distribution Misalignment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS4.SSS1" title="In 2.4 Item Recommendation: LLMs Show Severe Distribution Misalignment ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.1 </span>Observation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS4.SSS2" title="In 2.4 Item Recommendation: LLMs Show Severe Distribution Misalignment ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.4.2 </span>Impact</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3" title="In Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Framework</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS1" title="In 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Overview</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS2" title="In 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span><span class="ltx_text ltx_font_typewriter">Reindex</span> Step: Single-Token Items in LLMs</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS2.SSS1" title="In 3.2 Reindex Step: Single-Token Items in LLMs ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Identify Item Indices.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS2.SSS2" title="In 3.2 Reindex Step: Single-Token Items in LLMs ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Aggregate Multi-Token Embeddings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS2.SSS3" title="In 3.2 Reindex Step: Single-Token Items in LLMs ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Learning Process</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS3" title="In 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span><span class="ltx_text ltx_font_typewriter">Adapt</span> Step: Item Probabilities Adjustment</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS3.SSS1" title="In 3.3 Adapt Step: Item Probabilities Adjustment ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Bias Term Adjustment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS3.SSS2" title="In 3.3 Adapt Step: Item Probabilities Adjustment ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Traditional RecSys Gating</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.SS3.SSS3" title="In 3.3 Adapt Step: Item Probabilities Adjustment ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Learning Process</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4" title="In Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS1" title="In 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experiment Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS1.SSS1" title="In 4.1 Experiment Setup ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS1.SSS2" title="In 4.1 Experiment Setup ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS1.SSS3" title="In 4.1 Experiment Setup ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS2" title="In 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>General CRS Performance</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS2.SSS1" title="In 4.2 General CRS Performance ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Baseline Performance.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS2.SSS2" title="In 4.2 General CRS Performance ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Ours vs. Baselines.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS3" title="In 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Effectiveness of the <span class="ltx_text ltx_font_typewriter">Reindex</span> Step</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS3.SSS1" title="In 4.3 Effectiveness of the Reindex Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.1 </span>Experiment Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS3.SSS2" title="In 4.3 Effectiveness of the Reindex Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.2 </span>Embedding vs. Aggregator</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS3.SSS3" title="In 4.3 Effectiveness of the Reindex Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3.3 </span>Different Aggregators</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS4" title="In 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Effectiveness of the <span class="ltx_text ltx_font_typewriter">Adapt</span> Step</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS4.SSS1" title="In 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.1 </span>Component Analysis.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS4.SSS2" title="In 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.2 </span>Impact of Bias Term Types</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS4.SSS3" title="In 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4.3 </span>Impact of RecSys Model Types</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS5" title="In 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Discussions</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS5.SSS1" title="In 4.5 Discussions ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.1 </span>Conversational Recommendation Responses</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS5.SSS2" title="In 4.5 Discussions ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5.2 </span>Comparison with Proprietary Models</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S5" title="In Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S5.SS1" title="In 5 Related Work ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Conversational Recommendation (CRS)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S5.SS2" title="In 5 Related Work ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Large Language Models (LLMs)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S5.SS3" title="In 5 Related Work ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>LLMs for Recommendation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S6" title="In Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A1" title="In Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>More Details of Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A1.SS1" title="In Appendix A More Details of Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Baseline Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A1.SS2" title="In Appendix A More Details of Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A2" title="In Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Details of Prompts for LLMs</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A2.SS1" title="In Appendix B Details of Prompts for LLMs ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Prompt(s) for Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A2.SS2" title="In Appendix B Details of Prompts for LLMs ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Prompt(s) for Generation</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhankui He<sup class="ltx_sup" id="id12.2.id1"><span class="ltx_text ltx_font_italic" id="id12.2.id1.1">1,∗</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhouhang Xie<sup class="ltx_sup" id="id13.2.id1">1</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Harald Steck<sup class="ltx_sup" id="id14.2.id1">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dawen Liang<sup class="ltx_sup" id="id15.2.id1">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rahul Jha<sup class="ltx_sup" id="id16.2.id1">2</sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nathan Kallus<sup class="ltx_sup" id="id17.2.id1"><span class="ltx_text ltx_font_italic" id="id17.2.id1.1">2,3</span></sup>
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julian McAuley<sup class="ltx_sup" id="id18.2.id1">1</sup>
</span></span>
</div>
<div class="ltx_dates">(<sup class="ltx_sup" id="id19.id1">1</sup>UC San Diego <sup class="ltx_sup" id="id20.id2">2</sup>Netflix <sup class="ltx_sup" id="id21.id3">3</sup>Cornell University
<br class="ltx_break"/><sup class="ltx_sup" id="id22.id4">∗</sup><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">zhh004@ucsd.edu</span>
)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id23.id1">Large language models (LLMs) are revolutionizing conversational recommender systems by adeptly indexing item content, understanding complex conversational contexts, and generating relevant item titles. However, controlling the distribution of recommended items remains a challenge. This leads to suboptimal performance due to the failure to capture rapidly changing data distributions, such as item popularity, on targeted conversational recommendation platforms. In conversational recommendation,
LLMs recommend items by generating the titles (as multiple tokens) autoregressively, making it difficult to obtain and control the recommendations over all items.
Thus, we propose a <em class="ltx_emph ltx_font_italic" id="id23.id1.1">Reindex-Then-Adapt (RTA)</em> framework, which converts multi-token item titles into single tokens within LLMs, and then adjusts the probability distributions over these single-token item titles accordingly. The RTA framework marries the benefits of both LLMs and traditional recommender systems (RecSys): understanding complex queries as LLMs do; while efficiently controlling the recommended item distributions in conversational recommendations as traditional RecSys do. Our framework demonstrates improved accuracy metrics across three different conversational recommendation datasets and two adaptation settings.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="269" id="S1.F1.g1" src="extracted/5607491/figs/intro.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Representative items (<em class="ltx_emph ltx_font_italic" id="S1.F1.4.1">The Dark Knight</em> and <em class="ltx_emph ltx_font_italic" id="S1.F1.5.2">Black Panther</em>) demonstrate popularity misalignments between the dataset (ReDIAL <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite>) and the LLM (Llama2-7b <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib53" title="">2023</a>)</cite>). This misalignment implies a significant room for recommendation accuracy improvement. Our <em class="ltx_emph ltx_font_italic" id="S1.F1.6.3">Reindex-Then-Adapt (RTA)</em> framework addresses this gap by aligning the distributions (e.g., item popularites), leading to substantial accuracy improvements.
</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Conversational Recommender Systems <cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib14" title="">2016</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> (CRS) are an emerging recommendation task aiming to suggest relevant and personalized items via interactive dialogues between users and systems. Recently, Large Language Models <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib48" title="">2022</a>; Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib6" title="">2020b</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib18" title="">2023</a>; Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib31" title="">2023</a>)</cite> (LLMs) have demonstrated proficiency in understanding user intentions within natural language conversational contexts and exhibited substantial domain-specific knowledge (e.g., in movies). Consequently, LLMs offer distinct advantages for CRS and outperform existing non-LLM baselines <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib56" title="">2023b</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib18" title="">2023</a>)</cite>. This has garnered significant interest within the research community, positioning LLMs as an indispensable component of CRS.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this work, we first provide preliminary analysis for LLMs as conversational recommenders. In detail, we view LLMs for conversational recommendations as Differentiable Search Indexing (DSI) <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib51" title="">2022</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib10" title="">2023</a>)</cite> models, then study LLMs’ <span class="ltx_text ltx_font_bold" id="S1.p2.1.1">abilities</span> and <span class="ltx_text ltx_font_bold" id="S1.p2.1.2">limitations</span> for <span class="ltx_text ltx_font_italic" id="S1.p2.1.3">item-indexing</span> and <span class="ltx_text ltx_font_italic" id="S1.p2.1.4">item-recommendation</span> tasks:</p>
</div>
<div class="ltx_para" id="S1.p3">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Abilities</span>: LLMs have indexed numerous popular movies, potentially adequate to understand complex conversation contexts and address many movie conversational recommendation scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Limitations</span>: LLMs exhibit misalignment with data distributions from target platforms, as illustrated by item popularity in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S1.F1" title="In 1 Introduction ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Moreover, data distributions like item popularity evolve rapidly in practice, making adjusting LLMs more challenging.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We propose to overcome the aforementioned misalignment limitation by easily adjusting LLMs towards changing target distributions. <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S1.F1" title="In 1 Introduction ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a> illustrates an example with an LLM, Llama-7b <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib53" title="">2023</a>)</cite>, on the conversational recommendation dataset, ReDIAL <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite>. Despite the promising conversational recommendations by LLMs <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S1.F1" title="In 1 Introduction ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>(a) points out a lack of alignment with the data distribution of the target recommendation platform. For example, <em class="ltx_emph ltx_font_italic" id="S1.p4.1.1">The Dark Knight</em> is popular on ReDIAL <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite> but not within the LLM, while <em class="ltx_emph ltx_font_italic" id="S1.p4.1.2">Black Panther</em> presents a contrasting scenario. Our proposed approach alleviates this issue by adjusting the recommendation distributions for all target items from LLMs. <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S1.F1" title="In 1 Introduction ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>(b) shows our approach results in more aligned item popularity between LLMs and the target dataset or platform for recommended items such as <em class="ltx_emph ltx_font_italic" id="S1.p4.1.3">The Dark Knight</em> and <em class="ltx_emph ltx_font_italic" id="S1.p4.1.4">Black Panther</em>. These alignments bring additional recommendation accuracy improvements, and may have broader benefits, including controllability and fairness.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To achieve this recommendation probability distribution adjustment, there exists a technical challenge. Unlike adjusting recommendation probability distributions over all target items via tweaking the <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">logit</span> vectors in traditional RecSys, obtaining such <span class="ltx_text ltx_font_italic" id="S1.p5.1.2">logit</span> vectors from LLMs is challenging due to their <span class="ltx_text ltx_font_italic" id="S1.p5.1.3">generative retrieval</span> paradigm for CRS <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>. LLMs generate recommendations by auto-regressively producing multiple item titles (e.g., Top-10), represented by varying numbers of tokens. This process makes obtaining probability distributions over all recommended items computationally expensive, hindering subsequent control or adjustment efforts.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To overcome this challenge, we propose a <em class="ltx_emph ltx_font_italic" id="S1.p6.1.1">Reindex-Then-Adapt (RTA)</em> framework. With treating LLMs as DSI models for conversational recommendations, we first conduct a <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.p6.1.2">reindex step</span>: for original LLMs, we convert already-indexed multi-token item titles (e.g., <em class="ltx_emph ltx_font_italic" id="S1.p6.1.3">Edge of Tomorrow</em>) into single tokens (e.g., <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.4">|Edge_of_Tomorrow|</span>); then we conduct an <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S1.p6.1.5">adapt step</span>: for reindexed LLMs, the recommended item distributions can be obtained efficiently for following <span class="ltx_text ltx_font_typewriter" id="S1.p6.1.6">adapt</span> step, e.g., <span class="ltx_text ltx_font_italic" id="S1.p6.1.7">bias terms adjustment</span> or <span class="ltx_text ltx_font_italic" id="S1.p6.1.8">RecSys gating</span>.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Based on the <em class="ltx_emph ltx_font_italic" id="S1.p7.1.1">RTA</em> framework, we investigate four reindexing modules and two adaptation strategies across three CRS datasets. Our experimental results show improved recommendation accuracy in CRS. For instance, we improve the recommendation accuracy for the original Llama2-7b by 59.37% in terms of the Top-10 Hit Rate, surpassing all open-source baselines. Additionally, our studies highlight the significance of adjusting LLMs towards target distributions in CRS and provide insights into scheduling conversational recommendation modules with LLMs.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Task Formulation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.11">In CRS, a conversation is represented by <math alttext="C=(u_{t},s_{t},\mathbf{I}_{t})^{T}_{t=1}" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.3"><semantics id="S2.SS1.p1.1.m1.3a"><mrow id="S2.SS1.p1.1.m1.3.3" xref="S2.SS1.p1.1.m1.3.3.cmml"><mi id="S2.SS1.p1.1.m1.3.3.5" xref="S2.SS1.p1.1.m1.3.3.5.cmml">C</mi><mo id="S2.SS1.p1.1.m1.3.3.4" xref="S2.SS1.p1.1.m1.3.3.4.cmml">=</mo><msubsup id="S2.SS1.p1.1.m1.3.3.3" xref="S2.SS1.p1.1.m1.3.3.3.cmml"><mrow id="S2.SS1.p1.1.m1.3.3.3.3.3.3" xref="S2.SS1.p1.1.m1.3.3.3.3.3.4.cmml"><mo id="S2.SS1.p1.1.m1.3.3.3.3.3.3.4" stretchy="false" xref="S2.SS1.p1.1.m1.3.3.3.3.3.4.cmml">(</mo><msub id="S2.SS1.p1.1.m1.1.1.1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml">u</mi><mi id="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S2.SS1.p1.1.m1.3.3.3.3.3.3.5" xref="S2.SS1.p1.1.m1.3.3.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.cmml"><mi id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.2" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.2.cmml">s</mi><mi id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.3" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.3.cmml">t</mi></msub><mo id="S2.SS1.p1.1.m1.3.3.3.3.3.3.6" xref="S2.SS1.p1.1.m1.3.3.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.cmml"><mi id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.2" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.2.cmml">𝐈</mi><mi id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.3" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.3.cmml">t</mi></msub><mo id="S2.SS1.p1.1.m1.3.3.3.3.3.3.7" stretchy="false" xref="S2.SS1.p1.1.m1.3.3.3.3.3.4.cmml">)</mo></mrow><mrow id="S2.SS1.p1.1.m1.3.3.3.5" xref="S2.SS1.p1.1.m1.3.3.3.5.cmml"><mi id="S2.SS1.p1.1.m1.3.3.3.5.2" xref="S2.SS1.p1.1.m1.3.3.3.5.2.cmml">t</mi><mo id="S2.SS1.p1.1.m1.3.3.3.5.1" xref="S2.SS1.p1.1.m1.3.3.3.5.1.cmml">=</mo><mn id="S2.SS1.p1.1.m1.3.3.3.5.3" xref="S2.SS1.p1.1.m1.3.3.3.5.3.cmml">1</mn></mrow><mi id="S2.SS1.p1.1.m1.3.3.3.3.5" xref="S2.SS1.p1.1.m1.3.3.3.3.5.cmml">T</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.3b"><apply id="S2.SS1.p1.1.m1.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3"><eq id="S2.SS1.p1.1.m1.3.3.4.cmml" xref="S2.SS1.p1.1.m1.3.3.4"></eq><ci id="S2.SS1.p1.1.m1.3.3.5.cmml" xref="S2.SS1.p1.1.m1.3.3.5">𝐶</ci><apply id="S2.SS1.p1.1.m1.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.3.3.3.4.cmml" xref="S2.SS1.p1.1.m1.3.3.3">subscript</csymbol><apply id="S2.SS1.p1.1.m1.3.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.3.3.3.3.4.cmml" xref="S2.SS1.p1.1.m1.3.3.3">superscript</csymbol><vector id="S2.SS1.p1.1.m1.3.3.3.3.3.4.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3"><apply id="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.2">𝑢</ci><ci id="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.1.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.2">𝑠</ci><ci id="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.3.cmml" xref="S2.SS1.p1.1.m1.2.2.2.2.2.2.2.3">𝑡</ci></apply><apply id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.1.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.2.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.2">𝐈</ci><ci id="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.3.3.3.3">𝑡</ci></apply></vector><ci id="S2.SS1.p1.1.m1.3.3.3.3.5.cmml" xref="S2.SS1.p1.1.m1.3.3.3.3.5">𝑇</ci></apply><apply id="S2.SS1.p1.1.m1.3.3.3.5.cmml" xref="S2.SS1.p1.1.m1.3.3.3.5"><eq id="S2.SS1.p1.1.m1.3.3.3.5.1.cmml" xref="S2.SS1.p1.1.m1.3.3.3.5.1"></eq><ci id="S2.SS1.p1.1.m1.3.3.3.5.2.cmml" xref="S2.SS1.p1.1.m1.3.3.3.5.2">𝑡</ci><cn id="S2.SS1.p1.1.m1.3.3.3.5.3.cmml" type="integer" xref="S2.SS1.p1.1.m1.3.3.3.5.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.3c">C=(u_{t},s_{t},\mathbf{I}_{t})^{T}_{t=1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.3d">italic_C = ( italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT</annotation></semantics></math> involving users <math alttext="u_{t}\in\mathcal{U}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mrow id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><msub id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2.2" xref="S2.SS1.p1.2.m2.1.1.2.2.cmml">u</mi><mi id="S2.SS1.p1.2.m2.1.1.2.3" xref="S2.SS1.p1.2.m2.1.1.2.3.cmml">t</mi></msub><mo id="S2.SS1.p1.2.m2.1.1.1" xref="S2.SS1.p1.2.m2.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">𝒰</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><in id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1.1"></in><apply id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2">𝑢</ci><ci id="S2.SS1.p1.2.m2.1.1.2.3.cmml" xref="S2.SS1.p1.2.m2.1.1.2.3">𝑡</ci></apply><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">𝒰</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">u_{t}\in\mathcal{U}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ∈ caligraphic_U</annotation></semantics></math> and items <math alttext="\mathbf{I}_{t}\subseteq\mathcal{I}" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mrow id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><msub id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml"><mi id="S2.SS1.p1.3.m3.1.1.2.2" xref="S2.SS1.p1.3.m3.1.1.2.2.cmml">𝐈</mi><mi id="S2.SS1.p1.3.m3.1.1.2.3" xref="S2.SS1.p1.3.m3.1.1.2.3.cmml">t</mi></msub><mo id="S2.SS1.p1.3.m3.1.1.1" xref="S2.SS1.p1.3.m3.1.1.1.cmml">⊆</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.3.m3.1.1.3" xref="S2.SS1.p1.3.m3.1.1.3.cmml">ℐ</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><subset id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1.1"></subset><apply id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.2.1.cmml" xref="S2.SS1.p1.3.m3.1.1.2">subscript</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2.2">𝐈</ci><ci id="S2.SS1.p1.3.m3.1.1.2.3.cmml" xref="S2.SS1.p1.3.m3.1.1.2.3">𝑡</ci></apply><ci id="S2.SS1.p1.3.m3.1.1.3.cmml" xref="S2.SS1.p1.3.m3.1.1.3">ℐ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathbf{I}_{t}\subseteq\mathcal{I}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">bold_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ⊆ caligraphic_I</annotation></semantics></math> with <math alttext="T" class="ltx_Math" display="inline" id="S2.SS1.p1.4.m4.1"><semantics id="S2.SS1.p1.4.m4.1a"><mi id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝑇</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">T</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.4.m4.1d">italic_T</annotation></semantics></math> conversation turns. Each utterance <math alttext="s_{t}" class="ltx_Math" display="inline" id="S2.SS1.p1.5.m5.1"><semantics id="S2.SS1.p1.5.m5.1a"><msub id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">s</mi><mi id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2">𝑠</ci><ci id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">s_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.5.m5.1d">italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> comprises tokens <math alttext="v_{i}" class="ltx_Math" display="inline" id="S2.SS1.p1.6.m6.1"><semantics id="S2.SS1.p1.6.m6.1a"><msub id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">v</mi><mi id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2">𝑣</ci><ci id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">v_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.6.m6.1d">italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from vocabulary <math alttext="\mathcal{V}" class="ltx_Math" display="inline" id="S2.SS1.p1.7.m7.1"><semantics id="S2.SS1.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml">𝒱</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><ci id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1">𝒱</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.7.m7.1d">caligraphic_V</annotation></semantics></math>. A conversation typically involves a <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.11.1">seeker</span> and a <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.11.2">recommender</span>. As formulated in CRS studies  <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib9" title="">2019</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib65" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib57" title="">2022</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>, our goal is to learn a recommender to generate a ranked list of items <math alttext="\hat{\mathcal{I}}_{k}" class="ltx_Math" display="inline" id="S2.SS1.p1.8.m8.1"><semantics id="S2.SS1.p1.8.m8.1a"><msub id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml"><mover accent="true" id="S2.SS1.p1.8.m8.1.1.2" xref="S2.SS1.p1.8.m8.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.8.m8.1.1.2.2" xref="S2.SS1.p1.8.m8.1.1.2.2.cmml">ℐ</mi><mo id="S2.SS1.p1.8.m8.1.1.2.1" xref="S2.SS1.p1.8.m8.1.1.2.1.cmml">^</mo></mover><mi id="S2.SS1.p1.8.m8.1.1.3" xref="S2.SS1.p1.8.m8.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><apply id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.8.m8.1.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">subscript</csymbol><apply id="S2.SS1.p1.8.m8.1.1.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2"><ci id="S2.SS1.p1.8.m8.1.1.2.1.cmml" xref="S2.SS1.p1.8.m8.1.1.2.1">^</ci><ci id="S2.SS1.p1.8.m8.1.1.2.2.cmml" xref="S2.SS1.p1.8.m8.1.1.2.2">ℐ</ci></apply><ci id="S2.SS1.p1.8.m8.1.1.3.cmml" xref="S2.SS1.p1.8.m8.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">\hat{\mathcal{I}}_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.8.m8.1d">over^ start_ARG caligraphic_I end_ARG start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> at turn <math alttext="k" class="ltx_Math" display="inline" id="S2.SS1.p1.9.m9.1"><semantics id="S2.SS1.p1.9.m9.1a"><mi id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><ci id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.9.m9.1d">italic_k</annotation></semantics></math> that aligns with <math alttext="\mathcal{I}_{k}" class="ltx_Math" display="inline" id="S2.SS1.p1.10.m10.1"><semantics id="S2.SS1.p1.10.m10.1a"><msub id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.10.m10.1.1.2" xref="S2.SS1.p1.10.m10.1.1.2.cmml">ℐ</mi><mi id="S2.SS1.p1.10.m10.1.1.3" xref="S2.SS1.p1.10.m10.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><apply id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.10.m10.1.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="S2.SS1.p1.10.m10.1.1.2.cmml" xref="S2.SS1.p1.10.m10.1.1.2">ℐ</ci><ci id="S2.SS1.p1.10.m10.1.1.3.cmml" xref="S2.SS1.p1.10.m10.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">\mathcal{I}_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.10.m10.1d">caligraphic_I start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, based on the preceding context <math alttext="(u_{t},s_{t},\mathcal{I}_{t})^{k-1}_{t=1}" class="ltx_Math" display="inline" id="S2.SS1.p1.11.m11.3"><semantics id="S2.SS1.p1.11.m11.3a"><msubsup id="S2.SS1.p1.11.m11.3.3" xref="S2.SS1.p1.11.m11.3.3.cmml"><mrow id="S2.SS1.p1.11.m11.3.3.3.3.3" xref="S2.SS1.p1.11.m11.3.3.3.3.4.cmml"><mo id="S2.SS1.p1.11.m11.3.3.3.3.3.4" stretchy="false" xref="S2.SS1.p1.11.m11.3.3.3.3.4.cmml">(</mo><msub id="S2.SS1.p1.11.m11.1.1.1.1.1.1" xref="S2.SS1.p1.11.m11.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.11.m11.1.1.1.1.1.1.2" xref="S2.SS1.p1.11.m11.1.1.1.1.1.1.2.cmml">u</mi><mi id="S2.SS1.p1.11.m11.1.1.1.1.1.1.3" xref="S2.SS1.p1.11.m11.1.1.1.1.1.1.3.cmml">t</mi></msub><mo id="S2.SS1.p1.11.m11.3.3.3.3.3.5" xref="S2.SS1.p1.11.m11.3.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.11.m11.2.2.2.2.2.2" xref="S2.SS1.p1.11.m11.2.2.2.2.2.2.cmml"><mi id="S2.SS1.p1.11.m11.2.2.2.2.2.2.2" xref="S2.SS1.p1.11.m11.2.2.2.2.2.2.2.cmml">s</mi><mi id="S2.SS1.p1.11.m11.2.2.2.2.2.2.3" xref="S2.SS1.p1.11.m11.2.2.2.2.2.2.3.cmml">t</mi></msub><mo id="S2.SS1.p1.11.m11.3.3.3.3.3.6" xref="S2.SS1.p1.11.m11.3.3.3.3.4.cmml">,</mo><msub id="S2.SS1.p1.11.m11.3.3.3.3.3.3" xref="S2.SS1.p1.11.m11.3.3.3.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p1.11.m11.3.3.3.3.3.3.2" xref="S2.SS1.p1.11.m11.3.3.3.3.3.3.2.cmml">ℐ</mi><mi id="S2.SS1.p1.11.m11.3.3.3.3.3.3.3" xref="S2.SS1.p1.11.m11.3.3.3.3.3.3.3.cmml">t</mi></msub><mo id="S2.SS1.p1.11.m11.3.3.3.3.3.7" stretchy="false" xref="S2.SS1.p1.11.m11.3.3.3.3.4.cmml">)</mo></mrow><mrow id="S2.SS1.p1.11.m11.3.3.5" xref="S2.SS1.p1.11.m11.3.3.5.cmml"><mi id="S2.SS1.p1.11.m11.3.3.5.2" xref="S2.SS1.p1.11.m11.3.3.5.2.cmml">t</mi><mo id="S2.SS1.p1.11.m11.3.3.5.1" xref="S2.SS1.p1.11.m11.3.3.5.1.cmml">=</mo><mn id="S2.SS1.p1.11.m11.3.3.5.3" xref="S2.SS1.p1.11.m11.3.3.5.3.cmml">1</mn></mrow><mrow id="S2.SS1.p1.11.m11.3.3.3.5" xref="S2.SS1.p1.11.m11.3.3.3.5.cmml"><mi id="S2.SS1.p1.11.m11.3.3.3.5.2" xref="S2.SS1.p1.11.m11.3.3.3.5.2.cmml">k</mi><mo id="S2.SS1.p1.11.m11.3.3.3.5.1" xref="S2.SS1.p1.11.m11.3.3.3.5.1.cmml">−</mo><mn id="S2.SS1.p1.11.m11.3.3.3.5.3" xref="S2.SS1.p1.11.m11.3.3.3.5.3.cmml">1</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m11.3b"><apply id="S2.SS1.p1.11.m11.3.3.cmml" xref="S2.SS1.p1.11.m11.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.11.m11.3.3.4.cmml" xref="S2.SS1.p1.11.m11.3.3">subscript</csymbol><apply id="S2.SS1.p1.11.m11.3.3.3.cmml" xref="S2.SS1.p1.11.m11.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.11.m11.3.3.3.4.cmml" xref="S2.SS1.p1.11.m11.3.3">superscript</csymbol><vector id="S2.SS1.p1.11.m11.3.3.3.3.4.cmml" xref="S2.SS1.p1.11.m11.3.3.3.3.3"><apply id="S2.SS1.p1.11.m11.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.11.m11.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1.1.1.1.1">subscript</csymbol><ci id="S2.SS1.p1.11.m11.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.11.m11.1.1.1.1.1.1.2">𝑢</ci><ci id="S2.SS1.p1.11.m11.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.11.m11.1.1.1.1.1.1.3">𝑡</ci></apply><apply id="S2.SS1.p1.11.m11.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.11.m11.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.11.m11.2.2.2.2.2.2.1.cmml" xref="S2.SS1.p1.11.m11.2.2.2.2.2.2">subscript</csymbol><ci id="S2.SS1.p1.11.m11.2.2.2.2.2.2.2.cmml" xref="S2.SS1.p1.11.m11.2.2.2.2.2.2.2">𝑠</ci><ci id="S2.SS1.p1.11.m11.2.2.2.2.2.2.3.cmml" xref="S2.SS1.p1.11.m11.2.2.2.2.2.2.3">𝑡</ci></apply><apply id="S2.SS1.p1.11.m11.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.11.m11.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p1.11.m11.3.3.3.3.3.3.1.cmml" xref="S2.SS1.p1.11.m11.3.3.3.3.3.3">subscript</csymbol><ci id="S2.SS1.p1.11.m11.3.3.3.3.3.3.2.cmml" xref="S2.SS1.p1.11.m11.3.3.3.3.3.3.2">ℐ</ci><ci id="S2.SS1.p1.11.m11.3.3.3.3.3.3.3.cmml" xref="S2.SS1.p1.11.m11.3.3.3.3.3.3.3">𝑡</ci></apply></vector><apply id="S2.SS1.p1.11.m11.3.3.3.5.cmml" xref="S2.SS1.p1.11.m11.3.3.3.5"><minus id="S2.SS1.p1.11.m11.3.3.3.5.1.cmml" xref="S2.SS1.p1.11.m11.3.3.3.5.1"></minus><ci id="S2.SS1.p1.11.m11.3.3.3.5.2.cmml" xref="S2.SS1.p1.11.m11.3.3.3.5.2">𝑘</ci><cn id="S2.SS1.p1.11.m11.3.3.3.5.3.cmml" type="integer" xref="S2.SS1.p1.11.m11.3.3.3.5.3">1</cn></apply></apply><apply id="S2.SS1.p1.11.m11.3.3.5.cmml" xref="S2.SS1.p1.11.m11.3.3.5"><eq id="S2.SS1.p1.11.m11.3.3.5.1.cmml" xref="S2.SS1.p1.11.m11.3.3.5.1"></eq><ci id="S2.SS1.p1.11.m11.3.3.5.2.cmml" xref="S2.SS1.p1.11.m11.3.3.5.2">𝑡</ci><cn id="S2.SS1.p1.11.m11.3.3.5.3.cmml" type="integer" xref="S2.SS1.p1.11.m11.3.3.5.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m11.3c">(u_{t},s_{t},\mathcal{I}_{t})^{k-1}_{t=1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.11.m11.3d">( italic_u start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , caligraphic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S2.F2.g1" src="x1.png" width="748"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Item Indexing tasks by using movie descriptions from WikiPedia <cite class="ltx_cite ltx_citemacro_citep">(Auer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib4" title="">2007</a>)</cite> to prompt movie titles. We tested <span class="ltx_text ltx_font_typewriter" id="S2.F2.5.1">MPT-7b</span>, <span class="ltx_text ltx_font_typewriter" id="S2.F2.6.2">Mistrial-7b</span> , <span class="ltx_text ltx_font_typewriter" id="S2.F2.7.3">Llama2-7b</span> and <span class="ltx_text ltx_font_typewriter" id="S2.F2.8.4">GPT-Turbo-3.5</span> models and group the accuracy by the range of occurrences of the movies in ReDIAL CRS dataset <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite>. We measure the performance with HIT@5, i.e., whether the target movie in the top-K movie list generated by the LLMs to reflect the movie knowledge stored in the parameters of the LLMs.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Differentiable Search Index (DSI)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">The transformer model has shown proficiency in retrieval tasks, encoding item information within its parameters, which is a method termed Differentiable Search Index (DSI) <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib51" title="">2022</a>)</cite>. DSI involves two key training tasks for pre-trained language models: <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.1">Learn to Index</em> (<span class="ltx_text ltx_font_typewriter" id="S2.SS2.p1.1.2">L2I</span>) and <em class="ltx_emph ltx_font_italic" id="S2.SS2.p1.1.3">Learn to Retrieve</em> (<span class="ltx_text ltx_font_typewriter" id="S2.SS2.p1.1.4">L2R</span>),
which can be used to train a model jointly or in a sequential order. <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p1.1.5">L2I</span> focuses on mapping item content, such as movie description, to item indices, exemplified by linking a description of <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.6">Edge of Tomorrow</span> to its title:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<span class="ltx_ERROR undefined" id="S2.SS2.p2.2">\MakeFramed</span><span class="ltx_ERROR undefined" id="S2.SS2.p2.3">\FrameRestore</span><span class="ltx_ERROR undefined" id="S2.SS2.p2.4">{adjustwidth}</span>
<p class="ltx_p" id="S2.SS2.p2.1">7pt
<span class="ltx_text ltx_font_bold" id="S2.SS2.p2.1.1" style="font-size:90%;">L2I Example<span class="ltx_text ltx_font_medium" id="S2.SS2.p2.1.1.1">: <em class="ltx_emph ltx_font_italic" id="S2.SS2.p2.1.1.1.1">“A 2014 American science fiction action film starring Tom Cruise and Emily Blunt with …”</em> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS2.p2.1.1.1.m1.1"><semantics id="S2.SS2.p2.1.1.1.m1.1a"><mo id="S2.SS2.p2.1.1.1.m1.1.1" stretchy="false" xref="S2.SS2.p2.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.1.1.m1.1b"><ci id="S2.SS2.p2.1.1.1.m1.1.1.cmml" xref="S2.SS2.p2.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p2.1.1.1.m1.1d">→</annotation></semantics></math> </span>Edge of Tomorrow<span class="ltx_text ltx_font_medium" id="S2.SS2.p2.1.1.2">
<span class="ltx_ERROR undefined" id="S2.SS2.p2.1.1.2.1">\endMakeFramed</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1"><span class="ltx_text ltx_font_typewriter" id="S2.SS2.p3.1.1">L2R</span>, on the other hand, maps queries to item indices, such as:</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p4">
<span class="ltx_ERROR undefined" id="S2.SS2.p4.2">\MakeFramed</span><span class="ltx_ERROR undefined" id="S2.SS2.p4.3">\FrameRestore</span><span class="ltx_ERROR undefined" id="S2.SS2.p4.4">{adjustwidth}</span>
<p class="ltx_p" id="S2.SS2.p4.1">7pt
<span class="ltx_text ltx_font_bold" id="S2.SS2.p4.1.1" style="font-size:90%;">L2R Example<span class="ltx_text ltx_font_medium" id="S2.SS2.p4.1.1.1">: <em class="ltx_emph ltx_font_italic" id="S2.SS2.p4.1.1.1.1">“I’m feeling bored today and looking for a sci-fi action movie, preferably starring Tom Cruise.”</em> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S2.SS2.p4.1.1.1.m1.1"><semantics id="S2.SS2.p4.1.1.1.m1.1a"><mo id="S2.SS2.p4.1.1.1.m1.1.1" stretchy="false" xref="S2.SS2.p4.1.1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S2.SS2.p4.1.1.1.m1.1b"><ci id="S2.SS2.p4.1.1.1.m1.1.1.cmml" xref="S2.SS2.p4.1.1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p4.1.1.1.m1.1c">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p4.1.1.1.m1.1d">→</annotation></semantics></math> </span>Edge of Tomorrow<span class="ltx_text ltx_font_medium" id="S2.SS2.p4.1.1.2">
<span class="ltx_ERROR undefined" id="S2.SS2.p4.1.1.2.1">\endMakeFramed</span></span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">DSI models are originally proposed for text retrieval tasks <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib51" title="">2022</a>)</cite>, yet their formulation can be connected to LLMs used in CRS. Considering the <em class="ltx_emph ltx_font_italic" id="S2.SS2.p5.1.1">LLMs as CRS</em> framework proposed in <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> through the lens of DSI, we observe that:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Item Indexing</span>: LLMs index items by using the item titles (e.g., <em class="ltx_emph ltx_font_italic" id="S2.I1.i1.p1.1.2">“Edge of Tomorrow”</em>) as the item identifiers via <span class="ltx_text ltx_font_typewriter" id="S2.I1.i1.p1.1.3">L2I</span>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Item Recommendation</span>: LLMs use conversational context as queries to generate item indices via <span class="ltx_text ltx_font_typewriter" id="S2.I1.i2.p1.1.2">L2R</span>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">Thus, LLMs inherently function as DSI models, by including a certain number of training samples for <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p6.1.1">L2I</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS2.p6.1.2">L2R</span> tasks in their pre-training corpus. Compared to common two-tower models, DSI models require only a single model for item recommendations, by indexing item information into its parameters <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib51" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Item Indexing: LLMs Show Sufficient Item Content Knowledge</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">According to <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>, LLMs demonstrate superior knowledge in content and context, particularly in the movie domain. This proficiency is attributed to the performance in the “Learn to Index” (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.1">L2I</span>) task, as viewed through the DSI <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib51" title="">2022</a>)</cite>. Therefore, our primary concern is the extent to which item content has been indexed in LLMs through the pre-training corpus.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Additional statistics for cold items, warm items and popular items in ReDIAL <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite> datasets. #Items counts the total numbers of items and #Occurences sums the total occurences of such items in conversations.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1" style="font-size:90%;">ReDIAL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.2.1" style="font-size:90%;">Cold – [0, 10)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.3.1" style="font-size:90%;">Warm – [10, 100)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.4.1" style="font-size:90%;">Pop. – [100, +inf)</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.1.1" style="font-size:90%;">#Items</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.1.2.1" style="font-size:90%;">4,960 (78.97%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.3"><span class="ltx_text" id="S2.T1.1.2.1.3.1" style="font-size:90%;">1,193 (18.99%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.1.2.1.4"><span class="ltx_text" id="S2.T1.1.2.1.4.1" style="font-size:90%;">128 (2.04%)</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.1.3.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.3.2.1.1" style="font-size:90%;">#Occurences</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.3.2.2"><span class="ltx_text" id="S2.T1.1.3.2.2.1" style="font-size:90%;">12,523 (18.03%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.3.2.3"><span class="ltx_text ltx_font_bold" id="S2.T1.1.3.2.3.1" style="font-size:90%;">33,304 (47.94%)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.1.3.2.4"><span class="ltx_text ltx_font_bold" id="S2.T1.1.3.2.4.1" style="font-size:90%;">23,647 (34.04%)</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Observation</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">We gathered 6,281 pairs of movie titles from ReDIAL <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite> and the related descriptions from Wikipedia<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.wikipedia.org/</span></span></span></span> for experiments in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.F2" title="In 2.1 Task Formulation ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> to assess LLMs performance on <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS1.p1.1.1">L2I</span> task, and observe that:</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i1.p1.1.1">Good Content Knowledge for Popular Items</span>: All LLMs had indexed a considerable amount of movie content for conversational recommendation tasks. Notably, for frequently mentioned movies, as defined as movie occurences in the ReDIAL dataset <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite> that the item occurences range is [100, +inf), all LLMs exhibit impressive content knowledge.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i2.p1.1.1">Best LLMs</span>: The proprietary model GPT-3.5-t <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib48" title="">2022</a>)</cite> outperforms others. Among the open-sourced LLMs of similar size, Llama2 demonstrates the best performance in the given task, as shown in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.F2" title="In 2.1 Task Formulation ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>, making it the chosen base model for our subsequent experiments.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Impact</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.F2" title="In 2.1 Task Formulation ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a> shows that, in terms of item indexing capability as DSI models, LLMs without specific fine-tuning have already indexed numerous popular movies. <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.T1" title="In 2.3 Item Indexing: LLMs Show Sufficient Item Content Knowledge ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">1</span></a> shows the imbalance of item occurrences in conversational recommendations, where items labeled as <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS2.p1.1.1">warm</span> and <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS2.p1.1.2">pop.</span> constitute about 20% in terms of item counts but contribute to over 80% of occurrences. This suggests that <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS2.p1.1.3">zero-shot LLMs may be sufficient for handling many movie conversational recommendation scenarios</span>, because many are about warm or popular movies. Although, we admit fine-tuning LLMs to cover more cold items remains future work.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="258" id="S2.F3.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Visualization of item monthly relative popularity from Reddit-Movie <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> datasets, since this dataset is the only CRS dataset with long-range timestamps in the wild. Item popularities are shown changing overtime rapidly.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Item Recommendation: LLMs Show Severe Distribution Misalignment</h3>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.1 </span>Observation</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<p class="ltx_p" id="S2.SS4.SSS1.p1.1">In this section, we aim to show that even though LLMs can index items effectively, the data distributions LLMs fitted in training do not match the target inference distributions for CRS. We discuss this <em class="ltx_emph ltx_font_italic" id="S2.SS4.SSS1.p1.1.1">distribution misaligment</em> from two perspectives, using item popularity distribution as an example:</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS1.p2">
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i1.p1.1.1">Static Perspective:</span> As depicted in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S1.F1" title="In 1 Introduction ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>(a), LLMs reflect item popularities from the large-scale training corpus to some extent, which often do not align with popular items on the specific platform for CRS.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i2.p1.1.1">Dynamic Perspective:</span> Target data distributions, such as item popularity, undergo rapid changes over time due to factors like seasons and promotion strategies. For instance, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.F3" title="In 2.3.2 Impact ‣ 2.3 Item Indexing: LLMs Show Sufficient Item Content Knowledge ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows that monthly relative item popularities on the <em class="ltx_emph ltx_font_italic" id="S2.I3.i2.p1.1.2">Reddit-Movie</em> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> dataset change over time, which cannot be captured by a static LLM, even fine-tuned ones.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.4.2 </span>Impact</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">Our observations highlight the distribution misalignments between items on the target platform and those recommended by LLMs. This misalignment is considered from both static and dynamic perspectives, suggesting that: (1) despite LLMs exhibiting impressive performance in conversational recommendation<cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p1.1.1">there exists room for improving recommendation accuracy by aligning with the distributions of target platforms</span>; (2) due to the dynamic nature of recommendation platforms, target data distributions change rapidly, necessitating the <span class="ltx_text ltx_font_bold" id="S2.SS4.SSS2.p1.1.2">more efficient methods to adjust item recommendations from LLMs accordingly</span>.</p>
</div>
<figure class="ltx_figure" id="S2.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S2.F4.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Reindex-Then-Adapt (RTA) Framework. LLMs can generate a list of item titles as the recommendations given the conversation contexts. To further improve the accuracy and controllability, we conduct (1) <span class="ltx_text ltx_font_typewriter" id="S2.F4.5.1">reindex</span> step: reindexing the item (e.g., movie) titles in LLMs as single tokens to obtain the predicted <span class="ltx_text ltx_font_italic" id="S2.F4.6.2">logit</span> vectors efficiently; (2) <span class="ltx_text ltx_font_typewriter" id="S2.F4.7.3">adapt</span> step: adapting the recommenders towards target data distributions effectively with multiple options on the <span class="ltx_text ltx_font_italic" id="S2.F4.8.4">logit</span> vectors such as adjusting bias terms or combining RecSys models with Gating mechanism.</figcaption>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Framework</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Overview</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">As discussed in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S1" title="1 Introduction ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">1</span></a>, we argue that representing target items with varying token counts in LLMs poses challenges for adjusting recommendation distributions over all target items. To tackle this, we view LLMs as DSI models that have already indexed sufficient item content knowledge (see <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.SS3" title="2.3 Item Indexing: LLMs Show Sufficient Item Content Knowledge ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">2.3</span></a>), and propose the <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">Reindex-Then-Adapt (RTA)</span> framework, illustrated in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.F4" title="In 2.4.2 Impact ‣ 2.4 Item Recommendation: LLMs Show Severe Distribution Misalignment ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">4</span></a>:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i1.p1.1.1">Reindex</span> item indices with varying token numbers in LLMs into single-token item indices using a mixture of data samples from the <span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.1.2">L2I</span> corpus and/or <span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.1.3">L2R</span> corpus. This aims to remove the <span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.1.4">adapt</span> step barrier. In contrast to the <span class="ltx_text ltx_font_typewriter" id="S3.I1.i1.p1.1.5">index</span> step in the original DSI models <cite class="ltx_cite ltx_citemacro_citep">(Tay et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib51" title="">2022</a>)</cite>, the <span class="ltx_text ltx_font_typewriter ltx_font_bold" id="S3.I1.i1.p1.1.6">re<span class="ltx_text ltx_font_medium" id="S3.I1.i1.p1.1.6.1">index</span></span> step reuses the content of indexed items from the LLMs, thereby facilitating the learning process for the new item indices.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I1.i2.p1.1.1">Adapt</span> <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.2">logits</span> from the reindexed LLMs, achieved by transforming the <span class="ltx_text ltx_font_italic" id="S3.I1.i2.p1.1.3">logit</span> vectors or by combining with other traditional RecSys using Gating mechanism <cite class="ltx_cite ltx_citemacro_citep">(Hochreiter and Schmidhuber, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib26" title="">1997</a>; Chung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib15" title="">2014</a>; Gu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib21" title="">2020</a>)</cite>. This adjustment aims to effectively align the recommendation probability distributions over items with the target data distributions.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span><span class="ltx_text ltx_font_typewriter" id="S3.SS2.1.1">Reindex</span> Step: Single-Token Items in LLMs</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The key of <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.1">reindex</span> step is to “squeeze” multi-token item embeddings into single-token item embeddings efficiently, and preserves the semantics of the original item embeddings in LLMs generation.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Identify Item Indices.</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.6">Formally, for a sentence <math alttext="s=(v_{i})^{m}_{i=1}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.1.m1.1"><semantics id="S3.SS2.SSS1.p1.1.m1.1a"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml">s</mi><mo id="S3.SS2.SSS1.p1.1.m1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml">=</mo><msubsup id="S3.SS2.SSS1.p1.1.m1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml"><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.SS2.SSS1.p1.1.m1.1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.1.m1.1.1.1.3.2" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS1.p1.1.m1.1.1.1.3.1" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.SSS1.p1.1.m1.1.1.1.3.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.3" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.3.cmml">m</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.1.m1.1b"><apply id="S3.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1"><eq id="S3.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.2"></eq><ci id="S3.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.3">𝑠</ci><apply id="S3.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1">subscript</csymbol><apply id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.2">𝑣</ci><ci id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S3.SS2.SSS1.p1.1.m1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.1.3">𝑚</ci></apply><apply id="S3.SS2.SSS1.p1.1.m1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.3"><eq id="S3.SS2.SSS1.p1.1.m1.1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.3.1"></eq><ci id="S3.SS2.SSS1.p1.1.m1.1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.3.2">𝑖</ci><cn id="S3.SS2.SSS1.p1.1.m1.1.1.1.3.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.1.m1.1.1.1.3.3">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.1.m1.1c">s=(v_{i})^{m}_{i=1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.1.m1.1d">italic_s = ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT</annotation></semantics></math> consisting of tokens <math alttext="v_{i}\in\mathcal{V}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.2.m2.1"><semantics id="S3.SS2.SSS1.p1.2.m2.1a"><mrow id="S3.SS2.SSS1.p1.2.m2.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.cmml"><msub id="S3.SS2.SSS1.p1.2.m2.1.1.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml"><mi id="S3.SS2.SSS1.p1.2.m2.1.1.2.2" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.2.cmml">v</mi><mi id="S3.SS2.SSS1.p1.2.m2.1.1.2.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS2.SSS1.p1.2.m2.1.1.1" xref="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS1.p1.2.m2.1.1.3" xref="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml">𝒱</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.2.m2.1b"><apply id="S3.SS2.SSS1.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1"><in id="S3.SS2.SSS1.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.1"></in><apply id="S3.SS2.SSS1.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.2.m2.1.1.2.1.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.2.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.2">𝑣</ci><ci id="S3.SS2.SSS1.p1.2.m2.1.1.2.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.2.3">𝑖</ci></apply><ci id="S3.SS2.SSS1.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS1.p1.2.m2.1.1.3">𝒱</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.2.m2.1c">v_{i}\in\mathcal{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.2.m2.1d">italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ caligraphic_V</annotation></semantics></math>, we denote the tokens representing an item for CRS tasks as <math alttext="(v_{i})^{j+n}_{i=j}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.3.m3.1"><semantics id="S3.SS2.SSS1.p1.3.m3.1a"><msubsup id="S3.SS2.SSS1.p1.3.m3.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.cmml"><mrow id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.cmml"><mo id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.cmml">(</mo><msub id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.2.cmml">v</mi><mi id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.SS2.SSS1.p1.3.m3.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.SS2.SSS1.p1.3.m3.1.1.3.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.1.cmml">=</mo><mi id="S3.SS2.SSS1.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml">j</mi></mrow><mrow id="S3.SS2.SSS1.p1.3.m3.1.1.1.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.3.m3.1.1.1.3.2" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.3.2.cmml">j</mi><mo id="S3.SS2.SSS1.p1.3.m3.1.1.1.3.1" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.3.1.cmml">+</mo><mi id="S3.SS2.SSS1.p1.3.m3.1.1.1.3.3" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.3.3.cmml">n</mi></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.3.m3.1b"><apply id="S3.SS2.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">subscript</csymbol><apply id="S3.SS2.SSS1.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.2">𝑣</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.SS2.SSS1.p1.3.m3.1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.3"><plus id="S3.SS2.SSS1.p1.3.m3.1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.3.1"></plus><ci id="S3.SS2.SSS1.p1.3.m3.1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.3.2">𝑗</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.1.3.3">𝑛</ci></apply></apply><apply id="S3.SS2.SSS1.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3"><eq id="S3.SS2.SSS1.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.1"></eq><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.2">𝑖</ci><ci id="S3.SS2.SSS1.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.3.m3.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.3.m3.1c">(v_{i})^{j+n}_{i=j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.3.m3.1d">( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_j + italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = italic_j end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.4.m4.1"><semantics id="S3.SS2.SSS1.p1.4.m4.1a"><mi id="S3.SS2.SSS1.p1.4.m4.1.1" xref="S3.SS2.SSS1.p1.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.4.m4.1b"><ci id="S3.SS2.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.4.m4.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.4.m4.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.4.m4.1d">italic_j</annotation></semantics></math> indicates the starting position of the first token for the item in <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.5.m5.1"><semantics id="S3.SS2.SSS1.p1.5.m5.1a"><mi id="S3.SS2.SSS1.p1.5.m5.1.1" xref="S3.SS2.SSS1.p1.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.5.m5.1b"><ci id="S3.SS2.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS1.p1.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.5.m5.1d">italic_s</annotation></semantics></math>, and <math alttext="n" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.6.m6.1"><semantics id="S3.SS2.SSS1.p1.6.m6.1a"><mi id="S3.SS2.SSS1.p1.6.m6.1.1" xref="S3.SS2.SSS1.p1.6.m6.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.6.m6.1b"><ci id="S3.SS2.SSS1.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS1.p1.6.m6.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.6.m6.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.6.m6.1d">italic_n</annotation></semantics></math> is the number of tokens representing this item. Consequently, with the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS1.p1.6.1">Embed</span> layer, LLMs can retrieve the sequence of token embeddings for this item:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\left(\mathbf{v}_{i}\right)^{j+n}_{i=j}=\texttt{Embed}\left((v_{i})^{j+n}_{i=j%
}\right)," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml">𝐯</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.E1.m1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.3.1.cmml">=</mo><mi id="S3.E1.m1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.3.3.cmml">j</mi></mrow><mrow id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.1.1.3.2.cmml">j</mi><mo id="S3.E1.m1.1.1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.1.1.3.1.cmml">+</mo><mi id="S3.E1.m1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.1.1.3.3.cmml">n</mi></mrow></msubsup><mo id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3a.cmml">Embed</mtext><mo id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">⁢</mo><mrow id="S3.E1.m1.1.1.1.1.2.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.2.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.cmml">(</mo><msubsup id="S3.E1.m1.1.1.1.1.2.1.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.E1.m1.1.1.1.1.2.1.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.3.2.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.2.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.3.1.cmml">=</mo><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.3.3.cmml">j</mi></mrow><mrow id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.2.cmml">j</mi><mo id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.1.cmml">+</mo><mi id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.cmml">n</mi></mrow></msubsup><mo id="S3.E1.m1.1.1.1.1.2.1.1.3" xref="S3.E1.m1.1.1.1.1.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"></eq><apply id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.2">𝐯</ci><ci id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3"><plus id="S3.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.2">𝑗</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3.3">𝑛</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3"><eq id="S3.E1.m1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.1.3.1"></eq><ci id="S3.E1.m1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.E1.m1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.1.3.3">𝑗</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><times id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2"></times><ci id="S3.E1.m1.1.1.1.1.2.3a.cmml" xref="S3.E1.m1.1.1.1.1.2.3"><mtext class="ltx_mathvariant_monospace" id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3">Embed</mtext></ci><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1">subscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1">superscript</csymbol><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.2">𝑣</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3"><plus id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.1"></plus><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.2">𝑗</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.1.3.3">𝑛</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.2.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.3"><eq id="S3.E1.m1.1.1.1.1.2.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.3.1"></eq><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.3.2">𝑖</ci><ci id="S3.E1.m1.1.1.1.1.2.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\left(\mathbf{v}_{i}\right)^{j+n}_{i=j}=\texttt{Embed}\left((v_{i})^{j+n}_{i=j%
}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">( bold_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_j + italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = italic_j end_POSTSUBSCRIPT = Embed ( ( italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_j + italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = italic_j end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS1.p1.10">where <math alttext="\mathbf{v}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.7.m1.1"><semantics id="S3.SS2.SSS1.p1.7.m1.1a"><mrow id="S3.SS2.SSS1.p1.7.m1.1.1" xref="S3.SS2.SSS1.p1.7.m1.1.1.cmml"><mi id="S3.SS2.SSS1.p1.7.m1.1.1.2" xref="S3.SS2.SSS1.p1.7.m1.1.1.2.cmml">𝐯</mi><mo id="S3.SS2.SSS1.p1.7.m1.1.1.1" xref="S3.SS2.SSS1.p1.7.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS1.p1.7.m1.1.1.3" xref="S3.SS2.SSS1.p1.7.m1.1.1.3.cmml"><mi id="S3.SS2.SSS1.p1.7.m1.1.1.3.2" xref="S3.SS2.SSS1.p1.7.m1.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.SSS1.p1.7.m1.1.1.3.3" xref="S3.SS2.SSS1.p1.7.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.7.m1.1b"><apply id="S3.SS2.SSS1.p1.7.m1.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m1.1.1"><in id="S3.SS2.SSS1.p1.7.m1.1.1.1.cmml" xref="S3.SS2.SSS1.p1.7.m1.1.1.1"></in><ci id="S3.SS2.SSS1.p1.7.m1.1.1.2.cmml" xref="S3.SS2.SSS1.p1.7.m1.1.1.2">𝐯</ci><apply id="S3.SS2.SSS1.p1.7.m1.1.1.3.cmml" xref="S3.SS2.SSS1.p1.7.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS1.p1.7.m1.1.1.3.1.cmml" xref="S3.SS2.SSS1.p1.7.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS1.p1.7.m1.1.1.3.2.cmml" xref="S3.SS2.SSS1.p1.7.m1.1.1.3.2">ℝ</ci><ci id="S3.SS2.SSS1.p1.7.m1.1.1.3.3.cmml" xref="S3.SS2.SSS1.p1.7.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.7.m1.1c">\mathbf{v}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.7.m1.1d">bold_v ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is the token embedding, and <math alttext="d" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.8.m2.1"><semantics id="S3.SS2.SSS1.p1.8.m2.1a"><mi id="S3.SS2.SSS1.p1.8.m2.1.1" xref="S3.SS2.SSS1.p1.8.m2.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.8.m2.1b"><ci id="S3.SS2.SSS1.p1.8.m2.1.1.cmml" xref="S3.SS2.SSS1.p1.8.m2.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.8.m2.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.8.m2.1d">italic_d</annotation></semantics></math> is the embedding size.
For example, we may look up <em class="ltx_emph ltx_font_italic" id="S3.SS2.SSS1.p1.10.1">“Edge of Tomorrow”</em> embeddings represented by “<math alttext="[14,72,98]" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.9.m3.3"><semantics id="S3.SS2.SSS1.p1.9.m3.3a"><mrow id="S3.SS2.SSS1.p1.9.m3.3.4.2" xref="S3.SS2.SSS1.p1.9.m3.3.4.1.cmml"><mo id="S3.SS2.SSS1.p1.9.m3.3.4.2.1" stretchy="false" xref="S3.SS2.SSS1.p1.9.m3.3.4.1.cmml">[</mo><mn id="S3.SS2.SSS1.p1.9.m3.1.1" xref="S3.SS2.SSS1.p1.9.m3.1.1.cmml">14</mn><mo id="S3.SS2.SSS1.p1.9.m3.3.4.2.2" xref="S3.SS2.SSS1.p1.9.m3.3.4.1.cmml">,</mo><mn id="S3.SS2.SSS1.p1.9.m3.2.2" xref="S3.SS2.SSS1.p1.9.m3.2.2.cmml">72</mn><mo id="S3.SS2.SSS1.p1.9.m3.3.4.2.3" xref="S3.SS2.SSS1.p1.9.m3.3.4.1.cmml">,</mo><mn id="S3.SS2.SSS1.p1.9.m3.3.3" xref="S3.SS2.SSS1.p1.9.m3.3.3.cmml">98</mn><mo id="S3.SS2.SSS1.p1.9.m3.3.4.2.4" stretchy="false" xref="S3.SS2.SSS1.p1.9.m3.3.4.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.9.m3.3b"><list id="S3.SS2.SSS1.p1.9.m3.3.4.1.cmml" xref="S3.SS2.SSS1.p1.9.m3.3.4.2"><cn id="S3.SS2.SSS1.p1.9.m3.1.1.cmml" type="integer" xref="S3.SS2.SSS1.p1.9.m3.1.1">14</cn><cn id="S3.SS2.SSS1.p1.9.m3.2.2.cmml" type="integer" xref="S3.SS2.SSS1.p1.9.m3.2.2">72</cn><cn id="S3.SS2.SSS1.p1.9.m3.3.3.cmml" type="integer" xref="S3.SS2.SSS1.p1.9.m3.3.3">98</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.9.m3.3c">[14,72,98]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.9.m3.3d">[ 14 , 72 , 98 ]</annotation></semantics></math>” in the sentence <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.SSS1.p1.10.m4.1"><semantics id="S3.SS2.SSS1.p1.10.m4.1a"><mi id="S3.SS2.SSS1.p1.10.m4.1.1" xref="S3.SS2.SSS1.p1.10.m4.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS1.p1.10.m4.1b"><ci id="S3.SS2.SSS1.p1.10.m4.1.1.cmml" xref="S3.SS2.SSS1.p1.10.m4.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS1.p1.10.m4.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS1.p1.10.m4.1d">italic_s</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Aggregate Multi-Token Embeddings</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.2">In the proposed <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS2.p1.2.1">reindex</span> step, we assume that the semantics from multiple (typically shorter than 10) token embeddings can be aggregated into a new single token embedding with a trainable aggregator, such as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tilde{\mathbf{v}}=\texttt{Aggregator}\left(\left(\mathbf{v}_{i}\right)^{j+n}_%
{i=j}\right)," class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">𝐯</mi><mo id="S3.E2.m1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.3.1.cmml">~</mo></mover><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3a.cmml">Aggregator</mtext><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msubsup id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">𝐯</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml">=</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml">j</mi></mrow><mrow id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">j</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">+</mo><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">n</mi></mrow></msubsup><mo id="S3.E2.m1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><ci id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3.1">~</ci><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝐯</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><ci id="S3.E2.m1.1.1.1.1.1.3a.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3">Aggregator</mtext></ci><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐯</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3"><plus id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.1"></plus><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.2">𝑗</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.1.3.3">𝑛</ci></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3"><eq id="S3.E2.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.1"></eq><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.2">𝑖</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.3">𝑗</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\tilde{\mathbf{v}}=\texttt{Aggregator}\left(\left(\mathbf{v}_{i}\right)^{j+n}_%
{i=j}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">over~ start_ARG bold_v end_ARG = Aggregator ( ( bold_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT italic_j + italic_n end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = italic_j end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">where the aggregated <math alttext="\tilde{\mathbf{v}}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS2.SSS2.p1.1.m1.1"><semantics id="S3.SS2.SSS2.p1.1.m1.1a"><mrow id="S3.SS2.SSS2.p1.1.m1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.cmml"><mover accent="true" id="S3.SS2.SSS2.p1.1.m1.1.1.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.2.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.2.cmml">𝐯</mi><mo id="S3.SS2.SSS2.p1.1.m1.1.1.2.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.1.cmml">~</mo></mover><mo id="S3.SS2.SSS2.p1.1.m1.1.1.1" xref="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS2.p1.1.m1.1.1.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3.2" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.SSS2.p1.1.m1.1.1.3.3" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS2.p1.1.m1.1b"><apply id="S3.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1"><in id="S3.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.1"></in><apply id="S3.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2"><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.1">~</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.2.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.2.2">𝐯</ci></apply><apply id="S3.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.2">ℝ</ci><ci id="S3.SS2.SSS2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS2.SSS2.p1.1.m1.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS2.p1.1.m1.1c">\tilde{\mathbf{v}}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS2.p1.1.m1.1d">over~ start_ARG bold_v end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> serves as the new representation of the target item in LLMs generation. Therefore, if all target items are represented by the “squeezed” single embedding, scoring all the target items to obtain a logit vector from LLMs is efficient. In general, many existing model architectures can be used as <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS2.p1.1.1">Aggregator</span>, such as RNN-based <cite class="ltx_cite ltx_citemacro_citep">(Cho et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib11" title="">2014</a>)</cite>, Transformer-based models <cite class="ltx_cite ltx_citemacro_citep">(Vaswani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib54" title="">2017</a>)</cite>, or even Weighted Pooling. We discuss the details and the comparisons with pure new embeddings in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS3" title="4.3 Effectiveness of the Reindex Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.3</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Learning Process</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.7">The contrative loss <cite class="ltx_cite ltx_citemacro_citep">(Oord et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib45" title="">2018</a>)</cite> is used for learning the aggregator to “squeeze” multi-token item embeddings and preserve the semantics for LLMs:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\textit{reindex}}=-\frac{1}{|\mathcal{D}|}\sum_{\mathbf{q},%
\mathbf{\tilde{v}}\in\mathbf{\mathcal{D}}}\log\left[\frac{\exp\left(\mathbf{q}%
^{\top}\mathbf{\tilde{v}}\right)}{\exp\left(\mathbf{q}^{\top}\mathbf{\tilde{v}%
}\right)+\sum_{\mathbf{n}\in\mathcal{N}}\exp\left(\mathbf{q}^{\top}\mathbf{n}%
\right)}\right]," class="ltx_Math" display="block" id="S3.E3.m1.11"><semantics id="S3.E3.m1.11a"><mrow id="S3.E3.m1.11.11.1" xref="S3.E3.m1.11.11.1.1.cmml"><mrow id="S3.E3.m1.11.11.1.1" xref="S3.E3.m1.11.11.1.1.cmml"><msub id="S3.E3.m1.11.11.1.1.2" xref="S3.E3.m1.11.11.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.11.11.1.1.2.2" xref="S3.E3.m1.11.11.1.1.2.2.cmml">ℒ</mi><mtext class="ltx_mathvariant_italic" id="S3.E3.m1.11.11.1.1.2.3" xref="S3.E3.m1.11.11.1.1.2.3a.cmml">reindex</mtext></msub><mo id="S3.E3.m1.11.11.1.1.1" xref="S3.E3.m1.11.11.1.1.1.cmml">=</mo><mrow id="S3.E3.m1.11.11.1.1.3" xref="S3.E3.m1.11.11.1.1.3.cmml"><mo id="S3.E3.m1.11.11.1.1.3a" xref="S3.E3.m1.11.11.1.1.3.cmml">−</mo><mrow id="S3.E3.m1.11.11.1.1.3.2" xref="S3.E3.m1.11.11.1.1.3.2.cmml"><mfrac id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mn id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml">1</mn><mrow id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.2.cmml"><mo id="S3.E3.m1.1.1.1.3.1" stretchy="false" xref="S3.E3.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml">𝒟</mi><mo id="S3.E3.m1.1.1.1.3.2" stretchy="false" xref="S3.E3.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.E3.m1.11.11.1.1.3.2.1" xref="S3.E3.m1.11.11.1.1.3.2.1.cmml">⁢</mo><mrow id="S3.E3.m1.11.11.1.1.3.2.2" xref="S3.E3.m1.11.11.1.1.3.2.2.cmml"><munder id="S3.E3.m1.11.11.1.1.3.2.2.1" xref="S3.E3.m1.11.11.1.1.3.2.2.1.cmml"><mo id="S3.E3.m1.11.11.1.1.3.2.2.1.2" movablelimits="false" xref="S3.E3.m1.11.11.1.1.3.2.2.1.2.cmml">∑</mo><mrow id="S3.E3.m1.3.3.2" xref="S3.E3.m1.3.3.2.cmml"><mrow id="S3.E3.m1.3.3.2.4.2" xref="S3.E3.m1.3.3.2.4.1.cmml"><mi id="S3.E3.m1.2.2.1.1" xref="S3.E3.m1.2.2.1.1.cmml">𝐪</mi><mo id="S3.E3.m1.3.3.2.4.2.1" xref="S3.E3.m1.3.3.2.4.1.cmml">,</mo><mover accent="true" id="S3.E3.m1.3.3.2.2" xref="S3.E3.m1.3.3.2.2.cmml"><mi id="S3.E3.m1.3.3.2.2.2" xref="S3.E3.m1.3.3.2.2.2.cmml">𝐯</mi><mo id="S3.E3.m1.3.3.2.2.1" xref="S3.E3.m1.3.3.2.2.1.cmml">~</mo></mover></mrow><mo id="S3.E3.m1.3.3.2.3" xref="S3.E3.m1.3.3.2.3.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.3.3.2.5" xref="S3.E3.m1.3.3.2.5.cmml">𝒟</mi></mrow></munder><mrow id="S3.E3.m1.11.11.1.1.3.2.2.2.2" xref="S3.E3.m1.11.11.1.1.3.2.2.2.1.cmml"><mi id="S3.E3.m1.10.10" xref="S3.E3.m1.10.10.cmml">log</mi><mo id="S3.E3.m1.11.11.1.1.3.2.2.2.2a" xref="S3.E3.m1.11.11.1.1.3.2.2.2.1.cmml">⁡</mo><mrow id="S3.E3.m1.11.11.1.1.3.2.2.2.2.1" xref="S3.E3.m1.11.11.1.1.3.2.2.2.1.cmml"><mo id="S3.E3.m1.11.11.1.1.3.2.2.2.2.1.1" xref="S3.E3.m1.11.11.1.1.3.2.2.2.1.cmml">[</mo><mfrac id="S3.E3.m1.9.9" xref="S3.E3.m1.9.9.cmml"><mrow id="S3.E3.m1.5.5.2.2" xref="S3.E3.m1.5.5.2.3.cmml"><mi id="S3.E3.m1.4.4.1.1" xref="S3.E3.m1.4.4.1.1.cmml">exp</mi><mo id="S3.E3.m1.5.5.2.2a" xref="S3.E3.m1.5.5.2.3.cmml">⁡</mo><mrow id="S3.E3.m1.5.5.2.2.1" xref="S3.E3.m1.5.5.2.3.cmml"><mo id="S3.E3.m1.5.5.2.2.1.2" xref="S3.E3.m1.5.5.2.3.cmml">(</mo><mrow id="S3.E3.m1.5.5.2.2.1.1" xref="S3.E3.m1.5.5.2.2.1.1.cmml"><msup id="S3.E3.m1.5.5.2.2.1.1.2" xref="S3.E3.m1.5.5.2.2.1.1.2.cmml"><mi id="S3.E3.m1.5.5.2.2.1.1.2.2" xref="S3.E3.m1.5.5.2.2.1.1.2.2.cmml">𝐪</mi><mo id="S3.E3.m1.5.5.2.2.1.1.2.3" xref="S3.E3.m1.5.5.2.2.1.1.2.3.cmml">⊤</mo></msup><mo id="S3.E3.m1.5.5.2.2.1.1.1" xref="S3.E3.m1.5.5.2.2.1.1.1.cmml">⁢</mo><mover accent="true" id="S3.E3.m1.5.5.2.2.1.1.3" xref="S3.E3.m1.5.5.2.2.1.1.3.cmml"><mi id="S3.E3.m1.5.5.2.2.1.1.3.2" xref="S3.E3.m1.5.5.2.2.1.1.3.2.cmml">𝐯</mi><mo id="S3.E3.m1.5.5.2.2.1.1.3.1" xref="S3.E3.m1.5.5.2.2.1.1.3.1.cmml">~</mo></mover></mrow><mo id="S3.E3.m1.5.5.2.2.1.3" xref="S3.E3.m1.5.5.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.E3.m1.9.9.6" xref="S3.E3.m1.9.9.6.cmml"><mrow id="S3.E3.m1.8.8.5.3.1" xref="S3.E3.m1.8.8.5.3.2.cmml"><mi id="S3.E3.m1.6.6.3.1" xref="S3.E3.m1.6.6.3.1.cmml">exp</mi><mo id="S3.E3.m1.8.8.5.3.1a" xref="S3.E3.m1.8.8.5.3.2.cmml">⁡</mo><mrow id="S3.E3.m1.8.8.5.3.1.1" xref="S3.E3.m1.8.8.5.3.2.cmml"><mo id="S3.E3.m1.8.8.5.3.1.1.2" xref="S3.E3.m1.8.8.5.3.2.cmml">(</mo><mrow id="S3.E3.m1.8.8.5.3.1.1.1" xref="S3.E3.m1.8.8.5.3.1.1.1.cmml"><msup id="S3.E3.m1.8.8.5.3.1.1.1.2" xref="S3.E3.m1.8.8.5.3.1.1.1.2.cmml"><mi id="S3.E3.m1.8.8.5.3.1.1.1.2.2" xref="S3.E3.m1.8.8.5.3.1.1.1.2.2.cmml">𝐪</mi><mo id="S3.E3.m1.8.8.5.3.1.1.1.2.3" xref="S3.E3.m1.8.8.5.3.1.1.1.2.3.cmml">⊤</mo></msup><mo id="S3.E3.m1.8.8.5.3.1.1.1.1" xref="S3.E3.m1.8.8.5.3.1.1.1.1.cmml">⁢</mo><mover accent="true" id="S3.E3.m1.8.8.5.3.1.1.1.3" xref="S3.E3.m1.8.8.5.3.1.1.1.3.cmml"><mi id="S3.E3.m1.8.8.5.3.1.1.1.3.2" xref="S3.E3.m1.8.8.5.3.1.1.1.3.2.cmml">𝐯</mi><mo id="S3.E3.m1.8.8.5.3.1.1.1.3.1" xref="S3.E3.m1.8.8.5.3.1.1.1.3.1.cmml">~</mo></mover></mrow><mo id="S3.E3.m1.8.8.5.3.1.1.3" xref="S3.E3.m1.8.8.5.3.2.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.9.9.6.5" rspace="0.055em" xref="S3.E3.m1.9.9.6.5.cmml">+</mo><mrow id="S3.E3.m1.9.9.6.4" xref="S3.E3.m1.9.9.6.4.cmml"><msub id="S3.E3.m1.9.9.6.4.2" xref="S3.E3.m1.9.9.6.4.2.cmml"><mo id="S3.E3.m1.9.9.6.4.2.2" xref="S3.E3.m1.9.9.6.4.2.2.cmml">∑</mo><mrow id="S3.E3.m1.9.9.6.4.2.3" xref="S3.E3.m1.9.9.6.4.2.3.cmml"><mi id="S3.E3.m1.9.9.6.4.2.3.2" xref="S3.E3.m1.9.9.6.4.2.3.2.cmml">𝐧</mi><mo id="S3.E3.m1.9.9.6.4.2.3.1" xref="S3.E3.m1.9.9.6.4.2.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.9.9.6.4.2.3.3" xref="S3.E3.m1.9.9.6.4.2.3.3.cmml">𝒩</mi></mrow></msub><mrow id="S3.E3.m1.9.9.6.4.1.1" xref="S3.E3.m1.9.9.6.4.1.2.cmml"><mi id="S3.E3.m1.7.7.4.2" xref="S3.E3.m1.7.7.4.2.cmml">exp</mi><mo id="S3.E3.m1.9.9.6.4.1.1a" xref="S3.E3.m1.9.9.6.4.1.2.cmml">⁡</mo><mrow id="S3.E3.m1.9.9.6.4.1.1.1" xref="S3.E3.m1.9.9.6.4.1.2.cmml"><mo id="S3.E3.m1.9.9.6.4.1.1.1.2" xref="S3.E3.m1.9.9.6.4.1.2.cmml">(</mo><mrow id="S3.E3.m1.9.9.6.4.1.1.1.1" xref="S3.E3.m1.9.9.6.4.1.1.1.1.cmml"><msup id="S3.E3.m1.9.9.6.4.1.1.1.1.2" xref="S3.E3.m1.9.9.6.4.1.1.1.1.2.cmml"><mi id="S3.E3.m1.9.9.6.4.1.1.1.1.2.2" xref="S3.E3.m1.9.9.6.4.1.1.1.1.2.2.cmml">𝐪</mi><mo id="S3.E3.m1.9.9.6.4.1.1.1.1.2.3" xref="S3.E3.m1.9.9.6.4.1.1.1.1.2.3.cmml">⊤</mo></msup><mo id="S3.E3.m1.9.9.6.4.1.1.1.1.1" xref="S3.E3.m1.9.9.6.4.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E3.m1.9.9.6.4.1.1.1.1.3" xref="S3.E3.m1.9.9.6.4.1.1.1.1.3.cmml">𝐧</mi></mrow><mo id="S3.E3.m1.9.9.6.4.1.1.1.3" xref="S3.E3.m1.9.9.6.4.1.2.cmml">)</mo></mrow></mrow></mrow></mrow></mfrac><mo id="S3.E3.m1.11.11.1.1.3.2.2.2.2.1.2" xref="S3.E3.m1.11.11.1.1.3.2.2.2.1.cmml">]</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.E3.m1.11.11.1.2" xref="S3.E3.m1.11.11.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.11b"><apply id="S3.E3.m1.11.11.1.1.cmml" xref="S3.E3.m1.11.11.1"><eq id="S3.E3.m1.11.11.1.1.1.cmml" xref="S3.E3.m1.11.11.1.1.1"></eq><apply id="S3.E3.m1.11.11.1.1.2.cmml" xref="S3.E3.m1.11.11.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.11.11.1.1.2.1.cmml" xref="S3.E3.m1.11.11.1.1.2">subscript</csymbol><ci id="S3.E3.m1.11.11.1.1.2.2.cmml" xref="S3.E3.m1.11.11.1.1.2.2">ℒ</ci><ci id="S3.E3.m1.11.11.1.1.2.3a.cmml" xref="S3.E3.m1.11.11.1.1.2.3"><mtext class="ltx_mathvariant_italic" id="S3.E3.m1.11.11.1.1.2.3.cmml" mathsize="70%" xref="S3.E3.m1.11.11.1.1.2.3">reindex</mtext></ci></apply><apply id="S3.E3.m1.11.11.1.1.3.cmml" xref="S3.E3.m1.11.11.1.1.3"><minus id="S3.E3.m1.11.11.1.1.3.1.cmml" xref="S3.E3.m1.11.11.1.1.3"></minus><apply id="S3.E3.m1.11.11.1.1.3.2.cmml" xref="S3.E3.m1.11.11.1.1.3.2"><times id="S3.E3.m1.11.11.1.1.3.2.1.cmml" xref="S3.E3.m1.11.11.1.1.3.2.1"></times><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><divide id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1"></divide><cn id="S3.E3.m1.1.1.3.cmml" type="integer" xref="S3.E3.m1.1.1.3">1</cn><apply id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.3"><abs id="S3.E3.m1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.3.1"></abs><ci id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1">𝒟</ci></apply></apply><apply id="S3.E3.m1.11.11.1.1.3.2.2.cmml" xref="S3.E3.m1.11.11.1.1.3.2.2"><apply id="S3.E3.m1.11.11.1.1.3.2.2.1.cmml" xref="S3.E3.m1.11.11.1.1.3.2.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.11.11.1.1.3.2.2.1.1.cmml" xref="S3.E3.m1.11.11.1.1.3.2.2.1">subscript</csymbol><sum id="S3.E3.m1.11.11.1.1.3.2.2.1.2.cmml" xref="S3.E3.m1.11.11.1.1.3.2.2.1.2"></sum><apply id="S3.E3.m1.3.3.2.cmml" xref="S3.E3.m1.3.3.2"><in id="S3.E3.m1.3.3.2.3.cmml" xref="S3.E3.m1.3.3.2.3"></in><list id="S3.E3.m1.3.3.2.4.1.cmml" xref="S3.E3.m1.3.3.2.4.2"><ci id="S3.E3.m1.2.2.1.1.cmml" xref="S3.E3.m1.2.2.1.1">𝐪</ci><apply id="S3.E3.m1.3.3.2.2.cmml" xref="S3.E3.m1.3.3.2.2"><ci id="S3.E3.m1.3.3.2.2.1.cmml" xref="S3.E3.m1.3.3.2.2.1">~</ci><ci id="S3.E3.m1.3.3.2.2.2.cmml" xref="S3.E3.m1.3.3.2.2.2">𝐯</ci></apply></list><ci id="S3.E3.m1.3.3.2.5.cmml" xref="S3.E3.m1.3.3.2.5">𝒟</ci></apply></apply><apply id="S3.E3.m1.11.11.1.1.3.2.2.2.1.cmml" xref="S3.E3.m1.11.11.1.1.3.2.2.2.2"><log id="S3.E3.m1.10.10.cmml" xref="S3.E3.m1.10.10"></log><apply id="S3.E3.m1.9.9.cmml" xref="S3.E3.m1.9.9"><divide id="S3.E3.m1.9.9.7.cmml" xref="S3.E3.m1.9.9"></divide><apply id="S3.E3.m1.5.5.2.3.cmml" xref="S3.E3.m1.5.5.2.2"><exp id="S3.E3.m1.4.4.1.1.cmml" xref="S3.E3.m1.4.4.1.1"></exp><apply id="S3.E3.m1.5.5.2.2.1.1.cmml" xref="S3.E3.m1.5.5.2.2.1.1"><times id="S3.E3.m1.5.5.2.2.1.1.1.cmml" xref="S3.E3.m1.5.5.2.2.1.1.1"></times><apply id="S3.E3.m1.5.5.2.2.1.1.2.cmml" xref="S3.E3.m1.5.5.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.5.5.2.2.1.1.2.1.cmml" xref="S3.E3.m1.5.5.2.2.1.1.2">superscript</csymbol><ci id="S3.E3.m1.5.5.2.2.1.1.2.2.cmml" xref="S3.E3.m1.5.5.2.2.1.1.2.2">𝐪</ci><csymbol cd="latexml" id="S3.E3.m1.5.5.2.2.1.1.2.3.cmml" xref="S3.E3.m1.5.5.2.2.1.1.2.3">top</csymbol></apply><apply id="S3.E3.m1.5.5.2.2.1.1.3.cmml" xref="S3.E3.m1.5.5.2.2.1.1.3"><ci id="S3.E3.m1.5.5.2.2.1.1.3.1.cmml" xref="S3.E3.m1.5.5.2.2.1.1.3.1">~</ci><ci id="S3.E3.m1.5.5.2.2.1.1.3.2.cmml" xref="S3.E3.m1.5.5.2.2.1.1.3.2">𝐯</ci></apply></apply></apply><apply id="S3.E3.m1.9.9.6.cmml" xref="S3.E3.m1.9.9.6"><plus id="S3.E3.m1.9.9.6.5.cmml" xref="S3.E3.m1.9.9.6.5"></plus><apply id="S3.E3.m1.8.8.5.3.2.cmml" xref="S3.E3.m1.8.8.5.3.1"><exp id="S3.E3.m1.6.6.3.1.cmml" xref="S3.E3.m1.6.6.3.1"></exp><apply id="S3.E3.m1.8.8.5.3.1.1.1.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1"><times id="S3.E3.m1.8.8.5.3.1.1.1.1.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1.1"></times><apply id="S3.E3.m1.8.8.5.3.1.1.1.2.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.8.8.5.3.1.1.1.2.1.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1.2">superscript</csymbol><ci id="S3.E3.m1.8.8.5.3.1.1.1.2.2.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1.2.2">𝐪</ci><csymbol cd="latexml" id="S3.E3.m1.8.8.5.3.1.1.1.2.3.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1.2.3">top</csymbol></apply><apply id="S3.E3.m1.8.8.5.3.1.1.1.3.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1.3"><ci id="S3.E3.m1.8.8.5.3.1.1.1.3.1.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1.3.1">~</ci><ci id="S3.E3.m1.8.8.5.3.1.1.1.3.2.cmml" xref="S3.E3.m1.8.8.5.3.1.1.1.3.2">𝐯</ci></apply></apply></apply><apply id="S3.E3.m1.9.9.6.4.cmml" xref="S3.E3.m1.9.9.6.4"><apply id="S3.E3.m1.9.9.6.4.2.cmml" xref="S3.E3.m1.9.9.6.4.2"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.6.4.2.1.cmml" xref="S3.E3.m1.9.9.6.4.2">subscript</csymbol><sum id="S3.E3.m1.9.9.6.4.2.2.cmml" xref="S3.E3.m1.9.9.6.4.2.2"></sum><apply id="S3.E3.m1.9.9.6.4.2.3.cmml" xref="S3.E3.m1.9.9.6.4.2.3"><in id="S3.E3.m1.9.9.6.4.2.3.1.cmml" xref="S3.E3.m1.9.9.6.4.2.3.1"></in><ci id="S3.E3.m1.9.9.6.4.2.3.2.cmml" xref="S3.E3.m1.9.9.6.4.2.3.2">𝐧</ci><ci id="S3.E3.m1.9.9.6.4.2.3.3.cmml" xref="S3.E3.m1.9.9.6.4.2.3.3">𝒩</ci></apply></apply><apply id="S3.E3.m1.9.9.6.4.1.2.cmml" xref="S3.E3.m1.9.9.6.4.1.1"><exp id="S3.E3.m1.7.7.4.2.cmml" xref="S3.E3.m1.7.7.4.2"></exp><apply id="S3.E3.m1.9.9.6.4.1.1.1.1.cmml" xref="S3.E3.m1.9.9.6.4.1.1.1.1"><times id="S3.E3.m1.9.9.6.4.1.1.1.1.1.cmml" xref="S3.E3.m1.9.9.6.4.1.1.1.1.1"></times><apply id="S3.E3.m1.9.9.6.4.1.1.1.1.2.cmml" xref="S3.E3.m1.9.9.6.4.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.9.9.6.4.1.1.1.1.2.1.cmml" xref="S3.E3.m1.9.9.6.4.1.1.1.1.2">superscript</csymbol><ci id="S3.E3.m1.9.9.6.4.1.1.1.1.2.2.cmml" xref="S3.E3.m1.9.9.6.4.1.1.1.1.2.2">𝐪</ci><csymbol cd="latexml" id="S3.E3.m1.9.9.6.4.1.1.1.1.2.3.cmml" xref="S3.E3.m1.9.9.6.4.1.1.1.1.2.3">top</csymbol></apply><ci id="S3.E3.m1.9.9.6.4.1.1.1.1.3.cmml" xref="S3.E3.m1.9.9.6.4.1.1.1.1.3">𝐧</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.11c">\mathcal{L}_{\textit{reindex}}=-\frac{1}{|\mathcal{D}|}\sum_{\mathbf{q},%
\mathbf{\tilde{v}}\in\mathbf{\mathcal{D}}}\log\left[\frac{\exp\left(\mathbf{q}%
^{\top}\mathbf{\tilde{v}}\right)}{\exp\left(\mathbf{q}^{\top}\mathbf{\tilde{v}%
}\right)+\sum_{\mathbf{n}\in\mathcal{N}}\exp\left(\mathbf{q}^{\top}\mathbf{n}%
\right)}\right],</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.11d">caligraphic_L start_POSTSUBSCRIPT reindex end_POSTSUBSCRIPT = - divide start_ARG 1 end_ARG start_ARG | caligraphic_D | end_ARG ∑ start_POSTSUBSCRIPT bold_q , over~ start_ARG bold_v end_ARG ∈ caligraphic_D end_POSTSUBSCRIPT roman_log [ divide start_ARG roman_exp ( bold_q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT over~ start_ARG bold_v end_ARG ) end_ARG start_ARG roman_exp ( bold_q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT over~ start_ARG bold_v end_ARG ) + ∑ start_POSTSUBSCRIPT bold_n ∈ caligraphic_N end_POSTSUBSCRIPT roman_exp ( bold_q start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT bold_n ) end_ARG ] ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.SSS3.p1.6">where we loop over the training set <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.1.m1.1"><semantics id="S3.SS2.SSS3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS3.p1.1.m1.1.1" xref="S3.SS2.SSS3.p1.1.m1.1.1.cmml">𝒟</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.1.m1.1b"><ci id="S3.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS3.p1.1.m1.1.1">𝒟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.1.m1.1d">caligraphic_D</annotation></semantics></math> that consists of <math alttext="(\mathbf{q},\tilde{\mathbf{v}})" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.2.m2.2"><semantics id="S3.SS2.SSS3.p1.2.m2.2a"><mrow id="S3.SS2.SSS3.p1.2.m2.2.3.2" xref="S3.SS2.SSS3.p1.2.m2.2.3.1.cmml"><mo id="S3.SS2.SSS3.p1.2.m2.2.3.2.1" stretchy="false" xref="S3.SS2.SSS3.p1.2.m2.2.3.1.cmml">(</mo><mi id="S3.SS2.SSS3.p1.2.m2.1.1" xref="S3.SS2.SSS3.p1.2.m2.1.1.cmml">𝐪</mi><mo id="S3.SS2.SSS3.p1.2.m2.2.3.2.2" xref="S3.SS2.SSS3.p1.2.m2.2.3.1.cmml">,</mo><mover accent="true" id="S3.SS2.SSS3.p1.2.m2.2.2" xref="S3.SS2.SSS3.p1.2.m2.2.2.cmml"><mi id="S3.SS2.SSS3.p1.2.m2.2.2.2" xref="S3.SS2.SSS3.p1.2.m2.2.2.2.cmml">𝐯</mi><mo id="S3.SS2.SSS3.p1.2.m2.2.2.1" xref="S3.SS2.SSS3.p1.2.m2.2.2.1.cmml">~</mo></mover><mo id="S3.SS2.SSS3.p1.2.m2.2.3.2.3" stretchy="false" xref="S3.SS2.SSS3.p1.2.m2.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.2.m2.2b"><interval closure="open" id="S3.SS2.SSS3.p1.2.m2.2.3.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.2.3.2"><ci id="S3.SS2.SSS3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.1.1">𝐪</ci><apply id="S3.SS2.SSS3.p1.2.m2.2.2.cmml" xref="S3.SS2.SSS3.p1.2.m2.2.2"><ci id="S3.SS2.SSS3.p1.2.m2.2.2.1.cmml" xref="S3.SS2.SSS3.p1.2.m2.2.2.1">~</ci><ci id="S3.SS2.SSS3.p1.2.m2.2.2.2.cmml" xref="S3.SS2.SSS3.p1.2.m2.2.2.2">𝐯</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.2.m2.2c">(\mathbf{q},\tilde{\mathbf{v}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.2.m2.2d">( bold_q , over~ start_ARG bold_v end_ARG )</annotation></semantics></math> pairs. Those pairs are collected from sentences containing target items. Here, <math alttext="\mathbf{q}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.3.m3.1"><semantics id="S3.SS2.SSS3.p1.3.m3.1a"><mrow id="S3.SS2.SSS3.p1.3.m3.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.cmml"><mi id="S3.SS2.SSS3.p1.3.m3.1.1.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.2.cmml">𝐪</mi><mo id="S3.SS2.SSS3.p1.3.m3.1.1.1" xref="S3.SS2.SSS3.p1.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS3.p1.3.m3.1.1.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.3.cmml"><mi id="S3.SS2.SSS3.p1.3.m3.1.1.3.2" xref="S3.SS2.SSS3.p1.3.m3.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.SSS3.p1.3.m3.1.1.3.3" xref="S3.SS2.SSS3.p1.3.m3.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.3.m3.1b"><apply id="S3.SS2.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1"><in id="S3.SS2.SSS3.p1.3.m3.1.1.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.1"></in><ci id="S3.SS2.SSS3.p1.3.m3.1.1.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.2">𝐪</ci><apply id="S3.SS2.SSS3.p1.3.m3.1.1.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.3.m3.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS3.p1.3.m3.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.3.2">ℝ</ci><ci id="S3.SS2.SSS3.p1.3.m3.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.3.m3.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.3.m3.1c">\mathbf{q}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.3.m3.1d">bold_q ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> is the contextual embedding of the last position from a LLM, which is originally used to generate the first token of original indexed items, but now we aim to force the aggregated item representation <math alttext="\mathbf{\tilde{v}}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.4.m4.1"><semantics id="S3.SS2.SSS3.p1.4.m4.1a"><mrow id="S3.SS2.SSS3.p1.4.m4.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.cmml"><mover accent="true" id="S3.SS2.SSS3.p1.4.m4.1.1.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.2.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.2.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.2.2.cmml">𝐯</mi><mo id="S3.SS2.SSS3.p1.4.m4.1.1.2.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.2.1.cmml">~</mo></mover><mo id="S3.SS2.SSS3.p1.4.m4.1.1.1" xref="S3.SS2.SSS3.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS3.p1.4.m4.1.1.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.cmml"><mi id="S3.SS2.SSS3.p1.4.m4.1.1.3.2" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.SSS3.p1.4.m4.1.1.3.3" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.4.m4.1b"><apply id="S3.SS2.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1"><in id="S3.SS2.SSS3.p1.4.m4.1.1.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.1"></in><apply id="S3.SS2.SSS3.p1.4.m4.1.1.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.2"><ci id="S3.SS2.SSS3.p1.4.m4.1.1.2.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.2.1">~</ci><ci id="S3.SS2.SSS3.p1.4.m4.1.1.2.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.2.2">𝐯</ci></apply><apply id="S3.SS2.SSS3.p1.4.m4.1.1.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.4.m4.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS3.p1.4.m4.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.2">ℝ</ci><ci id="S3.SS2.SSS3.p1.4.m4.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.4.m4.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.4.m4.1c">\mathbf{\tilde{v}}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.4.m4.1d">over~ start_ARG bold_v end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> described in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.E2" title="In 3.2.2 Aggregate Multi-Token Embeddings ‣ 3.2 Reindex Step: Single-Token Items in LLMs ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">2</span></a> to be generated by LLMs. To achieve this reindex step, we also prepare negatives <math alttext="\mathbf{n}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.5.m5.1"><semantics id="S3.SS2.SSS3.p1.5.m5.1a"><mrow id="S3.SS2.SSS3.p1.5.m5.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.cmml"><mi id="S3.SS2.SSS3.p1.5.m5.1.1.2" xref="S3.SS2.SSS3.p1.5.m5.1.1.2.cmml">𝐧</mi><mo id="S3.SS2.SSS3.p1.5.m5.1.1.1" xref="S3.SS2.SSS3.p1.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS2.SSS3.p1.5.m5.1.1.3" xref="S3.SS2.SSS3.p1.5.m5.1.1.3.cmml"><mi id="S3.SS2.SSS3.p1.5.m5.1.1.3.2" xref="S3.SS2.SSS3.p1.5.m5.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS2.SSS3.p1.5.m5.1.1.3.3" xref="S3.SS2.SSS3.p1.5.m5.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.5.m5.1b"><apply id="S3.SS2.SSS3.p1.5.m5.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1"><in id="S3.SS2.SSS3.p1.5.m5.1.1.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.1"></in><ci id="S3.SS2.SSS3.p1.5.m5.1.1.2.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.2">𝐧</ci><apply id="S3.SS2.SSS3.p1.5.m5.1.1.3.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.SSS3.p1.5.m5.1.1.3.1.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS2.SSS3.p1.5.m5.1.1.3.2.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.3.2">ℝ</ci><ci id="S3.SS2.SSS3.p1.5.m5.1.1.3.3.cmml" xref="S3.SS2.SSS3.p1.5.m5.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.5.m5.1c">\mathbf{n}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.5.m5.1d">bold_n ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> from the negative representation set <math alttext="\mathcal{N}" class="ltx_Math" display="inline" id="S3.SS2.SSS3.p1.6.m6.1"><semantics id="S3.SS2.SSS3.p1.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.SSS3.p1.6.m6.1.1" xref="S3.SS2.SSS3.p1.6.m6.1.1.cmml">𝒩</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS3.p1.6.m6.1b"><ci id="S3.SS2.SSS3.p1.6.m6.1.1.cmml" xref="S3.SS2.SSS3.p1.6.m6.1.1">𝒩</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS3.p1.6.m6.1c">\mathcal{N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS3.p1.6.m6.1d">caligraphic_N</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">Two groups of corpus are considered in the <span class="ltx_text ltx_font_typewriter" id="S3.SS2.SSS3.p2.1.1">reindex</span> step:</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i1.p1.1.1">L2R Data</span>: In this training corpus, (query, target) pairs are used as <span class="ltx_text ltx_font_typewriter" id="S3.I2.i1.p1.1.2">L2R</span> samples. In CRS context, those are samples from the conversations.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i2.p1.1.1">L2I Data</span>: In this training corpus, (content, target) pairs are used as <span class="ltx_text ltx_font_typewriter" id="S3.I2.i2.p1.1.2">L2I</span> samples. In CRS context, those are samples from the item metadata like textual descriptions.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S3.I2.i3.p1.1.1">Data Mixture</span>: In this case, we consider mixing the both <span class="ltx_text ltx_font_typewriter" id="S3.I2.i3.p1.1.2">L2R</span> and <span class="ltx_text ltx_font_typewriter" id="S3.I2.i3.p1.1.3">L2I</span> samples as a unified corpus and use it to train our model jointly. We use this option and include details in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A1.SS2" title="A.2 Implementation Details ‣ Appendix A More Details of Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span><span class="ltx_text ltx_font_typewriter" id="S3.SS3.1.1">Adapt</span> Step: Item Probabilities Adjustment</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">After re-indexing, all the items are represented by single-token embeddings. It makes recommendation as easy as one-step decoding in LLMs, and also enables multiple efficient ways to adjust the recommendation item distributions to adapt towards target platforms or specific data distributions. We introduce two types of adaptation methods in the following sections, one for item popularity adjustments, and another one for combining with the traditional recommender systems. To start with, we assume the logit vector <math alttext="\mathbf{g}\in\mathbb{R}^{|\mathcal{I}|}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mrow id="S3.SS3.p1.1.m1.1.2" xref="S3.SS3.p1.1.m1.1.2.cmml"><mi id="S3.SS3.p1.1.m1.1.2.2" xref="S3.SS3.p1.1.m1.1.2.2.cmml">𝐠</mi><mo id="S3.SS3.p1.1.m1.1.2.1" xref="S3.SS3.p1.1.m1.1.2.1.cmml">∈</mo><msup id="S3.SS3.p1.1.m1.1.2.3" xref="S3.SS3.p1.1.m1.1.2.3.cmml"><mi id="S3.SS3.p1.1.m1.1.2.3.2" xref="S3.SS3.p1.1.m1.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p1.1.m1.1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.1.2.cmml"><mo id="S3.SS3.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S3.SS3.p1.1.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.1.m1.1.1.1.1" xref="S3.SS3.p1.1.m1.1.1.1.1.cmml">ℐ</mi><mo id="S3.SS3.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S3.SS3.p1.1.m1.1.1.1.2.1.cmml">|</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.2"><in id="S3.SS3.p1.1.m1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.2.1"></in><ci id="S3.SS3.p1.1.m1.1.2.2.cmml" xref="S3.SS3.p1.1.m1.1.2.2">𝐠</ci><apply id="S3.SS3.p1.1.m1.1.2.3.cmml" xref="S3.SS3.p1.1.m1.1.2.3"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.2.3.1.cmml" xref="S3.SS3.p1.1.m1.1.2.3">superscript</csymbol><ci id="S3.SS3.p1.1.m1.1.2.3.2.cmml" xref="S3.SS3.p1.1.m1.1.2.3.2">ℝ</ci><apply id="S3.SS3.p1.1.m1.1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3"><abs id="S3.SS3.p1.1.m1.1.1.1.2.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.3.1"></abs><ci id="S3.SS3.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1.1.1">ℐ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\mathbf{g}\in\mathbb{R}^{|\mathcal{I}|}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">bold_g ∈ blackboard_R start_POSTSUPERSCRIPT | caligraphic_I | end_POSTSUPERSCRIPT</annotation></semantics></math> has already been given by the LLMs, and the corresponding probability vector should be <math alttext="\mathbf{p}=\texttt{softmax}(\mathbf{g})" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mrow id="S3.SS3.p1.2.m2.1.2" xref="S3.SS3.p1.2.m2.1.2.cmml"><mi id="S3.SS3.p1.2.m2.1.2.2" xref="S3.SS3.p1.2.m2.1.2.2.cmml">𝐩</mi><mo id="S3.SS3.p1.2.m2.1.2.1" xref="S3.SS3.p1.2.m2.1.2.1.cmml">=</mo><mrow id="S3.SS3.p1.2.m2.1.2.3" xref="S3.SS3.p1.2.m2.1.2.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.p1.2.m2.1.2.3.2" xref="S3.SS3.p1.2.m2.1.2.3.2a.cmml">softmax</mtext><mo id="S3.SS3.p1.2.m2.1.2.3.1" xref="S3.SS3.p1.2.m2.1.2.3.1.cmml">⁢</mo><mrow id="S3.SS3.p1.2.m2.1.2.3.3.2" xref="S3.SS3.p1.2.m2.1.2.3.cmml"><mo id="S3.SS3.p1.2.m2.1.2.3.3.2.1" stretchy="false" xref="S3.SS3.p1.2.m2.1.2.3.cmml">(</mo><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">𝐠</mi><mo id="S3.SS3.p1.2.m2.1.2.3.3.2.2" stretchy="false" xref="S3.SS3.p1.2.m2.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.2.cmml" xref="S3.SS3.p1.2.m2.1.2"><eq id="S3.SS3.p1.2.m2.1.2.1.cmml" xref="S3.SS3.p1.2.m2.1.2.1"></eq><ci id="S3.SS3.p1.2.m2.1.2.2.cmml" xref="S3.SS3.p1.2.m2.1.2.2">𝐩</ci><apply id="S3.SS3.p1.2.m2.1.2.3.cmml" xref="S3.SS3.p1.2.m2.1.2.3"><times id="S3.SS3.p1.2.m2.1.2.3.1.cmml" xref="S3.SS3.p1.2.m2.1.2.3.1"></times><ci id="S3.SS3.p1.2.m2.1.2.3.2a.cmml" xref="S3.SS3.p1.2.m2.1.2.3.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.p1.2.m2.1.2.3.2.cmml" xref="S3.SS3.p1.2.m2.1.2.3.2">softmax</mtext></ci><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝐠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathbf{p}=\texttt{softmax}(\mathbf{g})</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">bold_p = softmax ( bold_g )</annotation></semantics></math>.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Bias Term Adjustment</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.6">Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib63" title="">2021</a>)</cite>, a common way to adjust logits is an affine transformation, i.e.:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathbf{p}}=\texttt{softmax}\left(\mathbf{g}\mathbf{W}+\mathbf{b}\right)," class="ltx_Math" display="block" id="S3.E4.m1.1"><semantics id="S3.E4.m1.1a"><mrow id="S3.E4.m1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mrow id="S3.E4.m1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.cmml"><mover accent="true" id="S3.E4.m1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.3.2.cmml">𝐩</mi><mo id="S3.E4.m1.1.1.1.1.3.1" xref="S3.E4.m1.1.1.1.1.3.1.cmml">^</mo></mover><mo id="S3.E4.m1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.3a.cmml">softmax</mtext><mo id="S3.E4.m1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml">𝐠𝐖</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">+</mo><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml">𝐛</mi></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E4.m1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.1b"><apply id="S3.E4.m1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1"><eq id="S3.E4.m1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.2"></eq><apply id="S3.E4.m1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.3"><ci id="S3.E4.m1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.3.1">^</ci><ci id="S3.E4.m1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.3.2">𝐩</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><times id="S3.E4.m1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.2"></times><ci id="S3.E4.m1.1.1.1.1.1.3a.cmml" xref="S3.E4.m1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E4.m1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.3">softmax</mtext></ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1"><plus id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1"></plus><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.2">𝐠𝐖</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.3">𝐛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.1c">\hat{\mathbf{p}}=\texttt{softmax}\left(\mathbf{g}\mathbf{W}+\mathbf{b}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.1d">over^ start_ARG bold_p end_ARG = softmax ( bold_gW + bold_b ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS1.p1.5">where <math alttext="\mathbf{W}\in\mathbb{R}^{|\mathcal{I}|\times|\mathcal{I}|}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.1.m1.2"><semantics id="S3.SS3.SSS1.p1.1.m1.2a"><mrow id="S3.SS3.SSS1.p1.1.m1.2.3" xref="S3.SS3.SSS1.p1.1.m1.2.3.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.2.3.2" xref="S3.SS3.SSS1.p1.1.m1.2.3.2.cmml">𝐖</mi><mo id="S3.SS3.SSS1.p1.1.m1.2.3.1" xref="S3.SS3.SSS1.p1.1.m1.2.3.1.cmml">∈</mo><msup id="S3.SS3.SSS1.p1.1.m1.2.3.3" xref="S3.SS3.SSS1.p1.1.m1.2.3.3.cmml"><mi id="S3.SS3.SSS1.p1.1.m1.2.3.3.2" xref="S3.SS3.SSS1.p1.1.m1.2.3.3.2.cmml">ℝ</mi><mrow id="S3.SS3.SSS1.p1.1.m1.2.2.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.cmml"><mrow id="S3.SS3.SSS1.p1.1.m1.2.2.2.4.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.4.1.cmml"><mo id="S3.SS3.SSS1.p1.1.m1.2.2.2.4.2.1" stretchy="false" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.4.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.1.m1.1.1.1.1" xref="S3.SS3.SSS1.p1.1.m1.1.1.1.1.cmml">ℐ</mi><mo id="S3.SS3.SSS1.p1.1.m1.2.2.2.4.2.2" rspace="0.055em" stretchy="false" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.4.1.1.cmml">|</mo></mrow><mo id="S3.SS3.SSS1.p1.1.m1.2.2.2.3" rspace="0.222em" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.3.cmml">×</mo><mrow id="S3.SS3.SSS1.p1.1.m1.2.2.2.5.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.5.1.cmml"><mo id="S3.SS3.SSS1.p1.1.m1.2.2.2.5.2.1" stretchy="false" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.5.1.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.1.m1.2.2.2.2" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.2.cmml">ℐ</mi><mo id="S3.SS3.SSS1.p1.1.m1.2.2.2.5.2.2" stretchy="false" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.5.1.1.cmml">|</mo></mrow></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.1.m1.2b"><apply id="S3.SS3.SSS1.p1.1.m1.2.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.3"><in id="S3.SS3.SSS1.p1.1.m1.2.3.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.3.1"></in><ci id="S3.SS3.SSS1.p1.1.m1.2.3.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.3.2">𝐖</ci><apply id="S3.SS3.SSS1.p1.1.m1.2.3.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.3.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.1.m1.2.3.3.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.3.3">superscript</csymbol><ci id="S3.SS3.SSS1.p1.1.m1.2.3.3.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.3.3.2">ℝ</ci><apply id="S3.SS3.SSS1.p1.1.m1.2.2.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.2"><times id="S3.SS3.SSS1.p1.1.m1.2.2.2.3.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.3"></times><apply id="S3.SS3.SSS1.p1.1.m1.2.2.2.4.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.4.2"><abs id="S3.SS3.SSS1.p1.1.m1.2.2.2.4.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.4.2.1"></abs><ci id="S3.SS3.SSS1.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.1.1.1.1">ℐ</ci></apply><apply id="S3.SS3.SSS1.p1.1.m1.2.2.2.5.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.5.2"><abs id="S3.SS3.SSS1.p1.1.m1.2.2.2.5.1.1.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.5.2.1"></abs><ci id="S3.SS3.SSS1.p1.1.m1.2.2.2.2.cmml" xref="S3.SS3.SSS1.p1.1.m1.2.2.2.2">ℐ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.1.m1.2c">\mathbf{W}\in\mathbb{R}^{|\mathcal{I}|\times|\mathcal{I}|}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.1.m1.2d">bold_W ∈ blackboard_R start_POSTSUPERSCRIPT | caligraphic_I | × | caligraphic_I | end_POSTSUPERSCRIPT</annotation></semantics></math> is a weight matrix and <math alttext="\mathbf{b}\in\mathbb{R}^{|\mathcal{I}|}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.2.m2.1"><semantics id="S3.SS3.SSS1.p1.2.m2.1a"><mrow id="S3.SS3.SSS1.p1.2.m2.1.2" xref="S3.SS3.SSS1.p1.2.m2.1.2.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.2.2" xref="S3.SS3.SSS1.p1.2.m2.1.2.2.cmml">𝐛</mi><mo id="S3.SS3.SSS1.p1.2.m2.1.2.1" xref="S3.SS3.SSS1.p1.2.m2.1.2.1.cmml">∈</mo><msup id="S3.SS3.SSS1.p1.2.m2.1.2.3" xref="S3.SS3.SSS1.p1.2.m2.1.2.3.cmml"><mi id="S3.SS3.SSS1.p1.2.m2.1.2.3.2" xref="S3.SS3.SSS1.p1.2.m2.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS3.SSS1.p1.2.m2.1.1.1.3" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.2.cmml"><mo id="S3.SS3.SSS1.p1.2.m2.1.1.1.3.1" stretchy="false" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS1.p1.2.m2.1.1.1.1" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.1.cmml">ℐ</mi><mo id="S3.SS3.SSS1.p1.2.m2.1.1.1.3.2" stretchy="false" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.2.1.cmml">|</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.2.m2.1b"><apply id="S3.SS3.SSS1.p1.2.m2.1.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.2"><in id="S3.SS3.SSS1.p1.2.m2.1.2.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.2.1"></in><ci id="S3.SS3.SSS1.p1.2.m2.1.2.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.2.2">𝐛</ci><apply id="S3.SS3.SSS1.p1.2.m2.1.2.3.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS1.p1.2.m2.1.2.3.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.2.3">superscript</csymbol><ci id="S3.SS3.SSS1.p1.2.m2.1.2.3.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.2.3.2">ℝ</ci><apply id="S3.SS3.SSS1.p1.2.m2.1.1.1.2.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.3"><abs id="S3.SS3.SSS1.p1.2.m2.1.1.1.2.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.3.1"></abs><ci id="S3.SS3.SSS1.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.SSS1.p1.2.m2.1.1.1.1">ℐ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.2.m2.1c">\mathbf{b}\in\mathbb{R}^{|\mathcal{I}|}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.2.m2.1d">bold_b ∈ blackboard_R start_POSTSUPERSCRIPT | caligraphic_I | end_POSTSUPERSCRIPT</annotation></semantics></math> is the bias term. Similar to <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib63" title="">2021</a>)</cite>, we restrict the matrix <math alttext="\mathbf{W}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.3.m3.1"><semantics id="S3.SS3.SSS1.p1.3.m3.1a"><mi id="S3.SS3.SSS1.p1.3.m3.1.1" xref="S3.SS3.SSS1.p1.3.m3.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.3.m3.1b"><ci id="S3.SS3.SSS1.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS1.p1.3.m3.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.3.m3.1c">\mathbf{W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.3.m3.1d">bold_W</annotation></semantics></math> to be diagonal to prevent the size of parameters from growing quadratically in the size of items. Therefore, in this special case, we are able to interpret the <math alttext="\mathbf{W}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.4.m4.1"><semantics id="S3.SS3.SSS1.p1.4.m4.1a"><mi id="S3.SS3.SSS1.p1.4.m4.1.1" xref="S3.SS3.SSS1.p1.4.m4.1.1.cmml">𝐖</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.4.m4.1b"><ci id="S3.SS3.SSS1.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS1.p1.4.m4.1.1">𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.4.m4.1c">\mathbf{W}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.4.m4.1d">bold_W</annotation></semantics></math> and <math alttext="\mathbf{b}" class="ltx_Math" display="inline" id="S3.SS3.SSS1.p1.5.m5.1"><semantics id="S3.SS3.SSS1.p1.5.m5.1a"><mi id="S3.SS3.SSS1.p1.5.m5.1.1" xref="S3.SS3.SSS1.p1.5.m5.1.1.cmml">𝐛</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS1.p1.5.m5.1b"><ci id="S3.SS3.SSS1.p1.5.m5.1.1.cmml" xref="S3.SS3.SSS1.p1.5.m5.1.1">𝐛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS1.p1.5.m5.1c">\mathbf{b}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS1.p1.5.m5.1d">bold_b</annotation></semantics></math> as <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p1.5.1">multiplicative</span> and <span class="ltx_text ltx_font_italic" id="S3.SS3.SSS1.p1.5.2">additive bias</span> terms towards the target data distributions, respectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Traditional RecSys Gating</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Inspired by <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>, we notice that LLMs excel at content/context knowledge, but traditional RecSys, where the output logit vector can be denoted as <math alttext="\tilde{\mathbf{g}}\in\mathbb{R}^{|\mathcal{I}|}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.1.m1.1"><semantics id="S3.SS3.SSS2.p1.1.m1.1a"><mrow id="S3.SS3.SSS2.p1.1.m1.1.2" xref="S3.SS3.SSS2.p1.1.m1.1.2.cmml"><mover accent="true" id="S3.SS3.SSS2.p1.1.m1.1.2.2" xref="S3.SS3.SSS2.p1.1.m1.1.2.2.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.1.2.2.2" xref="S3.SS3.SSS2.p1.1.m1.1.2.2.2.cmml">𝐠</mi><mo id="S3.SS3.SSS2.p1.1.m1.1.2.2.1" xref="S3.SS3.SSS2.p1.1.m1.1.2.2.1.cmml">~</mo></mover><mo id="S3.SS3.SSS2.p1.1.m1.1.2.1" xref="S3.SS3.SSS2.p1.1.m1.1.2.1.cmml">∈</mo><msup id="S3.SS3.SSS2.p1.1.m1.1.2.3" xref="S3.SS3.SSS2.p1.1.m1.1.2.3.cmml"><mi id="S3.SS3.SSS2.p1.1.m1.1.2.3.2" xref="S3.SS3.SSS2.p1.1.m1.1.2.3.2.cmml">ℝ</mi><mrow id="S3.SS3.SSS2.p1.1.m1.1.1.1.3" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.2.cmml"><mo id="S3.SS3.SSS2.p1.1.m1.1.1.1.3.1" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS2.p1.1.m1.1.1.1.1" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.1.cmml">ℐ</mi><mo id="S3.SS3.SSS2.p1.1.m1.1.1.1.3.2" stretchy="false" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.2.1.cmml">|</mo></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.1.m1.1b"><apply id="S3.SS3.SSS2.p1.1.m1.1.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.2"><in id="S3.SS3.SSS2.p1.1.m1.1.2.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.2.1"></in><apply id="S3.SS3.SSS2.p1.1.m1.1.2.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.2.2"><ci id="S3.SS3.SSS2.p1.1.m1.1.2.2.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.2.2.1">~</ci><ci id="S3.SS3.SSS2.p1.1.m1.1.2.2.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.2.2.2">𝐠</ci></apply><apply id="S3.SS3.SSS2.p1.1.m1.1.2.3.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.1.m1.1.2.3.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.2.3">superscript</csymbol><ci id="S3.SS3.SSS2.p1.1.m1.1.2.3.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.2.3.2">ℝ</ci><apply id="S3.SS3.SSS2.p1.1.m1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.3"><abs id="S3.SS3.SSS2.p1.1.m1.1.1.1.2.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.3.1"></abs><ci id="S3.SS3.SSS2.p1.1.m1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.1.m1.1.1.1.1">ℐ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.1.m1.1c">\tilde{\mathbf{g}}\in\mathbb{R}^{|\mathcal{I}|}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.1.m1.1d">over~ start_ARG bold_g end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT | caligraphic_I | end_POSTSUPERSCRIPT</annotation></semantics></math>, is good at collaborative knowledge instead. Motivated by this observation, combining those two types of models becomes easy after the re-indexing step:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\hat{\mathbf{p}}=\texttt{softmax}\left(\alpha\mathbf{g}+(1-\alpha)\tilde{%
\mathbf{g}}\right)," class="ltx_Math" display="block" id="S3.E5.m1.1"><semantics id="S3.E5.m1.1a"><mrow id="S3.E5.m1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.cmml"><mover accent="true" id="S3.E5.m1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.3.2.cmml">𝐩</mi><mo id="S3.E5.m1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.3.1.cmml">^</mo></mover><mo id="S3.E5.m1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E5.m1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.E5.m1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.3a.cmml">softmax</mtext><mo id="S3.E5.m1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml">α</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml">𝐠</mi></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2.cmml">+</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mn id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">−</mo><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">α</mi></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mover accent="true" id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">𝐠</mi><mo id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.1" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.1.cmml">~</mo></mover></mrow></mrow><mo id="S3.E5.m1.1.1.1.1.1.1.1.3" xref="S3.E5.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E5.m1.1.1.1.2" xref="S3.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E5.m1.1b"><apply id="S3.E5.m1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1"><eq id="S3.E5.m1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.2"></eq><apply id="S3.E5.m1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.3"><ci id="S3.E5.m1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.3.1">^</ci><ci id="S3.E5.m1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.3.2">𝐩</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.2"></times><ci id="S3.E5.m1.1.1.1.1.1.3a.cmml" xref="S3.E5.m1.1.1.1.1.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.E5.m1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.3">softmax</mtext></ci><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1"><plus id="S3.E5.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.2"></plus><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3"><times id="S3.E5.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.2">𝛼</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.3.3">𝐠</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1"><times id="S3.E5.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.1"></minus><cn id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝛼</ci></apply><apply id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3"><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.1">~</ci><ci id="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E5.m1.1.1.1.1.1.1.1.1.1.3.2">𝐠</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E5.m1.1c">\hat{\mathbf{p}}=\texttt{softmax}\left(\alpha\mathbf{g}+(1-\alpha)\tilde{%
\mathbf{g}}\right),</annotation><annotation encoding="application/x-llamapun" id="S3.E5.m1.1d">over^ start_ARG bold_p end_ARG = softmax ( italic_α bold_g + ( 1 - italic_α ) over~ start_ARG bold_g end_ARG ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS2.p1.6">where the coefficient <math alttext="\alpha\in[0,1]" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.2.m1.2"><semantics id="S3.SS3.SSS2.p1.2.m1.2a"><mrow id="S3.SS3.SSS2.p1.2.m1.2.3" xref="S3.SS3.SSS2.p1.2.m1.2.3.cmml"><mi id="S3.SS3.SSS2.p1.2.m1.2.3.2" xref="S3.SS3.SSS2.p1.2.m1.2.3.2.cmml">α</mi><mo id="S3.SS3.SSS2.p1.2.m1.2.3.1" xref="S3.SS3.SSS2.p1.2.m1.2.3.1.cmml">∈</mo><mrow id="S3.SS3.SSS2.p1.2.m1.2.3.3.2" xref="S3.SS3.SSS2.p1.2.m1.2.3.3.1.cmml"><mo id="S3.SS3.SSS2.p1.2.m1.2.3.3.2.1" stretchy="false" xref="S3.SS3.SSS2.p1.2.m1.2.3.3.1.cmml">[</mo><mn id="S3.SS3.SSS2.p1.2.m1.1.1" xref="S3.SS3.SSS2.p1.2.m1.1.1.cmml">0</mn><mo id="S3.SS3.SSS2.p1.2.m1.2.3.3.2.2" xref="S3.SS3.SSS2.p1.2.m1.2.3.3.1.cmml">,</mo><mn id="S3.SS3.SSS2.p1.2.m1.2.2" xref="S3.SS3.SSS2.p1.2.m1.2.2.cmml">1</mn><mo id="S3.SS3.SSS2.p1.2.m1.2.3.3.2.3" stretchy="false" xref="S3.SS3.SSS2.p1.2.m1.2.3.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.2.m1.2b"><apply id="S3.SS3.SSS2.p1.2.m1.2.3.cmml" xref="S3.SS3.SSS2.p1.2.m1.2.3"><in id="S3.SS3.SSS2.p1.2.m1.2.3.1.cmml" xref="S3.SS3.SSS2.p1.2.m1.2.3.1"></in><ci id="S3.SS3.SSS2.p1.2.m1.2.3.2.cmml" xref="S3.SS3.SSS2.p1.2.m1.2.3.2">𝛼</ci><interval closure="closed" id="S3.SS3.SSS2.p1.2.m1.2.3.3.1.cmml" xref="S3.SS3.SSS2.p1.2.m1.2.3.3.2"><cn id="S3.SS3.SSS2.p1.2.m1.1.1.cmml" type="integer" xref="S3.SS3.SSS2.p1.2.m1.1.1">0</cn><cn id="S3.SS3.SSS2.p1.2.m1.2.2.cmml" type="integer" xref="S3.SS3.SSS2.p1.2.m1.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.2.m1.2c">\alpha\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.2.m1.2d">italic_α ∈ [ 0 , 1 ]</annotation></semantics></math> can be set in many different ways. For simplicity sake, we use a learnable scalar <math alttext="\tilde{\alpha}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.3.m2.1"><semantics id="S3.SS3.SSS2.p1.3.m2.1a"><mover accent="true" id="S3.SS3.SSS2.p1.3.m2.1.1" xref="S3.SS3.SSS2.p1.3.m2.1.1.cmml"><mi id="S3.SS3.SSS2.p1.3.m2.1.1.2" xref="S3.SS3.SSS2.p1.3.m2.1.1.2.cmml">α</mi><mo id="S3.SS3.SSS2.p1.3.m2.1.1.1" xref="S3.SS3.SSS2.p1.3.m2.1.1.1.cmml">~</mo></mover><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.3.m2.1b"><apply id="S3.SS3.SSS2.p1.3.m2.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m2.1.1"><ci id="S3.SS3.SSS2.p1.3.m2.1.1.1.cmml" xref="S3.SS3.SSS2.p1.3.m2.1.1.1">~</ci><ci id="S3.SS3.SSS2.p1.3.m2.1.1.2.cmml" xref="S3.SS3.SSS2.p1.3.m2.1.1.2">𝛼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.3.m2.1c">\tilde{\alpha}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.3.m2.1d">over~ start_ARG italic_α end_ARG</annotation></semantics></math> for <math alttext="\alpha=\texttt{sigmoid}\left(\tilde{\alpha}\right)" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.4.m3.1"><semantics id="S3.SS3.SSS2.p1.4.m3.1a"><mrow id="S3.SS3.SSS2.p1.4.m3.1.2" xref="S3.SS3.SSS2.p1.4.m3.1.2.cmml"><mi id="S3.SS3.SSS2.p1.4.m3.1.2.2" xref="S3.SS3.SSS2.p1.4.m3.1.2.2.cmml">α</mi><mo id="S3.SS3.SSS2.p1.4.m3.1.2.1" xref="S3.SS3.SSS2.p1.4.m3.1.2.1.cmml">=</mo><mrow id="S3.SS3.SSS2.p1.4.m3.1.2.3" xref="S3.SS3.SSS2.p1.4.m3.1.2.3.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.SSS2.p1.4.m3.1.2.3.2" xref="S3.SS3.SSS2.p1.4.m3.1.2.3.2a.cmml">sigmoid</mtext><mo id="S3.SS3.SSS2.p1.4.m3.1.2.3.1" xref="S3.SS3.SSS2.p1.4.m3.1.2.3.1.cmml">⁢</mo><mrow id="S3.SS3.SSS2.p1.4.m3.1.2.3.3.2" xref="S3.SS3.SSS2.p1.4.m3.1.1.cmml"><mo id="S3.SS3.SSS2.p1.4.m3.1.2.3.3.2.1" xref="S3.SS3.SSS2.p1.4.m3.1.1.cmml">(</mo><mover accent="true" id="S3.SS3.SSS2.p1.4.m3.1.1" xref="S3.SS3.SSS2.p1.4.m3.1.1.cmml"><mi id="S3.SS3.SSS2.p1.4.m3.1.1.2" xref="S3.SS3.SSS2.p1.4.m3.1.1.2.cmml">α</mi><mo id="S3.SS3.SSS2.p1.4.m3.1.1.1" xref="S3.SS3.SSS2.p1.4.m3.1.1.1.cmml">~</mo></mover><mo id="S3.SS3.SSS2.p1.4.m3.1.2.3.3.2.2" xref="S3.SS3.SSS2.p1.4.m3.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.4.m3.1b"><apply id="S3.SS3.SSS2.p1.4.m3.1.2.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.2"><eq id="S3.SS3.SSS2.p1.4.m3.1.2.1.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.2.1"></eq><ci id="S3.SS3.SSS2.p1.4.m3.1.2.2.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.2.2">𝛼</ci><apply id="S3.SS3.SSS2.p1.4.m3.1.2.3.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.2.3"><times id="S3.SS3.SSS2.p1.4.m3.1.2.3.1.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.2.3.1"></times><ci id="S3.SS3.SSS2.p1.4.m3.1.2.3.2a.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.2.3.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.SSS2.p1.4.m3.1.2.3.2.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.2.3.2">sigmoid</mtext></ci><apply id="S3.SS3.SSS2.p1.4.m3.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.2.3.3.2"><ci id="S3.SS3.SSS2.p1.4.m3.1.1.1.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.1.1">~</ci><ci id="S3.SS3.SSS2.p1.4.m3.1.1.2.cmml" xref="S3.SS3.SSS2.p1.4.m3.1.1.2">𝛼</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.4.m3.1c">\alpha=\texttt{sigmoid}\left(\tilde{\alpha}\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.4.m3.1d">italic_α = sigmoid ( over~ start_ARG italic_α end_ARG )</annotation></semantics></math> in our experiments, but more options can be considered, such as being predicted by a MLP model to naturally determine how much we should weight the responses from LLMs or traditional RecSys, like <math alttext="\alpha=\texttt{sigmoid}\left(\texttt{MLP}(\mathbf{q})\right)" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.5.m4.2"><semantics id="S3.SS3.SSS2.p1.5.m4.2a"><mrow id="S3.SS3.SSS2.p1.5.m4.2.2" xref="S3.SS3.SSS2.p1.5.m4.2.2.cmml"><mi id="S3.SS3.SSS2.p1.5.m4.2.2.3" xref="S3.SS3.SSS2.p1.5.m4.2.2.3.cmml">α</mi><mo id="S3.SS3.SSS2.p1.5.m4.2.2.2" xref="S3.SS3.SSS2.p1.5.m4.2.2.2.cmml">=</mo><mrow id="S3.SS3.SSS2.p1.5.m4.2.2.1" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.SSS2.p1.5.m4.2.2.1.3" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.3a.cmml">sigmoid</mtext><mo id="S3.SS3.SSS2.p1.5.m4.2.2.1.2" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.2.cmml">⁢</mo><mrow id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.cmml"><mo id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.2" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.cmml">(</mo><mrow id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.2" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.2a.cmml">MLP</mtext><mo id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.1" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.3.2" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.cmml"><mo id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.3.2.1" stretchy="false" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.cmml">(</mo><mi id="S3.SS3.SSS2.p1.5.m4.1.1" xref="S3.SS3.SSS2.p1.5.m4.1.1.cmml">𝐪</mi><mo id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.3.2.2" stretchy="false" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.3" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.5.m4.2b"><apply id="S3.SS3.SSS2.p1.5.m4.2.2.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2"><eq id="S3.SS3.SSS2.p1.5.m4.2.2.2.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.2"></eq><ci id="S3.SS3.SSS2.p1.5.m4.2.2.3.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.3">𝛼</ci><apply id="S3.SS3.SSS2.p1.5.m4.2.2.1.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.1"><times id="S3.SS3.SSS2.p1.5.m4.2.2.1.2.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.2"></times><ci id="S3.SS3.SSS2.p1.5.m4.2.2.1.3a.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.3"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.SSS2.p1.5.m4.2.2.1.3.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.3">sigmoid</mtext></ci><apply id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1"><times id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.1"></times><ci id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.2a.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.2.cmml" xref="S3.SS3.SSS2.p1.5.m4.2.2.1.1.1.1.2">MLP</mtext></ci><ci id="S3.SS3.SSS2.p1.5.m4.1.1.cmml" xref="S3.SS3.SSS2.p1.5.m4.1.1">𝐪</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.5.m4.2c">\alpha=\texttt{sigmoid}\left(\texttt{MLP}(\mathbf{q})\right)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.5.m4.2d">italic_α = sigmoid ( MLP ( bold_q ) )</annotation></semantics></math>, where <math alttext="\mathbf{q}\in\mathbb{R}^{d}" class="ltx_Math" display="inline" id="S3.SS3.SSS2.p1.6.m5.1"><semantics id="S3.SS3.SSS2.p1.6.m5.1a"><mrow id="S3.SS3.SSS2.p1.6.m5.1.1" xref="S3.SS3.SSS2.p1.6.m5.1.1.cmml"><mi id="S3.SS3.SSS2.p1.6.m5.1.1.2" xref="S3.SS3.SSS2.p1.6.m5.1.1.2.cmml">𝐪</mi><mo id="S3.SS3.SSS2.p1.6.m5.1.1.1" xref="S3.SS3.SSS2.p1.6.m5.1.1.1.cmml">∈</mo><msup id="S3.SS3.SSS2.p1.6.m5.1.1.3" xref="S3.SS3.SSS2.p1.6.m5.1.1.3.cmml"><mi id="S3.SS3.SSS2.p1.6.m5.1.1.3.2" xref="S3.SS3.SSS2.p1.6.m5.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS3.SSS2.p1.6.m5.1.1.3.3" xref="S3.SS3.SSS2.p1.6.m5.1.1.3.3.cmml">d</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS2.p1.6.m5.1b"><apply id="S3.SS3.SSS2.p1.6.m5.1.1.cmml" xref="S3.SS3.SSS2.p1.6.m5.1.1"><in id="S3.SS3.SSS2.p1.6.m5.1.1.1.cmml" xref="S3.SS3.SSS2.p1.6.m5.1.1.1"></in><ci id="S3.SS3.SSS2.p1.6.m5.1.1.2.cmml" xref="S3.SS3.SSS2.p1.6.m5.1.1.2">𝐪</ci><apply id="S3.SS3.SSS2.p1.6.m5.1.1.3.cmml" xref="S3.SS3.SSS2.p1.6.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.SSS2.p1.6.m5.1.1.3.1.cmml" xref="S3.SS3.SSS2.p1.6.m5.1.1.3">superscript</csymbol><ci id="S3.SS3.SSS2.p1.6.m5.1.1.3.2.cmml" xref="S3.SS3.SSS2.p1.6.m5.1.1.3.2">ℝ</ci><ci id="S3.SS3.SSS2.p1.6.m5.1.1.3.3.cmml" xref="S3.SS3.SSS2.p1.6.m5.1.1.3.3">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS2.p1.6.m5.1c">\mathbf{q}\in\mathbb{R}^{d}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS2.p1.6.m5.1d">bold_q ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> can be the contextual embedding from a LLM in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.E3" title="In 3.2.3 Learning Process ‣ 3.2 Reindex Step: Single-Token Items in LLMs ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Equation</span> <span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Learning Process</h4>
<div class="ltx_para" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.5">We use maximum likelihood estimation to derive the loss for <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS3.p1.5.1">adapt</span> step, in order to learn the parameters of the bias terms or the recsys model. Note that the LLMs parameters are not involved in this step, ensuring an efficient learning process:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\textit{adapt}}=-\frac{1}{|\mathcal{D}^{*}|}\sum_{i=1}^{|\mathcal%
{D}^{*}|}\log\hat{\mathbf{p}}_{i,*}," class="ltx_Math" display="block" id="S3.E6.m1.5"><semantics id="S3.E6.m1.5a"><mrow id="S3.E6.m1.5.5.1" xref="S3.E6.m1.5.5.1.1.cmml"><mrow id="S3.E6.m1.5.5.1.1" xref="S3.E6.m1.5.5.1.1.cmml"><msub id="S3.E6.m1.5.5.1.1.2" xref="S3.E6.m1.5.5.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.5.5.1.1.2.2" xref="S3.E6.m1.5.5.1.1.2.2.cmml">ℒ</mi><mtext class="ltx_mathvariant_italic" id="S3.E6.m1.5.5.1.1.2.3" xref="S3.E6.m1.5.5.1.1.2.3a.cmml">adapt</mtext></msub><mo id="S3.E6.m1.5.5.1.1.1" xref="S3.E6.m1.5.5.1.1.1.cmml">=</mo><mrow id="S3.E6.m1.5.5.1.1.3" xref="S3.E6.m1.5.5.1.1.3.cmml"><mo id="S3.E6.m1.5.5.1.1.3a" xref="S3.E6.m1.5.5.1.1.3.cmml">−</mo><mrow id="S3.E6.m1.5.5.1.1.3.2" xref="S3.E6.m1.5.5.1.1.3.2.cmml"><mfrac id="S3.E6.m1.1.1" xref="S3.E6.m1.1.1.cmml"><mn id="S3.E6.m1.1.1.3" xref="S3.E6.m1.1.1.3.cmml">1</mn><mrow id="S3.E6.m1.1.1.1.1" xref="S3.E6.m1.1.1.1.2.cmml"><mo id="S3.E6.m1.1.1.1.1.2" stretchy="false" xref="S3.E6.m1.1.1.1.2.1.cmml">|</mo><msup id="S3.E6.m1.1.1.1.1.1" xref="S3.E6.m1.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.1.1.1.1.1.2" xref="S3.E6.m1.1.1.1.1.1.2.cmml">𝒟</mi><mo id="S3.E6.m1.1.1.1.1.1.3" xref="S3.E6.m1.1.1.1.1.1.3.cmml">∗</mo></msup><mo id="S3.E6.m1.1.1.1.1.3" stretchy="false" xref="S3.E6.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.E6.m1.5.5.1.1.3.2.1" xref="S3.E6.m1.5.5.1.1.3.2.1.cmml">⁢</mo><mrow id="S3.E6.m1.5.5.1.1.3.2.2" xref="S3.E6.m1.5.5.1.1.3.2.2.cmml"><munderover id="S3.E6.m1.5.5.1.1.3.2.2.1" xref="S3.E6.m1.5.5.1.1.3.2.2.1.cmml"><mo id="S3.E6.m1.5.5.1.1.3.2.2.1.2.2" movablelimits="false" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.2.cmml">∑</mo><mrow id="S3.E6.m1.5.5.1.1.3.2.2.1.2.3" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.cmml"><mi id="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.2" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.2.cmml">i</mi><mo id="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.1" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.1.cmml">=</mo><mn id="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.3" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.3.cmml">1</mn></mrow><mrow id="S3.E6.m1.2.2.1.1" xref="S3.E6.m1.2.2.1.2.cmml"><mo id="S3.E6.m1.2.2.1.1.2" stretchy="false" xref="S3.E6.m1.2.2.1.2.1.cmml">|</mo><msup id="S3.E6.m1.2.2.1.1.1" xref="S3.E6.m1.2.2.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E6.m1.2.2.1.1.1.2" xref="S3.E6.m1.2.2.1.1.1.2.cmml">𝒟</mi><mo id="S3.E6.m1.2.2.1.1.1.3" xref="S3.E6.m1.2.2.1.1.1.3.cmml">∗</mo></msup><mo id="S3.E6.m1.2.2.1.1.3" stretchy="false" xref="S3.E6.m1.2.2.1.2.1.cmml">|</mo></mrow></munderover><mrow id="S3.E6.m1.5.5.1.1.3.2.2.2" xref="S3.E6.m1.5.5.1.1.3.2.2.2.cmml"><mi id="S3.E6.m1.5.5.1.1.3.2.2.2.1" xref="S3.E6.m1.5.5.1.1.3.2.2.2.1.cmml">log</mi><mo id="S3.E6.m1.5.5.1.1.3.2.2.2a" lspace="0.167em" xref="S3.E6.m1.5.5.1.1.3.2.2.2.cmml">⁡</mo><msub id="S3.E6.m1.5.5.1.1.3.2.2.2.2" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2.cmml"><mover accent="true" id="S3.E6.m1.5.5.1.1.3.2.2.2.2.2" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.cmml"><mi id="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.2" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.2.cmml">𝐩</mi><mo id="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.1" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.1.cmml">^</mo></mover><mrow id="S3.E6.m1.4.4.2.4" xref="S3.E6.m1.4.4.2.3.cmml"><mi id="S3.E6.m1.3.3.1.1" xref="S3.E6.m1.3.3.1.1.cmml">i</mi><mo id="S3.E6.m1.4.4.2.4.1" rspace="0em" xref="S3.E6.m1.4.4.2.3.cmml">,</mo><mo id="S3.E6.m1.4.4.2.2" lspace="0em" xref="S3.E6.m1.4.4.2.2.cmml">∗</mo></mrow></msub></mrow></mrow></mrow></mrow></mrow><mo id="S3.E6.m1.5.5.1.2" xref="S3.E6.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E6.m1.5b"><apply id="S3.E6.m1.5.5.1.1.cmml" xref="S3.E6.m1.5.5.1"><eq id="S3.E6.m1.5.5.1.1.1.cmml" xref="S3.E6.m1.5.5.1.1.1"></eq><apply id="S3.E6.m1.5.5.1.1.2.cmml" xref="S3.E6.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.E6.m1.5.5.1.1.2.1.cmml" xref="S3.E6.m1.5.5.1.1.2">subscript</csymbol><ci id="S3.E6.m1.5.5.1.1.2.2.cmml" xref="S3.E6.m1.5.5.1.1.2.2">ℒ</ci><ci id="S3.E6.m1.5.5.1.1.2.3a.cmml" xref="S3.E6.m1.5.5.1.1.2.3"><mtext class="ltx_mathvariant_italic" id="S3.E6.m1.5.5.1.1.2.3.cmml" mathsize="70%" xref="S3.E6.m1.5.5.1.1.2.3">adapt</mtext></ci></apply><apply id="S3.E6.m1.5.5.1.1.3.cmml" xref="S3.E6.m1.5.5.1.1.3"><minus id="S3.E6.m1.5.5.1.1.3.1.cmml" xref="S3.E6.m1.5.5.1.1.3"></minus><apply id="S3.E6.m1.5.5.1.1.3.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2"><times id="S3.E6.m1.5.5.1.1.3.2.1.cmml" xref="S3.E6.m1.5.5.1.1.3.2.1"></times><apply id="S3.E6.m1.1.1.cmml" xref="S3.E6.m1.1.1"><divide id="S3.E6.m1.1.1.2.cmml" xref="S3.E6.m1.1.1"></divide><cn id="S3.E6.m1.1.1.3.cmml" type="integer" xref="S3.E6.m1.1.1.3">1</cn><apply id="S3.E6.m1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1"><abs id="S3.E6.m1.1.1.1.2.1.cmml" xref="S3.E6.m1.1.1.1.1.2"></abs><apply id="S3.E6.m1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.1.1.1.1.1.1.cmml" xref="S3.E6.m1.1.1.1.1.1">superscript</csymbol><ci id="S3.E6.m1.1.1.1.1.1.2.cmml" xref="S3.E6.m1.1.1.1.1.1.2">𝒟</ci><times id="S3.E6.m1.1.1.1.1.1.3.cmml" xref="S3.E6.m1.1.1.1.1.1.3"></times></apply></apply></apply><apply id="S3.E6.m1.5.5.1.1.3.2.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2"><apply id="S3.E6.m1.5.5.1.1.3.2.2.1.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.1"><csymbol cd="ambiguous" id="S3.E6.m1.5.5.1.1.3.2.2.1.1.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.1">superscript</csymbol><apply id="S3.E6.m1.5.5.1.1.3.2.2.1.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.1"><csymbol cd="ambiguous" id="S3.E6.m1.5.5.1.1.3.2.2.1.2.1.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.1">subscript</csymbol><sum id="S3.E6.m1.5.5.1.1.3.2.2.1.2.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.2"></sum><apply id="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.3"><eq id="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.1.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.1"></eq><ci id="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.2">𝑖</ci><cn id="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.3.cmml" type="integer" xref="S3.E6.m1.5.5.1.1.3.2.2.1.2.3.3">1</cn></apply></apply><apply id="S3.E6.m1.2.2.1.2.cmml" xref="S3.E6.m1.2.2.1.1"><abs id="S3.E6.m1.2.2.1.2.1.cmml" xref="S3.E6.m1.2.2.1.1.2"></abs><apply id="S3.E6.m1.2.2.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E6.m1.2.2.1.1.1.1.cmml" xref="S3.E6.m1.2.2.1.1.1">superscript</csymbol><ci id="S3.E6.m1.2.2.1.1.1.2.cmml" xref="S3.E6.m1.2.2.1.1.1.2">𝒟</ci><times id="S3.E6.m1.2.2.1.1.1.3.cmml" xref="S3.E6.m1.2.2.1.1.1.3"></times></apply></apply></apply><apply id="S3.E6.m1.5.5.1.1.3.2.2.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.2"><log id="S3.E6.m1.5.5.1.1.3.2.2.2.1.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.2.1"></log><apply id="S3.E6.m1.5.5.1.1.3.2.2.2.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2"><csymbol cd="ambiguous" id="S3.E6.m1.5.5.1.1.3.2.2.2.2.1.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2">subscript</csymbol><apply id="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2.2"><ci id="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.1.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.1">^</ci><ci id="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.2.cmml" xref="S3.E6.m1.5.5.1.1.3.2.2.2.2.2.2">𝐩</ci></apply><list id="S3.E6.m1.4.4.2.3.cmml" xref="S3.E6.m1.4.4.2.4"><ci id="S3.E6.m1.3.3.1.1.cmml" xref="S3.E6.m1.3.3.1.1">𝑖</ci><times id="S3.E6.m1.4.4.2.2.cmml" xref="S3.E6.m1.4.4.2.2"></times></list></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E6.m1.5c">\mathcal{L}_{\textit{adapt}}=-\frac{1}{|\mathcal{D}^{*}|}\sum_{i=1}^{|\mathcal%
{D}^{*}|}\log\hat{\mathbf{p}}_{i,*},</annotation><annotation encoding="application/x-llamapun" id="S3.E6.m1.5d">caligraphic_L start_POSTSUBSCRIPT adapt end_POSTSUBSCRIPT = - divide start_ARG 1 end_ARG start_ARG | caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT | end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT | end_POSTSUPERSCRIPT roman_log over^ start_ARG bold_p end_ARG start_POSTSUBSCRIPT italic_i , ∗ end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.SSS3.p1.4">where dataset <math alttext="\mathcal{D}^{*}" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p1.1.m1.1"><semantics id="S3.SS3.SSS3.p1.1.m1.1a"><msup id="S3.SS3.SSS3.p1.1.m1.1.1" xref="S3.SS3.SSS3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS3.p1.1.m1.1.1.2" xref="S3.SS3.SSS3.p1.1.m1.1.1.2.cmml">𝒟</mi><mo id="S3.SS3.SSS3.p1.1.m1.1.1.3" xref="S3.SS3.SSS3.p1.1.m1.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.1.m1.1b"><apply id="S3.SS3.SSS3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS3.SSS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1.2">𝒟</ci><times id="S3.SS3.SSS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS3.p1.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.1.m1.1c">\mathcal{D}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p1.1.m1.1d">caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> is collected from the target platform, such as ReDIAL <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite>, which is typically a small sized dataset. Here, <math alttext="\hat{\mathbf{p}}_{i,*}" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p1.2.m2.2"><semantics id="S3.SS3.SSS3.p1.2.m2.2a"><msub id="S3.SS3.SSS3.p1.2.m2.2.3" xref="S3.SS3.SSS3.p1.2.m2.2.3.cmml"><mover accent="true" id="S3.SS3.SSS3.p1.2.m2.2.3.2" xref="S3.SS3.SSS3.p1.2.m2.2.3.2.cmml"><mi id="S3.SS3.SSS3.p1.2.m2.2.3.2.2" xref="S3.SS3.SSS3.p1.2.m2.2.3.2.2.cmml">𝐩</mi><mo id="S3.SS3.SSS3.p1.2.m2.2.3.2.1" xref="S3.SS3.SSS3.p1.2.m2.2.3.2.1.cmml">^</mo></mover><mrow id="S3.SS3.SSS3.p1.2.m2.2.2.2.4" xref="S3.SS3.SSS3.p1.2.m2.2.2.2.3.cmml"><mi id="S3.SS3.SSS3.p1.2.m2.1.1.1.1" xref="S3.SS3.SSS3.p1.2.m2.1.1.1.1.cmml">i</mi><mo id="S3.SS3.SSS3.p1.2.m2.2.2.2.4.1" rspace="0em" xref="S3.SS3.SSS3.p1.2.m2.2.2.2.3.cmml">,</mo><mo id="S3.SS3.SSS3.p1.2.m2.2.2.2.2" lspace="0em" xref="S3.SS3.SSS3.p1.2.m2.2.2.2.2.cmml">∗</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.2.m2.2b"><apply id="S3.SS3.SSS3.p1.2.m2.2.3.cmml" xref="S3.SS3.SSS3.p1.2.m2.2.3"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.2.m2.2.3.1.cmml" xref="S3.SS3.SSS3.p1.2.m2.2.3">subscript</csymbol><apply id="S3.SS3.SSS3.p1.2.m2.2.3.2.cmml" xref="S3.SS3.SSS3.p1.2.m2.2.3.2"><ci id="S3.SS3.SSS3.p1.2.m2.2.3.2.1.cmml" xref="S3.SS3.SSS3.p1.2.m2.2.3.2.1">^</ci><ci id="S3.SS3.SSS3.p1.2.m2.2.3.2.2.cmml" xref="S3.SS3.SSS3.p1.2.m2.2.3.2.2">𝐩</ci></apply><list id="S3.SS3.SSS3.p1.2.m2.2.2.2.3.cmml" xref="S3.SS3.SSS3.p1.2.m2.2.2.2.4"><ci id="S3.SS3.SSS3.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.SSS3.p1.2.m2.1.1.1.1">𝑖</ci><times id="S3.SS3.SSS3.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.SSS3.p1.2.m2.2.2.2.2"></times></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.2.m2.2c">\hat{\mathbf{p}}_{i,*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p1.2.m2.2d">over^ start_ARG bold_p end_ARG start_POSTSUBSCRIPT italic_i , ∗ end_POSTSUBSCRIPT</annotation></semantics></math> denotes the probability of the ground-truth item in the <math alttext="i^{\text{th}}" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p1.3.m3.1"><semantics id="S3.SS3.SSS3.p1.3.m3.1a"><msup id="S3.SS3.SSS3.p1.3.m3.1.1" xref="S3.SS3.SSS3.p1.3.m3.1.1.cmml"><mi id="S3.SS3.SSS3.p1.3.m3.1.1.2" xref="S3.SS3.SSS3.p1.3.m3.1.1.2.cmml">i</mi><mtext id="S3.SS3.SSS3.p1.3.m3.1.1.3" xref="S3.SS3.SSS3.p1.3.m3.1.1.3a.cmml">th</mtext></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.3.m3.1b"><apply id="S3.SS3.SSS3.p1.3.m3.1.1.cmml" xref="S3.SS3.SSS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.3.m3.1.1.1.cmml" xref="S3.SS3.SSS3.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS3.SSS3.p1.3.m3.1.1.2.cmml" xref="S3.SS3.SSS3.p1.3.m3.1.1.2">𝑖</ci><ci id="S3.SS3.SSS3.p1.3.m3.1.1.3a.cmml" xref="S3.SS3.SSS3.p1.3.m3.1.1.3"><mtext id="S3.SS3.SSS3.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS3.SSS3.p1.3.m3.1.1.3">th</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.3.m3.1c">i^{\text{th}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p1.3.m3.1d">italic_i start_POSTSUPERSCRIPT th end_POSTSUPERSCRIPT</annotation></semantics></math> data sample. Our purpose is to adapt the model towards the underlying data distributions of <math alttext="\mathcal{D}^{*}" class="ltx_Math" display="inline" id="S3.SS3.SSS3.p1.4.m4.1"><semantics id="S3.SS3.SSS3.p1.4.m4.1a"><msup id="S3.SS3.SSS3.p1.4.m4.1.1" xref="S3.SS3.SSS3.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.SSS3.p1.4.m4.1.1.2" xref="S3.SS3.SSS3.p1.4.m4.1.1.2.cmml">𝒟</mi><mo id="S3.SS3.SSS3.p1.4.m4.1.1.3" xref="S3.SS3.SSS3.p1.4.m4.1.1.3.cmml">∗</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS3.p1.4.m4.1b"><apply id="S3.SS3.SSS3.p1.4.m4.1.1.cmml" xref="S3.SS3.SSS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS3.p1.4.m4.1.1.1.cmml" xref="S3.SS3.SSS3.p1.4.m4.1.1">superscript</csymbol><ci id="S3.SS3.SSS3.p1.4.m4.1.1.2.cmml" xref="S3.SS3.SSS3.p1.4.m4.1.1.2">𝒟</ci><times id="S3.SS3.SSS3.p1.4.m4.1.1.3.cmml" xref="S3.SS3.SSS3.p1.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS3.p1.4.m4.1c">\mathcal{D}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS3.p1.4.m4.1d">caligraphic_D start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> through this learning process.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2: </span>Dataset Statistics. We update <em class="ltx_emph ltx_font_italic" id="S3.T2.3.1">Reddit-Movie</em> CRS dataset as Reddit-V1.5 according to the raw data dump provided by <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> from 2012 to 2022. Specifically, conversation turns with valid recommended items are denoted as <em class="ltx_emph ltx_font_italic" id="S3.T2.4.2">R_Turns</em>. </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.5" style="width:346.9pt;height:67pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-155.4pt,29.8pt) scale(0.527443520878556,0.527443520878556) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.5.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T2.5.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.2.1">INSPIRED</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.3.1">ReDIAL</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.1.1.4.1">Reddit-V1.5</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T2.5.1.2.2.1"></th>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.2.2.2"><cite class="ltx_cite ltx_citemacro_citep">(Hayati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib23" title="">2020</a>)</cite></td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.2.2.3"><cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite></td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.2.2.4"><cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite></td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.5.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.3.3.1.1">#Conv.</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.3.3.2">999</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.3.3.3">11,348</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.5.1.3.3.4">2,726,471</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.5.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.4.4.1.1">#Turns</span></th>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.4.2">21,124</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.4.3">139,557</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.4.4.4">5,063,007</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.5.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.5.5.1.1">#R_Turns</span></th>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.5.2">1,950</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.5.3">30,322</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.5.5.4">1,787,050</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.5.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.6.6.1.1">#Users</span></th>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.6.6.2">999</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.6.6.3">764</td>
<td class="ltx_td ltx_align_center" id="S3.T2.5.1.6.6.4">520,913</td>
</tr>
<tr class="ltx_tr" id="S3.T2.5.1.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T2.5.1.7.7.1"><span class="ltx_text ltx_font_bold" id="S3.T2.5.1.7.7.1.1">#Items</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.5.1.7.7.2">1,472</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.5.1.7.7.3">6,281</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.5.1.7.7.4">68,285</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The main results for our models on conversational recommendation accuracy performance, compared against (1) traditional recommendation models; (2) zero-shot large language models (LLMs); (3) traditional conversational recommendation models; and (4) zero-shot dense retrievers. The size of the reported LLMs used here is 7B. We denote the model metrics with the best performance in bold. Llama2-R denotes the Llama2-7b model after our <span class="ltx_text ltx_font_typewriter" id="S3.T3.3.1">reindex</span> step. We also show the results after the <span class="ltx_text ltx_font_typewriter" id="S3.T3.4.2">adapt</span> step with bias terms (+Bias) or RecSys model combination with Gating mechanism (+RecSys). </figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.5" style="width:433.6pt;height:185.9pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-99.3pt,42.4pt) scale(0.685864960552119,0.685864960552119) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.5.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S3.T3.5.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S3.T3.5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.1.1.2.1">INSPIRED</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S3.T3.5.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.1.1.3.1">ReDIAL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="4" id="S3.T3.5.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.1.1.4.1">Reddit-V1.5</span></th>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.2.2">
<th class="ltx_td ltx_th ltx_th_row" id="S3.T3.5.1.2.2.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.2">H@5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.3">N@5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.4">H@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.5">N@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.6">H@5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.7">N@5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.8">H@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.9">N@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.10">H@5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.11">N@5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.12">H@10</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S3.T3.5.1.2.2.13">N@10</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.5.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.3.1.1.1">Popuplarity</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.2">.089 <span class="ltx_text" id="S3.T3.5.1.3.1.2.1" style="font-size:80%;color:#808080;">.020</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.3">.065 <span class="ltx_text" id="S3.T3.5.1.3.1.3.1" style="font-size:80%;color:#808080;">.015</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.4">.103 <span class="ltx_text" id="S3.T3.5.1.3.1.4.1" style="font-size:80%;color:#808080;">.021</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.5">.070 <span class="ltx_text" id="S3.T3.5.1.3.1.5.1" style="font-size:80%;color:#808080;">.015</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.6">.035 <span class="ltx_text" id="S3.T3.5.1.3.1.6.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.7">.025 <span class="ltx_text" id="S3.T3.5.1.3.1.7.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.8">.052 <span class="ltx_text" id="S3.T3.5.1.3.1.8.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.9">.030 <span class="ltx_text" id="S3.T3.5.1.3.1.9.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.10">.008 <span class="ltx_text" id="S3.T3.5.1.3.1.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.11">.004 <span class="ltx_text" id="S3.T3.5.1.3.1.11.1" style="font-size:80%;color:#808080;">.000</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.12">.014 <span class="ltx_text" id="S3.T3.5.1.3.1.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.5.1.3.1.13">.006 <span class="ltx_text" id="S3.T3.5.1.3.1.13.1" style="font-size:80%;color:#808080;">.000</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.1.4.2.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.4.2.1.1">FISM</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.2">.075 <span class="ltx_text" id="S3.T3.5.1.4.2.2.1" style="font-size:80%;color:#808080;">.018</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.3">.045 <span class="ltx_text" id="S3.T3.5.1.4.2.3.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.4">.103 <span class="ltx_text" id="S3.T3.5.1.4.2.4.1" style="font-size:80%;color:#808080;">.021</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.5">.054 <span class="ltx_text" id="S3.T3.5.1.4.2.5.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.6">.065 <span class="ltx_text" id="S3.T3.5.1.4.2.6.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.7">.040 <span class="ltx_text" id="S3.T3.5.1.4.2.7.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.8">.112 <span class="ltx_text" id="S3.T3.5.1.4.2.8.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.9">.054 <span class="ltx_text" id="S3.T3.5.1.4.2.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.10">.022 <span class="ltx_text" id="S3.T3.5.1.4.2.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.11">.012 <span class="ltx_text" id="S3.T3.5.1.4.2.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.4.2.12">.043 <span class="ltx_text" id="S3.T3.5.1.4.2.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.5.1.4.2.13">.019 <span class="ltx_text" id="S3.T3.5.1.4.2.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.1.5.3.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.5.3.1.1">SASRec</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.2">.061 <span class="ltx_text" id="S3.T3.5.1.5.3.2.1" style="font-size:80%;color:#808080;">.016</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.3">.037 <span class="ltx_text" id="S3.T3.5.1.5.3.3.1" style="font-size:80%;color:#808080;">.010</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.4">.103 <span class="ltx_text" id="S3.T3.5.1.5.3.4.1" style="font-size:80%;color:#808080;">.021</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.5">.051 <span class="ltx_text" id="S3.T3.5.1.5.3.5.1" style="font-size:80%;color:#808080;">.011</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.6">.068 <span class="ltx_text" id="S3.T3.5.1.5.3.6.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.7">.041 <span class="ltx_text" id="S3.T3.5.1.5.3.7.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.8">.116 <span class="ltx_text" id="S3.T3.5.1.5.3.8.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.9">.056 <span class="ltx_text" id="S3.T3.5.1.5.3.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.10">.022 <span class="ltx_text" id="S3.T3.5.1.5.3.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.11">.013 <span class="ltx_text" id="S3.T3.5.1.5.3.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.5.3.12">.039 <span class="ltx_text" id="S3.T3.5.1.5.3.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.5.1.5.3.13">.018 <span class="ltx_text" id="S3.T3.5.1.5.3.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.6.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.1.6.4.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.6.4.1.1">MPT</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.2">.075 <span class="ltx_text" id="S3.T3.5.1.6.4.2.1" style="font-size:80%;color:#808080;">.018</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.3">.045 <span class="ltx_text" id="S3.T3.5.1.6.4.3.1" style="font-size:80%;color:#808080;">.011</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.4">.099 <span class="ltx_text" id="S3.T3.5.1.6.4.4.1" style="font-size:80%;color:#808080;">.020</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.5">.052 <span class="ltx_text" id="S3.T3.5.1.6.4.5.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.6">.072 <span class="ltx_text" id="S3.T3.5.1.6.4.6.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.7">.045 <span class="ltx_text" id="S3.T3.5.1.6.4.7.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.8">.116 <span class="ltx_text" id="S3.T3.5.1.6.4.8.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.9">.059 <span class="ltx_text" id="S3.T3.5.1.6.4.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.10">.026 <span class="ltx_text" id="S3.T3.5.1.6.4.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.11">.017 <span class="ltx_text" id="S3.T3.5.1.6.4.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.12">.040 <span class="ltx_text" id="S3.T3.5.1.6.4.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.5.1.6.4.13">.021 <span class="ltx_text" id="S3.T3.5.1.6.4.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.1.7.5.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.7.5.1.1">Mistral</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.2">.061 <span class="ltx_text" id="S3.T3.5.1.7.5.2.1" style="font-size:80%;color:#808080;">.016</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.3">.040 <span class="ltx_text" id="S3.T3.5.1.7.5.3.1" style="font-size:80%;color:#808080;">.011</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.4">.066 <span class="ltx_text" id="S3.T3.5.1.7.5.4.1" style="font-size:80%;color:#808080;">.017</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.5">.041 <span class="ltx_text" id="S3.T3.5.1.7.5.5.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.6">.082 <span class="ltx_text" id="S3.T3.5.1.7.5.6.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.7">.056 <span class="ltx_text" id="S3.T3.5.1.7.5.7.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.8">.111 <span class="ltx_text" id="S3.T3.5.1.7.5.8.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.9">.065 <span class="ltx_text" id="S3.T3.5.1.7.5.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.10">.029 <span class="ltx_text" id="S3.T3.5.1.7.5.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.11">.020 <span class="ltx_text" id="S3.T3.5.1.7.5.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.7.5.12">.038 <span class="ltx_text" id="S3.T3.5.1.7.5.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.5.1.7.5.13">.023 <span class="ltx_text" id="S3.T3.5.1.7.5.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.8.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.1.8.6.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.8.6.1.1">Llama2</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.2">.080 <span class="ltx_text" id="S3.T3.5.1.8.6.2.1" style="font-size:80%;color:#808080;">.019</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.3">.050 <span class="ltx_text" id="S3.T3.5.1.8.6.3.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.4">.122 <span class="ltx_text" id="S3.T3.5.1.8.6.4.1" style="font-size:80%;color:#808080;">.022</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.5">.064 <span class="ltx_text" id="S3.T3.5.1.8.6.5.1" style="font-size:80%;color:#808080;">.013</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.6"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.8.6.6.1">.094 <span class="ltx_text" id="S3.T3.5.1.8.6.6.1.1" style="font-size:80%;color:#808080;">.004</span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.7">.059 <span class="ltx_text" id="S3.T3.5.1.8.6.7.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.8">.145 <span class="ltx_text" id="S3.T3.5.1.8.6.8.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.9">.075 <span class="ltx_text" id="S3.T3.5.1.8.6.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.10">.042 <span class="ltx_text" id="S3.T3.5.1.8.6.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.11">.027 <span class="ltx_text" id="S3.T3.5.1.8.6.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.8.6.12">.064 <span class="ltx_text" id="S3.T3.5.1.8.6.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.5.1.8.6.13">.034 <span class="ltx_text" id="S3.T3.5.1.8.6.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.1.9.7.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.9.7.1.1">ReDIAL</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.2">.060 <span class="ltx_text" id="S3.T3.5.1.9.7.2.1" style="font-size:80%;color:#808080;">.016</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.3">.041 <span class="ltx_text" id="S3.T3.5.1.9.7.3.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.4">.106 <span class="ltx_text" id="S3.T3.5.1.9.7.4.1" style="font-size:80%;color:#808080;">.021</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.5">.056 <span class="ltx_text" id="S3.T3.5.1.9.7.5.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.6">.067 <span class="ltx_text" id="S3.T3.5.1.9.7.6.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.7">.044 <span class="ltx_text" id="S3.T3.5.1.9.7.7.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.8">.106 <span class="ltx_text" id="S3.T3.5.1.9.7.8.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.9">.057 <span class="ltx_text" id="S3.T3.5.1.9.7.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.10">.029 <span class="ltx_text" id="S3.T3.5.1.9.7.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.11">.019 <span class="ltx_text" id="S3.T3.5.1.9.7.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.12">.044 <span class="ltx_text" id="S3.T3.5.1.9.7.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.5.1.9.7.13">.024 <span class="ltx_text" id="S3.T3.5.1.9.7.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.1.10.8.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.10.8.1.1">UniCRS</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.2">.091 <span class="ltx_text" id="S3.T3.5.1.10.8.2.1" style="font-size:80%;color:#808080;">.019</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.3">.055 <span class="ltx_text" id="S3.T3.5.1.10.8.3.1" style="font-size:80%;color:#808080;">.011</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.4">.132 <span class="ltx_text" id="S3.T3.5.1.10.8.4.1" style="font-size:80%;color:#808080;">.019</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.5">.073 <span class="ltx_text" id="S3.T3.5.1.10.8.5.1" style="font-size:80%;color:#808080;">.014</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.6">.085 <span class="ltx_text" id="S3.T3.5.1.10.8.6.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.7">.058 <span class="ltx_text" id="S3.T3.5.1.10.8.7.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.8">.112 <span class="ltx_text" id="S3.T3.5.1.10.8.8.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.9">.071 <span class="ltx_text" id="S3.T3.5.1.10.8.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.10">.028 <span class="ltx_text" id="S3.T3.5.1.10.8.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.11">.017 <span class="ltx_text" id="S3.T3.5.1.10.8.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.10.8.12">.040 <span class="ltx_text" id="S3.T3.5.1.10.8.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.5.1.10.8.13">.021 <span class="ltx_text" id="S3.T3.5.1.10.8.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.11.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.1.11.9.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.11.9.1.1">SBERT</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.2">.038 <span class="ltx_text" id="S3.T3.5.1.11.9.2.1" style="font-size:80%;color:#808080;">.013</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.3">.026 <span class="ltx_text" id="S3.T3.5.1.11.9.3.1" style="font-size:80%;color:#808080;">.010</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.4">.066 <span class="ltx_text" id="S3.T3.5.1.11.9.4.1" style="font-size:80%;color:#808080;">.017</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.5">.036 <span class="ltx_text" id="S3.T3.5.1.11.9.5.1" style="font-size:80%;color:#808080;">.010</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.6">.016 <span class="ltx_text" id="S3.T3.5.1.11.9.6.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.7">.010 <span class="ltx_text" id="S3.T3.5.1.11.9.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.8">.026 <span class="ltx_text" id="S3.T3.5.1.11.9.8.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.9">.013 <span class="ltx_text" id="S3.T3.5.1.11.9.9.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.10">.003 <span class="ltx_text" id="S3.T3.5.1.11.9.10.1" style="font-size:80%;color:#808080;">.000</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.11">.002 <span class="ltx_text" id="S3.T3.5.1.11.9.11.1" style="font-size:80%;color:#808080;">.000</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.12">.005 <span class="ltx_text" id="S3.T3.5.1.11.9.12.1" style="font-size:80%;color:#808080;">.000</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.5.1.11.9.13">.002 <span class="ltx_text" id="S3.T3.5.1.11.9.13.1" style="font-size:80%;color:#808080;">.000</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.1.12.10.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.12.10.1.1">Instructor</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.2">.052 <span class="ltx_text" id="S3.T3.5.1.12.10.2.1" style="font-size:80%;color:#808080;">.015</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.3">.034 <span class="ltx_text" id="S3.T3.5.1.12.10.3.1" style="font-size:80%;color:#808080;">.011</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.4">.085 <span class="ltx_text" id="S3.T3.5.1.12.10.4.1" style="font-size:80%;color:#808080;">.019</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.5">.045 <span class="ltx_text" id="S3.T3.5.1.12.10.5.1" style="font-size:80%;color:#808080;">.011</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.6">.025 <span class="ltx_text" id="S3.T3.5.1.12.10.6.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.7">.013 <span class="ltx_text" id="S3.T3.5.1.12.10.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.8">.043 <span class="ltx_text" id="S3.T3.5.1.12.10.8.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.9">.019 <span class="ltx_text" id="S3.T3.5.1.12.10.9.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.10">.009 <span class="ltx_text" id="S3.T3.5.1.12.10.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.11">.006 <span class="ltx_text" id="S3.T3.5.1.12.10.11.1" style="font-size:80%;color:#808080;">.000</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.12.10.12">.017 <span class="ltx_text" id="S3.T3.5.1.12.10.12.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.5.1.12.10.13">.008 <span class="ltx_text" id="S3.T3.5.1.12.10.13.1" style="font-size:80%;color:#808080;">.000</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.13.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T3.5.1.13.11.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.13.11.1.1">Llama2-R</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.2">.066 <span class="ltx_text" id="S3.T3.5.1.13.11.2.1" style="font-size:80%;color:#808080;">.017</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.3">.041 <span class="ltx_text" id="S3.T3.5.1.13.11.3.1" style="font-size:80%;color:#808080;">.011</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.4">.103 <span class="ltx_text" id="S3.T3.5.1.13.11.4.1" style="font-size:80%;color:#808080;">.021</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.5">.053 <span class="ltx_text" id="S3.T3.5.1.13.11.5.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.6">.071 <span class="ltx_text" id="S3.T3.5.1.13.11.6.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.7">.042 <span class="ltx_text" id="S3.T3.5.1.13.11.7.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.8">.117 <span class="ltx_text" id="S3.T3.5.1.13.11.8.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.9">.057 <span class="ltx_text" id="S3.T3.5.1.13.11.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.10">.055 <span class="ltx_text" id="S3.T3.5.1.13.11.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.11">.035 <span class="ltx_text" id="S3.T3.5.1.13.11.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.12">.093 <span class="ltx_text" id="S3.T3.5.1.13.11.12.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S3.T3.5.1.13.11.13">.047 <span class="ltx_text" id="S3.T3.5.1.13.11.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T3.5.1.14.12.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.14.12.1.1">+Bias</span></th>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.2"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.14.12.2.1">.103 <span class="ltx_text" id="S3.T3.5.1.14.12.2.1.1" style="font-size:80%;color:#808080;">.021</span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.3"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.14.12.3.1">.066 <span class="ltx_text" id="S3.T3.5.1.14.12.3.1.1" style="font-size:80%;color:#808080;">.014</span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.4"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.14.12.4.1">.164 <span class="ltx_text" id="S3.T3.5.1.14.12.4.1.1" style="font-size:80%;color:#808080;">.025</span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.5"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.14.12.5.1">.083 <span class="ltx_text" id="S3.T3.5.1.14.12.5.1.1" style="font-size:80%;color:#808080;">.015</span></span></td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.6">.083 <span class="ltx_text" id="S3.T3.5.1.14.12.6.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.7">.053 <span class="ltx_text" id="S3.T3.5.1.14.12.7.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.8">.123 <span class="ltx_text" id="S3.T3.5.1.14.12.8.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.9">.066 <span class="ltx_text" id="S3.T3.5.1.14.12.9.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.10">.059 <span class="ltx_text" id="S3.T3.5.1.14.12.10.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.11">.037 <span class="ltx_text" id="S3.T3.5.1.14.12.11.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.5.1.14.12.12">.096 <span class="ltx_text" id="S3.T3.5.1.14.12.12.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.5.1.14.12.13">.049 <span class="ltx_text" id="S3.T3.5.1.14.12.13.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T3.5.1.15.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S3.T3.5.1.15.13.1"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.1.1">+RecSys</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.2">.089 <span class="ltx_text" id="S3.T3.5.1.15.13.2.1" style="font-size:80%;color:#808080;">.020</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.3">.052 <span class="ltx_text" id="S3.T3.5.1.15.13.3.1" style="font-size:80%;color:#808080;">.013</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.4"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.4.1">.164 <span class="ltx_text" id="S3.T3.5.1.15.13.4.1.1" style="font-size:80%;color:#808080;">.025</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.5">.076 <span class="ltx_text" id="S3.T3.5.1.15.13.5.1" style="font-size:80%;color:#808080;">.013</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.6"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.6.1">.094 <span class="ltx_text" id="S3.T3.5.1.15.13.6.1.1" style="font-size:80%;color:#808080;">.004</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.7">
<span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.7.1">.060</span> <span class="ltx_text" id="S3.T3.5.1.15.13.7.2" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.8"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.8.1">.146 <span class="ltx_text" id="S3.T3.5.1.15.13.8.1.1" style="font-size:80%;color:#808080;">.005</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.9"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.9.1">.076 <span class="ltx_text" id="S3.T3.5.1.15.13.9.1.1" style="font-size:80%;color:#808080;">.003</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.10"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.10.1">.061 <span class="ltx_text" id="S3.T3.5.1.15.13.10.1.1" style="font-size:80%;color:#808080;">.001</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.11"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.11.1">.038 <span class="ltx_text" id="S3.T3.5.1.15.13.11.1.1" style="font-size:80%;color:#808080;">.001</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.12"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.12.1">.101 <span class="ltx_text" id="S3.T3.5.1.15.13.12.1.1" style="font-size:80%;color:#808080;">.002</span></span></td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S3.T3.5.1.15.13.13"><span class="ltx_text ltx_font_bold" id="S3.T3.5.1.15.13.13.1">.051 <span class="ltx_text" id="S3.T3.5.1.15.13.13.1.1" style="font-size:80%;color:#808080;">.001</span></span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setup</h3>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1 </span>Datasets</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Three conversational recommendation datasets <cite class="ltx_cite ltx_citemacro_citep">(Hayati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib23" title="">2020</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> are used in our experiments, where the statistics are summarized in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.T2" title="In 3.3.3 Learning Process ‣ 3.3 Adapt Step: Item Probabilities Adjustment ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>: <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.1">INSPIRED</span> <cite class="ltx_cite ltx_citemacro_citep">(Hayati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib23" title="">2020</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.2">ReDIAL</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite>: These two datasets consist of small-scale human-human conversations for movie recommendations with crowd-sourced annotations from MTurk <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://mturk.com</span></span></span></span>. Due to their short collection time span, temporal patterns are unlikely to be observed. Nevertheless, considering their widespread use, we present our model results based on these datasets. In the following experiments, we randomly split the datasets into training, validation, and test sets using an 8:1:1 ratio. <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p1.1.3">Reddit-V1.5</span> <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>: This dataset comprises large-scale movie discussions on Reddit, which were collected and processed by <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>. This dataset shows real movie conversation recommendations in the wild and includes corresponding timestamps for 10 years to study temporal patterns. For data splitting, we use the last two months (i.e., Nov. and Dec. in 2022) as validation and testing set respectively to approximate the real setting. Due to the large size of the given dataset, we uniformly sample 20% conversation turns for validation (i.e., 11,241 samples) and testing (i.e., 13,816 samples).</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2 </span>Baselines</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">We consider four groups of baseline models for comparison. (1) We consider some representative traditional <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p1.1.1">item-based<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote3.1.1.1">3</span></span><span class="ltx_text ltx_font_upright" id="footnote3.8">We only use item-based models since INSPIRED does not have historical user interactions.</span></span></span></span></span> RecSys models, including <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.2">Popularity</span>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.3">FISM</span> <cite class="ltx_cite ltx_citemacro_citep">(Kabbur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib29" title="">2013</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.4">SASRec</span> <cite class="ltx_cite ltx_citemacro_citep">(Kang and McAuley, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib30" title="">2018</a>)</cite>. (2) We consider some representative CRS models: <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.5">ReDIAL</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.6">UniCRS</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib57" title="">2022</a>)</cite>: This model uses a pre-trained language model. (3) We consider some dense retrieval models given the connections to document retrieval: <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.7">SBERT</span> <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib46" title="">2019</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.8">Instructor</span> <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib50" title="">2022</a>)</cite>. (4) We consider some zero-shot open-sourced LLMs as baselines like <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> and use the 7-billion-parameter version due to compute burden: <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.9">MPT-7b</span> <cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib52" title="">2023</a>)</cite>, <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.10">Mistral-7b</span> <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib28" title="">2023</a>)</cite> and <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.11">Llama2-7b</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib53" title="">2023</a>)</cite>. We also discuss the results from <span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p1.1.12">GPT-3.5-turbo</span> <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib48" title="">2022</a>)</cite>, which is a much larger proprietary model that can achieve state-of-the-art CRS performance even in a zero-shot setting <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>. The details of baseline models are found in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A1.SS1" title="A.1 Baseline Details ‣ Appendix A More Details of Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.1</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3 </span>Evaluation Metrics</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">We focus on recommendation accuracy using HIT@K (H@K) and NDCG@K (N@K), following <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib9" title="">2019</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib65" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib57" title="">2022</a>)</cite>. We consider the means and the standard errors<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We use error bars in our figures and gray numbers in our tables for standard errors.</span></span></span> of the metrics with <math alttext="K=\{5,10\}" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p1.1.m1.2"><semantics id="S4.SS1.SSS3.p1.1.m1.2a"><mrow id="S4.SS1.SSS3.p1.1.m1.2.3" xref="S4.SS1.SSS3.p1.1.m1.2.3.cmml"><mi id="S4.SS1.SSS3.p1.1.m1.2.3.2" xref="S4.SS1.SSS3.p1.1.m1.2.3.2.cmml">K</mi><mo id="S4.SS1.SSS3.p1.1.m1.2.3.1" xref="S4.SS1.SSS3.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S4.SS1.SSS3.p1.1.m1.2.3.3.2" xref="S4.SS1.SSS3.p1.1.m1.2.3.3.1.cmml"><mo id="S4.SS1.SSS3.p1.1.m1.2.3.3.2.1" stretchy="false" xref="S4.SS1.SSS3.p1.1.m1.2.3.3.1.cmml">{</mo><mn id="S4.SS1.SSS3.p1.1.m1.1.1" xref="S4.SS1.SSS3.p1.1.m1.1.1.cmml">5</mn><mo id="S4.SS1.SSS3.p1.1.m1.2.3.3.2.2" xref="S4.SS1.SSS3.p1.1.m1.2.3.3.1.cmml">,</mo><mn id="S4.SS1.SSS3.p1.1.m1.2.2" xref="S4.SS1.SSS3.p1.1.m1.2.2.cmml">10</mn><mo id="S4.SS1.SSS3.p1.1.m1.2.3.3.2.3" stretchy="false" xref="S4.SS1.SSS3.p1.1.m1.2.3.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p1.1.m1.2b"><apply id="S4.SS1.SSS3.p1.1.m1.2.3.cmml" xref="S4.SS1.SSS3.p1.1.m1.2.3"><eq id="S4.SS1.SSS3.p1.1.m1.2.3.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.2.3.1"></eq><ci id="S4.SS1.SSS3.p1.1.m1.2.3.2.cmml" xref="S4.SS1.SSS3.p1.1.m1.2.3.2">𝐾</ci><set id="S4.SS1.SSS3.p1.1.m1.2.3.3.1.cmml" xref="S4.SS1.SSS3.p1.1.m1.2.3.3.2"><cn id="S4.SS1.SSS3.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS1.SSS3.p1.1.m1.1.1">5</cn><cn id="S4.SS1.SSS3.p1.1.m1.2.2.cmml" type="integer" xref="S4.SS1.SSS3.p1.1.m1.2.2">10</cn></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p1.1.m1.2c">K=\{5,10\}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p1.1.m1.2d">italic_K = { 5 , 10 }</annotation></semantics></math>. Please find the implementation details in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A1.SS2" title="A.2 Implementation Details ‣ Appendix A More Details of Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>General CRS Performance</h3>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1 </span>Baseline Performance.</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.T3" title="In 3.3.3 Learning Process ‣ 3.3 Adapt Step: Item Probabilities Adjustment ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows the recommendation accuracy of four groups of baselines on three conversational recommendation datasets. There are some observations:</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">On Traditional RecSys.</span> Conventional recsys models effectively capture target popularity and further item-item similarities, resulting in reasonable recommendation accuracies. Interestingly, on INSPIRED, we find that non-personalized popularity serves as a strong baseline, because the limited size of the training set may restrict the ability to capture more complex item-item relationships. The results of traditional recommendation system models also indicate the potential of improving the recommendation accuracy by aligning with target data distributions.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p3.1.1">On LLMs.</span> LLMs with zero-shot prompting from <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> achieve impressive results, surpassing even the best results on ReDIAL datasets. Additionally, the rank of recommendation accuracy within the LLM group aligns with the performance from <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S2.F2" title="In 2.1 Task Formulation ‣ 2 Preliminaries ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">2</span></a>. Further details on the specific proprietary model <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS1.p3.1.2">GPT-3.5-t</span> are discussed in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS5.SSS2" title="4.5.2 Comparison with Proprietary Models ‣ 4.5 Discussions ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.5.2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p4.1.1">On Other Baselines.</span> We observe that zero-shot state-of-the-art dense retrievers are unable to achieve comparable performance as zero-shot LLMs; this may be due to two reasons: (1) Dense retrievers focus more on retrieving similar documents according to semantic similarities (e.g., similar contents), but LLMs show better understanding abilities for conversation contexts; (2) We are encoding the movie textual title rather than the description of the movie for fair comparison, which may limit the dense retrievers’ performance. As for traditional CRS models, since we follow the setting in <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> to remove “repeated” items, many popular CRS models perform relatively weaker in the corrected evaluation protocol.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2 </span>Ours vs. Baselines.</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">We construct a small-sized aggregator on top of
Llama2 as an example, then use this aggregator to reindex multi-token movie titles into single-token movie titles as recommendation candidates, namely Llama2-R.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p2.1.1">On Recommendation Accuracy.</span> <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.T3" title="In 3.3.3 Learning Process ‣ 3.3 Adapt Step: Item Probabilities Adjustment ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">3</span></a> shows that, following the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS2.p2.1.2">reindex</span> and <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS2.p2.1.3">adapt</span> steps, our model excels over baselines on INSPIRED and Reddit-V1.5 datasets, achieving the competitive best results on the ReDIAL dataset. Examining the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS2.p2.1.4">reindex</span> step (Llama2-R) and <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS2.p2.1.5">adapt</span> step (+Bias or +RecSys), we observe a potential performance decrease in the <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS2.p2.1.6">reindex</span> step due to the semantic gap from original token embeddings to the new single token embeddings from the relatively small aggregator. However, our models compensate by capturing the target data distribution through bias terms or traditional RecSys models. A more in-depth analysis of these <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS2.p2.1.7">adapt</span> methods will be discussed in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.SS4" title="4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Section</span> <span class="ltx_text ltx_ref_tag">4.4</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.3"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS2.p3.3.1">On Efficiency and Flexibility.</span> It is crucial to mention that the aggregator-based methods are around <math alttext="10\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS2.p3.1.m1.1"><semantics id="S4.SS2.SSS2.p3.1.m1.1a"><mrow id="S4.SS2.SSS2.p3.1.m1.1b"><mn id="S4.SS2.SSS2.p3.1.m1.1.1">10</mn><mo id="S4.SS2.SSS2.p3.1.m1.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.1.m1.1c">10\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.1.m1.1d">10 ×</annotation></semantics></math> smaller than the corresponding out-of-vocabulary item embedding tables and approximately <math alttext="233\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS2.p3.2.m2.1"><semantics id="S4.SS2.SSS2.p3.2.m2.1a"><mrow id="S4.SS2.SSS2.p3.2.m2.1b"><mn id="S4.SS2.SSS2.p3.2.m2.1.1">233</mn><mo id="S4.SS2.SSS2.p3.2.m2.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.2.m2.1c">233\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.2.m2.1d">233 ×</annotation></semantics></math> smaller than the Llama2-7b base model, emphasizing its space efficiency. Additionally, as all movie titles with varying numbers of tokens are ”squeezed” into single tokens, our model can rank all items with a single decoding step, making it around <math alttext="100\times" class="ltx_math_unparsed" display="inline" id="S4.SS2.SSS2.p3.3.m3.1"><semantics id="S4.SS2.SSS2.p3.3.m3.1a"><mrow id="S4.SS2.SSS2.p3.3.m3.1b"><mn id="S4.SS2.SSS2.p3.3.m3.1.1">100</mn><mo id="S4.SS2.SSS2.p3.3.m3.1.2" lspace="0.222em">×</mo></mrow><annotation encoding="application/x-tex" id="S4.SS2.SSS2.p3.3.m3.1c">100\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS2.p3.3.m3.1d">100 ×</annotation></semantics></math> faster than the generative retrieval from LLMs to recommend the top-20 items. Moreover, single tokens facilitate easy acquisition of the recommendation item distribution, enhancing flexibility in control or further adjustment of the recommendations.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="220" id="S4.F5.g1" src="x4.png" width="746"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Different methods to represent items in LLMs with single-token embeddings and the related recommendation accuracy HIT@5 after the <span class="ltx_text ltx_font_typewriter" id="S4.F5.2.1">reindex</span> step.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Effectiveness of the <span class="ltx_text ltx_font_typewriter" id="S4.SS3.1.1">Reindex</span> Step</h3>
<section class="ltx_subsubsection" id="S4.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.1 </span>Experiment Setup</h4>
<div class="ltx_para" id="S4.SS3.SSS1.p1">
<p class="ltx_p" id="S4.SS3.SSS1.p1.1">We explore methods for representing item titles with single-token embeddings in LLMs, investigating four approaches: (1) <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.1">Embed</span>: randomly initialized out-of-vocabulary (OOV) embeddings. Subsequently, three models aggregate existing LLM token embeddings into a single-token embedding and trained on the samples from those three datasets: (2) <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.2">Weighted</span>: learning position-wise attention weights to aggregate multi-token embeddings into a single one, followed by a simple linear projection; (3) <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.3">TRM</span>: employing a single-layer transformer to derive a contextual embedding from the output <span class="ltx_text ltx_font_typewriter" id="S4.SS3.SSS1.p1.1.4">CLS</span> token; (4) <span class="ltx_text ltx_font_bold" id="S4.SS3.SSS1.p1.1.5">RNN</span>: using a simple GRU model to aggregate multiple token embeddings, with the last hidden state vectors serving as the item representations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.2 </span>Embedding vs. Aggregator</h4>
<div class="ltx_para" id="S4.SS3.SSS2.p1">
<p class="ltx_p" id="S4.SS3.SSS2.p1.1">The embedding-based method cannot be shared across different datasets due to the practical challenge in normalizing item titles. However, the aggregators are shared across different datasets, using the raw text of item titles as inputs. <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.F5" title="In 4.2.2 Ours vs. Baselines. ‣ 4.2 General CRS Performance ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a> demonstrates that aggregators are not only generalizable across different datasets but also yield superior recommendation accuracy. Interestingly, despite Reddit having a dominant share of training samples (96%) as shown in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S3.T2" title="In 3.3.3 Learning Process ‣ 3.3 Adapt Step: Item Probabilities Adjustment ‣ 3 Framework ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">2</span></a>, the trained aggregators with mixed data samples perform even better than the dataset-specific new embeddings in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.F5" title="In 4.2.2 Ours vs. Baselines. ‣ 4.2 General CRS Performance ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.3.3 </span>Different Aggregators</h4>
<div class="ltx_para" id="S4.SS3.SSS3.p1">
<p class="ltx_p" id="S4.SS3.SSS3.p1.1">Among the three aggregators, the Weighted method demonstrates competitive performance despite its simple architecture. This suggests that the existing token embeddings from the LLMs are effective enough, making the weighted-sum with linear projection a reasonable approach to consolidating token embeddings. Additionally, TRM performs worse than RNN, possibly because (1) titles (e..g, movies) are typically short (fewer than 20 tokens), diminishing the significance of TRM’s advantages over RNN in handling long dependencies; (2) <span class="ltx_text ltx_font_typewriter" id="S4.SS3.SSS3.p1.1.1">CLS</span> tokens show difficulty in representing a sentence, as noted in the literature <cite class="ltx_cite ltx_citemacro_citep">(Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib12" title="">2021</a>)</cite>.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Recommendation accuracy comparison among Continual-Training on Llama2-R (Cont.), and the detailed configurations of adding bias terms or RecSys gating.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.3" style="width:390.3pt;height:221.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(20.8pt,-11.8pt) scale(1.11936621324115,1.11936621324115) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.3.3.4.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T4.3.3.4.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.3.3.4.1.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.4.1.2.1">INSPIRED</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.3.3.4.1.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.4.1.3.1">ReDIAL</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S4.T4.3.3.4.1.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.4.1.4.1">Reddit-V1.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.5.2">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T4.3.3.5.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.5.2.2">H@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.5.2.3">N@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.5.2.4">H@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.5.2.5">N@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.5.2.6">H@10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.5.2.7">N@10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.6.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.3.6.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.6.3.1.1">Llama2-R</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.6.3.2">.103 <span class="ltx_text" id="S4.T4.3.3.6.3.2.1" style="font-size:80%;color:#808080;">.021</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.6.3.3">.053 <span class="ltx_text" id="S4.T4.3.3.6.3.3.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.6.3.4">.117 <span class="ltx_text" id="S4.T4.3.3.6.3.4.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.6.3.5">.057 <span class="ltx_text" id="S4.T4.3.3.6.3.5.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.6.3.6">.093 <span class="ltx_text" id="S4.T4.3.3.6.3.6.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.6.3.7">.047 <span class="ltx_text" id="S4.T4.3.3.6.3.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.7.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.3.7.4.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.7.4.1.1">Cont.</span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.7.4.2">.146 <span class="ltx_text" id="S4.T4.3.3.7.4.2.1" style="font-size:80%;color:#808080;">.024</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.7.4.3">.081 <span class="ltx_text" id="S4.T4.3.3.7.4.3.1" style="font-size:80%;color:#808080;">.015</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.7.4.4">.124 <span class="ltx_text" id="S4.T4.3.3.7.4.4.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.7.4.5">.067 <span class="ltx_text" id="S4.T4.3.3.7.4.5.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.7.4.6">.093 <span class="ltx_text" id="S4.T4.3.3.7.4.6.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.7.4.7">.047 <span class="ltx_text" id="S4.T4.3.3.7.4.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.8.5">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.3.8.5.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="S4.T4.3.3.8.5.2"><span class="ltx_text ltx_font_italic" id="S4.T4.3.3.8.5.2.1">Bias Term Adjustment (+Bias)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.1.1.1">w/ <math alttext="\mathbf{gW}" class="ltx_Math" display="inline" id="S4.T4.1.1.1.1.1.m1.1"><semantics id="S4.T4.1.1.1.1.1.m1.1a"><mi id="S4.T4.1.1.1.1.1.m1.1.1" xref="S4.T4.1.1.1.1.1.m1.1.1.cmml">𝐠𝐖</mi><annotation-xml encoding="MathML-Content" id="S4.T4.1.1.1.1.1.m1.1b"><ci id="S4.T4.1.1.1.1.1.m1.1.1.cmml" xref="S4.T4.1.1.1.1.1.m1.1.1">𝐠𝐖</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.1.1.1.1.1.m1.1c">\mathbf{gW}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.1.1.1.1.1.m1.1d">bold_gW</annotation></semantics></math></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.2">.155 <span class="ltx_text" id="S4.T4.1.1.1.2.1" style="font-size:80%;color:#808080;">.025</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.3">.081 <span class="ltx_text" id="S4.T4.1.1.1.3.1" style="font-size:80%;color:#808080;">.014</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.4">.123 <span class="ltx_text" id="S4.T4.1.1.1.4.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.5">.066 <span class="ltx_text" id="S4.T4.1.1.1.5.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.6">.093 <span class="ltx_text" id="S4.T4.1.1.1.6.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.1.7">.048 <span class="ltx_text" id="S4.T4.1.1.1.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.2.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.2.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T4.2.2.2.1.1">w/ <math alttext="\mathbf{b}" class="ltx_Math" display="inline" id="S4.T4.2.2.2.1.1.m1.1"><semantics id="S4.T4.2.2.2.1.1.m1.1a"><mi id="S4.T4.2.2.2.1.1.m1.1.1" xref="S4.T4.2.2.2.1.1.m1.1.1.cmml">𝐛</mi><annotation-xml encoding="MathML-Content" id="S4.T4.2.2.2.1.1.m1.1b"><ci id="S4.T4.2.2.2.1.1.m1.1.1.cmml" xref="S4.T4.2.2.2.1.1.m1.1.1">𝐛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.2.2.2.1.1.m1.1c">\mathbf{b}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.2.2.2.1.1.m1.1d">bold_b</annotation></semantics></math></span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.2">.103 <span class="ltx_text" id="S4.T4.2.2.2.2.1" style="font-size:80%;color:#808080;">.021</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.3">.053 <span class="ltx_text" id="S4.T4.2.2.2.3.1" style="font-size:80%;color:#808080;">.012</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.4">.118 <span class="ltx_text" id="S4.T4.2.2.2.4.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.5">.057 <span class="ltx_text" id="S4.T4.2.2.2.5.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.6">.096 <span class="ltx_text" id="S4.T4.2.2.2.6.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.2.2.2.7">.049 <span class="ltx_text" id="S4.T4.2.2.2.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.3.3.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.3.1.1">w/ <math alttext="\mathbf{gW+b}" class="ltx_Math" display="inline" id="S4.T4.3.3.3.1.1.m1.1"><semantics id="S4.T4.3.3.3.1.1.m1.1a"><mrow id="S4.T4.3.3.3.1.1.m1.1.1" xref="S4.T4.3.3.3.1.1.m1.1.1.cmml"><mi id="S4.T4.3.3.3.1.1.m1.1.1.2" xref="S4.T4.3.3.3.1.1.m1.1.1.2.cmml">𝐠𝐖</mi><mo id="S4.T4.3.3.3.1.1.m1.1.1.1" xref="S4.T4.3.3.3.1.1.m1.1.1.1.cmml">+</mo><mi id="S4.T4.3.3.3.1.1.m1.1.1.3" xref="S4.T4.3.3.3.1.1.m1.1.1.3.cmml">𝐛</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.T4.3.3.3.1.1.m1.1b"><apply id="S4.T4.3.3.3.1.1.m1.1.1.cmml" xref="S4.T4.3.3.3.1.1.m1.1.1"><plus id="S4.T4.3.3.3.1.1.m1.1.1.1.cmml" xref="S4.T4.3.3.3.1.1.m1.1.1.1"></plus><ci id="S4.T4.3.3.3.1.1.m1.1.1.2.cmml" xref="S4.T4.3.3.3.1.1.m1.1.1.2">𝐠𝐖</ci><ci id="S4.T4.3.3.3.1.1.m1.1.1.3.cmml" xref="S4.T4.3.3.3.1.1.m1.1.1.3">𝐛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T4.3.3.3.1.1.m1.1c">\mathbf{gW+b}</annotation><annotation encoding="application/x-llamapun" id="S4.T4.3.3.3.1.1.m1.1d">bold_gW + bold_b</annotation></semantics></math></span></th>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.3.2.1">.164 <span class="ltx_text" id="S4.T4.3.3.3.2.1.1" style="font-size:80%;color:#808080;">.025</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.3.3.1">.083 <span class="ltx_text" id="S4.T4.3.3.3.3.1.1" style="font-size:80%;color:#808080;">.004</span></span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.4">.123 <span class="ltx_text" id="S4.T4.3.3.3.4.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.5">.066 <span class="ltx_text" id="S4.T4.3.3.3.5.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.6">.096 <span class="ltx_text" id="S4.T4.3.3.3.6.1" style="font-size:80%;color:#808080;">.001</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.3.3.3.7">.049 <span class="ltx_text" id="S4.T4.3.3.3.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.9.6">
<th class="ltx_td ltx_th ltx_th_row ltx_border_t" id="S4.T4.3.3.9.6.1"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="6" id="S4.T4.3.3.9.6.2"><span class="ltx_text ltx_font_italic" id="S4.T4.3.3.9.6.2.1">RecSys Model Gating (+RecSys)</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.10.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T4.3.3.10.7.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.10.7.1.1">+ FISM</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.10.7.2"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.10.7.2.1">.164 <span class="ltx_text" id="S4.T4.3.3.10.7.2.1.1" style="font-size:80%;color:#808080;">.025</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.10.7.3">.076 <span class="ltx_text" id="S4.T4.3.3.10.7.3.1" style="font-size:80%;color:#808080;">.013</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.10.7.4">.139 <span class="ltx_text" id="S4.T4.3.3.10.7.4.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.10.7.5">.072 <span class="ltx_text" id="S4.T4.3.3.10.7.5.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.10.7.6"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.10.7.6.1">.101 <span class="ltx_text" id="S4.T4.3.3.10.7.6.1.1" style="font-size:80%;color:#808080;">.002</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.3.3.10.7.7">.049 <span class="ltx_text" id="S4.T4.3.3.10.7.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.3.3.11.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T4.3.3.11.8.1"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.11.8.1.1">+ SASRec</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.3.3.11.8.2">.136 <span class="ltx_text" id="S4.T4.3.3.11.8.2.1" style="font-size:80%;color:#808080;">.023</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.3.3.11.8.3">.071 <span class="ltx_text" id="S4.T4.3.3.11.8.3.1" style="font-size:80%;color:#808080;">.014</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.3.3.11.8.4"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.11.8.4.1">.146 <span class="ltx_text" id="S4.T4.3.3.11.8.4.1.1" style="font-size:80%;color:#808080;">.005</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.3.3.11.8.5"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.11.8.5.1">.076 <span class="ltx_text" id="S4.T4.3.3.11.8.5.1.1" style="font-size:80%;color:#808080;">.003</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.3.3.11.8.6"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.11.8.6.1">.101 <span class="ltx_text" id="S4.T4.3.3.11.8.6.1.1" style="font-size:80%;color:#808080;">.002</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.3.3.11.8.7"><span class="ltx_text ltx_font_bold" id="S4.T4.3.3.11.8.7.1">.051 <span class="ltx_text" id="S4.T4.3.3.11.8.7.1.1" style="font-size:80%;color:#808080;">.001</span></span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Effectiveness of the <span class="ltx_text ltx_font_typewriter" id="S4.SS4.1.1">Adapt</span> Step</h3>
<section class="ltx_subsubsection" id="S4.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.1 </span>Component Analysis.</h4>
<div class="ltx_para" id="S4.SS4.SSS1.p1">
<p class="ltx_p" id="S4.SS4.SSS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.T4" title="In 4.3.3 Different Aggregators ‣ 4.3 Effectiveness of the Reindex Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> shows introducing bias terms after the <span class="ltx_text ltx_font_typewriter" id="S4.SS4.SSS1.p1.1.1">reindex</span> step is a simple yet effective strategy. This is attributed to the potential for improving recommendation accuracy by addressing popularity misalignments, as discussed in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S1.F1" title="In 1 Introduction ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">1</span></a>. Additionally, we observe that on the small dataset, INSPIRED, +Bias outperforms +RecSys. This is because the parameter space for learning is significantly reduced, changing from learning item-item relationships to learning item point-wise popularity, which can be effectively captured with a small number of training samples.</p>
</div>
<div class="ltx_para" id="S4.SS4.SSS1.p2">
<p class="ltx_p" id="S4.SS4.SSS1.p2.1">Meanwhile, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.T4" title="In 4.3.3 Different Aggregators ‣ 4.3 Effectiveness of the Reindex Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">4</span></a> demonstrates that introducing traditional RecSys models is effective when there is a large number of training samples available to adapt the recommendation distribution. On ReDIAL and Reddit-V1.5, this leads to improved recommendation accuracy compared to Cont. and +Bias. However, on the small dataset INSPIRED, using RecSys to learn item-item relationships tends to result in overfitting. This motivates us to consider different <span class="ltx_text ltx_font_typewriter" id="S4.SS4.SSS1.p2.1.1">adapt</span> steps by cases. For example, after collecting the most recent samples, bias-term adjustment (+Bias) is recommended if the number of new samples is limited. Otherwise, RecSys gating would be a good option.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.2 </span>Impact of Bias Term Types</h4>
<div class="ltx_para" id="S4.SS4.SSS2.p1">
<p class="ltx_p" id="S4.SS4.SSS2.p1.1">Both multiplicative and additive bias terms improve accuracy across diverse datasets, though their impact varies. Specifically, multiplicative bias terms exhibit significant improvement on INSPIRED and ReDIAL datasets, whereas additive bias terms play a pivotal role on Reddit-V1.5.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.4.3 </span>Impact of RecSys Model Types</h4>
<div class="ltx_para" id="S4.SS4.SSS3.p1">
<p class="ltx_p" id="S4.SS4.SSS3.p1.1">Our current focus is on ”item-based” RecSys models without incorporating long-term user representations. In this context, FISM and SASRec exhibit enhanced performance. Notably, FISM outperforms SASRec on the INPSIRED dataset, possibly due to the complexity of SASRec, a transformer-based model, being less suitable for smaller datasets. Conversely, on larger datasets such as ReDIAL and Reddit-V1.5, SASRec demonstrates superior performance, suggesting that employing transformer-based RecSys models is advantageous when dealing with larger data sizes. Specifically, on ReDIAL, characterized by longer conversation rounds, SASRec may bring additional benefits in capturing item-to-item sequential patterns within conversations.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="377" id="S4.F6.g1" src="x5.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An Example with real results from Llama2, Llama-R and Llama-RTA (+SASRec), followed by a natural language response from Llama2 (detailed prompts can be found in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#A2" title="Appendix B Details of Prompts for LLMs ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Appendix</span> <span class="ltx_text ltx_ref_tag">B</span></a>). This conversational context is from ReDIAL dataset, with the ground-truth movie <em class="ltx_emph ltx_font_italic" id="S4.F6.2.1">IT</em>.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Discussions</h3>
<section class="ltx_subsubsection" id="S4.SS5.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.1 </span>Conversational Recommendation Responses</h4>
<div class="ltx_para" id="S4.SS5.SSS1.p1">
<p class="ltx_p" id="S4.SS5.SSS1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.F6" title="In 4.4.3 Impact of RecSys Model Types ‣ 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a> illustrates the complete pipeline of generating results for conversational recommendation tasks. Our discussions are below:</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS1.p2">
<p class="ltx_p" id="S4.SS5.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.SSS1.p2.1.1">On Recommendation.</span> The outputs of the <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS1.p2.1.2">recommendation</span> phase are items. In <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.F6" title="In 4.4.3 Impact of RecSys Model Types ‣ 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>, the three models (Llama2 and its variants under our framework) understand contexts, yielding high-quality recommendations for scary movies. Specifically, Llama2-RTA builds a connection between the superhero movie <em class="ltx_emph ltx_font_italic" id="S4.SS5.SSS1.p2.1.3">Avengers: Infinity War</em> in the context and the candidate <em class="ltx_emph ltx_font_italic" id="S4.SS5.SSS1.p2.1.4">Wonder Woman</em>, using item-to-item relationships modeled by the SASRec <cite class="ltx_cite ltx_citemacro_citep">(Kang and McAuley, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib30" title="">2018</a>)</cite> model. Meanwhile, we posit that while multiple recommended items align with conversation contexts, the failure to adjust for the popularity of items on the target platform (e.g., movie <em class="ltx_emph ltx_font_italic" id="S4.SS5.SSS1.p2.1.5">IT</em> being popular on ReDIAL) leads to zero-shot LLMs failing to meet user interests.</p>
</div>
<div class="ltx_para" id="S4.SS5.SSS1.p3">
<p class="ltx_p" id="S4.SS5.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS5.SSS1.p3.1.1">On Generation.</span> The outputs of the <span class="ltx_text ltx_font_italic" id="S4.SS5.SSS1.p3.1.2">generation</span> phase are texts. In <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.F6" title="In 4.4.3 Impact of RecSys Model Types ‣ 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>, the generation phase is accomplished by prompting the Llama2 model. It is noted that our focus in this work is solely on the technical aspects of the recommendation phase. We treat the generation phase as a separate task that can be completed either by existing LLMs or adjusted based on user interface requirements. Still, we make some observations: (1) In many cases, presenting the recommendation phase suffices for users. However, our RTA framework, which introduces only a few additional parameters without changing the weights of the original LLMs, efficiently enables the reuse of LLMs for further generating natural-language responses as shown in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.F6" title="In 4.4.3 Impact of RecSys Model Types ‣ 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>; (2) In conversational recommendations, there is an ongoing debate about whether to perform the recommendation or generation phase first <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib65" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib57" title="">2022</a>)</cite>. Our example suggests that, if the recommendation phase is frequently adjusted (a common scenario due to distribution shift), it is advisable to perform the recommendation phase first and then the generation phase. Reversing the order may lead to text-item inconsistency issues (e.g., the generated response is specifically tailored for recommended movie <em class="ltx_emph ltx_font_italic" id="S4.SS5.SSS1.p3.1.3">IT</em>, leading to a mismatch with the recommendation from Llama2).</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Recommendation accuracy comparison our model based on a 7B open-sourced LLM (Llama2) and the proprietary model ChatGPT (GPT-3.5-t).</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T5.1" style="width:346.9pt;height:73.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(2.5pt,-0.5pt) scale(1.01468848877875,1.01468848877875) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T5.1.1.1.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T5.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.2.1">INSPIRED</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T5.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.3.1">ReDIAL</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S4.T5.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.1.1.4.1">Reddit-V1.5</span></th>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row" id="S4.T5.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.2.2.1.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.2.2.2">H@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.2.2.3">N@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.2.2.4">H@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.2.2.5">N@10</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.2.2.6">H@10</th>
<th class="ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T5.1.1.2.2.7">N@10</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.1.3.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T5.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.3.1.1.1">Ours.</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.3.1.2">.164 <span class="ltx_text" id="S4.T5.1.1.3.1.2.1" style="font-size:80%;color:#808080;">.025</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.3.1.3">.083 <span class="ltx_text" id="S4.T5.1.1.3.1.3.1" style="font-size:80%;color:#808080;">.004</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.3.1.4">.131 <span class="ltx_text" id="S4.T5.1.1.3.1.4.1" style="font-size:80%;color:#808080;">.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.3.1.5">.068 <span class="ltx_text" id="S4.T5.1.1.3.1.5.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.1.1.3.1.6">.102 <span class="ltx_text" id="S4.T5.1.1.3.1.6.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.1.1.3.1.7">.052 <span class="ltx_text" id="S4.T5.1.1.3.1.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T5.1.1.4.2.1"><span class="ltx_text ltx_font_bold" id="S4.T5.1.1.4.2.1.1">GPT-3.5-t</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.4.2.2">.150 <span class="ltx_text" id="S4.T5.1.1.4.2.2.1" style="font-size:80%;color:#808080;">.024</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.4.2.3">.089 <span class="ltx_text" id="S4.T5.1.1.4.2.3.1" style="font-size:80%;color:#808080;">.016</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.4.2.4">.163 <span class="ltx_text" id="S4.T5.1.1.4.2.4.1" style="font-size:80%;color:#808080;">.006</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.4.2.5">.089 <span class="ltx_text" id="S4.T5.1.1.4.2.5.1" style="font-size:80%;color:#808080;">.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.4.2.6">.104 <span class="ltx_text" id="S4.T5.1.1.4.2.6.1" style="font-size:80%;color:#808080;">.002</span>
</td>
<td class="ltx_td ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.1.1.4.2.7">.055 <span class="ltx_text" id="S4.T5.1.1.4.2.7.1" style="font-size:80%;color:#808080;">.001</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS5.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.5.2 </span>Comparison with Proprietary Models</h4>
<div class="ltx_para" id="S4.SS5.SSS2.p1">
<p class="ltx_p" id="S4.SS5.SSS2.p1.1">To deepen our understanding of the models, we adopt the setting in <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> to query the proprietary model GPT-3.5-t <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib48" title="">2022</a>)</cite>. As shown in <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.T5" title="In 4.5.1 Conversational Recommendation Responses ‣ 4.5 Discussions ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Table</span> <span class="ltx_text ltx_ref_tag">5</span></a>, GPT-3.5-t remains a competitive model for conversational recommendations with zero-shot prompting. However, it is reasonable to guess that, given our LLM-architecture-agnostic approach, improving recommendation accuracy based on GPT-3.5-t is possible if the weights are accessible. A reasonable next step involves working on models similar to GPT-3.5-t, such as Llama2-70b. This could be pursued as future work, if the required compute resources are available.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Work</h2>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Conversational Recommendation (CRS)</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The objective of conversational recommender systems (CRS) is to elicit user preferences and deliver tailored recommendations through interactive dialogues. Historically, CRS implementations have ranged from some template-driven systems <cite class="ltx_cite ltx_citemacro_citep">(Christakopoulou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib14" title="">2016</a>; Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib35" title="">2020b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib34" title="">a</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib25" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib62" title="">2022</a>)</cite> to critique-based approaches <cite class="ltx_cite ltx_citemacro_citep">(Chen and Pu, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib8" title="">2012</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib59" title="">2019</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib38" title="">2021</a>)</cite>. With the evolution of natural language processing, ”deep” CRS models <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib9" title="">2019</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib57" title="">2022</a>)</cite> have been developed, enabling more natural-language interactions.
Research indicates the utility of CRS models is enhanced by incorporating diverse supplementary data, such as knowledge-enriched models <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib9" title="">2019</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib65" title="">2020</a>)</cite> utilizing external knowledge bases <cite class="ltx_cite ltx_citemacro_citep">(Auer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib4" title="">2007</a>; Liu and Singh, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib41" title="">2004</a>)</cite>, review-centric models <cite class="ltx_cite ltx_citemacro_citep">(Lu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib43" title="">2021</a>)</cite>, and session/sequence-oriented models <cite class="ltx_cite ltx_citemacro_citep">(Zou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib66" title="">2022</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib39" title="">2022b</a>)</cite>.
UniCRS <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib57" title="">2022</a>)</cite> uses knowledge bases <cite class="ltx_cite ltx_citemacro_citep">(Auer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib4" title="">2007</a>)</cite>, built on DialoGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib61" title="">2020</a>)</cite> and employing prompt tuning <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib6" title="">2020b</a>)</cite>, represents a state-of-the-art CRS model on datasets like ReDIAL <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite> and INSPIRED <cite class="ltx_cite ltx_citemacro_citep">(Hayati et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib23" title="">2020</a>)</cite>. Recently, an emerging topic is to leverage LLMs in CRS, with <cite class="ltx_cite ltx_citemacro_citep">(Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib19" title="">2023</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> introducing a novel CRS pipeline, even in the zero-shot setting <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>, and <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib56" title="">2023b</a>)</cite> focusing on advanced user simulation for LLM evaluation. Our research is the first to study the distribution misalignments in zero-shot LLMs for CRS and solutions for this issue to improve recommendation accuracy.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Large Language Models (LLMs)</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Recent breakthroughs in natural language processing (NLP) have demonstrated that large language models (LLMs) possess a remarkable capacity for generalizing to unfamiliar tasks and areas <cite class="ltx_cite ltx_citemacro_citep">(Chowdhery et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib13" title="">2022</a>; Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib7" title="">2020a</a>; Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib58" title="">2022</a>)</cite> in zero-shot or few-shot settings. Studies have shown that scaling up LLMs can significantly enhance their performance and efficiency in downstream applications <cite class="ltx_cite ltx_citemacro_citep">(Kaplan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib32" title="">2020</a>)</cite>.
In line with these developments, LLMs have been successfully applied to various downstream tasks such as question answering, numerical reasoning, code generation, and commonsense reasoning, often without requiring gradient updates <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib64" title="">2023</a>; Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib7" title="">2020a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib40" title="">2022a</a>; Kaplan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib32" title="">2020</a>)</cite>. The recommendation field has recently begun integrating LLMs, either by adapting LLM architectures <cite class="ltx_cite ltx_citemacro_citep">(Geng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib20" title="">2022</a>; Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib16" title="">2022</a>)</cite> or repurposing existing LLMs for recommendation purposes <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib36" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib55" title="">2023a</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib42" title="">2023</a>)</cite>. Our study aligns with the research line of utilizing LLMs for conversational recommendations. We improvements in recommendation accuracy by adjusting item recommendations within the proposed framework.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>LLMs for Recommendation</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">There is growing interest in the academic community to harness LLMs for recommendation-related tasks. One research direction explores LLMs within conventional recommendation setup, which typically incorporate user feedback and item metadata <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib31" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib27" title="">2023</a>; Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib60" title="">2023</a>; Dai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib17" title="">2023</a>; Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib5" title="">2023</a>; Harte et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib22" title="">2023</a>; Sanner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib47" title="">2023</a>)</cite>. This includes tasks such as rating prediction <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib31" title="">2023</a>)</cite> and sequential recommendation <cite class="ltx_cite ltx_citemacro_citep">(Harte et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib22" title="">2023</a>; Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib60" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib27" title="">2023</a>)</cite>. In such contexts, employing LLMs as recommenders has shown potential, particularly in scenarios with extreme data sparsity <cite class="ltx_cite ltx_citemacro_citep">(Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib5" title="">2023</a>)</cite> or during the cold-start phase <cite class="ltx_cite ltx_citemacro_citep">(Sanner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib47" title="">2023</a>)</cite>. However, they often struggle to surpass simpler baseline methods, like non-personalized popularity-based models, in standard recommendation scenarios <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib31" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib27" title="">2023</a>)</cite>. Nevertheless, enhancing existing recommender systems with features generated by LLMs has yielded improved performance <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib3" title="">2023</a>)</cite>. Another significant research direction focuses on language-centric recommendation tasks <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>; Acharya et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib2" title="">2023</a>; Mysore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib44" title="">2023</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib18" title="">2023</a>; Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib19" title="">2023</a>)</cite>. These tasks include generating explanations for recommendations, narrative-based recommendations <cite class="ltx_cite ltx_citemacro_citep">(Mysore et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib44" title="">2023</a>)</cite>, and conversational recommendations <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>; Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib18" title="">2023</a>; Friedman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib19" title="">2023</a>)</cite>. LLMs exhibit proficient performance in understanding intricate textual inputs, allowing for personalized recommendation outputs. Recent investigations in conversational recommendation demonstrate encouraging outcomes leveraging LLMs, even in zero-shot configurations. Our study employs existing LLMs with minimal additional parameters, implementing the <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.1.1">Reindex-Then-Adapt</span> framework. Through the reindexing of item content within LLMs and fine-tuning recommendations to align with target data distributions, our framework enhances recommendation accuracy in CRS.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This study proposes a solution to mitigate distribution misalignments between zero-shot large language models (LLMs) and target recommendation platforms for conversational recommendations. We conceptualize LLMs as Differential Search Index (DSI) models and introduce the <em class="ltx_emph ltx_font_italic" id="S6.p1.1.1">Reindex-Then-Adapt (RTA)</em> framework. The framework involves converting multi-token item titles into single tokens within LLMs (<span class="ltx_text ltx_font_typewriter" id="S6.p1.1.2">reindex</span> step) and subsequently adjusting their probability distributions (<span class="ltx_text ltx_font_typewriter" id="S6.p1.1.3">adapt</span> step). By combining the strengths of LLMs and traditional RecSys, the RTA framework achieves improved recommendation accuracy metrics across various conversational recommendation datasets and adaptation settings.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Acharya et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Arkadeep Acharya, Brijraj Singh, and Naoyuki Onoe. 2023.

</span>
<span class="ltx_bibblock">LLM Based Generation of Item-Description for Recommendation System. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 1204–1207.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Saurabh Agrawal, John Trenkle, and Jaya Kawale. 2023.

</span>
<span class="ltx_bibblock">Beyond Labels: Leveraging Deep Learning and LLMs for Content Metadata. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 1–1.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Auer et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007.

</span>
<span class="ltx_bibblock">Dbpedia: A nucleus for a web of open data. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007, Busan, Korea, November 11-15, 2007. Proceedings</em>. Springer, 722–735.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023.

</span>
<span class="ltx_bibblock">Tallrec: An effective and efficient tuning framework to align large language model with recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv preprint arXiv:2305.00447</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al<span class="ltx_text" id="bib.bib6.3.1">.</span> 2020b.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.4.1">Advances in neural information processing systems</em> 33 (2020), 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. 2020a.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Advances in Neural Information Processing Systems</em>, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877–1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen and Pu (2012)</span>
<span class="ltx_bibblock">
Li Chen and Pearl Pu. 2012.

</span>
<span class="ltx_bibblock">Critiquing-based recommenders: survey and emerging trends.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">User Modeling and User-Adapted Interaction</em> 22 (2012), 125–150.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Qibin Chen, Junyang Lin, Yichang Zhang, Ming Ding, Yukuo Cen, Hongxia Yang, and Jie Tang. 2019.

</span>
<span class="ltx_bibblock">Towards Knowledge-Based Recommender Dialog System. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>. 1803–1813.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaoyang Chen, Yanjiang Liu, Ben He, Le Sun, and Yingfei Sun. 2023.

</span>
<span class="ltx_bibblock">Understanding Differential Search Index for Text Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">arXiv preprint arXiv:2305.02073</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cho et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">On the Properties of Neural Machine Translation: Encoder–Decoder Approaches. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation</em>. 103–111.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Hyunjin Choi, Judong Kim, Seongho Joe, and Youngjune Gwon. 2021.

</span>
<span class="ltx_bibblock">Evaluation of bert and albert sentence embedding performance on downstream nlp tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">2020 25th International conference on pattern recognition (ICPR)</em>. IEEE, 5482–5487.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chowdhery et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya,
Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier García, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Díaz, Orhan Firat,
Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.

</span>
<span class="ltx_bibblock">PaLM: Scaling Language Modeling with Pathways.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">ArXiv</em> abs/2204.02311 (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Christakopoulou et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Konstantina Christakopoulou, Filip Radlinski, and Katja Hofmann. 2016.

</span>
<span class="ltx_bibblock">Towards conversational recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</em>. 815–824.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014.

</span>
<span class="ltx_bibblock">Empirical evaluation of gated recurrent neural networks on sequence modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">arXiv preprint arXiv:1412.3555</em> (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zeyu Cui, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022.

</span>
<span class="ltx_bibblock">M6-Rec: Generative Pretrained Language Models are Open-Ended Recommender Systems.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2205.08084 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongxiang Sun, Xiao Zhang, and Jun Xu. 2023.

</span>
<span class="ltx_bibblock">Uncovering ChatGPT’s Capabilities in Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">arXiv preprint arXiv:2305.02182</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yue Feng, Shuchang Liu, Zhenghai Xue, Qingpeng Cai, Lantao Hu, Peng Jiang, Kun Gai, and Fei Sun. 2023.

</span>
<span class="ltx_bibblock">A Large Language Model Enhanced Conversational Recommender System.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">arXiv preprint arXiv:2308.06212</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Friedman et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Luke Friedman, Sameer Ahuja, David Allen, Terry Tan, Hakim Sidahmed, Changbo Long, Jun Xie, Gabriel Schubiner, Ajay Patel, Harsh Lara, et al<span class="ltx_text" id="bib.bib19.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Leveraging Large Language Models in Conversational Recommender Systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.4.1">arXiv preprint arXiv:2305.07961</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geng et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang. 2022.

</span>
<span class="ltx_bibblock">Recommendation as Language Processing (RLP): A Unified Pretrain, Personalized Prompt &amp; Predict Paradigm (P5). In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">RecSys ’22: Sixteenth ACM Conference on Recommender Systems, Seattle, WA, USA, September 18 - 23, 2022</em>, Jennifer Golbeck, F. Maxwell Harper, Vanessa Murdock, Michael D. Ekstrand, Bracha Shapira, Justin Basilico, Keld T. Lundgaard, and Even Oldridge (Eds.). ACM, 299–315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Albert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. 2020.

</span>
<span class="ltx_bibblock">Improving the gating mechanism of recurrent neural networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">International Conference on Machine Learning</em>. PMLR, 3800–3809.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harte et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jesse Harte, Wouter Zorgdrager, Panos Louridas, Asterios Katsifodimos, Dietmar Jannach, and Marios Fragkoulis. 2023.

</span>
<span class="ltx_bibblock">Leveraging Large Language Models for Sequential Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 1096–1102.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hayati et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Shirley Anugrah Hayati, Dongyeop Kang, Qingxiaoyang Zhu, Weiyan Shi, and Zhou Yu. 2020.

</span>
<span class="ltx_bibblock">INSPIRED: Toward Sociable Recommendation Dialog Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>. 8142–8152.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhankui He, Zhouhang Xie, Rahul Jha, Harald Steck, Dawen Liang, Yesu Feng, Bodhisattwa Prasad Majumder, Nathan Kallus, and Julian McAuley. 2023.

</span>
<span class="ltx_bibblock">Large language models as zero-shot conversational recommenders. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>. 720–730.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhankui He, Handong Zhao, Tong Yu, Sungchul Kim, Fan Du, and Julian McAuley. 2022.

</span>
<span class="ltx_bibblock">Bundle MCR: Towards Conversational Bundle Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the 16th ACM Conference on Recommender Systems</em>. 288–298.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hochreiter and Schmidhuber (1997)</span>
<span class="ltx_bibblock">
Sepp Hochreiter and Jürgen Schmidhuber. 1997.

</span>
<span class="ltx_bibblock">Long short-term memory.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Neural computation</em> 9, 8 (1997), 1735–1780.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2023.

</span>
<span class="ltx_bibblock">Large language models are zero-shot rankers for recommender systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">arXiv preprint arXiv:2305.08845</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al<span class="ltx_text" id="bib.bib28.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Mistral 7B.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.4.1">arXiv preprint arXiv:2310.06825</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kabbur et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Santosh Kabbur, Xia Ning, and George Karypis. 2013.

</span>
<span class="ltx_bibblock">Fism: factored item similarity models for top-n recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</em>. 659–667.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang and McAuley (2018)</span>
<span class="ltx_bibblock">
Wang-Cheng Kang and Julian McAuley. 2018.

</span>
<span class="ltx_bibblock">Self-attentive sequential recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">2018 IEEE international conference on data mining (ICDM)</em>. IEEE, 197–206.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wang-Cheng Kang, Jianmo Ni, Nikhil Mehta, Maheswaran Sathiamoorthy, Lichan Hong, Ed Chi, and Derek Zhiyuan Cheng. 2023.

</span>
<span class="ltx_bibblock">Do LLMs Understand User Preferences? Evaluating LLMs On User Rating Prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">arXiv preprint arXiv:2305.06474</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kaplan et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Scaling Laws for Neural Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">ArXiv</em> abs/2001.08361 (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kenton and Toutanova (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of NAACL-HLT</em>, Vol. 1. 2.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, and Tat-Seng Chua. 2020a.

</span>
<span class="ltx_bibblock">Estimation-action-reflection: Towards deep interaction between conversational and recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the 13th International Conference on Web Search and Data Mining</em>. 304–312.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Wenqiang Lei, Gangyi Zhang, Xiangnan He, Yisong Miao, Xiang Wang, Liang Chen, and Tat-Seng Chua. 2020b.

</span>
<span class="ltx_bibblock">Interactive path reasoning on graph for conversational recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 2073–2083.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni. 2023.

</span>
<span class="ltx_bibblock">GPT4Rec: A Generative Framework for Personalized Recommendation and User Interests Interpretation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.03879 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Raymond Li, Samira Ebrahimi Kahou, Hannes Schulz, Vincent Michalski, Laurent Charlin, and Chris Pal. 2018.

</span>
<span class="ltx_bibblock">Towards deep conversational recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Advances in neural information processing systems</em> 31 (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Shuyang Li, Bodhisattwa Prasad Majumder, and Julian McAuley. 2021.

</span>
<span class="ltx_bibblock">Self-Supervised Bot Play for Conversational Recommendation with Justifications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv preprint arXiv:2112.05197</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Shuokai Li, Ruobing Xie, Yongchun Zhu, Xiang Ao, Fuzhen Zhuang, and Qing He. 2022b.

</span>
<span class="ltx_bibblock">User-centric conversational recommendation with multi-aspect user modeling. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 223–233.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Yujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, Daniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and Oriol Vinyals.
2022a.

</span>
<span class="ltx_bibblock">Competition-level code generation with AlphaCode.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Science</em> 378 (2022), 1092 – 1097.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Singh (2004)</span>
<span class="ltx_bibblock">
Hugo Liu and Push Singh. 2004.

</span>
<span class="ltx_bibblock">ConceptNet—a practical commonsense reasoning tool-kit.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">BT technology journal</em> 22, 4 (2004), 211–226.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Junling Liu, Chao Liu, Renjie Lv, Kang Zhou, and Yan Zhang. 2023.

</span>
<span class="ltx_bibblock">Is ChatGPT a Good Recommender? A Preliminary Study.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.10149 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yu Lu, Junwei Bao, Yan Song, Zichen Ma, Shuguang Cui, Youzheng Wu, and Xiaodong He. 2021.

</span>
<span class="ltx_bibblock">RevCore: Review-Augmented Conversational Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>. 1161–1173.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mysore et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023.

</span>
<span class="ltx_bibblock">Large Language Model Augmented Narrative Driven Recommendations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">arXiv preprint arXiv:2306.02250</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oord et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018.

</span>
<span class="ltx_bibblock">Representation learning with contrastive predictive coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">arXiv preprint arXiv:1807.03748</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>. 3982–3992.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sanner et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Scott Sanner, Krisztian Balog, Filip Radlinski, Ben Wedin, and Lucas Dixon. 2023.

</span>
<span class="ltx_bibblock">Large Language Models are Competitive Near Cold-start Recommenders for Language-and Item-based Preferences. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>. 890–896.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schulman et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
John Schulman, Barret Zoph, C Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, Rapha Gontijo Lopes, and Sengjia Zhao. 2022.

</span>
<span class="ltx_bibblock">Chatgpt: Optimizing language models for dialogue.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">OpenAI</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sedhain et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.

</span>
<span class="ltx_bibblock">Autorec: Autoencoders meet collaborative filtering. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Proceedings of the 24th international conference on World Wide Web</em>. 111–112.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Su et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A Smith, Luke Zettlemoyer, and Tao Yu. 2022.

</span>
<span class="ltx_bibblock">One embedder, any task: Instruction-finetuned text embeddings.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">arXiv preprint arXiv:2212.09741</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, et al<span class="ltx_text" id="bib.bib51.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Transformer memory as a differentiable search index.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.4.1">Advances in Neural Information Processing Systems</em> 35 (2022), 21831–21843.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Team (2023)</span>
<span class="ltx_bibblock">
MosaicML NLP Team. 2023.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs</em>.

</span>
<span class="ltx_bibblock">
<span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">www.mosaicml.com/blog/mpt-7b</span>
</span>
<span class="ltx_bibblock">Accessed: 2023-05-05.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al<span class="ltx_text" id="bib.bib53.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.4.1">arXiv preprint arXiv:2307.09288</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">Advances in neural information processing systems</em>. 5998–6008.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, and Tat-Seng Chua. 2023a.

</span>
<span class="ltx_bibblock">Generative Recommendation: Towards Next-generation Recommender Paradigm.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2304.03516 [cs.IR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, and Ji-Rong Wen. 2023b.

</span>
<span class="ltx_bibblock">Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.3.1">arXiv preprint arXiv:2305.13112</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao. 2022.

</span>
<span class="ltx_bibblock">Towards Unified Conversational Recommender Systems via Knowledge-Enhanced Prompt Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 1929–1937.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Advances in Neural Information Processing Systems</em>, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Ga Wu, Kai Luo, Scott Sanner, and Harold Soh. 2019.

</span>
<span class="ltx_bibblock">Deep language-based critiquing for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">Proceedings of the 13th ACM Conference on Recommender Systems</em>. 137–145.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhenrui Yue, Sara Rabhi, Gabriel de Souza Pereira Moreira, Dong Wang, and Even Oldridge. 2023.

</span>
<span class="ltx_bibblock">LlamaRec: Two-Stage Recommendation using Large Language Models for Ranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">arXiv preprint arXiv:2311.02089</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and William B Dolan. 2020.

</span>
<span class="ltx_bibblock">DIALOGPT: Large-Scale Generative Pre-training for Conversational Response Generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</em>. 270–278.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yiming Zhang, Lingfei Wu, Qi Shen, Yitong Pang, Zhihua Wei, Fangli Xu, Bo Long, and Jian Pei. 2022.

</span>
<span class="ltx_bibblock">Multiple Choice Questions based Multi-Interest Policy Learning for Conversational Recommendation. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the ACM Web Conference 2022</em>. 2153–2162.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021.

</span>
<span class="ltx_bibblock">Calibrate before use: Improving few-shot performance of language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">International Conference on Machine Learning</em>. PMLR, 12697–12706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. 2023.

</span>
<span class="ltx_bibblock">CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Evaluations on HumanEval-X.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.17568 [cs.LG]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen, and Jingsong Yu. 2020.

</span>
<span class="ltx_bibblock">Improving conversational recommender systems via knowledge graph based semantic fusion. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</em>. 1006–1014.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jie Zou, Evangelos Kanoulas, Pengjie Ren, Zhaochun Ren, Aixin Sun, and Cheng Long. 2022.

</span>
<span class="ltx_bibblock">Improving conversational recommender systems via transformer-based sequential modelling. In <em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 2319–2324.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>More Details of Experiments</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Baseline Details</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.1">We consider four groups of baseline models for comparison. Firstly, we consider some representative traditional <span class="ltx_text ltx_font_italic" id="A1.SS1.p1.1.1">item-based</span> RecSys models:</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i1.p1.1.1">Popularity</span>: This method is non-personalized and returns the top-K most popular movies within the related datasets.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i2.p1.1.1">FISM</span> <cite class="ltx_cite ltx_citemacro_citep">(Kabbur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib29" title="">2013</a>)</cite>: A commonly used factored item similarity model for item-based collaborative filtering.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I1.i3.p1.1.1">SASRec</span> <cite class="ltx_cite ltx_citemacro_citep">(Kang and McAuley, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib30" title="">2018</a>)</cite>: A competitive self-attention-based sequential recommender system.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1">Secondly, we consider some representative CRS models:</p>
</div>
<div class="ltx_para" id="A1.SS1.p4">
<ul class="ltx_itemize" id="A1.I2">
<li class="ltx_item" id="A1.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i1.p1">
<p class="ltx_p" id="A1.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i1.p1.1.1">ReDIAL</span> <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib37" title="">2018</a>)</cite>: This model is released along with the ReDIAL dataset with an auto-encoder <cite class="ltx_cite ltx_citemacro_citep">(Sedhain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib49" title="">2015</a>)</cite>-based recommender.
</p>
</div>
</li>
<li class="ltx_item" id="A1.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I2.i2.p1">
<p class="ltx_p" id="A1.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I2.i2.p1.1.1">UniCRS</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib57" title="">2022</a>)</cite>: This model uses pre-trained language model, DialoGPT <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib61" title="">2020</a>)</cite>, with prompt tuning to conduct recommendation and conversation generation tasks respectively. This model is treated as a state-of-the-art CRS models before LLMs <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS1.p5">
<p class="ltx_p" id="A1.SS1.p5.1">Thirdly, we consider some dense retrieval models given the connections to document retrieval:</p>
</div>
<div class="ltx_para" id="A1.SS1.p6">
<ul class="ltx_itemize" id="A1.I3">
<li class="ltx_item" id="A1.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I3.i1.p1">
<p class="ltx_p" id="A1.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i1.p1.1.1">SBERT</span> <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib46" title="">2019</a>)</cite>: A modification of the pretrained BERT <cite class="ltx_cite ltx_citemacro_citep">(Kenton and Toutanova, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib33" title="">2019</a>)</cite> network. It uses siamese and triplet network structures to generate semantically meaningful sentence embeddings.</p>
</div>
</li>
<li class="ltx_item" id="A1.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I3.i2.p1">
<p class="ltx_p" id="A1.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I3.i2.p1.1.1">Instructor</span> <cite class="ltx_cite ltx_citemacro_citep">(Su et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib50" title="">2022</a>)</cite>: A text embedding model that has been fine-tuned for instructional purposes, which is considered state-of-the-art in dense retrieval tasks.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS1.p7">
<p class="ltx_p" id="A1.SS1.p7.1">Lastly, we consider some zero-shot open-sourced LLMs as baselines like <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite>, we are using the 7-billion version due to compute burden:</p>
</div>
<div class="ltx_para" id="A1.SS1.p8">
<ul class="ltx_itemize" id="A1.I4">
<li class="ltx_item" id="A1.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i1.p1">
<p class="ltx_p" id="A1.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I4.i1.p1.1.1">MPT-7b</span> <cite class="ltx_cite ltx_citemacro_citep">(Team, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib52" title="">2023</a>)</cite>: A recently released open-sourced LLM released by MosaicML’s team trained on 1T tokens.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i2.p1">
<p class="ltx_p" id="A1.I4.i2.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I4.i2.p1.1.1">Mistral-7b</span> <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib28" title="">2023</a>)</cite>: A recently released open-sourced Large Language Model with impressive performance on multiple tasks, trained by Mistral AI Team.</p>
</div>
</li>
<li class="ltx_item" id="A1.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I4.i3.p1">
<p class="ltx_p" id="A1.I4.i3.p1.1"><span class="ltx_text ltx_font_bold" id="A1.I4.i3.p1.1.1">Llama2-7b</span> <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib53" title="">2023</a>)</cite>: A commonly used open-sourced Large Language Model with a wide eco-system support.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="A1.SS1.p9">
<p class="ltx_p" id="A1.SS1.p9.1">We also discuss the results from <span class="ltx_text ltx_font_bold" id="A1.SS1.p9.1.1">GPT-3.5-turbo</span> <cite class="ltx_cite ltx_citemacro_citep">(Schulman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib48" title="">2022</a>)</cite>, which is a much larger proprietary model that can achieve state-of-the-art CRS performance even in a zero-shot setting <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>In particular, the GPT-3.5-turbo API was called in January 2024 with a temperature setting of 0. This note aims to enhance reproducibility of GPT APIs, considering the continuous updates made by OpenAI over time.</span></span></span>.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Implementation Details</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">For zero-shot baselines, we configured models based on links from <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p1.1.1">huggingface</span> official model pages for inference on our datasets. Trainable baselines utilized hyperparameters suggested by authors, with a batch size of 256. The learning rate search space is {1e-3, 1e-4, 1e-5}, and weight decay is {0, 1e-6, 1e-4, 1e-2, 1}. Baselines were trained for 200 epochs, and the best model was selected based on H@10 on the validation set. Reindex and adapt steps of our model followed the same hyper-parameter setup above. For L2R Data and L2I Data, we used the original data mixture without adjusting the sampling ratio. The initial data weights were approximately 98:2, and addressing the data mixture weighting is deferred to future work, as it may enhance recommendation results, though not the primary focus of this paper. For <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p1.1.2">Reindex</span> Step, the RNN we used is a GRU <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib15" title="">2014</a>)</cite> network, with embedding size as the same as Llama2-7b (i.e., 4096) and hidden size is 1024. We use the bidirectional single-layer GRU modules. For the <span class="ltx_text ltx_font_typewriter" id="A1.SS2.p1.1.3">Adapt</span> Step, the FISM models are with embedding size 64, and the SASRec models are using embedding size 64, 2 self-attention layers and 2 attention heads.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Details of Prompts for LLMs</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Prompt(s) for Recommendation</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">For LLMs, we follow <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> to define the recommendation prompts as follows, which can be used to obtain the LLM baseline results and used in our reindex and adapt steps.</p>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">For LLM baselines, this prompt is following <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib24" title="">2023</a>)</cite> with the fuzzy matching method to convert generated recommendation lists into within-dataset item ID lists. In the prompt example, we omit the “converstaion templates“, which are obtained them from <span class="ltx_text ltx_font_typewriter" id="A2.SS1.p2.1.1">FastChat<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_serif" id="footnote6.1.1.1">6</span></span><span class="ltx_ref ltx_nolink ltx_url ltx_ref_self">https://github.com/lm-sys/FastChat/blob/1db84d0906196673db361eac50d5aa65180a0ffe/fastchat/conversation.py</span></span></span></span></span> to ensure the zero-shot performance of LLM baselines.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p3">
<span class="ltx_ERROR undefined" id="A2.SS1.p3.1">\MakeFramed</span><span class="ltx_ERROR undefined" id="A2.SS1.p3.2">\FrameRestore</span><span class="ltx_ERROR undefined" id="A2.SS1.p3.3">{adjustwidth}</span>
<p class="ltx_p" id="A2.SS1.p3.4">7pt
<span class="ltx_text ltx_font_bold" id="A2.SS1.p3.4.1" style="font-size:90%;">Prompt for LLM Baselines<span class="ltx_text ltx_font_medium" id="A2.SS1.p3.4.1.1">: <em class="ltx_emph ltx_font_italic" id="A2.SS1.p3.4.1.1.1">Pretend you are a movie recommender system.
I will give you a conversation between a user and you (a recommender system). Based on the conversation, you reply me with 20 movie titles without extra sentences. Here is the conversation:</em> </span>{}<span class="ltx_text ltx_font_medium" id="A2.SS1.p3.4.1.2">
<span class="ltx_ERROR undefined" id="A2.SS1.p3.4.1.2.1">\endMakeFramed</span></span></span></p>
</div>
<div class="ltx_para" id="A2.SS1.p4">
<p class="ltx_p" id="A2.SS1.p4.1">Here, “{}” is the placeholder for the conversational context, which is examplified by <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.F6" title="In 4.4.3 Impact of RecSys Model Types ‣ 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<div class="ltx_para" id="A2.SS1.p5">
<p class="ltx_p" id="A2.SS1.p5.1">For our RTA framework, since the base model is Llama2-7b <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#bib.bib53" title="">2023</a>)</cite>, we specify the prompt to make it clear that how we “reindex” the generated items. This prompt is exactly ended with “1. ”, for which the original next steps are the tokens for the recommended item titles, such as “1. Edge of Tomorrow”. However, we record the query embedding ended by “1. ” and replace the embedding sequence for “<span class="ltx_text ltx_font_typewriter" id="A2.SS1.p5.1.1">Edge of Tomorrow</span>” with “<span class="ltx_text ltx_font_typewriter" id="A2.SS1.p5.1.2">|Edge_of_Tomorrow|</span>” for reindexing. The concrete prompt for the reindex step is:</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p6">
<span class="ltx_ERROR undefined" id="A2.SS1.p6.1">\MakeFramed</span><span class="ltx_ERROR undefined" id="A2.SS1.p6.2">\FrameRestore</span><span class="ltx_ERROR undefined" id="A2.SS1.p6.3">{adjustwidth}</span>
<p class="ltx_p" id="A2.SS1.p6.4">7pt
<span class="ltx_text ltx_font_bold" id="A2.SS1.p6.4.1" style="font-size:90%;">Prompt for Llama-RTA<span class="ltx_text ltx_font_medium" id="A2.SS1.p6.4.1.1">: <em class="ltx_emph ltx_font_italic" id="A2.SS1.p6.4.1.1.1">¡s¿ [INST] Pretend you are a movie recommender system. I will give you a conversation between a user and you (a recommender system). Based on the conversation, you reply me with 20 movie titles without extra sentences. Here is the conversation:</em> </span>{}<span class="ltx_text ltx_font_medium" id="A2.SS1.p6.4.1.2"> <em class="ltx_emph ltx_font_italic" id="A2.SS1.p6.4.1.2.1">[/INST] 1. </em>
<span class="ltx_ERROR undefined" id="A2.SS1.p6.4.1.2.2">\endMakeFramed</span></span></span></p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Prompt(s) for Generation</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">For the prompt used in case studies <a class="ltx_ref" href="https://arxiv.org/html/2405.12119v1#S4.F6" title="In 4.4.3 Impact of RecSys Model Types ‣ 4.4 Effectiveness of the Adapt Step ‣ 4 Experiments ‣ Reindex-Then-Adapt: Improving Large Language Models for Conversational Recommendation"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">6</span></a>, we define the prompt as below, where the first placeholder “{}” is for the conversational context, and the second placeholder “{}” is for the item recommendation list.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS2.p2">
<span class="ltx_ERROR undefined" id="A2.SS2.p2.1">\MakeFramed</span><span class="ltx_ERROR undefined" id="A2.SS2.p2.2">\FrameRestore</span><span class="ltx_ERROR undefined" id="A2.SS2.p2.3">{adjustwidth}</span>
<p class="ltx_p" id="A2.SS2.p2.4">7pt
<span class="ltx_text ltx_font_bold" id="A2.SS2.p2.4.1" style="font-size:90%;">Prompt for Llama-RTA<span class="ltx_text ltx_font_medium" id="A2.SS2.p2.4.1.1">: <em class="ltx_emph ltx_font_italic" id="A2.SS2.p2.4.1.1.1">Pretend you are a movie recommender system. I will give you a conversation between a user and you (a recommender system). Based on the conversation, you reply me with 20 movie titles without extra sentences. Here is the conversation:</em> </span>{}<span class="ltx_text ltx_font_medium" id="A2.SS2.p2.4.1.2"> <em class="ltx_emph ltx_font_italic" id="A2.SS2.p2.4.1.2.1">. Please respond the above conversations using the recommended items below, it is better if explaining why they are recommended, but do not list them as bullets. Insert them into your responses:</em> </span>{}<span class="ltx_text ltx_font_medium" id="A2.SS2.p2.4.1.3">
<span class="ltx_ERROR undefined" id="A2.SS2.p2.4.1.3.1">\endMakeFramed</span></span></span></p>
</div>
<div class="ltx_para" id="A2.SS2.p3">
<p class="ltx_p" id="A2.SS2.p3.1">It is noted that we do not aim to demonstrate the <span class="ltx_text ltx_font_italic" id="A2.SS2.p3.1.1">optimal</span> generation strategy, but rather provide an example of how the language model framework developed for our recommendation system can also be reused for generative tasks, for the use cases where natural-language responses are required.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 20 15:34:16 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
