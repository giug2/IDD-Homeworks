<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Unified Multi-Modal Interleaved Document Representation for Information Retrieval</title>
<!--Generated on Thu Oct  3 17:45:47 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.02729v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S1" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S2" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S2.SS0.SSS0.Px1" title="In 2 Related Work â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Information Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S2.SS0.SSS0.Px2" title="In 2 Related Work â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Multimodal Information Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S2.SS0.SSS0.Px3" title="In 2 Related Work â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Vision-Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S3" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S3.SS1" title="In 3 Method â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminary</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S3.SS1.SSS0.Px1" title="In 3.1 Preliminary â€£ 3 Method â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Information Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S3.SS1.SSS0.Px2" title="In 3.1 Preliminary â€£ 3 Method â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Vision-Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S3.SS2" title="In 3 Method â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Retriever</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S3.SS3" title="In 3 Method â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Reranker</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS1" title="In 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS2" title="In 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS2.SSS0.Px1" title="In 4.2 Implementation Details â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Model Training and Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS2.SSS0.Px2" title="In 4.2 Implementation Details â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS2.SSS0.Px3" title="In 4.2 Implementation Details â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Evaluation Metrics</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS3" title="In 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS3.SSS0.Px1" title="In 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Interleaved format improves document retrieval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS3.SSS0.Px2" title="In 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Interleaved format enhances document retrieval across modalities.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS3.SSS0.Px3" title="In 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Interleaved format is also beneficial in section retrieval.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS3.SSS0.Px4" title="In 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Information retrieval of tabular contents in interleaved documents is challenging.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS4" title="In 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Further Analysis and Ablation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS4.SSS0.Px1" title="In 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">More sections enhance document retrieval performance but raise computational costs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS4.SSS0.Px2" title="In 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Sections from the same document act as effective negatives to enhance reranker performance.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS4.SSS0.Px3" title="In 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">BCE loss applied to each section produces the best reranker performance.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS4.SSS0.Px4" title="In 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Rerankers require much larger datasets than retrievers.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S5" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A0.SS0.SSS0.Px1" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Appendix</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A0.SS0.SSS0.Px2" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Organization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A1" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A1.SS0.SSS0.Px1" title="In Appendix A Implementation Details â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Dataset configuration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A1.SS0.SSS0.Px2" title="In Appendix A Implementation Details â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title">Dataset pre-processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A2" title="In Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Limitations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Unified Multi-Modal Interleaved Document Representation for Information Retrieval</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Jaewoo Lee<sup class="ltx_sup" id="id8.8.id1"><span class="ltx_text ltx_font_italic" id="id8.8.id1.1">1,2</span></sup> â€„
Joonho Ko<sup class="ltx_sup" id="id9.9.id2"><span class="ltx_text ltx_font_italic" id="id9.9.id2.1">1âˆ—</span></sup> â€„
Jinheon Baek<sup class="ltx_sup" id="id10.10.id3"><span class="ltx_text ltx_font_italic" id="id10.10.id3.1">1âˆ—</span></sup> â€„
Soyeong Jeong<sup class="ltx_sup" id="id11.11.id4"><span class="ltx_text ltx_font_italic" id="id11.11.id4.1">1</span></sup> â€„
Sung Ju Hwang<sup class="ltx_sup" id="id12.12.id5"><span class="ltx_text ltx_font_italic" id="id12.12.id5.1">1,2</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id13.13.id6">1</sup>KAISTÂ Â Â <sup class="ltx_sup" id="id14.14.id7">2</sup>DeepAuto 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id15.15.id8">jwlee8877@gmail.com</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id16.16.id9">{joonho.ko, jinheon.baek, starsuzi, sjhwang82}@kaist.ac.kr</span>
</span><span class="ltx_author_notes">Equal Contribution</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id17.id1">Information Retrieval (IR) methods aim to identify relevant documents in response to a given query, which have gained remarkable attention due to their successful application in various natural language tasks. However, existing approaches typically consider only the textual information within the documents, which overlooks the fact that documents can contain multiple modalities, including texts, images, and tables. Further, they often segment each long document into multiple discrete passages for embedding, preventing them from capturing the overall document context and interactions between paragraphs. We argue that these two limitations lead to suboptimal document representations for retrieval. In this work, to address them, we aim to produce more comprehensive and nuanced document representations by holistically embedding documents interleaved with different modalities. Specifically, we achieve this by leveraging the capability of recent vision-language models that enable the processing and integration of text, images, and tables into a unified format and representation. Moreover, to mitigate the information loss from segmenting documents into passages, instead of representing and retrieving passages individually, we further merge the representations of segmented passages into one single document representation, while we additionally introduce a reranking strategy to decouple and identify the relevant passage within the document if necessary. Then, through extensive experiments on diverse information retrieval scenarios considering both the textual and multimodal queries, we show that our approach substantially outperforms relevant baselines, thanks to the consideration of the multimodal information interleaved within the documents in a unified way.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Information Retrieval (IR) is the task of fetching relevant documents from a large corpus in response to an input query, which becomes a fundamental process to various real-world applications including web search engines and question-answering systemsÂ <cite class="ltx_cite ltx_citemacro_citep">(Shah etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib38" title="">2019</a>; Lewis etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib22" title="">2020</a>; Guu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib10" title="">2020</a>)</cite>. Specifically, to retrieve documents for the query, traditional approaches have focused on their textual representations, utilizing either sparse retrieval methods such as TF-IDF and BM25Â <cite class="ltx_cite ltx_citemacro_citep">(Robertson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib37" title="">1994</a>; Jones, <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib16" title="">2004</a>)</cite>, which rely on exact term matching between the query and document, or dense retrieval methods such as DPR and ANCEÂ <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib17" title="">2020</a>; Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib43" title="">2021</a>)</cite>, which leverage neural embeddings of the query and document text to capture semantic similarities between them over a continuous vector space. Recently, dense retrieval methods have gained more popularity over sparse methods due to their capability to capture semantic nuances and context beyond simple keyword matching, leading to multiple successes with improved performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Despite their huge successes, existing (dense) retrieval methods face a couple of severe challenges. First, they primarily rely on the textual data for document embedding and retrieval, overlooking the fact that modern documents often contain multimodal content, such as images and tables (beyond the plain text), which can carry critical information that may be essential for accurately understanding and retrieving the relevant documents. To be specific, a diagram within a medical article can more effectively represent the structure of a molecule or the progression of a disease, offering more clarity that would be difficult to achieve with text alone, and omitting such multimodal content can lead to an incomplete understanding (and potentially inaccurate retrieval) of the documents. Also, the segmentation of long documents into discrete passages, which is commonly employed by retrieval models to handle the length limitation for embeddings, may prevent models from capturing the full context and the intricate relationships between different parts of the document, ultimately leading to suboptimal retrieval performance. It is worthwhile noting that, concurrent to our work, while there has been recent work that screen captures the document and then embed its screenshots (to consider different modalities in a unified format)Â <cite class="ltx_cite ltx_citemacro_citep">(Faysse etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib8" title="">2024</a>; Ma etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib27" title="">2024a</a>)</cite>, not only its content (such as paragraphs, images, and tables) can be fragmented into different sub-images, leading to the loss of contextual coherence across the entire document, but also the visual representation of text may hinder the modelâ€™s ability to capture the semantic relationships present in the original textual data, while increasing the image resolution leads to the critical concern on the memory requirements.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this work, we introduce a novel approach to holistic document embedding for IR, which addresses the aforementioned challenges by representing and retrieving the documents interleaved with different modalities in a unified manner. Specifically, our method revolves around the recent advance of Vision-Language Models (VLMs), which enables the processing and integration of multimodal content (such as text, images, and tables) directly into a single token sequence, thereby preserving the context and relationships between various parts of the document, unlike the previous approaches that rely on the fragmented visual representations. Furthermore, in cases where the number of tokens in a document is large and exceeds the capacity of a single context window of VLMs, we propose a strategy to segment the document into manageable passages, each represented within the token limit, and combine these passage representations into a unified document representation, which differs from existing IR approaches that independently represent and retrieve at the passage level, potentially losing the overall document context. Lastly, to accurately identify only the relevant sections within the lengthy documents, we introduce a reranking mechanism that is trained to pinpoint the passage most pertinent to the query (among all the other passages within the document), effectively allowing for both the coarse-grained document-level matching and the fine-grained passage-level retrieval. We provide the visual illustrations of the overall pipeline of IDentIfy against prior work in FigureÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">1</span></a>. We refer to our overall method as <span class="ltx_text ltx_font_bold" id="S1.p3.1.1">I</span>nterleaved <span class="ltx_text ltx_font_bold" id="S1.p3.1.2">D</span>ocum<span class="ltx_text ltx_font_bold" id="S1.p3.1.3">ent</span> <span class="ltx_text ltx_font_bold" id="S1.p3.1.4">I</span>n<span class="ltx_text ltx_font_bold" id="S1.p3.1.5">f</span>ormation Retrieval S<span class="ltx_text ltx_font_bold" id="S1.p3.1.6">y</span>stem (IDentIfy).</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We experimentally validate the effectiveness of IDentIfy on four different benchmark datasets, considering both the text-only and multimodal queries. On a battery of tests conducted, we then observe that our approach substantially outperforms relevant baselines that consider only the uni-modality for document representations, thanks to the consideration of multimodal content. Further, we find that the strategy to represent the whole document with its single representation (by merging embeddings of its splits if necessary) is superior to the approach of individually representing them for document retrieval, but also performing reranking over the sections of the retrieved document is superior to the approach of directly retrieving those sections, which confirm the efficacy of the proposed retrieval and reranking pipeline for document and passage retrieval, respectively.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="304" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Comparison of different IR approaches. <span class="ltx_text ltx_font_bold" id="S1.F1.4.1">(a)</span>: Conventional methods use a small portion of the text within the document for its representation. <span class="ltx_text ltx_font_bold" id="S1.F1.5.2">(b)</span>: Recent methods use first-page screenshot images to represent the document. <span class="ltx_text ltx_font_bold" id="S1.F1.6.3">(c)</span>: Our approach leverages the full contextual information within documents interleaved with multiple modalities by considering them in their original format, and is capable of pinpointing relevant sections.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Information Retrieval</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Information Retrieval (IR) is the task of accurately finding documents relevant to a given query from a large corpus, such as Wikipedia, which has been a crucial component for a variety of applications, including search engines, question-answering systems, and conversational agentsÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib49" title="">2023</a>; Gao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib9" title="">2023</a>; Ram etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib36" title="">2023</a>; Shi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib39" title="">2024</a>; Jeong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib13" title="">2024a</a>)</cite>. Specifically, to retrieve the relevant documents, earlier IR approaches measured similarity between queries and documents based on their lexical term matching, such as BM25 and TF-IDFÂ <cite class="ltx_cite ltx_citemacro_citep">(Robertson etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib37" title="">1994</a>; Jones, <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib16" title="">2004</a>)</cite>. Yet, these methods often struggled to capture the semantic nuances beyond surface-level term overlaps. To overcome this, along with advancements in language modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib6" title="">2019</a>; Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib24" title="">2019</a>)</cite>. there has been dense retrieval approaches that embed both the queries and documents into a shared dense vector spaceÂ <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib17" title="">2020</a>; Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib43" title="">2021</a>)</cite>, enabling the calculation of semantic similarity between them more effectively by capturing the deeper contextual information. However, previous IR studies have mainly focused on enhancing the textual representations of queries and documents, while overlooking the fact that documents often consist of diverse modalities (such as images and tables) beyond text, which can potentially provide richer context and aid in more accurate retrievalÂ <cite class="ltx_cite ltx_citemacro_citep">(Liu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib25" title="">2021</a>; Jeong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib14" title="">2024b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Multimodal Information Retrieval</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">Recent studies in IR have expanded the focus from purely text-based retrieval models to those that consider other modalities, such as imagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib35" title="">2021</a>; Xiao etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib42" title="">2024</a>)</cite>, tablesÂ <cite class="ltx_cite ltx_citemacro_citep">(Herzig etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib11" title="">2021</a>; Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib3" title="">2024</a>)</cite> and graphsÂ <cite class="ltx_cite ltx_citemacro_citep">(Baek etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib1" title="">2023</a>)</cite>; however, the majority of these approachesÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib48" title="">2024</a>; Long etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib26" title="">2024</a>; Lerner etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib21" title="">2024</a>; Nowak etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib30" title="">2024</a>; Caffagni etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib2" title="">2024</a>)</cite> have primarily explored how to process the multimodal <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.1">queries</span>, meanwhile, they often overlook the equally important multimodal characteristics of the <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.1.2">documents</span> being retrieved. Specifically, we argue that, while incorporating multimodal elements in queries has expanded the range and diversity of query types that IR systems can handle, considering the multimodal nature of the documents can lead to a more holistic representation of retrieval targets, which can ultimately lead to enhancing the overall retrieval performance. In efforts to handle diverse multimodal elements within documents, there are concurrent studies that have proposed to capture screenshots of documents, such as PDFsÂ <cite class="ltx_cite ltx_citemacro_citep">(Faysse etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib8" title="">2024</a>)</cite> or Wikipedia web pagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib27" title="">2024a</a>)</cite>, and subsequently encoding them through vision modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Ding etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib7" title="">2024</a>)</cite>. However, these methods are not only limited by factors, such as image resolution and computational memory, constraining their application to documents longer than a single page<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>For instance, <cite class="ltx_cite ltx_citemacro_cite">Ma etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib27" title="">2024a</a>)</cite> requires processing 9.8k image tokens just to process a single-page document, and it results in 2TB of memory for handling the entire Wikipedia corpus, which is not much practical.</span></span></span>, but also fall short by treating the diverse modalities within a document as a single visual entity, leading to suboptimal document representations that fail to effectively capture the nuanced interdependence between text and images. Furthermore, they do not address the critical issue of splitting documents into smaller fragments (e.g., sub-images), which may disrupt the holistic contextual view of the entire document.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Vision-Language Models</h4>
<div class="ltx_para ltx_noindent" id="S2.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px3.p1.1">Recent Vision-Language Models (VLMs) have emerged as a powerful tool for jointly processing visual and textual data, combining the image understanding capabilities of visual encodersÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib35" title="">2021</a>; Zhai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib45" title="">2023</a>)</cite> with the advanced reasoning abilities of language modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib31" title="">2022</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib32" title="">2023a</a>)</cite>. These models have achieved remarkable performance across diverse vision-language (VL) tasks (such as image captioning and visual question answering)Â <cite class="ltx_cite ltx_citemacro_citep">(Dai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib5" title="">2023</a>; OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib33" title="">2023b</a>)</cite>, with the substantially limited attention on their applications to IR. We note that the latest developments in this field have particularly focused on enabling VLMs to handle interleaved, multimodal content, which involves a mixed sequence of images and textÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib46" title="">2023</a>; Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib23" title="">2024</a>)</cite>. In particular, LLaVA-NeXT-InterleaveÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib23" title="">2024</a>)</cite> introduces a fine-tuning approach that specifically enhances the VLMsâ€™ capacity to understand complex interleavings of multiple images and text within a single context. Drawing inspiration from these advances, in this work, we propose to harness the capabilities of VLMs to create unified embeddings for documents interleaved with text and images (as well as tables) for IR, which is a big shift from even the recent IR approachÂ <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib28" title="">2024b</a>)</cite> that still embeds the documents with the recent but text-based models like LlamaÂ <cite class="ltx_cite ltx_citemacro_citep">(Touvron etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib40" title="">2023a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib41" title="">b</a>)</cite>, failing to fully capture the diverse multimodal content.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We present IDentIfy to holistically represent documents interleaved with multimodal elements.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminary</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We begin with preliminaries, formally explaining information retrieval and vision-language models.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Information Retrieval</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.7">Recall that Information Retrieval (IR) is the task of searching for relevant documents from a large corpus in response to a given query. Formally, let <math alttext="{\bm{q}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">ğ’’</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">ğ’’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">{\bm{q}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.1.m1.1d">bold_italic_q</annotation></semantics></math> denote a query, <math alttext="{\bm{d}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px1.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">ğ’…</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.2.m2.1.1">ğ’…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.2.m2.1c">{\bm{d}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.2.m2.1d">bold_italic_d</annotation></semantics></math> denote a document, and <math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px1.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S3.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.3.m3.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.3.m3.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.3.m3.1d">caligraphic_D</annotation></semantics></math> denote a collection of documents (<math alttext="{\bm{d}}\in\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px1.p1.4.m4.1a"><mrow id="S3.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml">ğ’…</mi><mo id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.1" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml">âˆˆ</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml">ğ’Ÿ</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1"><in id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.1"></in><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.2">ğ’…</ci><ci id="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.4.m4.1.1.3">ğ’Ÿ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.4.m4.1c">{\bm{d}}\in\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.4.m4.1d">bold_italic_d âˆˆ caligraphic_D</annotation></semantics></math>), where each query and document can be represented as a sequence of tokens: <math alttext="{\bm{q}}=[q_{1},q_{2},\ldots,q_{n}]" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.5.m5.4"><semantics id="S3.SS1.SSS0.Px1.p1.5.m5.4a"><mrow id="S3.SS1.SSS0.Px1.p1.5.m5.4.4" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.5" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.5.cmml">ğ’’</mi><mo id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.4" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.4.cmml">=</mo><mrow id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.4.cmml"><mo id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.4" stretchy="false" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.4.cmml">[</mo><msub id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.2.cmml">q</mi><mn id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.5" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.2.cmml">q</mi><mn id="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.6" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px1.p1.5.m5.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">â€¦</mi><mo id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.7" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.2" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.2.cmml">q</mi><mi id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.3" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.8" stretchy="false" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.5.m5.4b"><apply id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4"><eq id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.4.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.4"></eq><ci id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.5.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.5">ğ’’</ci><list id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.4.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3"><apply id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.2">ğ‘</ci><cn id="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.5.m5.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.2">ğ‘</ci><cn id="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.5.m5.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.1.1">â€¦</ci><apply id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.2">ğ‘</ci><ci id="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.5.m5.4.4.3.3.3.3">ğ‘›</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.5.m5.4c">{\bm{q}}=[q_{1},q_{2},\ldots,q_{n}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.5.m5.4d">bold_italic_q = [ italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_q start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math> and <math alttext="{\bm{d}}=[d_{1},d_{2},\ldots,d_{m}]" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.6.m6.4"><semantics id="S3.SS1.SSS0.Px1.p1.6.m6.4a"><mrow id="S3.SS1.SSS0.Px1.p1.6.m6.4.4" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.5" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.5.cmml">ğ’…</mi><mo id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.4" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.4.cmml">=</mo><mrow id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.4.cmml"><mo id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.4" stretchy="false" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.4.cmml">[</mo><msub id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.2.cmml">d</mi><mn id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.5" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.2.cmml">d</mi><mn id="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.6" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.4.cmml">,</mo><mi id="S3.SS1.SSS0.Px1.p1.6.m6.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">â€¦</mi><mo id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.7" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.4.cmml">,</mo><msub id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.2" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.2.cmml">d</mi><mi id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.3" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.3.cmml">m</mi></msub><mo id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.8" stretchy="false" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.6.m6.4b"><apply id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4"><eq id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.4.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.4"></eq><ci id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.5.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.5">ğ’…</ci><list id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.4.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3"><apply id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.2">ğ‘‘</ci><cn id="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.6.m6.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.2">ğ‘‘</ci><cn id="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p1.6.m6.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.1.1">â€¦</ci><apply id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.2">ğ‘‘</ci><ci id="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.3.cmml" xref="S3.SS1.SSS0.Px1.p1.6.m6.4.4.3.3.3.3">ğ‘š</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.6.m6.4c">{\bm{d}}=[d_{1},d_{2},\ldots,d_{m}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.6.m6.4d">bold_italic_d = [ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , â€¦ , italic_d start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ]</annotation></semantics></math> where <math alttext="\left[\cdot\right]" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.7.m7.1"><semantics id="S3.SS1.SSS0.Px1.p1.7.m7.1a"><mrow id="S3.SS1.SSS0.Px1.p1.7.m7.1.2.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.2.1.cmml"><mo id="S3.SS1.SSS0.Px1.p1.7.m7.1.2.2.1" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.2.1.1.cmml">[</mo><mo id="S3.SS1.SSS0.Px1.p1.7.m7.1.1" lspace="0em" rspace="0em" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1.cmml">â‹…</mo><mo id="S3.SS1.SSS0.Px1.p1.7.m7.1.2.2.2" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.7.m7.1b"><apply id="S3.SS1.SSS0.Px1.p1.7.m7.1.2.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.2.2"><csymbol cd="latexml" id="S3.SS1.SSS0.Px1.p1.7.m7.1.2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.2.2.1">delimited-[]</csymbol><ci id="S3.SS1.SSS0.Px1.p1.7.m7.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.7.m7.1.1">â‹…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.7.m7.1c">\left[\cdot\right]</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.7.m7.1d">[ â‹… ]</annotation></semantics></math> indicates a concatenation operation in a specific order. We note that traditional IR approaches typically consider these tokens as purely textual elements; however, in this work, we propose to extend this assumption to have the tokens of both the textual and visual content, to capture the multimodal nature of many real-world documents. Then, this new extension raises important questions of how can both the textual and visual content be represented within a unified token framework, and how can these multimodal tokens be seamlessly integrated and encoded for document representations. To answer those two questions, we harness the power of recent vision-language models below.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Vision-Language Models</h4>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.5">We now turn to describing Vision-Language Models (VLMs), which are designed to jointly encode the textual and visual information in a unified token framework. We note that these models are generally comprised of two main components: a visual encoder and a language model, interconnected through a projection layer. Specifically, given an input document that may contain interleaved modalities (e.g., text and images), the visual encoder extracts high-level visual features from (multiple) images embedded within the document, mapping them into a latent space. Then, these visual features are transformed into a sequence of visual tokens via the projection layer, represented as follows: <math alttext="\mathbf{V}\!\in\!\mathbb{R}^{V\!\times\!d_{\text{emb}}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S3.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">ğ•</mi><mo id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1" lspace="0.108em" rspace="0.108em" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml">V</mi><mo id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.1" lspace="0.052em" rspace="0.052em" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml">Ã—</mo><msub id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.2" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.2.cmml">d</mi><mtext id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.3" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.3a.cmml">emb</mtext></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1"><in id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.1"></in><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.2">ğ•</ci><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.2">â„</ci><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3"><times id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.2">ğ‘‰</ci><apply id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.2">ğ‘‘</ci><ci id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.3a.cmml" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.3"><mtext id="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.3.cmml" mathsize="50%" xref="S3.SS1.SSS0.Px2.p1.1.m1.1.1.3.3.3.3">emb</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.1.m1.1c">\mathbf{V}\!\in\!\mathbb{R}^{V\!\times\!d_{\text{emb}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.1.m1.1d">bold_V âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_V Ã— italic_d start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="V" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px2.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.2.m2.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.2.m2.1c">V</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.2.m2.1d">italic_V</annotation></semantics></math> denotes the visual token length and <math alttext="d_{\text{emb}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px2.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml">d</mi><mtext id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3a.cmml">emb</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.2">ğ‘‘</ci><ci id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3a.cmml" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3"><mtext id="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3.cmml" mathsize="70%" xref="S3.SS1.SSS0.Px2.p1.3.m3.1.1.3">emb</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.3.m3.1c">d_{\text{emb}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.3.m3.1d">italic_d start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT</annotation></semantics></math> is the token dimension size. Similarly, for the textual content embedded within the document alongside images, the language model uses a word embedding layer to convert the input text into a sequence of text tokens, represented as follows: <math alttext="\mathbf{L}\!\in\!\mathbb{R}^{L\!\times\!d_{\text{emb}}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px2.p1.4.m4.1a"><mrow id="S3.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml">ğ‹</mi><mo id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.1" lspace="0.108em" rspace="0.108em" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.2" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.2" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.2.cmml">L</mi><mo id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.1" lspace="0.052em" rspace="0.052em" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml">Ã—</mo><msub id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.2" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.2.cmml">d</mi><mtext id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.3" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.3a.cmml">emb</mtext></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.4.m4.1b"><apply id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1"><in id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.1"></in><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.2">ğ‹</ci><apply id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.2">â„</ci><apply id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3"><times id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.1"></times><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.2">ğ¿</ci><apply id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.2">ğ‘‘</ci><ci id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.3a.cmml" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.3"><mtext id="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.3.cmml" mathsize="50%" xref="S3.SS1.SSS0.Px2.p1.4.m4.1.1.3.3.3.3">emb</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.4.m4.1c">\mathbf{L}\!\in\!\mathbb{R}^{L\!\times\!d_{\text{emb}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.4.m4.1d">bold_L âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_L Ã— italic_d start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="L" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p1.5.m5.1"><semantics id="S3.SS1.SSS0.Px2.p1.5.m5.1a"><mi id="S3.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p1.5.m5.1b"><ci id="S3.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px2.p1.5.m5.1.1">ğ¿</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p1.5.m5.1c">L</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p1.5.m5.1d">italic_L</annotation></semantics></math> denotes the token length of text.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p2.2">In this work, we also propose to account for tables that are an integral modality for holistically representing the full content of documents. However, in contrast to text and images that have dedicated processing layers within the VLM architectures, tables do not have a specific representation layer. Nevertheless, we argue that recent VLMs are pre-trained on diverse web data, and subsequently they are implicitly learned to handle the table structures formatted in HTML. Consequently, we treat HTML-format table data as a linearized sequence of HTML words, applying the same word embedding layer as is used for plain text. To be formal, this process converts the table content into table tokens, as follows: <math alttext="\mathbf{T}\!\in\mathbb{R}^{T\!\times\!d_{\text{emb}}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p2.1.m1.1a"><mrow id="S3.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml">ğ“</mi><mo id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.1" lspace="0.108em" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.2" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.2.cmml">â„</mi><mrow id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.2" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.2.cmml">T</mi><mo id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.1" lspace="0.052em" rspace="0.052em" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.1.cmml">Ã—</mo><msub id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.cmml"><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.2" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.2.cmml">d</mi><mtext id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.3" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.3a.cmml">emb</mtext></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1"><in id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.1"></in><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2">ğ“</ci><apply id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.2">â„</ci><apply id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3"><times id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.2">ğ‘‡</ci><apply id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.2.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.2">ğ‘‘</ci><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.3a.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.3"><mtext id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.3.cmml" mathsize="50%" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.3.3.3">emb</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p2.1.m1.1c">\mathbf{T}\!\in\mathbb{R}^{T\!\times\!d_{\text{emb}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p2.1.m1.1d">bold_T âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_T Ã— italic_d start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="T" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.2.m2.1"><semantics id="S3.SS1.SSS0.Px2.p2.2.m2.1a"><mi id="S3.SS1.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1.cmml">T</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p2.2.m2.1b"><ci id="S3.SS1.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1">ğ‘‡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p2.2.m2.1c">T</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p2.2.m2.1d">italic_T</annotation></semantics></math> is the token length of the table. Lastly, once extracted, the visual tokens, text tokens, and table tokens are concatenated (to form a unified token sequence) and then passed through the remaining layers of VLMs, to capture both uni- and cross-modal relationships across different modalities, enabling the comprehensive understanding of the input document.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Retriever</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">We now turn to explaining how we design a retriever specifically tailored for multimodal interleaved document retrieval. In particular, to effectively retrieve documents that contain multiple modalities, our approach leverages a VLM capable of processing text, images, and tables within a single document. Further, following the standard practice of existing retrieval architecturesÂ <cite class="ltx_cite ltx_citemacro_citep">(Karpukhin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib17" title="">2020</a>; Xiong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib43" title="">2021</a>)</cite>, we use a dual-encoder structure, which consists of a query encoder and a section encoder, both are based on the VLM, which is illustrated inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S3.F2" title="In 3.3 Reranker â€£ 3 Method â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> <span class="ltx_text" id="S3.SS2.p1.1.1" style="color:#DB143D;">(a)</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.7">Specifically, thanks to the use of the VLM, our query encoder can take either purely textual queries <math alttext="{\bm{q}}\!=\!\mathbf{L}_{\mathrm{Q}}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">ğ’’</mi><mo id="S3.SS2.p2.1.m1.1.1.1" lspace="0.108em" rspace="0.108em" xref="S3.SS2.p2.1.m1.1.1.1.cmml">=</mo><msub id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">ğ‹</mi><mi id="S3.SS2.p2.1.m1.1.1.3.3" mathvariant="normal" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">Q</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><eq id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></eq><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">ğ’’</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">ğ‹</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">Q</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">{\bm{q}}\!=\!\mathbf{L}_{\mathrm{Q}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">bold_italic_q = bold_L start_POSTSUBSCRIPT roman_Q end_POSTSUBSCRIPT</annotation></semantics></math> or multimodal queries consisting of text and corresponding visual elements <math alttext="{\bm{q}}\!=\![\mathbf{V}_{\mathrm{Q}},\;\mathbf{L}_{\mathrm{Q}}]" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.2"><semantics id="S3.SS2.p2.2.m2.2a"><mrow id="S3.SS2.p2.2.m2.2.2" xref="S3.SS2.p2.2.m2.2.2.cmml"><mi id="S3.SS2.p2.2.m2.2.2.4" xref="S3.SS2.p2.2.m2.2.2.4.cmml">ğ’’</mi><mo id="S3.SS2.p2.2.m2.2.2.3" lspace="0.108em" rspace="0.108em" xref="S3.SS2.p2.2.m2.2.2.3.cmml">=</mo><mrow id="S3.SS2.p2.2.m2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml"><mo id="S3.SS2.p2.2.m2.2.2.2.2.3" stretchy="false" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml">[</mo><msub id="S3.SS2.p2.2.m2.1.1.1.1.1" xref="S3.SS2.p2.2.m2.1.1.1.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.1.1.1.2" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2.cmml">ğ•</mi><mi id="S3.SS2.p2.2.m2.1.1.1.1.1.3" mathvariant="normal" xref="S3.SS2.p2.2.m2.1.1.1.1.1.3.cmml">Q</mi></msub><mo id="S3.SS2.p2.2.m2.2.2.2.2.4" rspace="0.447em" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p2.2.m2.2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.2.2.cmml"><mi id="S3.SS2.p2.2.m2.2.2.2.2.2.2" xref="S3.SS2.p2.2.m2.2.2.2.2.2.2.cmml">ğ‹</mi><mi id="S3.SS2.p2.2.m2.2.2.2.2.2.3" mathvariant="normal" xref="S3.SS2.p2.2.m2.2.2.2.2.2.3.cmml">Q</mi></msub><mo id="S3.SS2.p2.2.m2.2.2.2.2.5" stretchy="false" xref="S3.SS2.p2.2.m2.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.2b"><apply id="S3.SS2.p2.2.m2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2"><eq id="S3.SS2.p2.2.m2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.3"></eq><ci id="S3.SS2.p2.2.m2.2.2.4.cmml" xref="S3.SS2.p2.2.m2.2.2.4">ğ’’</ci><interval closure="closed" id="S3.SS2.p2.2.m2.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2"><apply id="S3.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.2">ğ•</ci><ci id="S3.SS2.p2.2.m2.1.1.1.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.1.1.1.3">Q</ci></apply><apply id="S3.SS2.p2.2.m2.2.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.2.2.2.2.2.1.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p2.2.m2.2.2.2.2.2.2.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2.2">ğ‹</ci><ci id="S3.SS2.p2.2.m2.2.2.2.2.2.3.cmml" xref="S3.SS2.p2.2.m2.2.2.2.2.2.3">Q</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.2c">{\bm{q}}\!=\![\mathbf{V}_{\mathrm{Q}},\;\mathbf{L}_{\mathrm{Q}}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.2d">bold_italic_q = [ bold_V start_POSTSUBSCRIPT roman_Q end_POSTSUBSCRIPT , bold_L start_POSTSUBSCRIPT roman_Q end_POSTSUBSCRIPT ]</annotation></semantics></math>. Also, to obtain the final query representation, we introduce a learnable token called â€˜End of Queryâ€™, <math alttext="\texttt{[EoQ]}\!\!\in\!\mathbb{R}^{d_{\text{emb}}}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.1"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mpadded width="2.852em"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2a.cmml">[EoQ]</mtext></mpadded><mo id="S3.SS2.p2.3.m3.1.1.1" rspace="0.108em" xref="S3.SS2.p2.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.2" xref="S3.SS2.p2.3.m3.1.1.3.2.cmml">â„</mi><msub id="S3.SS2.p2.3.m3.1.1.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS2.p2.3.m3.1.1.3.3.2" xref="S3.SS2.p2.3.m3.1.1.3.3.2.cmml">d</mi><mtext id="S3.SS2.p2.3.m3.1.1.3.3.3" xref="S3.SS2.p2.3.m3.1.1.3.3.3a.cmml">emb</mtext></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><in id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></in><ci id="S3.SS2.p2.3.m3.1.1.2a.cmml" xref="S3.SS2.p2.3.m3.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">[EoQ]</mtext></ci><apply id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.2">â„</ci><apply id="S3.SS2.p2.3.m3.1.1.3.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.2">ğ‘‘</ci><ci id="S3.SS2.p2.3.m3.1.1.3.3.3a.cmml" xref="S3.SS2.p2.3.m3.1.1.3.3.3"><mtext id="S3.SS2.p2.3.m3.1.1.3.3.3.cmml" mathsize="50%" xref="S3.SS2.p2.3.m3.1.1.3.3.3">emb</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\texttt{[EoQ]}\!\!\in\!\mathbb{R}^{d_{\text{emb}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.1d">[EoQ] âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>. This token is appended to the end of the sequence of query tokens <math alttext="{\bm{q}}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.1"><semantics id="S3.SS2.p2.4.m4.1a"><mi id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml">ğ’’</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><ci id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1">ğ’’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">{\bm{q}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.1d">bold_italic_q</annotation></semantics></math>, and the final concatenated tokens <math alttext="\left[{\bm{q}},\;\texttt{[EoQ]}\right]" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.2"><semantics id="S3.SS2.p2.5.m5.2a"><mrow id="S3.SS2.p2.5.m5.2.3.2" xref="S3.SS2.p2.5.m5.2.3.1.cmml"><mo id="S3.SS2.p2.5.m5.2.3.2.1" xref="S3.SS2.p2.5.m5.2.3.1.cmml">[</mo><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">ğ’’</mi><mo id="S3.SS2.p2.5.m5.2.3.2.2" rspace="0.447em" xref="S3.SS2.p2.5.m5.2.3.1.cmml">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p2.5.m5.2.2" xref="S3.SS2.p2.5.m5.2.2a.cmml">[EoQ]</mtext><mo id="S3.SS2.p2.5.m5.2.3.2.3" xref="S3.SS2.p2.5.m5.2.3.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.2b"><interval closure="closed" id="S3.SS2.p2.5.m5.2.3.1.cmml" xref="S3.SS2.p2.5.m5.2.3.2"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">ğ’’</ci><ci id="S3.SS2.p2.5.m5.2.2a.cmml" xref="S3.SS2.p2.5.m5.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p2.5.m5.2.2.cmml" xref="S3.SS2.p2.5.m5.2.2">[EoQ]</mtext></ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.2c">\left[{\bm{q}},\;\texttt{[EoQ]}\right]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.2d">[ bold_italic_q , [EoQ] ]</annotation></semantics></math> are then passed through the query encoder. Then, the model output corresponding to <span class="ltx_text ltx_markedasmath ltx_font_typewriter" id="S3.SS2.p2.7.1">[EoQ]</span> is used as the final query representation, as follows: <math alttext="\mathbf{Z}_{\mathrm{Q}}\!\in\!\mathbb{R}^{d_{\text{emb}}}" class="ltx_Math" display="inline" id="S3.SS2.p2.7.m7.1"><semantics id="S3.SS2.p2.7.m7.1a"><mrow id="S3.SS2.p2.7.m7.1.1" xref="S3.SS2.p2.7.m7.1.1.cmml"><msub id="S3.SS2.p2.7.m7.1.1.2" xref="S3.SS2.p2.7.m7.1.1.2.cmml"><mi id="S3.SS2.p2.7.m7.1.1.2.2" xref="S3.SS2.p2.7.m7.1.1.2.2.cmml">ğ™</mi><mi id="S3.SS2.p2.7.m7.1.1.2.3" mathvariant="normal" xref="S3.SS2.p2.7.m7.1.1.2.3.cmml">Q</mi></msub><mo id="S3.SS2.p2.7.m7.1.1.1" lspace="0.108em" rspace="0.108em" xref="S3.SS2.p2.7.m7.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p2.7.m7.1.1.3" xref="S3.SS2.p2.7.m7.1.1.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.3.2" xref="S3.SS2.p2.7.m7.1.1.3.2.cmml">â„</mi><msub id="S3.SS2.p2.7.m7.1.1.3.3" xref="S3.SS2.p2.7.m7.1.1.3.3.cmml"><mi id="S3.SS2.p2.7.m7.1.1.3.3.2" xref="S3.SS2.p2.7.m7.1.1.3.3.2.cmml">d</mi><mtext id="S3.SS2.p2.7.m7.1.1.3.3.3" xref="S3.SS2.p2.7.m7.1.1.3.3.3a.cmml">emb</mtext></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.7.m7.1b"><apply id="S3.SS2.p2.7.m7.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1"><in id="S3.SS2.p2.7.m7.1.1.1.cmml" xref="S3.SS2.p2.7.m7.1.1.1"></in><apply id="S3.SS2.p2.7.m7.1.1.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.2.1.cmml" xref="S3.SS2.p2.7.m7.1.1.2">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.2.2.cmml" xref="S3.SS2.p2.7.m7.1.1.2.2">ğ™</ci><ci id="S3.SS2.p2.7.m7.1.1.2.3.cmml" xref="S3.SS2.p2.7.m7.1.1.2.3">Q</ci></apply><apply id="S3.SS2.p2.7.m7.1.1.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.3">superscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.3.2">â„</ci><apply id="S3.SS2.p2.7.m7.1.1.3.3.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p2.7.m7.1.1.3.3.1.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p2.7.m7.1.1.3.3.2.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3.2">ğ‘‘</ci><ci id="S3.SS2.p2.7.m7.1.1.3.3.3a.cmml" xref="S3.SS2.p2.7.m7.1.1.3.3.3"><mtext id="S3.SS2.p2.7.m7.1.1.3.3.3.cmml" mathsize="50%" xref="S3.SS2.p2.7.m7.1.1.3.3.3">emb</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.7.m7.1c">\mathbf{Z}_{\mathrm{Q}}\!\in\!\mathbb{R}^{d_{\text{emb}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.7.m7.1d">bold_Z start_POSTSUBSCRIPT roman_Q end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.13">For documents, we first represent each document <math alttext="{\bm{d}}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.1"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">ğ’…</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">ğ’…</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">{\bm{d}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.1d">bold_italic_d</annotation></semantics></math> as a sequence of sections <math alttext="{\bm{d}}\!=\![{\bm{s}}_{i}]_{i=1}^{S}" class="ltx_Math" display="inline" id="S3.SS2.p3.2.m2.1"><semantics id="S3.SS2.p3.2.m2.1a"><mrow id="S3.SS2.p3.2.m2.1.1" xref="S3.SS2.p3.2.m2.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.3" xref="S3.SS2.p3.2.m2.1.1.3.cmml">ğ’…</mi><mo id="S3.SS2.p3.2.m2.1.1.2" lspace="0.108em" rspace="0.108em" xref="S3.SS2.p3.2.m2.1.1.2.cmml">=</mo><msubsup id="S3.SS2.p3.2.m2.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.cmml"><mrow id="S3.SS2.p3.2.m2.1.1.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.1.2.cmml"><mo id="S3.SS2.p3.2.m2.1.1.1.1.1.1.2" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.1.1.1.2.1.cmml">[</mo><msub id="S3.SS2.p3.2.m2.1.1.1.1.1.1.1" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.2" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.2.cmml">ğ’”</mi><mi id="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p3.2.m2.1.1.1.1.1.1.3" stretchy="false" xref="S3.SS2.p3.2.m2.1.1.1.1.1.2.1.cmml">]</mo></mrow><mrow id="S3.SS2.p3.2.m2.1.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.2.m2.1.1.1.1.3.2" xref="S3.SS2.p3.2.m2.1.1.1.1.3.2.cmml">i</mi><mo id="S3.SS2.p3.2.m2.1.1.1.1.3.1" xref="S3.SS2.p3.2.m2.1.1.1.1.3.1.cmml">=</mo><mn id="S3.SS2.p3.2.m2.1.1.1.1.3.3" xref="S3.SS2.p3.2.m2.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p3.2.m2.1.1.1.3" xref="S3.SS2.p3.2.m2.1.1.1.3.cmml">S</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.2.m2.1b"><apply id="S3.SS2.p3.2.m2.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1"><eq id="S3.SS2.p3.2.m2.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.2"></eq><ci id="S3.SS2.p3.2.m2.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.3">ğ’…</ci><apply id="S3.SS2.p3.2.m2.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1">superscript</csymbol><apply id="S3.SS2.p3.2.m2.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1">subscript</csymbol><apply id="S3.SS2.p3.2.m2.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.SS2.p3.2.m2.1.1.1.1.1.2.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.2">ğ’”</ci><ci id="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></apply><apply id="S3.SS2.p3.2.m2.1.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.3"><eq id="S3.SS2.p3.2.m2.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.3.1"></eq><ci id="S3.SS2.p3.2.m2.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.2.m2.1.1.1.1.3.2">ğ‘–</ci><cn id="S3.SS2.p3.2.m2.1.1.1.1.3.3.cmml" type="integer" xref="S3.SS2.p3.2.m2.1.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS2.p3.2.m2.1.1.1.3.cmml" xref="S3.SS2.p3.2.m2.1.1.1.3">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.2.m2.1c">{\bm{d}}\!=\![{\bm{s}}_{i}]_{i=1}^{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.2.m2.1d">bold_italic_d = [ bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT</annotation></semantics></math> (with a total of <math alttext="S" class="ltx_Math" display="inline" id="S3.SS2.p3.3.m3.1"><semantics id="S3.SS2.p3.3.m3.1a"><mi id="S3.SS2.p3.3.m3.1.1" xref="S3.SS2.p3.3.m3.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.3.m3.1b"><ci id="S3.SS2.p3.3.m3.1.1.cmml" xref="S3.SS2.p3.3.m3.1.1">ğ‘†</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.3.m3.1c">S</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.3.m3.1d">italic_S</annotation></semantics></math> sections), where each section <math alttext="{\bm{s}}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.4.m4.1"><semantics id="S3.SS2.p3.4.m4.1a"><msub id="S3.SS2.p3.4.m4.1.1" xref="S3.SS2.p3.4.m4.1.1.cmml"><mi id="S3.SS2.p3.4.m4.1.1.2" xref="S3.SS2.p3.4.m4.1.1.2.cmml">ğ’”</mi><mi id="S3.SS2.p3.4.m4.1.1.3" xref="S3.SS2.p3.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.4.m4.1b"><apply id="S3.SS2.p3.4.m4.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.4.m4.1.1.1.cmml" xref="S3.SS2.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS2.p3.4.m4.1.1.2.cmml" xref="S3.SS2.p3.4.m4.1.1.2">ğ’”</ci><ci id="S3.SS2.p3.4.m4.1.1.3.cmml" xref="S3.SS2.p3.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.4.m4.1c">{\bm{s}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.4.m4.1d">bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is derived by dividing the document according to the subtitles in the document. <math alttext="{\bm{s}}_{i}" class="ltx_Math" display="inline" id="S3.SS2.p3.5.m5.1"><semantics id="S3.SS2.p3.5.m5.1a"><msub id="S3.SS2.p3.5.m5.1.1" xref="S3.SS2.p3.5.m5.1.1.cmml"><mi id="S3.SS2.p3.5.m5.1.1.2" xref="S3.SS2.p3.5.m5.1.1.2.cmml">ğ’”</mi><mi id="S3.SS2.p3.5.m5.1.1.3" xref="S3.SS2.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.5.m5.1b"><apply id="S3.SS2.p3.5.m5.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.5.m5.1.1.1.cmml" xref="S3.SS2.p3.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p3.5.m5.1.1.2.cmml" xref="S3.SS2.p3.5.m5.1.1.2">ğ’”</ci><ci id="S3.SS2.p3.5.m5.1.1.3.cmml" xref="S3.SS2.p3.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.5.m5.1c">{\bm{s}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.5.m5.1d">bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> can contain a combination of text tokens <math alttext="\mathbf{L}_{\mathrm{S}i}" class="ltx_Math" display="inline" id="S3.SS2.p3.6.m6.1"><semantics id="S3.SS2.p3.6.m6.1a"><msub id="S3.SS2.p3.6.m6.1.1" xref="S3.SS2.p3.6.m6.1.1.cmml"><mi id="S3.SS2.p3.6.m6.1.1.2" xref="S3.SS2.p3.6.m6.1.1.2.cmml">ğ‹</mi><mrow id="S3.SS2.p3.6.m6.1.1.3" xref="S3.SS2.p3.6.m6.1.1.3.cmml"><mi id="S3.SS2.p3.6.m6.1.1.3.2" mathvariant="normal" xref="S3.SS2.p3.6.m6.1.1.3.2.cmml">S</mi><mo id="S3.SS2.p3.6.m6.1.1.3.1" xref="S3.SS2.p3.6.m6.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p3.6.m6.1.1.3.3" xref="S3.SS2.p3.6.m6.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.6.m6.1b"><apply id="S3.SS2.p3.6.m6.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.6.m6.1.1.1.cmml" xref="S3.SS2.p3.6.m6.1.1">subscript</csymbol><ci id="S3.SS2.p3.6.m6.1.1.2.cmml" xref="S3.SS2.p3.6.m6.1.1.2">ğ‹</ci><apply id="S3.SS2.p3.6.m6.1.1.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3"><times id="S3.SS2.p3.6.m6.1.1.3.1.cmml" xref="S3.SS2.p3.6.m6.1.1.3.1"></times><ci id="S3.SS2.p3.6.m6.1.1.3.2.cmml" xref="S3.SS2.p3.6.m6.1.1.3.2">S</ci><ci id="S3.SS2.p3.6.m6.1.1.3.3.cmml" xref="S3.SS2.p3.6.m6.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.6.m6.1c">\mathbf{L}_{\mathrm{S}i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.6.m6.1d">bold_L start_POSTSUBSCRIPT roman_S italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, visual tokens from embedded images <math alttext="\mathbf{V}_{\mathrm{S}i}" class="ltx_Math" display="inline" id="S3.SS2.p3.7.m7.1"><semantics id="S3.SS2.p3.7.m7.1a"><msub id="S3.SS2.p3.7.m7.1.1" xref="S3.SS2.p3.7.m7.1.1.cmml"><mi id="S3.SS2.p3.7.m7.1.1.2" xref="S3.SS2.p3.7.m7.1.1.2.cmml">ğ•</mi><mrow id="S3.SS2.p3.7.m7.1.1.3" xref="S3.SS2.p3.7.m7.1.1.3.cmml"><mi id="S3.SS2.p3.7.m7.1.1.3.2" mathvariant="normal" xref="S3.SS2.p3.7.m7.1.1.3.2.cmml">S</mi><mo id="S3.SS2.p3.7.m7.1.1.3.1" xref="S3.SS2.p3.7.m7.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p3.7.m7.1.1.3.3" xref="S3.SS2.p3.7.m7.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.7.m7.1b"><apply id="S3.SS2.p3.7.m7.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.7.m7.1.1.1.cmml" xref="S3.SS2.p3.7.m7.1.1">subscript</csymbol><ci id="S3.SS2.p3.7.m7.1.1.2.cmml" xref="S3.SS2.p3.7.m7.1.1.2">ğ•</ci><apply id="S3.SS2.p3.7.m7.1.1.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3"><times id="S3.SS2.p3.7.m7.1.1.3.1.cmml" xref="S3.SS2.p3.7.m7.1.1.3.1"></times><ci id="S3.SS2.p3.7.m7.1.1.3.2.cmml" xref="S3.SS2.p3.7.m7.1.1.3.2">S</ci><ci id="S3.SS2.p3.7.m7.1.1.3.3.cmml" xref="S3.SS2.p3.7.m7.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.7.m7.1c">\mathbf{V}_{\mathrm{S}i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.7.m7.1d">bold_V start_POSTSUBSCRIPT roman_S italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, and table tokens <math alttext="\mathbf{T}_{\mathrm{S}i}" class="ltx_Math" display="inline" id="S3.SS2.p3.8.m8.1"><semantics id="S3.SS2.p3.8.m8.1a"><msub id="S3.SS2.p3.8.m8.1.1" xref="S3.SS2.p3.8.m8.1.1.cmml"><mi id="S3.SS2.p3.8.m8.1.1.2" xref="S3.SS2.p3.8.m8.1.1.2.cmml">ğ“</mi><mrow id="S3.SS2.p3.8.m8.1.1.3" xref="S3.SS2.p3.8.m8.1.1.3.cmml"><mi id="S3.SS2.p3.8.m8.1.1.3.2" mathvariant="normal" xref="S3.SS2.p3.8.m8.1.1.3.2.cmml">S</mi><mo id="S3.SS2.p3.8.m8.1.1.3.1" xref="S3.SS2.p3.8.m8.1.1.3.1.cmml">â¢</mo><mi id="S3.SS2.p3.8.m8.1.1.3.3" xref="S3.SS2.p3.8.m8.1.1.3.3.cmml">i</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.8.m8.1b"><apply id="S3.SS2.p3.8.m8.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.8.m8.1.1.1.cmml" xref="S3.SS2.p3.8.m8.1.1">subscript</csymbol><ci id="S3.SS2.p3.8.m8.1.1.2.cmml" xref="S3.SS2.p3.8.m8.1.1.2">ğ“</ci><apply id="S3.SS2.p3.8.m8.1.1.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3"><times id="S3.SS2.p3.8.m8.1.1.3.1.cmml" xref="S3.SS2.p3.8.m8.1.1.3.1"></times><ci id="S3.SS2.p3.8.m8.1.1.3.2.cmml" xref="S3.SS2.p3.8.m8.1.1.3.2">S</ci><ci id="S3.SS2.p3.8.m8.1.1.3.3.cmml" xref="S3.SS2.p3.8.m8.1.1.3.3">ğ‘–</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.8.m8.1c">\mathbf{T}_{\mathrm{S}i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.8.m8.1d">bold_T start_POSTSUBSCRIPT roman_S italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, denoted as follows: <math alttext="{\bm{s}}_{i}\!=\!\left[\mathbf{V}_{\mathrm{S}_{i}},\;\mathbf{L}_{\mathrm{S}_{i%
}},\;\mathbf{T}_{\mathrm{S}_{i}}\right]" class="ltx_Math" display="inline" id="S3.SS2.p3.9.m9.3"><semantics id="S3.SS2.p3.9.m9.3a"><mrow id="S3.SS2.p3.9.m9.3.3" xref="S3.SS2.p3.9.m9.3.3.cmml"><msub id="S3.SS2.p3.9.m9.3.3.5" xref="S3.SS2.p3.9.m9.3.3.5.cmml"><mi id="S3.SS2.p3.9.m9.3.3.5.2" xref="S3.SS2.p3.9.m9.3.3.5.2.cmml">ğ’”</mi><mi id="S3.SS2.p3.9.m9.3.3.5.3" xref="S3.SS2.p3.9.m9.3.3.5.3.cmml">i</mi></msub><mo id="S3.SS2.p3.9.m9.3.3.4" lspace="0.108em" rspace="0.108em" xref="S3.SS2.p3.9.m9.3.3.4.cmml">=</mo><mrow id="S3.SS2.p3.9.m9.3.3.3.3" xref="S3.SS2.p3.9.m9.3.3.3.4.cmml"><mo id="S3.SS2.p3.9.m9.3.3.3.3.4" xref="S3.SS2.p3.9.m9.3.3.3.4.cmml">[</mo><msub id="S3.SS2.p3.9.m9.1.1.1.1.1" xref="S3.SS2.p3.9.m9.1.1.1.1.1.cmml"><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.2" xref="S3.SS2.p3.9.m9.1.1.1.1.1.2.cmml">ğ•</mi><msub id="S3.SS2.p3.9.m9.1.1.1.1.1.3" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.cmml"><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.3.2" mathvariant="normal" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.SS2.p3.9.m9.1.1.1.1.1.3.3" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo id="S3.SS2.p3.9.m9.3.3.3.3.5" rspace="0.447em" xref="S3.SS2.p3.9.m9.3.3.3.4.cmml">,</mo><msub id="S3.SS2.p3.9.m9.2.2.2.2.2" xref="S3.SS2.p3.9.m9.2.2.2.2.2.cmml"><mi id="S3.SS2.p3.9.m9.2.2.2.2.2.2" xref="S3.SS2.p3.9.m9.2.2.2.2.2.2.cmml">ğ‹</mi><msub id="S3.SS2.p3.9.m9.2.2.2.2.2.3" xref="S3.SS2.p3.9.m9.2.2.2.2.2.3.cmml"><mi id="S3.SS2.p3.9.m9.2.2.2.2.2.3.2" mathvariant="normal" xref="S3.SS2.p3.9.m9.2.2.2.2.2.3.2.cmml">S</mi><mi id="S3.SS2.p3.9.m9.2.2.2.2.2.3.3" xref="S3.SS2.p3.9.m9.2.2.2.2.2.3.3.cmml">i</mi></msub></msub><mo id="S3.SS2.p3.9.m9.3.3.3.3.6" rspace="0.447em" xref="S3.SS2.p3.9.m9.3.3.3.4.cmml">,</mo><msub id="S3.SS2.p3.9.m9.3.3.3.3.3" xref="S3.SS2.p3.9.m9.3.3.3.3.3.cmml"><mi id="S3.SS2.p3.9.m9.3.3.3.3.3.2" xref="S3.SS2.p3.9.m9.3.3.3.3.3.2.cmml">ğ“</mi><msub id="S3.SS2.p3.9.m9.3.3.3.3.3.3" xref="S3.SS2.p3.9.m9.3.3.3.3.3.3.cmml"><mi id="S3.SS2.p3.9.m9.3.3.3.3.3.3.2" mathvariant="normal" xref="S3.SS2.p3.9.m9.3.3.3.3.3.3.2.cmml">S</mi><mi id="S3.SS2.p3.9.m9.3.3.3.3.3.3.3" xref="S3.SS2.p3.9.m9.3.3.3.3.3.3.3.cmml">i</mi></msub></msub><mo id="S3.SS2.p3.9.m9.3.3.3.3.7" xref="S3.SS2.p3.9.m9.3.3.3.4.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.9.m9.3b"><apply id="S3.SS2.p3.9.m9.3.3.cmml" xref="S3.SS2.p3.9.m9.3.3"><eq id="S3.SS2.p3.9.m9.3.3.4.cmml" xref="S3.SS2.p3.9.m9.3.3.4"></eq><apply id="S3.SS2.p3.9.m9.3.3.5.cmml" xref="S3.SS2.p3.9.m9.3.3.5"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.3.3.5.1.cmml" xref="S3.SS2.p3.9.m9.3.3.5">subscript</csymbol><ci id="S3.SS2.p3.9.m9.3.3.5.2.cmml" xref="S3.SS2.p3.9.m9.3.3.5.2">ğ’”</ci><ci id="S3.SS2.p3.9.m9.3.3.5.3.cmml" xref="S3.SS2.p3.9.m9.3.3.5.3">ğ‘–</ci></apply><list id="S3.SS2.p3.9.m9.3.3.3.4.cmml" xref="S3.SS2.p3.9.m9.3.3.3.3"><apply id="S3.SS2.p3.9.m9.1.1.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.1.1.1.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.2.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.2">ğ•</ci><apply id="S3.SS2.p3.9.m9.1.1.1.1.1.3.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.1.1.1.1.1.3.1.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3">subscript</csymbol><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.3.2.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.2">S</ci><ci id="S3.SS2.p3.9.m9.1.1.1.1.1.3.3.cmml" xref="S3.SS2.p3.9.m9.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><apply id="S3.SS2.p3.9.m9.2.2.2.2.2.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.2.2.2.2.2.1.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.9.m9.2.2.2.2.2.2.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.2.2">ğ‹</ci><apply id="S3.SS2.p3.9.m9.2.2.2.2.2.3.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.2.2.2.2.2.3.1.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.2.3">subscript</csymbol><ci id="S3.SS2.p3.9.m9.2.2.2.2.2.3.2.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.2.3.2">S</ci><ci id="S3.SS2.p3.9.m9.2.2.2.2.2.3.3.cmml" xref="S3.SS2.p3.9.m9.2.2.2.2.2.3.3">ğ‘–</ci></apply></apply><apply id="S3.SS2.p3.9.m9.3.3.3.3.3.cmml" xref="S3.SS2.p3.9.m9.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.3.3.3.3.3.1.cmml" xref="S3.SS2.p3.9.m9.3.3.3.3.3">subscript</csymbol><ci id="S3.SS2.p3.9.m9.3.3.3.3.3.2.cmml" xref="S3.SS2.p3.9.m9.3.3.3.3.3.2">ğ“</ci><apply id="S3.SS2.p3.9.m9.3.3.3.3.3.3.cmml" xref="S3.SS2.p3.9.m9.3.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.9.m9.3.3.3.3.3.3.1.cmml" xref="S3.SS2.p3.9.m9.3.3.3.3.3.3">subscript</csymbol><ci id="S3.SS2.p3.9.m9.3.3.3.3.3.3.2.cmml" xref="S3.SS2.p3.9.m9.3.3.3.3.3.3.2">S</ci><ci id="S3.SS2.p3.9.m9.3.3.3.3.3.3.3.cmml" xref="S3.SS2.p3.9.m9.3.3.3.3.3.3.3">ğ‘–</ci></apply></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.9.m9.3c">{\bm{s}}_{i}\!=\!\left[\mathbf{V}_{\mathrm{S}_{i}},\;\mathbf{L}_{\mathrm{S}_{i%
}},\;\mathbf{T}_{\mathrm{S}_{i}}\right]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.9.m9.3d">bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = [ bold_V start_POSTSUBSCRIPT roman_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_L start_POSTSUBSCRIPT roman_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_T start_POSTSUBSCRIPT roman_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ]</annotation></semantics></math>. Then, to obtain a section-level representation, similar to the query representation, we introduce a learnable token, called â€˜End of Sectionâ€™: <math alttext="\texttt{[EoS]}\!\!\in\!\mathbb{R}^{d_{\text{emb}}}" class="ltx_Math" display="inline" id="S3.SS2.p3.10.m10.1"><semantics id="S3.SS2.p3.10.m10.1a"><mrow id="S3.SS2.p3.10.m10.1.1" xref="S3.SS2.p3.10.m10.1.1.cmml"><mpadded width="2.733em"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p3.10.m10.1.1.2" xref="S3.SS2.p3.10.m10.1.1.2a.cmml">[EoS]</mtext></mpadded><mo id="S3.SS2.p3.10.m10.1.1.1" rspace="0.108em" xref="S3.SS2.p3.10.m10.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p3.10.m10.1.1.3" xref="S3.SS2.p3.10.m10.1.1.3.cmml"><mi id="S3.SS2.p3.10.m10.1.1.3.2" xref="S3.SS2.p3.10.m10.1.1.3.2.cmml">â„</mi><msub id="S3.SS2.p3.10.m10.1.1.3.3" xref="S3.SS2.p3.10.m10.1.1.3.3.cmml"><mi id="S3.SS2.p3.10.m10.1.1.3.3.2" xref="S3.SS2.p3.10.m10.1.1.3.3.2.cmml">d</mi><mtext id="S3.SS2.p3.10.m10.1.1.3.3.3" xref="S3.SS2.p3.10.m10.1.1.3.3.3a.cmml">emb</mtext></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.10.m10.1b"><apply id="S3.SS2.p3.10.m10.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1"><in id="S3.SS2.p3.10.m10.1.1.1.cmml" xref="S3.SS2.p3.10.m10.1.1.1"></in><ci id="S3.SS2.p3.10.m10.1.1.2a.cmml" xref="S3.SS2.p3.10.m10.1.1.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p3.10.m10.1.1.2.cmml" xref="S3.SS2.p3.10.m10.1.1.2">[EoS]</mtext></ci><apply id="S3.SS2.p3.10.m10.1.1.3.cmml" xref="S3.SS2.p3.10.m10.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.1.1.3.1.cmml" xref="S3.SS2.p3.10.m10.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.10.m10.1.1.3.2.cmml" xref="S3.SS2.p3.10.m10.1.1.3.2">â„</ci><apply id="S3.SS2.p3.10.m10.1.1.3.3.cmml" xref="S3.SS2.p3.10.m10.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.10.m10.1.1.3.3.1.cmml" xref="S3.SS2.p3.10.m10.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p3.10.m10.1.1.3.3.2.cmml" xref="S3.SS2.p3.10.m10.1.1.3.3.2">ğ‘‘</ci><ci id="S3.SS2.p3.10.m10.1.1.3.3.3a.cmml" xref="S3.SS2.p3.10.m10.1.1.3.3.3"><mtext id="S3.SS2.p3.10.m10.1.1.3.3.3.cmml" mathsize="50%" xref="S3.SS2.p3.10.m10.1.1.3.3.3">emb</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.10.m10.1c">\texttt{[EoS]}\!\!\in\!\mathbb{R}^{d_{\text{emb}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.10.m10.1d">[EoS] âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>, which is similarly appended at the end of each section. We then forward the concatenated tokens <math alttext="\left[{\bm{s}}_{i},\;\texttt{[EoS]}\right]" class="ltx_Math" display="inline" id="S3.SS2.p3.11.m11.2"><semantics id="S3.SS2.p3.11.m11.2a"><mrow id="S3.SS2.p3.11.m11.2.2.1" xref="S3.SS2.p3.11.m11.2.2.2.cmml"><mo id="S3.SS2.p3.11.m11.2.2.1.2" xref="S3.SS2.p3.11.m11.2.2.2.cmml">[</mo><msub id="S3.SS2.p3.11.m11.2.2.1.1" xref="S3.SS2.p3.11.m11.2.2.1.1.cmml"><mi id="S3.SS2.p3.11.m11.2.2.1.1.2" xref="S3.SS2.p3.11.m11.2.2.1.1.2.cmml">ğ’”</mi><mi id="S3.SS2.p3.11.m11.2.2.1.1.3" xref="S3.SS2.p3.11.m11.2.2.1.1.3.cmml">i</mi></msub><mo id="S3.SS2.p3.11.m11.2.2.1.3" rspace="0.447em" xref="S3.SS2.p3.11.m11.2.2.2.cmml">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p3.11.m11.1.1" xref="S3.SS2.p3.11.m11.1.1a.cmml">[EoS]</mtext><mo id="S3.SS2.p3.11.m11.2.2.1.4" xref="S3.SS2.p3.11.m11.2.2.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.11.m11.2b"><interval closure="closed" id="S3.SS2.p3.11.m11.2.2.2.cmml" xref="S3.SS2.p3.11.m11.2.2.1"><apply id="S3.SS2.p3.11.m11.2.2.1.1.cmml" xref="S3.SS2.p3.11.m11.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.11.m11.2.2.1.1.1.cmml" xref="S3.SS2.p3.11.m11.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p3.11.m11.2.2.1.1.2.cmml" xref="S3.SS2.p3.11.m11.2.2.1.1.2">ğ’”</ci><ci id="S3.SS2.p3.11.m11.2.2.1.1.3.cmml" xref="S3.SS2.p3.11.m11.2.2.1.1.3">ğ‘–</ci></apply><ci id="S3.SS2.p3.11.m11.1.1a.cmml" xref="S3.SS2.p3.11.m11.1.1"><mtext class="ltx_mathvariant_monospace" id="S3.SS2.p3.11.m11.1.1.cmml" xref="S3.SS2.p3.11.m11.1.1">[EoS]</mtext></ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.11.m11.2c">\left[{\bm{s}}_{i},\;\texttt{[EoS]}\right]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.11.m11.2d">[ bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , [EoS] ]</annotation></semantics></math> to the section encoder, and, after that, the output corresponding to <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p3.13.1">[EoS]</span> is used to form the section representation, as follows: <math alttext="\mathbf{Z}_{\mathrm{S}_{i}}\!\in\!\mathbb{R}^{d_{\text{emb}}}" class="ltx_Math" display="inline" id="S3.SS2.p3.12.m12.1"><semantics id="S3.SS2.p3.12.m12.1a"><mrow id="S3.SS2.p3.12.m12.1.1" xref="S3.SS2.p3.12.m12.1.1.cmml"><msub id="S3.SS2.p3.12.m12.1.1.2" xref="S3.SS2.p3.12.m12.1.1.2.cmml"><mi id="S3.SS2.p3.12.m12.1.1.2.2" xref="S3.SS2.p3.12.m12.1.1.2.2.cmml">ğ™</mi><msub id="S3.SS2.p3.12.m12.1.1.2.3" xref="S3.SS2.p3.12.m12.1.1.2.3.cmml"><mi id="S3.SS2.p3.12.m12.1.1.2.3.2" mathvariant="normal" xref="S3.SS2.p3.12.m12.1.1.2.3.2.cmml">S</mi><mi id="S3.SS2.p3.12.m12.1.1.2.3.3" xref="S3.SS2.p3.12.m12.1.1.2.3.3.cmml">i</mi></msub></msub><mo id="S3.SS2.p3.12.m12.1.1.1" lspace="0.108em" rspace="0.108em" xref="S3.SS2.p3.12.m12.1.1.1.cmml">âˆˆ</mo><msup id="S3.SS2.p3.12.m12.1.1.3" xref="S3.SS2.p3.12.m12.1.1.3.cmml"><mi id="S3.SS2.p3.12.m12.1.1.3.2" xref="S3.SS2.p3.12.m12.1.1.3.2.cmml">â„</mi><msub id="S3.SS2.p3.12.m12.1.1.3.3" xref="S3.SS2.p3.12.m12.1.1.3.3.cmml"><mi id="S3.SS2.p3.12.m12.1.1.3.3.2" xref="S3.SS2.p3.12.m12.1.1.3.3.2.cmml">d</mi><mtext id="S3.SS2.p3.12.m12.1.1.3.3.3" xref="S3.SS2.p3.12.m12.1.1.3.3.3a.cmml">emb</mtext></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.12.m12.1b"><apply id="S3.SS2.p3.12.m12.1.1.cmml" xref="S3.SS2.p3.12.m12.1.1"><in id="S3.SS2.p3.12.m12.1.1.1.cmml" xref="S3.SS2.p3.12.m12.1.1.1"></in><apply id="S3.SS2.p3.12.m12.1.1.2.cmml" xref="S3.SS2.p3.12.m12.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.12.m12.1.1.2.1.cmml" xref="S3.SS2.p3.12.m12.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.12.m12.1.1.2.2.cmml" xref="S3.SS2.p3.12.m12.1.1.2.2">ğ™</ci><apply id="S3.SS2.p3.12.m12.1.1.2.3.cmml" xref="S3.SS2.p3.12.m12.1.1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.12.m12.1.1.2.3.1.cmml" xref="S3.SS2.p3.12.m12.1.1.2.3">subscript</csymbol><ci id="S3.SS2.p3.12.m12.1.1.2.3.2.cmml" xref="S3.SS2.p3.12.m12.1.1.2.3.2">S</ci><ci id="S3.SS2.p3.12.m12.1.1.2.3.3.cmml" xref="S3.SS2.p3.12.m12.1.1.2.3.3">ğ‘–</ci></apply></apply><apply id="S3.SS2.p3.12.m12.1.1.3.cmml" xref="S3.SS2.p3.12.m12.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p3.12.m12.1.1.3.1.cmml" xref="S3.SS2.p3.12.m12.1.1.3">superscript</csymbol><ci id="S3.SS2.p3.12.m12.1.1.3.2.cmml" xref="S3.SS2.p3.12.m12.1.1.3.2">â„</ci><apply id="S3.SS2.p3.12.m12.1.1.3.3.cmml" xref="S3.SS2.p3.12.m12.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS2.p3.12.m12.1.1.3.3.1.cmml" xref="S3.SS2.p3.12.m12.1.1.3.3">subscript</csymbol><ci id="S3.SS2.p3.12.m12.1.1.3.3.2.cmml" xref="S3.SS2.p3.12.m12.1.1.3.3.2">ğ‘‘</ci><ci id="S3.SS2.p3.12.m12.1.1.3.3.3a.cmml" xref="S3.SS2.p3.12.m12.1.1.3.3.3"><mtext id="S3.SS2.p3.12.m12.1.1.3.3.3.cmml" mathsize="50%" xref="S3.SS2.p3.12.m12.1.1.3.3.3">emb</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.12.m12.1c">\mathbf{Z}_{\mathrm{S}_{i}}\!\in\!\mathbb{R}^{d_{\text{emb}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.12.m12.1d">bold_Z start_POSTSUBSCRIPT roman_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT emb end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>. Additionally, the overall document representation is obtained by averaging the representations of all sections within the document, defined as follows: <math alttext="\mathbf{Z}_{\mathrm{D}}\!=\!\frac{1}{S}\!\sum_{i=1}^{S}\mathbf{Z}_{\mathrm{S}_%
{i}}" class="ltx_Math" display="inline" id="S3.SS2.p3.13.m13.1"><semantics id="S3.SS2.p3.13.m13.1a"><mrow id="S3.SS2.p3.13.m13.1.1" xref="S3.SS2.p3.13.m13.1.1.cmml"><msub id="S3.SS2.p3.13.m13.1.1.2" xref="S3.SS2.p3.13.m13.1.1.2.cmml"><mi id="S3.SS2.p3.13.m13.1.1.2.2" xref="S3.SS2.p3.13.m13.1.1.2.2.cmml">ğ™</mi><mi id="S3.SS2.p3.13.m13.1.1.2.3" mathvariant="normal" xref="S3.SS2.p3.13.m13.1.1.2.3.cmml">D</mi></msub><mo id="S3.SS2.p3.13.m13.1.1.1" lspace="0.108em" rspace="0.108em" xref="S3.SS2.p3.13.m13.1.1.1.cmml">=</mo><mrow id="S3.SS2.p3.13.m13.1.1.3" xref="S3.SS2.p3.13.m13.1.1.3.cmml"><mfrac id="S3.SS2.p3.13.m13.1.1.3.2" xref="S3.SS2.p3.13.m13.1.1.3.2.cmml"><mn id="S3.SS2.p3.13.m13.1.1.3.2.2" xref="S3.SS2.p3.13.m13.1.1.3.2.2.cmml">1</mn><mi id="S3.SS2.p3.13.m13.1.1.3.2.3" xref="S3.SS2.p3.13.m13.1.1.3.2.3.cmml">S</mi></mfrac><mo id="S3.SS2.p3.13.m13.1.1.3.1" xref="S3.SS2.p3.13.m13.1.1.3.1.cmml">â¢</mo><mrow id="S3.SS2.p3.13.m13.1.1.3.3" xref="S3.SS2.p3.13.m13.1.1.3.3.cmml"><msubsup id="S3.SS2.p3.13.m13.1.1.3.3.1" xref="S3.SS2.p3.13.m13.1.1.3.3.1.cmml"><mo id="S3.SS2.p3.13.m13.1.1.3.3.1.2.2" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.SS2.p3.13.m13.1.1.3.3.1.2.3" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.cmml"><mi id="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.2" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.2.cmml">i</mi><mo id="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.1" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.3" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.SS2.p3.13.m13.1.1.3.3.1.3" xref="S3.SS2.p3.13.m13.1.1.3.3.1.3.cmml">S</mi></msubsup><msub id="S3.SS2.p3.13.m13.1.1.3.3.2" xref="S3.SS2.p3.13.m13.1.1.3.3.2.cmml"><mi id="S3.SS2.p3.13.m13.1.1.3.3.2.2" xref="S3.SS2.p3.13.m13.1.1.3.3.2.2.cmml">ğ™</mi><msub id="S3.SS2.p3.13.m13.1.1.3.3.2.3" xref="S3.SS2.p3.13.m13.1.1.3.3.2.3.cmml"><mi id="S3.SS2.p3.13.m13.1.1.3.3.2.3.2" mathvariant="normal" xref="S3.SS2.p3.13.m13.1.1.3.3.2.3.2.cmml">S</mi><mi id="S3.SS2.p3.13.m13.1.1.3.3.2.3.3" xref="S3.SS2.p3.13.m13.1.1.3.3.2.3.3.cmml">i</mi></msub></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.13.m13.1b"><apply id="S3.SS2.p3.13.m13.1.1.cmml" xref="S3.SS2.p3.13.m13.1.1"><eq id="S3.SS2.p3.13.m13.1.1.1.cmml" xref="S3.SS2.p3.13.m13.1.1.1"></eq><apply id="S3.SS2.p3.13.m13.1.1.2.cmml" xref="S3.SS2.p3.13.m13.1.1.2"><csymbol cd="ambiguous" id="S3.SS2.p3.13.m13.1.1.2.1.cmml" xref="S3.SS2.p3.13.m13.1.1.2">subscript</csymbol><ci id="S3.SS2.p3.13.m13.1.1.2.2.cmml" xref="S3.SS2.p3.13.m13.1.1.2.2">ğ™</ci><ci id="S3.SS2.p3.13.m13.1.1.2.3.cmml" xref="S3.SS2.p3.13.m13.1.1.2.3">D</ci></apply><apply id="S3.SS2.p3.13.m13.1.1.3.cmml" xref="S3.SS2.p3.13.m13.1.1.3"><times id="S3.SS2.p3.13.m13.1.1.3.1.cmml" xref="S3.SS2.p3.13.m13.1.1.3.1"></times><apply id="S3.SS2.p3.13.m13.1.1.3.2.cmml" xref="S3.SS2.p3.13.m13.1.1.3.2"><divide id="S3.SS2.p3.13.m13.1.1.3.2.1.cmml" xref="S3.SS2.p3.13.m13.1.1.3.2"></divide><cn id="S3.SS2.p3.13.m13.1.1.3.2.2.cmml" type="integer" xref="S3.SS2.p3.13.m13.1.1.3.2.2">1</cn><ci id="S3.SS2.p3.13.m13.1.1.3.2.3.cmml" xref="S3.SS2.p3.13.m13.1.1.3.2.3">ğ‘†</ci></apply><apply id="S3.SS2.p3.13.m13.1.1.3.3.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3"><apply id="S3.SS2.p3.13.m13.1.1.3.3.1.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p3.13.m13.1.1.3.3.1.1.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1">superscript</csymbol><apply id="S3.SS2.p3.13.m13.1.1.3.3.1.2.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1"><csymbol cd="ambiguous" id="S3.SS2.p3.13.m13.1.1.3.3.1.2.1.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1">subscript</csymbol><sum id="S3.SS2.p3.13.m13.1.1.3.3.1.2.2.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.2"></sum><apply id="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.3"><eq id="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.1.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.1"></eq><ci id="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.2.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.2">ğ‘–</ci><cn id="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.3.cmml" type="integer" xref="S3.SS2.p3.13.m13.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S3.SS2.p3.13.m13.1.1.3.3.1.3.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.1.3">ğ‘†</ci></apply><apply id="S3.SS2.p3.13.m13.1.1.3.3.2.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.SS2.p3.13.m13.1.1.3.3.2.1.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.2">subscript</csymbol><ci id="S3.SS2.p3.13.m13.1.1.3.3.2.2.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.2.2">ğ™</ci><apply id="S3.SS2.p3.13.m13.1.1.3.3.2.3.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p3.13.m13.1.1.3.3.2.3.1.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.2.3">subscript</csymbol><ci id="S3.SS2.p3.13.m13.1.1.3.3.2.3.2.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.2.3.2">S</ci><ci id="S3.SS2.p3.13.m13.1.1.3.3.2.3.3.cmml" xref="S3.SS2.p3.13.m13.1.1.3.3.2.3.3">ğ‘–</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.13.m13.1c">\mathbf{Z}_{\mathrm{D}}\!=\!\frac{1}{S}\!\sum_{i=1}^{S}\mathbf{Z}_{\mathrm{S}_%
{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.13.m13.1d">bold_Z start_POSTSUBSCRIPT roman_D end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_S end_ARG âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT bold_Z start_POSTSUBSCRIPT roman_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.3">The remaining step to discuss here is how to train those two query and document retrievers for IR. Recall that the goal of the retriever is to assess a relevance score between the query and the document. To achieve this goal, we use a contrastive learning loss based upon the query and document representations, whose objective is to assign higher similarity scores to relevant documents (positive samples) and lower scores to irrelevant ones (negative samples) for the query, formulated as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.EGx1">
<tbody id="S3.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\mathcal{L}_{\text{retriever}}\!" class="ltx_Math" display="inline" id="S3.Ex1.m1.1"><semantics id="S3.Ex1.m1.1a"><msub id="S3.Ex1.m1.1.1" xref="S3.Ex1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.Ex1.m1.1.1.2" xref="S3.Ex1.m1.1.1.2.cmml">â„’</mi><mtext id="S3.Ex1.m1.1.1.3" xref="S3.Ex1.m1.1.1.3a.cmml">retriever</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.1b"><apply id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.1.1.1.cmml" xref="S3.Ex1.m1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.1.1.2.cmml" xref="S3.Ex1.m1.1.1.2">â„’</ci><ci id="S3.Ex1.m1.1.1.3a.cmml" xref="S3.Ex1.m1.1.1.3"><mtext id="S3.Ex1.m1.1.1.3.cmml" mathsize="70%" xref="S3.Ex1.m1.1.1.3">retriever</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.1c">\displaystyle\mathcal{L}_{\text{retriever}}\!</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.1d">caligraphic_L start_POSTSUBSCRIPT retriever end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\!-\frac{1}{B}\sum^{B}_{i=1}\log\left(\frac{\texttt{sim}(\mathbf%
{Z}_{\mathrm{Q}_{i}},\mathbf{Z}_{\mathrm{D}_{i}})}{\sum^{B}_{j=1}\texttt{sim}(%
\mathbf{Z}_{\mathrm{Q}_{i}},\mathbf{Z}_{\mathrm{D}_{j}})}\right),\;\;\texttt{%
sim}\left(\mathbf{Z}_{\mathrm{Q}},\;\mathbf{Z}_{\mathrm{D}}\right)\!=\!\frac{%
\mathbf{Z}_{\mathrm{Q}}^{\top}\mathbf{Z}_{\mathrm{D}}}{\lVert\mathbf{Z}_{%
\mathrm{Q}}\rVert\lVert\mathbf{Z}_{\mathrm{D}}\rVert}," class="ltx_Math" display="inline" id="S3.Ex1.m2.8"><semantics id="S3.Ex1.m2.8a"><mrow id="S3.Ex1.m2.8.8.1"><mrow id="S3.Ex1.m2.8.8.1.1.2" xref="S3.Ex1.m2.8.8.1.1.3.cmml"><mrow id="S3.Ex1.m2.8.8.1.1.1.1" xref="S3.Ex1.m2.8.8.1.1.1.1.cmml"><mi id="S3.Ex1.m2.8.8.1.1.1.1.2" xref="S3.Ex1.m2.8.8.1.1.1.1.2.cmml"></mi><mo id="S3.Ex1.m2.8.8.1.1.1.1.1" rspace="0.108em" xref="S3.Ex1.m2.8.8.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m2.8.8.1.1.1.1.3" xref="S3.Ex1.m2.8.8.1.1.1.1.3.cmml"><mo id="S3.Ex1.m2.8.8.1.1.1.1.3a" xref="S3.Ex1.m2.8.8.1.1.1.1.3.cmml">âˆ’</mo><mrow id="S3.Ex1.m2.8.8.1.1.1.1.3.2" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.cmml"><mstyle displaystyle="true" id="S3.Ex1.m2.8.8.1.1.1.1.3.2.2" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.cmml"><mfrac id="S3.Ex1.m2.8.8.1.1.1.1.3.2.2a" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.cmml"><mn id="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.2" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.2.cmml">1</mn><mi id="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.3" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.3.cmml">B</mi></mfrac></mstyle><mo id="S3.Ex1.m2.8.8.1.1.1.1.3.2.1" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.1.cmml">â¢</mo><mrow id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.cmml"><mstyle displaystyle="true" id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.cmml"><munderover id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1a" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.cmml"><mo id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.2" movablelimits="false" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.cmml"><mi id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.2" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.2.cmml">i</mi><mo id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.1" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.1.cmml">=</mo><mn id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.3" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.3.cmml">1</mn></mrow><mi id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.3" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.3.cmml">B</mi></munderover></mstyle><mrow id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.2" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.1.cmml"><mi id="S3.Ex1.m2.7.7" xref="S3.Ex1.m2.7.7.cmml">log</mi><mo id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.2a" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.1.cmml">â¡</mo><mrow id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.2.1" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.1.cmml"><mo id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.2.1.1" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.1.cmml">(</mo><mstyle displaystyle="true" id="S3.Ex1.m2.4.4" xref="S3.Ex1.m2.4.4.cmml"><mfrac id="S3.Ex1.m2.4.4a" xref="S3.Ex1.m2.4.4.cmml"><mrow id="S3.Ex1.m2.2.2.2" xref="S3.Ex1.m2.2.2.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m2.2.2.2.4" xref="S3.Ex1.m2.2.2.2.4a.cmml">sim</mtext><mo id="S3.Ex1.m2.2.2.2.3" xref="S3.Ex1.m2.2.2.2.3.cmml">â¢</mo><mrow id="S3.Ex1.m2.2.2.2.2.2" xref="S3.Ex1.m2.2.2.2.2.3.cmml"><mo id="S3.Ex1.m2.2.2.2.2.2.3" stretchy="false" xref="S3.Ex1.m2.2.2.2.2.3.cmml">(</mo><msub id="S3.Ex1.m2.1.1.1.1.1.1" xref="S3.Ex1.m2.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m2.1.1.1.1.1.1.2" xref="S3.Ex1.m2.1.1.1.1.1.1.2.cmml">ğ™</mi><msub id="S3.Ex1.m2.1.1.1.1.1.1.3" xref="S3.Ex1.m2.1.1.1.1.1.1.3.cmml"><mi id="S3.Ex1.m2.1.1.1.1.1.1.3.2" mathvariant="normal" xref="S3.Ex1.m2.1.1.1.1.1.1.3.2.cmml">Q</mi><mi id="S3.Ex1.m2.1.1.1.1.1.1.3.3" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo id="S3.Ex1.m2.2.2.2.2.2.4" xref="S3.Ex1.m2.2.2.2.2.3.cmml">,</mo><msub id="S3.Ex1.m2.2.2.2.2.2.2" xref="S3.Ex1.m2.2.2.2.2.2.2.cmml"><mi id="S3.Ex1.m2.2.2.2.2.2.2.2" xref="S3.Ex1.m2.2.2.2.2.2.2.2.cmml">ğ™</mi><msub id="S3.Ex1.m2.2.2.2.2.2.2.3" xref="S3.Ex1.m2.2.2.2.2.2.2.3.cmml"><mi id="S3.Ex1.m2.2.2.2.2.2.2.3.2" mathvariant="normal" xref="S3.Ex1.m2.2.2.2.2.2.2.3.2.cmml">D</mi><mi id="S3.Ex1.m2.2.2.2.2.2.2.3.3" xref="S3.Ex1.m2.2.2.2.2.2.2.3.3.cmml">i</mi></msub></msub><mo id="S3.Ex1.m2.2.2.2.2.2.5" stretchy="false" xref="S3.Ex1.m2.2.2.2.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.Ex1.m2.4.4.4" xref="S3.Ex1.m2.4.4.4.cmml"><msubsup id="S3.Ex1.m2.4.4.4.3" xref="S3.Ex1.m2.4.4.4.3.cmml"><mo id="S3.Ex1.m2.4.4.4.3.2.2" xref="S3.Ex1.m2.4.4.4.3.2.2.cmml">âˆ‘</mo><mrow id="S3.Ex1.m2.4.4.4.3.3" xref="S3.Ex1.m2.4.4.4.3.3.cmml"><mi id="S3.Ex1.m2.4.4.4.3.3.2" xref="S3.Ex1.m2.4.4.4.3.3.2.cmml">j</mi><mo id="S3.Ex1.m2.4.4.4.3.3.1" xref="S3.Ex1.m2.4.4.4.3.3.1.cmml">=</mo><mn id="S3.Ex1.m2.4.4.4.3.3.3" xref="S3.Ex1.m2.4.4.4.3.3.3.cmml">1</mn></mrow><mi id="S3.Ex1.m2.4.4.4.3.2.3" xref="S3.Ex1.m2.4.4.4.3.2.3.cmml">B</mi></msubsup><mrow id="S3.Ex1.m2.4.4.4.2" xref="S3.Ex1.m2.4.4.4.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m2.4.4.4.2.4" xref="S3.Ex1.m2.4.4.4.2.4a.cmml">sim</mtext><mo id="S3.Ex1.m2.4.4.4.2.3" xref="S3.Ex1.m2.4.4.4.2.3.cmml">â¢</mo><mrow id="S3.Ex1.m2.4.4.4.2.2.2" xref="S3.Ex1.m2.4.4.4.2.2.3.cmml"><mo id="S3.Ex1.m2.4.4.4.2.2.2.3" stretchy="false" xref="S3.Ex1.m2.4.4.4.2.2.3.cmml">(</mo><msub id="S3.Ex1.m2.3.3.3.1.1.1.1" xref="S3.Ex1.m2.3.3.3.1.1.1.1.cmml"><mi id="S3.Ex1.m2.3.3.3.1.1.1.1.2" xref="S3.Ex1.m2.3.3.3.1.1.1.1.2.cmml">ğ™</mi><msub id="S3.Ex1.m2.3.3.3.1.1.1.1.3" xref="S3.Ex1.m2.3.3.3.1.1.1.1.3.cmml"><mi id="S3.Ex1.m2.3.3.3.1.1.1.1.3.2" mathvariant="normal" xref="S3.Ex1.m2.3.3.3.1.1.1.1.3.2.cmml">Q</mi><mi id="S3.Ex1.m2.3.3.3.1.1.1.1.3.3" xref="S3.Ex1.m2.3.3.3.1.1.1.1.3.3.cmml">i</mi></msub></msub><mo id="S3.Ex1.m2.4.4.4.2.2.2.4" xref="S3.Ex1.m2.4.4.4.2.2.3.cmml">,</mo><msub id="S3.Ex1.m2.4.4.4.2.2.2.2" xref="S3.Ex1.m2.4.4.4.2.2.2.2.cmml"><mi id="S3.Ex1.m2.4.4.4.2.2.2.2.2" xref="S3.Ex1.m2.4.4.4.2.2.2.2.2.cmml">ğ™</mi><msub id="S3.Ex1.m2.4.4.4.2.2.2.2.3" xref="S3.Ex1.m2.4.4.4.2.2.2.2.3.cmml"><mi id="S3.Ex1.m2.4.4.4.2.2.2.2.3.2" mathvariant="normal" xref="S3.Ex1.m2.4.4.4.2.2.2.2.3.2.cmml">D</mi><mi id="S3.Ex1.m2.4.4.4.2.2.2.2.3.3" xref="S3.Ex1.m2.4.4.4.2.2.2.2.3.3.cmml">j</mi></msub></msub><mo id="S3.Ex1.m2.4.4.4.2.2.2.5" stretchy="false" xref="S3.Ex1.m2.4.4.4.2.2.3.cmml">)</mo></mrow></mrow></mrow></mfrac></mstyle><mo id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.2.1.2" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S3.Ex1.m2.8.8.1.1.2.3" rspace="0.727em" xref="S3.Ex1.m2.8.8.1.1.3a.cmml">,</mo><mrow id="S3.Ex1.m2.8.8.1.1.2.2" xref="S3.Ex1.m2.8.8.1.1.2.2.cmml"><mrow id="S3.Ex1.m2.8.8.1.1.2.2.2" xref="S3.Ex1.m2.8.8.1.1.2.2.2.cmml"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m2.8.8.1.1.2.2.2.4" xref="S3.Ex1.m2.8.8.1.1.2.2.2.4a.cmml">sim</mtext><mo id="S3.Ex1.m2.8.8.1.1.2.2.2.3" xref="S3.Ex1.m2.8.8.1.1.2.2.2.3.cmml">â¢</mo><mrow id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.3.cmml"><mo id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.3" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.3.cmml">(</mo><msub id="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1" xref="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.cmml"><mi id="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.2" xref="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.2.cmml">ğ™</mi><mi id="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.3" mathvariant="normal" xref="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.3.cmml">Q</mi></msub><mo id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.4" rspace="0.447em" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.3.cmml">,</mo><msub id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.cmml"><mi id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.2" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.2.cmml">ğ™</mi><mi id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.3" mathvariant="normal" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.3.cmml">D</mi></msub><mo id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.5" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.Ex1.m2.8.8.1.1.2.2.3" lspace="0.108em" rspace="0.108em" xref="S3.Ex1.m2.8.8.1.1.2.2.3.cmml">=</mo><mstyle displaystyle="true" id="S3.Ex1.m2.6.6" xref="S3.Ex1.m2.6.6.cmml"><mfrac id="S3.Ex1.m2.6.6a" xref="S3.Ex1.m2.6.6.cmml"><mrow id="S3.Ex1.m2.6.6.4" xref="S3.Ex1.m2.6.6.4.cmml"><msubsup id="S3.Ex1.m2.6.6.4.2" xref="S3.Ex1.m2.6.6.4.2.cmml"><mi id="S3.Ex1.m2.6.6.4.2.2.2" xref="S3.Ex1.m2.6.6.4.2.2.2.cmml">ğ™</mi><mi id="S3.Ex1.m2.6.6.4.2.2.3" mathvariant="normal" xref="S3.Ex1.m2.6.6.4.2.2.3.cmml">Q</mi><mo id="S3.Ex1.m2.6.6.4.2.3" xref="S3.Ex1.m2.6.6.4.2.3.cmml">âŠ¤</mo></msubsup><mo id="S3.Ex1.m2.6.6.4.1" xref="S3.Ex1.m2.6.6.4.1.cmml">â¢</mo><msub id="S3.Ex1.m2.6.6.4.3" xref="S3.Ex1.m2.6.6.4.3.cmml"><mi id="S3.Ex1.m2.6.6.4.3.2" xref="S3.Ex1.m2.6.6.4.3.2.cmml">ğ™</mi><mi id="S3.Ex1.m2.6.6.4.3.3" mathvariant="normal" xref="S3.Ex1.m2.6.6.4.3.3.cmml">D</mi></msub></mrow><mrow id="S3.Ex1.m2.6.6.2" xref="S3.Ex1.m2.6.6.2.cmml"><mrow id="S3.Ex1.m2.5.5.1.1.1" xref="S3.Ex1.m2.5.5.1.1.2.cmml"><mo fence="true" id="S3.Ex1.m2.5.5.1.1.1.2" rspace="0em" xref="S3.Ex1.m2.5.5.1.1.2.1.cmml">âˆ¥</mo><msub id="S3.Ex1.m2.5.5.1.1.1.1" xref="S3.Ex1.m2.5.5.1.1.1.1.cmml"><mi id="S3.Ex1.m2.5.5.1.1.1.1.2" xref="S3.Ex1.m2.5.5.1.1.1.1.2.cmml">ğ™</mi><mi id="S3.Ex1.m2.5.5.1.1.1.1.3" mathvariant="normal" xref="S3.Ex1.m2.5.5.1.1.1.1.3.cmml">Q</mi></msub><mo fence="true" id="S3.Ex1.m2.5.5.1.1.1.3" lspace="0em" xref="S3.Ex1.m2.5.5.1.1.2.1.cmml">âˆ¥</mo></mrow><mo id="S3.Ex1.m2.6.6.2.3" lspace="0em" xref="S3.Ex1.m2.6.6.2.3.cmml">â¢</mo><mrow id="S3.Ex1.m2.6.6.2.2.1" xref="S3.Ex1.m2.6.6.2.2.2.cmml"><mo fence="true" id="S3.Ex1.m2.6.6.2.2.1.2" rspace="0em" xref="S3.Ex1.m2.6.6.2.2.2.1.cmml">âˆ¥</mo><msub id="S3.Ex1.m2.6.6.2.2.1.1" xref="S3.Ex1.m2.6.6.2.2.1.1.cmml"><mi id="S3.Ex1.m2.6.6.2.2.1.1.2" xref="S3.Ex1.m2.6.6.2.2.1.1.2.cmml">ğ™</mi><mi id="S3.Ex1.m2.6.6.2.2.1.1.3" mathvariant="normal" xref="S3.Ex1.m2.6.6.2.2.1.1.3.cmml">D</mi></msub><mo fence="true" id="S3.Ex1.m2.6.6.2.2.1.3" lspace="0em" xref="S3.Ex1.m2.6.6.2.2.2.1.cmml">âˆ¥</mo></mrow></mrow></mfrac></mstyle></mrow></mrow><mo id="S3.Ex1.m2.8.8.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m2.8b"><apply id="S3.Ex1.m2.8.8.1.1.3.cmml" xref="S3.Ex1.m2.8.8.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.8.8.1.1.3a.cmml" xref="S3.Ex1.m2.8.8.1.1.2.3">formulae-sequence</csymbol><apply id="S3.Ex1.m2.8.8.1.1.1.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1"><eq id="S3.Ex1.m2.8.8.1.1.1.1.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.1"></eq><csymbol cd="latexml" id="S3.Ex1.m2.8.8.1.1.1.1.2.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.2">absent</csymbol><apply id="S3.Ex1.m2.8.8.1.1.1.1.3.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3"><minus id="S3.Ex1.m2.8.8.1.1.1.1.3.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3"></minus><apply id="S3.Ex1.m2.8.8.1.1.1.1.3.2.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2"><times id="S3.Ex1.m2.8.8.1.1.1.1.3.2.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.1"></times><apply id="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.2"><divide id="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.2"></divide><cn id="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.2.cmml" type="integer" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.2">1</cn><ci id="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.3.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.2.3">ğµ</ci></apply><apply id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3"><apply id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1">subscript</csymbol><apply id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1">superscript</csymbol><sum id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.2.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.2"></sum><ci id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.3.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.2.3">ğµ</ci></apply><apply id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3"><eq id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.1"></eq><ci id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.2.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.2">ğ‘–</ci><cn id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.3.cmml" type="integer" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.1.3.3">1</cn></apply></apply><apply id="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.1.cmml" xref="S3.Ex1.m2.8.8.1.1.1.1.3.2.3.2.2"><log id="S3.Ex1.m2.7.7.cmml" xref="S3.Ex1.m2.7.7"></log><apply id="S3.Ex1.m2.4.4.cmml" xref="S3.Ex1.m2.4.4"><divide id="S3.Ex1.m2.4.4.5.cmml" xref="S3.Ex1.m2.4.4"></divide><apply id="S3.Ex1.m2.2.2.2.cmml" xref="S3.Ex1.m2.2.2.2"><times id="S3.Ex1.m2.2.2.2.3.cmml" xref="S3.Ex1.m2.2.2.2.3"></times><ci id="S3.Ex1.m2.2.2.2.4a.cmml" xref="S3.Ex1.m2.2.2.2.4"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m2.2.2.2.4.cmml" xref="S3.Ex1.m2.2.2.2.4">sim</mtext></ci><interval closure="open" id="S3.Ex1.m2.2.2.2.2.3.cmml" xref="S3.Ex1.m2.2.2.2.2.2"><apply id="S3.Ex1.m2.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.2">ğ™</ci><apply id="S3.Ex1.m2.1.1.1.1.1.1.3.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.1.1.1.1.1.1.3.1.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m2.1.1.1.1.1.1.3.2.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3.2">Q</ci><ci id="S3.Ex1.m2.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m2.1.1.1.1.1.1.3.3">ğ‘–</ci></apply></apply><apply id="S3.Ex1.m2.2.2.2.2.2.2.cmml" xref="S3.Ex1.m2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.2.2.2.2.2.2.1.cmml" xref="S3.Ex1.m2.2.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m2.2.2.2.2.2.2.2.cmml" xref="S3.Ex1.m2.2.2.2.2.2.2.2">ğ™</ci><apply id="S3.Ex1.m2.2.2.2.2.2.2.3.cmml" xref="S3.Ex1.m2.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.2.2.2.2.2.2.3.1.cmml" xref="S3.Ex1.m2.2.2.2.2.2.2.3">subscript</csymbol><ci id="S3.Ex1.m2.2.2.2.2.2.2.3.2.cmml" xref="S3.Ex1.m2.2.2.2.2.2.2.3.2">D</ci><ci id="S3.Ex1.m2.2.2.2.2.2.2.3.3.cmml" xref="S3.Ex1.m2.2.2.2.2.2.2.3.3">ğ‘–</ci></apply></apply></interval></apply><apply id="S3.Ex1.m2.4.4.4.cmml" xref="S3.Ex1.m2.4.4.4"><apply id="S3.Ex1.m2.4.4.4.3.cmml" xref="S3.Ex1.m2.4.4.4.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.4.4.4.3.1.cmml" xref="S3.Ex1.m2.4.4.4.3">subscript</csymbol><apply id="S3.Ex1.m2.4.4.4.3.2.cmml" xref="S3.Ex1.m2.4.4.4.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.4.4.4.3.2.1.cmml" xref="S3.Ex1.m2.4.4.4.3">superscript</csymbol><sum id="S3.Ex1.m2.4.4.4.3.2.2.cmml" xref="S3.Ex1.m2.4.4.4.3.2.2"></sum><ci id="S3.Ex1.m2.4.4.4.3.2.3.cmml" xref="S3.Ex1.m2.4.4.4.3.2.3">ğµ</ci></apply><apply id="S3.Ex1.m2.4.4.4.3.3.cmml" xref="S3.Ex1.m2.4.4.4.3.3"><eq id="S3.Ex1.m2.4.4.4.3.3.1.cmml" xref="S3.Ex1.m2.4.4.4.3.3.1"></eq><ci id="S3.Ex1.m2.4.4.4.3.3.2.cmml" xref="S3.Ex1.m2.4.4.4.3.3.2">ğ‘—</ci><cn id="S3.Ex1.m2.4.4.4.3.3.3.cmml" type="integer" xref="S3.Ex1.m2.4.4.4.3.3.3">1</cn></apply></apply><apply id="S3.Ex1.m2.4.4.4.2.cmml" xref="S3.Ex1.m2.4.4.4.2"><times id="S3.Ex1.m2.4.4.4.2.3.cmml" xref="S3.Ex1.m2.4.4.4.2.3"></times><ci id="S3.Ex1.m2.4.4.4.2.4a.cmml" xref="S3.Ex1.m2.4.4.4.2.4"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m2.4.4.4.2.4.cmml" xref="S3.Ex1.m2.4.4.4.2.4">sim</mtext></ci><interval closure="open" id="S3.Ex1.m2.4.4.4.2.2.3.cmml" xref="S3.Ex1.m2.4.4.4.2.2.2"><apply id="S3.Ex1.m2.3.3.3.1.1.1.1.cmml" xref="S3.Ex1.m2.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.3.3.3.1.1.1.1.1.cmml" xref="S3.Ex1.m2.3.3.3.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m2.3.3.3.1.1.1.1.2.cmml" xref="S3.Ex1.m2.3.3.3.1.1.1.1.2">ğ™</ci><apply id="S3.Ex1.m2.3.3.3.1.1.1.1.3.cmml" xref="S3.Ex1.m2.3.3.3.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.3.3.3.1.1.1.1.3.1.cmml" xref="S3.Ex1.m2.3.3.3.1.1.1.1.3">subscript</csymbol><ci id="S3.Ex1.m2.3.3.3.1.1.1.1.3.2.cmml" xref="S3.Ex1.m2.3.3.3.1.1.1.1.3.2">Q</ci><ci id="S3.Ex1.m2.3.3.3.1.1.1.1.3.3.cmml" xref="S3.Ex1.m2.3.3.3.1.1.1.1.3.3">ğ‘–</ci></apply></apply><apply id="S3.Ex1.m2.4.4.4.2.2.2.2.cmml" xref="S3.Ex1.m2.4.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.4.4.4.2.2.2.2.1.cmml" xref="S3.Ex1.m2.4.4.4.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m2.4.4.4.2.2.2.2.2.cmml" xref="S3.Ex1.m2.4.4.4.2.2.2.2.2">ğ™</ci><apply id="S3.Ex1.m2.4.4.4.2.2.2.2.3.cmml" xref="S3.Ex1.m2.4.4.4.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.4.4.4.2.2.2.2.3.1.cmml" xref="S3.Ex1.m2.4.4.4.2.2.2.2.3">subscript</csymbol><ci id="S3.Ex1.m2.4.4.4.2.2.2.2.3.2.cmml" xref="S3.Ex1.m2.4.4.4.2.2.2.2.3.2">D</ci><ci id="S3.Ex1.m2.4.4.4.2.2.2.2.3.3.cmml" xref="S3.Ex1.m2.4.4.4.2.2.2.2.3.3">ğ‘—</ci></apply></apply></interval></apply></apply></apply></apply></apply></apply></apply></apply><apply id="S3.Ex1.m2.8.8.1.1.2.2.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2"><eq id="S3.Ex1.m2.8.8.1.1.2.2.3.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.3"></eq><apply id="S3.Ex1.m2.8.8.1.1.2.2.2.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2"><times id="S3.Ex1.m2.8.8.1.1.2.2.2.3.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2.3"></times><ci id="S3.Ex1.m2.8.8.1.1.2.2.2.4a.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2.4"><mtext class="ltx_mathvariant_monospace" id="S3.Ex1.m2.8.8.1.1.2.2.2.4.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2.4">sim</mtext></ci><interval closure="open" id="S3.Ex1.m2.8.8.1.1.2.2.2.2.3.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.2"><apply id="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.1.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.2.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.2">ğ™</ci><ci id="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.3.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.1.1.1.1.3">Q</ci></apply><apply id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.1.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2">subscript</csymbol><ci id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.2.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.2">ğ™</ci><ci id="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.3.cmml" xref="S3.Ex1.m2.8.8.1.1.2.2.2.2.2.2.3">D</ci></apply></interval></apply><apply id="S3.Ex1.m2.6.6.cmml" xref="S3.Ex1.m2.6.6"><divide id="S3.Ex1.m2.6.6.3.cmml" xref="S3.Ex1.m2.6.6"></divide><apply id="S3.Ex1.m2.6.6.4.cmml" xref="S3.Ex1.m2.6.6.4"><times id="S3.Ex1.m2.6.6.4.1.cmml" xref="S3.Ex1.m2.6.6.4.1"></times><apply id="S3.Ex1.m2.6.6.4.2.cmml" xref="S3.Ex1.m2.6.6.4.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.6.6.4.2.1.cmml" xref="S3.Ex1.m2.6.6.4.2">superscript</csymbol><apply id="S3.Ex1.m2.6.6.4.2.2.cmml" xref="S3.Ex1.m2.6.6.4.2"><csymbol cd="ambiguous" id="S3.Ex1.m2.6.6.4.2.2.1.cmml" xref="S3.Ex1.m2.6.6.4.2">subscript</csymbol><ci id="S3.Ex1.m2.6.6.4.2.2.2.cmml" xref="S3.Ex1.m2.6.6.4.2.2.2">ğ™</ci><ci id="S3.Ex1.m2.6.6.4.2.2.3.cmml" xref="S3.Ex1.m2.6.6.4.2.2.3">Q</ci></apply><csymbol cd="latexml" id="S3.Ex1.m2.6.6.4.2.3.cmml" xref="S3.Ex1.m2.6.6.4.2.3">top</csymbol></apply><apply id="S3.Ex1.m2.6.6.4.3.cmml" xref="S3.Ex1.m2.6.6.4.3"><csymbol cd="ambiguous" id="S3.Ex1.m2.6.6.4.3.1.cmml" xref="S3.Ex1.m2.6.6.4.3">subscript</csymbol><ci id="S3.Ex1.m2.6.6.4.3.2.cmml" xref="S3.Ex1.m2.6.6.4.3.2">ğ™</ci><ci id="S3.Ex1.m2.6.6.4.3.3.cmml" xref="S3.Ex1.m2.6.6.4.3.3">D</ci></apply></apply><apply id="S3.Ex1.m2.6.6.2.cmml" xref="S3.Ex1.m2.6.6.2"><times id="S3.Ex1.m2.6.6.2.3.cmml" xref="S3.Ex1.m2.6.6.2.3"></times><apply id="S3.Ex1.m2.5.5.1.1.2.cmml" xref="S3.Ex1.m2.5.5.1.1.1"><csymbol cd="latexml" id="S3.Ex1.m2.5.5.1.1.2.1.cmml" xref="S3.Ex1.m2.5.5.1.1.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S3.Ex1.m2.5.5.1.1.1.1.cmml" xref="S3.Ex1.m2.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.5.5.1.1.1.1.1.cmml" xref="S3.Ex1.m2.5.5.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m2.5.5.1.1.1.1.2.cmml" xref="S3.Ex1.m2.5.5.1.1.1.1.2">ğ™</ci><ci id="S3.Ex1.m2.5.5.1.1.1.1.3.cmml" xref="S3.Ex1.m2.5.5.1.1.1.1.3">Q</ci></apply></apply><apply id="S3.Ex1.m2.6.6.2.2.2.cmml" xref="S3.Ex1.m2.6.6.2.2.1"><csymbol cd="latexml" id="S3.Ex1.m2.6.6.2.2.2.1.cmml" xref="S3.Ex1.m2.6.6.2.2.1.2">delimited-âˆ¥âˆ¥</csymbol><apply id="S3.Ex1.m2.6.6.2.2.1.1.cmml" xref="S3.Ex1.m2.6.6.2.2.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m2.6.6.2.2.1.1.1.cmml" xref="S3.Ex1.m2.6.6.2.2.1.1">subscript</csymbol><ci id="S3.Ex1.m2.6.6.2.2.1.1.2.cmml" xref="S3.Ex1.m2.6.6.2.2.1.1.2">ğ™</ci><ci id="S3.Ex1.m2.6.6.2.2.1.1.3.cmml" xref="S3.Ex1.m2.6.6.2.2.1.1.3">D</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m2.8c">\displaystyle=\!-\frac{1}{B}\sum^{B}_{i=1}\log\left(\frac{\texttt{sim}(\mathbf%
{Z}_{\mathrm{Q}_{i}},\mathbf{Z}_{\mathrm{D}_{i}})}{\sum^{B}_{j=1}\texttt{sim}(%
\mathbf{Z}_{\mathrm{Q}_{i}},\mathbf{Z}_{\mathrm{D}_{j}})}\right),\;\;\texttt{%
sim}\left(\mathbf{Z}_{\mathrm{Q}},\;\mathbf{Z}_{\mathrm{D}}\right)\!=\!\frac{%
\mathbf{Z}_{\mathrm{Q}}^{\top}\mathbf{Z}_{\mathrm{D}}}{\lVert\mathbf{Z}_{%
\mathrm{Q}}\rVert\lVert\mathbf{Z}_{\mathrm{D}}\rVert},</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m2.8d">= - divide start_ARG 1 end_ARG start_ARG italic_B end_ARG âˆ‘ start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT roman_log ( divide start_ARG sim ( bold_Z start_POSTSUBSCRIPT roman_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_Z start_POSTSUBSCRIPT roman_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) end_ARG start_ARG âˆ‘ start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT sim ( bold_Z start_POSTSUBSCRIPT roman_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT , bold_Z start_POSTSUBSCRIPT roman_D start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT end_POSTSUBSCRIPT ) end_ARG ) , sim ( bold_Z start_POSTSUBSCRIPT roman_Q end_POSTSUBSCRIPT , bold_Z start_POSTSUBSCRIPT roman_D end_POSTSUBSCRIPT ) = divide start_ARG bold_Z start_POSTSUBSCRIPT roman_Q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âŠ¤ end_POSTSUPERSCRIPT bold_Z start_POSTSUBSCRIPT roman_D end_POSTSUBSCRIPT end_ARG start_ARG âˆ¥ bold_Z start_POSTSUBSCRIPT roman_Q end_POSTSUBSCRIPT âˆ¥ âˆ¥ bold_Z start_POSTSUBSCRIPT roman_D end_POSTSUBSCRIPT âˆ¥ end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p4.2">where <math alttext="B" class="ltx_Math" display="inline" id="S3.SS2.p4.1.m1.1"><semantics id="S3.SS2.p4.1.m1.1a"><mi id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><ci id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.m1.1d">italic_B</annotation></semantics></math> is the batch size during the training phase. Here, by minimizing <math alttext="\mathcal{L}_{\text{retriever}}" class="ltx_Math" display="inline" id="S3.SS2.p4.2.m2.1"><semantics id="S3.SS2.p4.2.m2.1a"><msub id="S3.SS2.p4.2.m2.1.1" xref="S3.SS2.p4.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p4.2.m2.1.1.2" xref="S3.SS2.p4.2.m2.1.1.2.cmml">â„’</mi><mtext id="S3.SS2.p4.2.m2.1.1.3" xref="S3.SS2.p4.2.m2.1.1.3a.cmml">retriever</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.2.m2.1b"><apply id="S3.SS2.p4.2.m2.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p4.2.m2.1.1.1.cmml" xref="S3.SS2.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p4.2.m2.1.1.2.cmml" xref="S3.SS2.p4.2.m2.1.1.2">â„’</ci><ci id="S3.SS2.p4.2.m2.1.1.3a.cmml" xref="S3.SS2.p4.2.m2.1.1.3"><mtext id="S3.SS2.p4.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS2.p4.2.m2.1.1.3">retriever</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.2.m2.1c">\mathcal{L}_{\text{retriever}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.2.m2.1d">caligraphic_L start_POSTSUBSCRIPT retriever end_POSTSUBSCRIPT</annotation></semantics></math>, the retriever learns to optimize the similarity between queries and their relevant documents, enabling the retrieval of the most pertinent documents (among all) for the given input query during inference.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Reranker</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="374" id="S3.F2.g1" src="x2.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Overview of IDentIfy. <span class="ltx_text ltx_font_bold" id="S3.F2.6.1">(a)</span>: In our document retriever, a query encoder represents a query (<span class="ltx_text" id="S3.F2.7.2" style="color:#800080;">purple</span>), and sections are encoded with a section encoder whose embeddings averaged to form a document representation (<span class="ltx_text" id="S3.F2.8.3" style="color:#0000FF;">blue</span>). Contrastive learning loss (<span class="ltx_text" id="S3.F2.9.4" style="color:#FF0000;">red</span>) is used for training the document retriever. <span class="ltx_text ltx_font_bold" id="S3.F2.10.5">(b)</span>: Reranker scores query-section relevance with the concatenation of the query and section, trained using Binary Cross-Entropy loss.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">To enable fine-grained retrieval within documents beyond the retrieval of documents themselves, we introduce a section-level reranking mechanism that identifies the section most relevant to the input query. In particular, once the document is retrieved, the objective of the reranker <math alttext="f_{\mathrm{R}}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">f</mi><mi id="S3.SS3.p1.1.m1.1.1.3" mathvariant="normal" xref="S3.SS3.p1.1.m1.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">ğ‘“</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">R</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">f_{\mathrm{R}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_f start_POSTSUBSCRIPT roman_R end_POSTSUBSCRIPT</annotation></semantics></math> is to pinpoint the specific sections within the document that best match the query. We also note that this reranker is similarly operationalized with a single VLM along with a binary classifier on top of it, which directly measures the relevance of each query-section pair, illustrated in inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S3.F2" title="In 3.3 Reranker â€£ 3 Method â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> <span class="ltx_text" id="S3.SS3.p1.1.1" style="color:#DB143D;">(b)</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.3">Formally, for a retrieved document, we take each of its sections <math alttext="{\bm{s}}_{i}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">ğ’”</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">ğ’”</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">{\bm{s}}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and concatenate it with the query <math alttext="{\bm{q}}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mi id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml">ğ’’</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><ci id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">ğ’’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">{\bm{q}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">bold_italic_q</annotation></semantics></math> and a learnable token for section embedding <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.3.1">[EoS]</span>, forming the input sequence of <math alttext="\left[{\bm{q}},\;{\bm{s}}_{i},\;\texttt{[EoS]}\right]" class="ltx_Math" display="inline" id="S3.SS3.p2.3.m3.3"><semantics id="S3.SS3.p2.3.m3.3a"><mrow id="S3.SS3.p2.3.m3.3.3.1" xref="S3.SS3.p2.3.m3.3.3.2.cmml"><mo id="S3.SS3.p2.3.m3.3.3.1.2" xref="S3.SS3.p2.3.m3.3.3.2.cmml">[</mo><mi id="S3.SS3.p2.3.m3.1.1" xref="S3.SS3.p2.3.m3.1.1.cmml">ğ’’</mi><mo id="S3.SS3.p2.3.m3.3.3.1.3" rspace="0.447em" xref="S3.SS3.p2.3.m3.3.3.2.cmml">,</mo><msub id="S3.SS3.p2.3.m3.3.3.1.1" xref="S3.SS3.p2.3.m3.3.3.1.1.cmml"><mi id="S3.SS3.p2.3.m3.3.3.1.1.2" xref="S3.SS3.p2.3.m3.3.3.1.1.2.cmml">ğ’”</mi><mi id="S3.SS3.p2.3.m3.3.3.1.1.3" xref="S3.SS3.p2.3.m3.3.3.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.p2.3.m3.3.3.1.4" rspace="0.447em" xref="S3.SS3.p2.3.m3.3.3.2.cmml">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.SS3.p2.3.m3.2.2" xref="S3.SS3.p2.3.m3.2.2a.cmml">[EoS]</mtext><mo id="S3.SS3.p2.3.m3.3.3.1.5" xref="S3.SS3.p2.3.m3.3.3.2.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.3.m3.3b"><list id="S3.SS3.p2.3.m3.3.3.2.cmml" xref="S3.SS3.p2.3.m3.3.3.1"><ci id="S3.SS3.p2.3.m3.1.1.cmml" xref="S3.SS3.p2.3.m3.1.1">ğ’’</ci><apply id="S3.SS3.p2.3.m3.3.3.1.1.cmml" xref="S3.SS3.p2.3.m3.3.3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.3.m3.3.3.1.1.1.cmml" xref="S3.SS3.p2.3.m3.3.3.1.1">subscript</csymbol><ci id="S3.SS3.p2.3.m3.3.3.1.1.2.cmml" xref="S3.SS3.p2.3.m3.3.3.1.1.2">ğ’”</ci><ci id="S3.SS3.p2.3.m3.3.3.1.1.3.cmml" xref="S3.SS3.p2.3.m3.3.3.1.1.3">ğ‘–</ci></apply><ci id="S3.SS3.p2.3.m3.2.2a.cmml" xref="S3.SS3.p2.3.m3.2.2"><mtext class="ltx_mathvariant_monospace" id="S3.SS3.p2.3.m3.2.2.cmml" xref="S3.SS3.p2.3.m3.2.2">[EoS]</mtext></ci></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.3.m3.3c">\left[{\bm{q}},\;{\bm{s}}_{i},\;\texttt{[EoS]}\right]</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.3.m3.3d">[ bold_italic_q , bold_italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , [EoS] ]</annotation></semantics></math>. The concatenated tokens are then processed through the reranker, and the model output corresponding to <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p2.3.2">[EoS]</span> captures the relevant between the query and section, which is further subsequently passed to a binary classifier consisting of a linear layer followed by a Sigmoid function. Through this process, the classifier outputs a probability score indicating the likelihood of the section being relevant to the query, i.e., a score close to one denotes a high relevance (positive section), meanwhile, a score near zero indicates irrelevance (negative section).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.10">To train this reranker, we use the Binary Cross-Entropy (BCE) loss, formalized as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="A2.EGx2">
<tbody id="S3.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_center ltx_eqn_cell" colspan="2"><math alttext="\displaystyle\begin{split}\mathcal{L}_{\text{reranker}}\!=\!\sum_{i=1}^{B}\sum%
_{j=1}^{S_{i}}\!\frac{1}{BS_{i}}\ell\left(\mathbf{y}_{(\mathrm{S}_{i,j})},\;f_%
{\mathrm{R}}\left(\left[{\bm{q}},\;{\bm{s}}_{i,j},\;\texttt{[EoS]}\right]%
\right)\right),\;\ell\left(y,\hat{y}\right)\!=\!-\left[y\log\hat{y}\!+\!(1\!-%
\!y)\log(1\!-\!\hat{y})\right],\end{split}" class="ltx_Math" display="inline" id="S3.Ex2.m1.57"><semantics id="S3.Ex2.m1.57a"><mtable id="S3.Ex2.m1.57.57.2"><mtr id="S3.Ex2.m1.57.57.2a"><mtd class="ltx_align_right" columnalign="right" id="S3.Ex2.m1.57.57.2b"><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56"><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1"><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1"><msub id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.3"><mi class="ltx_font_mathcaligraphic" id="S3.Ex2.m1.1.1.1.1.1.1" xref="S3.Ex2.m1.1.1.1.1.1.1.cmml">â„’</mi><mtext id="S3.Ex2.m1.2.2.2.2.2.2.1" xref="S3.Ex2.m1.2.2.2.2.2.2.1a.cmml">reranker</mtext></msub><mo id="S3.Ex2.m1.3.3.3.3.3.3" lspace="0.108em" rspace="0.108em" xref="S3.Ex2.m1.3.3.3.3.3.3.cmml">=</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2"><mstyle displaystyle="true" id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.3"><munderover id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.3a"><mo id="S3.Ex2.m1.4.4.4.4.4.4" movablelimits="false" xref="S3.Ex2.m1.4.4.4.4.4.4.cmml">âˆ‘</mo><mrow id="S3.Ex2.m1.5.5.5.5.5.5.1" xref="S3.Ex2.m1.5.5.5.5.5.5.1.cmml"><mi id="S3.Ex2.m1.5.5.5.5.5.5.1.2" xref="S3.Ex2.m1.5.5.5.5.5.5.1.2.cmml">i</mi><mo id="S3.Ex2.m1.5.5.5.5.5.5.1.1" xref="S3.Ex2.m1.5.5.5.5.5.5.1.1.cmml">=</mo><mn id="S3.Ex2.m1.5.5.5.5.5.5.1.3" xref="S3.Ex2.m1.5.5.5.5.5.5.1.3.cmml">1</mn></mrow><mi id="S3.Ex2.m1.6.6.6.6.6.6.1" xref="S3.Ex2.m1.6.6.6.6.6.6.1.cmml">B</mi></munderover></mstyle><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2"><mstyle displaystyle="true" id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.3"><munderover id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.3a"><mo id="S3.Ex2.m1.7.7.7.7.7.7" movablelimits="false" xref="S3.Ex2.m1.7.7.7.7.7.7.cmml">âˆ‘</mo><mrow id="S3.Ex2.m1.8.8.8.8.8.8.1" xref="S3.Ex2.m1.8.8.8.8.8.8.1.cmml"><mi id="S3.Ex2.m1.8.8.8.8.8.8.1.2" xref="S3.Ex2.m1.8.8.8.8.8.8.1.2.cmml">j</mi><mo id="S3.Ex2.m1.8.8.8.8.8.8.1.1" xref="S3.Ex2.m1.8.8.8.8.8.8.1.1.cmml">=</mo><mn id="S3.Ex2.m1.8.8.8.8.8.8.1.3" xref="S3.Ex2.m1.8.8.8.8.8.8.1.3.cmml">1</mn></mrow><msub id="S3.Ex2.m1.9.9.9.9.9.9.1" xref="S3.Ex2.m1.9.9.9.9.9.9.1.cmml"><mi id="S3.Ex2.m1.9.9.9.9.9.9.1.2" xref="S3.Ex2.m1.9.9.9.9.9.9.1.2.cmml">S</mi><mi id="S3.Ex2.m1.9.9.9.9.9.9.1.3" xref="S3.Ex2.m1.9.9.9.9.9.9.1.3.cmml">i</mi></msub></munderover></mstyle><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2"><mstyle displaystyle="true" id="S3.Ex2.m1.10.10.10.10.10.10" xref="S3.Ex2.m1.10.10.10.10.10.10.cmml"><mfrac id="S3.Ex2.m1.10.10.10.10.10.10a" xref="S3.Ex2.m1.10.10.10.10.10.10.cmml"><mn id="S3.Ex2.m1.10.10.10.10.10.10.2" xref="S3.Ex2.m1.10.10.10.10.10.10.2.cmml">1</mn><mrow id="S3.Ex2.m1.10.10.10.10.10.10.3" xref="S3.Ex2.m1.10.10.10.10.10.10.3.cmml"><mi id="S3.Ex2.m1.10.10.10.10.10.10.3.2" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2.cmml">B</mi><mo id="S3.Ex2.m1.10.10.10.10.10.10.3.1" xref="S3.Ex2.m1.10.10.10.10.10.10.3.1.cmml">â¢</mo><msub id="S3.Ex2.m1.10.10.10.10.10.10.3.3" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3.cmml"><mi id="S3.Ex2.m1.10.10.10.10.10.10.3.3.2" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3.2.cmml">S</mi><mi id="S3.Ex2.m1.10.10.10.10.10.10.3.3.3" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3.3.cmml">i</mi></msub></mrow></mfrac></mstyle><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.3">â¢</mo><mi id="S3.Ex2.m1.11.11.11.11.11.11" mathvariant="normal" xref="S3.Ex2.m1.11.11.11.11.11.11.cmml">â„“</mi><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.3a">â¢</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.2.2"><mo id="S3.Ex2.m1.12.12.12.12.12.12">(</mo><msub id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.1.1.1.1.1.1"><mi id="S3.Ex2.m1.13.13.13.13.13.13" xref="S3.Ex2.m1.13.13.13.13.13.13.cmml">ğ²</mi><mrow id="S3.Ex2.m1.14.14.14.14.14.14.1.3" xref="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.cmml"><mo id="S3.Ex2.m1.14.14.14.14.14.14.1.3.2" stretchy="false" xref="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.cmml">(</mo><msub id="S3.Ex2.m1.14.14.14.14.14.14.1.3.1" xref="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.cmml"><mi id="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.2" mathvariant="normal" xref="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.2.cmml">S</mi><mrow id="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.4" xref="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.3.cmml"><mi id="S3.Ex2.m1.14.14.14.14.14.14.1.1.1.1" xref="S3.Ex2.m1.14.14.14.14.14.14.1.1.1.1.cmml">i</mi><mo id="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.4.1" xref="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.3.cmml">,</mo><mi id="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.2" xref="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.Ex2.m1.14.14.14.14.14.14.1.3.3" stretchy="false" xref="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.cmml">)</mo></mrow></msub><mo id="S3.Ex2.m1.15.15.15.15.15.15" rspace="0.447em">,</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.2.2.2"><msub id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.2.2.2.3"><mi id="S3.Ex2.m1.16.16.16.16.16.16" xref="S3.Ex2.m1.16.16.16.16.16.16.cmml">f</mi><mi id="S3.Ex2.m1.17.17.17.17.17.17.1" mathvariant="normal" xref="S3.Ex2.m1.17.17.17.17.17.17.1.cmml">R</mi></msub><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.2.2.2.2">â¢</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.2.2.2.1.1"><mo id="S3.Ex2.m1.18.18.18.18.18.18">(</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.2.2.2.1.1.1"><mo id="S3.Ex2.m1.19.19.19.19.19.19">[</mo><mi id="S3.Ex2.m1.20.20.20.20.20.20" xref="S3.Ex2.m1.20.20.20.20.20.20.cmml">ğ’’</mi><mo id="S3.Ex2.m1.21.21.21.21.21.21" rspace="0.447em">,</mo><msub id="S3.Ex2.m1.57.57.2.56.56.56.56.1.1.1.2.2.2.2.2.2.1.1.1.1.1"><mi id="S3.Ex2.m1.22.22.22.22.22.22" xref="S3.Ex2.m1.22.22.22.22.22.22.cmml">ğ’”</mi><mrow id="S3.Ex2.m1.23.23.23.23.23.23.1.4" xref="S3.Ex2.m1.23.23.23.23.23.23.1.3.cmml"><mi id="S3.Ex2.m1.23.23.23.23.23.23.1.1" xref="S3.Ex2.m1.23.23.23.23.23.23.1.1.cmml">i</mi><mo id="S3.Ex2.m1.23.23.23.23.23.23.1.4.1" xref="S3.Ex2.m1.23.23.23.23.23.23.1.3.cmml">,</mo><mi id="S3.Ex2.m1.23.23.23.23.23.23.1.2" xref="S3.Ex2.m1.23.23.23.23.23.23.1.2.cmml">j</mi></mrow></msub><mo id="S3.Ex2.m1.24.24.24.24.24.24" rspace="0.447em">,</mo><mtext class="ltx_mathvariant_monospace" id="S3.Ex2.m1.25.25.25.25.25.25" xref="S3.Ex2.m1.25.25.25.25.25.25a.cmml">[EoS]</mtext><mo id="S3.Ex2.m1.26.26.26.26.26.26">]</mo></mrow><mo id="S3.Ex2.m1.27.27.27.27.27.27">)</mo></mrow></mrow><mo id="S3.Ex2.m1.28.28.28.28.28.28">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.29.29.29.29.29.29" rspace="0.447em">,</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2"><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.2"><mi id="S3.Ex2.m1.30.30.30.30.30.30" mathvariant="normal" xref="S3.Ex2.m1.30.30.30.30.30.30.cmml">â„“</mi><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.2.1">â¢</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.2.2"><mo id="S3.Ex2.m1.31.31.31.31.31.31">(</mo><mi id="S3.Ex2.m1.32.32.32.32.32.32" xref="S3.Ex2.m1.32.32.32.32.32.32.cmml">y</mi><mo id="S3.Ex2.m1.33.33.33.33.33.33">,</mo><mover accent="true" id="S3.Ex2.m1.34.34.34.34.34.34" xref="S3.Ex2.m1.34.34.34.34.34.34.cmml"><mi id="S3.Ex2.m1.34.34.34.34.34.34.2" xref="S3.Ex2.m1.34.34.34.34.34.34.2.cmml">y</mi><mo id="S3.Ex2.m1.34.34.34.34.34.34.1" xref="S3.Ex2.m1.34.34.34.34.34.34.1.cmml">^</mo></mover><mo id="S3.Ex2.m1.35.35.35.35.35.35">)</mo></mrow></mrow><mo id="S3.Ex2.m1.36.36.36.36.36.36" lspace="0.108em" rspace="0.108em" xref="S3.Ex2.m1.36.36.36.36.36.36.cmml">=</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1"><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1a">âˆ’</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1"><mo id="S3.Ex2.m1.38.38.38.38.38.38">[</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1"><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.3"><mi id="S3.Ex2.m1.39.39.39.39.39.39" xref="S3.Ex2.m1.39.39.39.39.39.39.cmml">y</mi><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.3.1" lspace="0.167em">â¢</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.3.2"><mi id="S3.Ex2.m1.40.40.40.40.40.40" xref="S3.Ex2.m1.40.40.40.40.40.40.cmml">log</mi><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.3.2a" lspace="0.167em">â¡</mo><mover accent="true" id="S3.Ex2.m1.41.41.41.41.41.41" xref="S3.Ex2.m1.41.41.41.41.41.41.cmml"><mi id="S3.Ex2.m1.41.41.41.41.41.41.2" xref="S3.Ex2.m1.41.41.41.41.41.41.2.cmml">y</mi><mo id="S3.Ex2.m1.41.41.41.41.41.41.1" xref="S3.Ex2.m1.41.41.41.41.41.41.1.cmml">^</mo></mover></mrow></mrow><mo id="S3.Ex2.m1.42.42.42.42.42.42" rspace="0.052em" xref="S3.Ex2.m1.42.42.42.42.42.42.cmml">+</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.2"><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.1.1.1"><mo id="S3.Ex2.m1.43.43.43.43.43.43" stretchy="false">(</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.1.1.1.1"><mn id="S3.Ex2.m1.44.44.44.44.44.44" xref="S3.Ex2.m1.44.44.44.44.44.44.cmml">1</mn><mo id="S3.Ex2.m1.45.45.45.45.45.45" lspace="0.052em" rspace="0.052em" xref="S3.Ex2.m1.45.45.45.45.45.45.cmml">âˆ’</mo><mi id="S3.Ex2.m1.46.46.46.46.46.46" xref="S3.Ex2.m1.46.46.46.46.46.46.cmml">y</mi></mrow><mo id="S3.Ex2.m1.47.47.47.47.47.47" stretchy="false">)</mo></mrow><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.2.3" lspace="0.167em">â¢</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.2.2.1"><mi id="S3.Ex2.m1.48.48.48.48.48.48" xref="S3.Ex2.m1.48.48.48.48.48.48.cmml">log</mi><mo id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.2.2.1a">â¡</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.2.2.1.1"><mo id="S3.Ex2.m1.49.49.49.49.49.49" stretchy="false">(</mo><mrow id="S3.Ex2.m1.57.57.2.56.56.56.56.1.2.2.1.1.1.1.2.2.1.1.1"><mn id="S3.Ex2.m1.50.50.50.50.50.50" xref="S3.Ex2.m1.50.50.50.50.50.50.cmml">1</mn><mo id="S3.Ex2.m1.51.51.51.51.51.51" lspace="0.052em" rspace="0.052em" xref="S3.Ex2.m1.51.51.51.51.51.51.cmml">âˆ’</mo><mover accent="true" id="S3.Ex2.m1.52.52.52.52.52.52" xref="S3.Ex2.m1.52.52.52.52.52.52.cmml"><mi id="S3.Ex2.m1.52.52.52.52.52.52.2" xref="S3.Ex2.m1.52.52.52.52.52.52.2.cmml">y</mi><mo id="S3.Ex2.m1.52.52.52.52.52.52.1" xref="S3.Ex2.m1.52.52.52.52.52.52.1.cmml">^</mo></mover></mrow><mo id="S3.Ex2.m1.53.53.53.53.53.53" stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.54.54.54.54.54.54">]</mo></mrow></mrow></mrow></mrow><mo id="S3.Ex2.m1.55.55.55.55.55.55">,</mo></mrow></mtd></mtr></mtable><annotation-xml encoding="MathML-Content" id="S3.Ex2.m1.57b"><apply id="S3.Ex2.m1.56.56.1.1.1.3.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.3a.cmml">formulae-sequence</csymbol><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.cmml"><eq id="S3.Ex2.m1.3.3.3.3.3.3.cmml" xref="S3.Ex2.m1.3.3.3.3.3.3"></eq><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.4.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.1.1.4.1.cmml">subscript</csymbol><ci id="S3.Ex2.m1.1.1.1.1.1.1.cmml" xref="S3.Ex2.m1.1.1.1.1.1.1">â„’</ci><ci id="S3.Ex2.m1.2.2.2.2.2.2.1a.cmml" xref="S3.Ex2.m1.2.2.2.2.2.2.1"><mtext id="S3.Ex2.m1.2.2.2.2.2.2.1.cmml" mathsize="70%" xref="S3.Ex2.m1.2.2.2.2.2.2.1">reranker</mtext></ci></apply><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.cmml"><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.3.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.1.1.2.3.1.cmml">superscript</csymbol><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.3.2.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.1.1.2.3.2.1.cmml">subscript</csymbol><sum id="S3.Ex2.m1.4.4.4.4.4.4.cmml" xref="S3.Ex2.m1.4.4.4.4.4.4"></sum><apply id="S3.Ex2.m1.5.5.5.5.5.5.1.cmml" xref="S3.Ex2.m1.5.5.5.5.5.5.1"><eq id="S3.Ex2.m1.5.5.5.5.5.5.1.1.cmml" xref="S3.Ex2.m1.5.5.5.5.5.5.1.1"></eq><ci id="S3.Ex2.m1.5.5.5.5.5.5.1.2.cmml" xref="S3.Ex2.m1.5.5.5.5.5.5.1.2">ğ‘–</ci><cn id="S3.Ex2.m1.5.5.5.5.5.5.1.3.cmml" type="integer" xref="S3.Ex2.m1.5.5.5.5.5.5.1.3">1</cn></apply></apply><ci id="S3.Ex2.m1.6.6.6.6.6.6.1.cmml" xref="S3.Ex2.m1.6.6.6.6.6.6.1">ğµ</ci></apply><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.cmml"><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.3.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.3.1.cmml">superscript</csymbol><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.3.2.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.3.2.1.cmml">subscript</csymbol><sum id="S3.Ex2.m1.7.7.7.7.7.7.cmml" xref="S3.Ex2.m1.7.7.7.7.7.7"></sum><apply id="S3.Ex2.m1.8.8.8.8.8.8.1.cmml" xref="S3.Ex2.m1.8.8.8.8.8.8.1"><eq id="S3.Ex2.m1.8.8.8.8.8.8.1.1.cmml" xref="S3.Ex2.m1.8.8.8.8.8.8.1.1"></eq><ci id="S3.Ex2.m1.8.8.8.8.8.8.1.2.cmml" xref="S3.Ex2.m1.8.8.8.8.8.8.1.2">ğ‘—</ci><cn id="S3.Ex2.m1.8.8.8.8.8.8.1.3.cmml" type="integer" xref="S3.Ex2.m1.8.8.8.8.8.8.1.3">1</cn></apply></apply><apply id="S3.Ex2.m1.9.9.9.9.9.9.1.cmml" xref="S3.Ex2.m1.9.9.9.9.9.9.1"><csymbol cd="ambiguous" id="S3.Ex2.m1.9.9.9.9.9.9.1.1.cmml" xref="S3.Ex2.m1.9.9.9.9.9.9.1">subscript</csymbol><ci id="S3.Ex2.m1.9.9.9.9.9.9.1.2.cmml" xref="S3.Ex2.m1.9.9.9.9.9.9.1.2">ğ‘†</ci><ci id="S3.Ex2.m1.9.9.9.9.9.9.1.3.cmml" xref="S3.Ex2.m1.9.9.9.9.9.9.1.3">ğ‘–</ci></apply></apply><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.cmml"><times id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.3.cmml"></times><apply id="S3.Ex2.m1.10.10.10.10.10.10.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10"><divide id="S3.Ex2.m1.10.10.10.10.10.10.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10"></divide><cn id="S3.Ex2.m1.10.10.10.10.10.10.2.cmml" type="integer" xref="S3.Ex2.m1.10.10.10.10.10.10.2">1</cn><apply id="S3.Ex2.m1.10.10.10.10.10.10.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3"><times id="S3.Ex2.m1.10.10.10.10.10.10.3.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.1"></times><ci id="S3.Ex2.m1.10.10.10.10.10.10.3.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.2">ğµ</ci><apply id="S3.Ex2.m1.10.10.10.10.10.10.3.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.10.10.10.10.10.10.3.3.1.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3">subscript</csymbol><ci id="S3.Ex2.m1.10.10.10.10.10.10.3.3.2.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3.2">ğ‘†</ci><ci id="S3.Ex2.m1.10.10.10.10.10.10.3.3.3.cmml" xref="S3.Ex2.m1.10.10.10.10.10.10.3.3.3">ğ‘–</ci></apply></apply></apply><ci id="S3.Ex2.m1.11.11.11.11.11.11.cmml" xref="S3.Ex2.m1.11.11.11.11.11.11">â„“</ci><interval closure="open" id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.2.3.cmml"><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.1.1.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.1.1.1.1.1.1.1.1.1.cmml">subscript</csymbol><ci id="S3.Ex2.m1.13.13.13.13.13.13.cmml" xref="S3.Ex2.m1.13.13.13.13.13.13">ğ²</ci><apply id="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.cmml" xref="S3.Ex2.m1.14.14.14.14.14.14.1.3"><csymbol cd="ambiguous" id="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.1.cmml" xref="S3.Ex2.m1.14.14.14.14.14.14.1.3">subscript</csymbol><ci id="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.2.cmml" xref="S3.Ex2.m1.14.14.14.14.14.14.1.3.1.2">S</ci><list id="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.3.cmml" xref="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.4"><ci id="S3.Ex2.m1.14.14.14.14.14.14.1.1.1.1.cmml" xref="S3.Ex2.m1.14.14.14.14.14.14.1.1.1.1">ğ‘–</ci><ci id="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.2.cmml" xref="S3.Ex2.m1.14.14.14.14.14.14.1.2.2.2">ğ‘—</ci></list></apply></apply><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.2.2.2.cmml"><times id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.2.2.2.2.cmml"></times><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.2.2.2.3.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.2.2.2.3.1.cmml">subscript</csymbol><ci id="S3.Ex2.m1.16.16.16.16.16.16.cmml" xref="S3.Ex2.m1.16.16.16.16.16.16">ğ‘“</ci><ci id="S3.Ex2.m1.17.17.17.17.17.17.1.cmml" xref="S3.Ex2.m1.17.17.17.17.17.17.1">R</ci></apply><list id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.2.2.2.1.1.1.2.cmml"><ci id="S3.Ex2.m1.20.20.20.20.20.20.cmml" xref="S3.Ex2.m1.20.20.20.20.20.20">ğ’’</ci><apply id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.2.2.2.1.1.1.1.1.cmml"><csymbol cd="ambiguous" id="S3.Ex2.m1.56.56.1.1.1.1.1.2.2.2.2.2.2.1.1.1.1.1.1.cmml">subscript</csymbol><ci id="S3.Ex2.m1.22.22.22.22.22.22.cmml" xref="S3.Ex2.m1.22.22.22.22.22.22">ğ’”</ci><list id="S3.Ex2.m1.23.23.23.23.23.23.1.3.cmml" xref="S3.Ex2.m1.23.23.23.23.23.23.1.4"><ci id="S3.Ex2.m1.23.23.23.23.23.23.1.1.cmml" xref="S3.Ex2.m1.23.23.23.23.23.23.1.1">ğ‘–</ci><ci id="S3.Ex2.m1.23.23.23.23.23.23.1.2.cmml" xref="S3.Ex2.m1.23.23.23.23.23.23.1.2">ğ‘—</ci></list></apply><ci id="S3.Ex2.m1.25.25.25.25.25.25a.cmml" xref="S3.Ex2.m1.25.25.25.25.25.25"><mtext class="ltx_mathvariant_monospace" id="S3.Ex2.m1.25.25.25.25.25.25.cmml" xref="S3.Ex2.m1.25.25.25.25.25.25">[EoS]</mtext></ci></list></apply></interval></apply></apply></apply></apply><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.cmml"><eq id="S3.Ex2.m1.36.36.36.36.36.36.cmml" xref="S3.Ex2.m1.36.36.36.36.36.36"></eq><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.3.cmml"><times id="S3.Ex2.m1.56.56.1.1.1.2.2.3.1.cmml"></times><ci id="S3.Ex2.m1.30.30.30.30.30.30.cmml" xref="S3.Ex2.m1.30.30.30.30.30.30">â„“</ci><interval closure="open" id="S3.Ex2.m1.56.56.1.1.1.2.2.3.3.cmml"><ci id="S3.Ex2.m1.32.32.32.32.32.32.cmml" xref="S3.Ex2.m1.32.32.32.32.32.32">ğ‘¦</ci><apply id="S3.Ex2.m1.34.34.34.34.34.34.cmml" xref="S3.Ex2.m1.34.34.34.34.34.34"><ci id="S3.Ex2.m1.34.34.34.34.34.34.1.cmml" xref="S3.Ex2.m1.34.34.34.34.34.34.1">^</ci><ci id="S3.Ex2.m1.34.34.34.34.34.34.2.cmml" xref="S3.Ex2.m1.34.34.34.34.34.34.2">ğ‘¦</ci></apply></interval></apply><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.cmml"><minus id="S3.Ex2.m1.37.37.37.37.37.37.cmml"></minus><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.2.cmml"><csymbol cd="latexml" id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.2.1.cmml">delimited-[]</csymbol><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.cmml"><plus id="S3.Ex2.m1.42.42.42.42.42.42.cmml" xref="S3.Ex2.m1.42.42.42.42.42.42"></plus><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.4.cmml"><times id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.4.1.cmml"></times><ci id="S3.Ex2.m1.39.39.39.39.39.39.cmml" xref="S3.Ex2.m1.39.39.39.39.39.39">ğ‘¦</ci><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.4.3.cmml"><log id="S3.Ex2.m1.40.40.40.40.40.40.cmml" xref="S3.Ex2.m1.40.40.40.40.40.40"></log><apply id="S3.Ex2.m1.41.41.41.41.41.41.cmml" xref="S3.Ex2.m1.41.41.41.41.41.41"><ci id="S3.Ex2.m1.41.41.41.41.41.41.1.cmml" xref="S3.Ex2.m1.41.41.41.41.41.41.1">^</ci><ci id="S3.Ex2.m1.41.41.41.41.41.41.2.cmml" xref="S3.Ex2.m1.41.41.41.41.41.41.2">ğ‘¦</ci></apply></apply></apply><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.2.cmml"><times id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.2.3.cmml"></times><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.1.1.1.1.cmml"><minus id="S3.Ex2.m1.45.45.45.45.45.45.cmml" xref="S3.Ex2.m1.45.45.45.45.45.45"></minus><cn id="S3.Ex2.m1.44.44.44.44.44.44.cmml" type="integer" xref="S3.Ex2.m1.44.44.44.44.44.44">1</cn><ci id="S3.Ex2.m1.46.46.46.46.46.46.cmml" xref="S3.Ex2.m1.46.46.46.46.46.46">ğ‘¦</ci></apply><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.2.2.2.cmml"><log id="S3.Ex2.m1.48.48.48.48.48.48.cmml" xref="S3.Ex2.m1.48.48.48.48.48.48"></log><apply id="S3.Ex2.m1.56.56.1.1.1.2.2.1.1.1.1.2.2.1.1.1.cmml"><minus id="S3.Ex2.m1.51.51.51.51.51.51.cmml" xref="S3.Ex2.m1.51.51.51.51.51.51"></minus><cn id="S3.Ex2.m1.50.50.50.50.50.50.cmml" type="integer" xref="S3.Ex2.m1.50.50.50.50.50.50">1</cn><apply id="S3.Ex2.m1.52.52.52.52.52.52.cmml" xref="S3.Ex2.m1.52.52.52.52.52.52"><ci id="S3.Ex2.m1.52.52.52.52.52.52.1.cmml" xref="S3.Ex2.m1.52.52.52.52.52.52.1">^</ci><ci id="S3.Ex2.m1.52.52.52.52.52.52.2.cmml" xref="S3.Ex2.m1.52.52.52.52.52.52.2">ğ‘¦</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex2.m1.57c">\displaystyle\begin{split}\mathcal{L}_{\text{reranker}}\!=\!\sum_{i=1}^{B}\sum%
_{j=1}^{S_{i}}\!\frac{1}{BS_{i}}\ell\left(\mathbf{y}_{(\mathrm{S}_{i,j})},\;f_%
{\mathrm{R}}\left(\left[{\bm{q}},\;{\bm{s}}_{i,j},\;\texttt{[EoS]}\right]%
\right)\right),\;\ell\left(y,\hat{y}\right)\!=\!-\left[y\log\hat{y}\!+\!(1\!-%
\!y)\log(1\!-\!\hat{y})\right],\end{split}</annotation><annotation encoding="application/x-llamapun" id="S3.Ex2.m1.57d">start_ROW start_CELL caligraphic_L start_POSTSUBSCRIPT reranker end_POSTSUBSCRIPT = âˆ‘ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT âˆ‘ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT divide start_ARG 1 end_ARG start_ARG italic_B italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG roman_â„“ ( bold_y start_POSTSUBSCRIPT ( roman_S start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT roman_R end_POSTSUBSCRIPT ( [ bold_italic_q , bold_italic_s start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , [EoS] ] ) ) , roman_â„“ ( italic_y , over^ start_ARG italic_y end_ARG ) = - [ italic_y roman_log over^ start_ARG italic_y end_ARG + ( 1 - italic_y ) roman_log ( 1 - over^ start_ARG italic_y end_ARG ) ] , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p3.9">where <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><msub id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml"><mi id="S3.SS3.p3.1.m1.1.1.2" xref="S3.SS3.p3.1.m1.1.1.2.cmml">S</mi><mi id="S3.SS3.p3.1.m1.1.1.3" xref="S3.SS3.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><apply id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.1.m1.1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p3.1.m1.1.1.2.cmml" xref="S3.SS3.p3.1.m1.1.1.2">ğ‘†</ci><ci id="S3.SS3.p3.1.m1.1.1.3.cmml" xref="S3.SS3.p3.1.m1.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the number of sections in the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mi id="S3.SS3.p3.2.m2.1.1" xref="S3.SS3.p3.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">italic_i</annotation></semantics></math>-th document, <math alttext="\mathbf{y}_{(\mathrm{S}_{i,j})}" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.3"><semantics id="S3.SS3.p3.3.m3.3a"><msub id="S3.SS3.p3.3.m3.3.4" xref="S3.SS3.p3.3.m3.3.4.cmml"><mi id="S3.SS3.p3.3.m3.3.4.2" xref="S3.SS3.p3.3.m3.3.4.2.cmml">ğ²</mi><mrow id="S3.SS3.p3.3.m3.3.3.3.3" xref="S3.SS3.p3.3.m3.3.3.3.3.1.cmml"><mo id="S3.SS3.p3.3.m3.3.3.3.3.2" stretchy="false" xref="S3.SS3.p3.3.m3.3.3.3.3.1.cmml">(</mo><msub id="S3.SS3.p3.3.m3.3.3.3.3.1" xref="S3.SS3.p3.3.m3.3.3.3.3.1.cmml"><mi id="S3.SS3.p3.3.m3.3.3.3.3.1.2" mathvariant="normal" xref="S3.SS3.p3.3.m3.3.3.3.3.1.2.cmml">S</mi><mrow id="S3.SS3.p3.3.m3.2.2.2.2.2.4" xref="S3.SS3.p3.3.m3.2.2.2.2.2.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.1.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p3.3.m3.2.2.2.2.2.4.1" xref="S3.SS3.p3.3.m3.2.2.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p3.3.m3.2.2.2.2.2.2" xref="S3.SS3.p3.3.m3.2.2.2.2.2.2.cmml">j</mi></mrow></msub><mo id="S3.SS3.p3.3.m3.3.3.3.3.3" stretchy="false" xref="S3.SS3.p3.3.m3.3.3.3.3.1.cmml">)</mo></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.3b"><apply id="S3.SS3.p3.3.m3.3.4.cmml" xref="S3.SS3.p3.3.m3.3.4"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.3.4.1.cmml" xref="S3.SS3.p3.3.m3.3.4">subscript</csymbol><ci id="S3.SS3.p3.3.m3.3.4.2.cmml" xref="S3.SS3.p3.3.m3.3.4.2">ğ²</ci><apply id="S3.SS3.p3.3.m3.3.3.3.3.1.cmml" xref="S3.SS3.p3.3.m3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.3.3.3.3.1.1.cmml" xref="S3.SS3.p3.3.m3.3.3.3.3">subscript</csymbol><ci id="S3.SS3.p3.3.m3.3.3.3.3.1.2.cmml" xref="S3.SS3.p3.3.m3.3.3.3.3.1.2">S</ci><list id="S3.SS3.p3.3.m3.2.2.2.2.2.3.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2.2.4"><ci id="S3.SS3.p3.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1.1.1.1">ğ‘–</ci><ci id="S3.SS3.p3.3.m3.2.2.2.2.2.2.cmml" xref="S3.SS3.p3.3.m3.2.2.2.2.2.2">ğ‘—</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.3c">\mathbf{y}_{(\mathrm{S}_{i,j})}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.3d">bold_y start_POSTSUBSCRIPT ( roman_S start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT</annotation></semantics></math> is the label for the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m4.1"><semantics id="S3.SS3.p3.4.m4.1a"><mi id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><ci id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">ğ‘—</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m4.1d">italic_j</annotation></semantics></math>-th section of the <math alttext="i" class="ltx_Math" display="inline" id="S3.SS3.p3.5.m5.1"><semantics id="S3.SS3.p3.5.m5.1a"><mi id="S3.SS3.p3.5.m5.1.1" xref="S3.SS3.p3.5.m5.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.5.m5.1b"><ci id="S3.SS3.p3.5.m5.1.1.cmml" xref="S3.SS3.p3.5.m5.1.1">ğ‘–</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.5.m5.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.5.m5.1d">italic_i</annotation></semantics></math>-th document <math alttext="{\bm{s}}_{i,j}" class="ltx_Math" display="inline" id="S3.SS3.p3.6.m6.2"><semantics id="S3.SS3.p3.6.m6.2a"><msub id="S3.SS3.p3.6.m6.2.3" xref="S3.SS3.p3.6.m6.2.3.cmml"><mi id="S3.SS3.p3.6.m6.2.3.2" xref="S3.SS3.p3.6.m6.2.3.2.cmml">ğ’”</mi><mrow id="S3.SS3.p3.6.m6.2.2.2.4" xref="S3.SS3.p3.6.m6.2.2.2.3.cmml"><mi id="S3.SS3.p3.6.m6.1.1.1.1" xref="S3.SS3.p3.6.m6.1.1.1.1.cmml">i</mi><mo id="S3.SS3.p3.6.m6.2.2.2.4.1" xref="S3.SS3.p3.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS3.p3.6.m6.2.2.2.2" xref="S3.SS3.p3.6.m6.2.2.2.2.cmml">j</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.6.m6.2b"><apply id="S3.SS3.p3.6.m6.2.3.cmml" xref="S3.SS3.p3.6.m6.2.3"><csymbol cd="ambiguous" id="S3.SS3.p3.6.m6.2.3.1.cmml" xref="S3.SS3.p3.6.m6.2.3">subscript</csymbol><ci id="S3.SS3.p3.6.m6.2.3.2.cmml" xref="S3.SS3.p3.6.m6.2.3.2">ğ’”</ci><list id="S3.SS3.p3.6.m6.2.2.2.3.cmml" xref="S3.SS3.p3.6.m6.2.2.2.4"><ci id="S3.SS3.p3.6.m6.1.1.1.1.cmml" xref="S3.SS3.p3.6.m6.1.1.1.1">ğ‘–</ci><ci id="S3.SS3.p3.6.m6.2.2.2.2.cmml" xref="S3.SS3.p3.6.m6.2.2.2.2">ğ‘—</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.6.m6.2c">{\bm{s}}_{i,j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.6.m6.2d">bold_italic_s start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT</annotation></semantics></math> (with its value of one if relevant to the query <math alttext="{\bm{q}}" class="ltx_Math" display="inline" id="S3.SS3.p3.7.m7.1"><semantics id="S3.SS3.p3.7.m7.1a"><mi id="S3.SS3.p3.7.m7.1.1" xref="S3.SS3.p3.7.m7.1.1.cmml">ğ’’</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.7.m7.1b"><ci id="S3.SS3.p3.7.m7.1.1.cmml" xref="S3.SS3.p3.7.m7.1.1">ğ’’</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.7.m7.1c">{\bm{q}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.7.m7.1d">bold_italic_q</annotation></semantics></math> and zero otherwise), and <math alttext="B" class="ltx_Math" display="inline" id="S3.SS3.p3.8.m8.1"><semantics id="S3.SS3.p3.8.m8.1a"><mi id="S3.SS3.p3.8.m8.1.1" xref="S3.SS3.p3.8.m8.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.8.m8.1b"><ci id="S3.SS3.p3.8.m8.1.1.cmml" xref="S3.SS3.p3.8.m8.1.1">ğµ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.8.m8.1c">B</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.8.m8.1d">italic_B</annotation></semantics></math> is the batch size during training. Also, in this training process, the sections not labeled as relevant to the query are considered negative samples. Then, by minimizing <math alttext="\mathcal{L}_{\text{reranker}}" class="ltx_Math" display="inline" id="S3.SS3.p3.9.m9.1"><semantics id="S3.SS3.p3.9.m9.1a"><msub id="S3.SS3.p3.9.m9.1.1" xref="S3.SS3.p3.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.9.m9.1.1.2" xref="S3.SS3.p3.9.m9.1.1.2.cmml">â„’</mi><mtext id="S3.SS3.p3.9.m9.1.1.3" xref="S3.SS3.p3.9.m9.1.1.3a.cmml">reranker</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.9.m9.1b"><apply id="S3.SS3.p3.9.m9.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.p3.9.m9.1.1.1.cmml" xref="S3.SS3.p3.9.m9.1.1">subscript</csymbol><ci id="S3.SS3.p3.9.m9.1.1.2.cmml" xref="S3.SS3.p3.9.m9.1.1.2">â„’</ci><ci id="S3.SS3.p3.9.m9.1.1.3a.cmml" xref="S3.SS3.p3.9.m9.1.1.3"><mtext id="S3.SS3.p3.9.m9.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p3.9.m9.1.1.3">reranker</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.9.m9.1c">\mathcal{L}_{\text{reranker}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.9.m9.1d">caligraphic_L start_POSTSUBSCRIPT reranker end_POSTSUBSCRIPT</annotation></semantics></math>, the reranker learns to predict section relevance for any query, thus refining our overall retrieval process by allowing the retrival of not just whole documents but also their most relevant sections, for multiple use cases of IR.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">To evaluate the effectivenss of IDentIfy, we focus on multimodal IR tasks that require understanding of both the textual and visual cues within queries and documents, which align well with our goal of enhancing retrieval of multimodal interleaved documents. The datasets considered are as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">Encyclopedic-VQA</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Mensink etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib29" title="">2023</a>)</cite> is a large-scale visual question-answering (VQA) benchmark dataset, widely used for measuring the performance of multimodal IR models. Each query is linked to a specific section of a Wikipedia document (containing an answer for it) and is manually annotated by humans. Also, this dataset offers both text-only and multimodal queries. In addition to this, the queries are related to fine-grained properties of species and landmarks.
Our experiments focus on the single-hop category where questions that can be answered in a single retrieval step.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p3.1.1">InfoSeek</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib4" title="">2023</a>)</cite> is a dataset designed for knowledge-intensive VQA, covering a wide range of entities (such as landmarks, animals, and food). Questions are generated by filling human-written templates with knowledge triples (subject, relation, object) available from Wikidata, which involve only the multimodal queries. As the test dataset is not available, we use the validation set as our test set, and split the training set into training and validation subsets with a 9:1 ratio.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">ViQuAE</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Lerner etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib20" title="">2022</a>)</cite> is a dataset focused about human entities. It provides both text-based and multimodal queries, with each query linked to a specific section of a Wikipedia document that contains an answer (annotated by humans), which makes it an idea benchmark for section retrieval.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p5.1.1">Open-WikiTable</span>Â <cite class="ltx_cite ltx_citemacro_citep">(Kweon etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib18" title="">2023</a>)</cite> is an extension of WikiSQLÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib47" title="">2017</a>)</cite> and WikiTableQuestionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Pasupat &amp; Liang, <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib34" title="">2015</a>)</cite>, designed for open-domain table question answering that requires retrieval of the most relevant table from a broader corpus. For our experiments, we adapt this dataset, aiming at identifying the document or document section containing the target table, and correspondingly, utilize the WikiTableQuestions subset of Open-WikiTable that has labels for it.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Model Training and Evaluation</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We use LLaVA-NeXT-InterleaveÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib23" title="">2024</a>)</cite> of 0.5B parameters as the basis VLM, for both the retriever and reranker. To take the advantage of larger batch sizes (while reducing GPU memory usage), we apply LoRAÂ <cite class="ltx_cite ltx_citemacro_citep">(Hu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib12" title="">2022</a>)</cite>. Also, to further optimize the GPU usage, we combine four images into one, scaling each down to half of its original height and width. During retriever and reranker training, we consider four sections per document in representing documents and selecting negative samples. In contrast, during inference, we consider all sections within each document. For section retrieval, the top 25 documents retrieved are split into sections and passed to the rerankers. All experiments are conducted using a single H100 GPU.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We compare our approach against a variety of IR baselines designed to capture different document representations. We start with Entity and Summary baselines, which are trained to retrieve documents based on their titles and summary sections. Next, we consider the Text-document retriever, which retrieves documents based on their textual content. Additionally, to consider a visual component, we consider the Single-image baseline, incorporating the first document image.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Evaluation Metrics</h4>
<div class="ltx_para ltx_noindent" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">We evaluate the performance of the retriever and reranker with standard metrics: Recall@K (R@K) and Mean Reciprocal Rank@K (MRR@K). First, R@K measures whether the relevant document or section is retrieved within the top-K results. MRR@K evaluates the ranking quality by measuring the position of the first relevant item among the top-K retrieved results.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Results and Discussion</h3>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Interleaved format improves document retrieval.</h4>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S4.T2.fig1" style="width:225.5pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 1: </span>Comparison results of the document formats for document retrieval with multimodal queries. Entity uses the document title, Summary uses its first summary section, and Text-document uses only textual content. Based on the Text-document, we add a single image (+ Single-image) or interleaved multimodal content (+ Interleaved).</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.fig1.1" style="width:397.5pt;height:193.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.8pt,-42.7pt) scale(1.79114213882767,1.79114213882767) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.fig1.1.1">
<tr class="ltx_tr" id="S4.T2.fig1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T2.fig1.1.1.1.1" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.1.1.1">Format</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.fig1.1.1.1.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.1.2.1">R@1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.fig1.1.1.1.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.1.3.1">R@10</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.fig1.1.1.1.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.1.4.1">R@100</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.fig1.1.1.1.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.1.5.1">MRR@10</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.fig1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.fig1.1.1.2.1" style="padding:0.5pt 0.0pt;">Entity</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.fig1.1.1.2.2" style="padding:0.5pt 0.0pt;">3.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.fig1.1.1.2.3" style="padding:0.5pt 0.0pt;">15.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.fig1.1.1.2.4" style="padding:0.5pt 0.0pt;">39.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.fig1.1.1.2.5" style="padding:0.5pt 0.0pt;">6.1</td>
</tr>
<tr class="ltx_tr" id="S4.T2.fig1.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.fig1.1.1.3.1" style="padding:0.5pt 0.0pt;">Summary</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.3.2" style="padding:0.5pt 0.0pt;">13.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.3.3" style="padding:0.5pt 0.0pt;">41.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.3.4" style="padding:0.5pt 0.0pt;">66.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.3.5" style="padding:0.5pt 0.0pt;">21.6</td>
</tr>
<tr class="ltx_tr" id="S4.T2.fig1.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.fig1.1.1.4.1" style="padding:0.5pt 0.0pt;">Text-document</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.4.2" style="padding:0.5pt 0.0pt;">12.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.4.3" style="padding:0.5pt 0.0pt;">37.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.4.4" style="padding:0.5pt 0.0pt;">68.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.4.5" style="padding:0.5pt 0.0pt;">19.8</td>
</tr>
<tr class="ltx_tr" id="S4.T2.fig1.1.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.fig1.1.1.5.1" style="padding:0.5pt 0.0pt;">+ Single-image</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.5.2" style="padding:0.5pt 0.0pt;">16.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.5.3" style="padding:0.5pt 0.0pt;">45.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.5.4" style="padding:0.5pt 0.0pt;">77.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig1.1.1.5.5" style="padding:0.5pt 0.0pt;">25.3</td>
</tr>
<tr class="ltx_tr" id="S4.T2.fig1.1.1.6">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T2.fig1.1.1.6.1" style="background-color:#E0FEFE;padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T2.fig1.1.1.6.1.1" style="background-color:#E0FEFE;">+ Interleaved (Ours)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.fig1.1.1.6.2" style="background-color:#E0FEFE;padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.6.2.1" style="background-color:#E0FEFE;">20.5</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.fig1.1.1.6.3" style="background-color:#E0FEFE;padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.6.3.1" style="background-color:#E0FEFE;">50.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.fig1.1.1.6.4" style="background-color:#E0FEFE;padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.6.4.1" style="background-color:#E0FEFE;">78.0</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.fig1.1.1.6.5" style="background-color:#E0FEFE;padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig1.1.1.6.5.1" style="background-color:#E0FEFE;">29.4</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S4.T2.fig2" style="width:199.5pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Table 2: </span>Investigation of retrieval granularity in information retrieval. Two granularities are tested on section retrieval with multimodal queries: Passage splits a document by section boundaries, and, using the split passages as retrieval targets, trains the retriever; meanwhile, Document uses documents as retrieval targets. The same reranker is applied to items retrieved by each method. * indicates a result that does not leverage reranking.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.fig2.1" style="width:397.5pt;height:141.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(97.5pt,-34.6pt) scale(1.96207773027372,1.96207773027372) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.fig2.1.1">
<tr class="ltx_tr" id="S4.T2.fig2.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T2.fig2.1.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.1.1.1">Granularity</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.fig2.1.1.1.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.1.2.1">R@1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.fig2.1.1.1.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.1.3.1">R@10</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.fig2.1.1.1.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.1.4.1">R@20</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T2.fig2.1.1.1.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.1.5.1">MRR@10</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.fig2.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T2.fig2.1.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;">Passage*</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.fig2.1.1.2.2" style="padding-left:0.0pt;padding-right:0.0pt;">3.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.fig2.1.1.2.3" style="padding-left:0.0pt;padding-right:0.0pt;">16.9</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.fig2.1.1.2.4" style="padding-left:0.0pt;padding-right:0.0pt;">22.0</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T2.fig2.1.1.2.5" style="padding-left:0.0pt;padding-right:0.0pt;">7.5</td>
</tr>
<tr class="ltx_tr" id="S4.T2.fig2.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T2.fig2.1.1.3.1" style="padding-left:0.0pt;padding-right:0.0pt;">Passage</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig2.1.1.3.2" style="padding-left:0.0pt;padding-right:0.0pt;">28.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig2.1.1.3.3" style="padding-left:0.0pt;padding-right:0.0pt;">36.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig2.1.1.3.4" style="padding-left:0.0pt;padding-right:0.0pt;">37.8</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T2.fig2.1.1.3.5" style="padding-left:0.0pt;padding-right:0.0pt;">31.2</td>
</tr>
<tr class="ltx_tr" id="S4.T2.fig2.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T2.fig2.1.1.4.1" style="background-color:#E0FEFE;padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text" id="S4.T2.fig2.1.1.4.1.1" style="background-color:#E0FEFE;">Document (Ours)</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.fig2.1.1.4.2" style="background-color:#E0FEFE;padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.4.2.1" style="background-color:#E0FEFE;">35.1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.fig2.1.1.4.3" style="background-color:#E0FEFE;padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.4.3.1" style="background-color:#E0FEFE;">50.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.fig2.1.1.4.4" style="background-color:#E0FEFE;padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.4.4.1" style="background-color:#E0FEFE;">53.6</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T2.fig2.1.1.4.5" style="background-color:#E0FEFE;padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.fig2.1.1.4.5.1" style="background-color:#E0FEFE;">40.3</span></td>
</tr>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p1.1">First, we report the retrieval performance on the Encyclopedic-VQA dataset in <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T2" title="In Interleaved format improves document retrieval. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, where each query consists of both image and text content. From this, we observe that our approach achieves the best performance, with R@1 score improvements of 53.0%, 64.0%, and 25.0% compared to the Summary, Text-document, and Single-image retrieval baselines, respectively. The MRR@10 score similarly shows significant gains, with improvements of 36.1%, 48.5%, and 16.2% over the same baselines. This demonstrates the effectiveness of our approach in incorporating the interleaved multimodal format for document representations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px1.p2">
<p class="ltx_p" id="S4.SS3.SSS0.Px1.p2.1">To further understand the source of these performance gains, we explore two levels of retrieval granularity: passages and documents. Specifically, the passage retriever uses individual sections of documents as retrieval units, while the document retriever treats entire documents as single units. Both models are trained on the Encyclopedic-VQA dataset for multimodal retrieval. Then, we use the same reranker to both sets of results from passage and document retrievers, to directly compare their performance. In <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T2" title="In Interleaved format improves document retrieval. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, we observe that relying solely on the passage retriever (Passage*) results in suboptimal retrieval performance, highlighting the challenge in pinpointing the most relevant section within a document using traditional retrieval methods. In contrast, when the reranker is used alongside the document retriever, the performance significantly surpasses that of the passage retrieval, achieving a 22.7% improvement in R@1 and a 29.2% improvement in MRR@10, even though the document retriever provides eight times fewer retrieval units to the reranker. These results confirm the importance of leveraging holistic context from multiple, interrelated sections within documents. In addition to this, these findings also demonstrate the notable advantages of using the interleaved multimodal elements within documents, emphasizing the potential of this direction.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Interleaved format enhances document retrieval across modalities.</h4>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>Performance in document retrievals. <span class="ltx_text ltx_font_bold" id="S4.T3.3.1">(a)</span>: Results of retrieval for multimodal queries on InfoSeek and ViQuAE. <span class="ltx_text ltx_font_bold" id="S4.T3.4.2">(b)</span>: Results of retrieval for textual queries on Encyclopedic-VQA (Enc-VQA) and ViQuAE.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T3.5" style="width:194.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.1">(a) Document Retrieval with Multimodal Queries
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.5.2" style="width:397.5pt;height:152.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(81.5pt,-31.3pt) scale(1.69543028866196,1.69543028866196) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.5.2.1">
<span class="ltx_tr" id="S4.T3.5.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T3.5.2.1.1.1" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.1.1.1">Foramt</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T3.5.2.1.1.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.1.2.1">Dataset</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.5.2.1.1.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.1.3.1">R@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.5.2.1.1.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.1.4.1">R@10</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.5.2.1.1.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.1.5.1">R@100</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.5.2.1.1.6" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.1.6.1">MRR@10</span></span></span>
<span class="ltx_tr" id="S4.T3.5.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.5.2.1.2.1" style="padding:0.5pt 0.0pt;">Text-document</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T3.5.2.1.2.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T3.5.2.1.2.2.1">InfoSeek</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.1.2.3" style="padding:0.5pt 0.0pt;">6.8</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.1.2.4" style="padding:0.5pt 0.0pt;">23.6</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.1.2.5" style="padding:0.5pt 0.0pt;">52.5</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.1.2.6" style="padding:0.5pt 0.0pt;">11.2</span></span>
<span class="ltx_tr" id="S4.T3.5.2.1.3">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T3.5.2.1.3.1" style="padding:0.5pt 0.0pt;">+ Interleaved</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.5.2.1.3.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.3.2.1">10.2</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.5.2.1.3.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.3.3.1">30.4</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.5.2.1.3.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.3.4.1">57.3</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.5.2.1.3.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.3.5.1">15.7</span></span></span>
<span class="ltx_tr" id="S4.T3.5.2.1.4">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.5.2.1.4.1" style="padding:0.5pt 0.0pt;">Text-document</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T3.5.2.1.4.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T3.5.2.1.4.2.1">ViQuAE</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.1.4.3" style="padding:0.5pt 0.0pt;">13.5</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.1.4.4" style="padding:0.5pt 0.0pt;">40.4</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.1.4.5" style="padding:0.5pt 0.0pt;">67.4</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.5.2.1.4.6" style="padding:0.5pt 0.0pt;">20.9</span></span>
<span class="ltx_tr" id="S4.T3.5.2.1.5">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T3.5.2.1.5.1" style="padding:0.5pt 0.0pt;">+ Interleaved</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.5.2.1.5.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.5.2.1">17.5</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.5.2.1.5.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.5.3.1">46.0</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.5.2.1.5.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.5.4.1">69.4</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.5.2.1.5.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.5.2.1.5.5.1">26.3</span></span></span>
</span>
</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T3.6" style="width:194.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.1">(b) Document Retrieval with Textual Queries
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T3.6.2" style="width:397.5pt;height:149.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(79.2pt,-29.8pt) scale(1.66195512265333,1.66195512265333) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T3.6.2.1">
<span class="ltx_tr" id="S4.T3.6.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T3.6.2.1.1.1" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.1.1.1">Format</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T3.6.2.1.1.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.1.2.1">Dataset</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.6.2.1.1.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.1.3.1">R@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.6.2.1.1.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.1.4.1">R@10</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.6.2.1.1.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.1.5.1">R@100</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T3.6.2.1.1.6" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.1.6.1">MRR@10</span></span></span>
<span class="ltx_tr" id="S4.T3.6.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.6.2.1.2.1" style="padding:0.5pt 0.0pt;">Text-document</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T3.6.2.1.2.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T3.6.2.1.2.2.1">Enc-VQA</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.6.2.1.2.3" style="padding:0.5pt 0.0pt;">62.7</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.6.2.1.2.4" style="padding:0.5pt 0.0pt;">76.3</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.6.2.1.2.5" style="padding:0.5pt 0.0pt;">87.4</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.6.2.1.2.6" style="padding:0.5pt 0.0pt;">67.0</span></span>
<span class="ltx_tr" id="S4.T3.6.2.1.3">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T3.6.2.1.3.1" style="padding:0.5pt 0.0pt;">+ Interleaved</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.6.2.1.3.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.3.2.1">65.4</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.6.2.1.3.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.3.3.1">76.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.6.2.1.3.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.3.4.1">87.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T3.6.2.1.3.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.3.5.1">69.0</span></span></span>
<span class="ltx_tr" id="S4.T3.6.2.1.4">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T3.6.2.1.4.1" style="padding:0.5pt 0.0pt;">Text-document</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T3.6.2.1.4.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T3.6.2.1.4.2.1">ViQuAE</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.6.2.1.4.3" style="padding:0.5pt 0.0pt;">55.8</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.6.2.1.4.4" style="padding:0.5pt 0.0pt;">71.5</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.6.2.1.4.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.4.5.1">83.0</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T3.6.2.1.4.6" style="padding:0.5pt 0.0pt;">60.9</span></span>
<span class="ltx_tr" id="S4.T3.6.2.1.5">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T3.6.2.1.5.1" style="padding:0.5pt 0.0pt;">+ Interleaved</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.6.2.1.5.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.5.2.1">56.5</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.6.2.1.5.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.5.3.1">72.2</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.6.2.1.5.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.5.4.1">83.0</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T3.6.2.1.5.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.6.2.1.5.5.1">61.6</span></span></span>
</span>
</span></span></p>
</div>
</div>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>Performance of section retrievals. <span class="ltx_text ltx_font_bold" id="S4.T4.3.1">(a)</span>: Results of retrieval for multimodal queries on Encyclopedic-VQA (Enc-VQA) and ViQuAE. <span class="ltx_text ltx_font_bold" id="S4.T4.4.2">(b)</span>: Results of retrieval for textual queries on Enc-VQA and ViQuAE. The same document retrieval outcomes are used in section retrieval to solely measure the rerankerâ€™s performance.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T4.5" style="width:194.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.1">(a) Section Retrieval with Multimodal Queries
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T4.5.2" style="width:397.5pt;height:152.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(81.7pt,-31.4pt) scale(1.69744175399348,1.69744175399348) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.5.2.1">
<span class="ltx_tr" id="S4.T4.5.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T4.5.2.1.1.1" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.1.1.1">Format</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T4.5.2.1.1.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.1.2.1">Dataset</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T4.5.2.1.1.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.1.3.1">R@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T4.5.2.1.1.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.1.4.1">R@10</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T4.5.2.1.1.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.1.5.1">R@20</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T4.5.2.1.1.6" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.1.6.1">MRR@10</span></span></span>
<span class="ltx_tr" id="S4.T4.5.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.5.2.1.2.1" style="padding:0.5pt 0.0pt;">Text-document</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T4.5.2.1.2.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T4.5.2.1.2.2.1">Enc-VQA</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.2.1.2.3" style="padding:0.5pt 0.0pt;">40.7</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.2.1.2.4" style="padding:0.5pt 0.0pt;">52.8</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.2.1.2.5" style="padding:0.5pt 0.0pt;">55.5</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.2.1.2.6" style="padding:0.5pt 0.0pt;">44.8</span></span>
<span class="ltx_tr" id="S4.T4.5.2.1.3">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T4.5.2.1.3.1" style="padding:0.5pt 0.0pt;">+ Interleaved</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T4.5.2.1.3.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.3.2.1">42.4</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T4.5.2.1.3.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.3.3.1">53.6</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T4.5.2.1.3.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.3.4.1">55.7</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T4.5.2.1.3.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.3.5.1">46.3</span></span></span>
<span class="ltx_tr" id="S4.T4.5.2.1.4">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.5.2.1.4.1" style="padding:0.5pt 0.0pt;">Text-document</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T4.5.2.1.4.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T4.5.2.1.4.2.1">ViQuAE</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.2.1.4.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.4.3.1">12.6</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.2.1.4.4" style="padding:0.5pt 0.0pt;">31.7</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.2.1.4.5" style="padding:0.5pt 0.0pt;">37.7</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.5.2.1.4.6" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.4.6.1">18.2</span></span></span>
<span class="ltx_tr" id="S4.T4.5.2.1.5">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T4.5.2.1.5.1" style="padding:0.5pt 0.0pt;">+ Interleaved</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.5.2.1.5.2" style="padding:0.5pt 0.0pt;">11.4</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.5.2.1.5.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.5.3.1">32.1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.5.2.1.5.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.5.2.1.5.4.1">39.2</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.5.2.1.5.5" style="padding:0.5pt 0.0pt;">17.5</span></span>
</span>
</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T4.6" style="width:194.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.1">(b) Section Retrieval with Textual Queries
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T4.6.2" style="width:397.5pt;height:152.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(81.7pt,-31.4pt) scale(1.69744175399348,1.69744175399348) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.6.2.1">
<span class="ltx_tr" id="S4.T4.6.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T4.6.2.1.1.1" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.1.1.1">Format</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T4.6.2.1.1.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.1.2.1">Dataset</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T4.6.2.1.1.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.1.3.1">R@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T4.6.2.1.1.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.1.4.1">R@10</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T4.6.2.1.1.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.1.5.1">R@20</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T4.6.2.1.1.6" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.1.6.1">MRR@10</span></span></span>
<span class="ltx_tr" id="S4.T4.6.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.6.2.1.2.1" style="padding:0.5pt 0.0pt;">Text-document</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T4.6.2.1.2.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T4.6.2.1.2.2.1">Enc-VQA</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.2.1.2.3" style="padding:0.5pt 0.0pt;">68.1</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.2.1.2.4" style="padding:0.5pt 0.0pt;">79.4</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.2.1.2.5" style="padding:0.5pt 0.0pt;">80.2</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.2.1.2.6" style="padding:0.5pt 0.0pt;">72.3</span></span>
<span class="ltx_tr" id="S4.T4.6.2.1.3">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T4.6.2.1.3.1" style="padding:0.5pt 0.0pt;">+ Interleaved</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T4.6.2.1.3.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.3.2.1">69.7</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T4.6.2.1.3.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.3.3.1">80.1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T4.6.2.1.3.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.3.4.1">80.6</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T4.6.2.1.3.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.3.5.1">73.6</span></span></span>
<span class="ltx_tr" id="S4.T4.6.2.1.4">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T4.6.2.1.4.1" style="padding:0.5pt 0.0pt;">Text-document</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T4.6.2.1.4.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text" id="S4.T4.6.2.1.4.2.1">ViQuAE</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.2.1.4.3" style="padding:0.5pt 0.0pt;">27.8</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.2.1.4.4" style="padding:0.5pt 0.0pt;">50.2</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.2.1.4.5" style="padding:0.5pt 0.0pt;">57.7</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T4.6.2.1.4.6" style="padding:0.5pt 0.0pt;">35.0</span></span>
<span class="ltx_tr" id="S4.T4.6.2.1.5">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T4.6.2.1.5.1" style="padding:0.5pt 0.0pt;">+ Interleaved</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.6.2.1.5.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.5.2.1">29.9</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.6.2.1.5.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.5.3.1">50.9</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.6.2.1.5.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.5.4.1">59.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T4.6.2.1.5.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.6.2.1.5.5.1">36.7</span></span></span>
</span>
</span></span></p>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px2.p1.1">We further expand our experiments to two additional IR datasets, the InfoSeek and ViQuAE. As shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T3" title="In Interleaved format enhances document retrieval across modalities. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px2.p1.1.1" style="color:#DB143D;">(a)</span>, our proposed retriever consistently surpasses the Text-document baseline in document retrieval with multimodal queries. Specifically, this leads to 50.0% and 29.6% improvements in the R@1 score, and 40.2% and 25.7% improvements in the MRR@10 score for the InfoSeek and ViQuAE, respectively. We also examine the impact of interleaved documents on textual retrieval tasks, where queries consist solely of text, and report the results inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T3" title="In Interleaved format enhances document retrieval across modalities. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">3</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px2.p1.1.2" style="color:#DB143D;">(b)</span>. Then, the results demonstrate that the interleaved format offers advantages in retrieval of textual queries as well, resulting in 4.3% and 1.3% improvements in the R@1 score and 3.0% and 1.1% improvements in the MRR@10 score for the Encyclopedic-VQA and ViQuAE, respectively. We attribute these gains to the integration of multimodal content within documents, enabling the VLM to capture the multimodal alignment and to exploit its pre-existing knowledge for more effective document representationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Xu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib44" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Interleaved format is also beneficial in section retrieval.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px3.p1.1">Similarly, we evaluate the efficacy of our approach in section retrieval across both multimodal and textual queries, using the Encyclopedic-VQA and ViQuAE datasets. First, in section retrieval with multimodal queries, our model attains 4.2% improvement in the R@1 score and 3.3% improvement in the MRR@10 score for the Encyclopedic VQA, as shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T4" title="In Interleaved format enhances document retrieval across modalities. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px3.p1.1.1" style="color:#DB143D;">(a)</span>. Similarly, in section retrieval with textual queries, our model achieves 2.3% and 7.5% improvements in the R@1 score and 1.8% and 4.9% improvements in the MRR@10 score for the Encyclopedic and ViQuAE datasets, as shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T4" title="In Interleaved format enhances document retrieval across modalities. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px3.p1.1.2" style="color:#DB143D;">(b)</span>. Overall, the design of our Interleaved rerankers exhibit superior or comparable performance to the Text-document rerankers. However, since the rerankers assess query relevance using a single section, they may lack access to broader contextual information from a document, which limits the potential performance gain compared to the retrievers. Nonetheless, the multimodal content interleaved within documents improves the rerankerâ€™s ability to evaluate the relevance of the query to individual sections.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS3.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Information retrieval of tabular contents in interleaved documents is challenging.</h4>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>Document and section retrieval results for tables, where Zero-shot denotes a model finetuned on Encyclopedic-VQA but not trained on the target dataset. Finetuned refers to additional training of the model on the target dataset. <span class="ltx_text ltx_font_bold" id="S4.T5.4.1">(a)</span>: Results for tabular document retrieval on the Open-WikiTable (OWT) dataset. <span class="ltx_text ltx_font_bold" id="S4.T5.5.2">(b)</span>: Textual section retrieval results on the ViQuAE dataset and tabular section retrieval results on the OWT dataset. <span class="ltx_text ltx_font_bold" id="S4.T5.6.3">(c)</span>: Reranker accuracy (Acc@1) of a classification task that identifies the section containing the query-associated table within a gold document containing multiple tables. Random indicates random selection in the task.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T5.7" style="width:166.9pt;">
<p class="ltx_p ltx_minipage ltx_align_middle" id="S4.T5.7.1" style="width:433.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.1">(a) Document Retrieval for Tables
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T5.7.1.2" style="width:397.5pt;height:122.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(111.4pt,-34.4pt) scale(2.27494935207864,2.27494935207864) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.7.1.2.1">
<span class="ltx_tr" id="S4.T5.7.1.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T5.7.1.2.1.1.1" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.1.1.1">Model</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.7.1.2.1.1.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.1.2.1">R@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.7.1.2.1.1.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.1.3.1">R@10</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.7.1.2.1.1.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.1.4.1">R@100</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.7.1.2.1.1.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.1.5.1">MRR@10</span></span></span>
<span class="ltx_tr" id="S4.T5.7.1.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T5.7.1.2.1.2.1" style="padding-left:0.0pt;padding-right:0.0pt;">Zero-shot</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.7.1.2.1.2.2" style="padding-left:0.0pt;padding-right:0.0pt;">29.4</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.7.1.2.1.2.3" style="padding-left:0.0pt;padding-right:0.0pt;">58.0</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.7.1.2.1.2.4" style="padding-left:0.0pt;padding-right:0.0pt;">86.0</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.7.1.2.1.2.5" style="padding-left:0.0pt;padding-right:0.0pt;">38.1</span></span>
<span class="ltx_tr" id="S4.T5.7.1.2.1.3">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T5.7.1.2.1.3.1" style="padding-left:0.0pt;padding-right:0.0pt;">Finetuned</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.7.1.2.1.3.2" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.3.2.1">55.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.7.1.2.1.3.3" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.3.3.1">84.1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.7.1.2.1.3.4" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.3.4.1">93.5</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.7.1.2.1.3.5" style="padding-left:0.0pt;padding-right:0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.1.2.1.3.5.1">66.1</span></span></span>
</span>
</span></span></p>
<p class="ltx_p ltx_minipage ltx_align_middle" id="S4.T5.7.2" style="width:433.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.2.1">(c) Tabular Classification
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T5.7.2.2" style="width:397.5pt;height:86.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(116.3pt,-25.4pt) scale(2.41183788185894,2.41183788185894) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.7.2.2.1">
<span class="ltx_tr" id="S4.T5.7.2.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T5.7.2.2.1.1.1" style="padding:-0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.2.2.1.1.1.1">Model</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.7.2.2.1.1.2" style="padding:-0.5pt 0.0pt;">Random</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.7.2.2.1.1.3" style="padding:-0.5pt 0.0pt;">Zero-shot</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.7.2.2.1.1.4" style="padding:-0.5pt 0.0pt;">Finetuned</span></span>
<span class="ltx_tr" id="S4.T5.7.2.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t" id="S4.T5.7.2.2.1.2.1" style="padding:-0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.2.2.1.2.1.1">Acc@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.7.2.2.1.2.2" style="padding:-0.5pt 0.0pt;">11.9</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.7.2.2.1.2.3" style="padding:-0.5pt 0.0pt;">9.3</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t" id="S4.T5.7.2.2.1.2.4" style="padding:-0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.7.2.2.1.2.4.1">56.5</span></span></span>
</span>
</span></span></p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T5.8" style="width:222.6pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.1">(b) Section Retrieval for Tables
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T5.8.2" style="width:397.5pt;height:144.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(75.2pt,-27.4pt) scale(1.60870466042224,1.60870466042224) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T5.8.2.1">
<span class="ltx_tr" id="S4.T5.8.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T5.8.2.1.1.1" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.1.1.1">Model</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.8.2.1.1.2" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.1.2.1">Modality</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.8.2.1.1.3" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.1.3.1">Dataset</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.8.2.1.1.4" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.1.4.1">R@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.8.2.1.1.5" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.1.5.1">R@10</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.8.2.1.1.6" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.1.6.1">R@20</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T5.8.2.1.1.7" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.1.7.1">MRR@10</span></span></span>
<span class="ltx_tr" id="S4.T5.8.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T5.8.2.1.2.1" style="padding:1.9pt 0.0pt;">Zero-shot</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T5.8.2.1.2.2" style="padding:1.9pt 0.0pt;"><span class="ltx_text" id="S4.T5.8.2.1.2.2.1">Text</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T5.8.2.1.2.3" style="padding:1.9pt 0.0pt;"><span class="ltx_text" id="S4.T5.8.2.1.2.3.1">ViQuAE</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.2.1.2.4" style="padding:1.9pt 0.0pt;">20.3</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.2.1.2.5" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.2.5.1" style="color:#FF0000;">49.0</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.2.1.2.6" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.2.6.1" style="color:#FF0000;">57.7</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.2.1.2.7" style="padding:1.9pt 0.0pt;">28.9</span></span>
<span class="ltx_tr" id="S4.T5.8.2.1.3">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T5.8.2.1.3.1" style="padding:1.9pt 0.0pt;">Finetuned</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T5.8.2.1.3.2" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.3.2.1">29.9</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T5.8.2.1.3.3" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.3.3.1">50.9</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T5.8.2.1.3.4" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.3.4.1">59.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T5.8.2.1.3.5" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.3.5.1">36.7</span></span></span>
<span class="ltx_tr" id="S4.T5.8.2.1.4">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T5.8.2.1.4.1" style="padding:1.9pt 0.0pt;">Zero-shot</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T5.8.2.1.4.2" style="padding:1.9pt 0.0pt;"><span class="ltx_text" id="S4.T5.8.2.1.4.2.1">Table</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t ltx_rowspan ltx_rowspan_2" id="S4.T5.8.2.1.4.3" style="padding:1.9pt 0.0pt;"><span class="ltx_text" id="S4.T5.8.2.1.4.3.1">OWT</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.2.1.4.4" style="padding:1.9pt 0.0pt;">5.9</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.2.1.4.5" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.4.5.1" style="color:#FF0000;">20.5</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.2.1.4.6" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.4.6.1" style="color:#FF0000;">29.4</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T5.8.2.1.4.7" style="padding:1.9pt 0.0pt;">9.1</span></span>
<span class="ltx_tr" id="S4.T5.8.2.1.5">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T5.8.2.1.5.1" style="padding:1.9pt 0.0pt;">Finetuned</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.8.2.1.5.2" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.5.2.1">8.4</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.8.2.1.5.3" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.5.3.1">36.7</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.8.2.1.5.4" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.5.4.1">52.8</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T5.8.2.1.5.5" style="padding:1.9pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.8.2.1.5.5.1">15.2</span></span></span>
</span>
</span></span></p>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS3.SSS0.Px4.p1.1">We explore a retrieval task for tabular data, whose goal is to select the document or section containing the target table relevant to the input query. Specifically, we use the Open-WikiTable dataset to train the retriever and reranker, and then compare these trained models (Finetuned) with the models trained on the Encyclopedic-VQA dataset (Zero-shot). Then, as shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T5" title="In Information retrieval of tabular contents in interleaved documents is challenging. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px4.p1.1.1" style="color:#DB143D;">(a)</span>, despite Open-WikiTable consisting of only 3.2k training samples, the Finetuned retriever achieves strong retrieval performance. Meanwhile, the Zero-shot retriever demonstrates only about half of the R@1 score and the MRR@10 score of the Finetuned retriever, though it remains competitive in R@100.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px4.p2">
<p class="ltx_p" id="S4.SS3.SSS0.Px4.p2.1">In contrast, the performance trends for the rerankers exhibit notable differences. The discrepancies in R@10 and R@20 scores between the Zero-shot and Finetuned retrievers, denoted in color <span class="ltx_text" id="S4.SS3.SSS0.Px4.p2.1.1" style="color:#FF0000;">red</span> inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T5" title="In Information retrieval of tabular contents in interleaved documents is challenging. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px4.p2.1.2" style="color:#DB143D;">(b)</span>, are much more pronounced in the Open-WikiTable (table retrieval) experiments than the ones in the ViQuAE (text retrieval) experiments. This highlights a substantial difference between textual and tabular modalities, despite both being represented using word tokens. This suggests that these two modalities may require different handling for retrieval, which we leave as future work.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.SSS0.Px4.p3">
<p class="ltx_p" id="S4.SS3.SSS0.Px4.p3.1">Notably, the R@1 scores for tabular section retrieval are significantly lower than those for textual section retrieval (<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T5" title="In Information retrieval of tabular contents in interleaved documents is challenging. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px4.p3.1.1" style="color:#DB143D;">(b)</span>). To better understand the difficulty of identifying the query-relevant table, we use the Open-WikiTable dataset and design a classification task. In this task, both Zero-shot and Finetuned rerankers are provided with a golden document â€” a document containing the target of the input query â€” and should identify the section that contains the target table. Notably, this setup isolates the rerankerâ€™s ability to locate the target within a golden document. To ensure that the difficulty of the task is accurately assessed, we focus on documents containing multiple tables. Then, as shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T5" title="In Information retrieval of tabular contents in interleaved documents is challenging. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px4.p3.1.2" style="color:#DB143D;">(c)</span>, the Zero-shot reranker performs similarly to a random selection, failing to find the correct section. This accounts for its low R@10 and R@20 scores in the tabular retrieval task (<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T5" title="In Information retrieval of tabular contents in interleaved documents is challenging. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px4.p3.1.3" style="color:#DB143D;">(b)</span>). In aggregate, while the Finetuned reranker shows improved performance, it still misclassifies nearly half of the tables due to the high similarity between tables within the same document. When combined with tables from other documents, this further complicates the task of identifying the exact query-relevant table, as shown in Finetuned rerankerâ€™s low score inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T5" title="In Information retrieval of tabular contents in interleaved documents is challenging. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">5</span></a> <span class="ltx_text" id="S4.SS3.SSS0.Px4.p3.1.4" style="color:#DB143D;">(b)</span>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Further Analysis and Ablation</h3>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">More sections enhance document retrieval performance but raise computational costs.</h4>
<figure class="ltx_figure" id="S4.SS4.SSS0.Px1.2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_top" id="S4.F3" style="width:182.1pt;">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Trade-off between retrieval performance and training cost.</figcaption>
<br class="ltx_break ltx_break"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="447" id="S4.SS4.SSS0.Px1.1.1.g1" src="x3.png" width="830"/>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_center ltx_align_top" id="S4.SS4.SSS0.Px1.2.fig1" style="width:242.8pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure class="ltx_table ltx_figure_panel" id="S4.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>Impact of negative sample selection in the reranker training. Top-K selects the top-k retrieved sections from the retriever as negatives. In-batch uses negatives from other sections in the same batch. In-document selects negatives from sections within the document containing the positive section.</figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_inline-block ltx_figure_panel ltx_align_center ltx_transformed_outer" id="S4.SS4.SSS0.Px1.2.fig1.1" style="width:433.6pt;height:146.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(110.3pt,-37.3pt) scale(2.03577083318707,2.03577083318707) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.SS4.SSS0.Px1.2.fig1.1.1">
<tr class="ltx_tr" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.1" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.1.1">Negative</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.2" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.2.1">R@1</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.3" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.3.1">R@10</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.4" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.4.1">R@20</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.5" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.1.5.1">MRR@10</span></td>
</tr>
<tr class="ltx_tr" id="S4.SS4.SSS0.Px1.2.fig1.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.SS4.SSS0.Px1.2.fig1.1.1.2.1" style="padding:1pt 0.0pt;">Top-K</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.SS4.SSS0.Px1.2.fig1.1.1.2.2" style="padding:1pt 0.0pt;">38.1</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.SS4.SSS0.Px1.2.fig1.1.1.2.3" style="padding:1pt 0.0pt;">53.7</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.SS4.SSS0.Px1.2.fig1.1.1.2.4" style="padding:1pt 0.0pt;">55.3</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.SS4.SSS0.Px1.2.fig1.1.1.2.5" style="padding:1pt 0.0pt;">44.4</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.SSS0.Px1.2.fig1.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.SS4.SSS0.Px1.2.fig1.1.1.3.1" style="padding:1pt 0.0pt;">In-batch</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.SS4.SSS0.Px1.2.fig1.1.1.3.2" style="padding:1pt 0.0pt;">39.5</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.SS4.SSS0.Px1.2.fig1.1.1.3.3" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.3.3.1">53.8</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.SS4.SSS0.Px1.2.fig1.1.1.3.4" style="padding:1pt 0.0pt;">55.4</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.SS4.SSS0.Px1.2.fig1.1.1.3.5" style="padding:1pt 0.0pt;">45.0</td>
</tr>
<tr class="ltx_tr" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4.1" style="padding:1pt 0.0pt;">In-document (Ours)</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4.2" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4.2.1">42.4</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4.3" style="padding:1pt 0.0pt;">53.6</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4.4" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4.4.1">55.7</span></td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4.5" style="padding:1pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.SS4.SSS0.Px1.2.fig1.1.1.4.5.1">46.3</span></td>
</tr>
</table>
</span></div>
</div>
</div>
</figure>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px1.p1.1">InÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T2" title="In Interleaved format improves document retrieval. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T2" title="In Interleaved format improves document retrieval. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">2</span></a>, we observe that using the comprehensive multimodal content and enriched contextual information significantly improves document retrieval performance. Accordingly, we anticipate further improvements as more sections are gathered to represent the document, during training. To validate this, we measure the document retrieval performance with varying the number of sections per document on the InfoSeek dataset for training. The results shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS4.SSS0.Px1" title="More sections enhance document retrieval performance but raise computational costs. â€£ 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4.4</span></a> then indicate that incorporating more sections raises the MRR@10 score from 7.5 to 15.7. However, this performance boost comes with a clear trade-off; as the number of sections increases, the retriever must process additional end-of-section tokens, leading to higher GPU memory consumption. To balance resource limitations and performance gains, we select four sections per document for all experiments.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Sections from the same document act as effective negatives to enhance reranker performance.</h4>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px2.p1.1">We explore another method to improve IR effectiveness by leveraging the entire document. Specifically, we investigate the use of sections from the same document as negatives for reranker training, namely In-document. We compare this approach with traditional methods, including Top-K, which selects the top-K retrieved sections as negatives, and In-batch, which uses the positive sections for other samples in the same batch as negatives. After training rerankers with each method, we evaluate section retrieval on the Encyclopedic-VQA dataset. The results shown inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.SS4.SSS0.Px1" title="More sections enhance document retrieval performance but raise computational costs. â€£ 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4.4</span></a> demonstrate that our In-document approach achieves superior R@1 and MRR@10 scores. This suggests that the use of sections from the same document as negatives provides natural, cost-effective advantages thanks to their high similarity to the positive section. However, it does not consistently outperform the other methods on the R@10 score. We hypothesize that this inconsistency may arise from the strengths of each method: the In-document approach excels at distinguishing sections from the same document, while Top-K and In-batch methods better differentiate sections from different documents.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">BCE loss applied to each section produces the best reranker performance.</h4>
<figure class="ltx_table" id="S4.T7">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 7: </span>Performance of different reranker designs. Contrastive follows the same training pipeline used for retrievers but here it uses sections for retrieval. Document+BCE concatenates the input query with multiple sections gathered from the same document and uses BCE loss to train the re-ranker. Section+BCE concatenates the query with each section of the document, and the re-ranker is trained with the BCE loss.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T7.1" style="width:194.8pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.1">(a) Section Retrieval for Multimodal Queries
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T7.1.2" style="width:397.5pt;height:131.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(89.6pt,-29.6pt) scale(1.82100177000059,1.82100177000059) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T7.1.2.1">
<span class="ltx_tr" id="S4.T7.1.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T7.1.2.1.1.1" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.1.1.1">Train Loss</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T7.1.2.1.1.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.1.2.1">R@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T7.1.2.1.1.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.1.3.1">R@10</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T7.1.2.1.1.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.1.4.1">R@20</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T7.1.2.1.1.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.1.5.1">MRR@10</span></span></span>
<span class="ltx_tr" id="S4.T7.1.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T7.1.2.1.2.1" style="padding:0.5pt 0.0pt;">Contrastive</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T7.1.2.1.2.2" style="padding:0.5pt 0.0pt;">3.6</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T7.1.2.1.2.3" style="padding:0.5pt 0.0pt;">15.0</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T7.1.2.1.2.4" style="padding:0.5pt 0.0pt;">21.3</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T7.1.2.1.2.5" style="padding:0.5pt 0.0pt;">6.5</span></span>
<span class="ltx_tr" id="S4.T7.1.2.1.3">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T7.1.2.1.3.1" style="padding:0.5pt 0.0pt;">Document+BCE</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T7.1.2.1.3.2" style="padding:0.5pt 0.0pt;">13.6</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T7.1.2.1.3.3" style="padding:0.5pt 0.0pt;">29.6</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T7.1.2.1.3.4" style="padding:0.5pt 0.0pt;">32.9</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T7.1.2.1.3.5" style="padding:0.5pt 0.0pt;">24.1</span></span>
<span class="ltx_tr" id="S4.T7.1.2.1.4">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T7.1.2.1.4.1" style="padding:0.5pt 0.0pt;">Section+BCE (Ours)</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T7.1.2.1.4.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.4.2.1">42.4</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T7.1.2.1.4.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.4.3.1">53.6</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T7.1.2.1.4.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.4.4.1">55.7</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T7.1.2.1.4.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.1.2.1.4.5.1">46.3</span></span></span>
</span>
</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.T7.2" style="width:194.8pt;">
<p class="ltx_p ltx_align_center" id="S4.T7.2.1"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.1">(b) Section Retrieval for Textual Queries
<br class="ltx_break"/></span>
<span class="ltx_inline-block ltx_transformed_outer" id="S4.T7.2.1.2" style="width:397.5pt;height:131.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(89.6pt,-29.6pt) scale(1.82100177000059,1.82100177000059) ;">
<span class="ltx_tabular ltx_align_middle" id="S4.T7.2.1.2.1">
<span class="ltx_tr" id="S4.T7.2.1.2.1.1">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="S4.T7.2.1.2.1.1.1" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.1.1.1">Train Loss</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T7.2.1.2.1.1.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.1.2.1">R@1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T7.2.1.2.1.1.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.1.3.1">R@10</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T7.2.1.2.1.1.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.1.4.1">R@20</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="S4.T7.2.1.2.1.1.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.1.5.1">MRR@10</span></span></span>
<span class="ltx_tr" id="S4.T7.2.1.2.1.2">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="S4.T7.2.1.2.1.2.1" style="padding:0.5pt 0.0pt;">Contrastive</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T7.2.1.2.1.2.2" style="padding:0.5pt 0.0pt;">13.6</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T7.2.1.2.1.2.3" style="padding:0.5pt 0.0pt;">37.7</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T7.2.1.2.1.2.4" style="padding:0.5pt 0.0pt;">45.1</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S4.T7.2.1.2.1.2.5" style="padding:0.5pt 0.0pt;">20.6</span></span>
<span class="ltx_tr" id="S4.T7.2.1.2.1.3">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="S4.T7.2.1.2.1.3.1" style="padding:0.5pt 0.0pt;">Document+BCE</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T7.2.1.2.1.3.2" style="padding:0.5pt 0.0pt;">23.8</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T7.2.1.2.1.3.3" style="padding:0.5pt 0.0pt;">43.4</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T7.2.1.2.1.3.4" style="padding:0.5pt 0.0pt;">47.2</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S4.T7.2.1.2.1.3.5" style="padding:0.5pt 0.0pt;">39.1</span></span>
<span class="ltx_tr" id="S4.T7.2.1.2.1.4">
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="S4.T7.2.1.2.1.4.1" style="padding:0.5pt 0.0pt;">Section+BCE (Ours)</span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T7.2.1.2.1.4.2" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.4.2.1">69.7</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T7.2.1.2.1.4.3" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.4.3.1">80.1</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T7.2.1.2.1.4.4" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.4.4.1">80.6</span></span>
<span class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="S4.T7.2.1.2.1.4.5" style="padding:0.5pt 0.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T7.2.1.2.1.4.5.1">73.6</span></span></span>
</span>
</span></span></p>
</div>
</div>
</div>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px3.p1.2">In our reranker design, we apply BCE loss using the query concatenated with each document section (Section+BCE). We also explore alternative training objectives to identify the most effective approach for section retrieval in interleaved documents. One such objective is contrastive loss (Contrastive). This approach is similar to the retriever, but the retrieval unit is a section. Additionally, we also explore a variant of the BCE loss (Document+BCE), where, unlike Section+BCE, the query is concatenated with multiple sections from the same document, including both positive and negative sections. An <math alttext="[\texttt{EoS}]" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS4.SSS0.Px3.p1.1.m1.1a"><mrow id="S4.SS4.SSS0.Px3.p1.1.m1.1.2.2" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.2.1.cmml"><mo id="S4.SS4.SSS0.Px3.p1.1.m1.1.2.2.1" stretchy="false" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.2.1.1.cmml">[</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS4.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1a.cmml">EoS</mtext><mo id="S4.SS4.SSS0.Px3.p1.1.m1.1.2.2.2" stretchy="false" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px3.p1.1.m1.1b"><apply id="S4.SS4.SSS0.Px3.p1.1.m1.1.2.1.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.2.2"><csymbol cd="latexml" id="S4.SS4.SSS0.Px3.p1.1.m1.1.2.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.2.2.1">delimited-[]</csymbol><ci id="S4.SS4.SSS0.Px3.p1.1.m1.1.1a.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1"><mtext class="ltx_mathvariant_monospace" id="S4.SS4.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.1.m1.1.1">EoS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.1.m1.1c">[\texttt{EoS}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px3.p1.1.m1.1d">[ EoS ]</annotation></semantics></math> token is appended to each section, and the Document+BCE follows the same BCE loss calculation of the Section+BCE using its <math alttext="[\texttt{EoS}]" class="ltx_Math" display="inline" id="S4.SS4.SSS0.Px3.p1.2.m2.1"><semantics id="S4.SS4.SSS0.Px3.p1.2.m2.1a"><mrow id="S4.SS4.SSS0.Px3.p1.2.m2.1.2.2" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.2.1.cmml"><mo id="S4.SS4.SSS0.Px3.p1.2.m2.1.2.2.1" stretchy="false" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.2.1.1.cmml">[</mo><mtext class="ltx_mathvariant_monospace" id="S4.SS4.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1a.cmml">EoS</mtext><mo id="S4.SS4.SSS0.Px3.p1.2.m2.1.2.2.2" stretchy="false" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.2.1.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.SSS0.Px3.p1.2.m2.1b"><apply id="S4.SS4.SSS0.Px3.p1.2.m2.1.2.1.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.2.2"><csymbol cd="latexml" id="S4.SS4.SSS0.Px3.p1.2.m2.1.2.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.2.2.1">delimited-[]</csymbol><ci id="S4.SS4.SSS0.Px3.p1.2.m2.1.1a.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1"><mtext class="ltx_mathvariant_monospace" id="S4.SS4.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS4.SSS0.Px3.p1.2.m2.1.1">EoS</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.SSS0.Px3.p1.2.m2.1c">[\texttt{EoS}]</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.SSS0.Px3.p1.2.m2.1d">[ EoS ]</annotation></semantics></math> outputs. This design allows the Document+BCE reranker to leverage the long-context understanding of VLMs to improve section retrieval in interleaved documents.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px3.p2">
<p class="ltx_p" id="S4.SS4.SSS0.Px3.p2.1">Then, inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T7" title="In BCE loss applied to each section produces the best reranker performance. â€£ 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">7</span></a>, we compare the section retrieval performance of different reranker designs on the Encyclopedic-VQA dataset. We find that the Contrastive reranker performs the worst, indicating that directly concatenating the query with the section at the input level provides more effective clues for query-section relevance assessment. Notably, this observation is consistent with conventional reranker approaches. Further, the Document+BCE reranker underperforms compared to the Section+BCE reranker, likely due to training constraints. Specifically, while the evaluation phase uses all sections within each document, the training phase is limited to a maximum of four sections per document, with an average of eight sections per document. Such a mismatch may degrade the modelâ€™s performance. Building on concurrent discoveriesÂ <cite class="ltx_cite ltx_citemacro_citep">(Jiang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib15" title="">2024</a>; Lee etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib19" title="">2024</a>)</cite>, addressing these training constraints will potentially open up new reranker designs that can better handle long, interleaved documents using VLMs, ultimately improving section retrieval performance.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS4.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Rerankers require much larger datasets than retrievers.</h4>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F4.2" style="width:190.8pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="225" id="S4.F4.1.g1" src="x4.png" width="830"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S4.F4.2.g2" src="x5.png" width="830"/>
<p class="ltx_p ltx_align_center" id="S4.F4.2.1"><span class="ltx_text ltx_font_bold" id="S4.F4.2.1.1">(a) Retriever performance</span></p>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S4.F4.4" style="width:190.8pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="225" id="S4.F4.3.g1" src="x6.png" width="830"/><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="225" id="S4.F4.4.g2" src="x7.png" width="830"/>
<p class="ltx_p ltx_align_center" id="S4.F4.4.1"><span class="ltx_text ltx_font_bold" id="S4.F4.4.1.1">(b) Reranker performance</span></p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Retrieval performance with different dataset sizes for training. <span class="ltx_text ltx_font_bold" id="S4.F4.7.1">(a)</span>: When training a retriever, large datasets rather deteriorate the retrieval performance as it may be overfitted, resulting in low generalization. <span class="ltx_text ltx_font_bold" id="S4.F4.8.2">(b)</span>: On the other hand, a larger dataset size is beneficial to training a re-ranker.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.SS4.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS4.SSS0.Px4.p1.1">We analyze the effect of different dataset sizes for training on retriever and reranker performance. To achieve this, we randomly prune samples in the Encyclopedic-VQA dataset at various ratios and report the performance of models trained on these subsets. InÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.F4" title="In Rerankers require much larger datasets than retrievers. â€£ 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> <span class="ltx_text" id="S4.SS4.SSS0.Px4.p1.1.1" style="color:#DB143D;">(a)</span>, we observe that too many samples can degrade retrieval performance. Also, retrieval of textual queries requires fewer samples to reach its optimal performance compared to multimodal retrieval. Similarly, inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.F4" title="In Rerankers require much larger datasets than retrievers. â€£ 4.4 Further Analysis and Ablation â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Figure</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> <span class="ltx_text" id="S4.SS4.SSS0.Px4.p1.1.2" style="color:#DB143D;">(b)</span>, section retrieval for multimodal queries requires 10% of the dataset to achieve 80% of the full-dataset performance, while section retrieval for textual queries needs only 5%. These observations suggest that additional modalities increase the need for more data. This accounts for the inferior performance of the interleaved format in the ViQuAE experiments (<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T4" title="In Interleaved format enhances document retrieval across modalities. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> <span class="ltx_text" id="S4.SS4.SSS0.Px4.p1.1.3" style="color:#DB143D;">(a)</span>). The ViQuAE dataset, at only 2.2% of the size of Encyclopedic-VQA, may be small for the reranker to effectively learn multimodal query-section alignments. We also observe that section retrieval is more challenging, with more samples improving the rerankerâ€™s performance. This explains why the ViQuAE reranker has much lower section retrieval scores compared to the one trained on the Encyclopedic-VQA (<a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4.T4" title="In Interleaved format enhances document retrieval across modalities. â€£ 4.3 Results and Discussion â€£ 4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> <span class="ltx_text" id="S4.SS4.SSS0.Px4.p1.1.4" style="color:#DB143D;">(b)</span>). Given the challenge of obtaining large query-section pair samples, exploring more effective reranker training pipelines is necessary.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we introduced IDentIfy, a novel IR framework designed to address the limitations of conventional methods that rely on solely textual content of documents and their segmented passages. Specifically, our approach sits on top of recent VLMs, which enables integration and representation of diverse multimodal content (including text, images, and tables) into a unified document representation. Also, unlike previous strategies that segment documents at the passage level, our method merges these segments to maintain the documentâ€™s structural coherence, while further introducing a reranking strategy for precise identification of relevant sections. Extensive experiments across various IR datasets demonstrated that IDentIfy consistently outperforms existing baselines, confirming that the interleaved multimodal representation significantly enhances the quality of the document retrieval. We believe IDentIfy represents a crucial step toward more comprehensive and contextually aware IR systems, capable of handling the increasing multimodality of modern information sources.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Reproducibility Statement</h2>
<div class="ltx_para ltx_noindent" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our codes are based on publicly available LLaVA-NeXTÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib23" title="">2024</a>)</cite>. The experimental setup and details can be found inÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#S4" title="4 Experiments â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Section</span>Â <span class="ltx_text ltx_ref_tag">4</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A1" title="Appendix A Implementation Details â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Appendix</span>Â <span class="ltx_text ltx_ref_tag">A</span></a>. The experiments are conducted with publicly available datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Mensink etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib29" title="">2023</a>; Chen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib4" title="">2023</a>; Lerner etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib20" title="">2022</a>; Kweon etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#bib.bib18" title="">2023</a>)</cite>. We have included our codes in the supplementary material and will publicly release our code.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baek etÂ al. (2023)</span>
<span class="ltx_bibblock">
Jinheon Baek, AlhamÂ Fikri Aji, Jens Lehmann, and SungÂ Ju Hwang.

</span>
<span class="ltx_bibblock">Direct fact retrieval from knowledge graphs without entity linking.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the Association for Computational Linguistics (ACL)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caffagni etÂ al. (2024)</span>
<span class="ltx_bibblock">
Davide Caffagni, Federico Cocchi, Nicholas Moratelli, Sara Sarto, Marcella Cornia, Lorenzo Baraldi, and Rita Cucchiara.

</span>
<span class="ltx_bibblock">Wiki-llava: Hierarchical retrieval-augmented generation for multimodal llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2404.15406</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2404.15406" title="">https://doi.org/10.48550/arXiv.2404.15406</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2024)</span>
<span class="ltx_bibblock">
PeterÂ Baile Chen, YiÂ Zhang, and Dan Roth.

</span>
<span class="ltx_bibblock">Is table retrieval a solved problem? exploring join-aware multi-table retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the Association for Computational Linguistics (ACL)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yang Chen, Hexiang Hu, YiÂ Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-Wei Chang.

</span>
<span class="ltx_bibblock">Can pre-trained vision and language models answer visual information-seeking questions?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony MengÂ Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C.Â H. Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">BERT: pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yihao Ding, Kaixuan Ren, Jiabin Huang, Siwen Luo, and SoyeonÂ Caren Han.

</span>
<span class="ltx_bibblock">PDF-MVQA: A dataset for multimodal information retrieval in pdf-based visual question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2404.12720</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2404.12720" title="">https://doi.org/10.48550/arXiv.2404.12720</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Faysse etÂ al. (2024)</span>
<span class="ltx_bibblock">
Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, CÃ©line Hudelot, and Pierre Colombo.

</span>
<span class="ltx_bibblock">Colpali: Efficient document retrieval with vision language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">arXiv preprint arXiv:2407.01449</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2407.01449" title="">https://doi.org/10.48550/arXiv.2407.01449</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, YiÂ Dai, Jiawei Sun, Qianyu Guo, Meng Wang, and Haofen Wang.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2312.10997</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2312.10997" title="">https://doi.org/10.48550/arXiv.2312.10997</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guu etÂ al. (2020)</span>
<span class="ltx_bibblock">
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.

</span>
<span class="ltx_bibblock">REALM: retrieval-augmented language model pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2002.08909</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2002.08909" title="">https://arxiv.org/abs/2002.08909</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Herzig etÂ al. (2021)</span>
<span class="ltx_bibblock">
Jonathan Herzig, Thomas MÃ¼ller, Syrine Krichene, and JulianÂ Martin Eisenschlos.

</span>
<span class="ltx_bibblock">Open domain question answering over tables via dense retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu etÂ al. (2022)</span>
<span class="ltx_bibblock">
EdwardÂ J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, LuÂ Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the International Conference on Learning Representations (ICLR)</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Soyeong Jeong, Jinheon Baek, Sukmin Cho, SungÂ Ju Hwang, and Jong Park.

</span>
<span class="ltx_bibblock">Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Soyeong Jeong, Jinheon Baek, Sukmin Cho, SungÂ Ju Hwang, and JongÂ C. Park.

</span>
<span class="ltx_bibblock">Database-augmented query representation for information retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2406.16013</em>, 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2406.16013" title="">https://doi.org/10.48550/arXiv.2406.16013</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Ziyan Jiang, Xueguang Ma, and Wenhu Chen.

</span>
<span class="ltx_bibblock">Longrag: Enhancing retrieval-augmented generation with long-context llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2406.15319</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2406.15319" title="">https://doi.org/10.48550/arXiv.2406.15319</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jones (2004)</span>
<span class="ltx_bibblock">
KarenÂ SpÃ¤rck Jones.

</span>
<span class="ltx_bibblock">A statistical interpretation of term specificity and its application in retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">J. Documentation</em>, 60(5):493â€“502, 2004.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1108/00220410410560573" title="">https://doi.org/10.1108/00220410410560573</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpukhin etÂ al. (2020)</span>
<span class="ltx_bibblock">
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick S.Â H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.

</span>
<span class="ltx_bibblock">Dense passage retrieval for open-domain question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kweon etÂ al. (2023)</span>
<span class="ltx_bibblock">
Sunjun Kweon, Yeonsu Kwon, Seonhee Cho, Yohan Jo, and Edward Choi.

</span>
<span class="ltx_bibblock">Open-wikitable : Dataset for open domain question answering with complex reasoning over table.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Findings of the Association for Computational Linguistics (ACL)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Anthony Chen, Zhuyun Dai, Dheeru Dua, DevendraÂ Singh Sachan, Michael Boratko, YiÂ Luan, SÃ©bastien M.Â R. Arnold, Vincent Perot, Siddharth Dalmia, Hexiang Hu, Xudong Lin, Panupong Pasupat, Aida Amini, JeremyÂ R. Cole, Sebastian Riedel, Iftekhar Naim, Ming-Wei Chang, and Kelvin Guu.

</span>
<span class="ltx_bibblock">Can long-context language models subsume retrieval, rag, sql, and more?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2406.13121</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2406.13121" title="">https://doi.org/10.48550/arXiv.2406.13121</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lerner etÂ al. (2022)</span>
<span class="ltx_bibblock">
Paul Lerner, Olivier Ferret, Camille Guinaudeau, HervÃ©Â Le Borgne, Romaric BesanÃ§on, JosÃ©Â G. Moreno, and JesÃºs LovÃ³n-Melgarejo.

</span>
<span class="ltx_bibblock">Viquae, a dataset for knowledge-based visual question answering about named entities.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">SIGIR â€™22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, Madrid, Spain, July 11 - 15, 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lerner etÂ al. (2024)</span>
<span class="ltx_bibblock">
Paul Lerner, Olivier Ferret, and Camille Guinaudeau.

</span>
<span class="ltx_bibblock">Cross-modal retrieval for knowledge-based visual question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in Information Retrieval - 46th European Conference on Information Retrieval</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al. (2020)</span>
<span class="ltx_bibblock">
Patrick S.Â H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive NLP tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Advances in Neural Information Processing Systems (NeurIPS)</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2024)</span>
<span class="ltx_bibblock">
Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, BoÂ Li, Wei Li, Zejun Ma, and Chunyuan Li.

</span>
<span class="ltx_bibblock">Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2407.07895</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2407.07895" title="">https://doi.org/10.48550/arXiv.2407.07895</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2019)</span>
<span class="ltx_bibblock">
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.

</span>
<span class="ltx_bibblock">Roberta: A robustly optimized BERT pretraining approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:1907.11692</em>, 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1907.11692" title="">http://arxiv.org/abs/1907.11692</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu etÂ al. (2021)</span>
<span class="ltx_bibblock">
Zheyuan Liu, CristianÂ Rodriguez Opazo, Damien Teney, and Stephen Gould.

</span>
<span class="ltx_bibblock">Image retrieval on real-life images with pre-trained vision-and-language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the International Conference on Computer Vision (ICCV)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Long etÂ al. (2024)</span>
<span class="ltx_bibblock">
Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen Zhou, and Jie Zhou.

</span>
<span class="ltx_bibblock">Generative multi-modal knowledge retrieval with large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the AAAI National Conference on Artificial Intelligence (AAAI)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2024a)</span>
<span class="ltx_bibblock">
Xueguang Ma, Sheng-Chieh Lin, Minghan Li, Wenhu Chen, and Jimmy Lin.

</span>
<span class="ltx_bibblock">Unifying multimodal retrieval via document screenshot embedding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2406.11251</em>, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2406.11251" title="">https://doi.org/10.48550/arXiv.2406.11251</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al. (2024b)</span>
<span class="ltx_bibblock">
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin.

</span>
<span class="ltx_bibblock">Fine-tuning llama for multi-stage text retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mensink etÂ al. (2023)</span>
<span class="ltx_bibblock">
Thomas Mensink, Jasper R.Â R. Uijlings, LluÃ­s CastrejÃ³n, Arushi Goel, Felipe Cadar, Howard Zhou, Fei Sha, AndrÃ© AraÃºjo, and Vittorio Ferrari.

</span>
<span class="ltx_bibblock">Encyclopedic VQA: visual questions about detailed properties of fine-grained categories.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the International Conference on Computer Vision (ICCV)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nowak etÂ al. (2024)</span>
<span class="ltx_bibblock">
Averi Nowak, Francesco Piccinno, and Yasemin Altun.

</span>
<span class="ltx_bibblock">Multimodal chart retrieval: A comparison of text, table and image based approaches.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2022)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Introducing chatgpt.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/blog/chatgpt" title="">https://openai.com/blog/chatgpt</a>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2303.08774</em>, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.08774" title="">https://doi.org/10.48550/arXiv.2303.08774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">GPT-4V(ision) system card.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/gpt-4v-system-card/" title="">https://openai.com/index/gpt-4v-system-card/</a>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pasupat &amp; Liang (2015)</span>
<span class="ltx_bibblock">
Panupong Pasupat and Percy Liang.

</span>
<span class="ltx_bibblock">Compositional semantic parsing on semi-structured tables.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the Association for Computational Linguistics (ACL)</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Proceedings of the International Conference on Machine Learning (ICML)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ram etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.

</span>
<span class="ltx_bibblock">In-context retrieval-augmented language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Trans. Assoc. Comput. Linguistics</em>, 11:1316â€“1331, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1162/TACLâ€œË™Aâ€œË™00605</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1162/tacl_a_00605" title="">https://doi.org/10.1162/tacl_a_00605</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Robertson etÂ al. (1994)</span>
<span class="ltx_bibblock">
StephenÂ E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford.

</span>
<span class="ltx_bibblock">Okapi at TREC-3.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of The Third Text REtrieval Conference, TREC 1994, Gaithersburg, Maryland, USA, November 2-4, 1994</em>, 1994.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah etÂ al. (2019)</span>
<span class="ltx_bibblock">
Sanket Shah, Anand Mishra, Naganand Yadati, and ParthaÂ Pratim Talukdar.

</span>
<span class="ltx_bibblock">KVQA: knowledge-aware visual question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the AAAI National Conference on Artificial Intelligence (AAAI)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi etÂ al. (2024)</span>
<span class="ltx_bibblock">
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Richard James, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih.

</span>
<span class="ltx_bibblock">REPLUG: retrieval-augmented black-box language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023a)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, AurÃ©lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2302.13971</em>, 2023a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2302.13971" title="">https://doi.org/10.48550/arXiv.2302.13971</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron etÂ al. (2023b)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, PunitÂ Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, EricÂ Michael Smith, Ranjan Subramanian, XiaoqingÂ Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, JianÂ Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, AurÃ©lien Rodriguez, Robert Stojnic, Sergey Edunov,
and Thomas Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2307.09288</em>, 2023b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2307.09288" title="">https://doi.org/10.48550/arXiv.2307.09288</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao etÂ al. (2024)</span>
<span class="ltx_bibblock">
Zilin Xiao, Ming Gong, Paola Cascante-Bonilla, Xingyao Zhang, Jie Wu, and Vicente Ordonez.

</span>
<span class="ltx_bibblock">Grounding language models for visual entity recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2402.18695</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2402.18695" title="">https://doi.org/10.48550/arXiv.2402.18695</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong etÂ al. (2021)</span>
<span class="ltx_bibblock">
Lee Xiong, Chenyan Xiong, YeÂ Li, Kwok-Fung Tang, Jialin Liu, PaulÂ N. Bennett, Junaid Ahmed, and Arnold Overwijk.

</span>
<span class="ltx_bibblock">Approximate nearest neighbor negative contrastive learning for dense text retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the International Conference on Learning Representations (ICLR)</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jialiang Xu, Michael Moor, and Jure Leskovec.

</span>
<span class="ltx_bibblock">Reverse image retrieval cues parametric memory in multimodal llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2405.18740</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2405.18740" title="">https://doi.org/10.48550/arXiv.2405.18740</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer.

</span>
<span class="ltx_bibblock">Sigmoid loss for language image pre-training.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Proceedings of the International Conference on Computer Vision (ICCV)</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, YuÂ Qiao, Dahua Lin, and Jiaqi Wang.

</span>
<span class="ltx_bibblock">Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2309.15112</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2309.15112" title="">https://doi.org/10.48550/arXiv.2309.15112</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong etÂ al. (2017)</span>
<span class="ltx_bibblock">
Victor Zhong, Caiming Xiong, and Richard Socher.

</span>
<span class="ltx_bibblock">Seq2sql: Generating structured queries from natural language using reinforcement learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">arXiv preprint arXiv:1709.00103</em>, 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1709.00103" title="">http://arxiv.org/abs/1709.00103</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou etÂ al. (2024)</span>
<span class="ltx_bibblock">
Junjie Zhou, Zheng Liu, Shitao Xiao, BoÂ Zhao, and Yongping Xiong.

</span>
<span class="ltx_bibblock">VISTA: visualized text embedding for universal multi-modal retrieval.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the Association for Computational Linguistics (ACL)</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2023)</span>
<span class="ltx_bibblock">
Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">Large language models for information retrieval: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2308.07107</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2308.07107" title="">https://doi.org/10.48550/arXiv.2308.07107</a>.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_paragraph" id="A0.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Appendix</h4>
</section>
<section class="ltx_paragraph" id="A0.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Organization</h4>
<div class="ltx_para ltx_noindent" id="A0.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A0.SS0.SSS0.Px2.p1.1">The supplementary file is organized as follows: InÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A1" title="Appendix A Implementation Details â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Appendix</span>Â <span class="ltx_text ltx_ref_tag">A</span></a>, we explain the implementation details for our experiments. InÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A2" title="Appendix B Limitations â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Appendix</span>Â <span class="ltx_text ltx_ref_tag">B</span></a>, we outline the limitations of our study.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Implementation Details </h2>
<figure class="ltx_table" id="A1.T8">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Information retrieval datasets summary.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T8.1" style="width:397.5pt;height:76pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-39.2pt,7.4pt) scale(0.835294790269776,0.835294790269776) ;">
<table class="ltx_tabular ltx_align_middle" id="A1.T8.1.1">
<tr class="ltx_tr" id="A1.T8.1.1.1">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_tt" id="A1.T8.1.1.1.1" style="padding:1.25pt 0.0pt;">Dataset</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.2" style="padding:1.25pt 0.0pt;">Query Modality</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.3" style="padding:1.25pt 0.0pt;">Target</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.4" style="padding:1.25pt 0.0pt;">Domain</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.5" style="padding:1.25pt 0.0pt;">Entities</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.6" style="padding:1.25pt 0.0pt;">Section ID</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.7" style="padding:1.25pt 0.0pt;">Train</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.8" style="padding:1.25pt 0.0pt;">Eval</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.9" style="padding:1.25pt 0.0pt;">Test</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_tt" id="A1.T8.1.1.1.10" style="padding:1.25pt 0.0pt;">Corpus size</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.2">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_t" id="A1.T8.1.1.2.1" style="padding:1.25pt 0.0pt;">Encyclopedic-VQA</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.2" style="padding:1.25pt 0.0pt;">Text, Text-Image</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.3" style="padding:1.25pt 0.0pt;">Text</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.4" style="padding:1.25pt 0.0pt;">Species, Landmarks</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.5" style="padding:1.25pt 0.0pt;">17k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.6" style="padding:1.25pt 0.0pt;">o</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.7" style="padding:1.25pt 0.0pt;">177k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.8" style="padding:1.25pt 0.0pt;">2.2k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.9" style="padding:1.25pt 0.0pt;">3.8k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="A1.T8.1.1.2.10" style="padding:1.25pt 0.0pt;">100k</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.3">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T8.1.1.3.1" style="padding:1.25pt 0.0pt;">InfoSeek</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.2" style="padding:1.25pt 0.0pt;">Text-Image</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.3" style="padding:1.25pt 0.0pt;">Text</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.4" style="padding:1.25pt 0.0pt;">Diverse</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.5" style="padding:1.25pt 0.0pt;">11k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.6" style="padding:1.25pt 0.0pt;">x</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.7" style="padding:1.25pt 0.0pt;">209k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.8" style="padding:1.25pt 0.0pt;">23k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.9" style="padding:1.25pt 0.0pt;">74k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.3.10" style="padding:1.25pt 0.0pt;">500k</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.4">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left" id="A1.T8.1.1.4.1" style="padding:1.25pt 0.0pt;">ViQuAE</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.2" style="padding:1.25pt 0.0pt;">Text, Text-Image</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.3" style="padding:1.25pt 0.0pt;">Text</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.4" style="padding:1.25pt 0.0pt;">Human</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.5" style="padding:1.25pt 0.0pt;">1k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.6" style="padding:1.25pt 0.0pt;">o</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.7" style="padding:1.25pt 0.0pt;">1.2k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.8" style="padding:1.25pt 0.0pt;">1.2k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.9" style="padding:1.25pt 0.0pt;">1.2k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="A1.T8.1.1.4.10" style="padding:1.25pt 0.0pt;">100k</td>
</tr>
<tr class="ltx_tr" id="A1.T8.1.1.5">
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_border_bb" id="A1.T8.1.1.5.1" style="padding:1.25pt 0.0pt;">Open-WikiTable</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.2" style="padding:1.25pt 0.0pt;">Text</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.3" style="padding:1.25pt 0.0pt;">Table</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.4" style="padding:1.25pt 0.0pt;">Table</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.5" style="padding:1.25pt 0.0pt;">-</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.6" style="padding:1.25pt 0.0pt;">o</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.7" style="padding:1.25pt 0.0pt;">3.3k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.8" style="padding:1.25pt 0.0pt;">0.4k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.9" style="padding:1.25pt 0.0pt;">0.4k</td>
<td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb" id="A1.T8.1.1.5.10" style="padding:1.25pt 0.0pt;">1.8k</td>
</tr>
</table>
</span></div>
</figure>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Dataset configuration</h4>
<div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px1.p1.1"><a class="ltx_ref" href="https://arxiv.org/html/2410.02729v1#A1.T8" title="In Appendix A Implementation Details â€£ Unified Multi-Modal Interleaved Document Representation for Information Retrieval"><span class="ltx_text ltx_ref_tag">Table</span>Â <span class="ltx_text ltx_ref_tag">8</span></a> summarizes the key properties of the datasets used in our experiment, including query modality, target item, entity domain, number of entities, and whether a section ID is provided to indicate the section containing the answer. Additionally, we provide the number of samples in the training, evaluation, and test splits, as well as the size of the corpus.</p>
</div>
</section>
<section class="ltx_paragraph" id="A1.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Dataset pre-processing</h4>
<div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p1.1">In our study, we leverage interleaved multimodal content from Wikipedia documents. However, the existing corpora associated with our IR datasets often lack this content, typically only including the first few words of each document. Therefore, we augment the corpora by downloading the HTML file of each Wikipedia document.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p2.1">If the dataset provides Wikipedia URLs for its corpus, we use them to download the HTML files. Alternatively, if only entity names are provided, we generate Wikipedia URLs using those names. If a Wikipedia URL is deprecated, we remove the corresponding document from the corpus along with any associated queries. From the HTML files, we extract text, image URLs, and tables. We then split the contents by subtitles in the document where each chunk corresponds to a section. For the images, we use the image URLs to download the corresponding images, removing any invalid URLs. This process produces a dictionary that organizes text, images, and tables by section.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS0.SSS0.Px2.p3">
<p class="ltx_p" id="A1.SS0.SSS0.Px2.p3.1">Since downloading the complete contents for all documents across datasets is time- and memory-intensive, we preprocess the subsets of each corpus, including documents relevant to queries in the training, evaluation, and test splits, as well as unrelated entity documents.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Limitations </h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">Due to the limitations of a single H100 GPU, we represent documents by selecting a limited number of sections and averaging their corresponding embeddings. While this reduces the computational demands, our findings suggest that capturing a broader document context leads to improved retrieval performance. Hence, leveraging the long context window of LVLMs could further enhance document retrieval by capturing more comprehensive information from the full document. Moreover, our reranker design follows the conventional approach of concatenating the input query with individual sections. However, we believe that providing the reranker with all the sections together would allow the model to better leverage the contextual information from the entire interleaved document, potentially resulting in improved performance. In order to fully leverage the interleaved format in the IR system, addressing the issues by reducing the GPU load when processing interleaved documents would greatly boost overall IR performance. We leave these explorations for future work.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Oct  3 17:45:47 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
