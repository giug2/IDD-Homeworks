<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints</title>
<!--Generated on Mon Sep 23 16:12:38 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.16382v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S1" title="In Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S2" title="In Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S2.SS1" title="In 2 Related Work â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Pain Recognition Methods</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S2.SS2" title="In 2 Related Work â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Challenges in Pain Dataset Collection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S2.SS3" title="In 2 Related Work â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Synthetic data for Pain Recognition</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S3" title="In Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Synthetic Dataset Generation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S3.SS1" title="In 3 Synthetic Dataset Generation â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Mesh Extraction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S3.SS2" title="In 3 Synthetic Dataset Generation â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Texture Synthesis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S3.SS3" title="In 3 Synthetic Dataset Generation â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Rendering and Optimization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S4" title="In Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S4.SS1" title="In 4 Experimental Setup â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Baseline Model (Real to Real)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S4.SS2" title="In 4 Experimental Setup â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Synthetic Only Model (Synth to Real)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S4.SS3" title="In 4 Experimental Setup â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Synthetic and Real Model (Mixed to Real)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S5" title="In Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S5.SS1" title="In 5 Results â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Comparison of Training Configurations for Real-World Pain Recognition</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S5.SS2" title="In 5 Results â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Aspect of Texture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S5.SS3" title="In 5 Results â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Aspect of View</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S6" title="In Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion and Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S6.SS0.SSS0.Px1" title="In 6 Discussion and Conclusion â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_title">Acknowledgements.</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn" lang="en">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\cormark</span>
<p class="ltx_p" id="p1.2">[2]</p>
</div>
<div class="ltx_para" id="p2">
<span class="ltx_ERROR undefined" id="p2.1">\fnmark</span>
<p class="ltx_p" id="p2.2">[1]</p>
</div>
<div class="ltx_para" id="p3">
<p class="ltx_p" id="p3.1">1]organization=University Stuttgart, Institute for AI,
addressline=UniversitÃ¤tsstraÃŸe 32 ,
city=Stuttgart,
postcode=70569,
state=Baden-WÃ¼rttemberg,
country=Germany</p>
</div>
<div class="ltx_para" id="p4">
<p class="ltx_p" id="p4.1">2]organization=University Clinic Essen,
addressline=GiradetstraÃŸe 2,
city=Essen,
postcode=45131,
state=North-Rhine-Westphalia,
country=Germany</p>
</div>
<div class="ltx_para" id="p5">
<span class="ltx_ERROR undefined" id="p5.1">\cortext</span>
<p class="ltx_p" id="p5.2">[1]corresponding author. (E-Mail: <span class="ltx_text ltx_font_typewriter" id="p5.2.1">constantin.seibold@uk-essen.de</span>)
<br class="ltx_break"/>All author emails: <span class="ltx_text ltx_font_typewriter" id="p5.2.2">st171793@stud.uni-stuttgart.de</span> (J. Nasimzada); <span class="ltx_text ltx_font_typewriter" id="p5.2.3">jens.kleesiek@uk-essen.de</span> (J. Kleesiek); <span class="ltx_text ltx_font_typewriter" id="p5.2.4">ken.herrmann@uk-essen.de</span> (K. Herrmann); <span class="ltx_text ltx_font_typewriter" id="p5.2.5">alina.roitberg@ki.uni-stuttgart.de</span> (A. Roitberg); <span class="ltx_text ltx_font_typewriter" id="p5.2.6">constantin.seibold@uk-essen.de</span> (C. Seibold)</p>
</div>
<div class="ltx_para" id="p6">
<span class="ltx_ERROR undefined" id="p6.2">\nonumnote</span>
<p class="ltx_p" id="p6.1"><sup class="ltx_sup" id="p6.1.1">â€ </sup> equal senior co-authorship.</p>
</div>
<h1 class="ltx_title ltx_title_document">Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jonas Nasimzada
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jens Kleesiek
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ken Herrmann
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alina Roitberg<sup class="ltx_sup" id="id5.2.id1">â€ </sup>
</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Constantin Seibold<sup class="ltx_sup" id="id6.2.id1">â€ </sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">[
</span>
<span class="ltx_contact ltx_role_affiliation">[
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1"><span class="ltx_text" id="id7.id1.1">Recognizing pain in video is crucial for improving patient-computer interaction systems, yet traditional data collection in this domain raises significant ethical and logistical challenges.
This study introduces a novel approach that leverages synthetic data to enhance video-based pain recognition models, providing an ethical and scalable alternative.
We present a pipeline that synthesizes realistic 3D facial models by capturing nuanced facial movements from a small participant pool, and mapping these onto diverse synthetic avatars. This process generates 8,600 synthetic faces, accurately reflecting genuine pain expressions from varied angles and perspectives.</span></p>
<p class="ltx_p" id="id8.id2"><span class="ltx_text" id="id8.id2.1">Utilizing advanced facial capture techniques, and leveraging public datasets like CelebV-HQ and FFHQ-UV for demographic diversity, our new synthetic dataset significantly enhances model training while ensuring privacy by anonymizing identities through facial replacements.</span></p>
<p class="ltx_p" id="id9.id3"><span class="ltx_text" id="id9.id3.1">Experimental results demonstrate that models trained on combinations of synthetic data paired with a small amount of real participants achieve superior performance in pain recognition, effectively bridging the gap between synthetic simulations and real-world applications. Our approach addresses data scarcity and ethical concerns, offering a new solution for pain detection and opening new avenues for research in privacy-preserving dataset generation. All resources are publicly available to encourage further innovation in this field.</span></p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
<span class="ltx_text" id="id10.id1">
Pain Recognition <span class="ltx_ERROR undefined" id="id10.id1.1">\sep</span>Synthetic Data <span class="ltx_ERROR undefined" id="id10.id1.2">\sep</span>Video Analysis <span class="ltx_ERROR undefined" id="id10.id1.3">\sep</span>Privacy Preserving
</span>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Pain significantly impairs the daily life of millions of people with up to 11.2 percent of American population experiencing significant pain on a daily basisÂ <cite class="ltx_cite ltx_citemacro_citep">(Nahin, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib19" title="">2015</a>)</cite>. Similarly, in 2021, 20.9% of U.S. adults experienced chronic pain, with 6.9% enduring high-impact chronic pain that significantly restricts daily activitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Rikard, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib28" title="">2023</a>)</cite>, whereas. If untreated, such pain conditions, both acute and chronic, can lead to massive loss in economy productivityÂ Â <cite class="ltx_cite ltx_citemacro_citep">(Phillips, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib21" title="">2009</a>)</cite>. To minimize pain across the population, it is of utmost importance to identify scenarios in which pain can occur not only in patient careÂ <cite class="ltx_cite ltx_citemacro_citep">(Matthias etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib17" title="">2010</a>)</cite>, but also in daily livingÂ <cite class="ltx_cite ltx_citemacro_citep">(Resnick etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib24" title="">2019</a>)</cite> and working scenariosÂ <cite class="ltx_cite ltx_citemacro_citep">(CÃ´tÃ© etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib3" title="">2008</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While automated video analysis shows potential in various aspects of understanding human actionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Kay etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib13" title="">2017</a>; Carreira and Zisserman, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib2" title="">2017</a>)</cite> and emotionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Soleymani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib34" title="">2011</a>; EbrahimiÂ Kahou etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib6" title="">2015</a>)</cite>, pain recognition poses a rather difficult issue due to the difficulty of dataset aggregation. Regulations such as the GDPRÂ <cite class="ltx_cite ltx_citemacro_citep">(European Union, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib7" title="">2016</a>)</cite> make it difficult to gather pain in the wild. Capturing spontaneous pain episodes in real-world situations is difficult and costly, requiring researchers to follow participants in their natural environments or use wearable recording devicesÂ <cite class="ltx_cite ltx_citemacro_citep">(Patel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib20" title="">2003</a>)</cite>. As this is a major intrusion in the participants personal life, such a study must pass a rigorous ethics reviewsÂ <cite class="ltx_cite ltx_citemacro_citep">(Price and Cohen, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib23" title="">2019</a>)</cite>. Similarly, if one were to construct such a dataset under a controlled setting, participants have to undergo considerable deliberate pain. Designing such a study in an ethical manner extremely and finding willing participants in a non-exploitable social situation can pose a challenge. As such, publicly available datasets as well as the number of subjects on this topic are limitedÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib39" title="">2013</a>; Walter etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib37" title="">2013b</a>)</cite>. In return, this data scarcity might lead data-hungry networks, which are prevalent in video processingÂ <cite class="ltx_cite ltx_citemacro_citep">(Monfort etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib18" title="">2019</a>; Carreira and Zisserman, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib2" title="">2017</a>)</cite>, not only be to be lacking in performance but also be fragile in face of new scenarios such as underrepresented races, sex, or locationsÂ <cite class="ltx_cite ltx_citemacro_citep">(Schiappa etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib33" title="">2023</a>; Dooley etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib5" title="">2021</a>; Karkkainen and Joo, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib12" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1"><span class="ltx_text ltx_font_italic" id="S1.p3.1.1">How can we enable dataset sizes that lead to increased performance and robustness of neural networks without running into the same ethical consideration?</span></p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="182" id="S1.F1.g1" src="extracted/5873866/visualabstract.drawio.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We leverage facial features extracted from a small dataset with real recordings with people in pain for generating a synthetic dataset with diverse faces and viewpoints to address logistical, ethical and bias-related challenges of such data collection.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">A prevalent path to generate additional in many areas of computer vision is the simulation of the world to generate a digital twin for various scenarios like drivingÂ <cite class="ltx_cite ltx_citemacro_citep">(Ros etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib32" title="">2016</a>; Richter etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib26" title="">2016a</a>)</cite> or daily livingÂ <cite class="ltx_cite ltx_citemacro_citep">(Roitberg etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib30" title="">2021a</a>)</cite>. In these settings, the task that has to be solved is well understood by the vast majority of the population. When diving into more specialized scenarios, rough simulations such as games become difficult to utilize as the problems themselves are topics of active research, as is the case with the manifestation of painÂ <cite class="ltx_cite ltx_citemacro_citep">(Cowen etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib4" title="">2015</a>)</cite>. While facial expressions that are supposed to represent pain can be identified a variety of video games, the origin of the expression is derived from actors rather than a real stimulus. Hence, to synthetically create real facial movements in painful situations, rather than to rely on acted pain expressions, we propose the use of a pipeline which
utilizes a small set of participants, captures their facial 3D movements and translates these onto a diverse set of faces. This allows us to enhance gender and ethnic diversity while also introducing perspectives from previously unseen viewpoints. In addition to these benefits, such a pipeline also enables further perspectives in regards to privacy preserving dataset generation, as the identities of the participants can be fully hidden by replacing their faces with people of the public domain.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">This study created an efficient pipeline for human mesh and texture generation, resulting in a dataset of 8,600 synthetic heads generated per perspective and texture. The datasets encompass varying facial textures and perspectives. We utilize different combinations of dataset subset, facial textures, and perspectives for the training of video networks to assess the feasibility of synthetic data for pain recognition in real data. We show that the use of synthetic data gained from this pipeline can significantly improve the performance of video-based pain recognition models.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We estimate that our pipeline offers not only an alternative to pain recognition based on data
collection under immense ethical considerations but also provides potential perspectives on privacy-preserving dataset collection under facial requirements. We hope creates a new avenue for future research focusing on capturing the essence of human behaviour in both, synthetic and real domains. All our code is publicly available at <a class="ltx_ref ltx_href" href="https://github.com/JonasNasimzada/LetsPlay4Emotion" title="">github</a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Pain Recognition Methods</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Early approaches to pain classification often relied on precomputed features extracted from facial images or videos. Martinez et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Martinez etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib16" title="">2017</a>)</cite> utilized facial landmarks and Recurrent Neural Networks (RNNs) to estimate Prkachin and Solomon pain intensity (PSPI) levels. They employed personalized Hidden Conditional Random Fields (HCRFs) for individual Visual Analog Scale (VAS) assessments.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">With the rise of deep learning, Convolutional Neural Networks (CNNs) have become prevalent for end-to-end pain recognition. Haque et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Haque etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib10" title="">2018</a>)</cite> proposed a method using RGB, depth, and thermal images to classify pain levels, leveraging a 2D CNN for feature extraction and Long Short-Term Memory (LSTM) networks for temporal analysis. Rodriguez et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Rodriguez etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib29" title="">2022</a>)</cite> employed CNNs to extract features from the VGG-Faces dataset, followed by an LSTM for binary pain classification, utilizing the UNBCâ€“McMaster pain database. Wang et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib38" title="">2017</a>)</cite> adopted a face verification network trained on the WebFace dataset, fine-tuned for pain estimation as a regression problem.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">While these methods demonstrate the effectiveness of CNN-LSTM architectures, they primarily depend on pre-existing datasets, which may lack diversity and introduce biases. Our work diverges by implementing 3D CNNs for video-based pain recognition, leveraging synthetic data to supplement existing datasets, thus enhancing model robustness and generalizability.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Challenges in Pain Dataset Collection</h3>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Adult Pain Recognition Databases. <math alttext="\star" class="ltx_Math" display="inline" id="S2.T1.3.m1.1"><semantics id="S2.T1.3.m1.1b"><mo id="S2.T1.3.m1.1.1" xref="S2.T1.3.m1.1.1.cmml">â‹†</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.m1.1c"><ci id="S2.T1.3.m1.1.1.cmml" xref="S2.T1.3.m1.1.1">â‹†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.m1.1e">â‹†</annotation></semantics></math> denotes no longer available datasets. <math alttext="\dagger" class="ltx_Math" display="inline" id="S2.T1.4.m2.1"><semantics id="S2.T1.4.m2.1b"><mo id="S2.T1.4.m2.1.1" xref="S2.T1.4.m2.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.m2.1c"><ci id="S2.T1.4.m2.1.1.cmml" xref="S2.T1.4.m2.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.m2.1d">\dagger</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.m2.1e">â€ </annotation></semantics></math> denotes datasets that are not specifically focused on pain. UNBC-McMaster SPEAÂ <cite class="ltx_cite ltx_citemacro_citep">(Lucey etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib15" title="">2011</a>)</cite>
BioVid Heat Â <cite class="ltx_cite ltx_citemacro_citep">(Walter etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib37" title="">2013b</a>)</cite>
BP4D-SpontaneousÂ <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib42" title="">2014</a>)</cite>
BP4D+Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib41" title="">2016b</a>)</cite>
MIntPAINÂ <cite class="ltx_cite ltx_citemacro_citep">(Tobon etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib35" title="">2020</a>)</cite></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.7">
<tr class="ltx_tr" id="S2.T1.7.4">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S2.T1.7.4.1"><span class="ltx_text ltx_font_bold" id="S2.T1.7.4.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.7.4.2"><span class="ltx_text ltx_font_bold" id="S2.T1.7.4.2.1">Age</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.7.4.3"><span class="ltx_text ltx_font_bold" id="S2.T1.7.4.3.1">Status</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.7.4.4"><span class="ltx_text ltx_font_bold" id="S2.T1.7.4.4.1">Stimulus</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.7.4.5"><span class="ltx_text ltx_font_bold" id="S2.T1.7.4.5.1">Participants</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.7.4.6"><span class="ltx_text ltx_font_bold" id="S2.T1.7.4.6.1">Origin</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.5.1.1">UNBC-McMaster SPEA<math alttext="\star" class="ltx_Math" display="inline" id="S2.T1.5.1.1.m1.1"><semantics id="S2.T1.5.1.1.m1.1a"><mo id="S2.T1.5.1.1.m1.1.1" xref="S2.T1.5.1.1.m1.1.1.cmml">â‹†</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.1.1.m1.1b"><ci id="S2.T1.5.1.1.m1.1.1.cmml" xref="S2.T1.5.1.1.m1.1.1">â‹†</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.1.1.m1.1d">â‹†</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.3">shoulder pain</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.5">25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.1.6">-</td>
</tr>
<tr class="ltx_tr" id="S2.T1.7.5">
<td class="ltx_td ltx_align_left" id="S2.T1.7.5.1">BioVid Heat</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.2">20-65 years</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.3">healthy</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.4">Heat</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.5">90</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.5.6">European</td>
</tr>
<tr class="ltx_tr" id="S2.T1.6.2">
<td class="ltx_td ltx_align_left" id="S2.T1.6.2.1">BP4D-Spontaneous<math alttext="\dagger" class="ltx_Math" display="inline" id="S2.T1.6.2.1.m1.1"><semantics id="S2.T1.6.2.1.m1.1a"><mo id="S2.T1.6.2.1.m1.1.1" xref="S2.T1.6.2.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.2.1.m1.1b"><ci id="S2.T1.6.2.1.m1.1.1.cmml" xref="S2.T1.6.2.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.2.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.2.1.m1.1d">â€ </annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.6.2.2">18-29 years</td>
<td class="ltx_td ltx_align_center" id="S2.T1.6.2.3">healthy</td>
<td class="ltx_td ltx_align_center" id="S2.T1.6.2.4">Cold</td>
<td class="ltx_td ltx_align_center" id="S2.T1.6.2.5">41</td>
<td class="ltx_td ltx_align_center" id="S2.T1.6.2.6">Diverse</td>
</tr>
<tr class="ltx_tr" id="S2.T1.7.3">
<td class="ltx_td ltx_align_left" id="S2.T1.7.3.1">BP4D+<math alttext="\dagger" class="ltx_Math" display="inline" id="S2.T1.7.3.1.m1.1"><semantics id="S2.T1.7.3.1.m1.1a"><mo id="S2.T1.7.3.1.m1.1.1" xref="S2.T1.7.3.1.m1.1.1.cmml">â€ </mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.3.1.m1.1b"><ci id="S2.T1.7.3.1.m1.1.1.cmml" xref="S2.T1.7.3.1.m1.1.1">â€ </ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.3.1.m1.1c">\dagger</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.3.1.m1.1d">â€ </annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.3.2">18-66 years</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.3.3">healthy</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.3.4">Cold</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.3.5">140</td>
<td class="ltx_td ltx_align_center" id="S2.T1.7.3.6">Diverse</td>
</tr>
<tr class="ltx_tr" id="S2.T1.7.6">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.7.6.1">MIntPAIN</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.7.6.2">22-42 years</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.7.6.3">healthy</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.7.6.4">Electrity</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.7.6.5">20</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.7.6.6">-</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Collecting pain datasets presents significant ethical and logistical challenges. Conducting such studies often requires invasive methods, including following participants or using wearable recording devices, necessitating rigorous ethical review processes Â <cite class="ltx_cite ltx_citemacro_citep">(Patel etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib20" title="">2003</a>)</cite>. Controlled experiments, on the other hand, involve administering deliberate pain, raising ethical concerns and participant recruitment challenges.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">These factors contribute to the scarcity of publicly available pain datasets, limiting the diversity and volume of data necessary for training deep learning models Â <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib39" title="">2013</a>; Walter etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib37" title="">2013b</a>; Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib40" title="">2016a</a>)</cite>. Consequently, data scarcity can hinder model performance, making them fragile in scenarios involving underrepresented demographics. We display currently existing datasets for pain recognition in Table<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S2.T1" title="Table 1 â€£ 2.2 Challenges in Pain Dataset Collection â€£ 2 Related Work â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_tag">1</span></a>. Here we can notice the general limitation in number of participants.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Our work addresses these challenges by employing synthetic data generation techniques, creating datasets that enhance demographic diversity and provide robust training sources for pain recognition models.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Synthetic data for Pain Recognition</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Synthetic data in computer vision has gained traction, providing scalable solutions for data augmentation and training. Previous studies have successfully utilized synthetic environments for various tasks. Richter et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Richter etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib27" title="">2016b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib25" title="">2017</a>)</cite> exploited the Grand Theft Auto V (GTA V) environment to gather data for object detection, segmentation, and optical flow. Similarly, KrÃ¤henbÃ¼hl Â <cite class="ltx_cite ltx_citemacro_citep">(Krahenbuhl, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib14" title="">2018</a>)</cite> used DirectX 11 to extract ground truth from video games for tasks like depth estimation and semantic segmentation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1"><cite class="ltx_cite ltx_citemacro_cite">Roitberg etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib31" title="">2021b</a>)</cite> leveraged the life simulation game THE SIMS 4 to generate training examples for Activities of Daily Living (ADL) recognition. These studies underscore synthetic dataâ€™s potential in scenarios with limited real-world data availability. However, applying synthetic data to pain recognition remains underexplored.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Despite its potential, synthetic data for pain recognition is still emerging. Prior work by Pikulkaew et al. Â <cite class="ltx_cite ltx_citemacro_citep">(Pikulkaew and Chouvatut, <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib22" title="">2021</a>)</cite> used Generative Adversarial Networks (GANs) to create synthetic facial expressions but faced challenges in achieving realistic representations. These limitations underscore the complexity of accurately modeling pain expressions and the need for innovative approaches to synthetic data generation.</p>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Our work addresses these challenges by capturing detailed 3D facial movements and translating them into diverse facial textures, creating a synthetic dataset that mirrors pain expressions across demographics. This method facilitates the training of robust pain recognition models and offers insights into privacy-preserving dataset generation. By synthesizing human behavior in synthetic domains, our contributions pave the way for future research in ethical and scalable pain recognition under privacy considerations.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Synthetic Dataset Generation</h2>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="184" id="S3.F2.g1" src="extracted/5873866/pipeline.jpg" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Generation Pipeline of Synthetic Head Videos. We extract facial textures from public domain faces for later translation. In parallel, we extract 3D volumes for each frame of the pain videos. Finally, we map the facial textures onto each frame-wise volume and render a scene from different perspectives. </figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our pipeline for generating synthetic head videos builds on existing datasets such as the BioVid Heat Pain Database <cite class="ltx_cite ltx_citemacro_cite">Walter etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib36" title="">2013a</a>)</cite>. Our approach integrates advanced methods in 3D mesh generation, texture mapping, and rendering to produce a diverse set of synthetic head videos for pain recognition research. The outline of this process is displayed in <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S3.F2" title="Figure 2 â€£ 3 Synthetic Dataset Generation â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Mesh Extraction</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Our pipeline begins with the generation of 3D head meshes from input video frames using the EMOCA framework <cite class="ltx_cite ltx_citemacro_cite">Bai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib1" title="">2023</a>)</cite>. For each video frame the head region is extracted via segmentation. The EMOCA framework processes these frames to produce consistent 3D head meshes. This process involves encoding each frame into context vectors that capture critical information about facial expressions, geometry, and pose. These context vectors are then used by a decoder to reconstruct detailed 3D head meshes. The resulting meshes are highly accurate and serve as the foundation for our synthetic head videos.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="862" id="S3.F3.g1" src="x1.jpg" width="1196"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Example of different textures that are translated onto a participant of the BioVidHeat dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Texture Synthesis</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Texture mapping is performed via the FFHQ-UV repository <cite class="ltx_cite ltx_citemacro_cite">Bai etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib1" title="">2023</a>)</cite>, which provides a robust framework for generating high-resolution facial textures. The process begins with the detection of facial landmarks to ensure accurate texture application. A synthetic face is initially created from these landmarks, and high-resolution textures are then applied to this base mesh. To address common issues such as blank eye textures, we incorporate additional mapping refinements that enhance the realism of the synthetic heads. These refinements are essential for creating diverse and visually compelling head textures that reflect variations in age, gender, and ethnicity, as sourced from CelebV-HQ <cite class="ltx_cite ltx_citemacro_cite">Zhu etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib43" title="">2022</a>)</cite>. We display how final translations might look in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S3.F3" title="Figure 3 â€£ 3.1 Mesh Extraction â€£ 3 Synthetic Dataset Generation â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Rendering and Optimization</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Rendering is performed using Blender, where the Stop Motion OBJ plugin <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib11" title="">Jensen </a></cite> is utilized to import and animate sequences of mesh files. This plugin enables seamless integration of mesh sequences into Blender, allowing for the efficient rendering of high-resolution synthetic videos. Our rendering pipeline supports two main perspectives: frontal and side views of the head. Each perspective contributes to the datasetâ€™s diversity, providing various viewpoints for improved model robustness.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To manage the large-scale rendering tasks, we employed a cluster system consisting of 20 nodes, each with 16 threads, totaling 320 concurrent threads. This distributed approach significantly enhances rendering efficiency, reducing the processing time for 8,600 videos to approximately 2 hours. As a side note, with more high-performance hardware, such as the Apple M1 chip, the render time per video can be reduced to approximately 1 minute, and thus will significantly improve the scalability of the process.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">This synthetic dataset generation pipeline offers a robust solution for augmenting data availability for pain recognition models while addressing ethical concerns related to real data collection. By combining advanced 3D modeling, texture mapping, and high-performance rendering techniques, our approach provides a scalable and privacy-preserving alternative to traditional dataset creation methods.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="179" id="S4.F4.g1" src="extracted/5873866/modelsetup.jpg" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>We consider three training configurations in our experiments: (1) using real data only (real to real), (2) using synthetic data exclusively (synth to real) and using both for training (mixed to real). </figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.5">To estimate the efficacy of our synthetic dataset generation pipeline, we consider a comparison of models trained on different combinations of real and synthetic data. This comparison assesses the potential of synthetic data to replicate real-world scenarios and enhance the performance and robustness of pain recognition models.
Let <math alttext="D_{R}" class="ltx_Math" display="inline" id="S4.p1.1.m1.1"><semantics id="S4.p1.1.m1.1a"><msub id="S4.p1.1.m1.1.1" xref="S4.p1.1.m1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.2" xref="S4.p1.1.m1.1.1.2.cmml">D</mi><mi id="S4.p1.1.m1.1.1.3" xref="S4.p1.1.m1.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.1b"><apply id="S4.p1.1.m1.1.1.cmml" xref="S4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.1.cmml" xref="S4.p1.1.m1.1.1">subscript</csymbol><ci id="S4.p1.1.m1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.2">ğ·</ci><ci id="S4.p1.1.m1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.3">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.1c">D_{R}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math> represent the real data and <math alttext="D_{S}" class="ltx_Math" display="inline" id="S4.p1.2.m2.1"><semantics id="S4.p1.2.m2.1a"><msub id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mi id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml">D</mi><mi id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2">ğ·</ci><ci id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">D_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> denote the synthetic data. Let <math alttext="D_{Tr}" class="ltx_Math" display="inline" id="S4.p1.3.m3.1"><semantics id="S4.p1.3.m3.1a"><msub id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml"><mi id="S4.p1.3.m3.1.1.2" xref="S4.p1.3.m3.1.1.2.cmml">D</mi><mrow id="S4.p1.3.m3.1.1.3" xref="S4.p1.3.m3.1.1.3.cmml"><mi id="S4.p1.3.m3.1.1.3.2" xref="S4.p1.3.m3.1.1.3.2.cmml">T</mi><mo id="S4.p1.3.m3.1.1.3.1" xref="S4.p1.3.m3.1.1.3.1.cmml">â¢</mo><mi id="S4.p1.3.m3.1.1.3.3" xref="S4.p1.3.m3.1.1.3.3.cmml">r</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><apply id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.cmml" xref="S4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.p1.3.m3.1.1.2.cmml" xref="S4.p1.3.m3.1.1.2">ğ·</ci><apply id="S4.p1.3.m3.1.1.3.cmml" xref="S4.p1.3.m3.1.1.3"><times id="S4.p1.3.m3.1.1.3.1.cmml" xref="S4.p1.3.m3.1.1.3.1"></times><ci id="S4.p1.3.m3.1.1.3.2.cmml" xref="S4.p1.3.m3.1.1.3.2">ğ‘‡</ci><ci id="S4.p1.3.m3.1.1.3.3.cmml" xref="S4.p1.3.m3.1.1.3.3">ğ‘Ÿ</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">D_{Tr}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_T italic_r end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="D_{Val}" class="ltx_Math" display="inline" id="S4.p1.4.m4.1"><semantics id="S4.p1.4.m4.1a"><msub id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml"><mi id="S4.p1.4.m4.1.1.2" xref="S4.p1.4.m4.1.1.2.cmml">D</mi><mrow id="S4.p1.4.m4.1.1.3" xref="S4.p1.4.m4.1.1.3.cmml"><mi id="S4.p1.4.m4.1.1.3.2" xref="S4.p1.4.m4.1.1.3.2.cmml">V</mi><mo id="S4.p1.4.m4.1.1.3.1" xref="S4.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S4.p1.4.m4.1.1.3.3" xref="S4.p1.4.m4.1.1.3.3.cmml">a</mi><mo id="S4.p1.4.m4.1.1.3.1a" xref="S4.p1.4.m4.1.1.3.1.cmml">â¢</mo><mi id="S4.p1.4.m4.1.1.3.4" xref="S4.p1.4.m4.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><apply id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p1.4.m4.1.1.1.cmml" xref="S4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.p1.4.m4.1.1.2.cmml" xref="S4.p1.4.m4.1.1.2">ğ·</ci><apply id="S4.p1.4.m4.1.1.3.cmml" xref="S4.p1.4.m4.1.1.3"><times id="S4.p1.4.m4.1.1.3.1.cmml" xref="S4.p1.4.m4.1.1.3.1"></times><ci id="S4.p1.4.m4.1.1.3.2.cmml" xref="S4.p1.4.m4.1.1.3.2">ğ‘‰</ci><ci id="S4.p1.4.m4.1.1.3.3.cmml" xref="S4.p1.4.m4.1.1.3.3">ğ‘</ci><ci id="S4.p1.4.m4.1.1.3.4.cmml" xref="S4.p1.4.m4.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">D_{Val}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT italic_V italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="D_{Test}" class="ltx_Math" display="inline" id="S4.p1.5.m5.1"><semantics id="S4.p1.5.m5.1a"><msub id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml"><mi id="S4.p1.5.m5.1.1.2" xref="S4.p1.5.m5.1.1.2.cmml">D</mi><mrow id="S4.p1.5.m5.1.1.3" xref="S4.p1.5.m5.1.1.3.cmml"><mi id="S4.p1.5.m5.1.1.3.2" xref="S4.p1.5.m5.1.1.3.2.cmml">T</mi><mo id="S4.p1.5.m5.1.1.3.1" xref="S4.p1.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S4.p1.5.m5.1.1.3.3" xref="S4.p1.5.m5.1.1.3.3.cmml">e</mi><mo id="S4.p1.5.m5.1.1.3.1a" xref="S4.p1.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S4.p1.5.m5.1.1.3.4" xref="S4.p1.5.m5.1.1.3.4.cmml">s</mi><mo id="S4.p1.5.m5.1.1.3.1b" xref="S4.p1.5.m5.1.1.3.1.cmml">â¢</mo><mi id="S4.p1.5.m5.1.1.3.5" xref="S4.p1.5.m5.1.1.3.5.cmml">t</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><apply id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.1.cmml" xref="S4.p1.5.m5.1.1">subscript</csymbol><ci id="S4.p1.5.m5.1.1.2.cmml" xref="S4.p1.5.m5.1.1.2">ğ·</ci><apply id="S4.p1.5.m5.1.1.3.cmml" xref="S4.p1.5.m5.1.1.3"><times id="S4.p1.5.m5.1.1.3.1.cmml" xref="S4.p1.5.m5.1.1.3.1"></times><ci id="S4.p1.5.m5.1.1.3.2.cmml" xref="S4.p1.5.m5.1.1.3.2">ğ‘‡</ci><ci id="S4.p1.5.m5.1.1.3.3.cmml" xref="S4.p1.5.m5.1.1.3.3">ğ‘’</ci><ci id="S4.p1.5.m5.1.1.3.4.cmml" xref="S4.p1.5.m5.1.1.3.4">ğ‘ </ci><ci id="S4.p1.5.m5.1.1.3.5.cmml" xref="S4.p1.5.m5.1.1.3.5">ğ‘¡</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">D_{Test}</annotation><annotation encoding="application/x-llamapun" id="S4.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT italic_T italic_e italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math> denote the training, validation and testing data that was used respectively.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">All experiments consider the same setup of hyperparameters. The networks is a SlowFast-R50<cite class="ltx_cite ltx_citemacro_cite">Feichtenhofer etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib8" title="">2019</a>)</cite> that is optimized for 100 epochs with a weighted BCE-loss using SGD with a batch size of 64, learning rate of 0.01, weight decay of 1e-5, and momentum of 0.9. During training, videos undergo random short side scale, random cropping, and random horizontal flip as augmentation. During validation, videos were subsampled, short-side scaled, center cropped. After every 10 epochs of training the SlowFast-R50 model, the modelâ€™s progress is assessed using the <math alttext="D_{Val}" class="ltx_Math" display="inline" id="S4.p2.1.m1.1"><semantics id="S4.p2.1.m1.1a"><msub id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">D</mi><mrow id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml"><mi id="S4.p2.1.m1.1.1.3.2" xref="S4.p2.1.m1.1.1.3.2.cmml">V</mi><mo id="S4.p2.1.m1.1.1.3.1" xref="S4.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.3.3" xref="S4.p2.1.m1.1.1.3.3.cmml">a</mi><mo id="S4.p2.1.m1.1.1.3.1a" xref="S4.p2.1.m1.1.1.3.1.cmml">â¢</mo><mi id="S4.p2.1.m1.1.1.3.4" xref="S4.p2.1.m1.1.1.3.4.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1">subscript</csymbol><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">ğ·</ci><apply id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3"><times id="S4.p2.1.m1.1.1.3.1.cmml" xref="S4.p2.1.m1.1.1.3.1"></times><ci id="S4.p2.1.m1.1.1.3.2.cmml" xref="S4.p2.1.m1.1.1.3.2">ğ‘‰</ci><ci id="S4.p2.1.m1.1.1.3.3.cmml" xref="S4.p2.1.m1.1.1.3.3">ğ‘</ci><ci id="S4.p2.1.m1.1.1.3.4.cmml" xref="S4.p2.1.m1.1.1.3.4">ğ‘™</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">D_{Val}</annotation><annotation encoding="application/x-llamapun" id="S4.p2.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_V italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>.
To ensure representative training and validation datasets, data splitting followed a strategy proposed in previous research <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib9" title="">Gkikas </a></cite>. This strategy aimed to maintain similar gender, age, and expressiveness distributions between the training and validation sets.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baseline Model (Real to Real)</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.5">We use the BioVid Heat Pain Database <math alttext="D_{R}" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><msub id="S4.SS1.p1.1.m1.1.1" xref="S4.SS1.p1.1.m1.1.1.cmml"><mi id="S4.SS1.p1.1.m1.1.1.2" xref="S4.SS1.p1.1.m1.1.1.2.cmml">D</mi><mi id="S4.SS1.p1.1.m1.1.1.3" xref="S4.SS1.p1.1.m1.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><apply id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.1.m1.1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS1.p1.1.m1.1.1.2.cmml" xref="S4.SS1.p1.1.m1.1.1.2">ğ·</ci><ci id="S4.SS1.p1.1.m1.1.1.3.cmml" xref="S4.SS1.p1.1.m1.1.1.3">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">D_{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math> as the baseline dataset. The model trained solely on <math alttext="D_{R}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi id="S4.SS1.p1.2.m2.1.1.2" xref="S4.SS1.p1.2.m2.1.1.2.cmml">D</mi><mi id="S4.SS1.p1.2.m2.1.1.3" xref="S4.SS1.p1.2.m2.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">ğ·</ci><ci id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">D_{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math> provides a reference for further experiments with <math alttext="D_{Tr}\cup D_{Val}=D_{R}" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mrow id="S4.SS1.p1.3.m3.1.1" xref="S4.SS1.p1.3.m3.1.1.cmml"><mrow id="S4.SS1.p1.3.m3.1.1.2" xref="S4.SS1.p1.3.m3.1.1.2.cmml"><msub id="S4.SS1.p1.3.m3.1.1.2.2" xref="S4.SS1.p1.3.m3.1.1.2.2.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2.2.2" xref="S4.SS1.p1.3.m3.1.1.2.2.2.cmml">D</mi><mrow id="S4.SS1.p1.3.m3.1.1.2.2.3" xref="S4.SS1.p1.3.m3.1.1.2.2.3.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2.2.3.2" xref="S4.SS1.p1.3.m3.1.1.2.2.3.2.cmml">T</mi><mo id="S4.SS1.p1.3.m3.1.1.2.2.3.1" xref="S4.SS1.p1.3.m3.1.1.2.2.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.3.m3.1.1.2.2.3.3" xref="S4.SS1.p1.3.m3.1.1.2.2.3.3.cmml">r</mi></mrow></msub><mo id="S4.SS1.p1.3.m3.1.1.2.1" xref="S4.SS1.p1.3.m3.1.1.2.1.cmml">âˆª</mo><msub id="S4.SS1.p1.3.m3.1.1.2.3" xref="S4.SS1.p1.3.m3.1.1.2.3.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2.3.2" xref="S4.SS1.p1.3.m3.1.1.2.3.2.cmml">D</mi><mrow id="S4.SS1.p1.3.m3.1.1.2.3.3" xref="S4.SS1.p1.3.m3.1.1.2.3.3.cmml"><mi id="S4.SS1.p1.3.m3.1.1.2.3.3.2" xref="S4.SS1.p1.3.m3.1.1.2.3.3.2.cmml">V</mi><mo id="S4.SS1.p1.3.m3.1.1.2.3.3.1" xref="S4.SS1.p1.3.m3.1.1.2.3.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.3.m3.1.1.2.3.3.3" xref="S4.SS1.p1.3.m3.1.1.2.3.3.3.cmml">a</mi><mo id="S4.SS1.p1.3.m3.1.1.2.3.3.1a" xref="S4.SS1.p1.3.m3.1.1.2.3.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.3.m3.1.1.2.3.3.4" xref="S4.SS1.p1.3.m3.1.1.2.3.3.4.cmml">l</mi></mrow></msub></mrow><mo id="S4.SS1.p1.3.m3.1.1.1" xref="S4.SS1.p1.3.m3.1.1.1.cmml">=</mo><msub id="S4.SS1.p1.3.m3.1.1.3" xref="S4.SS1.p1.3.m3.1.1.3.cmml"><mi id="S4.SS1.p1.3.m3.1.1.3.2" xref="S4.SS1.p1.3.m3.1.1.3.2.cmml">D</mi><mi id="S4.SS1.p1.3.m3.1.1.3.3" xref="S4.SS1.p1.3.m3.1.1.3.3.cmml">R</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><apply id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1"><eq id="S4.SS1.p1.3.m3.1.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1.1"></eq><apply id="S4.SS1.p1.3.m3.1.1.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2"><union id="S4.SS1.p1.3.m3.1.1.2.1.cmml" xref="S4.SS1.p1.3.m3.1.1.2.1"></union><apply id="S4.SS1.p1.3.m3.1.1.2.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.2.2.1.cmml" xref="S4.SS1.p1.3.m3.1.1.2.2">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.2.2.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2.2.2">ğ·</ci><apply id="S4.SS1.p1.3.m3.1.1.2.2.3.cmml" xref="S4.SS1.p1.3.m3.1.1.2.2.3"><times id="S4.SS1.p1.3.m3.1.1.2.2.3.1.cmml" xref="S4.SS1.p1.3.m3.1.1.2.2.3.1"></times><ci id="S4.SS1.p1.3.m3.1.1.2.2.3.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2.2.3.2">ğ‘‡</ci><ci id="S4.SS1.p1.3.m3.1.1.2.2.3.3.cmml" xref="S4.SS1.p1.3.m3.1.1.2.2.3.3">ğ‘Ÿ</ci></apply></apply><apply id="S4.SS1.p1.3.m3.1.1.2.3.cmml" xref="S4.SS1.p1.3.m3.1.1.2.3"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.2.3.1.cmml" xref="S4.SS1.p1.3.m3.1.1.2.3">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.2.3.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2.3.2">ğ·</ci><apply id="S4.SS1.p1.3.m3.1.1.2.3.3.cmml" xref="S4.SS1.p1.3.m3.1.1.2.3.3"><times id="S4.SS1.p1.3.m3.1.1.2.3.3.1.cmml" xref="S4.SS1.p1.3.m3.1.1.2.3.3.1"></times><ci id="S4.SS1.p1.3.m3.1.1.2.3.3.2.cmml" xref="S4.SS1.p1.3.m3.1.1.2.3.3.2">ğ‘‰</ci><ci id="S4.SS1.p1.3.m3.1.1.2.3.3.3.cmml" xref="S4.SS1.p1.3.m3.1.1.2.3.3.3">ğ‘</ci><ci id="S4.SS1.p1.3.m3.1.1.2.3.3.4.cmml" xref="S4.SS1.p1.3.m3.1.1.2.3.3.4">ğ‘™</ci></apply></apply></apply><apply id="S4.SS1.p1.3.m3.1.1.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.3.m3.1.1.3.1.cmml" xref="S4.SS1.p1.3.m3.1.1.3">subscript</csymbol><ci id="S4.SS1.p1.3.m3.1.1.3.2.cmml" xref="S4.SS1.p1.3.m3.1.1.3.2">ğ·</ci><ci id="S4.SS1.p1.3.m3.1.1.3.3.cmml" xref="S4.SS1.p1.3.m3.1.1.3.3">ğ‘…</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">D_{Tr}\cup D_{Val}=D_{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_T italic_r end_POSTSUBSCRIPT âˆª italic_D start_POSTSUBSCRIPT italic_V italic_a italic_l end_POSTSUBSCRIPT = italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="D_{Tr}\cap D_{Val}=\emptyset" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><mrow id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mrow id="S4.SS1.p1.4.m4.1.1.2" xref="S4.SS1.p1.4.m4.1.1.2.cmml"><msub id="S4.SS1.p1.4.m4.1.1.2.2" xref="S4.SS1.p1.4.m4.1.1.2.2.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2.2.2" xref="S4.SS1.p1.4.m4.1.1.2.2.2.cmml">D</mi><mrow id="S4.SS1.p1.4.m4.1.1.2.2.3" xref="S4.SS1.p1.4.m4.1.1.2.2.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2.2.3.2" xref="S4.SS1.p1.4.m4.1.1.2.2.3.2.cmml">T</mi><mo id="S4.SS1.p1.4.m4.1.1.2.2.3.1" xref="S4.SS1.p1.4.m4.1.1.2.2.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.4.m4.1.1.2.2.3.3" xref="S4.SS1.p1.4.m4.1.1.2.2.3.3.cmml">r</mi></mrow></msub><mo id="S4.SS1.p1.4.m4.1.1.2.1" xref="S4.SS1.p1.4.m4.1.1.2.1.cmml">âˆ©</mo><msub id="S4.SS1.p1.4.m4.1.1.2.3" xref="S4.SS1.p1.4.m4.1.1.2.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2.3.2" xref="S4.SS1.p1.4.m4.1.1.2.3.2.cmml">D</mi><mrow id="S4.SS1.p1.4.m4.1.1.2.3.3" xref="S4.SS1.p1.4.m4.1.1.2.3.3.cmml"><mi id="S4.SS1.p1.4.m4.1.1.2.3.3.2" xref="S4.SS1.p1.4.m4.1.1.2.3.3.2.cmml">V</mi><mo id="S4.SS1.p1.4.m4.1.1.2.3.3.1" xref="S4.SS1.p1.4.m4.1.1.2.3.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.4.m4.1.1.2.3.3.3" xref="S4.SS1.p1.4.m4.1.1.2.3.3.3.cmml">a</mi><mo id="S4.SS1.p1.4.m4.1.1.2.3.3.1a" xref="S4.SS1.p1.4.m4.1.1.2.3.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.4.m4.1.1.2.3.3.4" xref="S4.SS1.p1.4.m4.1.1.2.3.3.4.cmml">l</mi></mrow></msub></mrow><mo id="S4.SS1.p1.4.m4.1.1.1" xref="S4.SS1.p1.4.m4.1.1.1.cmml">=</mo><mi id="S4.SS1.p1.4.m4.1.1.3" mathvariant="normal" xref="S4.SS1.p1.4.m4.1.1.3.cmml">âˆ…</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><eq id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1.1"></eq><apply id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2"><intersect id="S4.SS1.p1.4.m4.1.1.2.1.cmml" xref="S4.SS1.p1.4.m4.1.1.2.1"></intersect><apply id="S4.SS1.p1.4.m4.1.1.2.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.2.2.1.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2">subscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.2.2.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2.2">ğ·</ci><apply id="S4.SS1.p1.4.m4.1.1.2.2.3.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2.3"><times id="S4.SS1.p1.4.m4.1.1.2.2.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2.3.1"></times><ci id="S4.SS1.p1.4.m4.1.1.2.2.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2.3.2">ğ‘‡</ci><ci id="S4.SS1.p1.4.m4.1.1.2.2.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2.3.3">ğ‘Ÿ</ci></apply></apply><apply id="S4.SS1.p1.4.m4.1.1.2.3.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.2.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3">subscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.2.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3.2">ğ·</ci><apply id="S4.SS1.p1.4.m4.1.1.2.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3.3"><times id="S4.SS1.p1.4.m4.1.1.2.3.3.1.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3.3.1"></times><ci id="S4.SS1.p1.4.m4.1.1.2.3.3.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3.3.2">ğ‘‰</ci><ci id="S4.SS1.p1.4.m4.1.1.2.3.3.3.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3.3.3">ğ‘</ci><ci id="S4.SS1.p1.4.m4.1.1.2.3.3.4.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3.3.4">ğ‘™</ci></apply></apply></apply><emptyset id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"></emptyset></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">D_{Tr}\cap D_{Val}=\emptyset</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT italic_T italic_r end_POSTSUBSCRIPT âˆ© italic_D start_POSTSUBSCRIPT italic_V italic_a italic_l end_POSTSUBSCRIPT = âˆ…</annotation></semantics></math> with <math alttext="D_{Val}=D_{Test}" class="ltx_Math" display="inline" id="S4.SS1.p1.5.m5.1"><semantics id="S4.SS1.p1.5.m5.1a"><mrow id="S4.SS1.p1.5.m5.1.1" xref="S4.SS1.p1.5.m5.1.1.cmml"><msub id="S4.SS1.p1.5.m5.1.1.2" xref="S4.SS1.p1.5.m5.1.1.2.cmml"><mi id="S4.SS1.p1.5.m5.1.1.2.2" xref="S4.SS1.p1.5.m5.1.1.2.2.cmml">D</mi><mrow id="S4.SS1.p1.5.m5.1.1.2.3" xref="S4.SS1.p1.5.m5.1.1.2.3.cmml"><mi id="S4.SS1.p1.5.m5.1.1.2.3.2" xref="S4.SS1.p1.5.m5.1.1.2.3.2.cmml">V</mi><mo id="S4.SS1.p1.5.m5.1.1.2.3.1" xref="S4.SS1.p1.5.m5.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.5.m5.1.1.2.3.3" xref="S4.SS1.p1.5.m5.1.1.2.3.3.cmml">a</mi><mo id="S4.SS1.p1.5.m5.1.1.2.3.1a" xref="S4.SS1.p1.5.m5.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.5.m5.1.1.2.3.4" xref="S4.SS1.p1.5.m5.1.1.2.3.4.cmml">l</mi></mrow></msub><mo id="S4.SS1.p1.5.m5.1.1.1" xref="S4.SS1.p1.5.m5.1.1.1.cmml">=</mo><msub id="S4.SS1.p1.5.m5.1.1.3" xref="S4.SS1.p1.5.m5.1.1.3.cmml"><mi id="S4.SS1.p1.5.m5.1.1.3.2" xref="S4.SS1.p1.5.m5.1.1.3.2.cmml">D</mi><mrow id="S4.SS1.p1.5.m5.1.1.3.3" xref="S4.SS1.p1.5.m5.1.1.3.3.cmml"><mi id="S4.SS1.p1.5.m5.1.1.3.3.2" xref="S4.SS1.p1.5.m5.1.1.3.3.2.cmml">T</mi><mo id="S4.SS1.p1.5.m5.1.1.3.3.1" xref="S4.SS1.p1.5.m5.1.1.3.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.5.m5.1.1.3.3.3" xref="S4.SS1.p1.5.m5.1.1.3.3.3.cmml">e</mi><mo id="S4.SS1.p1.5.m5.1.1.3.3.1a" xref="S4.SS1.p1.5.m5.1.1.3.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.5.m5.1.1.3.3.4" xref="S4.SS1.p1.5.m5.1.1.3.3.4.cmml">s</mi><mo id="S4.SS1.p1.5.m5.1.1.3.3.1b" xref="S4.SS1.p1.5.m5.1.1.3.3.1.cmml">â¢</mo><mi id="S4.SS1.p1.5.m5.1.1.3.3.5" xref="S4.SS1.p1.5.m5.1.1.3.3.5.cmml">t</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.5.m5.1b"><apply id="S4.SS1.p1.5.m5.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1"><eq id="S4.SS1.p1.5.m5.1.1.1.cmml" xref="S4.SS1.p1.5.m5.1.1.1"></eq><apply id="S4.SS1.p1.5.m5.1.1.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.1.1.2.1.cmml" xref="S4.SS1.p1.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS1.p1.5.m5.1.1.2.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2.2">ğ·</ci><apply id="S4.SS1.p1.5.m5.1.1.2.3.cmml" xref="S4.SS1.p1.5.m5.1.1.2.3"><times id="S4.SS1.p1.5.m5.1.1.2.3.1.cmml" xref="S4.SS1.p1.5.m5.1.1.2.3.1"></times><ci id="S4.SS1.p1.5.m5.1.1.2.3.2.cmml" xref="S4.SS1.p1.5.m5.1.1.2.3.2">ğ‘‰</ci><ci id="S4.SS1.p1.5.m5.1.1.2.3.3.cmml" xref="S4.SS1.p1.5.m5.1.1.2.3.3">ğ‘</ci><ci id="S4.SS1.p1.5.m5.1.1.2.3.4.cmml" xref="S4.SS1.p1.5.m5.1.1.2.3.4">ğ‘™</ci></apply></apply><apply id="S4.SS1.p1.5.m5.1.1.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.p1.5.m5.1.1.3.1.cmml" xref="S4.SS1.p1.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS1.p1.5.m5.1.1.3.2.cmml" xref="S4.SS1.p1.5.m5.1.1.3.2">ğ·</ci><apply id="S4.SS1.p1.5.m5.1.1.3.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3"><times id="S4.SS1.p1.5.m5.1.1.3.3.1.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.1"></times><ci id="S4.SS1.p1.5.m5.1.1.3.3.2.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.2">ğ‘‡</ci><ci id="S4.SS1.p1.5.m5.1.1.3.3.3.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.3">ğ‘’</ci><ci id="S4.SS1.p1.5.m5.1.1.3.3.4.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.4">ğ‘ </ci><ci id="S4.SS1.p1.5.m5.1.1.3.3.5.cmml" xref="S4.SS1.p1.5.m5.1.1.3.3.5">ğ‘¡</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.5.m5.1c">D_{Val}=D_{Test}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT italic_V italic_a italic_l end_POSTSUBSCRIPT = italic_D start_POSTSUBSCRIPT italic_T italic_e italic_s italic_t end_POSTSUBSCRIPT</annotation></semantics></math>
to stay consistent to the provided splits by <cite class="ltx_cite ltx_citemacro_cite"><a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#bib.bib9" title="">Gkikas </a></cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Synthetic Only Model (Synth to Real)</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">In this setup, the model is trained exclusively on synthetic data <math alttext="D_{Tr},D_{Val}\in D_{S}" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.2"><semantics id="S4.SS2.p1.1.m1.2a"><mrow id="S4.SS2.p1.1.m1.2.2" xref="S4.SS2.p1.1.m1.2.2.cmml"><mrow id="S4.SS2.p1.1.m1.2.2.2.2" xref="S4.SS2.p1.1.m1.2.2.2.3.cmml"><msub id="S4.SS2.p1.1.m1.1.1.1.1.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.cmml"><mi id="S4.SS2.p1.1.m1.1.1.1.1.1.2" xref="S4.SS2.p1.1.m1.1.1.1.1.1.2.cmml">D</mi><mrow id="S4.SS2.p1.1.m1.1.1.1.1.1.3" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.SS2.p1.1.m1.1.1.1.1.1.3.2" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3.2.cmml">T</mi><mo id="S4.SS2.p1.1.m1.1.1.1.1.1.3.1" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3.1.cmml">â¢</mo><mi id="S4.SS2.p1.1.m1.1.1.1.1.1.3.3" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3.3.cmml">r</mi></mrow></msub><mo id="S4.SS2.p1.1.m1.2.2.2.2.3" xref="S4.SS2.p1.1.m1.2.2.2.3.cmml">,</mo><msub id="S4.SS2.p1.1.m1.2.2.2.2.2" xref="S4.SS2.p1.1.m1.2.2.2.2.2.cmml"><mi id="S4.SS2.p1.1.m1.2.2.2.2.2.2" xref="S4.SS2.p1.1.m1.2.2.2.2.2.2.cmml">D</mi><mrow id="S4.SS2.p1.1.m1.2.2.2.2.2.3" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.cmml"><mi id="S4.SS2.p1.1.m1.2.2.2.2.2.3.2" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.2.cmml">V</mi><mo id="S4.SS2.p1.1.m1.2.2.2.2.2.3.1" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p1.1.m1.2.2.2.2.2.3.3" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.3.cmml">a</mi><mo id="S4.SS2.p1.1.m1.2.2.2.2.2.3.1a" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p1.1.m1.2.2.2.2.2.3.4" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.4.cmml">l</mi></mrow></msub></mrow><mo id="S4.SS2.p1.1.m1.2.2.3" xref="S4.SS2.p1.1.m1.2.2.3.cmml">âˆˆ</mo><msub id="S4.SS2.p1.1.m1.2.2.4" xref="S4.SS2.p1.1.m1.2.2.4.cmml"><mi id="S4.SS2.p1.1.m1.2.2.4.2" xref="S4.SS2.p1.1.m1.2.2.4.2.cmml">D</mi><mi id="S4.SS2.p1.1.m1.2.2.4.3" xref="S4.SS2.p1.1.m1.2.2.4.3.cmml">S</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.2b"><apply id="S4.SS2.p1.1.m1.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2"><in id="S4.SS2.p1.1.m1.2.2.3.cmml" xref="S4.SS2.p1.1.m1.2.2.3"></in><list id="S4.SS2.p1.1.m1.2.2.2.3.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2"><apply id="S4.SS2.p1.1.m1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.SS2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.2">ğ·</ci><apply id="S4.SS2.p1.1.m1.1.1.1.1.1.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3"><times id="S4.SS2.p1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3.1"></times><ci id="S4.SS2.p1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3.2">ğ‘‡</ci><ci id="S4.SS2.p1.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.SS2.p1.1.m1.1.1.1.1.1.3.3">ğ‘Ÿ</ci></apply></apply><apply id="S4.SS2.p1.1.m1.2.2.2.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.2.2.2.2.2.1.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.SS2.p1.1.m1.2.2.2.2.2.2.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2.2">ğ·</ci><apply id="S4.SS2.p1.1.m1.2.2.2.2.2.3.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3"><times id="S4.SS2.p1.1.m1.2.2.2.2.2.3.1.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.1"></times><ci id="S4.SS2.p1.1.m1.2.2.2.2.2.3.2.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.2">ğ‘‰</ci><ci id="S4.SS2.p1.1.m1.2.2.2.2.2.3.3.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.3">ğ‘</ci><ci id="S4.SS2.p1.1.m1.2.2.2.2.2.3.4.cmml" xref="S4.SS2.p1.1.m1.2.2.2.2.2.3.4">ğ‘™</ci></apply></apply></list><apply id="S4.SS2.p1.1.m1.2.2.4.cmml" xref="S4.SS2.p1.1.m1.2.2.4"><csymbol cd="ambiguous" id="S4.SS2.p1.1.m1.2.2.4.1.cmml" xref="S4.SS2.p1.1.m1.2.2.4">subscript</csymbol><ci id="S4.SS2.p1.1.m1.2.2.4.2.cmml" xref="S4.SS2.p1.1.m1.2.2.4.2">ğ·</ci><ci id="S4.SS2.p1.1.m1.2.2.4.3.cmml" xref="S4.SS2.p1.1.m1.2.2.4.3">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.2c">D_{Tr},D_{Val}\in D_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.2d">italic_D start_POSTSUBSCRIPT italic_T italic_r end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_V italic_a italic_l end_POSTSUBSCRIPT âˆˆ italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>. Each test patient from the original dataset is augmented with multiple synthetic head textures. The final performance is then evaluated on the real data <math alttext="D_{Test}\in D_{R}" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mrow id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml"><msub id="S4.SS2.p1.2.m2.1.1.2" xref="S4.SS2.p1.2.m2.1.1.2.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2.2" xref="S4.SS2.p1.2.m2.1.1.2.2.cmml">D</mi><mrow id="S4.SS2.p1.2.m2.1.1.2.3" xref="S4.SS2.p1.2.m2.1.1.2.3.cmml"><mi id="S4.SS2.p1.2.m2.1.1.2.3.2" xref="S4.SS2.p1.2.m2.1.1.2.3.2.cmml">T</mi><mo id="S4.SS2.p1.2.m2.1.1.2.3.1" xref="S4.SS2.p1.2.m2.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p1.2.m2.1.1.2.3.3" xref="S4.SS2.p1.2.m2.1.1.2.3.3.cmml">e</mi><mo id="S4.SS2.p1.2.m2.1.1.2.3.1a" xref="S4.SS2.p1.2.m2.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p1.2.m2.1.1.2.3.4" xref="S4.SS2.p1.2.m2.1.1.2.3.4.cmml">s</mi><mo id="S4.SS2.p1.2.m2.1.1.2.3.1b" xref="S4.SS2.p1.2.m2.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS2.p1.2.m2.1.1.2.3.5" xref="S4.SS2.p1.2.m2.1.1.2.3.5.cmml">t</mi></mrow></msub><mo id="S4.SS2.p1.2.m2.1.1.1" xref="S4.SS2.p1.2.m2.1.1.1.cmml">âˆˆ</mo><msub id="S4.SS2.p1.2.m2.1.1.3" xref="S4.SS2.p1.2.m2.1.1.3.cmml"><mi id="S4.SS2.p1.2.m2.1.1.3.2" xref="S4.SS2.p1.2.m2.1.1.3.2.cmml">D</mi><mi id="S4.SS2.p1.2.m2.1.1.3.3" xref="S4.SS2.p1.2.m2.1.1.3.3.cmml">R</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><apply id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"><in id="S4.SS2.p1.2.m2.1.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1.1"></in><apply id="S4.SS2.p1.2.m2.1.1.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.2.1.cmml" xref="S4.SS2.p1.2.m2.1.1.2">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.2.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2.2">ğ·</ci><apply id="S4.SS2.p1.2.m2.1.1.2.3.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3"><times id="S4.SS2.p1.2.m2.1.1.2.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.1"></times><ci id="S4.SS2.p1.2.m2.1.1.2.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.2">ğ‘‡</ci><ci id="S4.SS2.p1.2.m2.1.1.2.3.3.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.3">ğ‘’</ci><ci id="S4.SS2.p1.2.m2.1.1.2.3.4.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.4">ğ‘ </ci><ci id="S4.SS2.p1.2.m2.1.1.2.3.5.cmml" xref="S4.SS2.p1.2.m2.1.1.2.3.5">ğ‘¡</ci></apply></apply><apply id="S4.SS2.p1.2.m2.1.1.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S4.SS2.p1.2.m2.1.1.3.1.cmml" xref="S4.SS2.p1.2.m2.1.1.3">subscript</csymbol><ci id="S4.SS2.p1.2.m2.1.1.3.2.cmml" xref="S4.SS2.p1.2.m2.1.1.3.2">ğ·</ci><ci id="S4.SS2.p1.2.m2.1.1.3.3.cmml" xref="S4.SS2.p1.2.m2.1.1.3.3">ğ‘…</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">D_{Test}\in D_{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_T italic_e italic_s italic_t end_POSTSUBSCRIPT âˆˆ italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math>. This configuration tests the feasibility of synthetic data in isolation and its potential to match or surpass the baseline performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Synthetic and Real Model (Mixed to Real)</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.5">To explore the benefits of combining synthetic and real data, we train a model on the combined dataset <math alttext="D_{C}=D_{R}\cup D_{S}" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mrow id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml"><msub id="S4.SS3.p1.1.m1.1.1.2" xref="S4.SS3.p1.1.m1.1.1.2.cmml"><mi id="S4.SS3.p1.1.m1.1.1.2.2" xref="S4.SS3.p1.1.m1.1.1.2.2.cmml">D</mi><mi id="S4.SS3.p1.1.m1.1.1.2.3" xref="S4.SS3.p1.1.m1.1.1.2.3.cmml">C</mi></msub><mo id="S4.SS3.p1.1.m1.1.1.1" xref="S4.SS3.p1.1.m1.1.1.1.cmml">=</mo><mrow id="S4.SS3.p1.1.m1.1.1.3" xref="S4.SS3.p1.1.m1.1.1.3.cmml"><msub id="S4.SS3.p1.1.m1.1.1.3.2" xref="S4.SS3.p1.1.m1.1.1.3.2.cmml"><mi id="S4.SS3.p1.1.m1.1.1.3.2.2" xref="S4.SS3.p1.1.m1.1.1.3.2.2.cmml">D</mi><mi id="S4.SS3.p1.1.m1.1.1.3.2.3" xref="S4.SS3.p1.1.m1.1.1.3.2.3.cmml">R</mi></msub><mo id="S4.SS3.p1.1.m1.1.1.3.1" xref="S4.SS3.p1.1.m1.1.1.3.1.cmml">âˆª</mo><msub id="S4.SS3.p1.1.m1.1.1.3.3" xref="S4.SS3.p1.1.m1.1.1.3.3.cmml"><mi id="S4.SS3.p1.1.m1.1.1.3.3.2" xref="S4.SS3.p1.1.m1.1.1.3.3.2.cmml">D</mi><mi id="S4.SS3.p1.1.m1.1.1.3.3.3" xref="S4.SS3.p1.1.m1.1.1.3.3.3.cmml">S</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><apply id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1"><eq id="S4.SS3.p1.1.m1.1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1.1"></eq><apply id="S4.SS3.p1.1.m1.1.1.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.2.2.cmml" xref="S4.SS3.p1.1.m1.1.1.2.2">ğ·</ci><ci id="S4.SS3.p1.1.m1.1.1.2.3.cmml" xref="S4.SS3.p1.1.m1.1.1.2.3">ğ¶</ci></apply><apply id="S4.SS3.p1.1.m1.1.1.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3"><union id="S4.SS3.p1.1.m1.1.1.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3.1"></union><apply id="S4.SS3.p1.1.m1.1.1.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.3.2.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.3.2.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2.2">ğ·</ci><ci id="S4.SS3.p1.1.m1.1.1.3.2.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3.2.3">ğ‘…</ci></apply><apply id="S4.SS3.p1.1.m1.1.1.3.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S4.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3">subscript</csymbol><ci id="S4.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3.2">ğ·</ci><ci id="S4.SS3.p1.1.m1.1.1.3.3.3.cmml" xref="S4.SS3.p1.1.m1.1.1.3.3.3">ğ‘†</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">D_{C}=D_{R}\cup D_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT = italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT âˆª italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>. This setup evaluates whether integrating synthetic data <math alttext="D_{S}" class="ltx_Math" display="inline" id="S4.SS3.p1.2.m2.1"><semantics id="S4.SS3.p1.2.m2.1a"><msub id="S4.SS3.p1.2.m2.1.1" xref="S4.SS3.p1.2.m2.1.1.cmml"><mi id="S4.SS3.p1.2.m2.1.1.2" xref="S4.SS3.p1.2.m2.1.1.2.cmml">D</mi><mi id="S4.SS3.p1.2.m2.1.1.3" xref="S4.SS3.p1.2.m2.1.1.3.cmml">S</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.2.m2.1b"><apply id="S4.SS3.p1.2.m2.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.2.m2.1.1.1.cmml" xref="S4.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS3.p1.2.m2.1.1.2.cmml" xref="S4.SS3.p1.2.m2.1.1.2">ğ·</ci><ci id="S4.SS3.p1.2.m2.1.1.3.cmml" xref="S4.SS3.p1.2.m2.1.1.3">ğ‘†</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.2.m2.1c">D_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math> with real data <math alttext="D_{R}" class="ltx_Math" display="inline" id="S4.SS3.p1.3.m3.1"><semantics id="S4.SS3.p1.3.m3.1a"><msub id="S4.SS3.p1.3.m3.1.1" xref="S4.SS3.p1.3.m3.1.1.cmml"><mi id="S4.SS3.p1.3.m3.1.1.2" xref="S4.SS3.p1.3.m3.1.1.2.cmml">D</mi><mi id="S4.SS3.p1.3.m3.1.1.3" xref="S4.SS3.p1.3.m3.1.1.3.cmml">R</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.3.m3.1b"><apply id="S4.SS3.p1.3.m3.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS3.p1.3.m3.1.1.1.cmml" xref="S4.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S4.SS3.p1.3.m3.1.1.2.cmml" xref="S4.SS3.p1.3.m3.1.1.2">ğ·</ci><ci id="S4.SS3.p1.3.m3.1.1.3.cmml" xref="S4.SS3.p1.3.m3.1.1.3">ğ‘…</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.3.m3.1c">D_{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math> enhances model performance beyond what is achievable with real data alone.
<math alttext="D_{Val}\in D_{S}" class="ltx_Math" display="inline" id="S4.SS3.p1.4.m4.1"><semantics id="S4.SS3.p1.4.m4.1a"><mrow id="S4.SS3.p1.4.m4.1.1" xref="S4.SS3.p1.4.m4.1.1.cmml"><msub id="S4.SS3.p1.4.m4.1.1.2" xref="S4.SS3.p1.4.m4.1.1.2.cmml"><mi id="S4.SS3.p1.4.m4.1.1.2.2" xref="S4.SS3.p1.4.m4.1.1.2.2.cmml">D</mi><mrow id="S4.SS3.p1.4.m4.1.1.2.3" xref="S4.SS3.p1.4.m4.1.1.2.3.cmml"><mi id="S4.SS3.p1.4.m4.1.1.2.3.2" xref="S4.SS3.p1.4.m4.1.1.2.3.2.cmml">V</mi><mo id="S4.SS3.p1.4.m4.1.1.2.3.1" xref="S4.SS3.p1.4.m4.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS3.p1.4.m4.1.1.2.3.3" xref="S4.SS3.p1.4.m4.1.1.2.3.3.cmml">a</mi><mo id="S4.SS3.p1.4.m4.1.1.2.3.1a" xref="S4.SS3.p1.4.m4.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS3.p1.4.m4.1.1.2.3.4" xref="S4.SS3.p1.4.m4.1.1.2.3.4.cmml">l</mi></mrow></msub><mo id="S4.SS3.p1.4.m4.1.1.1" xref="S4.SS3.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msub id="S4.SS3.p1.4.m4.1.1.3" xref="S4.SS3.p1.4.m4.1.1.3.cmml"><mi id="S4.SS3.p1.4.m4.1.1.3.2" xref="S4.SS3.p1.4.m4.1.1.3.2.cmml">D</mi><mi id="S4.SS3.p1.4.m4.1.1.3.3" xref="S4.SS3.p1.4.m4.1.1.3.3.cmml">S</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.4.m4.1b"><apply id="S4.SS3.p1.4.m4.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1"><in id="S4.SS3.p1.4.m4.1.1.1.cmml" xref="S4.SS3.p1.4.m4.1.1.1"></in><apply id="S4.SS3.p1.4.m4.1.1.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.4.m4.1.1.2.1.cmml" xref="S4.SS3.p1.4.m4.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.4.m4.1.1.2.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2.2">ğ·</ci><apply id="S4.SS3.p1.4.m4.1.1.2.3.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3"><times id="S4.SS3.p1.4.m4.1.1.2.3.1.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3.1"></times><ci id="S4.SS3.p1.4.m4.1.1.2.3.2.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3.2">ğ‘‰</ci><ci id="S4.SS3.p1.4.m4.1.1.2.3.3.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3.3">ğ‘</ci><ci id="S4.SS3.p1.4.m4.1.1.2.3.4.cmml" xref="S4.SS3.p1.4.m4.1.1.2.3.4">ğ‘™</ci></apply></apply><apply id="S4.SS3.p1.4.m4.1.1.3.cmml" xref="S4.SS3.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p1.4.m4.1.1.3.1.cmml" xref="S4.SS3.p1.4.m4.1.1.3">subscript</csymbol><ci id="S4.SS3.p1.4.m4.1.1.3.2.cmml" xref="S4.SS3.p1.4.m4.1.1.3.2">ğ·</ci><ci id="S4.SS3.p1.4.m4.1.1.3.3.cmml" xref="S4.SS3.p1.4.m4.1.1.3.3">ğ‘†</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.4.m4.1c">D_{Val}\in D_{S}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.4.m4.1d">italic_D start_POSTSUBSCRIPT italic_V italic_a italic_l end_POSTSUBSCRIPT âˆˆ italic_D start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT</annotation></semantics></math>. Each test patient from the original dataset is augmented with multiple synthetic head textures. The final performance is then evaluated on the real data <math alttext="D_{Test}\in D_{R}" class="ltx_Math" display="inline" id="S4.SS3.p1.5.m5.1"><semantics id="S4.SS3.p1.5.m5.1a"><mrow id="S4.SS3.p1.5.m5.1.1" xref="S4.SS3.p1.5.m5.1.1.cmml"><msub id="S4.SS3.p1.5.m5.1.1.2" xref="S4.SS3.p1.5.m5.1.1.2.cmml"><mi id="S4.SS3.p1.5.m5.1.1.2.2" xref="S4.SS3.p1.5.m5.1.1.2.2.cmml">D</mi><mrow id="S4.SS3.p1.5.m5.1.1.2.3" xref="S4.SS3.p1.5.m5.1.1.2.3.cmml"><mi id="S4.SS3.p1.5.m5.1.1.2.3.2" xref="S4.SS3.p1.5.m5.1.1.2.3.2.cmml">T</mi><mo id="S4.SS3.p1.5.m5.1.1.2.3.1" xref="S4.SS3.p1.5.m5.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS3.p1.5.m5.1.1.2.3.3" xref="S4.SS3.p1.5.m5.1.1.2.3.3.cmml">e</mi><mo id="S4.SS3.p1.5.m5.1.1.2.3.1a" xref="S4.SS3.p1.5.m5.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS3.p1.5.m5.1.1.2.3.4" xref="S4.SS3.p1.5.m5.1.1.2.3.4.cmml">s</mi><mo id="S4.SS3.p1.5.m5.1.1.2.3.1b" xref="S4.SS3.p1.5.m5.1.1.2.3.1.cmml">â¢</mo><mi id="S4.SS3.p1.5.m5.1.1.2.3.5" xref="S4.SS3.p1.5.m5.1.1.2.3.5.cmml">t</mi></mrow></msub><mo id="S4.SS3.p1.5.m5.1.1.1" xref="S4.SS3.p1.5.m5.1.1.1.cmml">âˆˆ</mo><msub id="S4.SS3.p1.5.m5.1.1.3" xref="S4.SS3.p1.5.m5.1.1.3.cmml"><mi id="S4.SS3.p1.5.m5.1.1.3.2" xref="S4.SS3.p1.5.m5.1.1.3.2.cmml">D</mi><mi id="S4.SS3.p1.5.m5.1.1.3.3" xref="S4.SS3.p1.5.m5.1.1.3.3.cmml">R</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.5.m5.1b"><apply id="S4.SS3.p1.5.m5.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1"><in id="S4.SS3.p1.5.m5.1.1.1.cmml" xref="S4.SS3.p1.5.m5.1.1.1"></in><apply id="S4.SS3.p1.5.m5.1.1.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S4.SS3.p1.5.m5.1.1.2.1.cmml" xref="S4.SS3.p1.5.m5.1.1.2">subscript</csymbol><ci id="S4.SS3.p1.5.m5.1.1.2.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2.2">ğ·</ci><apply id="S4.SS3.p1.5.m5.1.1.2.3.cmml" xref="S4.SS3.p1.5.m5.1.1.2.3"><times id="S4.SS3.p1.5.m5.1.1.2.3.1.cmml" xref="S4.SS3.p1.5.m5.1.1.2.3.1"></times><ci id="S4.SS3.p1.5.m5.1.1.2.3.2.cmml" xref="S4.SS3.p1.5.m5.1.1.2.3.2">ğ‘‡</ci><ci id="S4.SS3.p1.5.m5.1.1.2.3.3.cmml" xref="S4.SS3.p1.5.m5.1.1.2.3.3">ğ‘’</ci><ci id="S4.SS3.p1.5.m5.1.1.2.3.4.cmml" xref="S4.SS3.p1.5.m5.1.1.2.3.4">ğ‘ </ci><ci id="S4.SS3.p1.5.m5.1.1.2.3.5.cmml" xref="S4.SS3.p1.5.m5.1.1.2.3.5">ğ‘¡</ci></apply></apply><apply id="S4.SS3.p1.5.m5.1.1.3.cmml" xref="S4.SS3.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S4.SS3.p1.5.m5.1.1.3.1.cmml" xref="S4.SS3.p1.5.m5.1.1.3">subscript</csymbol><ci id="S4.SS3.p1.5.m5.1.1.3.2.cmml" xref="S4.SS3.p1.5.m5.1.1.3.2">ğ·</ci><ci id="S4.SS3.p1.5.m5.1.1.3.3.cmml" xref="S4.SS3.p1.5.m5.1.1.3.3">ğ‘…</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.5.m5.1c">D_{Test}\in D_{R}</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.5.m5.1d">italic_D start_POSTSUBSCRIPT italic_T italic_e italic_s italic_t end_POSTSUBSCRIPT âˆˆ italic_D start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT</annotation></semantics></math>.
This evaluation aims to determine if the combination of synthetic and real data provides a substantial performance improvement.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section presents the results of our experiments, evaluating the performance of pain recognition models trained on different datasets: real, synthetic, and a combination of both. Subsequently, we provide an analysis of design decisions regarding the generation of synthetic data. The evaluation metrics used are Area Under the Receiver Operating Characteristic Curve (AUROC), F1-Score, and Accuracy. These metrics provide a comprehensive overview of the modelsâ€™ capabilities in handling the BioVid Heat Pain dataset, as detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S5.T2" title="Table 2 â€£ 5.1 Comparison of Training Configurations for Real-World Pain Recognition â€£ 5 Results â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison of Training Configurations for Real-World Pain Recognition</h3>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Pain recognition results on the <span class="ltx_text ltx_font_bold" id="S5.T2.2.1">BioVid Heat Pain dataset</span>. We consider models trained on only real data (Real to Real), only synthetic data with 10 different textures per patient (Synth to Real), and a mix of both (Mixed to Real).</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.3">
<tr class="ltx_tr" id="S5.T2.3.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" id="S5.T2.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.1.1">Case</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.3.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.2.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.3.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.3.1">F1-Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.3.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.3.1.4.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_tt" id="S5.T2.3.2.1">Real to Real</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.3.2.2">0.741</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.3.2.3">0.666</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.3.2.4">0.654</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" id="S5.T2.3.3.1">Synth to Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.3.3.2">0.581</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.3.3.3">0.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.3.3.4">0.666</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr" id="S5.T2.3.4.1">Mixed to Real</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.3.4.2">0.78</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.3.4.3">0.817</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T2.3.4.4">0.708</td>
</tr>
</table>
</figure>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">The baseline model, trained exclusively on the BioVid Heat Pain Database (Real to Real), achieved an AUROC of 0.741, an F1-Score of 0.666, and an Accuracy of 0.654. These results set a benchmark for evaluating pain recognition systems, illustrating the inherent challenges of using only real data, which is often limited in variability. The Synth to Real configuration, where the model was trained solely on synthetic data, demonstrated a lower AUROC of 0.581 but achieved a higher F1-Score of 0.800 and Accuracy of 0.666. This suggests that while synthetic data alone may struggle with overall class discrimination, it effectively captures the positive class, improving the balance between precision and recall. This indicates the potential of synthetic data to address class imbalance in pain recognition. Notably, the Mixed to Real model, which integrates both real and synthetic data, outperformed the individual datasets with an AUROC of 0.780, an F1-Score of 0.817, and an Accuracy of 0.708. The superior AUROC reflects an enhanced capacity to generalize across varied scenarios, leveraging the variability introduced by synthetic data. This configuration underscores the advantage of combining datasets, leading to improved generalization and robustness in pain recognition tasks. Overall, these findings affirm that incorporating synthetic data significantly enhances model performance, offering a practical solution to the limitations of traditional data collection methods and underscoring its potential as a valuable tool in advancing pain recognition technology.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Aspect of Texture</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S5.T3" title="Table 3 â€£ 5.2 Aspect of Texture â€£ 5 Results â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_tag">3</span></a> evaluates model performance on synthetic datasets with varying texture complexity and patient representations. The <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.1">10 Textures per Patient</span> configuration achieved the highest AUROC (0.655) and F1-Score (0.799), illustrating the positive impact of extensive texture diversity on pain recognition.
The baseline <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.2">Only Mesh</span> setup showed that models relying solely on mesh geometry achieved a lower AUROC of 0.624, highlighting the limitations of texture-free models. As texture diversity increased, there was a notable improvement in both AUROC and accuracy metrics, demonstrating that richer texture information enhances the modelâ€™s ability to capture nuanced pain indicators. Notably, the <span class="ltx_text ltx_font_italic" id="S5.SS2.p1.1.3">10 Textures per Patient</span> configuration resulted in the best overall performance, underscoring synthetic data capability to complement traditional pain recognition methods and improve generalization across varied scenarios.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Training result with <span class="ltx_text ltx_font_bold" id="S5.T3.2.1">synthetic data as validation set</span> on the epoch with the highest AUROC-Score</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.3">
<tr class="ltx_tr" id="S5.T3.3.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" id="S5.T3.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.1.1">Research Case</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.2"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.2.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.3"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.3.1">F1-Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.3.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.3.1.4.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_tt" id="S5.T3.3.2.1">Only Mesh</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.3.2.2">0.624</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.3.2.3">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.3.2.4">0.665</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" id="S5.T3.3.3.1">1 Texture/patient</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.3.2">0.643</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.3.3">0.776</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.3.4">0.667</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" id="S5.T3.3.4.1">2 Textures/patient</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.4.2">0.629</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.4.3">0.752</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.4.4">0.634</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.5">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" id="S5.T3.3.5.1">3 Textures/patient</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.5.2">0.637</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.5.3">0.745</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.5.4">0.647</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" id="S5.T3.3.6.1">5 Textures/patient</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.6.2">0.642</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.6.3">0.767</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.3.6.4">0.664</td>
</tr>
<tr class="ltx_tr" id="S5.T3.3.7">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr" id="S5.T3.3.7.1">10 Textures/patient</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T3.3.7.2">0.655</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T3.3.7.3">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T3.3.7.4">0.668</td>
</tr>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Aspect of View</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.16382v1#S5.T4" title="Table 4 â€£ 5.3 Aspect of View â€£ 5 Results â€£ Towards Synthetic Data Generation for Improved Pain Recognition in Videos under Patient Constraints"><span class="ltx_text ltx_ref_tag">4</span></a> summarizes the performance of models trained on synthetic data across different view configurations. Both the Front View and Side View models yielded identical metrics: AUROC of 0.641, F1-Score of 0.786, and Accuracy of 0.658, indicating that each view independently offers a similar level of information for pain recognition, and thus are both helpful in their own way. The Multiple Views configuration using both views achieved the highest performance with an AUROC of 0.653, F1-Score of 0.799, and Accuracy of 0.666. This demonstrates the enhanced generalization capability afforded by combining diverse perspectives, suggesting that multi-view synthetic data significantly improves the modelâ€™s ability to capture pain-related expressions.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Training result with <span class="ltx_text ltx_font_bold" id="S5.T4.2.1">synthetic data as validation set</span> on the epoch with the highest AUROC-Score</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T4.3">
<tr class="ltx_tr" id="S5.T4.3.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_t" id="S5.T4.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T4.3.1.1.1">Research Case</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.2"><span class="ltx_text ltx_font_bold" id="S5.T4.3.1.2.1">AUROC</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.3"><span class="ltx_text ltx_font_bold" id="S5.T4.3.1.3.1">F1-Score</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.3.1.4.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr ltx_border_tt" id="S5.T4.3.2.1">Front View</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T4.3.2.2">0.641</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T4.3.2.3">0.786</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T4.3.2.4">0.658</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_rr" id="S5.T4.3.3.1">Side View</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.2">0.641</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.3">0.786</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T4.3.3.4">0.658</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_rr" id="S5.T4.3.4.1">Multiple Views</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T4.3.4.2">0.653</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T4.3.4.3">0.799</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T4.3.4.4">0.666</td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion and Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Our work advances video-based pain recognition by introducing a pipeline for generating synthetic video data using deep learning techniques. Leveraging the BioVid Heat Pain Database, we combined the EMOCA repository for meshes, FFHQ-UV for textures, and Blender for rendering to create a diverse dataset with varied textures and angles. Our results show that while synthetic data alone has limitations, integrating it with real data significantly enhances pain recognition performance.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">This approach addresses the ethical and privacy concerns associated with real-world pain data collection by offering a privacy-preserving alternative. Synthetic data generation not only circumvents privacy issues but also enables the inclusion of diverse demographics, enhancing the datasetâ€™s representativeness.
However, models trained solely on synthetic data show limited performance improvements, highlighting the need for effective synthetic-to-real generalization. Future work should focus on enhancing this generalization, exploring advanced generative models, and incorporating additional data sources such as biomedical signals to improve the applicability of synthetic data in real-world scenarios.
In conclusion, our study lays the groundwork for more efficient and ethical medical data collection, advancing pain recognition models while respecting patient privacy and promoting inclusivity of different appearances.</p>
</div>
<section class="ltx_paragraph" id="S6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Acknowledgements.</h4>
<div class="ltx_para" id="S6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS0.SSS0.Px1.p1.1">A. Roitberg was supported by DFG under Germanyâ€™s Excellence Strategy - EXC 2075 (SIMTECH).
This work received funding from â€˜KITEâ€™ (Plattform fÃ¼r KI-Translation Essen) from the REACT-EU initiative 
<br class="ltx_break"/>(<a class="ltx_ref ltx_href" href="https://kite.ikim.nrw/" title="">https://kite.ikim.nrw/</a>, EFRE-0801977) and the Cancer Research Center Cologne Essen (CCCE).</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai etÂ al. (2023)</span>
<span class="ltx_bibblock">
Bai, H., Kang, D., Zhang, H., Pan, J., Bao, L., 2023.

</span>
<span class="ltx_bibblock">Ffhq-uv: Normalized facial uv-texture dataset for 3d face reconstruction, in: IEEE Conference on Computer Vision and Pattern Recognition.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carreira and Zisserman (2017)</span>
<span class="ltx_bibblock">
Carreira, J., Zisserman, A., 2017.

</span>
<span class="ltx_bibblock">Quo vadis, action recognition? a new model and the kinetics dataset, in: proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6299â€“6308.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">CÃ´tÃ© etÂ al. (2008)</span>
<span class="ltx_bibblock">
CÃ´tÃ©, P., vanÂ der Velde, G., Cassidy, J.D., Carroll, L.J., Hogg-Johnson, S., Holm, L.W., Carragee, E.J., Haldeman, S., Nordin, M., Hurwitz, E.L., etÂ al., 2008.

</span>
<span class="ltx_bibblock">The burden and determinants of neck pain in workers: results of the bone and joint decade 2000â€“2010 task force on neck pain and its associated disorders.

</span>
<span class="ltx_bibblock">Spine 33, S60â€“S74.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cowen etÂ al. (2015)</span>
<span class="ltx_bibblock">
Cowen, R., Stasiowska, M.K., Laycock, H., Bantel, C., 2015.

</span>
<span class="ltx_bibblock">Assessing pain objectively: the use of physiological markers.

</span>
<span class="ltx_bibblock">Anaesthesia 70, 828â€“847.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dooley etÂ al. (2021)</span>
<span class="ltx_bibblock">
Dooley, S., Goldstein, T., Dickerson, J.P., 2021.

</span>
<span class="ltx_bibblock">Robustness disparities in commercial face detection.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2108.12508 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">EbrahimiÂ Kahou etÂ al. (2015)</span>
<span class="ltx_bibblock">
EbrahimiÂ Kahou, S., Michalski, V., Konda, K., Memisevic, R., Pal, C., 2015.

</span>
<span class="ltx_bibblock">Recurrent neural networks for emotion recognition in video, in: Proceedings of the 2015 ACM on international conference on multimodal interaction, pp. 467â€“474.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">European Union (2016)</span>
<span class="ltx_bibblock">
European Union, 2016.

</span>
<span class="ltx_bibblock">Regulation (eu) 2016/679 of the european parliament and of the council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data (general data protection regulation).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://eur-lex.europa.eu/legal-content/DE/TXT/PDF/?uri=CELEX:32016R0679" title="">https://eur-lex.europa.eu/legal-content/DE/TXT/PDF/?uri=CELEX:32016R0679</a>.

</span>
<span class="ltx_bibblock">Accessed: 2024-07-31.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feichtenhofer etÂ al. (2019)</span>
<span class="ltx_bibblock">
Feichtenhofer, C., Fan, H., Malik, J., He, K., 2019.

</span>
<span class="ltx_bibblock">Slowfast networks for video recognition, in: Proceedings of the IEEE/CVF international conference on computer vision, pp. 6202â€“6211.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(9)</span>
<span class="ltx_bibblock">
Gkikas, S., .

</span>
<span class="ltx_bibblock">Biovid holdouteval (2023).

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nit.ovgu.de/nitmedia/Bilder/Dokumente/BIOVIDDokumente/BioVidHoldOutEvalProposal.pdf" title="">https://www.nit.ovgu.de/nitmedia/Bilder/Dokumente/BIOVIDDokumente/BioVidHoldOutEvalProposal.pdf</a>.

</span>
<span class="ltx_bibblock">[Accessed 17-04-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haque etÂ al. (2018)</span>
<span class="ltx_bibblock">
Haque, M.A., Bautista, R.B., Noroozi, F., Kulkarni, K., Laursen, C.B., Irani, R., Bellantonio, M., Escalera, S., Anbarjafari, G., Nasrollahi, K., Andersen, O.K., Spaich, E.G., Moeslund, T.B., 2018.

</span>
<span class="ltx_bibblock">Deep multimodal pain recognition: A database and comparison of spatio-temporal visual modalities, in: 2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG 2018), pp. 250â€“257.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/FG.2018.00044" title="">10.1109/FG.2018.00044</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(11)</span>
<span class="ltx_bibblock">
Jensen, J., .

</span>
<span class="ltx_bibblock">GitHub - neverhood311/Stop-motion-OBJ: A Blender add-on for importing a sequence of OBJ meshes as frames â€” github.com.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/neverhood311/Stop-motion-OBJ/tree/master" title="">https://github.com/neverhood311/Stop-motion-OBJ/tree/master</a>.

</span>
<span class="ltx_bibblock">[Accessed 12-04-2024].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karkkainen and Joo (2021)</span>
<span class="ltx_bibblock">
Karkkainen, K., Joo, J., 2021.

</span>
<span class="ltx_bibblock">Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation, in: Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 1548â€“1558.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kay etÂ al. (2017)</span>
<span class="ltx_bibblock">
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan, S., Viola, F., Green, T., Back, T., Natsev, P., etÂ al., 2017.

</span>
<span class="ltx_bibblock">The kinetics human action video dataset.

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:1705.06950 .

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krahenbuhl (2018)</span>
<span class="ltx_bibblock">
Krahenbuhl, P., 2018.

</span>
<span class="ltx_bibblock">Free supervision from video games, in: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2955â€“2964.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/CVPR.2018.00312" title="">10.1109/CVPR.2018.00312</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lucey etÂ al. (2011)</span>
<span class="ltx_bibblock">
Lucey, P., Cohn, J.F., Prkachin, K.M., Solomon, P.E., Matthews, I., 2011.

</span>
<span class="ltx_bibblock">Painful data: The unbc-mcmaster shoulder pain expression archive database.

</span>
<span class="ltx_bibblock">IEEE Transactions on Affective Computing 2, 64â€“73.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Martinez etÂ al. (2017)</span>
<span class="ltx_bibblock">
Martinez, D.L., Rudovic, O., Picard, R., 2017.

</span>
<span class="ltx_bibblock">Personalized automatic estimation of self-reported pain intensity from facial expressions.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1706.07154" title="">https://arxiv.org/abs/1706.07154</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.48550/ARXIV.1706.07154" title="">10.48550/ARXIV.1706.07154</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Matthias etÂ al. (2010)</span>
<span class="ltx_bibblock">
Matthias, M.S., Parpart, A.L., Nyland, K.A., Huffman, M.A., Stubbs, D.L., Sargent, C., Bair, M.J., 2010.

</span>
<span class="ltx_bibblock">The patientâ€“provider relationship in chronic pain care: providersâ€™ perspectives.

</span>
<span class="ltx_bibblock">Pain Medicine 11, 1688â€“1697.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Monfort etÂ al. (2019)</span>
<span class="ltx_bibblock">
Monfort, M., Andonian, A., Zhou, B., Ramakrishnan, K., Bargal, S.A., Yan, T., Brown, L., Fan, Q., Gutfreund, D., Vondrick, C., etÂ al., 2019.

</span>
<span class="ltx_bibblock">Moments in time dataset: one million videos for event understanding.

</span>
<span class="ltx_bibblock">IEEE transactions on pattern analysis and machine intelligence 42, 502â€“508.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nahin (2015)</span>
<span class="ltx_bibblock">
Nahin, R.L., 2015.

</span>
<span class="ltx_bibblock">Estimates of pain prevalence and severity in adults: United states, 2012.

</span>
<span class="ltx_bibblock">The journal of pain 16, 769â€“780.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel etÂ al. (2003)</span>
<span class="ltx_bibblock">
Patel, M.X., Doku, V., Tennakoon, L., 2003.

</span>
<span class="ltx_bibblock">Challenges in recruitment of research participants.

</span>
<span class="ltx_bibblock">Advances in Psychiatric Treatment 9, 229â€“238.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1192/apt.9.3.229" title="">10.1192/apt.9.3.229</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phillips (2009)</span>
<span class="ltx_bibblock">
Phillips, C.J., 2009.

</span>
<span class="ltx_bibblock">The cost and burden of chronic pain.

</span>
<span class="ltx_bibblock">Reviews in pain 3, 2â€“5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pikulkaew and Chouvatut (2021)</span>
<span class="ltx_bibblock">
Pikulkaew, K., Chouvatut, V., 2021.

</span>
<span class="ltx_bibblock">Enhanced pain detection and movement of motion with data augmentation based on deep learning, in: 2021 13th International Conference on Knowledge and Smart Technology (KST), pp. 197â€“201.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/KST51265.2021.9415827" title="">10.1109/KST51265.2021.9415827</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Price and Cohen (2019)</span>
<span class="ltx_bibblock">
Price, W.N., Cohen, I.G., 2019.

</span>
<span class="ltx_bibblock">Privacy in the age of medical big data.

</span>
<span class="ltx_bibblock">Nature medicine 25, 37â€“43.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Resnick etÂ al. (2019)</span>
<span class="ltx_bibblock">
Resnick, B., Boltz, M., Galik, E., Holmes, S., Vigne, E., Fix, S., Zhu, S., 2019.

</span>
<span class="ltx_bibblock">Pain assessment, management, and impact among older adults in assisted living.

</span>
<span class="ltx_bibblock">Pain Management Nursing 20, 192â€“197.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richter etÂ al. (2017)</span>
<span class="ltx_bibblock">
Richter, S.R., Hayder, Z., Koltun, V., 2017.

</span>
<span class="ltx_bibblock">Playing for benchmarks.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1709.07322" title="">https://arxiv.org/abs/1709.07322</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.48550/ARXIV.1709.07322" title="">10.48550/ARXIV.1709.07322</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richter etÂ al. (2016a)</span>
<span class="ltx_bibblock">
Richter, S.R., Vineet, V., Roth, S., Koltun, V., 2016a.

</span>
<span class="ltx_bibblock">Playing for data: Ground truth from computer games, in: Computer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, Springer. pp. 102â€“118.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richter etÂ al. (2016b)</span>
<span class="ltx_bibblock">
Richter, S.R., Vineet, V., Roth, S., Koltun, V., 2016b.

</span>
<span class="ltx_bibblock">Playing for data: Ground truth from computer games.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1608.02192" title="">https://arxiv.org/abs/1608.02192</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.48550/ARXIV.1608.02192" title="">10.48550/ARXIV.1608.02192</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rikard (2023)</span>
<span class="ltx_bibblock">
Rikard, S.M., 2023.

</span>
<span class="ltx_bibblock">Chronic pain among adultsâ€”united states, 2019â€“2021.

</span>
<span class="ltx_bibblock">MMWR. Morbidity and Mortality Weekly Report 72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rodriguez etÂ al. (2022)</span>
<span class="ltx_bibblock">
Rodriguez, P., Cucurull, G., GonzÃ lez, J., Gonfaus, J.M., Nasrollahi, K., Moeslund, T.B., Roca, F.X., 2022.

</span>
<span class="ltx_bibblock">Deep pain: Exploiting long short-term memory networks for facial expression classification.

</span>
<span class="ltx_bibblock">IEEE Transactions on Cybernetics 52, 3314â€“3324.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/TCYB.2017.2662199" title="">10.1109/TCYB.2017.2662199</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roitberg etÂ al. (2021a)</span>
<span class="ltx_bibblock">
Roitberg, A., Schneider, D., Djamal, A., Seibold, C., ReiÃŸ, S., Stiefelhagen, R., 2021a.

</span>
<span class="ltx_bibblock">Letâ€™s play for action: Recognizing activities of daily living by learning from life simulation video games, in: 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE. pp. 8563â€“8569.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roitberg etÂ al. (2021b)</span>
<span class="ltx_bibblock">
Roitberg, A., Schneider, D., Djamal, A., Seibold, C., ReiÃŸ, S., Stiefelhagen, R., 2021b.

</span>
<span class="ltx_bibblock">Letâ€™s play for action: Recognizing activities of daily living by learning from life simulation video games.

</span>
<span class="ltx_bibblock">URL: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2107.05617" title="">https://arxiv.org/abs/2107.05617</a>, doi:<a class="ltx_ref" href="https:/doi.org/10.48550/ARXIV.2107.05617" title="">10.48550/ARXIV.2107.05617</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ros etÂ al. (2016)</span>
<span class="ltx_bibblock">
Ros, G., Sellart, L., Materzynska, J., Vazquez, D., Lopez, A.M., 2016.

</span>
<span class="ltx_bibblock">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes, in: Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3234â€“3243.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schiappa etÂ al. (2023)</span>
<span class="ltx_bibblock">
Schiappa, M.C., Biyani, N., Kamtam, P., Vyas, S., Palangi, H., Vineet, V., Rawat, Y.S., 2023.

</span>
<span class="ltx_bibblock">A large-scale robustness analysis of video action recognition models, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14698â€“14708.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soleymani etÂ al. (2011)</span>
<span class="ltx_bibblock">
Soleymani, M., Pantic, M., Pun, T., 2011.

</span>
<span class="ltx_bibblock">Multimodal emotion recognition in response to videos.

</span>
<span class="ltx_bibblock">IEEE transactions on affective computing 3, 211â€“223.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tobon etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tobon, D.P., Arguello, D.P., Gomez-Garcia, J.A., Lopez, J.D., Castellanos-Dominguez, G., 2020.

</span>
<span class="ltx_bibblock">Mintpain: A multimodal database for the assessment of pain in laboratory-induced and clinical environments.

</span>
<span class="ltx_bibblock">IEEE Transactions on Affective Computing 11, 256â€“270.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Walter etÂ al. (2013a)</span>
<span class="ltx_bibblock">
Walter, S., Gruss, S., Ehleiter, H., Tan, J., Traue, H.C., Werner, P., Al-Hamadi, A., Crawcour, S., Andrade, A.O., MoreiraÂ da Silva, G., 2013a.

</span>
<span class="ltx_bibblock">The biovid heat pain database data for the advancement and systematic validation of an automated pain recognition system, in: 2013 IEEE International Conference on Cybernetics (CYBCO), pp. 128â€“131.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/CYBConf.2013.6617456" title="">10.1109/CYBConf.2013.6617456</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Walter etÂ al. (2013b)</span>
<span class="ltx_bibblock">
Walter, S., Gruss, S., Ehleiter, H., Tan, J., Traue, H.C., Werner, P., Al-Hamadi, A., Crawcour, S., Andrade, A.O., daÂ Silva, G.M., 2013b.

</span>
<span class="ltx_bibblock">Data fusion for automated pain recognition, in: Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics, IEEE. pp. 115â€“122.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2017)</span>
<span class="ltx_bibblock">
Wang, F., Xiang, X., Liu, C., Tran, T.D., Reiter, A., Hager, G.D., Quon, H., Cheng, J., Yuille, A.L., 2017.

</span>
<span class="ltx_bibblock">Regularizing face verification nets for pain intensity regression, in: 2017 IEEE International Conference on Image Processing (ICIP), pp. 1087â€“1091.

</span>
<span class="ltx_bibblock">doi:<a class="ltx_ref" href="https:/doi.org/10.1109/ICIP.2017.8296449" title="">10.1109/ICIP.2017.8296449</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2013)</span>
<span class="ltx_bibblock">
Zhang, X., Yin, L., Cohn, J.F., Canavan, S., Reale, M., Horowitz, A., Liu, P., 2013.

</span>
<span class="ltx_bibblock">A high-resolution spontaneous 3d dynamic facial expression database, in: 2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG), IEEE. pp. 1â€“6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2016a)</span>
<span class="ltx_bibblock">
Zhang, Z., Girard, J.M., Wu, Y., Zhang, X., Liu, P., Ciftci, U., Canavan, S., Reale, M., Horowitz, A., Yang, H., Cohn, J.F., Ji, Q., Yin, L., 2016a.

</span>
<span class="ltx_bibblock">Multimodal spontaneous emotion corpus for human behavior analysis, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2016b)</span>
<span class="ltx_bibblock">
Zhang, Z., Yin, L., Cohn, J.F., Canavan, S., Horowitz, A., Liu, P., Girard, J.M., Reale, M., 2016b.

</span>
<span class="ltx_bibblock">Bp4d+ : A multi-modal spontaneous facial expression database with annotations of basic expressions and micro-expressions.

</span>
<span class="ltx_bibblock">Image and Vision Computing 55, 36â€“48.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2014)</span>
<span class="ltx_bibblock">
Zhang, Z., Yin, L., Cohn, J.F., Canavan, S., Reale, M., Horowitz, A., Liu, P., Girard, J.M., 2014.

</span>
<span class="ltx_bibblock">Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database.

</span>
<span class="ltx_bibblock">Image and Vision Computing 32, 692â€“706.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhu, H., Wu, W., Zhu, W., Jiang, L., Tang, S., Zhang, L., Liu, Z., Loy, C.C., 2022.

</span>
<span class="ltx_bibblock">CelebV-HQ: A large-scale video facial attributes dataset, in: ECCV.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Sep 23 16:12:38 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
